{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# 选择预训练模型，例如 \"bert-base-uncased\"\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "def get_text_embedding(text):\n",
    "    # 将文本进行 tokenization，注意这里可以设置 truncation、padding 等参数\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    # 前向推理，不计算梯度\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # outputs.last_hidden_state 的 shape 为 [batch_size, sequence_length, hidden_dim]\n",
    "    # 通常我们取 [CLS] 位置的向量作为句子的表示，即 outputs.last_hidden_state[:, 0, :]\n",
    "    embedding = outputs.last_hidden_state[:, 0, :]\n",
    "    return embedding\n",
    "\n",
    "# 示例调用\n",
    "ground_truth_text = \"This is a sample text.\"\n",
    "llm_text_embedding = get_text_embedding(ground_truth_text)\n",
    "print(llm_text_embedding.shape)  # 输出类似 torch.Size([1, 768])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
