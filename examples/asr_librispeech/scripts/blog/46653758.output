/work/van-speech-nlp/jindaznb/slamenv/bin/python
task_flag: all
train_data_folder: aphasia_phoneme
test_data_folder: aphasia_phoneme
use_peft: true
seed: 
debug: 
Is test_run? 
freeze_encoder: true
Is save_embedding? false
projector_transfer_learning: true
transfer_data_folder: librispeech-100_phoneme
llm_inference_config: repetition_penalty
eval_ckpt: best
----------
----------
Final identifier: aphasia_phoneme_wavlm_llama32_1b_linear_peft
ckpt_folder: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_2_step_23834_loss_0.4641912281513214



----- Transfer Learning Information -----
Resume Epoch: 1
Resume Step: 0
Train Data Path: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/librispeech-100_phoneme/train.jsonl
Validation Data Path: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/librispeech-100_phoneme/validation.jsonl
Test Data Path: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/librispeech-100_phoneme/test.jsonl
Identifier: aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme
Output Directory: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme
----------------------------------------
----------------------------------------
Resume epoch: 1
Resume step: 0
[2025-02-13 02:24:51][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 2, 'resume_step': 0, 'resume_epoch': 1, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 3000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': True, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': False, 'freeze_encoder': True, 'freeze_encoder2': True, 'save_embedding': False}
[2025-02-13 02:24:51][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': True, 'sharding_strategy': <ShardingStrategy.NO_SHARD: 3>, 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2025-02-13 02:24:51][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'llm_inference_config': 'repetition_penalty', 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'qformer_layers': 8, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': '', 'identifier': 'aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme'}
[2025-02-13 02:24:51][root][INFO] - log_config: {'use_wandb': True, 'wandb_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/wandb_log', 'wandb_entity_name': 'jindaz-work', 'wandb_project_name': 'SLAM-LLM', 'wandb_exp_name': 'aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme', 'log_file': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/2025-02-13_02-24-51.txt', 'log_interval': 5}
[2025-02-13 02:25:13][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2025-02-13 02:25:19][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-02-13 02:25:19][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2025-02-13 02:25:19][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-02-13 02:25:19][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2025-02-13 02:25:26][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-02-13 02:25:26][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2025-02-13 02:25:26][slam_llm.models.slam_model][INFO] - setup peft...
trainable params: 5,636,096 || all params: 1,241,450,496 || trainable%: 0.4539928106807088
[2025-02-13 02:25:26][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-02-13 02:25:26][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2025-02-13 02:25:26][slam_llm.utils.train_utils][INFO] - --> Module linear
[2025-02-13 02:25:26][slam_llm.utils.train_utils][INFO] - --> linear has 14.68416 Million params

[2025-02-13 02:25:26][slam_model_asr.py][INFO] - loading other parts from: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_2_step_23834_loss_0.4641912281513214/model.pt
[2025-02-13 02:25:26][slam_llm.utils.train_utils][INFO] - --> Model asr
[2025-02-13 02:25:26][slam_llm.utils.train_utils][INFO] - --> asr has 20.320256 Million params

[2025-02-13 02:25:28][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/librispeech-100_phoneme/train.jsonl', 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/librispeech-100_phoneme/validation.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': False, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2025-02-13 02:25:30][root][INFO] - --> Training Set Length = 28539
[2025-02-13 02:25:30][root][INFO] - --> Validation Set Length = 2703
[2025-02-13 02:25:30][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2025-02-13 02:25:30][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2025-02-13 02:25:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:34][root][INFO] - Training Epoch: 1/2, step 0/7134 completed (loss: 1.81589674949646, acc: 0.7072808146476746)
[2025-02-13 02:25:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:35][root][INFO] - Training Epoch: 1/2, step 1/7134 completed (loss: 1.7824138402938843, acc: 0.6806136965751648)
[2025-02-13 02:25:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:35][root][INFO] - Training Epoch: 1/2, step 2/7134 completed (loss: 1.7117947340011597, acc: 0.6867470145225525)
[2025-02-13 02:25:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:36][root][INFO] - Training Epoch: 1/2, step 3/7134 completed (loss: 1.6336023807525635, acc: 0.7221511006355286)
[2025-02-13 02:25:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:36][root][INFO] - Training Epoch: 1/2, step 4/7134 completed (loss: 1.7705706357955933, acc: 0.6807639598846436)
[2025-02-13 02:25:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:37][root][INFO] - Training Epoch: 1/2, step 5/7134 completed (loss: 1.7480140924453735, acc: 0.6814720630645752)
[2025-02-13 02:25:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:37][root][INFO] - Training Epoch: 1/2, step 6/7134 completed (loss: 1.7428572177886963, acc: 0.6869983673095703)
[2025-02-13 02:25:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:38][root][INFO] - Training Epoch: 1/2, step 7/7134 completed (loss: 1.7149430513381958, acc: 0.6787048578262329)
[2025-02-13 02:25:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:38][root][INFO] - Training Epoch: 1/2, step 8/7134 completed (loss: 1.7345303297042847, acc: 0.6891334056854248)
[2025-02-13 02:25:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:39][root][INFO] - Training Epoch: 1/2, step 9/7134 completed (loss: 1.7656646966934204, acc: 0.739534854888916)
[2025-02-13 02:25:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:39][root][INFO] - Training Epoch: 1/2, step 10/7134 completed (loss: 1.899036169052124, acc: 0.6216216087341309)
[2025-02-13 02:25:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:40][root][INFO] - Training Epoch: 1/2, step 11/7134 completed (loss: 1.9038385152816772, acc: 0.6547842621803284)
[2025-02-13 02:25:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:40][root][INFO] - Training Epoch: 1/2, step 12/7134 completed (loss: 1.6931838989257812, acc: 0.6985195279121399)
[2025-02-13 02:25:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:41][root][INFO] - Training Epoch: 1/2, step 13/7134 completed (loss: 1.6875983476638794, acc: 0.685006856918335)
[2025-02-13 02:25:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:41][root][INFO] - Training Epoch: 1/2, step 14/7134 completed (loss: 1.7099201679229736, acc: 0.7058823704719543)
[2025-02-13 02:25:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:42][root][INFO] - Training Epoch: 1/2, step 15/7134 completed (loss: 1.6830589771270752, acc: 0.7125645279884338)
[2025-02-13 02:25:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:42][root][INFO] - Training Epoch: 1/2, step 16/7134 completed (loss: 1.6518266201019287, acc: 0.7337398529052734)
[2025-02-13 02:25:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:43][root][INFO] - Training Epoch: 1/2, step 17/7134 completed (loss: 1.9363434314727783, acc: 0.6891891956329346)
[2025-02-13 02:25:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:43][root][INFO] - Training Epoch: 1/2, step 18/7134 completed (loss: 1.5763130187988281, acc: 0.7201704382896423)
[2025-02-13 02:25:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:44][root][INFO] - Training Epoch: 1/2, step 19/7134 completed (loss: 1.7060070037841797, acc: 0.6841379404067993)
[2025-02-13 02:25:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:44][root][INFO] - Training Epoch: 1/2, step 20/7134 completed (loss: 1.595381736755371, acc: 0.699999988079071)
[2025-02-13 02:25:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:45][root][INFO] - Training Epoch: 1/2, step 21/7134 completed (loss: 1.5044167041778564, acc: 0.7036082744598389)
[2025-02-13 02:25:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:45][root][INFO] - Training Epoch: 1/2, step 22/7134 completed (loss: 1.5406852960586548, acc: 0.7017291188240051)
[2025-02-13 02:25:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:45][root][INFO] - Training Epoch: 1/2, step 23/7134 completed (loss: 1.541621446609497, acc: 0.7024456262588501)
[2025-02-13 02:25:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:46][root][INFO] - Training Epoch: 1/2, step 24/7134 completed (loss: 1.6791553497314453, acc: 0.6573815941810608)
[2025-02-13 02:25:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:46][root][INFO] - Training Epoch: 1/2, step 25/7134 completed (loss: 1.5273948907852173, acc: 0.6889204382896423)
[2025-02-13 02:25:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:47][root][INFO] - Training Epoch: 1/2, step 26/7134 completed (loss: 1.4928319454193115, acc: 0.7123894095420837)
[2025-02-13 02:25:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:47][root][INFO] - Training Epoch: 1/2, step 27/7134 completed (loss: 1.4179331064224243, acc: 0.7075332403182983)
[2025-02-13 02:25:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:48][root][INFO] - Training Epoch: 1/2, step 28/7134 completed (loss: 1.5665186643600464, acc: 0.6855421662330627)
[2025-02-13 02:25:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:48][root][INFO] - Training Epoch: 1/2, step 29/7134 completed (loss: 1.5320942401885986, acc: 0.6798866987228394)
[2025-02-13 02:25:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:49][root][INFO] - Training Epoch: 1/2, step 30/7134 completed (loss: 1.5113273859024048, acc: 0.701694905757904)
[2025-02-13 02:25:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:49][root][INFO] - Training Epoch: 1/2, step 31/7134 completed (loss: 1.383274793624878, acc: 0.7012711763381958)
[2025-02-13 02:25:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:50][root][INFO] - Training Epoch: 1/2, step 32/7134 completed (loss: 1.4673463106155396, acc: 0.6736021041870117)
[2025-02-13 02:25:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:50][root][INFO] - Training Epoch: 1/2, step 33/7134 completed (loss: 1.5968679189682007, acc: 0.7091633677482605)
[2025-02-13 02:25:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:51][root][INFO] - Training Epoch: 1/2, step 34/7134 completed (loss: 1.4732706546783447, acc: 0.6998827457427979)
[2025-02-13 02:25:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:51][root][INFO] - Training Epoch: 1/2, step 35/7134 completed (loss: 1.3803714513778687, acc: 0.6974552273750305)
[2025-02-13 02:25:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:52][root][INFO] - Training Epoch: 1/2, step 36/7134 completed (loss: 1.425329327583313, acc: 0.7079261541366577)
[2025-02-13 02:25:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:52][root][INFO] - Training Epoch: 1/2, step 37/7134 completed (loss: 1.365985631942749, acc: 0.6883604526519775)
[2025-02-13 02:25:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:53][root][INFO] - Training Epoch: 1/2, step 38/7134 completed (loss: 1.250481128692627, acc: 0.7172619104385376)
[2025-02-13 02:25:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:53][root][INFO] - Training Epoch: 1/2, step 39/7134 completed (loss: 1.130679965019226, acc: 0.736129879951477)
[2025-02-13 02:25:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:54][root][INFO] - Training Epoch: 1/2, step 40/7134 completed (loss: 1.3949105739593506, acc: 0.7219731211662292)
[2025-02-13 02:25:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:54][root][INFO] - Training Epoch: 1/2, step 41/7134 completed (loss: 1.3116445541381836, acc: 0.708737850189209)
[2025-02-13 02:25:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:55][root][INFO] - Training Epoch: 1/2, step 42/7134 completed (loss: 1.0851092338562012, acc: 0.7265077233314514)
[2025-02-13 02:25:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:55][root][INFO] - Training Epoch: 1/2, step 43/7134 completed (loss: 0.9873887896537781, acc: 0.7470967769622803)
[2025-02-13 02:25:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:56][root][INFO] - Training Epoch: 1/2, step 44/7134 completed (loss: 1.104315161705017, acc: 0.7235668897628784)
[2025-02-13 02:25:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:56][root][INFO] - Training Epoch: 1/2, step 45/7134 completed (loss: 1.0178989171981812, acc: 0.7356446385383606)
[2025-02-13 02:25:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:57][root][INFO] - Training Epoch: 1/2, step 46/7134 completed (loss: 1.0468528270721436, acc: 0.7386666536331177)
[2025-02-13 02:25:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:57][root][INFO] - Training Epoch: 1/2, step 47/7134 completed (loss: 1.205553412437439, acc: 0.701298713684082)
[2025-02-13 02:25:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:58][root][INFO] - Training Epoch: 1/2, step 48/7134 completed (loss: 1.0245317220687866, acc: 0.7191011309623718)
[2025-02-13 02:25:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:58][root][INFO] - Training Epoch: 1/2, step 49/7134 completed (loss: 0.9178121089935303, acc: 0.7297667860984802)
[2025-02-13 02:25:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:59][root][INFO] - Training Epoch: 1/2, step 50/7134 completed (loss: 0.9737257957458496, acc: 0.7208765745162964)
[2025-02-13 02:25:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:25:59][root][INFO] - Training Epoch: 1/2, step 51/7134 completed (loss: 0.9865908026695251, acc: 0.7271573543548584)
[2025-02-13 02:25:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:00][root][INFO] - Training Epoch: 1/2, step 52/7134 completed (loss: 0.9357008934020996, acc: 0.7317073345184326)
[2025-02-13 02:26:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:00][root][INFO] - Training Epoch: 1/2, step 53/7134 completed (loss: 0.8352912664413452, acc: 0.7572178244590759)
[2025-02-13 02:26:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:01][root][INFO] - Training Epoch: 1/2, step 54/7134 completed (loss: 0.8201045989990234, acc: 0.746835470199585)
[2025-02-13 02:26:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:01][root][INFO] - Training Epoch: 1/2, step 55/7134 completed (loss: 0.8122846484184265, acc: 0.7588832378387451)
[2025-02-13 02:26:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:01][root][INFO] - Training Epoch: 1/2, step 56/7134 completed (loss: 0.7417284846305847, acc: 0.7678571343421936)
[2025-02-13 02:26:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:02][root][INFO] - Training Epoch: 1/2, step 57/7134 completed (loss: 0.7689116597175598, acc: 0.7577639818191528)
[2025-02-13 02:26:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:02][root][INFO] - Training Epoch: 1/2, step 58/7134 completed (loss: 0.7638981938362122, acc: 0.7618438005447388)
[2025-02-13 02:26:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:03][root][INFO] - Training Epoch: 1/2, step 59/7134 completed (loss: 0.6828663945198059, acc: 0.7576832175254822)
[2025-02-13 02:26:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:03][root][INFO] - Training Epoch: 1/2, step 60/7134 completed (loss: 0.6959280967712402, acc: 0.7784090638160706)
[2025-02-13 02:26:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:04][root][INFO] - Training Epoch: 1/2, step 61/7134 completed (loss: 0.6470330357551575, acc: 0.7845777273178101)
[2025-02-13 02:26:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:04][root][INFO] - Training Epoch: 1/2, step 62/7134 completed (loss: 0.580108106136322, acc: 0.804320216178894)
[2025-02-13 02:26:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:05][root][INFO] - Training Epoch: 1/2, step 63/7134 completed (loss: 0.735623836517334, acc: 0.7721046209335327)
[2025-02-13 02:26:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:05][root][INFO] - Training Epoch: 1/2, step 64/7134 completed (loss: 0.5593338012695312, acc: 0.8131720423698425)
[2025-02-13 02:26:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:06][root][INFO] - Training Epoch: 1/2, step 65/7134 completed (loss: 0.6632651686668396, acc: 0.7985258102416992)
[2025-02-13 02:26:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:06][root][INFO] - Training Epoch: 1/2, step 66/7134 completed (loss: 0.618274986743927, acc: 0.8118279576301575)
[2025-02-13 02:26:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:07][root][INFO] - Training Epoch: 1/2, step 67/7134 completed (loss: 0.6734211444854736, acc: 0.7921568751335144)
[2025-02-13 02:26:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:07][root][INFO] - Training Epoch: 1/2, step 68/7134 completed (loss: 0.638731837272644, acc: 0.8258064389228821)
[2025-02-13 02:26:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:08][root][INFO] - Training Epoch: 1/2, step 69/7134 completed (loss: 0.6016532778739929, acc: 0.8329176902770996)
[2025-02-13 02:26:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:08][root][INFO] - Training Epoch: 1/2, step 70/7134 completed (loss: 0.5612282752990723, acc: 0.840350866317749)
[2025-02-13 02:26:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:09][root][INFO] - Training Epoch: 1/2, step 71/7134 completed (loss: 0.4152522087097168, acc: 0.8977590799331665)
[2025-02-13 02:26:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:09][root][INFO] - Training Epoch: 1/2, step 72/7134 completed (loss: 0.5133895874023438, acc: 0.8682864308357239)
[2025-02-13 02:26:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:10][root][INFO] - Training Epoch: 1/2, step 73/7134 completed (loss: 0.4626431167125702, acc: 0.885869562625885)
[2025-02-13 02:26:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:10][root][INFO] - Training Epoch: 1/2, step 74/7134 completed (loss: 0.592807412147522, acc: 0.8635097742080688)
[2025-02-13 02:26:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:11][root][INFO] - Training Epoch: 1/2, step 75/7134 completed (loss: 0.6335718035697937, acc: 0.8624812960624695)
[2025-02-13 02:26:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:11][root][INFO] - Training Epoch: 1/2, step 76/7134 completed (loss: 0.3318762481212616, acc: 0.9231737852096558)
[2025-02-13 02:26:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:12][root][INFO] - Training Epoch: 1/2, step 77/7134 completed (loss: 0.499213844537735, acc: 0.8922305703163147)
[2025-02-13 02:26:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:12][root][INFO] - Training Epoch: 1/2, step 78/7134 completed (loss: 0.5311733484268188, acc: 0.8632075190544128)
[2025-02-13 02:26:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:13][root][INFO] - Training Epoch: 1/2, step 79/7134 completed (loss: 0.49307042360305786, acc: 0.895061731338501)
[2025-02-13 02:26:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:13][root][INFO] - Training Epoch: 1/2, step 80/7134 completed (loss: 0.3226231038570404, acc: 0.9127604365348816)
[2025-02-13 02:26:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:13][root][INFO] - Training Epoch: 1/2, step 81/7134 completed (loss: 0.41015803813934326, acc: 0.9123036861419678)
[2025-02-13 02:26:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:14][root][INFO] - Training Epoch: 1/2, step 82/7134 completed (loss: 0.36053329706192017, acc: 0.9222065210342407)
[2025-02-13 02:26:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:14][root][INFO] - Training Epoch: 1/2, step 83/7134 completed (loss: 0.5311400890350342, acc: 0.8740360140800476)
[2025-02-13 02:26:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:15][root][INFO] - Training Epoch: 1/2, step 84/7134 completed (loss: 0.43147262930870056, acc: 0.8894646167755127)
[2025-02-13 02:26:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:15][root][INFO] - Training Epoch: 1/2, step 85/7134 completed (loss: 0.42810556292533875, acc: 0.8949880599975586)
[2025-02-13 02:26:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:16][root][INFO] - Training Epoch: 1/2, step 86/7134 completed (loss: 0.46969521045684814, acc: 0.8830274939537048)
[2025-02-13 02:26:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:16][root][INFO] - Training Epoch: 1/2, step 87/7134 completed (loss: 0.4672994017601013, acc: 0.8751835823059082)
[2025-02-13 02:26:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:17][root][INFO] - Training Epoch: 1/2, step 88/7134 completed (loss: 0.5318566560745239, acc: 0.8769018054008484)
[2025-02-13 02:26:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:17][root][INFO] - Training Epoch: 1/2, step 89/7134 completed (loss: 0.3963176906108856, acc: 0.912708580493927)
[2025-02-13 02:26:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:18][root][INFO] - Training Epoch: 1/2, step 90/7134 completed (loss: 0.3476097881793976, acc: 0.8968531489372253)
[2025-02-13 02:26:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:18][root][INFO] - Training Epoch: 1/2, step 91/7134 completed (loss: 0.4210209548473358, acc: 0.8979591727256775)
[2025-02-13 02:26:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:19][root][INFO] - Training Epoch: 1/2, step 92/7134 completed (loss: 0.43068861961364746, acc: 0.8892005681991577)
[2025-02-13 02:26:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:19][root][INFO] - Training Epoch: 1/2, step 93/7134 completed (loss: 0.34267550706863403, acc: 0.918732762336731)
[2025-02-13 02:26:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:20][root][INFO] - Training Epoch: 1/2, step 94/7134 completed (loss: 0.4818271994590759, acc: 0.8886311054229736)
[2025-02-13 02:26:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:20][root][INFO] - Training Epoch: 1/2, step 95/7134 completed (loss: 0.42687535285949707, acc: 0.8856784105300903)
[2025-02-13 02:26:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:21][root][INFO] - Training Epoch: 1/2, step 96/7134 completed (loss: 0.3723772168159485, acc: 0.9143389463424683)
[2025-02-13 02:26:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:21][root][INFO] - Training Epoch: 1/2, step 97/7134 completed (loss: 0.7417758107185364, acc: 0.7831325531005859)
[2025-02-13 02:26:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:22][root][INFO] - Training Epoch: 1/2, step 98/7134 completed (loss: 0.4997236132621765, acc: 0.8802589178085327)
[2025-02-13 02:26:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:22][root][INFO] - Training Epoch: 1/2, step 99/7134 completed (loss: 0.4001445472240448, acc: 0.9027522802352905)
[2025-02-13 02:26:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:23][root][INFO] - Training Epoch: 1/2, step 100/7134 completed (loss: 0.3738180994987488, acc: 0.9076923131942749)
[2025-02-13 02:26:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:23][root][INFO] - Training Epoch: 1/2, step 101/7134 completed (loss: 0.39760348200798035, acc: 0.8848684430122375)
[2025-02-13 02:26:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:23][root][INFO] - Training Epoch: 1/2, step 102/7134 completed (loss: 0.3454701602458954, acc: 0.9242979288101196)
[2025-02-13 02:26:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:24][root][INFO] - Training Epoch: 1/2, step 103/7134 completed (loss: 0.2707419693470001, acc: 0.925000011920929)
[2025-02-13 02:26:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:24][root][INFO] - Training Epoch: 1/2, step 104/7134 completed (loss: 0.34490907192230225, acc: 0.9108761548995972)
[2025-02-13 02:26:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:25][root][INFO] - Training Epoch: 1/2, step 105/7134 completed (loss: 0.3771321177482605, acc: 0.8988580703735352)
[2025-02-13 02:26:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:25][root][INFO] - Training Epoch: 1/2, step 106/7134 completed (loss: 0.3269002139568329, acc: 0.9146800637245178)
[2025-02-13 02:26:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:26][root][INFO] - Training Epoch: 1/2, step 107/7134 completed (loss: 0.539667546749115, acc: 0.8696275353431702)
[2025-02-13 02:26:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:26][root][INFO] - Training Epoch: 1/2, step 108/7134 completed (loss: 0.33464545011520386, acc: 0.9172661900520325)
[2025-02-13 02:26:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:27][root][INFO] - Training Epoch: 1/2, step 109/7134 completed (loss: 0.3415128290653229, acc: 0.9099264740943909)
[2025-02-13 02:26:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:27][root][INFO] - Training Epoch: 1/2, step 110/7134 completed (loss: 0.37906235456466675, acc: 0.9001175165176392)
[2025-02-13 02:26:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:28][root][INFO] - Training Epoch: 1/2, step 111/7134 completed (loss: 0.3100696802139282, acc: 0.911149799823761)
[2025-02-13 02:26:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:28][root][INFO] - Training Epoch: 1/2, step 112/7134 completed (loss: 0.2787919044494629, acc: 0.9337441921234131)
[2025-02-13 02:26:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:29][root][INFO] - Training Epoch: 1/2, step 113/7134 completed (loss: 0.24222755432128906, acc: 0.945569634437561)
[2025-02-13 02:26:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:29][root][INFO] - Training Epoch: 1/2, step 114/7134 completed (loss: 0.46228277683258057, acc: 0.895225465297699)
[2025-02-13 02:26:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:30][root][INFO] - Training Epoch: 1/2, step 115/7134 completed (loss: 0.5711004734039307, acc: 0.8766404390335083)
[2025-02-13 02:26:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:30][root][INFO] - Training Epoch: 1/2, step 116/7134 completed (loss: 0.37823206186294556, acc: 0.8963486552238464)
[2025-02-13 02:26:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:31][root][INFO] - Training Epoch: 1/2, step 117/7134 completed (loss: 0.29920992255210876, acc: 0.9335179924964905)
[2025-02-13 02:26:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:31][root][INFO] - Training Epoch: 1/2, step 118/7134 completed (loss: 0.2994917333126068, acc: 0.9196428656578064)
[2025-02-13 02:26:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:32][root][INFO] - Training Epoch: 1/2, step 119/7134 completed (loss: 0.3429003059864044, acc: 0.9225181341171265)
[2025-02-13 02:26:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:32][root][INFO] - Training Epoch: 1/2, step 120/7134 completed (loss: 0.23130826652050018, acc: 0.9410222768783569)
[2025-02-13 02:26:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:32][root][INFO] - Training Epoch: 1/2, step 121/7134 completed (loss: 0.209426611661911, acc: 0.9445178508758545)
[2025-02-13 02:26:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:33][root][INFO] - Training Epoch: 1/2, step 122/7134 completed (loss: 0.2895333468914032, acc: 0.9245033264160156)
[2025-02-13 02:26:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:33][root][INFO] - Training Epoch: 1/2, step 123/7134 completed (loss: 0.24122656881809235, acc: 0.9459459185600281)
[2025-02-13 02:26:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:34][root][INFO] - Training Epoch: 1/2, step 124/7134 completed (loss: 0.30129361152648926, acc: 0.9293103218078613)
[2025-02-13 02:26:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:34][root][INFO] - Training Epoch: 1/2, step 125/7134 completed (loss: 0.27473801374435425, acc: 0.9336283206939697)
[2025-02-13 02:26:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:35][root][INFO] - Training Epoch: 1/2, step 126/7134 completed (loss: 0.27494922280311584, acc: 0.9359999895095825)
[2025-02-13 02:26:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:35][root][INFO] - Training Epoch: 1/2, step 127/7134 completed (loss: 0.24758440256118774, acc: 0.934567928314209)
[2025-02-13 02:26:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:36][root][INFO] - Training Epoch: 1/2, step 128/7134 completed (loss: 0.3281082808971405, acc: 0.926182210445404)
[2025-02-13 02:26:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:36][root][INFO] - Training Epoch: 1/2, step 129/7134 completed (loss: 0.31976044178009033, acc: 0.9166666865348816)
[2025-02-13 02:26:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:37][root][INFO] - Training Epoch: 1/2, step 130/7134 completed (loss: 0.2742856442928314, acc: 0.9236842393875122)
[2025-02-13 02:26:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:37][root][INFO] - Training Epoch: 1/2, step 131/7134 completed (loss: 0.2186175286769867, acc: 0.9447368383407593)
[2025-02-13 02:26:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:38][root][INFO] - Training Epoch: 1/2, step 132/7134 completed (loss: 0.3034435510635376, acc: 0.9230769276618958)
[2025-02-13 02:26:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:38][root][INFO] - Training Epoch: 1/2, step 133/7134 completed (loss: 0.25808003544807434, acc: 0.9351081252098083)
[2025-02-13 02:26:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:39][root][INFO] - Training Epoch: 1/2, step 134/7134 completed (loss: 0.15091460943222046, acc: 0.9681416153907776)
[2025-02-13 02:26:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:39][root][INFO] - Training Epoch: 1/2, step 135/7134 completed (loss: 0.24609802663326263, acc: 0.9444444179534912)
[2025-02-13 02:26:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:40][root][INFO] - Training Epoch: 1/2, step 136/7134 completed (loss: 0.16311071813106537, acc: 0.9474313259124756)
[2025-02-13 02:26:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:40][root][INFO] - Training Epoch: 1/2, step 137/7134 completed (loss: 0.31252777576446533, acc: 0.9323493242263794)
[2025-02-13 02:26:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:41][root][INFO] - Training Epoch: 1/2, step 138/7134 completed (loss: 0.31264206767082214, acc: 0.9304677844047546)
[2025-02-13 02:26:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:41][root][INFO] - Training Epoch: 1/2, step 139/7134 completed (loss: 0.2976612448692322, acc: 0.9335038065910339)
[2025-02-13 02:26:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:42][root][INFO] - Training Epoch: 1/2, step 140/7134 completed (loss: 0.298213928937912, acc: 0.937313437461853)
[2025-02-13 02:26:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:42][root][INFO] - Training Epoch: 1/2, step 141/7134 completed (loss: 0.2593723237514496, acc: 0.9422169923782349)
[2025-02-13 02:26:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:43][root][INFO] - Training Epoch: 1/2, step 142/7134 completed (loss: 0.2874986231327057, acc: 0.9304589629173279)
[2025-02-13 02:26:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:43][root][INFO] - Training Epoch: 1/2, step 143/7134 completed (loss: 0.17879463732242584, acc: 0.958279013633728)
[2025-02-13 02:26:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:43][root][INFO] - Training Epoch: 1/2, step 144/7134 completed (loss: 0.31650152802467346, acc: 0.9248747825622559)
[2025-02-13 02:26:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:44][root][INFO] - Training Epoch: 1/2, step 145/7134 completed (loss: 0.3933663070201874, acc: 0.9112675786018372)
[2025-02-13 02:26:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:44][root][INFO] - Training Epoch: 1/2, step 146/7134 completed (loss: 0.35669559240341187, acc: 0.9074074029922485)
[2025-02-13 02:26:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:45][root][INFO] - Training Epoch: 1/2, step 147/7134 completed (loss: 0.34696611762046814, acc: 0.923743486404419)
[2025-02-13 02:26:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:45][root][INFO] - Training Epoch: 1/2, step 148/7134 completed (loss: 0.4911014139652252, acc: 0.8793503642082214)
[2025-02-13 02:26:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:46][root][INFO] - Training Epoch: 1/2, step 149/7134 completed (loss: 0.35773253440856934, acc: 0.9200000166893005)
[2025-02-13 02:26:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:46][root][INFO] - Training Epoch: 1/2, step 150/7134 completed (loss: 0.3522765636444092, acc: 0.9197860956192017)
[2025-02-13 02:26:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:47][root][INFO] - Training Epoch: 1/2, step 151/7134 completed (loss: 0.23682308197021484, acc: 0.946300745010376)
[2025-02-13 02:26:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:47][root][INFO] - Training Epoch: 1/2, step 152/7134 completed (loss: 0.29042258858680725, acc: 0.9419134259223938)
[2025-02-13 02:26:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:48][root][INFO] - Training Epoch: 1/2, step 153/7134 completed (loss: 0.32051682472229004, acc: 0.9203910827636719)
[2025-02-13 02:26:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:48][root][INFO] - Training Epoch: 1/2, step 154/7134 completed (loss: 0.31979021430015564, acc: 0.9209726452827454)
[2025-02-13 02:26:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:49][root][INFO] - Training Epoch: 1/2, step 155/7134 completed (loss: 0.2999839186668396, acc: 0.9311688542366028)
[2025-02-13 02:26:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:49][root][INFO] - Training Epoch: 1/2, step 156/7134 completed (loss: 0.2318909615278244, acc: 0.9404900670051575)
[2025-02-13 02:26:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:50][root][INFO] - Training Epoch: 1/2, step 157/7134 completed (loss: 0.31619203090667725, acc: 0.9216867685317993)
[2025-02-13 02:26:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:50][root][INFO] - Training Epoch: 1/2, step 158/7134 completed (loss: 0.13962584733963013, acc: 0.95772784948349)
[2025-02-13 02:26:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:51][root][INFO] - Training Epoch: 1/2, step 159/7134 completed (loss: 0.27013447880744934, acc: 0.9336219429969788)
[2025-02-13 02:26:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:51][root][INFO] - Training Epoch: 1/2, step 160/7134 completed (loss: 0.31372568011283875, acc: 0.931174099445343)
[2025-02-13 02:26:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:51][root][INFO] - Training Epoch: 1/2, step 161/7134 completed (loss: 0.1773071438074112, acc: 0.9389534592628479)
[2025-02-13 02:26:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:52][root][INFO] - Training Epoch: 1/2, step 162/7134 completed (loss: 0.1422172635793686, acc: 0.9590792655944824)
[2025-02-13 02:26:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:52][root][INFO] - Training Epoch: 1/2, step 163/7134 completed (loss: 0.3002340495586395, acc: 0.9261447787284851)
[2025-02-13 02:26:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:53][root][INFO] - Training Epoch: 1/2, step 164/7134 completed (loss: 0.18297138810157776, acc: 0.9425531625747681)
[2025-02-13 02:26:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:53][root][INFO] - Training Epoch: 1/2, step 165/7134 completed (loss: 0.25426599383354187, acc: 0.9330143332481384)
[2025-02-13 02:26:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:53][root][INFO] - Training Epoch: 1/2, step 166/7134 completed (loss: 0.18127313256263733, acc: 0.9462686777114868)
[2025-02-13 02:26:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:54][root][INFO] - Training Epoch: 1/2, step 167/7134 completed (loss: 0.7976261377334595, acc: 0.843137264251709)
[2025-02-13 02:26:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:54][root][INFO] - Training Epoch: 1/2, step 168/7134 completed (loss: 2.192894697189331, acc: 0.5921568870544434)
[2025-02-13 02:26:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:55][root][INFO] - Training Epoch: 1/2, step 169/7134 completed (loss: 1.6097214221954346, acc: 0.686274528503418)
[2025-02-13 02:26:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:55][root][INFO] - Training Epoch: 1/2, step 170/7134 completed (loss: 0.7936126589775085, acc: 0.8121442198753357)
[2025-02-13 02:26:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:56][root][INFO] - Training Epoch: 1/2, step 171/7134 completed (loss: 1.0918822288513184, acc: 0.7759740352630615)
[2025-02-13 02:26:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:56][root][INFO] - Training Epoch: 1/2, step 172/7134 completed (loss: 0.45611071586608887, acc: 0.8801956176757812)
[2025-02-13 02:26:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:56][root][INFO] - Training Epoch: 1/2, step 173/7134 completed (loss: 0.3325079381465912, acc: 0.9137167930603027)
[2025-02-13 02:26:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:57][root][INFO] - Training Epoch: 1/2, step 174/7134 completed (loss: 0.353627473115921, acc: 0.9154929518699646)
[2025-02-13 02:26:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:57][root][INFO] - Training Epoch: 1/2, step 175/7134 completed (loss: 0.37847816944122314, acc: 0.9127272963523865)
[2025-02-13 02:26:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:58][root][INFO] - Training Epoch: 1/2, step 176/7134 completed (loss: 0.37690266966819763, acc: 0.9072463512420654)
[2025-02-13 02:26:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:58][root][INFO] - Training Epoch: 1/2, step 177/7134 completed (loss: 0.34888219833374023, acc: 0.9259259104728699)
[2025-02-13 02:26:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:59][root][INFO] - Training Epoch: 1/2, step 178/7134 completed (loss: 0.37882983684539795, acc: 0.8837209343910217)
[2025-02-13 02:26:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:26:59][root][INFO] - Training Epoch: 1/2, step 179/7134 completed (loss: 0.36292850971221924, acc: 0.9090909361839294)
[2025-02-13 02:26:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:00][root][INFO] - Training Epoch: 1/2, step 180/7134 completed (loss: 0.20818781852722168, acc: 0.9473007917404175)
[2025-02-13 02:27:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:00][root][INFO] - Training Epoch: 1/2, step 181/7134 completed (loss: 0.16962069272994995, acc: 0.9538866877555847)
[2025-02-13 02:27:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:01][root][INFO] - Training Epoch: 1/2, step 182/7134 completed (loss: 0.16642870008945465, acc: 0.9560439586639404)
[2025-02-13 02:27:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:01][root][INFO] - Training Epoch: 1/2, step 183/7134 completed (loss: 0.2167319506406784, acc: 0.9414169192314148)
[2025-02-13 02:27:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:02][root][INFO] - Training Epoch: 1/2, step 184/7134 completed (loss: 0.17659610509872437, acc: 0.9569892287254333)
[2025-02-13 02:27:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:02][root][INFO] - Training Epoch: 1/2, step 185/7134 completed (loss: 0.12703105807304382, acc: 0.9685264825820923)
[2025-02-13 02:27:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:02][root][INFO] - Training Epoch: 1/2, step 186/7134 completed (loss: 0.1891937404870987, acc: 0.9439102411270142)
[2025-02-13 02:27:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:03][root][INFO] - Training Epoch: 1/2, step 187/7134 completed (loss: 0.1759214997291565, acc: 0.9553072452545166)
[2025-02-13 02:27:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:03][root][INFO] - Training Epoch: 1/2, step 188/7134 completed (loss: 0.24504360556602478, acc: 0.9291666746139526)
[2025-02-13 02:27:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:04][root][INFO] - Training Epoch: 1/2, step 189/7134 completed (loss: 0.21399329602718353, acc: 0.949999988079071)
[2025-02-13 02:27:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:04][root][INFO] - Training Epoch: 1/2, step 190/7134 completed (loss: 0.2630062401294708, acc: 0.9375975131988525)
[2025-02-13 02:27:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:05][root][INFO] - Training Epoch: 1/2, step 191/7134 completed (loss: 0.18430942296981812, acc: 0.9551281929016113)
[2025-02-13 02:27:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:05][root][INFO] - Training Epoch: 1/2, step 192/7134 completed (loss: 0.12459320574998856, acc: 0.9667811989784241)
[2025-02-13 02:27:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:06][root][INFO] - Training Epoch: 1/2, step 193/7134 completed (loss: 0.23413461446762085, acc: 0.9516778588294983)
[2025-02-13 02:27:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:06][root][INFO] - Training Epoch: 1/2, step 194/7134 completed (loss: 0.2064940631389618, acc: 0.944045901298523)
[2025-02-13 02:27:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:07][root][INFO] - Training Epoch: 1/2, step 195/7134 completed (loss: 0.19716982543468475, acc: 0.9546218514442444)
[2025-02-13 02:27:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:07][root][INFO] - Training Epoch: 1/2, step 196/7134 completed (loss: 0.18381647765636444, acc: 0.9612299203872681)
[2025-02-13 02:27:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:08][root][INFO] - Training Epoch: 1/2, step 197/7134 completed (loss: 0.15316110849380493, acc: 0.9601139426231384)
[2025-02-13 02:27:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:08][root][INFO] - Training Epoch: 1/2, step 198/7134 completed (loss: 0.15212294459342957, acc: 0.9615877270698547)
[2025-02-13 02:27:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:08][root][INFO] - Training Epoch: 1/2, step 199/7134 completed (loss: 0.19363681972026825, acc: 0.9523809552192688)
[2025-02-13 02:27:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:09][root][INFO] - Training Epoch: 1/2, step 200/7134 completed (loss: 0.11893943697214127, acc: 0.9651006460189819)
[2025-02-13 02:27:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:09][root][INFO] - Training Epoch: 1/2, step 201/7134 completed (loss: 0.1917383074760437, acc: 0.9508426785469055)
[2025-02-13 02:27:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:10][root][INFO] - Training Epoch: 1/2, step 202/7134 completed (loss: 0.1545846164226532, acc: 0.9627586007118225)
[2025-02-13 02:27:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:10][root][INFO] - Training Epoch: 1/2, step 203/7134 completed (loss: 0.1663352996110916, acc: 0.9573002457618713)
[2025-02-13 02:27:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:11][root][INFO] - Training Epoch: 1/2, step 204/7134 completed (loss: 0.1957705169916153, acc: 0.9483282566070557)
[2025-02-13 02:27:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:11][root][INFO] - Training Epoch: 1/2, step 205/7134 completed (loss: 0.2524321377277374, acc: 0.9314359426498413)
[2025-02-13 02:27:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:12][root][INFO] - Training Epoch: 1/2, step 206/7134 completed (loss: 0.17638517916202545, acc: 0.9582664370536804)
[2025-02-13 02:27:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:12][root][INFO] - Training Epoch: 1/2, step 207/7134 completed (loss: 0.23417426645755768, acc: 0.9436860084533691)
[2025-02-13 02:27:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:13][root][INFO] - Training Epoch: 1/2, step 208/7134 completed (loss: 0.2911745607852936, acc: 0.9333333373069763)
[2025-02-13 02:27:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:13][root][INFO] - Training Epoch: 1/2, step 209/7134 completed (loss: 0.3284664750099182, acc: 0.9222797751426697)
[2025-02-13 02:27:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:14][root][INFO] - Training Epoch: 1/2, step 210/7134 completed (loss: 0.27685773372650146, acc: 0.9363745450973511)
[2025-02-13 02:27:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:14][root][INFO] - Training Epoch: 1/2, step 211/7134 completed (loss: 0.34178394079208374, acc: 0.9264705777168274)
[2025-02-13 02:27:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:14][root][INFO] - Training Epoch: 1/2, step 212/7134 completed (loss: 0.36147981882095337, acc: 0.9276859760284424)
[2025-02-13 02:27:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:15][root][INFO] - Training Epoch: 1/2, step 213/7134 completed (loss: 0.2176026552915573, acc: 0.9417475461959839)
[2025-02-13 02:27:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:15][root][INFO] - Training Epoch: 1/2, step 214/7134 completed (loss: 0.20270611345767975, acc: 0.942307710647583)
[2025-02-13 02:27:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:16][root][INFO] - Training Epoch: 1/2, step 215/7134 completed (loss: 0.19463148713111877, acc: 0.9548611044883728)
[2025-02-13 02:27:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:16][root][INFO] - Training Epoch: 1/2, step 216/7134 completed (loss: 0.17706100642681122, acc: 0.9638888835906982)
[2025-02-13 02:27:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:17][root][INFO] - Training Epoch: 1/2, step 217/7134 completed (loss: 0.12245727330446243, acc: 0.9660605788230896)
[2025-02-13 02:27:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:17][root][INFO] - Training Epoch: 1/2, step 218/7134 completed (loss: 0.06902018934488297, acc: 0.9847328066825867)
[2025-02-13 02:27:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:18][root][INFO] - Training Epoch: 1/2, step 219/7134 completed (loss: 0.11386178433895111, acc: 0.9805068373680115)
[2025-02-13 02:27:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:18][root][INFO] - Training Epoch: 1/2, step 220/7134 completed (loss: 0.16264818608760834, acc: 0.9644351601600647)
[2025-02-13 02:27:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:19][root][INFO] - Training Epoch: 1/2, step 221/7134 completed (loss: 0.21089570224285126, acc: 0.9522387981414795)
[2025-02-13 02:27:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:19][root][INFO] - Training Epoch: 1/2, step 222/7134 completed (loss: 0.12452569603919983, acc: 0.9659686088562012)
[2025-02-13 02:27:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:19][root][INFO] - Training Epoch: 1/2, step 223/7134 completed (loss: 0.20067140460014343, acc: 0.9537869095802307)
[2025-02-13 02:27:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:20][root][INFO] - Training Epoch: 1/2, step 224/7134 completed (loss: 0.22866958379745483, acc: 0.9338521361351013)
[2025-02-13 02:27:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:20][root][INFO] - Training Epoch: 1/2, step 225/7134 completed (loss: 0.20191362500190735, acc: 0.9473684430122375)
[2025-02-13 02:27:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:21][root][INFO] - Training Epoch: 1/2, step 226/7134 completed (loss: 0.15768463909626007, acc: 0.9633252024650574)
[2025-02-13 02:27:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:21][root][INFO] - Training Epoch: 1/2, step 227/7134 completed (loss: 0.3579280376434326, acc: 0.9230769276618958)
[2025-02-13 02:27:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:22][root][INFO] - Training Epoch: 1/2, step 228/7134 completed (loss: 0.18579857051372528, acc: 0.9450072646141052)
[2025-02-13 02:27:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:22][root][INFO] - Training Epoch: 1/2, step 229/7134 completed (loss: 0.23164375126361847, acc: 0.9430379867553711)
[2025-02-13 02:27:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:23][root][INFO] - Training Epoch: 1/2, step 230/7134 completed (loss: 0.13034722208976746, acc: 0.9594594836235046)
[2025-02-13 02:27:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:23][root][INFO] - Training Epoch: 1/2, step 231/7134 completed (loss: 0.12239839136600494, acc: 0.9623352289199829)
[2025-02-13 02:27:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:24][root][INFO] - Training Epoch: 1/2, step 232/7134 completed (loss: 0.17263279855251312, acc: 0.95222407579422)
[2025-02-13 02:27:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:24][root][INFO] - Training Epoch: 1/2, step 233/7134 completed (loss: 0.15742544829845428, acc: 0.9603803753852844)
[2025-02-13 02:27:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:25][root][INFO] - Training Epoch: 1/2, step 234/7134 completed (loss: 0.20614217221736908, acc: 0.9577702879905701)
[2025-02-13 02:27:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:25][root][INFO] - Training Epoch: 1/2, step 235/7134 completed (loss: 0.18112333118915558, acc: 0.9658119678497314)
[2025-02-13 02:27:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:26][root][INFO] - Training Epoch: 1/2, step 236/7134 completed (loss: 0.0971510037779808, acc: 0.9703124761581421)
[2025-02-13 02:27:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:26][root][INFO] - Training Epoch: 1/2, step 237/7134 completed (loss: 0.21068847179412842, acc: 0.9493293762207031)
[2025-02-13 02:27:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:26][root][INFO] - Training Epoch: 1/2, step 238/7134 completed (loss: 0.20576617121696472, acc: 0.9514925479888916)
[2025-02-13 02:27:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:27][root][INFO] - Training Epoch: 1/2, step 239/7134 completed (loss: 0.09954128414392471, acc: 0.9642857313156128)
[2025-02-13 02:27:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:27][root][INFO] - Training Epoch: 1/2, step 240/7134 completed (loss: 0.18825650215148926, acc: 0.9618644118309021)
[2025-02-13 02:27:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:28][root][INFO] - Training Epoch: 1/2, step 241/7134 completed (loss: 0.2040329873561859, acc: 0.9516907930374146)
[2025-02-13 02:27:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:28][root][INFO] - Training Epoch: 1/2, step 242/7134 completed (loss: 0.2642013430595398, acc: 0.9341421127319336)
[2025-02-13 02:27:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:29][root][INFO] - Training Epoch: 1/2, step 243/7134 completed (loss: 0.3249014616012573, acc: 0.9200680255889893)
[2025-02-13 02:27:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:29][root][INFO] - Training Epoch: 1/2, step 244/7134 completed (loss: 0.1222464069724083, acc: 0.965309202671051)
[2025-02-13 02:27:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:30][root][INFO] - Training Epoch: 1/2, step 245/7134 completed (loss: 0.19061093032360077, acc: 0.9509631991386414)
[2025-02-13 02:27:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:30][root][INFO] - Training Epoch: 1/2, step 246/7134 completed (loss: 0.12497252970933914, acc: 0.965299665927887)
[2025-02-13 02:27:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:30][root][INFO] - Training Epoch: 1/2, step 247/7134 completed (loss: 0.19078963994979858, acc: 0.9455535411834717)
[2025-02-13 02:27:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:31][root][INFO] - Training Epoch: 1/2, step 248/7134 completed (loss: 0.1933644860982895, acc: 0.9379844665527344)
[2025-02-13 02:27:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:31][root][INFO] - Training Epoch: 1/2, step 249/7134 completed (loss: 0.16072052717208862, acc: 0.9544236063957214)
[2025-02-13 02:27:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:32][root][INFO] - Training Epoch: 1/2, step 250/7134 completed (loss: 0.2033415138721466, acc: 0.9477847814559937)
[2025-02-13 02:27:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:32][root][INFO] - Training Epoch: 1/2, step 251/7134 completed (loss: 0.17244015634059906, acc: 0.9538461565971375)
[2025-02-13 02:27:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:33][root][INFO] - Training Epoch: 1/2, step 252/7134 completed (loss: 0.14467045664787292, acc: 0.9590163826942444)
[2025-02-13 02:27:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:33][root][INFO] - Training Epoch: 1/2, step 253/7134 completed (loss: 0.12620534002780914, acc: 0.9607843160629272)
[2025-02-13 02:27:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:34][root][INFO] - Training Epoch: 1/2, step 254/7134 completed (loss: 0.15697433054447174, acc: 0.9660714268684387)
[2025-02-13 02:27:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:34][root][INFO] - Training Epoch: 1/2, step 255/7134 completed (loss: 0.14686092734336853, acc: 0.9649122953414917)
[2025-02-13 02:27:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:35][root][INFO] - Training Epoch: 1/2, step 256/7134 completed (loss: 0.20178548991680145, acc: 0.9448529481887817)
[2025-02-13 02:27:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:35][root][INFO] - Training Epoch: 1/2, step 257/7134 completed (loss: 0.157247394323349, acc: 0.9509202241897583)
[2025-02-13 02:27:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:36][root][INFO] - Training Epoch: 1/2, step 258/7134 completed (loss: 0.1560918390750885, acc: 0.9537190198898315)
[2025-02-13 02:27:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:36][root][INFO] - Training Epoch: 1/2, step 259/7134 completed (loss: 0.1374599039554596, acc: 0.9611021280288696)
[2025-02-13 02:27:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:37][root][INFO] - Training Epoch: 1/2, step 260/7134 completed (loss: 0.17714953422546387, acc: 0.9543147087097168)
[2025-02-13 02:27:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:37][root][INFO] - Training Epoch: 1/2, step 261/7134 completed (loss: 0.10390136390924454, acc: 0.9672801494598389)
[2025-02-13 02:27:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:37][root][INFO] - Training Epoch: 1/2, step 262/7134 completed (loss: 0.06497082114219666, acc: 0.981697142124176)
[2025-02-13 02:27:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:38][root][INFO] - Training Epoch: 1/2, step 263/7134 completed (loss: 0.09897233545780182, acc: 0.9706258177757263)
[2025-02-13 02:27:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:38][root][INFO] - Training Epoch: 1/2, step 264/7134 completed (loss: 0.12738221883773804, acc: 0.9602803587913513)
[2025-02-13 02:27:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:39][root][INFO] - Training Epoch: 1/2, step 265/7134 completed (loss: 0.16652852296829224, acc: 0.9561855792999268)
[2025-02-13 02:27:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:39][root][INFO] - Training Epoch: 1/2, step 266/7134 completed (loss: 0.11614791303873062, acc: 0.9713302850723267)
[2025-02-13 02:27:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:40][root][INFO] - Training Epoch: 1/2, step 267/7134 completed (loss: 0.16913685202598572, acc: 0.9471264481544495)
[2025-02-13 02:27:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:40][root][INFO] - Training Epoch: 1/2, step 268/7134 completed (loss: 0.15804900228977203, acc: 0.9597924947738647)
[2025-02-13 02:27:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:41][root][INFO] - Training Epoch: 1/2, step 269/7134 completed (loss: 0.20174063742160797, acc: 0.9499304294586182)
[2025-02-13 02:27:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:41][root][INFO] - Training Epoch: 1/2, step 270/7134 completed (loss: 0.12202753126621246, acc: 0.9731343388557434)
[2025-02-13 02:27:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:42][root][INFO] - Training Epoch: 1/2, step 271/7134 completed (loss: 0.13633129000663757, acc: 0.9646910429000854)
[2025-02-13 02:27:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:42][root][INFO] - Training Epoch: 1/2, step 272/7134 completed (loss: 0.10900051146745682, acc: 0.9694656729698181)
[2025-02-13 02:27:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:43][root][INFO] - Training Epoch: 1/2, step 273/7134 completed (loss: 0.08541040122509003, acc: 0.9772455096244812)
[2025-02-13 02:27:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:43][root][INFO] - Training Epoch: 1/2, step 274/7134 completed (loss: 0.09078142791986465, acc: 0.9807427525520325)
[2025-02-13 02:27:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:44][root][INFO] - Training Epoch: 1/2, step 275/7134 completed (loss: 0.14621320366859436, acc: 0.9582366347312927)
[2025-02-13 02:27:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:44][root][INFO] - Training Epoch: 1/2, step 276/7134 completed (loss: 0.10928291827440262, acc: 0.971985399723053)
[2025-02-13 02:27:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:45][root][INFO] - Training Epoch: 1/2, step 277/7134 completed (loss: 0.14703573286533356, acc: 0.9678249955177307)
[2025-02-13 02:27:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:45][root][INFO] - Training Epoch: 1/2, step 278/7134 completed (loss: 0.10423249751329422, acc: 0.9719029664993286)
[2025-02-13 02:27:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:46][root][INFO] - Training Epoch: 1/2, step 279/7134 completed (loss: 0.11220067739486694, acc: 0.9724409580230713)
[2025-02-13 02:27:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:46][root][INFO] - Training Epoch: 1/2, step 280/7134 completed (loss: 0.14730118215084076, acc: 0.9605734944343567)
[2025-02-13 02:27:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:47][root][INFO] - Training Epoch: 1/2, step 281/7134 completed (loss: 0.1762608289718628, acc: 0.9539999961853027)
[2025-02-13 02:27:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:47][root][INFO] - Training Epoch: 1/2, step 282/7134 completed (loss: 0.23763379454612732, acc: 0.9419847130775452)
[2025-02-13 02:27:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:48][root][INFO] - Training Epoch: 1/2, step 283/7134 completed (loss: 0.17601768672466278, acc: 0.9575551748275757)
[2025-02-13 02:27:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:48][root][INFO] - Training Epoch: 1/2, step 284/7134 completed (loss: 0.18018822371959686, acc: 0.9546979665756226)
[2025-02-13 02:27:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:49][root][INFO] - Training Epoch: 1/2, step 285/7134 completed (loss: 0.1394895762205124, acc: 0.9647266268730164)
[2025-02-13 02:27:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:49][root][INFO] - Training Epoch: 1/2, step 286/7134 completed (loss: 0.1696522980928421, acc: 0.9592920541763306)
[2025-02-13 02:27:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:49][root][INFO] - Training Epoch: 1/2, step 287/7134 completed (loss: 0.15602527558803558, acc: 0.966292142868042)
[2025-02-13 02:27:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:50][root][INFO] - Training Epoch: 1/2, step 288/7134 completed (loss: 0.14581458270549774, acc: 0.9613259434700012)
[2025-02-13 02:27:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:50][root][INFO] - Training Epoch: 1/2, step 289/7134 completed (loss: 0.11451422423124313, acc: 0.9638336300849915)
[2025-02-13 02:27:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:51][root][INFO] - Training Epoch: 1/2, step 290/7134 completed (loss: 0.12840138375759125, acc: 0.9680851101875305)
[2025-02-13 02:27:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:51][root][INFO] - Training Epoch: 1/2, step 291/7134 completed (loss: 0.14950233697891235, acc: 0.9605262875556946)
[2025-02-13 02:27:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:52][root][INFO] - Training Epoch: 1/2, step 292/7134 completed (loss: 0.16831493377685547, acc: 0.9570552110671997)
[2025-02-13 02:27:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:52][root][INFO] - Training Epoch: 1/2, step 293/7134 completed (loss: 0.12303948402404785, acc: 0.9618473649024963)
[2025-02-13 02:27:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:53][root][INFO] - Training Epoch: 1/2, step 294/7134 completed (loss: 0.11039776355028152, acc: 0.9688109159469604)
[2025-02-13 02:27:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:53][root][INFO] - Training Epoch: 1/2, step 295/7134 completed (loss: 0.18836095929145813, acc: 0.9573378562927246)
[2025-02-13 02:27:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:54][root][INFO] - Training Epoch: 1/2, step 296/7134 completed (loss: 0.0815909132361412, acc: 0.9798449873924255)
[2025-02-13 02:27:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:54][root][INFO] - Training Epoch: 1/2, step 297/7134 completed (loss: 0.09807360172271729, acc: 0.9769093990325928)
[2025-02-13 02:27:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:55][root][INFO] - Training Epoch: 1/2, step 298/7134 completed (loss: 0.10656644403934479, acc: 0.979200005531311)
[2025-02-13 02:27:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:55][root][INFO] - Training Epoch: 1/2, step 299/7134 completed (loss: 0.09891167283058167, acc: 0.9744361042976379)
[2025-02-13 02:27:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:56][root][INFO] - Training Epoch: 1/2, step 300/7134 completed (loss: 0.09993691742420197, acc: 0.9724919199943542)
[2025-02-13 02:27:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:56][root][INFO] - Training Epoch: 1/2, step 301/7134 completed (loss: 0.08555250614881516, acc: 0.9646393060684204)
[2025-02-13 02:27:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:57][root][INFO] - Training Epoch: 1/2, step 302/7134 completed (loss: 0.12198394536972046, acc: 0.9727095365524292)
[2025-02-13 02:27:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:57][root][INFO] - Training Epoch: 1/2, step 303/7134 completed (loss: 0.08793570101261139, acc: 0.9786407947540283)
[2025-02-13 02:27:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:57][root][INFO] - Training Epoch: 1/2, step 304/7134 completed (loss: 0.058005765080451965, acc: 0.979629635810852)
[2025-02-13 02:27:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:58][root][INFO] - Training Epoch: 1/2, step 305/7134 completed (loss: 0.05036986619234085, acc: 0.9904610514640808)
[2025-02-13 02:27:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:58][root][INFO] - Training Epoch: 1/2, step 306/7134 completed (loss: 0.03879520297050476, acc: 0.9909420013427734)
[2025-02-13 02:27:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:59][root][INFO] - Training Epoch: 1/2, step 307/7134 completed (loss: 0.11955320835113525, acc: 0.9590747356414795)
[2025-02-13 02:27:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:27:59][root][INFO] - Training Epoch: 1/2, step 308/7134 completed (loss: 0.058777760714292526, acc: 0.9819079041481018)
[2025-02-13 02:27:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:00][root][INFO] - Training Epoch: 1/2, step 309/7134 completed (loss: 0.10738693922758102, acc: 0.9724137783050537)
[2025-02-13 02:28:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:00][root][INFO] - Training Epoch: 1/2, step 310/7134 completed (loss: 0.06312649697065353, acc: 0.9853801131248474)
[2025-02-13 02:28:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:01][root][INFO] - Training Epoch: 1/2, step 311/7134 completed (loss: 0.0692933201789856, acc: 0.9782270789146423)
[2025-02-13 02:28:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:01][root][INFO] - Training Epoch: 1/2, step 312/7134 completed (loss: 0.0696478933095932, acc: 0.9722703695297241)
[2025-02-13 02:28:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:02][root][INFO] - Training Epoch: 1/2, step 313/7134 completed (loss: 0.07206247746944427, acc: 0.9878261089324951)
[2025-02-13 02:28:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:02][root][INFO] - Training Epoch: 1/2, step 314/7134 completed (loss: 0.10580865293741226, acc: 0.9703124761581421)
[2025-02-13 02:28:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:02][root][INFO] - Training Epoch: 1/2, step 315/7134 completed (loss: 0.1113971546292305, acc: 0.965573787689209)
[2025-02-13 02:28:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:03][root][INFO] - Training Epoch: 1/2, step 316/7134 completed (loss: 0.07888902723789215, acc: 0.9783393740653992)
[2025-02-13 02:28:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:03][root][INFO] - Training Epoch: 1/2, step 317/7134 completed (loss: 0.031113548204302788, acc: 0.9945873022079468)
[2025-02-13 02:28:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:04][root][INFO] - Training Epoch: 1/2, step 318/7134 completed (loss: 0.14576002955436707, acc: 0.9633699655532837)
[2025-02-13 02:28:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:04][root][INFO] - Training Epoch: 1/2, step 319/7134 completed (loss: 0.13029667735099792, acc: 0.9677419066429138)
[2025-02-13 02:28:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:05][root][INFO] - Training Epoch: 1/2, step 320/7134 completed (loss: 0.10328986495733261, acc: 0.968616247177124)
[2025-02-13 02:28:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:05][root][INFO] - Training Epoch: 1/2, step 321/7134 completed (loss: 0.18848170340061188, acc: 0.9565772414207458)
[2025-02-13 02:28:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:06][root][INFO] - Training Epoch: 1/2, step 322/7134 completed (loss: 0.21389220654964447, acc: 0.9414414167404175)
[2025-02-13 02:28:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:06][root][INFO] - Training Epoch: 1/2, step 323/7134 completed (loss: 0.2060008943080902, acc: 0.9473684430122375)
[2025-02-13 02:28:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:07][root][INFO] - Training Epoch: 1/2, step 324/7134 completed (loss: 0.17632432281970978, acc: 0.9516685605049133)
[2025-02-13 02:28:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:07][root][INFO] - Training Epoch: 1/2, step 325/7134 completed (loss: 0.08375784754753113, acc: 0.974293053150177)
[2025-02-13 02:28:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:08][root][INFO] - Training Epoch: 1/2, step 326/7134 completed (loss: 0.1003054529428482, acc: 0.9768574833869934)
[2025-02-13 02:28:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:08][root][INFO] - Training Epoch: 1/2, step 327/7134 completed (loss: 0.1549486666917801, acc: 0.9594594836235046)
[2025-02-13 02:28:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:09][root][INFO] - Training Epoch: 1/2, step 328/7134 completed (loss: 0.13469308614730835, acc: 0.965925931930542)
[2025-02-13 02:28:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:09][root][INFO] - Training Epoch: 1/2, step 329/7134 completed (loss: 0.09673930704593658, acc: 0.977673351764679)
[2025-02-13 02:28:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:10][root][INFO] - Training Epoch: 1/2, step 330/7134 completed (loss: 0.1691180318593979, acc: 0.9546827673912048)
[2025-02-13 02:28:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:11][root][INFO] - Training Epoch: 1/2, step 331/7134 completed (loss: 0.10665921121835709, acc: 0.9635119438171387)
[2025-02-13 02:28:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:11][root][INFO] - Training Epoch: 1/2, step 332/7134 completed (loss: 0.08654344826936722, acc: 0.9787485003471375)
[2025-02-13 02:28:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:11][root][INFO] - Training Epoch: 1/2, step 333/7134 completed (loss: 0.16199377179145813, acc: 0.9537712931632996)
[2025-02-13 02:28:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:12][root][INFO] - Training Epoch: 1/2, step 334/7134 completed (loss: 0.07843098044395447, acc: 0.9784537553787231)
[2025-02-13 02:28:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:12][root][INFO] - Training Epoch: 1/2, step 335/7134 completed (loss: 0.07963781803846359, acc: 0.9752547144889832)
[2025-02-13 02:28:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:13][root][INFO] - Training Epoch: 1/2, step 336/7134 completed (loss: 0.12090805172920227, acc: 0.9688622951507568)
[2025-02-13 02:28:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:13][root][INFO] - Training Epoch: 1/2, step 337/7134 completed (loss: 0.0995301827788353, acc: 0.9723926186561584)
[2025-02-13 02:28:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:14][root][INFO] - Training Epoch: 1/2, step 338/7134 completed (loss: 0.12051871418952942, acc: 0.9748110771179199)
[2025-02-13 02:28:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:14][root][INFO] - Training Epoch: 1/2, step 339/7134 completed (loss: 0.10057159513235092, acc: 0.9712499976158142)
[2025-02-13 02:28:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:15][root][INFO] - Training Epoch: 1/2, step 340/7134 completed (loss: 0.09660053998231888, acc: 0.9720101952552795)
[2025-02-13 02:28:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:15][root][INFO] - Training Epoch: 1/2, step 341/7134 completed (loss: 0.06233684718608856, acc: 0.9785624146461487)
[2025-02-13 02:28:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:16][root][INFO] - Training Epoch: 1/2, step 342/7134 completed (loss: 0.07117153704166412, acc: 0.9818181991577148)
[2025-02-13 02:28:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:16][root][INFO] - Training Epoch: 1/2, step 343/7134 completed (loss: 0.09901032596826553, acc: 0.9735350012779236)
[2025-02-13 02:28:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:17][root][INFO] - Training Epoch: 1/2, step 344/7134 completed (loss: 0.06391153484582901, acc: 0.9872262477874756)
[2025-02-13 02:28:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:17][root][INFO] - Training Epoch: 1/2, step 345/7134 completed (loss: 0.06575839221477509, acc: 0.9821428656578064)
[2025-02-13 02:28:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:18][root][INFO] - Training Epoch: 1/2, step 346/7134 completed (loss: 0.06406721472740173, acc: 0.9841017723083496)
[2025-02-13 02:28:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:18][root][INFO] - Training Epoch: 1/2, step 347/7134 completed (loss: 0.08740608394145966, acc: 0.9761570692062378)
[2025-02-13 02:28:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:19][root][INFO] - Training Epoch: 1/2, step 348/7134 completed (loss: 0.07621847838163376, acc: 0.9782903790473938)
[2025-02-13 02:28:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:19][root][INFO] - Training Epoch: 1/2, step 349/7134 completed (loss: 0.07559950649738312, acc: 0.9785234928131104)
[2025-02-13 02:28:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:20][root][INFO] - Training Epoch: 1/2, step 350/7134 completed (loss: 0.08709009736776352, acc: 0.9819494485855103)
[2025-02-13 02:28:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:20][root][INFO] - Training Epoch: 1/2, step 351/7134 completed (loss: 0.11133383214473724, acc: 0.974554717540741)
[2025-02-13 02:28:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:21][root][INFO] - Training Epoch: 1/2, step 352/7134 completed (loss: 0.08599855750799179, acc: 0.97826087474823)
[2025-02-13 02:28:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:21][root][INFO] - Training Epoch: 1/2, step 353/7134 completed (loss: 0.08409269154071808, acc: 0.9734042286872864)
[2025-02-13 02:28:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:22][root][INFO] - Training Epoch: 1/2, step 354/7134 completed (loss: 0.08084963262081146, acc: 0.9755529761314392)
[2025-02-13 02:28:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:22][root][INFO] - Training Epoch: 1/2, step 355/7134 completed (loss: 0.09318958967924118, acc: 0.9776536226272583)
[2025-02-13 02:28:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:23][root][INFO] - Training Epoch: 1/2, step 356/7134 completed (loss: 0.08783067762851715, acc: 0.9788359999656677)
[2025-02-13 02:28:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:23][root][INFO] - Training Epoch: 1/2, step 357/7134 completed (loss: 0.06583180278539658, acc: 0.983460545539856)
[2025-02-13 02:28:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:24][root][INFO] - Training Epoch: 1/2, step 358/7134 completed (loss: 0.10889432579278946, acc: 0.9713506102561951)
[2025-02-13 02:28:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:24][root][INFO] - Training Epoch: 1/2, step 359/7134 completed (loss: 0.06733064353466034, acc: 0.9824324250221252)
[2025-02-13 02:28:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:25][root][INFO] - Training Epoch: 1/2, step 360/7134 completed (loss: 0.06700225919485092, acc: 0.9864457845687866)
[2025-02-13 02:28:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:25][root][INFO] - Training Epoch: 1/2, step 361/7134 completed (loss: 0.12536758184432983, acc: 0.9706314206123352)
[2025-02-13 02:28:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:26][root][INFO] - Training Epoch: 1/2, step 362/7134 completed (loss: 0.08922100812196732, acc: 0.9805389046669006)
[2025-02-13 02:28:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:26][root][INFO] - Training Epoch: 1/2, step 363/7134 completed (loss: 0.10102808475494385, acc: 0.9752604365348816)
[2025-02-13 02:28:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:26][root][INFO] - Training Epoch: 1/2, step 364/7134 completed (loss: 0.07807459682226181, acc: 0.9789719581604004)
[2025-02-13 02:28:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:27][root][INFO] - Training Epoch: 1/2, step 365/7134 completed (loss: 0.09029139578342438, acc: 0.9779614210128784)
[2025-02-13 02:28:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:27][root][INFO] - Training Epoch: 1/2, step 366/7134 completed (loss: 0.22457799315452576, acc: 0.9576802253723145)
[2025-02-13 02:28:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:28][root][INFO] - Training Epoch: 1/2, step 367/7134 completed (loss: 0.08893173933029175, acc: 0.9737499952316284)
[2025-02-13 02:28:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:28][root][INFO] - Training Epoch: 1/2, step 368/7134 completed (loss: 0.07862731069326401, acc: 0.9762258529663086)
[2025-02-13 02:28:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:29][root][INFO] - Training Epoch: 1/2, step 369/7134 completed (loss: 0.15778380632400513, acc: 0.9663093686103821)
[2025-02-13 02:28:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:29][root][INFO] - Training Epoch: 1/2, step 370/7134 completed (loss: 0.11691398918628693, acc: 0.9737991094589233)
[2025-02-13 02:28:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:30][root][INFO] - Training Epoch: 1/2, step 371/7134 completed (loss: 0.11339767277240753, acc: 0.9759398698806763)
[2025-02-13 02:28:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:30][root][INFO] - Training Epoch: 1/2, step 372/7134 completed (loss: 0.11078865826129913, acc: 0.9732540845870972)
[2025-02-13 02:28:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:31][root][INFO] - Training Epoch: 1/2, step 373/7134 completed (loss: 0.062153905630111694, acc: 0.9850522875785828)
[2025-02-13 02:28:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:31][root][INFO] - Training Epoch: 1/2, step 374/7134 completed (loss: 0.10126805305480957, acc: 0.9823151230812073)
[2025-02-13 02:28:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:32][root][INFO] - Training Epoch: 1/2, step 375/7134 completed (loss: 0.132836252450943, acc: 0.9745649099349976)
[2025-02-13 02:28:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:32][root][INFO] - Training Epoch: 1/2, step 376/7134 completed (loss: 0.17025139927864075, acc: 0.9558620452880859)
[2025-02-13 02:28:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:33][root][INFO] - Training Epoch: 1/2, step 377/7134 completed (loss: 0.11758735775947571, acc: 0.9683631658554077)
[2025-02-13 02:28:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:33][root][INFO] - Training Epoch: 1/2, step 378/7134 completed (loss: 0.09959504008293152, acc: 0.971389651298523)
[2025-02-13 02:28:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:33][root][INFO] - Training Epoch: 1/2, step 379/7134 completed (loss: 0.11520034819841385, acc: 0.9685929417610168)
[2025-02-13 02:28:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:34][root][INFO] - Training Epoch: 1/2, step 380/7134 completed (loss: 0.16446839272975922, acc: 0.947437584400177)
[2025-02-13 02:28:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:34][root][INFO] - Training Epoch: 1/2, step 381/7134 completed (loss: 0.11988916993141174, acc: 0.9702842235565186)
[2025-02-13 02:28:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:35][root][INFO] - Training Epoch: 1/2, step 382/7134 completed (loss: 0.14355333149433136, acc: 0.9640564918518066)
[2025-02-13 02:28:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:35][root][INFO] - Training Epoch: 1/2, step 383/7134 completed (loss: 0.0988752618432045, acc: 0.9773156642913818)
[2025-02-13 02:28:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:36][root][INFO] - Training Epoch: 1/2, step 384/7134 completed (loss: 0.10726077854633331, acc: 0.969292402267456)
[2025-02-13 02:28:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:36][root][INFO] - Training Epoch: 1/2, step 385/7134 completed (loss: 0.07801859825849533, acc: 0.9843013882637024)
[2025-02-13 02:28:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:37][root][INFO] - Training Epoch: 1/2, step 386/7134 completed (loss: 0.07533533871173859, acc: 0.9836400747299194)
[2025-02-13 02:28:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:37][root][INFO] - Training Epoch: 1/2, step 387/7134 completed (loss: 0.13758966326713562, acc: 0.9650145769119263)
[2025-02-13 02:28:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:38][root][INFO] - Training Epoch: 1/2, step 388/7134 completed (loss: 0.08243399858474731, acc: 0.9765258431434631)
[2025-02-13 02:28:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:38][root][INFO] - Training Epoch: 1/2, step 389/7134 completed (loss: 0.15497784316539764, acc: 0.9628099203109741)
[2025-02-13 02:28:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:39][root][INFO] - Training Epoch: 1/2, step 390/7134 completed (loss: 0.08988334983587265, acc: 0.9789643883705139)
[2025-02-13 02:28:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:39][root][INFO] - Training Epoch: 1/2, step 391/7134 completed (loss: 0.12997908890247345, acc: 0.9659863710403442)
[2025-02-13 02:28:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:39][root][INFO] - Training Epoch: 1/2, step 392/7134 completed (loss: 0.13636192679405212, acc: 0.9555555582046509)
[2025-02-13 02:28:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:40][root][INFO] - Training Epoch: 1/2, step 393/7134 completed (loss: 0.17602656781673431, acc: 0.9527778029441833)
[2025-02-13 02:28:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:40][root][INFO] - Training Epoch: 1/2, step 394/7134 completed (loss: 0.1361488550901413, acc: 0.9630350470542908)
[2025-02-13 02:28:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:41][root][INFO] - Training Epoch: 1/2, step 395/7134 completed (loss: 0.1399732232093811, acc: 0.9615912437438965)
[2025-02-13 02:28:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:41][root][INFO] - Training Epoch: 1/2, step 396/7134 completed (loss: 0.1125933825969696, acc: 0.9741935729980469)
[2025-02-13 02:28:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:42][root][INFO] - Training Epoch: 1/2, step 397/7134 completed (loss: 0.14540113508701324, acc: 0.9589946866035461)
[2025-02-13 02:28:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:42][root][INFO] - Training Epoch: 1/2, step 398/7134 completed (loss: 0.09962286055088043, acc: 0.9755154848098755)
[2025-02-13 02:28:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:43][root][INFO] - Training Epoch: 1/2, step 399/7134 completed (loss: 0.12362150847911835, acc: 0.9667195081710815)
[2025-02-13 02:28:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:43][root][INFO] - Training Epoch: 1/2, step 400/7134 completed (loss: 0.20705348253250122, acc: 0.9492537379264832)
[2025-02-13 02:28:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:44][root][INFO] - Training Epoch: 1/2, step 401/7134 completed (loss: 0.10929366946220398, acc: 0.9754902124404907)
[2025-02-13 02:28:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:44][root][INFO] - Training Epoch: 1/2, step 402/7134 completed (loss: 0.11531492322683334, acc: 0.9683143496513367)
[2025-02-13 02:28:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:45][root][INFO] - Training Epoch: 1/2, step 403/7134 completed (loss: 0.11247709393501282, acc: 0.9665226936340332)
[2025-02-13 02:28:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:45][root][INFO] - Training Epoch: 1/2, step 404/7134 completed (loss: 0.09578998386859894, acc: 0.978205144405365)
[2025-02-13 02:28:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:46][root][INFO] - Training Epoch: 1/2, step 405/7134 completed (loss: 0.1396128088235855, acc: 0.9657320976257324)
[2025-02-13 02:28:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:46][root][INFO] - Training Epoch: 1/2, step 406/7134 completed (loss: 0.14326950907707214, acc: 0.9639249444007874)
[2025-02-13 02:28:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:47][root][INFO] - Training Epoch: 1/2, step 407/7134 completed (loss: 0.22236552834510803, acc: 0.9525423645973206)
[2025-02-13 02:28:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:47][root][INFO] - Training Epoch: 1/2, step 408/7134 completed (loss: 0.12724223732948303, acc: 0.9711934328079224)
[2025-02-13 02:28:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:48][root][INFO] - Training Epoch: 1/2, step 409/7134 completed (loss: 0.12122964859008789, acc: 0.9735848903656006)
[2025-02-13 02:28:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:48][root][INFO] - Training Epoch: 1/2, step 410/7134 completed (loss: 0.13050460815429688, acc: 0.9673590660095215)
[2025-02-13 02:28:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:49][root][INFO] - Training Epoch: 1/2, step 411/7134 completed (loss: 0.1571113020181656, acc: 0.964326798915863)
[2025-02-13 02:28:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:49][root][INFO] - Training Epoch: 1/2, step 412/7134 completed (loss: 0.12189871072769165, acc: 0.970251739025116)
[2025-02-13 02:28:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:49][root][INFO] - Training Epoch: 1/2, step 413/7134 completed (loss: 0.13536933064460754, acc: 0.9666221737861633)
[2025-02-13 02:28:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:50][root][INFO] - Training Epoch: 1/2, step 414/7134 completed (loss: 0.1012573093175888, acc: 0.9703264236450195)
[2025-02-13 02:28:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:50][root][INFO] - Training Epoch: 1/2, step 415/7134 completed (loss: 0.0856093093752861, acc: 0.9745370149612427)
[2025-02-13 02:28:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:51][root][INFO] - Training Epoch: 1/2, step 416/7134 completed (loss: 0.12000959366559982, acc: 0.9657614827156067)
[2025-02-13 02:28:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:51][root][INFO] - Training Epoch: 1/2, step 417/7134 completed (loss: 0.13592340052127838, acc: 0.9619565010070801)
[2025-02-13 02:28:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:52][root][INFO] - Training Epoch: 1/2, step 418/7134 completed (loss: 0.10317474603652954, acc: 0.9667049646377563)
[2025-02-13 02:28:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:52][root][INFO] - Training Epoch: 1/2, step 419/7134 completed (loss: 0.12780430912971497, acc: 0.9677419066429138)
[2025-02-13 02:28:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:53][root][INFO] - Training Epoch: 1/2, step 420/7134 completed (loss: 0.07269413024187088, acc: 0.9847715497016907)
[2025-02-13 02:28:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:53][root][INFO] - Training Epoch: 1/2, step 421/7134 completed (loss: 0.06966770440340042, acc: 0.9856687784194946)
[2025-02-13 02:28:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:54][root][INFO] - Training Epoch: 1/2, step 422/7134 completed (loss: 0.07611498236656189, acc: 0.9838235378265381)
[2025-02-13 02:28:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:54][root][INFO] - Training Epoch: 1/2, step 423/7134 completed (loss: 0.06127040833234787, acc: 0.9900000095367432)
[2025-02-13 02:28:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:55][root][INFO] - Training Epoch: 1/2, step 424/7134 completed (loss: 0.09372568875551224, acc: 0.9731012582778931)
[2025-02-13 02:28:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:55][root][INFO] - Training Epoch: 1/2, step 425/7134 completed (loss: 0.04715743660926819, acc: 0.9883720874786377)
[2025-02-13 02:28:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:56][root][INFO] - Training Epoch: 1/2, step 426/7134 completed (loss: 0.055453356355428696, acc: 0.9791086316108704)
[2025-02-13 02:28:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:56][root][INFO] - Training Epoch: 1/2, step 427/7134 completed (loss: 0.06590242683887482, acc: 0.9848254919052124)
[2025-02-13 02:28:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:57][root][INFO] - Training Epoch: 1/2, step 428/7134 completed (loss: 0.07484205812215805, acc: 0.9804216623306274)
[2025-02-13 02:28:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:57][root][INFO] - Training Epoch: 1/2, step 429/7134 completed (loss: 0.10443433374166489, acc: 0.9697933197021484)
[2025-02-13 02:28:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:58][root][INFO] - Training Epoch: 1/2, step 430/7134 completed (loss: 0.07133857160806656, acc: 0.9784792065620422)
[2025-02-13 02:28:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:58][root][INFO] - Training Epoch: 1/2, step 431/7134 completed (loss: 0.06445596367120743, acc: 0.9812679886817932)
[2025-02-13 02:28:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:58][root][INFO] - Training Epoch: 1/2, step 432/7134 completed (loss: 0.054706670343875885, acc: 0.9872029423713684)
[2025-02-13 02:28:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:59][root][INFO] - Training Epoch: 1/2, step 433/7134 completed (loss: 0.0730157196521759, acc: 0.97826087474823)
[2025-02-13 02:28:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:28:59][root][INFO] - Training Epoch: 1/2, step 434/7134 completed (loss: 0.06808119267225266, acc: 0.9824304580688477)
[2025-02-13 02:29:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:00][root][INFO] - Training Epoch: 1/2, step 435/7134 completed (loss: 0.09996197372674942, acc: 0.9773049354553223)
[2025-02-13 02:29:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:00][root][INFO] - Training Epoch: 1/2, step 436/7134 completed (loss: 0.10323934257030487, acc: 0.9665354490280151)
[2025-02-13 02:29:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:01][root][INFO] - Training Epoch: 1/2, step 437/7134 completed (loss: 0.11514255404472351, acc: 0.9681978821754456)
[2025-02-13 02:29:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:01][root][INFO] - Training Epoch: 1/2, step 438/7134 completed (loss: 0.07991646230220795, acc: 0.9807692170143127)
[2025-02-13 02:29:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:02][root][INFO] - Training Epoch: 1/2, step 439/7134 completed (loss: 0.07666033506393433, acc: 0.977748692035675)
[2025-02-13 02:29:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:02][root][INFO] - Training Epoch: 1/2, step 440/7134 completed (loss: 0.10524722188711166, acc: 0.9722703695297241)
[2025-02-13 02:29:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:03][root][INFO] - Training Epoch: 1/2, step 441/7134 completed (loss: 0.033973317593336105, acc: 0.9910045266151428)
[2025-02-13 02:29:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:03][root][INFO] - Training Epoch: 1/2, step 442/7134 completed (loss: 0.026478983461856842, acc: 0.9925373196601868)
[2025-02-13 02:29:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:03][root][INFO] - Training Epoch: 1/2, step 443/7134 completed (loss: 0.0400797501206398, acc: 0.9937984347343445)
[2025-02-13 02:29:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:04][root][INFO] - Training Epoch: 1/2, step 444/7134 completed (loss: 0.03538664057850838, acc: 0.9883333444595337)
[2025-02-13 02:29:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:04][root][INFO] - Training Epoch: 1/2, step 445/7134 completed (loss: 0.10407678782939911, acc: 0.9708454608917236)
[2025-02-13 02:29:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:05][root][INFO] - Training Epoch: 1/2, step 446/7134 completed (loss: 0.036459095776081085, acc: 0.9874652028083801)
[2025-02-13 02:29:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:05][root][INFO] - Training Epoch: 1/2, step 447/7134 completed (loss: 0.05210831016302109, acc: 0.9847434163093567)
[2025-02-13 02:29:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:06][root][INFO] - Training Epoch: 1/2, step 448/7134 completed (loss: 0.16467535495758057, acc: 0.9603580832481384)
[2025-02-13 02:29:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:06][root][INFO] - Training Epoch: 1/2, step 449/7134 completed (loss: 0.17585982382297516, acc: 0.9528796076774597)
[2025-02-13 02:29:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:07][root][INFO] - Training Epoch: 1/2, step 450/7134 completed (loss: 0.15942367911338806, acc: 0.9607218503952026)
[2025-02-13 02:29:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:07][root][INFO] - Training Epoch: 1/2, step 451/7134 completed (loss: 0.22882287204265594, acc: 0.9464922547340393)
[2025-02-13 02:29:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:08][root][INFO] - Training Epoch: 1/2, step 452/7134 completed (loss: 0.16850394010543823, acc: 0.9589040875434875)
[2025-02-13 02:29:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:08][root][INFO] - Training Epoch: 1/2, step 453/7134 completed (loss: 0.1345834732055664, acc: 0.9706422090530396)
[2025-02-13 02:29:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:09][root][INFO] - Training Epoch: 1/2, step 454/7134 completed (loss: 0.12446977943181992, acc: 0.966630756855011)
[2025-02-13 02:29:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:09][root][INFO] - Training Epoch: 1/2, step 455/7134 completed (loss: 0.15341238677501678, acc: 0.9555555582046509)
[2025-02-13 02:29:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:09][root][INFO] - Training Epoch: 1/2, step 456/7134 completed (loss: 0.087542824447155, acc: 0.9759206771850586)
[2025-02-13 02:29:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:10][root][INFO] - Training Epoch: 1/2, step 457/7134 completed (loss: 0.13338564336299896, acc: 0.968120813369751)
[2025-02-13 02:29:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:10][root][INFO] - Training Epoch: 1/2, step 458/7134 completed (loss: 0.10479307919740677, acc: 0.9681908488273621)
[2025-02-13 02:29:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:11][root][INFO] - Training Epoch: 1/2, step 459/7134 completed (loss: 0.15832610428333282, acc: 0.9644444584846497)
[2025-02-13 02:29:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:11][root][INFO] - Training Epoch: 1/2, step 460/7134 completed (loss: 0.10587240010499954, acc: 0.9732283353805542)
[2025-02-13 02:29:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:12][root][INFO] - Training Epoch: 1/2, step 461/7134 completed (loss: 0.12359652668237686, acc: 0.9691444635391235)
[2025-02-13 02:29:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:12][root][INFO] - Training Epoch: 1/2, step 462/7134 completed (loss: 0.15435539186000824, acc: 0.9567757248878479)
[2025-02-13 02:29:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:13][root][INFO] - Training Epoch: 1/2, step 463/7134 completed (loss: 0.10962525755167007, acc: 0.9698870778083801)
[2025-02-13 02:29:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:13][root][INFO] - Training Epoch: 1/2, step 464/7134 completed (loss: 0.12511968612670898, acc: 0.9617563486099243)
[2025-02-13 02:29:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:14][root][INFO] - Training Epoch: 1/2, step 465/7134 completed (loss: 0.13183936476707458, acc: 0.9660574197769165)
[2025-02-13 02:29:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:14][root][INFO] - Training Epoch: 1/2, step 466/7134 completed (loss: 0.1617661565542221, acc: 0.9601029753684998)
[2025-02-13 02:29:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:15][root][INFO] - Training Epoch: 1/2, step 467/7134 completed (loss: 0.17688213288784027, acc: 0.9580973982810974)
[2025-02-13 02:29:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:15][root][INFO] - Training Epoch: 1/2, step 468/7134 completed (loss: 0.2429494559764862, acc: 0.9422283172607422)
[2025-02-13 02:29:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:16][root][INFO] - Training Epoch: 1/2, step 469/7134 completed (loss: 0.12127456068992615, acc: 0.9639769196510315)
[2025-02-13 02:29:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:16][root][INFO] - Training Epoch: 1/2, step 470/7134 completed (loss: 0.1273963302373886, acc: 0.9632652997970581)
[2025-02-13 02:29:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:17][root][INFO] - Training Epoch: 1/2, step 471/7134 completed (loss: 0.1342223882675171, acc: 0.9704142212867737)
[2025-02-13 02:29:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:17][root][INFO] - Training Epoch: 1/2, step 472/7134 completed (loss: 0.18546365201473236, acc: 0.9574779868125916)
[2025-02-13 02:29:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:18][root][INFO] - Training Epoch: 1/2, step 473/7134 completed (loss: 0.07797633111476898, acc: 0.9722222089767456)
[2025-02-13 02:29:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:18][root][INFO] - Training Epoch: 1/2, step 474/7134 completed (loss: 0.12315112352371216, acc: 0.9644268751144409)
[2025-02-13 02:29:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:18][root][INFO] - Training Epoch: 1/2, step 475/7134 completed (loss: 0.21187537908554077, acc: 0.9426470398902893)
[2025-02-13 02:29:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:19][root][INFO] - Training Epoch: 1/2, step 476/7134 completed (loss: 0.17237119376659393, acc: 0.9426934123039246)
[2025-02-13 02:29:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:19][root][INFO] - Training Epoch: 1/2, step 477/7134 completed (loss: 0.14051732420921326, acc: 0.955990195274353)
[2025-02-13 02:29:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:20][root][INFO] - Training Epoch: 1/2, step 478/7134 completed (loss: 0.21692384779453278, acc: 0.9471458792686462)
[2025-02-13 02:29:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:20][root][INFO] - Training Epoch: 1/2, step 479/7134 completed (loss: 0.08733806014060974, acc: 0.9784792065620422)
[2025-02-13 02:29:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:21][root][INFO] - Training Epoch: 1/2, step 480/7134 completed (loss: 0.05799202620983124, acc: 0.985401451587677)
[2025-02-13 02:29:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:21][root][INFO] - Training Epoch: 1/2, step 481/7134 completed (loss: 0.10374081134796143, acc: 0.9778645634651184)
[2025-02-13 02:29:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:22][root][INFO] - Training Epoch: 1/2, step 482/7134 completed (loss: 0.048121511936187744, acc: 0.9857142567634583)
[2025-02-13 02:29:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:22][root][INFO] - Training Epoch: 1/2, step 483/7134 completed (loss: 0.06002185121178627, acc: 0.9840294718742371)
[2025-02-13 02:29:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:23][root][INFO] - Training Epoch: 1/2, step 484/7134 completed (loss: 0.10808201879262924, acc: 0.970588207244873)
[2025-02-13 02:29:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:23][root][INFO] - Training Epoch: 1/2, step 485/7134 completed (loss: 0.1367548108100891, acc: 0.9697428345680237)
[2025-02-13 02:29:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:23][root][INFO] - Training Epoch: 1/2, step 486/7134 completed (loss: 0.13887910544872284, acc: 0.9605262875556946)
[2025-02-13 02:29:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:24][root][INFO] - Training Epoch: 1/2, step 487/7134 completed (loss: 0.07694900780916214, acc: 0.9823788404464722)
[2025-02-13 02:29:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:24][root][INFO] - Training Epoch: 1/2, step 488/7134 completed (loss: 0.15607167780399323, acc: 0.9619565010070801)
[2025-02-13 02:29:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:25][root][INFO] - Training Epoch: 1/2, step 489/7134 completed (loss: 0.0893840417265892, acc: 0.9764705896377563)
[2025-02-13 02:29:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:25][root][INFO] - Training Epoch: 1/2, step 490/7134 completed (loss: 0.09016077220439911, acc: 0.9768707752227783)
[2025-02-13 02:29:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:26][root][INFO] - Training Epoch: 1/2, step 491/7134 completed (loss: 0.08775345981121063, acc: 0.9769737124443054)
[2025-02-13 02:29:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:26][root][INFO] - Training Epoch: 1/2, step 492/7134 completed (loss: 0.03613639995455742, acc: 0.9924356937408447)
[2025-02-13 02:29:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:27][root][INFO] - Training Epoch: 1/2, step 493/7134 completed (loss: 0.0938972532749176, acc: 0.9684813618659973)
[2025-02-13 02:29:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:27][root][INFO] - Training Epoch: 1/2, step 494/7134 completed (loss: 0.06360773742198944, acc: 0.981249988079071)
[2025-02-13 02:29:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:27][root][INFO] - Training Epoch: 1/2, step 495/7134 completed (loss: 0.07416142523288727, acc: 0.9823529124259949)
[2025-02-13 02:29:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:28][root][INFO] - Training Epoch: 1/2, step 496/7134 completed (loss: 0.11589744687080383, acc: 0.9756097793579102)
[2025-02-13 02:29:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:28][root][INFO] - Training Epoch: 1/2, step 497/7134 completed (loss: 0.10851602256298065, acc: 0.9709302186965942)
[2025-02-13 02:29:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:29][root][INFO] - Training Epoch: 1/2, step 498/7134 completed (loss: 0.052936069667339325, acc: 0.9871794581413269)
[2025-02-13 02:29:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:29][root][INFO] - Training Epoch: 1/2, step 499/7134 completed (loss: 0.06531768292188644, acc: 0.9819193482398987)
[2025-02-13 02:29:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:30][root][INFO] - Training Epoch: 1/2, step 500/7134 completed (loss: 0.09713994711637497, acc: 0.9773333072662354)
[2025-02-13 02:29:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:30][root][INFO] - Training Epoch: 1/2, step 501/7134 completed (loss: 0.06157930940389633, acc: 0.9861496090888977)
[2025-02-13 02:29:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:31][root][INFO] - Training Epoch: 1/2, step 502/7134 completed (loss: 0.03993672505021095, acc: 0.9908257126808167)
[2025-02-13 02:29:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:31][root][INFO] - Training Epoch: 1/2, step 503/7134 completed (loss: 0.08990771323442459, acc: 0.9758388996124268)
[2025-02-13 02:29:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:31][root][INFO] - Training Epoch: 1/2, step 504/7134 completed (loss: 0.03062456287443638, acc: 0.994413435459137)
[2025-02-13 02:29:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:32][root][INFO] - Training Epoch: 1/2, step 505/7134 completed (loss: 0.03805026412010193, acc: 0.9973822236061096)
[2025-02-13 02:29:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:32][root][INFO] - Training Epoch: 1/2, step 506/7134 completed (loss: 0.04899932071566582, acc: 0.9838308691978455)
[2025-02-13 02:29:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:33][root][INFO] - Training Epoch: 1/2, step 507/7134 completed (loss: 0.08834827691316605, acc: 0.9802631735801697)
[2025-02-13 02:29:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:33][root][INFO] - Training Epoch: 1/2, step 508/7134 completed (loss: 0.0705108717083931, acc: 0.9818435907363892)
[2025-02-13 02:29:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:34][root][INFO] - Training Epoch: 1/2, step 509/7134 completed (loss: 0.09838173538446426, acc: 0.9721792936325073)
[2025-02-13 02:29:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:34][root][INFO] - Training Epoch: 1/2, step 510/7134 completed (loss: 0.19730247557163239, acc: 0.9584199786186218)
[2025-02-13 02:29:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:35][root][INFO] - Training Epoch: 1/2, step 511/7134 completed (loss: 0.122969850897789, acc: 0.97555011510849)
[2025-02-13 02:29:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:35][root][INFO] - Training Epoch: 1/2, step 512/7134 completed (loss: 0.14346905052661896, acc: 0.9679358601570129)
[2025-02-13 02:29:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:36][root][INFO] - Training Epoch: 1/2, step 513/7134 completed (loss: 0.12052424997091293, acc: 0.9690189361572266)
[2025-02-13 02:29:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:36][root][INFO] - Training Epoch: 1/2, step 514/7134 completed (loss: 0.16502615809440613, acc: 0.9547738432884216)
[2025-02-13 02:29:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:36][root][INFO] - Training Epoch: 1/2, step 515/7134 completed (loss: 0.07737639546394348, acc: 0.9816124439239502)
[2025-02-13 02:29:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:37][root][INFO] - Training Epoch: 1/2, step 516/7134 completed (loss: 0.0510849803686142, acc: 0.9839816689491272)
[2025-02-13 02:29:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:37][root][INFO] - Training Epoch: 1/2, step 517/7134 completed (loss: 0.15016554296016693, acc: 0.9569892287254333)
[2025-02-13 02:29:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:38][root][INFO] - Training Epoch: 1/2, step 518/7134 completed (loss: 0.06013026833534241, acc: 0.9850075244903564)
[2025-02-13 02:29:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:38][root][INFO] - Training Epoch: 1/2, step 519/7134 completed (loss: 0.08820099383592606, acc: 0.969072163105011)
[2025-02-13 02:29:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:39][root][INFO] - Training Epoch: 1/2, step 520/7134 completed (loss: 0.06159201264381409, acc: 0.9873417615890503)
[2025-02-13 02:29:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:39][root][INFO] - Training Epoch: 1/2, step 521/7134 completed (loss: 0.11125726997852325, acc: 0.9773030877113342)
[2025-02-13 02:29:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:39][root][INFO] - Training Epoch: 1/2, step 522/7134 completed (loss: 0.0680711567401886, acc: 0.9839141964912415)
[2025-02-13 02:29:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:40][root][INFO] - Training Epoch: 1/2, step 523/7134 completed (loss: 0.07070735096931458, acc: 0.9766082167625427)
[2025-02-13 02:29:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:40][root][INFO] - Training Epoch: 1/2, step 524/7134 completed (loss: 0.06183978542685509, acc: 0.9852150678634644)
[2025-02-13 02:29:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:41][root][INFO] - Training Epoch: 1/2, step 525/7134 completed (loss: 0.12648369371891022, acc: 0.9768160581588745)
[2025-02-13 02:29:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:41][root][INFO] - Training Epoch: 1/2, step 526/7134 completed (loss: 0.07152720540761948, acc: 0.9849315285682678)
[2025-02-13 02:29:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:42][root][INFO] - Training Epoch: 1/2, step 527/7134 completed (loss: 0.06762338429689407, acc: 0.9906716346740723)
[2025-02-13 02:29:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:42][root][INFO] - Training Epoch: 1/2, step 528/7134 completed (loss: 0.035807039588689804, acc: 0.98591548204422)
[2025-02-13 02:29:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:43][root][INFO] - Training Epoch: 1/2, step 529/7134 completed (loss: 0.1073392704129219, acc: 0.9682539701461792)
[2025-02-13 02:29:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:43][root][INFO] - Training Epoch: 1/2, step 530/7134 completed (loss: 0.08355031162500381, acc: 0.9759863018989563)
[2025-02-13 02:29:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:44][root][INFO] - Training Epoch: 1/2, step 531/7134 completed (loss: 0.12170811742544174, acc: 0.9726495742797852)
[2025-02-13 02:29:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:44][root][INFO] - Training Epoch: 1/2, step 532/7134 completed (loss: 0.045798368752002716, acc: 0.9937629699707031)
[2025-02-13 02:29:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:44][root][INFO] - Training Epoch: 1/2, step 533/7134 completed (loss: 0.13157670199871063, acc: 0.9656357169151306)
[2025-02-13 02:29:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:45][root][INFO] - Training Epoch: 1/2, step 534/7134 completed (loss: 0.03250287473201752, acc: 0.9931318759918213)
[2025-02-13 02:29:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:45][root][INFO] - Training Epoch: 1/2, step 535/7134 completed (loss: 0.07209492474794388, acc: 0.982425332069397)
[2025-02-13 02:29:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:46][root][INFO] - Training Epoch: 1/2, step 536/7134 completed (loss: 0.1306246519088745, acc: 0.9605839252471924)
[2025-02-13 02:29:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:46][root][INFO] - Training Epoch: 1/2, step 537/7134 completed (loss: 0.11809239536523819, acc: 0.9675993919372559)
[2025-02-13 02:29:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:47][root][INFO] - Training Epoch: 1/2, step 538/7134 completed (loss: 0.09621021896600723, acc: 0.9762611389160156)
[2025-02-13 02:29:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:47][root][INFO] - Training Epoch: 1/2, step 539/7134 completed (loss: 0.07866688072681427, acc: 0.9757785201072693)
[2025-02-13 02:29:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:48][root][INFO] - Training Epoch: 1/2, step 540/7134 completed (loss: 0.08790749311447144, acc: 0.9676767587661743)
[2025-02-13 02:29:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:48][root][INFO] - Training Epoch: 1/2, step 541/7134 completed (loss: 0.09213148057460785, acc: 0.9747340679168701)
[2025-02-13 02:29:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:49][root][INFO] - Training Epoch: 1/2, step 542/7134 completed (loss: 0.1499432772397995, acc: 0.9624413251876831)
[2025-02-13 02:29:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:49][root][INFO] - Training Epoch: 1/2, step 543/7134 completed (loss: 0.15831080079078674, acc: 0.9547920227050781)
[2025-02-13 02:29:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:49][root][INFO] - Training Epoch: 1/2, step 544/7134 completed (loss: 0.15832014381885529, acc: 0.9574132561683655)
[2025-02-13 02:29:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:50][root][INFO] - Training Epoch: 1/2, step 545/7134 completed (loss: 0.14225651323795319, acc: 0.9645732641220093)
[2025-02-13 02:29:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:50][root][INFO] - Training Epoch: 1/2, step 546/7134 completed (loss: 0.21291393041610718, acc: 0.9456264972686768)
[2025-02-13 02:29:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:51][root][INFO] - Training Epoch: 1/2, step 547/7134 completed (loss: 0.1255907416343689, acc: 0.9695210456848145)
[2025-02-13 02:29:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:51][root][INFO] - Training Epoch: 1/2, step 548/7134 completed (loss: 0.06570041179656982, acc: 0.9843546152114868)
[2025-02-13 02:29:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:52][root][INFO] - Training Epoch: 1/2, step 549/7134 completed (loss: 0.11856066435575485, acc: 0.9642857313156128)
[2025-02-13 02:29:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:52][root][INFO] - Training Epoch: 1/2, step 550/7134 completed (loss: 0.12228182703256607, acc: 0.9726962447166443)
[2025-02-13 02:29:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:52][root][INFO] - Training Epoch: 1/2, step 551/7134 completed (loss: 0.11483347415924072, acc: 0.9650092124938965)
[2025-02-13 02:29:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:53][root][INFO] - Training Epoch: 1/2, step 552/7134 completed (loss: 0.10810469835996628, acc: 0.9643874764442444)
[2025-02-13 02:29:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:53][root][INFO] - Training Epoch: 1/2, step 553/7134 completed (loss: 0.09873804450035095, acc: 0.969298243522644)
[2025-02-13 02:29:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:54][root][INFO] - Training Epoch: 1/2, step 554/7134 completed (loss: 0.08312384784221649, acc: 0.9743223786354065)
[2025-02-13 02:29:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:54][root][INFO] - Training Epoch: 1/2, step 555/7134 completed (loss: 0.13577108085155487, acc: 0.9652448892593384)
[2025-02-13 02:29:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:55][root][INFO] - Training Epoch: 1/2, step 556/7134 completed (loss: 0.07136274129152298, acc: 0.9787836074829102)
[2025-02-13 02:29:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:55][root][INFO] - Training Epoch: 1/2, step 557/7134 completed (loss: 0.10695655643939972, acc: 0.9644669890403748)
[2025-02-13 02:29:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:56][root][INFO] - Training Epoch: 1/2, step 558/7134 completed (loss: 0.07626665383577347, acc: 0.978723406791687)
[2025-02-13 02:29:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:56][root][INFO] - Training Epoch: 1/2, step 559/7134 completed (loss: 0.1589587777853012, acc: 0.9501424431800842)
[2025-02-13 02:29:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:56][root][INFO] - Training Epoch: 1/2, step 560/7134 completed (loss: 0.08071770519018173, acc: 0.9806678295135498)
[2025-02-13 02:29:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:57][root][INFO] - Training Epoch: 1/2, step 561/7134 completed (loss: 0.11877058446407318, acc: 0.9719029664993286)
[2025-02-13 02:29:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:58][root][INFO] - Training Epoch: 1/2, step 562/7134 completed (loss: 0.09820099920034409, acc: 0.9761589169502258)
[2025-02-13 02:29:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:58][root][INFO] - Training Epoch: 1/2, step 563/7134 completed (loss: 0.15719789266586304, acc: 0.9572784900665283)
[2025-02-13 02:29:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:58][root][INFO] - Training Epoch: 1/2, step 564/7134 completed (loss: 0.14003492891788483, acc: 0.9653893709182739)
[2025-02-13 02:29:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:59][root][INFO] - Training Epoch: 1/2, step 565/7134 completed (loss: 0.09030980616807938, acc: 0.9731993079185486)
[2025-02-13 02:29:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:29:59][root][INFO] - Training Epoch: 1/2, step 566/7134 completed (loss: 0.18537601828575134, acc: 0.9559164643287659)
[2025-02-13 02:29:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:00][root][INFO] - Training Epoch: 1/2, step 567/7134 completed (loss: 0.11911085993051529, acc: 0.9747545719146729)
[2025-02-13 02:30:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:00][root][INFO] - Training Epoch: 1/2, step 568/7134 completed (loss: 0.07667344808578491, acc: 0.9764705896377563)
[2025-02-13 02:30:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:01][root][INFO] - Training Epoch: 1/2, step 569/7134 completed (loss: 0.08131056278944016, acc: 0.9753086566925049)
[2025-02-13 02:30:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:01][root][INFO] - Training Epoch: 1/2, step 570/7134 completed (loss: 0.07537222653627396, acc: 0.977673351764679)
[2025-02-13 02:30:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:02][root][INFO] - Training Epoch: 1/2, step 571/7134 completed (loss: 0.05351363867521286, acc: 0.983627200126648)
[2025-02-13 02:30:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:02][root][INFO] - Training Epoch: 1/2, step 572/7134 completed (loss: 0.049828074872493744, acc: 0.9832689762115479)
[2025-02-13 02:30:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:02][root][INFO] - Training Epoch: 1/2, step 573/7134 completed (loss: 0.056432224810123444, acc: 0.9846547245979309)
[2025-02-13 02:30:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:03][root][INFO] - Training Epoch: 1/2, step 574/7134 completed (loss: 0.06291528791189194, acc: 0.9828721880912781)
[2025-02-13 02:30:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:03][root][INFO] - Training Epoch: 1/2, step 575/7134 completed (loss: 0.08177126944065094, acc: 0.9820051193237305)
[2025-02-13 02:30:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:04][root][INFO] - Training Epoch: 1/2, step 576/7134 completed (loss: 0.04791323095560074, acc: 0.9876237511634827)
[2025-02-13 02:30:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:04][root][INFO] - Training Epoch: 1/2, step 577/7134 completed (loss: 0.07012797892093658, acc: 0.9815546870231628)
[2025-02-13 02:30:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:05][root][INFO] - Training Epoch: 1/2, step 578/7134 completed (loss: 0.06879817694425583, acc: 0.9779582619667053)
[2025-02-13 02:30:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:05][root][INFO] - Training Epoch: 1/2, step 579/7134 completed (loss: 0.141177698969841, acc: 0.9614890813827515)
[2025-02-13 02:30:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:06][root][INFO] - Training Epoch: 1/2, step 580/7134 completed (loss: 0.13958516716957092, acc: 0.9609882831573486)
[2025-02-13 02:30:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:06][root][INFO] - Training Epoch: 1/2, step 581/7134 completed (loss: 0.05306858941912651, acc: 0.9874476790428162)
[2025-02-13 02:30:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:07][root][INFO] - Training Epoch: 1/2, step 582/7134 completed (loss: 0.0880441665649414, acc: 0.9784430861473083)
[2025-02-13 02:30:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:07][root][INFO] - Training Epoch: 1/2, step 583/7134 completed (loss: 0.06293197721242905, acc: 0.983116865158081)
[2025-02-13 02:30:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:08][root][INFO] - Training Epoch: 1/2, step 584/7134 completed (loss: 0.06250382959842682, acc: 0.9784688949584961)
[2025-02-13 02:30:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:08][root][INFO] - Training Epoch: 1/2, step 585/7134 completed (loss: 0.08477873355150223, acc: 0.9806763529777527)
[2025-02-13 02:30:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:09][root][INFO] - Training Epoch: 1/2, step 586/7134 completed (loss: 0.07509054243564606, acc: 0.9778357148170471)
[2025-02-13 02:30:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:09][root][INFO] - Training Epoch: 1/2, step 587/7134 completed (loss: 0.0792776569724083, acc: 0.9822109341621399)
[2025-02-13 02:30:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:09][root][INFO] - Training Epoch: 1/2, step 588/7134 completed (loss: 0.07614835351705551, acc: 0.9843013882637024)
[2025-02-13 02:30:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:10][root][INFO] - Training Epoch: 1/2, step 589/7134 completed (loss: 0.07096721976995468, acc: 0.9823788404464722)
[2025-02-13 02:30:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:10][root][INFO] - Training Epoch: 1/2, step 590/7134 completed (loss: 0.041741743683815, acc: 0.9909774661064148)
[2025-02-13 02:30:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:11][root][INFO] - Training Epoch: 1/2, step 591/7134 completed (loss: 0.07666440308094025, acc: 0.9769975543022156)
[2025-02-13 02:30:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:11][root][INFO] - Training Epoch: 1/2, step 592/7134 completed (loss: 0.04459652304649353, acc: 0.9860405921936035)
[2025-02-13 02:30:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:12][root][INFO] - Training Epoch: 1/2, step 593/7134 completed (loss: 0.0779343843460083, acc: 0.9803149700164795)
[2025-02-13 02:30:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:12][root][INFO] - Training Epoch: 1/2, step 594/7134 completed (loss: 0.06541313230991364, acc: 0.9748427867889404)
[2025-02-13 02:30:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:13][root][INFO] - Training Epoch: 1/2, step 595/7134 completed (loss: 0.10535729676485062, acc: 0.975979745388031)
[2025-02-13 02:30:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:13][root][INFO] - Training Epoch: 1/2, step 596/7134 completed (loss: 0.05125785246491432, acc: 0.9896907210350037)
[2025-02-13 02:30:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:14][root][INFO] - Training Epoch: 1/2, step 597/7134 completed (loss: 0.1660420000553131, acc: 0.9591568112373352)
[2025-02-13 02:30:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:14][root][INFO] - Training Epoch: 1/2, step 598/7134 completed (loss: 0.12987357378005981, acc: 0.9594045877456665)
[2025-02-13 02:30:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:14][root][INFO] - Training Epoch: 1/2, step 599/7134 completed (loss: 0.07781289517879486, acc: 0.9749373197555542)
[2025-02-13 02:30:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:15][root][INFO] - Training Epoch: 1/2, step 600/7134 completed (loss: 0.07734496891498566, acc: 0.9807460904121399)
[2025-02-13 02:30:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:15][root][INFO] - Training Epoch: 1/2, step 601/7134 completed (loss: 0.08130086213350296, acc: 0.9723374843597412)
[2025-02-13 02:30:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:16][root][INFO] - Training Epoch: 1/2, step 602/7134 completed (loss: 0.08372573554515839, acc: 0.9779582619667053)
[2025-02-13 02:30:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:16][root][INFO] - Training Epoch: 1/2, step 603/7134 completed (loss: 0.06980301439762115, acc: 0.9845758080482483)
[2025-02-13 02:30:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:17][root][INFO] - Training Epoch: 1/2, step 604/7134 completed (loss: 0.08533172309398651, acc: 0.9692533016204834)
[2025-02-13 02:30:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:17][root][INFO] - Training Epoch: 1/2, step 605/7134 completed (loss: 0.08351125568151474, acc: 0.9727979302406311)
[2025-02-13 02:30:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:18][root][INFO] - Training Epoch: 1/2, step 606/7134 completed (loss: 0.1720464527606964, acc: 0.9641532897949219)
[2025-02-13 02:30:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:18][root][INFO] - Training Epoch: 1/2, step 607/7134 completed (loss: 0.08431914448738098, acc: 0.9826435446739197)
[2025-02-13 02:30:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:19][root][INFO] - Training Epoch: 1/2, step 608/7134 completed (loss: 0.06480227410793304, acc: 0.977979302406311)
[2025-02-13 02:30:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:19][root][INFO] - Training Epoch: 1/2, step 609/7134 completed (loss: 0.12073846161365509, acc: 0.9683840870857239)
[2025-02-13 02:30:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:20][root][INFO] - Training Epoch: 1/2, step 610/7134 completed (loss: 0.11992418766021729, acc: 0.9646017551422119)
[2025-02-13 02:30:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:20][root][INFO] - Training Epoch: 1/2, step 611/7134 completed (loss: 0.09549035131931305, acc: 0.9756097793579102)
[2025-02-13 02:30:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:21][root][INFO] - Training Epoch: 1/2, step 612/7134 completed (loss: 0.08830418437719345, acc: 0.9722955226898193)
[2025-02-13 02:30:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:21][root][INFO] - Training Epoch: 1/2, step 613/7134 completed (loss: 0.09661932289600372, acc: 0.9718639850616455)
[2025-02-13 02:30:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:22][root][INFO] - Training Epoch: 1/2, step 614/7134 completed (loss: 0.0691417008638382, acc: 0.9830124378204346)
[2025-02-13 02:30:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:22][root][INFO] - Training Epoch: 1/2, step 615/7134 completed (loss: 0.07436750829219818, acc: 0.979141116142273)
[2025-02-13 02:30:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:23][root][INFO] - Training Epoch: 1/2, step 616/7134 completed (loss: 0.08919811248779297, acc: 0.9761363863945007)
[2025-02-13 02:30:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:23][root][INFO] - Training Epoch: 1/2, step 617/7134 completed (loss: 0.07248730212450027, acc: 0.9760820269584656)
[2025-02-13 02:30:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:23][root][INFO] - Training Epoch: 1/2, step 618/7134 completed (loss: 0.07286974042654037, acc: 0.9732620120048523)
[2025-02-13 02:30:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:24][root][INFO] - Training Epoch: 1/2, step 619/7134 completed (loss: 0.10725206136703491, acc: 0.9797570705413818)
[2025-02-13 02:30:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:24][root][INFO] - Training Epoch: 1/2, step 620/7134 completed (loss: 0.16060349345207214, acc: 0.9665604829788208)
[2025-02-13 02:30:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:25][root][INFO] - Training Epoch: 1/2, step 621/7134 completed (loss: 0.11579275131225586, acc: 0.9756468534469604)
[2025-02-13 02:30:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:25][root][INFO] - Training Epoch: 1/2, step 622/7134 completed (loss: 0.05155997350811958, acc: 0.9856528043746948)
[2025-02-13 02:30:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:26][root][INFO] - Training Epoch: 1/2, step 623/7134 completed (loss: 0.10516616702079773, acc: 0.975218653678894)
[2025-02-13 02:30:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:26][root][INFO] - Training Epoch: 1/2, step 624/7134 completed (loss: 0.05879032611846924, acc: 0.9851351380348206)
[2025-02-13 02:30:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:27][root][INFO] - Training Epoch: 1/2, step 625/7134 completed (loss: 0.0894179418683052, acc: 0.9683377146720886)
[2025-02-13 02:30:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:27][root][INFO] - Training Epoch: 1/2, step 626/7134 completed (loss: 0.09199126064777374, acc: 0.9742268323898315)
[2025-02-13 02:30:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:28][root][INFO] - Training Epoch: 1/2, step 627/7134 completed (loss: 0.08592753857374191, acc: 0.9778761267662048)
[2025-02-13 02:30:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:28][root][INFO] - Training Epoch: 1/2, step 628/7134 completed (loss: 0.07004959881305695, acc: 0.9833564758300781)
[2025-02-13 02:30:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:29][root][INFO] - Training Epoch: 1/2, step 629/7134 completed (loss: 0.07750046998262405, acc: 0.9828326106071472)
[2025-02-13 02:30:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:29][root][INFO] - Training Epoch: 1/2, step 630/7134 completed (loss: 0.05678447335958481, acc: 0.9818652868270874)
[2025-02-13 02:30:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:29][root][INFO] - Training Epoch: 1/2, step 631/7134 completed (loss: 0.07163216173648834, acc: 0.9842105507850647)
[2025-02-13 02:30:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:30][root][INFO] - Training Epoch: 1/2, step 632/7134 completed (loss: 0.07965629547834396, acc: 0.9822006225585938)
[2025-02-13 02:30:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:30][root][INFO] - Training Epoch: 1/2, step 633/7134 completed (loss: 0.06990387290716171, acc: 0.9811320900917053)
[2025-02-13 02:30:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:31][root][INFO] - Training Epoch: 1/2, step 634/7134 completed (loss: 0.05238362029194832, acc: 0.9905063509941101)
[2025-02-13 02:30:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:31][root][INFO] - Training Epoch: 1/2, step 635/7134 completed (loss: 0.05289260670542717, acc: 0.9862595200538635)
[2025-02-13 02:30:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:32][root][INFO] - Training Epoch: 1/2, step 636/7134 completed (loss: 0.11242613941431046, acc: 0.9724137783050537)
[2025-02-13 02:30:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:32][root][INFO] - Training Epoch: 1/2, step 637/7134 completed (loss: 0.0665125921368599, acc: 0.9832402467727661)
[2025-02-13 02:30:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:33][root][INFO] - Training Epoch: 1/2, step 638/7134 completed (loss: 0.08268990367650986, acc: 0.9812792539596558)
[2025-02-13 02:30:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:33][root][INFO] - Training Epoch: 1/2, step 639/7134 completed (loss: 0.07538540661334991, acc: 0.9816666841506958)
[2025-02-13 02:30:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:33][root][INFO] - Training Epoch: 1/2, step 640/7134 completed (loss: 0.0764494240283966, acc: 0.9808695912361145)
[2025-02-13 02:30:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:34][root][INFO] - Training Epoch: 1/2, step 641/7134 completed (loss: 0.06414276361465454, acc: 0.978723406791687)
[2025-02-13 02:30:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:34][root][INFO] - Training Epoch: 1/2, step 642/7134 completed (loss: 0.040238093584775925, acc: 0.9846938848495483)
[2025-02-13 02:30:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:35][root][INFO] - Training Epoch: 1/2, step 643/7134 completed (loss: 0.04862939193844795, acc: 0.9871630072593689)
[2025-02-13 02:30:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:35][root][INFO] - Training Epoch: 1/2, step 644/7134 completed (loss: 0.03707054257392883, acc: 0.992277979850769)
[2025-02-13 02:30:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:36][root][INFO] - Training Epoch: 1/2, step 645/7134 completed (loss: 0.032482825219631195, acc: 0.9898989796638489)
[2025-02-13 02:30:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:36][root][INFO] - Training Epoch: 1/2, step 646/7134 completed (loss: 0.0702957957983017, acc: 0.9847095012664795)
[2025-02-13 02:30:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:36][root][INFO] - Training Epoch: 1/2, step 647/7134 completed (loss: 0.054136767983436584, acc: 0.9813874959945679)
[2025-02-13 02:30:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:37][root][INFO] - Training Epoch: 1/2, step 648/7134 completed (loss: 0.12785273790359497, acc: 0.9616056084632874)
[2025-02-13 02:30:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:37][root][INFO] - Training Epoch: 1/2, step 649/7134 completed (loss: 0.08743192255496979, acc: 0.9794871807098389)
[2025-02-13 02:30:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:38][root][INFO] - Training Epoch: 1/2, step 650/7134 completed (loss: 0.09530162066221237, acc: 0.9780219793319702)
[2025-02-13 02:30:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:38][root][INFO] - Training Epoch: 1/2, step 651/7134 completed (loss: 0.21259348094463348, acc: 0.9418960213661194)
[2025-02-13 02:30:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:39][root][INFO] - Training Epoch: 1/2, step 652/7134 completed (loss: 0.08912728726863861, acc: 0.9707903861999512)
[2025-02-13 02:30:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:39][root][INFO] - Training Epoch: 1/2, step 653/7134 completed (loss: 0.07958751171827316, acc: 0.9773049354553223)
[2025-02-13 02:30:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:40][root][INFO] - Training Epoch: 1/2, step 654/7134 completed (loss: 0.23935633897781372, acc: 0.9438444972038269)
[2025-02-13 02:30:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:40][root][INFO] - Training Epoch: 1/2, step 655/7134 completed (loss: 0.0984976515173912, acc: 0.975806474685669)
[2025-02-13 02:30:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:40][root][INFO] - Training Epoch: 1/2, step 656/7134 completed (loss: 0.14017127454280853, acc: 0.9525066018104553)
[2025-02-13 02:30:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:41][root][INFO] - Training Epoch: 1/2, step 657/7134 completed (loss: 0.10320048034191132, acc: 0.971107542514801)
[2025-02-13 02:30:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:41][root][INFO] - Training Epoch: 1/2, step 658/7134 completed (loss: 0.07871416956186295, acc: 0.9815546870231628)
[2025-02-13 02:30:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:42][root][INFO] - Training Epoch: 1/2, step 659/7134 completed (loss: 0.058639392256736755, acc: 0.9794721603393555)
[2025-02-13 02:30:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:42][root][INFO] - Training Epoch: 1/2, step 660/7134 completed (loss: 0.10235176235437393, acc: 0.9795321822166443)
[2025-02-13 02:30:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:43][root][INFO] - Training Epoch: 1/2, step 661/7134 completed (loss: 0.10818185657262802, acc: 0.9686192274093628)
[2025-02-13 02:30:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:43][root][INFO] - Training Epoch: 1/2, step 662/7134 completed (loss: 0.07551924139261246, acc: 0.9825737476348877)
[2025-02-13 02:30:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:44][root][INFO] - Training Epoch: 1/2, step 663/7134 completed (loss: 0.06992324441671371, acc: 0.9797979593276978)
[2025-02-13 02:30:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:44][root][INFO] - Training Epoch: 1/2, step 664/7134 completed (loss: 0.06778719276189804, acc: 0.9800000190734863)
[2025-02-13 02:30:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:45][root][INFO] - Training Epoch: 1/2, step 665/7134 completed (loss: 0.1270003765821457, acc: 0.9623149633407593)
[2025-02-13 02:30:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:45][root][INFO] - Training Epoch: 1/2, step 666/7134 completed (loss: 0.09480275958776474, acc: 0.9752747416496277)
[2025-02-13 02:30:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:46][root][INFO] - Training Epoch: 1/2, step 667/7134 completed (loss: 0.08804222196340561, acc: 0.9749631881713867)
[2025-02-13 02:30:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:46][root][INFO] - Training Epoch: 1/2, step 668/7134 completed (loss: 0.05281337723135948, acc: 0.9885931611061096)
[2025-02-13 02:30:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:47][root][INFO] - Training Epoch: 1/2, step 669/7134 completed (loss: 0.07246873527765274, acc: 0.9809004068374634)
[2025-02-13 02:30:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:47][root][INFO] - Training Epoch: 1/2, step 670/7134 completed (loss: 0.06641848385334015, acc: 0.9804928302764893)
[2025-02-13 02:30:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:48][root][INFO] - Training Epoch: 1/2, step 671/7134 completed (loss: 0.07119486480951309, acc: 0.9774919748306274)
[2025-02-13 02:30:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:48][root][INFO] - Training Epoch: 1/2, step 672/7134 completed (loss: 0.08966472744941711, acc: 0.9836065769195557)
[2025-02-13 02:30:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:49][root][INFO] - Training Epoch: 1/2, step 673/7134 completed (loss: 0.08458277583122253, acc: 0.9791154861450195)
[2025-02-13 02:30:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:49][root][INFO] - Training Epoch: 1/2, step 674/7134 completed (loss: 0.08953632414340973, acc: 0.9713423848152161)
[2025-02-13 02:30:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:50][root][INFO] - Training Epoch: 1/2, step 675/7134 completed (loss: 0.0958004742860794, acc: 0.9804432988166809)
[2025-02-13 02:30:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:50][root][INFO] - Training Epoch: 1/2, step 676/7134 completed (loss: 0.08658663928508759, acc: 0.9822485446929932)
[2025-02-13 02:30:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:51][root][INFO] - Training Epoch: 1/2, step 677/7134 completed (loss: 0.0400235578417778, acc: 0.9913686513900757)
[2025-02-13 02:30:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:51][root][INFO] - Training Epoch: 1/2, step 678/7134 completed (loss: 0.06573035567998886, acc: 0.9798115491867065)
[2025-02-13 02:30:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:52][root][INFO] - Training Epoch: 1/2, step 679/7134 completed (loss: 0.024830764159560204, acc: 0.9943289160728455)
[2025-02-13 02:30:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:52][root][INFO] - Training Epoch: 1/2, step 680/7134 completed (loss: 0.11632775515317917, acc: 0.9716981053352356)
[2025-02-13 02:30:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:52][root][INFO] - Training Epoch: 1/2, step 681/7134 completed (loss: 0.06217201426625252, acc: 0.9852761030197144)
[2025-02-13 02:30:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:53][root][INFO] - Training Epoch: 1/2, step 682/7134 completed (loss: 0.06234658882021904, acc: 0.9827089309692383)
[2025-02-13 02:30:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:53][root][INFO] - Training Epoch: 1/2, step 683/7134 completed (loss: 0.056703805923461914, acc: 0.9876543283462524)
[2025-02-13 02:30:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:54][root][INFO] - Training Epoch: 1/2, step 684/7134 completed (loss: 0.060983192175626755, acc: 0.9792243838310242)
[2025-02-13 02:30:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:54][root][INFO] - Training Epoch: 1/2, step 685/7134 completed (loss: 0.057750120759010315, acc: 0.9837905168533325)
[2025-02-13 02:30:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:55][root][INFO] - Training Epoch: 1/2, step 686/7134 completed (loss: 0.025086557492613792, acc: 0.9947159886360168)
[2025-02-13 02:30:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:55][root][INFO] - Training Epoch: 1/2, step 687/7134 completed (loss: 0.06810791790485382, acc: 0.9864603281021118)
[2025-02-13 02:30:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:56][root][INFO] - Training Epoch: 1/2, step 688/7134 completed (loss: 0.04446619376540184, acc: 0.9866864085197449)
[2025-02-13 02:30:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:56][root][INFO] - Training Epoch: 1/2, step 689/7134 completed (loss: 0.024563651531934738, acc: 0.9897435903549194)
[2025-02-13 02:30:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:57][root][INFO] - Training Epoch: 1/2, step 690/7134 completed (loss: 0.08347354829311371, acc: 0.9761525988578796)
[2025-02-13 02:30:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:57][root][INFO] - Training Epoch: 1/2, step 691/7134 completed (loss: 0.09904033690690994, acc: 0.9741379022598267)
[2025-02-13 02:30:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:58][root][INFO] - Training Epoch: 1/2, step 692/7134 completed (loss: 0.10199260711669922, acc: 0.9679999947547913)
[2025-02-13 02:30:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:58][root][INFO] - Training Epoch: 1/2, step 693/7134 completed (loss: 0.09510097652673721, acc: 0.9676898121833801)
[2025-02-13 02:30:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:59][root][INFO] - Training Epoch: 1/2, step 694/7134 completed (loss: 0.10882662981748581, acc: 0.970802903175354)
[2025-02-13 02:30:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:30:59][root][INFO] - Training Epoch: 1/2, step 695/7134 completed (loss: 0.12825065851211548, acc: 0.9690189361572266)
[2025-02-13 02:30:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:00][root][INFO] - Training Epoch: 1/2, step 696/7134 completed (loss: 0.11911999434232712, acc: 0.96278315782547)
[2025-02-13 02:31:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:00][root][INFO] - Training Epoch: 1/2, step 697/7134 completed (loss: 0.07335720956325531, acc: 0.9838709831237793)
[2025-02-13 02:31:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:00][root][INFO] - Training Epoch: 1/2, step 698/7134 completed (loss: 0.07133668661117554, acc: 0.9820846915245056)
[2025-02-13 02:31:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:01][root][INFO] - Training Epoch: 1/2, step 699/7134 completed (loss: 0.04962396249175072, acc: 0.985981285572052)
[2025-02-13 02:31:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:01][root][INFO] - Training Epoch: 1/2, step 700/7134 completed (loss: 0.09248039126396179, acc: 0.9813432693481445)
[2025-02-13 02:31:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:02][root][INFO] - Training Epoch: 1/2, step 701/7134 completed (loss: 0.06076285243034363, acc: 0.978723406791687)
[2025-02-13 02:31:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:02][root][INFO] - Training Epoch: 1/2, step 702/7134 completed (loss: 0.07628612965345383, acc: 0.9805194735527039)
[2025-02-13 02:31:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:03][root][INFO] - Training Epoch: 1/2, step 703/7134 completed (loss: 0.06548286229372025, acc: 0.9806138873100281)
[2025-02-13 02:31:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:03][root][INFO] - Training Epoch: 1/2, step 704/7134 completed (loss: 0.10175186395645142, acc: 0.9635258316993713)
[2025-02-13 02:31:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:04][root][INFO] - Training Epoch: 1/2, step 705/7134 completed (loss: 0.12548281252384186, acc: 0.9653631448745728)
[2025-02-13 02:31:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:04][root][INFO] - Training Epoch: 1/2, step 706/7134 completed (loss: 0.05078420042991638, acc: 0.9846827387809753)
[2025-02-13 02:31:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:05][root][INFO] - Training Epoch: 1/2, step 707/7134 completed (loss: 0.07689285278320312, acc: 0.9724220633506775)
[2025-02-13 02:31:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:05][root][INFO] - Training Epoch: 1/2, step 708/7134 completed (loss: 0.05382620915770531, acc: 0.9826517701148987)
[2025-02-13 02:31:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:06][root][INFO] - Training Epoch: 1/2, step 709/7134 completed (loss: 0.07715167105197906, acc: 0.9737470149993896)
[2025-02-13 02:31:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:06][root][INFO] - Training Epoch: 1/2, step 710/7134 completed (loss: 0.07172579318284988, acc: 0.9782082438468933)
[2025-02-13 02:31:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:07][root][INFO] - Training Epoch: 1/2, step 711/7134 completed (loss: 0.062196943908929825, acc: 0.9834710955619812)
[2025-02-13 02:31:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:07][root][INFO] - Training Epoch: 1/2, step 712/7134 completed (loss: 0.06010597199201584, acc: 0.9841269850730896)
[2025-02-13 02:31:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:08][root][INFO] - Training Epoch: 1/2, step 713/7134 completed (loss: 0.03905496746301651, acc: 0.9873125553131104)
[2025-02-13 02:31:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:08][root][INFO] - Training Epoch: 1/2, step 714/7134 completed (loss: 0.04293620586395264, acc: 0.9855072498321533)
[2025-02-13 02:31:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:08][root][INFO] - Training Epoch: 1/2, step 715/7134 completed (loss: 0.06285664439201355, acc: 0.9815340638160706)
[2025-02-13 02:31:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:09][root][INFO] - Training Epoch: 1/2, step 716/7134 completed (loss: 0.0610087513923645, acc: 0.979522168636322)
[2025-02-13 02:31:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:09][root][INFO] - Training Epoch: 1/2, step 717/7134 completed (loss: 0.06137937679886818, acc: 0.9873577952384949)
[2025-02-13 02:31:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:10][root][INFO] - Training Epoch: 1/2, step 718/7134 completed (loss: 0.04353668913245201, acc: 0.9887217879295349)
[2025-02-13 02:31:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:10][root][INFO] - Training Epoch: 1/2, step 719/7134 completed (loss: 0.02972099743783474, acc: 0.9913793206214905)
[2025-02-13 02:31:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:11][root][INFO] - Training Epoch: 1/2, step 720/7134 completed (loss: 0.0729152262210846, acc: 0.9799498915672302)
[2025-02-13 02:31:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:11][root][INFO] - Training Epoch: 1/2, step 721/7134 completed (loss: 0.09402089565992355, acc: 0.9769526124000549)
[2025-02-13 02:31:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:12][root][INFO] - Training Epoch: 1/2, step 722/7134 completed (loss: 0.06511443853378296, acc: 0.9801633358001709)
[2025-02-13 02:31:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:12][root][INFO] - Training Epoch: 1/2, step 723/7134 completed (loss: 0.032238904386758804, acc: 0.990314781665802)
[2025-02-13 02:31:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:13][root][INFO] - Training Epoch: 1/2, step 724/7134 completed (loss: 0.07524733990430832, acc: 0.9856938719749451)
[2025-02-13 02:31:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:13][root][INFO] - Training Epoch: 1/2, step 725/7134 completed (loss: 0.06391323357820511, acc: 0.9840810298919678)
[2025-02-13 02:31:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:14][root][INFO] - Training Epoch: 1/2, step 726/7134 completed (loss: 0.05363183468580246, acc: 0.9874476790428162)
[2025-02-13 02:31:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:14][root][INFO] - Training Epoch: 1/2, step 727/7134 completed (loss: 0.058274269104003906, acc: 0.9826839566230774)
[2025-02-13 02:31:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:15][root][INFO] - Training Epoch: 1/2, step 728/7134 completed (loss: 0.04967609792947769, acc: 0.9876033067703247)
[2025-02-13 02:31:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:15][root][INFO] - Training Epoch: 1/2, step 729/7134 completed (loss: 0.04217559099197388, acc: 0.9860917925834656)
[2025-02-13 02:31:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:16][root][INFO] - Training Epoch: 1/2, step 730/7134 completed (loss: 0.050316017121076584, acc: 0.9870298504829407)
[2025-02-13 02:31:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:16][root][INFO] - Training Epoch: 1/2, step 731/7134 completed (loss: 0.05497531592845917, acc: 0.9893333315849304)
[2025-02-13 02:31:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:16][root][INFO] - Training Epoch: 1/2, step 732/7134 completed (loss: 0.11953096091747284, acc: 0.9679245352745056)
[2025-02-13 02:31:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:17][root][INFO] - Training Epoch: 1/2, step 733/7134 completed (loss: 0.06766519695520401, acc: 0.9751332402229309)
[2025-02-13 02:31:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:17][root][INFO] - Training Epoch: 1/2, step 734/7134 completed (loss: 0.06724245101213455, acc: 0.9784946441650391)
[2025-02-13 02:31:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:18][root][INFO] - Training Epoch: 1/2, step 735/7134 completed (loss: 0.04751267284154892, acc: 0.9871382713317871)
[2025-02-13 02:31:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:18][root][INFO] - Training Epoch: 1/2, step 736/7134 completed (loss: 0.07045285403728485, acc: 0.9714285731315613)
[2025-02-13 02:31:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:19][root][INFO] - Training Epoch: 1/2, step 737/7134 completed (loss: 0.18430499732494354, acc: 0.953346848487854)
[2025-02-13 02:31:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:19][root][INFO] - Training Epoch: 1/2, step 738/7134 completed (loss: 0.028543703258037567, acc: 0.9942307472229004)
[2025-02-13 02:31:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:19][root][INFO] - Training Epoch: 1/2, step 739/7134 completed (loss: 0.058150336146354675, acc: 0.9816513657569885)
[2025-02-13 02:31:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:20][root][INFO] - Training Epoch: 1/2, step 740/7134 completed (loss: 0.02076447196304798, acc: 0.9936507940292358)
[2025-02-13 02:31:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:20][root][INFO] - Training Epoch: 1/2, step 741/7134 completed (loss: 0.0675794780254364, acc: 0.9858044385910034)
[2025-02-13 02:31:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:21][root][INFO] - Training Epoch: 1/2, step 742/7134 completed (loss: 0.0502777025103569, acc: 0.9811320900917053)
[2025-02-13 02:31:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:21][root][INFO] - Training Epoch: 1/2, step 743/7134 completed (loss: 0.0900689885020256, acc: 0.9774096608161926)
[2025-02-13 02:31:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:22][root][INFO] - Training Epoch: 1/2, step 744/7134 completed (loss: 0.06635463237762451, acc: 0.9742063283920288)
[2025-02-13 02:31:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:22][root][INFO] - Training Epoch: 1/2, step 745/7134 completed (loss: 0.04495907202363014, acc: 0.9864048361778259)
[2025-02-13 02:31:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:22][root][INFO] - Training Epoch: 1/2, step 746/7134 completed (loss: 0.05755244940519333, acc: 0.9852216839790344)
[2025-02-13 02:31:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:23][root][INFO] - Training Epoch: 1/2, step 747/7134 completed (loss: 0.1361318677663803, acc: 0.9684385657310486)
[2025-02-13 02:31:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:23][root][INFO] - Training Epoch: 1/2, step 748/7134 completed (loss: 0.05990851670503616, acc: 0.9849624037742615)
[2025-02-13 02:31:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:24][root][INFO] - Training Epoch: 1/2, step 749/7134 completed (loss: 0.03160225600004196, acc: 0.9942362904548645)
[2025-02-13 02:31:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:24][root][INFO] - Training Epoch: 1/2, step 750/7134 completed (loss: 0.06554495543241501, acc: 0.976576566696167)
[2025-02-13 02:31:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:25][root][INFO] - Training Epoch: 1/2, step 751/7134 completed (loss: 0.10163725167512894, acc: 0.9761499166488647)
[2025-02-13 02:31:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:25][root][INFO] - Training Epoch: 1/2, step 752/7134 completed (loss: 0.07158814370632172, acc: 0.9789789915084839)
[2025-02-13 02:31:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:26][root][INFO] - Training Epoch: 1/2, step 753/7134 completed (loss: 0.06430767476558685, acc: 0.9763513803482056)
[2025-02-13 02:31:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:26][root][INFO] - Training Epoch: 1/2, step 754/7134 completed (loss: 0.04659107327461243, acc: 0.9909502267837524)
[2025-02-13 02:31:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:27][root][INFO] - Training Epoch: 1/2, step 755/7134 completed (loss: 0.09238208085298538, acc: 0.9785478711128235)
[2025-02-13 02:31:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:27][root][INFO] - Training Epoch: 1/2, step 756/7134 completed (loss: 0.07670685648918152, acc: 0.977570116519928)
[2025-02-13 02:31:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:27][root][INFO] - Training Epoch: 1/2, step 757/7134 completed (loss: 0.041872695088386536, acc: 0.9865900278091431)
[2025-02-13 02:31:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:28][root][INFO] - Training Epoch: 1/2, step 758/7134 completed (loss: 0.09232456982135773, acc: 0.9845094680786133)
[2025-02-13 02:31:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:28][root][INFO] - Training Epoch: 1/2, step 759/7134 completed (loss: 0.0948113203048706, acc: 0.9689922332763672)
[2025-02-13 02:31:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:29][root][INFO] - Training Epoch: 1/2, step 760/7134 completed (loss: 0.030682366341352463, acc: 0.9913344979286194)
[2025-02-13 02:31:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:29][root][INFO] - Training Epoch: 1/2, step 761/7134 completed (loss: 0.032231882214546204, acc: 0.992175281047821)
[2025-02-13 02:31:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:29][root][INFO] - Training Epoch: 1/2, step 762/7134 completed (loss: 0.058002449572086334, acc: 0.9856630563735962)
[2025-02-13 02:31:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:30][root][INFO] - Training Epoch: 1/2, step 763/7134 completed (loss: 0.10646791756153107, acc: 0.9674267172813416)
[2025-02-13 02:31:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:30][root][INFO] - Training Epoch: 1/2, step 764/7134 completed (loss: 0.04609295353293419, acc: 0.9856528043746948)
[2025-02-13 02:31:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:31][root][INFO] - Training Epoch: 1/2, step 765/7134 completed (loss: 0.03569643571972847, acc: 0.9882869720458984)
[2025-02-13 02:31:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:31][root][INFO] - Training Epoch: 1/2, step 766/7134 completed (loss: 0.02329099364578724, acc: 0.9955489635467529)
[2025-02-13 02:31:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:32][root][INFO] - Training Epoch: 1/2, step 767/7134 completed (loss: 0.04170636832714081, acc: 0.9897511005401611)
[2025-02-13 02:31:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:32][root][INFO] - Training Epoch: 1/2, step 768/7134 completed (loss: 0.0677688866853714, acc: 0.9815863966941833)
[2025-02-13 02:31:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:33][root][INFO] - Training Epoch: 1/2, step 769/7134 completed (loss: 0.05518564581871033, acc: 0.984240710735321)
[2025-02-13 02:31:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:33][root][INFO] - Training Epoch: 1/2, step 770/7134 completed (loss: 0.053759846836328506, acc: 0.9862174391746521)
[2025-02-13 02:31:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:33][root][INFO] - Training Epoch: 1/2, step 771/7134 completed (loss: 0.05992463603615761, acc: 0.9841583967208862)
[2025-02-13 02:31:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:34][root][INFO] - Training Epoch: 1/2, step 772/7134 completed (loss: 0.05658631771802902, acc: 0.9899497628211975)
[2025-02-13 02:31:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:34][root][INFO] - Training Epoch: 1/2, step 773/7134 completed (loss: 0.08045554161071777, acc: 0.9794344305992126)
[2025-02-13 02:31:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:35][root][INFO] - Training Epoch: 1/2, step 774/7134 completed (loss: 0.04465821012854576, acc: 0.9867109656333923)
[2025-02-13 02:31:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:35][root][INFO] - Training Epoch: 1/2, step 775/7134 completed (loss: 0.03571366146206856, acc: 0.989347517490387)
[2025-02-13 02:31:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:36][root][INFO] - Training Epoch: 1/2, step 776/7134 completed (loss: 0.027861233800649643, acc: 0.9955621361732483)
[2025-02-13 02:31:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:36][root][INFO] - Training Epoch: 1/2, step 777/7134 completed (loss: 0.06045055761933327, acc: 0.9887640476226807)
[2025-02-13 02:31:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:37][root][INFO] - Training Epoch: 1/2, step 778/7134 completed (loss: 0.0624283105134964, acc: 0.9871794581413269)
[2025-02-13 02:31:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:37][root][INFO] - Training Epoch: 1/2, step 779/7134 completed (loss: 0.04291713610291481, acc: 0.9912408590316772)
[2025-02-13 02:31:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:38][root][INFO] - Training Epoch: 1/2, step 780/7134 completed (loss: 0.09288155287504196, acc: 0.9733840227127075)
[2025-02-13 02:31:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:38][root][INFO] - Training Epoch: 1/2, step 781/7134 completed (loss: 0.08842670172452927, acc: 0.9792429804801941)
[2025-02-13 02:31:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:38][root][INFO] - Training Epoch: 1/2, step 782/7134 completed (loss: 0.06027950346469879, acc: 0.9810126423835754)
[2025-02-13 02:31:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:39][root][INFO] - Training Epoch: 1/2, step 783/7134 completed (loss: 0.09837114810943604, acc: 0.9643312096595764)
[2025-02-13 02:31:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:39][root][INFO] - Training Epoch: 1/2, step 784/7134 completed (loss: 0.04340452328324318, acc: 0.9902557730674744)
[2025-02-13 02:31:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:40][root][INFO] - Training Epoch: 1/2, step 785/7134 completed (loss: 0.048543598502874374, acc: 0.9882943034172058)
[2025-02-13 02:31:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:40][root][INFO] - Training Epoch: 1/2, step 786/7134 completed (loss: 0.057456620037555695, acc: 0.9888476133346558)
[2025-02-13 02:31:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:41][root][INFO] - Training Epoch: 1/2, step 787/7134 completed (loss: 0.014629830606281757, acc: 0.9963503479957581)
[2025-02-13 02:31:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:41][root][INFO] - Training Epoch: 1/2, step 788/7134 completed (loss: 0.08326249569654465, acc: 0.9735682606697083)
[2025-02-13 02:31:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:42][root][INFO] - Training Epoch: 1/2, step 789/7134 completed (loss: 0.0971374362707138, acc: 0.9694656729698181)
[2025-02-13 02:31:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:42][root][INFO] - Training Epoch: 1/2, step 790/7134 completed (loss: 0.13785025477409363, acc: 0.9704142212867737)
[2025-02-13 02:31:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:42][root][INFO] - Training Epoch: 1/2, step 791/7134 completed (loss: 0.11659833788871765, acc: 0.9723320007324219)
[2025-02-13 02:31:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:43][root][INFO] - Training Epoch: 1/2, step 792/7134 completed (loss: 0.10229911655187607, acc: 0.9768160581588745)
[2025-02-13 02:31:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:43][root][INFO] - Training Epoch: 1/2, step 793/7134 completed (loss: 0.05434389039874077, acc: 0.9837278127670288)
[2025-02-13 02:31:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:44][root][INFO] - Training Epoch: 1/2, step 794/7134 completed (loss: 0.05508643016219139, acc: 0.9836829900741577)
[2025-02-13 02:31:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:44][root][INFO] - Training Epoch: 1/2, step 795/7134 completed (loss: 0.11738793551921844, acc: 0.9688473343849182)
[2025-02-13 02:31:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:45][root][INFO] - Training Epoch: 1/2, step 796/7134 completed (loss: 0.12347336858510971, acc: 0.9696969985961914)
[2025-02-13 02:31:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:45][root][INFO] - Training Epoch: 1/2, step 797/7134 completed (loss: 0.08585098385810852, acc: 0.977911651134491)
[2025-02-13 02:31:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:45][root][INFO] - Training Epoch: 1/2, step 798/7134 completed (loss: 0.044572845101356506, acc: 0.9920634627342224)
[2025-02-13 02:31:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:46][root][INFO] - Training Epoch: 1/2, step 799/7134 completed (loss: 0.066800057888031, acc: 0.9827957153320312)
[2025-02-13 02:31:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:46][root][INFO] - Training Epoch: 1/2, step 800/7134 completed (loss: 0.10086458176374435, acc: 0.9795158505439758)
[2025-02-13 02:31:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:47][root][INFO] - Training Epoch: 1/2, step 801/7134 completed (loss: 0.10201733559370041, acc: 0.9760000109672546)
[2025-02-13 02:31:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:47][root][INFO] - Training Epoch: 1/2, step 802/7134 completed (loss: 0.0570104718208313, acc: 0.9806763529777527)
[2025-02-13 02:31:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:48][root][INFO] - Training Epoch: 1/2, step 803/7134 completed (loss: 0.0492621473968029, acc: 0.987500011920929)
[2025-02-13 02:31:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:48][root][INFO] - Training Epoch: 1/2, step 804/7134 completed (loss: 0.0628725215792656, acc: 0.9796333909034729)
[2025-02-13 02:31:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:48][root][INFO] - Training Epoch: 1/2, step 805/7134 completed (loss: 0.0743449479341507, acc: 0.9737827777862549)
[2025-02-13 02:31:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:49][root][INFO] - Training Epoch: 1/2, step 806/7134 completed (loss: 0.028449146077036858, acc: 0.9916201233863831)
[2025-02-13 02:31:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:49][root][INFO] - Training Epoch: 1/2, step 807/7134 completed (loss: 0.06399572640657425, acc: 0.9817517995834351)
[2025-02-13 02:31:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:50][root][INFO] - Training Epoch: 1/2, step 808/7134 completed (loss: 0.03414487466216087, acc: 0.9892215728759766)
[2025-02-13 02:31:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:50][root][INFO] - Training Epoch: 1/2, step 809/7134 completed (loss: 0.02878381870687008, acc: 0.98959881067276)
[2025-02-13 02:31:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:51][root][INFO] - Training Epoch: 1/2, step 810/7134 completed (loss: 0.06623533368110657, acc: 0.9811066389083862)
[2025-02-13 02:31:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:51][root][INFO] - Training Epoch: 1/2, step 811/7134 completed (loss: 0.09734799712896347, acc: 0.9771528840065002)
[2025-02-13 02:31:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:52][root][INFO] - Training Epoch: 1/2, step 812/7134 completed (loss: 0.05652811750769615, acc: 0.9810844659805298)
[2025-02-13 02:31:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:52][root][INFO] - Training Epoch: 1/2, step 813/7134 completed (loss: 0.05471052601933479, acc: 0.9840348362922668)
[2025-02-13 02:31:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:53][root][INFO] - Training Epoch: 1/2, step 814/7134 completed (loss: 0.06826436519622803, acc: 0.9845161437988281)
[2025-02-13 02:31:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:53][root][INFO] - Training Epoch: 1/2, step 815/7134 completed (loss: 0.09417310357093811, acc: 0.9752704501152039)
[2025-02-13 02:31:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:53][root][INFO] - Training Epoch: 1/2, step 816/7134 completed (loss: 0.0723421573638916, acc: 0.9796609878540039)
[2025-02-13 02:31:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:54][root][INFO] - Training Epoch: 1/2, step 817/7134 completed (loss: 0.10712368786334991, acc: 0.9762340188026428)
[2025-02-13 02:31:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:54][root][INFO] - Training Epoch: 1/2, step 818/7134 completed (loss: 0.12332046031951904, acc: 0.9604811072349548)
[2025-02-13 02:31:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:55][root][INFO] - Training Epoch: 1/2, step 819/7134 completed (loss: 0.07144208997488022, acc: 0.9855999946594238)
[2025-02-13 02:31:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:55][root][INFO] - Training Epoch: 1/2, step 820/7134 completed (loss: 0.04505513980984688, acc: 0.9860896468162537)
[2025-02-13 02:31:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:56][root][INFO] - Training Epoch: 1/2, step 821/7134 completed (loss: 0.0414653904736042, acc: 0.9946380853652954)
[2025-02-13 02:31:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:56][root][INFO] - Training Epoch: 1/2, step 822/7134 completed (loss: 0.03705161064863205, acc: 0.9858712553977966)
[2025-02-13 02:31:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:57][root][INFO] - Training Epoch: 1/2, step 823/7134 completed (loss: 0.025590695440769196, acc: 0.9924952983856201)
[2025-02-13 02:31:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:57][root][INFO] - Training Epoch: 1/2, step 824/7134 completed (loss: 0.09729118645191193, acc: 0.971731424331665)
[2025-02-13 02:31:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:58][root][INFO] - Training Epoch: 1/2, step 825/7134 completed (loss: 0.0618896521627903, acc: 0.990777313709259)
[2025-02-13 02:31:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:58][root][INFO] - Training Epoch: 1/2, step 826/7134 completed (loss: 0.054899804294109344, acc: 0.9836512207984924)
[2025-02-13 02:31:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:59][root][INFO] - Training Epoch: 1/2, step 827/7134 completed (loss: 0.05395464226603508, acc: 0.9816272854804993)
[2025-02-13 02:31:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:59][root][INFO] - Training Epoch: 1/2, step 828/7134 completed (loss: 0.05546783283352852, acc: 0.9891135096549988)
[2025-02-13 02:31:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:31:59][root][INFO] - Training Epoch: 1/2, step 829/7134 completed (loss: 0.051961250603199005, acc: 0.987864077091217)
[2025-02-13 02:32:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:00][root][INFO] - Training Epoch: 1/2, step 830/7134 completed (loss: 0.03873002156615257, acc: 0.985049843788147)
[2025-02-13 02:32:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:00][root][INFO] - Training Epoch: 1/2, step 831/7134 completed (loss: 0.041120342910289764, acc: 0.9890410900115967)
[2025-02-13 02:32:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:01][root][INFO] - Training Epoch: 1/2, step 832/7134 completed (loss: 0.12791599333286285, acc: 0.9761092066764832)
[2025-02-13 02:32:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:01][root][INFO] - Training Epoch: 1/2, step 833/7134 completed (loss: 0.03430585563182831, acc: 0.9887955188751221)
[2025-02-13 02:32:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:02][root][INFO] - Training Epoch: 1/2, step 834/7134 completed (loss: 0.026942571625113487, acc: 0.9958041906356812)
[2025-02-13 02:32:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:02][root][INFO] - Training Epoch: 1/2, step 835/7134 completed (loss: 0.06734593957662582, acc: 0.9846153855323792)
[2025-02-13 02:32:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:03][root][INFO] - Training Epoch: 1/2, step 836/7134 completed (loss: 0.05988514423370361, acc: 0.9767981171607971)
[2025-02-13 02:32:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:03][root][INFO] - Training Epoch: 1/2, step 837/7134 completed (loss: 0.04479226842522621, acc: 0.9918256402015686)
[2025-02-13 02:32:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:04][root][INFO] - Training Epoch: 1/2, step 838/7134 completed (loss: 0.03089318238198757, acc: 0.9888712167739868)
[2025-02-13 02:32:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:04][root][INFO] - Training Epoch: 1/2, step 839/7134 completed (loss: 0.024498483166098595, acc: 0.9880383014678955)
[2025-02-13 02:32:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:04][root][INFO] - Training Epoch: 1/2, step 840/7134 completed (loss: 0.04453545808792114, acc: 0.9902371168136597)
[2025-02-13 02:32:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:05][root][INFO] - Training Epoch: 1/2, step 841/7134 completed (loss: 0.07052668929100037, acc: 0.9792531132698059)
[2025-02-13 02:32:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:05][root][INFO] - Training Epoch: 1/2, step 842/7134 completed (loss: 0.07087446749210358, acc: 0.9811320900917053)
[2025-02-13 02:32:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:06][root][INFO] - Training Epoch: 1/2, step 843/7134 completed (loss: 0.04080634564161301, acc: 0.9916550517082214)
[2025-02-13 02:32:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:06][root][INFO] - Training Epoch: 1/2, step 844/7134 completed (loss: 0.05492177605628967, acc: 0.9842022061347961)
[2025-02-13 02:32:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:07][root][INFO] - Training Epoch: 1/2, step 845/7134 completed (loss: 0.08982020616531372, acc: 0.9787610769271851)
[2025-02-13 02:32:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:07][root][INFO] - Training Epoch: 1/2, step 846/7134 completed (loss: 0.059612102806568146, acc: 0.9851632118225098)
[2025-02-13 02:32:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:08][root][INFO] - Training Epoch: 1/2, step 847/7134 completed (loss: 0.05308869108557701, acc: 0.984635055065155)
[2025-02-13 02:32:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:08][root][INFO] - Training Epoch: 1/2, step 848/7134 completed (loss: 0.07186203449964523, acc: 0.9802225232124329)
[2025-02-13 02:32:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:09][root][INFO] - Training Epoch: 1/2, step 849/7134 completed (loss: 0.06577961891889572, acc: 0.9818621277809143)
[2025-02-13 02:32:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:09][root][INFO] - Training Epoch: 1/2, step 850/7134 completed (loss: 0.06244000792503357, acc: 0.9812080264091492)
[2025-02-13 02:32:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:10][root][INFO] - Training Epoch: 1/2, step 851/7134 completed (loss: 0.05991256237030029, acc: 0.9868593811988831)
[2025-02-13 02:32:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:10][root][INFO] - Training Epoch: 1/2, step 852/7134 completed (loss: 0.04000953584909439, acc: 0.9853479862213135)
[2025-02-13 02:32:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:11][root][INFO] - Training Epoch: 1/2, step 853/7134 completed (loss: 0.04146356135606766, acc: 0.9889867901802063)
[2025-02-13 02:32:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:11][root][INFO] - Training Epoch: 1/2, step 854/7134 completed (loss: 0.05367884039878845, acc: 0.9863184094429016)
[2025-02-13 02:32:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:11][root][INFO] - Training Epoch: 1/2, step 855/7134 completed (loss: 0.06412450969219208, acc: 0.9847775101661682)
[2025-02-13 02:32:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:12][root][INFO] - Training Epoch: 1/2, step 856/7134 completed (loss: 0.04091310501098633, acc: 0.9866828322410583)
[2025-02-13 02:32:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:12][root][INFO] - Training Epoch: 1/2, step 857/7134 completed (loss: 0.03710072115063667, acc: 0.988252580165863)
[2025-02-13 02:32:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:13][root][INFO] - Training Epoch: 1/2, step 858/7134 completed (loss: 0.0962274968624115, acc: 0.9701492786407471)
[2025-02-13 02:32:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:13][root][INFO] - Training Epoch: 1/2, step 859/7134 completed (loss: 0.09964319318532944, acc: 0.9691942930221558)
[2025-02-13 02:32:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:14][root][INFO] - Training Epoch: 1/2, step 860/7134 completed (loss: 0.07631265372037888, acc: 0.9734513163566589)
[2025-02-13 02:32:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:14][root][INFO] - Training Epoch: 1/2, step 861/7134 completed (loss: 0.12624089419841766, acc: 0.9696641564369202)
[2025-02-13 02:32:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:15][root][INFO] - Training Epoch: 1/2, step 862/7134 completed (loss: 0.07293292135000229, acc: 0.9747292399406433)
[2025-02-13 02:32:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:15][root][INFO] - Training Epoch: 1/2, step 863/7134 completed (loss: 0.09399788081645966, acc: 0.9767171144485474)
[2025-02-13 02:32:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:16][root][INFO] - Training Epoch: 1/2, step 864/7134 completed (loss: 0.14737629890441895, acc: 0.9575220942497253)
[2025-02-13 02:32:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:16][root][INFO] - Training Epoch: 1/2, step 865/7134 completed (loss: 0.10267984122037888, acc: 0.970802903175354)
[2025-02-13 02:32:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:17][root][INFO] - Training Epoch: 1/2, step 866/7134 completed (loss: 0.14534148573875427, acc: 0.9585185050964355)
[2025-02-13 02:32:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:17][root][INFO] - Training Epoch: 1/2, step 867/7134 completed (loss: 0.10599001497030258, acc: 0.969962477684021)
[2025-02-13 02:32:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:17][root][INFO] - Training Epoch: 1/2, step 868/7134 completed (loss: 0.13267402350902557, acc: 0.9646596908569336)
[2025-02-13 02:32:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:18][root][INFO] - Training Epoch: 1/2, step 869/7134 completed (loss: 0.10177499800920486, acc: 0.9681528806686401)
[2025-02-13 02:32:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:18][root][INFO] - Training Epoch: 1/2, step 870/7134 completed (loss: 0.056087881326675415, acc: 0.9824798107147217)
[2025-02-13 02:32:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:19][root][INFO] - Training Epoch: 1/2, step 871/7134 completed (loss: 0.10423620790243149, acc: 0.9723126888275146)
[2025-02-13 02:32:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:19][root][INFO] - Training Epoch: 1/2, step 872/7134 completed (loss: 0.05041523650288582, acc: 0.9890109896659851)
[2025-02-13 02:32:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:20][root][INFO] - Training Epoch: 1/2, step 873/7134 completed (loss: 0.0920960009098053, acc: 0.9774647951126099)
[2025-02-13 02:32:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:20][root][INFO] - Training Epoch: 1/2, step 874/7134 completed (loss: 0.08332394063472748, acc: 0.9753466844558716)
[2025-02-13 02:32:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:21][root][INFO] - Training Epoch: 1/2, step 875/7134 completed (loss: 0.07389268279075623, acc: 0.9780380725860596)
[2025-02-13 02:32:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:21][root][INFO] - Training Epoch: 1/2, step 876/7134 completed (loss: 0.0638846904039383, acc: 0.9809941649436951)
[2025-02-13 02:32:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:21][root][INFO] - Training Epoch: 1/2, step 877/7134 completed (loss: 0.12756125628948212, acc: 0.9695364236831665)
[2025-02-13 02:32:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:22][root][INFO] - Training Epoch: 1/2, step 878/7134 completed (loss: 0.1286352127790451, acc: 0.9628646969795227)
[2025-02-13 02:32:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:22][root][INFO] - Training Epoch: 1/2, step 879/7134 completed (loss: 0.09219875931739807, acc: 0.9708878993988037)
[2025-02-13 02:32:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:23][root][INFO] - Training Epoch: 1/2, step 880/7134 completed (loss: 0.025875449180603027, acc: 0.9927219748497009)
[2025-02-13 02:32:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:23][root][INFO] - Training Epoch: 1/2, step 881/7134 completed (loss: 0.04042717441916466, acc: 0.991983950138092)
[2025-02-13 02:32:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:24][root][INFO] - Training Epoch: 1/2, step 882/7134 completed (loss: 0.0669318437576294, acc: 0.9838129281997681)
[2025-02-13 02:32:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:24][root][INFO] - Training Epoch: 1/2, step 883/7134 completed (loss: 0.13066813349723816, acc: 0.961773693561554)
[2025-02-13 02:32:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:25][root][INFO] - Training Epoch: 1/2, step 884/7134 completed (loss: 0.07135698944330215, acc: 0.974530816078186)
[2025-02-13 02:32:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:25][root][INFO] - Training Epoch: 1/2, step 885/7134 completed (loss: 0.01856156811118126, acc: 0.9943820238113403)
[2025-02-13 02:32:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:25][root][INFO] - Training Epoch: 1/2, step 886/7134 completed (loss: 0.11099360883235931, acc: 0.9746588468551636)
[2025-02-13 02:32:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:26][root][INFO] - Training Epoch: 1/2, step 887/7134 completed (loss: 0.06171540915966034, acc: 0.9893805384635925)
[2025-02-13 02:32:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:26][root][INFO] - Training Epoch: 1/2, step 888/7134 completed (loss: 0.08266372233629227, acc: 0.9802731275558472)
[2025-02-13 02:32:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:27][root][INFO] - Training Epoch: 1/2, step 889/7134 completed (loss: 0.04113177955150604, acc: 0.9866666793823242)
[2025-02-13 02:32:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:27][root][INFO] - Training Epoch: 1/2, step 890/7134 completed (loss: 0.07623634487390518, acc: 0.9847198724746704)
[2025-02-13 02:32:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:28][root][INFO] - Training Epoch: 1/2, step 891/7134 completed (loss: 0.08871494233608246, acc: 0.9847009778022766)
[2025-02-13 02:32:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:28][root][INFO] - Training Epoch: 1/2, step 892/7134 completed (loss: 0.05038228631019592, acc: 0.9883720874786377)
[2025-02-13 02:32:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:29][root][INFO] - Training Epoch: 1/2, step 893/7134 completed (loss: 0.045153647661209106, acc: 0.9876161217689514)
[2025-02-13 02:32:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:29][root][INFO] - Training Epoch: 1/2, step 894/7134 completed (loss: 0.062280621379613876, acc: 0.9860334992408752)
[2025-02-13 02:32:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:29][root][INFO] - Training Epoch: 1/2, step 895/7134 completed (loss: 0.039039891213178635, acc: 0.9846153855323792)
[2025-02-13 02:32:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:30][root][INFO] - Training Epoch: 1/2, step 896/7134 completed (loss: 0.0468636192381382, acc: 0.9900000095367432)
[2025-02-13 02:32:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:30][root][INFO] - Training Epoch: 1/2, step 897/7134 completed (loss: 0.03570543974637985, acc: 0.9902507066726685)
[2025-02-13 02:32:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:31][root][INFO] - Training Epoch: 1/2, step 898/7134 completed (loss: 0.031017404049634933, acc: 0.9900373816490173)
[2025-02-13 02:32:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:31][root][INFO] - Training Epoch: 1/2, step 899/7134 completed (loss: 0.03209086135029793, acc: 0.9888059496879578)
[2025-02-13 02:32:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:32][root][INFO] - Training Epoch: 1/2, step 900/7134 completed (loss: 0.045174647122621536, acc: 0.9901546835899353)
[2025-02-13 02:32:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:32][root][INFO] - Training Epoch: 1/2, step 901/7134 completed (loss: 0.056633178144693375, acc: 0.9853333234786987)
[2025-02-13 02:32:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:33][root][INFO] - Training Epoch: 1/2, step 902/7134 completed (loss: 0.031140051782131195, acc: 0.991391658782959)
[2025-02-13 02:32:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:33][root][INFO] - Training Epoch: 1/2, step 903/7134 completed (loss: 0.015963058918714523, acc: 0.998678982257843)
[2025-02-13 02:32:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:34][root][INFO] - Training Epoch: 1/2, step 904/7134 completed (loss: 0.02534799464046955, acc: 0.9921875)
[2025-02-13 02:32:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:34][root][INFO] - Training Epoch: 1/2, step 905/7134 completed (loss: 0.028967484831809998, acc: 0.9878706336021423)
[2025-02-13 02:32:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:35][root][INFO] - Training Epoch: 1/2, step 906/7134 completed (loss: 0.045204464346170425, acc: 0.98525470495224)
[2025-02-13 02:32:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:35][root][INFO] - Training Epoch: 1/2, step 907/7134 completed (loss: 0.039745740592479706, acc: 0.9842105507850647)
[2025-02-13 02:32:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:35][root][INFO] - Training Epoch: 1/2, step 908/7134 completed (loss: 0.036920078098773956, acc: 0.9856957197189331)
[2025-02-13 02:32:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:36][root][INFO] - Training Epoch: 1/2, step 909/7134 completed (loss: 0.05972643569111824, acc: 0.9849849939346313)
[2025-02-13 02:32:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:36][root][INFO] - Training Epoch: 1/2, step 910/7134 completed (loss: 0.06279715895652771, acc: 0.9826435446739197)
[2025-02-13 02:32:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:37][root][INFO] - Training Epoch: 1/2, step 911/7134 completed (loss: 0.05469271168112755, acc: 0.9846153855323792)
[2025-02-13 02:32:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:37][root][INFO] - Training Epoch: 1/2, step 912/7134 completed (loss: 0.06696558743715286, acc: 0.9823455214500427)
[2025-02-13 02:32:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:38][root][INFO] - Training Epoch: 1/2, step 913/7134 completed (loss: 0.035758502781391144, acc: 0.9886934757232666)
[2025-02-13 02:32:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:38][root][INFO] - Training Epoch: 1/2, step 914/7134 completed (loss: 0.03881644830107689, acc: 0.9872449040412903)
[2025-02-13 02:32:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:39][root][INFO] - Training Epoch: 1/2, step 915/7134 completed (loss: 0.09238975495100021, acc: 0.9724919199943542)
[2025-02-13 02:32:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:39][root][INFO] - Training Epoch: 1/2, step 916/7134 completed (loss: 0.03272824361920357, acc: 0.9860383868217468)
[2025-02-13 02:32:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:40][root][INFO] - Training Epoch: 1/2, step 917/7134 completed (loss: 0.08128384500741959, acc: 0.9788838624954224)
[2025-02-13 02:32:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:40][root][INFO] - Training Epoch: 1/2, step 918/7134 completed (loss: 0.021497635170817375, acc: 0.992343008518219)
[2025-02-13 02:32:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:40][root][INFO] - Training Epoch: 1/2, step 919/7134 completed (loss: 0.03833534196019173, acc: 0.9855700135231018)
[2025-02-13 02:32:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:41][root][INFO] - Training Epoch: 1/2, step 920/7134 completed (loss: 0.05722435563802719, acc: 0.9775910377502441)
[2025-02-13 02:32:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:41][root][INFO] - Training Epoch: 1/2, step 921/7134 completed (loss: 0.05044076591730118, acc: 0.9850948452949524)
[2025-02-13 02:32:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:42][root][INFO] - Training Epoch: 1/2, step 922/7134 completed (loss: 0.06826534867286682, acc: 0.9737704992294312)
[2025-02-13 02:32:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:42][root][INFO] - Training Epoch: 1/2, step 923/7134 completed (loss: 0.06677347421646118, acc: 0.976190447807312)
[2025-02-13 02:32:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:43][root][INFO] - Training Epoch: 1/2, step 924/7134 completed (loss: 0.03230839595198631, acc: 0.9927536249160767)
[2025-02-13 02:32:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:43][root][INFO] - Training Epoch: 1/2, step 925/7134 completed (loss: 0.059812817722558975, acc: 0.988252580165863)
[2025-02-13 02:32:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:44][root][INFO] - Training Epoch: 1/2, step 926/7134 completed (loss: 0.058647409081459045, acc: 0.9850187301635742)
[2025-02-13 02:32:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:44][root][INFO] - Training Epoch: 1/2, step 927/7134 completed (loss: 0.06075679510831833, acc: 0.9901960492134094)
[2025-02-13 02:32:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:45][root][INFO] - Training Epoch: 1/2, step 928/7134 completed (loss: 0.08512275665998459, acc: 0.9822485446929932)
[2025-02-13 02:32:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:45][root][INFO] - Training Epoch: 1/2, step 929/7134 completed (loss: 0.02893557772040367, acc: 0.9952606558799744)
[2025-02-13 02:32:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:45][root][INFO] - Training Epoch: 1/2, step 930/7134 completed (loss: 0.05326466262340546, acc: 0.9818548560142517)
[2025-02-13 02:32:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:46][root][INFO] - Training Epoch: 1/2, step 931/7134 completed (loss: 0.06670744717121124, acc: 0.9805996417999268)
[2025-02-13 02:32:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:46][root][INFO] - Training Epoch: 1/2, step 932/7134 completed (loss: 0.04093169793486595, acc: 0.9871060252189636)
[2025-02-13 02:32:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:47][root][INFO] - Training Epoch: 1/2, step 933/7134 completed (loss: 0.057827435433864594, acc: 0.9802538752555847)
[2025-02-13 02:32:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:47][root][INFO] - Training Epoch: 1/2, step 934/7134 completed (loss: 0.03439616039395332, acc: 0.9921466112136841)
[2025-02-13 02:32:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:48][root][INFO] - Training Epoch: 1/2, step 935/7134 completed (loss: 0.054155975580215454, acc: 0.9849749803543091)
[2025-02-13 02:32:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:48][root][INFO] - Training Epoch: 1/2, step 936/7134 completed (loss: 0.055995337665081024, acc: 0.9876977205276489)
[2025-02-13 02:32:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:49][root][INFO] - Training Epoch: 1/2, step 937/7134 completed (loss: 0.051841747015714645, acc: 0.9876760840415955)
[2025-02-13 02:32:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:49][root][INFO] - Training Epoch: 1/2, step 938/7134 completed (loss: 0.05309108644723892, acc: 0.9885386824607849)
[2025-02-13 02:32:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:50][root][INFO] - Training Epoch: 1/2, step 939/7134 completed (loss: 0.07907061278820038, acc: 0.9828660488128662)
[2025-02-13 02:32:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:50][root][INFO] - Training Epoch: 1/2, step 940/7134 completed (loss: 0.04686696082353592, acc: 0.9814019799232483)
[2025-02-13 02:32:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:51][root][INFO] - Training Epoch: 1/2, step 941/7134 completed (loss: 0.04593214392662048, acc: 0.9848275780677795)
[2025-02-13 02:32:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:51][root][INFO] - Training Epoch: 1/2, step 942/7134 completed (loss: 0.053633563220500946, acc: 0.9869375824928284)
[2025-02-13 02:32:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:51][root][INFO] - Training Epoch: 1/2, step 943/7134 completed (loss: 0.03444384038448334, acc: 0.9847792983055115)
[2025-02-13 02:32:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:52][root][INFO] - Training Epoch: 1/2, step 944/7134 completed (loss: 0.04117836058139801, acc: 0.9880059957504272)
[2025-02-13 02:32:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:52][root][INFO] - Training Epoch: 1/2, step 945/7134 completed (loss: 0.06710950285196304, acc: 0.9846153855323792)
[2025-02-13 02:32:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:53][root][INFO] - Training Epoch: 1/2, step 946/7134 completed (loss: 0.02375963144004345, acc: 0.9935759902000427)
[2025-02-13 02:32:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:53][root][INFO] - Training Epoch: 1/2, step 947/7134 completed (loss: 0.04444645717740059, acc: 0.9913941621780396)
[2025-02-13 02:32:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:54][root][INFO] - Training Epoch: 1/2, step 948/7134 completed (loss: 0.0599249005317688, acc: 0.9819193482398987)
[2025-02-13 02:32:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:54][root][INFO] - Training Epoch: 1/2, step 949/7134 completed (loss: 0.04500524327158928, acc: 0.9901823401451111)
[2025-02-13 02:32:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:55][root][INFO] - Training Epoch: 1/2, step 950/7134 completed (loss: 0.04349692910909653, acc: 0.9861111044883728)
[2025-02-13 02:32:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:55][root][INFO] - Training Epoch: 1/2, step 951/7134 completed (loss: 0.061892516911029816, acc: 0.9841549396514893)
[2025-02-13 02:32:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:56][root][INFO] - Training Epoch: 1/2, step 952/7134 completed (loss: 0.07927744090557098, acc: 0.9767054915428162)
[2025-02-13 02:32:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:56][root][INFO] - Training Epoch: 1/2, step 953/7134 completed (loss: 0.08984769880771637, acc: 0.9742331504821777)
[2025-02-13 02:32:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:57][root][INFO] - Training Epoch: 1/2, step 954/7134 completed (loss: 0.06742367148399353, acc: 0.9823788404464722)
[2025-02-13 02:32:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:57][root][INFO] - Training Epoch: 1/2, step 955/7134 completed (loss: 0.0642620399594307, acc: 0.9799196720123291)
[2025-02-13 02:32:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:57][root][INFO] - Training Epoch: 1/2, step 956/7134 completed (loss: 0.056703317910432816, acc: 0.9829268455505371)
[2025-02-13 02:32:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:58][root][INFO] - Training Epoch: 1/2, step 957/7134 completed (loss: 0.04231764376163483, acc: 0.9908987283706665)
[2025-02-13 02:32:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:58][root][INFO] - Training Epoch: 1/2, step 958/7134 completed (loss: 0.0996820479631424, acc: 0.9742646813392639)
[2025-02-13 02:32:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:59][root][INFO] - Training Epoch: 1/2, step 959/7134 completed (loss: 0.16867512464523315, acc: 0.9598930478096008)
[2025-02-13 02:32:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:32:59][root][INFO] - Training Epoch: 1/2, step 960/7134 completed (loss: 0.11308721452951431, acc: 0.964083194732666)
[2025-02-13 02:32:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:00][root][INFO] - Training Epoch: 1/2, step 961/7134 completed (loss: 0.09417007118463516, acc: 0.9757412672042847)
[2025-02-13 02:33:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:00][root][INFO] - Training Epoch: 1/2, step 962/7134 completed (loss: 0.23843418061733246, acc: 0.951724112033844)
[2025-02-13 02:33:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:01][root][INFO] - Training Epoch: 1/2, step 963/7134 completed (loss: 0.10999938100576401, acc: 0.9618902206420898)
[2025-02-13 02:33:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:01][root][INFO] - Training Epoch: 1/2, step 964/7134 completed (loss: 0.163467139005661, acc: 0.952023983001709)
[2025-02-13 02:33:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:01][root][INFO] - Training Epoch: 1/2, step 965/7134 completed (loss: 0.11948744207620621, acc: 0.9641320109367371)
[2025-02-13 02:33:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:02][root][INFO] - Training Epoch: 1/2, step 966/7134 completed (loss: 0.09955314546823502, acc: 0.9688524603843689)
[2025-02-13 02:33:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:02][root][INFO] - Training Epoch: 1/2, step 967/7134 completed (loss: 0.10071947425603867, acc: 0.9727148413658142)
[2025-02-13 02:33:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:03][root][INFO] - Training Epoch: 1/2, step 968/7134 completed (loss: 0.10756248235702515, acc: 0.9662027955055237)
[2025-02-13 02:33:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:03][root][INFO] - Training Epoch: 1/2, step 969/7134 completed (loss: 0.08101112395524979, acc: 0.9758672714233398)
[2025-02-13 02:33:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:04][root][INFO] - Training Epoch: 1/2, step 970/7134 completed (loss: 0.11720915138721466, acc: 0.9656488299369812)
[2025-02-13 02:33:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:04][root][INFO] - Training Epoch: 1/2, step 971/7134 completed (loss: 0.07994049787521362, acc: 0.9742331504821777)
[2025-02-13 02:33:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:05][root][INFO] - Training Epoch: 1/2, step 972/7134 completed (loss: 0.09250450134277344, acc: 0.9713574051856995)
[2025-02-13 02:33:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:05][root][INFO] - Training Epoch: 1/2, step 973/7134 completed (loss: 0.14133964478969574, acc: 0.9615384340286255)
[2025-02-13 02:33:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:06][root][INFO] - Training Epoch: 1/2, step 974/7134 completed (loss: 0.0883556678891182, acc: 0.972633957862854)
[2025-02-13 02:33:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:06][root][INFO] - Training Epoch: 1/2, step 975/7134 completed (loss: 0.10740776360034943, acc: 0.9748803973197937)
[2025-02-13 02:33:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:06][root][INFO] - Training Epoch: 1/2, step 976/7134 completed (loss: 0.10444618761539459, acc: 0.9657320976257324)
[2025-02-13 02:33:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:07][root][INFO] - Training Epoch: 1/2, step 977/7134 completed (loss: 0.11430034041404724, acc: 0.9626623392105103)
[2025-02-13 02:33:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:07][root][INFO] - Training Epoch: 1/2, step 978/7134 completed (loss: 0.05615118518471718, acc: 0.9843562245368958)
[2025-02-13 02:33:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:08][root][INFO] - Training Epoch: 1/2, step 979/7134 completed (loss: 0.05609455704689026, acc: 0.9852941036224365)
[2025-02-13 02:33:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:08][root][INFO] - Training Epoch: 1/2, step 980/7134 completed (loss: 0.09129804372787476, acc: 0.9737499952316284)
[2025-02-13 02:33:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:09][root][INFO] - Training Epoch: 1/2, step 981/7134 completed (loss: 0.12701423466205597, acc: 0.9758842587471008)
[2025-02-13 02:33:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:09][root][INFO] - Training Epoch: 1/2, step 982/7134 completed (loss: 0.06200423836708069, acc: 0.9887640476226807)
[2025-02-13 02:33:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:10][root][INFO] - Training Epoch: 1/2, step 983/7134 completed (loss: 0.08205816894769669, acc: 0.9743150472640991)
[2025-02-13 02:33:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:10][root][INFO] - Training Epoch: 1/2, step 984/7134 completed (loss: 0.07790874689817429, acc: 0.9834558963775635)
[2025-02-13 02:33:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:11][root][INFO] - Training Epoch: 1/2, step 985/7134 completed (loss: 0.06601177901029587, acc: 0.9783163070678711)
[2025-02-13 02:33:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:11][root][INFO] - Training Epoch: 1/2, step 986/7134 completed (loss: 0.060975849628448486, acc: 0.977337121963501)
[2025-02-13 02:33:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:11][root][INFO] - Training Epoch: 1/2, step 987/7134 completed (loss: 0.06721744686365128, acc: 0.9742765426635742)
[2025-02-13 02:33:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:12][root][INFO] - Training Epoch: 1/2, step 988/7134 completed (loss: 0.13848565518856049, acc: 0.96809983253479)
[2025-02-13 02:33:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:12][root][INFO] - Training Epoch: 1/2, step 989/7134 completed (loss: 0.08106472343206406, acc: 0.9805285334587097)
[2025-02-13 02:33:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:13][root][INFO] - Training Epoch: 1/2, step 990/7134 completed (loss: 0.07501989603042603, acc: 0.974715530872345)
[2025-02-13 02:33:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:13][root][INFO] - Training Epoch: 1/2, step 991/7134 completed (loss: 0.10853941738605499, acc: 0.9749608635902405)
[2025-02-13 02:33:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:14][root][INFO] - Training Epoch: 1/2, step 992/7134 completed (loss: 0.08959050476551056, acc: 0.9697368144989014)
[2025-02-13 02:33:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:14][root][INFO] - Training Epoch: 1/2, step 993/7134 completed (loss: 0.06785108894109726, acc: 0.9819967150688171)
[2025-02-13 02:33:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:15][root][INFO] - Training Epoch: 1/2, step 994/7134 completed (loss: 0.05762862414121628, acc: 0.9853603839874268)
[2025-02-13 02:33:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:15][root][INFO] - Training Epoch: 1/2, step 995/7134 completed (loss: 0.08633721619844437, acc: 0.9783950448036194)
[2025-02-13 02:33:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:16][root][INFO] - Training Epoch: 1/2, step 996/7134 completed (loss: 0.07676273584365845, acc: 0.9776536226272583)
[2025-02-13 02:33:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:16][root][INFO] - Training Epoch: 1/2, step 997/7134 completed (loss: 0.04387647286057472, acc: 0.9871060252189636)
[2025-02-13 02:33:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:17][root][INFO] - Training Epoch: 1/2, step 998/7134 completed (loss: 0.07613298296928406, acc: 0.9760563373565674)
[2025-02-13 02:33:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:17][root][INFO] - Training Epoch: 1/2, step 999/7134 completed (loss: 0.03803122416138649, acc: 0.9874551892280579)
[2025-02-13 02:33:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:17][root][INFO] - Training Epoch: 1/2, step 1000/7134 completed (loss: 0.054440826177597046, acc: 0.9863013625144958)
[2025-02-13 02:33:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:18][root][INFO] - Training Epoch: 1/2, step 1001/7134 completed (loss: 0.050169687718153, acc: 0.9878787994384766)
[2025-02-13 02:33:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:18][root][INFO] - Training Epoch: 1/2, step 1002/7134 completed (loss: 0.06912628561258316, acc: 0.980028510093689)
[2025-02-13 02:33:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:19][root][INFO] - Training Epoch: 1/2, step 1003/7134 completed (loss: 0.0354362428188324, acc: 0.9925705790519714)
[2025-02-13 02:33:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:19][root][INFO] - Training Epoch: 1/2, step 1004/7134 completed (loss: 0.09382657706737518, acc: 0.9731958508491516)
[2025-02-13 02:33:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:20][root][INFO] - Training Epoch: 1/2, step 1005/7134 completed (loss: 0.10512536764144897, acc: 0.9768211841583252)
[2025-02-13 02:33:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:20][root][INFO] - Training Epoch: 1/2, step 1006/7134 completed (loss: 0.0739433616399765, acc: 0.9777158498764038)
[2025-02-13 02:33:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:20][root][INFO] - Training Epoch: 1/2, step 1007/7134 completed (loss: 0.09925957024097443, acc: 0.9780219793319702)
[2025-02-13 02:33:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:21][root][INFO] - Training Epoch: 1/2, step 1008/7134 completed (loss: 0.07525473833084106, acc: 0.980141818523407)
[2025-02-13 02:33:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:21][root][INFO] - Training Epoch: 1/2, step 1009/7134 completed (loss: 0.08529721945524216, acc: 0.9893428087234497)
[2025-02-13 02:33:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:22][root][INFO] - Training Epoch: 1/2, step 1010/7134 completed (loss: 0.077329620718956, acc: 0.9743083119392395)
[2025-02-13 02:33:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:22][root][INFO] - Training Epoch: 1/2, step 1011/7134 completed (loss: 0.07721558213233948, acc: 0.9806362390518188)
[2025-02-13 02:33:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:23][root][INFO] - Training Epoch: 1/2, step 1012/7134 completed (loss: 0.07427433878183365, acc: 0.980169951915741)
[2025-02-13 02:33:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:23][root][INFO] - Training Epoch: 1/2, step 1013/7134 completed (loss: 0.08373989164829254, acc: 0.9742120504379272)
[2025-02-13 02:33:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:24][root][INFO] - Training Epoch: 1/2, step 1014/7134 completed (loss: 0.059003349393606186, acc: 0.9786535501480103)
[2025-02-13 02:33:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:24][root][INFO] - Training Epoch: 1/2, step 1015/7134 completed (loss: 0.06712104380130768, acc: 0.9819276928901672)
[2025-02-13 02:33:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:24][root][INFO] - Training Epoch: 1/2, step 1016/7134 completed (loss: 0.076911062002182, acc: 0.9770554304122925)
[2025-02-13 02:33:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:25][root][INFO] - Training Epoch: 1/2, step 1017/7134 completed (loss: 0.02616775594651699, acc: 0.9932885766029358)
[2025-02-13 02:33:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:25][root][INFO] - Training Epoch: 1/2, step 1018/7134 completed (loss: 0.07735635340213776, acc: 0.9897511005401611)
[2025-02-13 02:33:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:26][root][INFO] - Training Epoch: 1/2, step 1019/7134 completed (loss: 0.051728811115026474, acc: 0.9871794581413269)
[2025-02-13 02:33:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:26][root][INFO] - Training Epoch: 1/2, step 1020/7134 completed (loss: 0.04656316712498665, acc: 0.9895522594451904)
[2025-02-13 02:33:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:27][root][INFO] - Training Epoch: 1/2, step 1021/7134 completed (loss: 0.041710399091243744, acc: 0.9840764403343201)
[2025-02-13 02:33:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:27][root][INFO] - Training Epoch: 1/2, step 1022/7134 completed (loss: 0.035753123462200165, acc: 0.990777313709259)
[2025-02-13 02:33:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:28][root][INFO] - Training Epoch: 1/2, step 1023/7134 completed (loss: 0.051209792494773865, acc: 0.992277979850769)
[2025-02-13 02:33:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:28][root][INFO] - Training Epoch: 1/2, step 1024/7134 completed (loss: 0.05259070545434952, acc: 0.985637366771698)
[2025-02-13 02:33:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:28][root][INFO] - Training Epoch: 1/2, step 1025/7134 completed (loss: 0.03859364986419678, acc: 0.9893238544464111)
[2025-02-13 02:33:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:29][root][INFO] - Training Epoch: 1/2, step 1026/7134 completed (loss: 0.06467866152524948, acc: 0.9810526371002197)
[2025-02-13 02:33:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:29][root][INFO] - Training Epoch: 1/2, step 1027/7134 completed (loss: 0.021352333948016167, acc: 0.9918808937072754)
[2025-02-13 02:33:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:30][root][INFO] - Training Epoch: 1/2, step 1028/7134 completed (loss: 0.025981144979596138, acc: 0.9925261735916138)
[2025-02-13 02:33:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:30][root][INFO] - Training Epoch: 1/2, step 1029/7134 completed (loss: 0.061000172048807144, acc: 0.9806835055351257)
[2025-02-13 02:33:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:31][root][INFO] - Training Epoch: 1/2, step 1030/7134 completed (loss: 0.016215043142437935, acc: 0.9959568977355957)
[2025-02-13 02:33:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:31][root][INFO] - Training Epoch: 1/2, step 1031/7134 completed (loss: 0.07291034609079361, acc: 0.9847561120986938)
[2025-02-13 02:33:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:32][root][INFO] - Training Epoch: 1/2, step 1032/7134 completed (loss: 0.06897315382957458, acc: 0.9827188849449158)
[2025-02-13 02:33:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:32][root][INFO] - Training Epoch: 1/2, step 1033/7134 completed (loss: 0.053298287093639374, acc: 0.9827833771705627)
[2025-02-13 02:33:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:32][root][INFO] - Training Epoch: 1/2, step 1034/7134 completed (loss: 0.07713117450475693, acc: 0.9805510640144348)
[2025-02-13 02:33:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:33][root][INFO] - Training Epoch: 1/2, step 1035/7134 completed (loss: 0.32998648285865784, acc: 0.9473684430122375)
[2025-02-13 02:33:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:33][root][INFO] - Training Epoch: 1/2, step 1036/7134 completed (loss: 0.3474116623401642, acc: 0.948888897895813)
[2025-02-13 02:33:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:34][root][INFO] - Training Epoch: 1/2, step 1037/7134 completed (loss: 0.03215000405907631, acc: 0.9892857074737549)
[2025-02-13 02:33:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:34][root][INFO] - Training Epoch: 1/2, step 1038/7134 completed (loss: 0.0618831068277359, acc: 0.9765100479125977)
[2025-02-13 02:33:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:35][root][INFO] - Training Epoch: 1/2, step 1039/7134 completed (loss: 0.03582156449556351, acc: 0.9855072498321533)
[2025-02-13 02:33:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:35][root][INFO] - Training Epoch: 1/2, step 1040/7134 completed (loss: 0.03254624828696251, acc: 0.9897260069847107)
[2025-02-13 02:33:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:35][root][INFO] - Training Epoch: 1/2, step 1041/7134 completed (loss: 0.054982587695121765, acc: 0.9875389337539673)
[2025-02-13 02:33:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:36][root][INFO] - Training Epoch: 1/2, step 1042/7134 completed (loss: 0.053512491285800934, acc: 0.9846153855323792)
[2025-02-13 02:33:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:36][root][INFO] - Training Epoch: 1/2, step 1043/7134 completed (loss: 0.05269093066453934, acc: 0.9855595827102661)
[2025-02-13 02:33:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:37][root][INFO] - Training Epoch: 1/2, step 1044/7134 completed (loss: 0.07059077173471451, acc: 0.9849315285682678)
[2025-02-13 02:33:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:37][root][INFO] - Training Epoch: 1/2, step 1045/7134 completed (loss: 0.11466855555772781, acc: 0.9709208607673645)
[2025-02-13 02:33:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:38][root][INFO] - Training Epoch: 1/2, step 1046/7134 completed (loss: 0.051875125616788864, acc: 0.9864864945411682)
[2025-02-13 02:33:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:38][root][INFO] - Training Epoch: 1/2, step 1047/7134 completed (loss: 0.0980389192700386, acc: 0.9807407259941101)
[2025-02-13 02:33:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:39][root][INFO] - Training Epoch: 1/2, step 1048/7134 completed (loss: 0.04315108805894852, acc: 0.9912023544311523)
[2025-02-13 02:33:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:39][root][INFO] - Training Epoch: 1/2, step 1049/7134 completed (loss: 0.0823902040719986, acc: 0.9795022010803223)
[2025-02-13 02:33:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:40][root][INFO] - Training Epoch: 1/2, step 1050/7134 completed (loss: 0.07420583069324493, acc: 0.9781420826911926)
[2025-02-13 02:33:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:40][root][INFO] - Training Epoch: 1/2, step 1051/7134 completed (loss: 0.05752174183726311, acc: 0.9808219075202942)
[2025-02-13 02:33:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:41][root][INFO] - Training Epoch: 1/2, step 1052/7134 completed (loss: 0.1021285429596901, acc: 0.9757673740386963)
[2025-02-13 02:33:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:41][root][INFO] - Training Epoch: 1/2, step 1053/7134 completed (loss: 0.03975706920027733, acc: 0.9873239398002625)
[2025-02-13 02:33:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:42][root][INFO] - Training Epoch: 1/2, step 1054/7134 completed (loss: 0.05704125389456749, acc: 0.9894039630889893)
[2025-02-13 02:33:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:42][root][INFO] - Training Epoch: 1/2, step 1055/7134 completed (loss: 0.09588522464036942, acc: 0.9711751937866211)
[2025-02-13 02:33:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:42][root][INFO] - Training Epoch: 1/2, step 1056/7134 completed (loss: 0.11985175311565399, acc: 0.9631490707397461)
[2025-02-13 02:33:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:43][root][INFO] - Training Epoch: 1/2, step 1057/7134 completed (loss: 0.07710884511470795, acc: 0.97398841381073)
[2025-02-13 02:33:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:43][root][INFO] - Training Epoch: 1/2, step 1058/7134 completed (loss: 0.06229962781071663, acc: 0.9797101616859436)
[2025-02-13 02:33:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:44][root][INFO] - Training Epoch: 1/2, step 1059/7134 completed (loss: 0.06944356858730316, acc: 0.9819355010986328)
[2025-02-13 02:33:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:44][root][INFO] - Training Epoch: 1/2, step 1060/7134 completed (loss: 0.07265282422304153, acc: 0.9763407111167908)
[2025-02-13 02:33:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:45][root][INFO] - Training Epoch: 1/2, step 1061/7134 completed (loss: 0.06716565042734146, acc: 0.9767441749572754)
[2025-02-13 02:33:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:45][root][INFO] - Training Epoch: 1/2, step 1062/7134 completed (loss: 0.06998930126428604, acc: 0.9779661297798157)
[2025-02-13 02:33:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:46][root][INFO] - Training Epoch: 1/2, step 1063/7134 completed (loss: 0.030936552211642265, acc: 0.9918962717056274)
[2025-02-13 02:33:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:46][root][INFO] - Training Epoch: 1/2, step 1064/7134 completed (loss: 0.056068792939186096, acc: 0.9814814925193787)
[2025-02-13 02:33:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:46][root][INFO] - Training Epoch: 1/2, step 1065/7134 completed (loss: 0.037498246878385544, acc: 0.9878048896789551)
[2025-02-13 02:33:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:47][root][INFO] - Training Epoch: 1/2, step 1066/7134 completed (loss: 0.07945630699396133, acc: 0.9815436005592346)
[2025-02-13 02:33:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:47][root][INFO] - Training Epoch: 1/2, step 1067/7134 completed (loss: 0.03680751845240593, acc: 0.9876733422279358)
[2025-02-13 02:33:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:48][root][INFO] - Training Epoch: 1/2, step 1068/7134 completed (loss: 0.05717523396015167, acc: 0.9831144213676453)
[2025-02-13 02:33:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:48][root][INFO] - Training Epoch: 1/2, step 1069/7134 completed (loss: 0.05416717380285263, acc: 0.9849749803543091)
[2025-02-13 02:33:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:49][root][INFO] - Training Epoch: 1/2, step 1070/7134 completed (loss: 0.045651428401470184, acc: 0.9868637323379517)
[2025-02-13 02:33:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:49][root][INFO] - Training Epoch: 1/2, step 1071/7134 completed (loss: 0.024670502170920372, acc: 0.9919678568840027)
[2025-02-13 02:33:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:50][root][INFO] - Training Epoch: 1/2, step 1072/7134 completed (loss: 0.008007087744772434, acc: 0.9959404468536377)
[2025-02-13 02:33:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:50][root][INFO] - Training Epoch: 1/2, step 1073/7134 completed (loss: 0.020884422585368156, acc: 0.9927954077720642)
[2025-02-13 02:33:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:50][root][INFO] - Training Epoch: 1/2, step 1074/7134 completed (loss: 0.10438019037246704, acc: 0.9740932583808899)
[2025-02-13 02:33:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:51][root][INFO] - Training Epoch: 1/2, step 1075/7134 completed (loss: 0.15949887037277222, acc: 0.9613259434700012)
[2025-02-13 02:33:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:51][root][INFO] - Training Epoch: 1/2, step 1076/7134 completed (loss: 0.11630759388208389, acc: 0.9654255509376526)
[2025-02-13 02:33:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:52][root][INFO] - Training Epoch: 1/2, step 1077/7134 completed (loss: 0.16311310231685638, acc: 0.9684684872627258)
[2025-02-13 02:33:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:52][root][INFO] - Training Epoch: 1/2, step 1078/7134 completed (loss: 0.14393490552902222, acc: 0.9644588232040405)
[2025-02-13 02:33:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:53][root][INFO] - Training Epoch: 1/2, step 1079/7134 completed (loss: 0.05847383290529251, acc: 0.979238748550415)
[2025-02-13 02:33:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:53][root][INFO] - Training Epoch: 1/2, step 1080/7134 completed (loss: 0.0504644550383091, acc: 0.9868420958518982)
[2025-02-13 02:33:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:53][root][INFO] - Training Epoch: 1/2, step 1081/7134 completed (loss: 0.03226785734295845, acc: 0.9887797832489014)
[2025-02-13 02:33:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:54][root][INFO] - Training Epoch: 1/2, step 1082/7134 completed (loss: 0.029278315603733063, acc: 0.9928057789802551)
[2025-02-13 02:33:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:54][root][INFO] - Training Epoch: 1/2, step 1083/7134 completed (loss: 0.04634528234601021, acc: 0.9819208979606628)
[2025-02-13 02:33:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:55][root][INFO] - Training Epoch: 1/2, step 1084/7134 completed (loss: 0.04099980369210243, acc: 0.986369252204895)
[2025-02-13 02:33:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:55][root][INFO] - Training Epoch: 1/2, step 1085/7134 completed (loss: 0.03684382885694504, acc: 0.9919678568840027)
[2025-02-13 02:33:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:56][root][INFO] - Training Epoch: 1/2, step 1086/7134 completed (loss: 0.11101172864437103, acc: 0.967863917350769)
[2025-02-13 02:33:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:56][root][INFO] - Training Epoch: 1/2, step 1087/7134 completed (loss: 0.060867197811603546, acc: 0.9792683124542236)
[2025-02-13 02:33:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:57][root][INFO] - Training Epoch: 1/2, step 1088/7134 completed (loss: 0.049385618418455124, acc: 0.9874301552772522)
[2025-02-13 02:33:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:57][root][INFO] - Training Epoch: 1/2, step 1089/7134 completed (loss: 0.0569668747484684, acc: 0.9821162223815918)
[2025-02-13 02:33:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:58][root][INFO] - Training Epoch: 1/2, step 1090/7134 completed (loss: 0.05226549506187439, acc: 0.9828392863273621)
[2025-02-13 02:33:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:58][root][INFO] - Training Epoch: 1/2, step 1091/7134 completed (loss: 0.05602066591382027, acc: 0.9832572340965271)
[2025-02-13 02:33:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:58][root][INFO] - Training Epoch: 1/2, step 1092/7134 completed (loss: 0.03669098764657974, acc: 0.9913294911384583)
[2025-02-13 02:33:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:59][root][INFO] - Training Epoch: 1/2, step 1093/7134 completed (loss: 0.11380154639482498, acc: 0.9702194333076477)
[2025-02-13 02:33:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:33:59][root][INFO] - Training Epoch: 1/2, step 1094/7134 completed (loss: 0.10375265032052994, acc: 0.9665427803993225)
[2025-02-13 02:33:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:00][root][INFO] - Training Epoch: 1/2, step 1095/7134 completed (loss: 0.05506718531250954, acc: 0.9817296266555786)
[2025-02-13 02:34:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:00][root][INFO] - Training Epoch: 1/2, step 1096/7134 completed (loss: 0.05224150791764259, acc: 0.9886075854301453)
[2025-02-13 02:34:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:01][root][INFO] - Training Epoch: 1/2, step 1097/7134 completed (loss: 0.10581963509321213, acc: 0.966292142868042)
[2025-02-13 02:34:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:01][root][INFO] - Training Epoch: 1/2, step 1098/7134 completed (loss: 0.056473929435014725, acc: 0.9851668477058411)
[2025-02-13 02:34:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:02][root][INFO] - Training Epoch: 1/2, step 1099/7134 completed (loss: 0.05708661302924156, acc: 0.9820512533187866)
[2025-02-13 02:34:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:02][root][INFO] - Training Epoch: 1/2, step 1100/7134 completed (loss: 0.047575853765010834, acc: 0.989130437374115)
[2025-02-13 02:34:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:03][root][INFO] - Training Epoch: 1/2, step 1101/7134 completed (loss: 0.048559464514255524, acc: 0.9896103739738464)
[2025-02-13 02:34:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:03][root][INFO] - Training Epoch: 1/2, step 1102/7134 completed (loss: 0.0649610310792923, acc: 0.9823874831199646)
[2025-02-13 02:34:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:04][root][INFO] - Training Epoch: 1/2, step 1103/7134 completed (loss: 0.10357186198234558, acc: 0.9746666550636292)
[2025-02-13 02:34:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:04][root][INFO] - Training Epoch: 1/2, step 1104/7134 completed (loss: 0.05567067116498947, acc: 0.9841498732566833)
[2025-02-13 02:34:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:04][root][INFO] - Training Epoch: 1/2, step 1105/7134 completed (loss: 0.06144557148218155, acc: 0.9811320900917053)
[2025-02-13 02:34:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:05][root][INFO] - Training Epoch: 1/2, step 1106/7134 completed (loss: 0.07867410778999329, acc: 0.9752066135406494)
[2025-02-13 02:34:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:05][root][INFO] - Training Epoch: 1/2, step 1107/7134 completed (loss: 0.044066984206438065, acc: 0.9835873246192932)
[2025-02-13 02:34:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:06][root][INFO] - Training Epoch: 1/2, step 1108/7134 completed (loss: 0.035383760929107666, acc: 0.9922680258750916)
[2025-02-13 02:34:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:06][root][INFO] - Training Epoch: 1/2, step 1109/7134 completed (loss: 0.07107088714838028, acc: 0.9762569665908813)
[2025-02-13 02:34:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:07][root][INFO] - Training Epoch: 1/2, step 1110/7134 completed (loss: 0.0697004422545433, acc: 0.9790301322937012)
[2025-02-13 02:34:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:07][root][INFO] - Training Epoch: 1/2, step 1111/7134 completed (loss: 0.043081820011138916, acc: 0.9903314709663391)
[2025-02-13 02:34:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:08][root][INFO] - Training Epoch: 1/2, step 1112/7134 completed (loss: 0.07270391285419464, acc: 0.979411780834198)
[2025-02-13 02:34:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:08][root][INFO] - Training Epoch: 1/2, step 1113/7134 completed (loss: 0.029155300930142403, acc: 0.9858430027961731)
[2025-02-13 02:34:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:09][root][INFO] - Training Epoch: 1/2, step 1114/7134 completed (loss: 0.053529173135757446, acc: 0.9829890727996826)
[2025-02-13 02:34:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:09][root][INFO] - Training Epoch: 1/2, step 1115/7134 completed (loss: 0.06236583739519119, acc: 0.9795597195625305)
[2025-02-13 02:34:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:09][root][INFO] - Training Epoch: 1/2, step 1116/7134 completed (loss: 0.04072551429271698, acc: 0.9925373196601868)
[2025-02-13 02:34:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:10][root][INFO] - Training Epoch: 1/2, step 1117/7134 completed (loss: 0.03862328827381134, acc: 0.987730085849762)
[2025-02-13 02:34:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:10][root][INFO] - Training Epoch: 1/2, step 1118/7134 completed (loss: 0.020136041566729546, acc: 0.9931034445762634)
[2025-02-13 02:34:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:11][root][INFO] - Training Epoch: 1/2, step 1119/7134 completed (loss: 0.05594296380877495, acc: 0.9778831005096436)
[2025-02-13 02:34:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:11][root][INFO] - Training Epoch: 1/2, step 1120/7134 completed (loss: 0.0693594217300415, acc: 0.9795597195625305)
[2025-02-13 02:34:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:12][root][INFO] - Training Epoch: 1/2, step 1121/7134 completed (loss: 0.053408484905958176, acc: 0.9837518334388733)
[2025-02-13 02:34:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:12][root][INFO] - Training Epoch: 1/2, step 1122/7134 completed (loss: 0.03755493834614754, acc: 0.9864077568054199)
[2025-02-13 02:34:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:12][root][INFO] - Training Epoch: 1/2, step 1123/7134 completed (loss: 0.06838362663984299, acc: 0.9807383418083191)
[2025-02-13 02:34:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:13][root][INFO] - Training Epoch: 1/2, step 1124/7134 completed (loss: 0.05648280680179596, acc: 0.9845201373100281)
[2025-02-13 02:34:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:13][root][INFO] - Training Epoch: 1/2, step 1125/7134 completed (loss: 0.02951873652637005, acc: 0.9864681959152222)
[2025-02-13 02:34:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:14][root][INFO] - Training Epoch: 1/2, step 1126/7134 completed (loss: 0.050412941724061966, acc: 0.9848066568374634)
[2025-02-13 02:34:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:14][root][INFO] - Training Epoch: 1/2, step 1127/7134 completed (loss: 0.03172759339213371, acc: 0.9968000054359436)
[2025-02-13 02:34:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:15][root][INFO] - Training Epoch: 1/2, step 1128/7134 completed (loss: 0.04186595231294632, acc: 0.9861111044883728)
[2025-02-13 02:34:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:15][root][INFO] - Training Epoch: 1/2, step 1129/7134 completed (loss: 0.05599774047732353, acc: 0.9843049049377441)
[2025-02-13 02:34:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:15][root][INFO] - Training Epoch: 1/2, step 1130/7134 completed (loss: 0.04516620561480522, acc: 0.9919224381446838)
[2025-02-13 02:34:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:16][root][INFO] - Training Epoch: 1/2, step 1131/7134 completed (loss: 0.0679151862859726, acc: 0.9811320900917053)
[2025-02-13 02:34:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:16][root][INFO] - Training Epoch: 1/2, step 1132/7134 completed (loss: 0.1221897080540657, acc: 0.9709035158157349)
[2025-02-13 02:34:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:17][root][INFO] - Training Epoch: 1/2, step 1133/7134 completed (loss: 0.11212445795536041, acc: 0.9700428247451782)
[2025-02-13 02:34:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:17][root][INFO] - Training Epoch: 1/2, step 1134/7134 completed (loss: 0.0856815055012703, acc: 0.9763092398643494)
[2025-02-13 02:34:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:18][root][INFO] - Training Epoch: 1/2, step 1135/7134 completed (loss: 0.06438884139060974, acc: 0.9807999730110168)
[2025-02-13 02:34:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:18][root][INFO] - Training Epoch: 1/2, step 1136/7134 completed (loss: 0.11516987532377243, acc: 0.9768707752227783)
[2025-02-13 02:34:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:19][root][INFO] - Training Epoch: 1/2, step 1137/7134 completed (loss: 0.06145124509930611, acc: 0.9863636493682861)
[2025-02-13 02:34:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:19][root][INFO] - Training Epoch: 1/2, step 1138/7134 completed (loss: 0.08006656169891357, acc: 0.9726775884628296)
[2025-02-13 02:34:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:19][root][INFO] - Training Epoch: 1/2, step 1139/7134 completed (loss: 0.10702713578939438, acc: 0.9645868539810181)
[2025-02-13 02:34:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:20][root][INFO] - Training Epoch: 1/2, step 1140/7134 completed (loss: 0.03217919543385506, acc: 0.9924812316894531)
[2025-02-13 02:34:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:20][root][INFO] - Training Epoch: 1/2, step 1141/7134 completed (loss: 0.04285139963030815, acc: 0.9953161478042603)
[2025-02-13 02:34:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:21][root][INFO] - Training Epoch: 1/2, step 1142/7134 completed (loss: 0.0660596415400505, acc: 0.9869791865348816)
[2025-02-13 02:34:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:21][root][INFO] - Training Epoch: 1/2, step 1143/7134 completed (loss: 0.1128184050321579, acc: 0.9707927703857422)
[2025-02-13 02:34:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:22][root][INFO] - Training Epoch: 1/2, step 1144/7134 completed (loss: 0.0975206270813942, acc: 0.9712499976158142)
[2025-02-13 02:34:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:22][root][INFO] - Training Epoch: 1/2, step 1145/7134 completed (loss: 0.036921482533216476, acc: 0.9904076457023621)
[2025-02-13 02:34:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:23][root][INFO] - Training Epoch: 1/2, step 1146/7134 completed (loss: 0.04968087747693062, acc: 0.989130437374115)
[2025-02-13 02:34:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:23][root][INFO] - Training Epoch: 1/2, step 1147/7134 completed (loss: 0.02679348550736904, acc: 0.9937655925750732)
[2025-02-13 02:34:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:23][root][INFO] - Training Epoch: 1/2, step 1148/7134 completed (loss: 0.03830545395612717, acc: 0.9897611141204834)
[2025-02-13 02:34:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:24][root][INFO] - Training Epoch: 1/2, step 1149/7134 completed (loss: 0.12355101108551025, acc: 0.9538461565971375)
[2025-02-13 02:34:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:24][root][INFO] - Training Epoch: 1/2, step 1150/7134 completed (loss: 0.049703534692525864, acc: 0.987596869468689)
[2025-02-13 02:34:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:25][root][INFO] - Training Epoch: 1/2, step 1151/7134 completed (loss: 0.020305993035435677, acc: 0.993914783000946)
[2025-02-13 02:34:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:25][root][INFO] - Training Epoch: 1/2, step 1152/7134 completed (loss: 0.031885240226984024, acc: 0.988034188747406)
[2025-02-13 02:34:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:26][root][INFO] - Training Epoch: 1/2, step 1153/7134 completed (loss: 0.07739722728729248, acc: 0.9836512207984924)
[2025-02-13 02:34:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:26][root][INFO] - Training Epoch: 1/2, step 1154/7134 completed (loss: 0.06483224779367447, acc: 0.982300877571106)
[2025-02-13 02:34:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:27][root][INFO] - Training Epoch: 1/2, step 1155/7134 completed (loss: 0.12733569741249084, acc: 0.9561403393745422)
[2025-02-13 02:34:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:27][root][INFO] - Training Epoch: 1/2, step 1156/7134 completed (loss: 0.03254874795675278, acc: 0.9923195242881775)
[2025-02-13 02:34:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:28][root][INFO] - Training Epoch: 1/2, step 1157/7134 completed (loss: 0.06604219228029251, acc: 0.9813242554664612)
[2025-02-13 02:34:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:28][root][INFO] - Training Epoch: 1/2, step 1158/7134 completed (loss: 0.10626743733882904, acc: 0.9783037304878235)
[2025-02-13 02:34:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:29][root][INFO] - Training Epoch: 1/2, step 1159/7134 completed (loss: 0.07368731498718262, acc: 0.980141818523407)
[2025-02-13 02:34:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:29][root][INFO] - Training Epoch: 1/2, step 1160/7134 completed (loss: 0.05585575848817825, acc: 0.9851190447807312)
[2025-02-13 02:34:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:30][root][INFO] - Training Epoch: 1/2, step 1161/7134 completed (loss: 0.09002761542797089, acc: 0.9771615266799927)
[2025-02-13 02:34:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:30][root][INFO] - Training Epoch: 1/2, step 1162/7134 completed (loss: 0.0452720932662487, acc: 0.9801653027534485)
[2025-02-13 02:34:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:30][root][INFO] - Training Epoch: 1/2, step 1163/7134 completed (loss: 0.057383473962545395, acc: 0.988054633140564)
[2025-02-13 02:34:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:31][root][INFO] - Training Epoch: 1/2, step 1164/7134 completed (loss: 0.050661664456129074, acc: 0.9874826073646545)
[2025-02-13 02:34:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:31][root][INFO] - Training Epoch: 1/2, step 1165/7134 completed (loss: 0.04253232851624489, acc: 0.9891451597213745)
[2025-02-13 02:34:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:32][root][INFO] - Training Epoch: 1/2, step 1166/7134 completed (loss: 0.04003559425473213, acc: 0.9876543283462524)
[2025-02-13 02:34:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:32][root][INFO] - Training Epoch: 1/2, step 1167/7134 completed (loss: 0.03181908279657364, acc: 0.990176796913147)
[2025-02-13 02:34:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:33][root][INFO] - Training Epoch: 1/2, step 1168/7134 completed (loss: 0.020487383008003235, acc: 0.9934210777282715)
[2025-02-13 02:34:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:33][root][INFO] - Training Epoch: 1/2, step 1169/7134 completed (loss: 0.04572528600692749, acc: 0.9864864945411682)
[2025-02-13 02:34:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:34][root][INFO] - Training Epoch: 1/2, step 1170/7134 completed (loss: 0.03707839921116829, acc: 0.992977499961853)
[2025-02-13 02:34:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:34][root][INFO] - Training Epoch: 1/2, step 1171/7134 completed (loss: 0.04901367053389549, acc: 0.9866443872451782)
[2025-02-13 02:34:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:35][root][INFO] - Training Epoch: 1/2, step 1172/7134 completed (loss: 0.05755508318543434, acc: 0.9848713874816895)
[2025-02-13 02:34:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:35][root][INFO] - Training Epoch: 1/2, step 1173/7134 completed (loss: 0.047339387238025665, acc: 0.9826086759567261)
[2025-02-13 02:34:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:35][root][INFO] - Training Epoch: 1/2, step 1174/7134 completed (loss: 0.019445564597845078, acc: 0.9954954981803894)
[2025-02-13 02:34:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:36][root][INFO] - Training Epoch: 1/2, step 1175/7134 completed (loss: 0.03824717923998833, acc: 0.9927536249160767)
[2025-02-13 02:34:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:36][root][INFO] - Training Epoch: 1/2, step 1176/7134 completed (loss: 0.018520141020417213, acc: 0.9965753555297852)
[2025-02-13 02:34:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:37][root][INFO] - Training Epoch: 1/2, step 1177/7134 completed (loss: 0.014994606375694275, acc: 0.998039186000824)
[2025-02-13 02:34:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:37][root][INFO] - Training Epoch: 1/2, step 1178/7134 completed (loss: 0.027805130928754807, acc: 0.9906542301177979)
[2025-02-13 02:34:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:38][root][INFO] - Training Epoch: 1/2, step 1179/7134 completed (loss: 0.03750606253743172, acc: 0.990439772605896)
[2025-02-13 02:34:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:38][root][INFO] - Training Epoch: 1/2, step 1180/7134 completed (loss: 0.019839750602841377, acc: 0.9963503479957581)
[2025-02-13 02:34:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:39][root][INFO] - Training Epoch: 1/2, step 1181/7134 completed (loss: 0.03525848314166069, acc: 0.9870503544807434)
[2025-02-13 02:34:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:39][root][INFO] - Training Epoch: 1/2, step 1182/7134 completed (loss: 0.0186851155012846, acc: 0.9948717951774597)
[2025-02-13 02:34:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:40][root][INFO] - Training Epoch: 1/2, step 1183/7134 completed (loss: 0.023376213386654854, acc: 0.9941262602806091)
[2025-02-13 02:34:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:40][root][INFO] - Training Epoch: 1/2, step 1184/7134 completed (loss: 0.07317132502794266, acc: 0.9842632412910461)
[2025-02-13 02:34:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:40][root][INFO] - Training Epoch: 1/2, step 1185/7134 completed (loss: 0.013510683551430702, acc: 0.9939849376678467)
[2025-02-13 02:34:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:41][root][INFO] - Training Epoch: 1/2, step 1186/7134 completed (loss: 0.03405787795782089, acc: 0.993514895439148)
[2025-02-13 02:34:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:41][root][INFO] - Training Epoch: 1/2, step 1187/7134 completed (loss: 0.034714020788669586, acc: 0.990304708480835)
[2025-02-13 02:34:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:42][root][INFO] - Training Epoch: 1/2, step 1188/7134 completed (loss: 0.012626150622963905, acc: 0.9947229623794556)
[2025-02-13 02:34:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:42][root][INFO] - Training Epoch: 1/2, step 1189/7134 completed (loss: 0.013285134918987751, acc: 0.9985548853874207)
[2025-02-13 02:34:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:43][root][INFO] - Training Epoch: 1/2, step 1190/7134 completed (loss: 0.039128515869379044, acc: 0.9872773289680481)
[2025-02-13 02:34:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:43][root][INFO] - Training Epoch: 1/2, step 1191/7134 completed (loss: 0.05391748994588852, acc: 0.9837518334388733)
[2025-02-13 02:34:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:44][root][INFO] - Training Epoch: 1/2, step 1192/7134 completed (loss: 0.04694676771759987, acc: 0.9842725992202759)
[2025-02-13 02:34:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:44][root][INFO] - Training Epoch: 1/2, step 1193/7134 completed (loss: 0.06797812134027481, acc: 0.9856459498405457)
[2025-02-13 02:34:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:45][root][INFO] - Training Epoch: 1/2, step 1194/7134 completed (loss: 0.06680456548929214, acc: 0.9835164546966553)
[2025-02-13 02:34:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:45][root][INFO] - Training Epoch: 1/2, step 1195/7134 completed (loss: 0.09243802726268768, acc: 0.9771241545677185)
[2025-02-13 02:34:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:46][root][INFO] - Training Epoch: 1/2, step 1196/7134 completed (loss: 0.06922334432601929, acc: 0.9770491719245911)
[2025-02-13 02:34:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:46][root][INFO] - Training Epoch: 1/2, step 1197/7134 completed (loss: 0.046349115669727325, acc: 0.9882698059082031)
[2025-02-13 02:34:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:46][root][INFO] - Training Epoch: 1/2, step 1198/7134 completed (loss: 0.03199952095746994, acc: 0.9899280667304993)
[2025-02-13 02:34:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:47][root][INFO] - Training Epoch: 1/2, step 1199/7134 completed (loss: 0.04361671209335327, acc: 0.9911634922027588)
[2025-02-13 02:34:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:47][root][INFO] - Training Epoch: 1/2, step 1200/7134 completed (loss: 0.062294140458106995, acc: 0.987679660320282)
[2025-02-13 02:34:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:48][root][INFO] - Training Epoch: 1/2, step 1201/7134 completed (loss: 0.06195290759205818, acc: 0.9837996959686279)
[2025-02-13 02:34:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:48][root][INFO] - Training Epoch: 1/2, step 1202/7134 completed (loss: 0.06875407695770264, acc: 0.9744279980659485)
[2025-02-13 02:34:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:49][root][INFO] - Training Epoch: 1/2, step 1203/7134 completed (loss: 0.019533755257725716, acc: 0.9954476356506348)
[2025-02-13 02:34:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:49][root][INFO] - Training Epoch: 1/2, step 1204/7134 completed (loss: 0.05743439495563507, acc: 0.9813432693481445)
[2025-02-13 02:34:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:50][root][INFO] - Training Epoch: 1/2, step 1205/7134 completed (loss: 0.10527019202709198, acc: 0.9760563373565674)
[2025-02-13 02:34:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:50][root][INFO] - Training Epoch: 1/2, step 1206/7134 completed (loss: 0.050909239798784256, acc: 0.9863523840904236)
[2025-02-13 02:34:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:51][root][INFO] - Training Epoch: 1/2, step 1207/7134 completed (loss: 0.05850424990057945, acc: 0.9849246144294739)
[2025-02-13 02:34:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:51][root][INFO] - Training Epoch: 1/2, step 1208/7134 completed (loss: 0.0794711485505104, acc: 0.9738805890083313)
[2025-02-13 02:34:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:52][root][INFO] - Training Epoch: 1/2, step 1209/7134 completed (loss: 0.10136328637599945, acc: 0.974405825138092)
[2025-02-13 02:34:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:52][root][INFO] - Training Epoch: 1/2, step 1210/7134 completed (loss: 0.07037776708602905, acc: 0.9744816422462463)
[2025-02-13 02:34:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:52][root][INFO] - Training Epoch: 1/2, step 1211/7134 completed (loss: 0.1577509343624115, acc: 0.9556840062141418)
[2025-02-13 02:34:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:53][root][INFO] - Training Epoch: 1/2, step 1212/7134 completed (loss: 0.11190366744995117, acc: 0.9710366129875183)
[2025-02-13 02:34:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:53][root][INFO] - Training Epoch: 1/2, step 1213/7134 completed (loss: 0.057499758899211884, acc: 0.9773691892623901)
[2025-02-13 02:34:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:54][root][INFO] - Training Epoch: 1/2, step 1214/7134 completed (loss: 0.07307828217744827, acc: 0.9723076820373535)
[2025-02-13 02:34:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:54][root][INFO] - Training Epoch: 1/2, step 1215/7134 completed (loss: 0.1114865392446518, acc: 0.973793089389801)
[2025-02-13 02:34:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:55][root][INFO] - Training Epoch: 1/2, step 1216/7134 completed (loss: 0.0620180182158947, acc: 0.985401451587677)
[2025-02-13 02:34:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:55][root][INFO] - Training Epoch: 1/2, step 1217/7134 completed (loss: 0.06553223729133606, acc: 0.9870129823684692)
[2025-02-13 02:34:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:56][root][INFO] - Training Epoch: 1/2, step 1218/7134 completed (loss: 0.1168552115559578, acc: 0.9701492786407471)
[2025-02-13 02:34:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:56][root][INFO] - Training Epoch: 1/2, step 1219/7134 completed (loss: 0.11466064304113388, acc: 0.9666081070899963)
[2025-02-13 02:34:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:56][root][INFO] - Training Epoch: 1/2, step 1220/7134 completed (loss: 0.12651890516281128, acc: 0.9720812439918518)
[2025-02-13 02:34:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:57][root][INFO] - Training Epoch: 1/2, step 1221/7134 completed (loss: 0.10183845460414886, acc: 0.9758771657943726)
[2025-02-13 02:34:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:57][root][INFO] - Training Epoch: 1/2, step 1222/7134 completed (loss: 0.10019482672214508, acc: 0.9785459041595459)
[2025-02-13 02:34:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:58][root][INFO] - Training Epoch: 1/2, step 1223/7134 completed (loss: 0.20886929333209991, acc: 0.9492455124855042)
[2025-02-13 02:34:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:58][root][INFO] - Training Epoch: 1/2, step 1224/7134 completed (loss: 0.06774911284446716, acc: 0.9867549538612366)
[2025-02-13 02:34:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:59][root][INFO] - Training Epoch: 1/2, step 1225/7134 completed (loss: 0.0872628465294838, acc: 0.9738932847976685)
[2025-02-13 02:34:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:34:59][root][INFO] - Training Epoch: 1/2, step 1226/7134 completed (loss: 0.15774814784526825, acc: 0.9597585797309875)
[2025-02-13 02:34:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:00][root][INFO] - Training Epoch: 1/2, step 1227/7134 completed (loss: 0.1128988191485405, acc: 0.9651474356651306)
[2025-02-13 02:35:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:00][root][INFO] - Training Epoch: 1/2, step 1228/7134 completed (loss: 0.13226757943630219, acc: 0.9756795167922974)
[2025-02-13 02:35:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:01][root][INFO] - Training Epoch: 1/2, step 1229/7134 completed (loss: 0.06719084084033966, acc: 0.9807923436164856)
[2025-02-13 02:35:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:01][root][INFO] - Training Epoch: 1/2, step 1230/7134 completed (loss: 0.12040577083826065, acc: 0.9740113019943237)
[2025-02-13 02:35:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:02][root][INFO] - Training Epoch: 1/2, step 1231/7134 completed (loss: 0.09722999483346939, acc: 0.974146842956543)
[2025-02-13 02:35:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:02][root][INFO] - Training Epoch: 1/2, step 1232/7134 completed (loss: 0.08036252856254578, acc: 0.9818956255912781)
[2025-02-13 02:35:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:03][root][INFO] - Training Epoch: 1/2, step 1233/7134 completed (loss: 0.13584622740745544, acc: 0.9618496894836426)
[2025-02-13 02:35:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:03][root][INFO] - Training Epoch: 1/2, step 1234/7134 completed (loss: 0.06109679490327835, acc: 0.9860248565673828)
[2025-02-13 02:35:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:04][root][INFO] - Training Epoch: 1/2, step 1235/7134 completed (loss: 0.10268548130989075, acc: 0.9760192036628723)
[2025-02-13 02:35:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:04][root][INFO] - Training Epoch: 1/2, step 1236/7134 completed (loss: 0.14248621463775635, acc: 0.9604685306549072)
[2025-02-13 02:35:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:05][root][INFO] - Training Epoch: 1/2, step 1237/7134 completed (loss: 0.0987028256058693, acc: 0.9751552939414978)
[2025-02-13 02:35:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:05][root][INFO] - Training Epoch: 1/2, step 1238/7134 completed (loss: 0.14789144694805145, acc: 0.9563318490982056)
[2025-02-13 02:35:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:06][root][INFO] - Training Epoch: 1/2, step 1239/7134 completed (loss: 0.11531716585159302, acc: 0.9642346501350403)
[2025-02-13 02:35:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:06][root][INFO] - Training Epoch: 1/2, step 1240/7134 completed (loss: 0.08413449674844742, acc: 0.9801587462425232)
[2025-02-13 02:35:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:07][root][INFO] - Training Epoch: 1/2, step 1241/7134 completed (loss: 0.09899093210697174, acc: 0.9638554453849792)
[2025-02-13 02:35:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:07][root][INFO] - Training Epoch: 1/2, step 1242/7134 completed (loss: 0.07931706309318542, acc: 0.9771634340286255)
[2025-02-13 02:35:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:08][root][INFO] - Training Epoch: 1/2, step 1243/7134 completed (loss: 0.09486164897680283, acc: 0.9780334830284119)
[2025-02-13 02:35:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:08][root][INFO] - Training Epoch: 1/2, step 1244/7134 completed (loss: 0.10786506533622742, acc: 0.9718543291091919)
[2025-02-13 02:35:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:08][root][INFO] - Training Epoch: 1/2, step 1245/7134 completed (loss: 0.08694873005151749, acc: 0.9785894155502319)
[2025-02-13 02:35:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:09][root][INFO] - Training Epoch: 1/2, step 1246/7134 completed (loss: 0.1154407486319542, acc: 0.9756097793579102)
[2025-02-13 02:35:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:09][root][INFO] - Training Epoch: 1/2, step 1247/7134 completed (loss: 0.06209523230791092, acc: 0.9826338887214661)
[2025-02-13 02:35:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:10][root][INFO] - Training Epoch: 1/2, step 1248/7134 completed (loss: 0.10465922951698303, acc: 0.972540020942688)
[2025-02-13 02:35:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:10][root][INFO] - Training Epoch: 1/2, step 1249/7134 completed (loss: 0.05516490340232849, acc: 0.9875444769859314)
[2025-02-13 02:35:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:11][root][INFO] - Training Epoch: 1/2, step 1250/7134 completed (loss: 0.04603471979498863, acc: 0.9872521162033081)
[2025-02-13 02:35:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:11][root][INFO] - Training Epoch: 1/2, step 1251/7134 completed (loss: 0.06466929614543915, acc: 0.9821673631668091)
[2025-02-13 02:35:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:12][root][INFO] - Training Epoch: 1/2, step 1252/7134 completed (loss: 0.049824632704257965, acc: 0.9907407164573669)
[2025-02-13 02:35:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:12][root][INFO] - Training Epoch: 1/2, step 1253/7134 completed (loss: 0.09071891754865646, acc: 0.9806950092315674)
[2025-02-13 02:35:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:13][root][INFO] - Training Epoch: 1/2, step 1254/7134 completed (loss: 0.09848619997501373, acc: 0.9712121486663818)
[2025-02-13 02:35:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:13][root][INFO] - Training Epoch: 1/2, step 1255/7134 completed (loss: 0.063972108066082, acc: 0.9839650392532349)
[2025-02-13 02:35:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:13][root][INFO] - Training Epoch: 1/2, step 1256/7134 completed (loss: 0.07129507511854172, acc: 0.9807074069976807)
[2025-02-13 02:35:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:14][root][INFO] - Training Epoch: 1/2, step 1257/7134 completed (loss: 0.04987158626317978, acc: 0.9854604005813599)
[2025-02-13 02:35:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:14][root][INFO] - Training Epoch: 1/2, step 1258/7134 completed (loss: 0.0896674394607544, acc: 0.9721815586090088)
[2025-02-13 02:35:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:15][root][INFO] - Training Epoch: 1/2, step 1259/7134 completed (loss: 0.07256948202848434, acc: 0.9871244430541992)
[2025-02-13 02:35:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:15][root][INFO] - Training Epoch: 1/2, step 1260/7134 completed (loss: 0.04074142500758171, acc: 0.9898843765258789)
[2025-02-13 02:35:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:16][root][INFO] - Training Epoch: 1/2, step 1261/7134 completed (loss: 0.0670052319765091, acc: 0.9836795330047607)
[2025-02-13 02:35:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:16][root][INFO] - Training Epoch: 1/2, step 1262/7134 completed (loss: 0.01664264313876629, acc: 0.9926739931106567)
[2025-02-13 02:35:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:17][root][INFO] - Training Epoch: 1/2, step 1263/7134 completed (loss: 0.026452146470546722, acc: 0.9957143068313599)
[2025-02-13 02:35:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:17][root][INFO] - Training Epoch: 1/2, step 1264/7134 completed (loss: 0.06215164437890053, acc: 0.9844236969947815)
[2025-02-13 02:35:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:18][root][INFO] - Training Epoch: 1/2, step 1265/7134 completed (loss: 0.043754320591688156, acc: 0.9874686598777771)
[2025-02-13 02:35:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:18][root][INFO] - Training Epoch: 1/2, step 1266/7134 completed (loss: 0.051156122237443924, acc: 0.9845094680786133)
[2025-02-13 02:35:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:19][root][INFO] - Training Epoch: 1/2, step 1267/7134 completed (loss: 0.050383057445287704, acc: 0.9889655113220215)
[2025-02-13 02:35:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:19][root][INFO] - Training Epoch: 1/2, step 1268/7134 completed (loss: 0.04606654495000839, acc: 0.985228955745697)
[2025-02-13 02:35:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:19][root][INFO] - Training Epoch: 1/2, step 1269/7134 completed (loss: 0.10745292901992798, acc: 0.9724137783050537)
[2025-02-13 02:35:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:20][root][INFO] - Training Epoch: 1/2, step 1270/7134 completed (loss: 0.04466351866722107, acc: 0.9874476790428162)
[2025-02-13 02:35:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:20][root][INFO] - Training Epoch: 1/2, step 1271/7134 completed (loss: 0.07351858168840408, acc: 0.9851852059364319)
[2025-02-13 02:35:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:21][root][INFO] - Training Epoch: 1/2, step 1272/7134 completed (loss: 0.03482520952820778, acc: 0.9867899417877197)
[2025-02-13 02:35:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:21][root][INFO] - Training Epoch: 1/2, step 1273/7134 completed (loss: 0.03867833688855171, acc: 0.9903225898742676)
[2025-02-13 02:35:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:22][root][INFO] - Training Epoch: 1/2, step 1274/7134 completed (loss: 0.04584549739956856, acc: 0.9881756901741028)
[2025-02-13 02:35:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:22][root][INFO] - Training Epoch: 1/2, step 1275/7134 completed (loss: 0.05835498496890068, acc: 0.9784560203552246)
[2025-02-13 02:35:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:23][root][INFO] - Training Epoch: 1/2, step 1276/7134 completed (loss: 0.039886850863695145, acc: 0.9870967864990234)
[2025-02-13 02:35:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:23][root][INFO] - Training Epoch: 1/2, step 1277/7134 completed (loss: 0.04935505986213684, acc: 0.9854604005813599)
[2025-02-13 02:35:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:23][root][INFO] - Training Epoch: 1/2, step 1278/7134 completed (loss: 0.051774900406599045, acc: 0.9813374876976013)
[2025-02-13 02:35:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:24][root][INFO] - Training Epoch: 1/2, step 1279/7134 completed (loss: 0.061353426426649094, acc: 0.9826302528381348)
[2025-02-13 02:35:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:24][root][INFO] - Training Epoch: 1/2, step 1280/7134 completed (loss: 0.08479276299476624, acc: 0.9829171895980835)
[2025-02-13 02:35:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:25][root][INFO] - Training Epoch: 1/2, step 1281/7134 completed (loss: 0.08134464174509048, acc: 0.979345977306366)
[2025-02-13 02:35:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:25][root][INFO] - Training Epoch: 1/2, step 1282/7134 completed (loss: 0.1118343397974968, acc: 0.9759358167648315)
[2025-02-13 02:35:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:26][root][INFO] - Training Epoch: 1/2, step 1283/7134 completed (loss: 0.10599963366985321, acc: 0.9735915660858154)
[2025-02-13 02:35:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:26][root][INFO] - Training Epoch: 1/2, step 1284/7134 completed (loss: 0.07858016341924667, acc: 0.9768392443656921)
[2025-02-13 02:35:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:27][root][INFO] - Training Epoch: 1/2, step 1285/7134 completed (loss: 0.07899860292673111, acc: 0.9884560108184814)
[2025-02-13 02:35:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:27][root][INFO] - Training Epoch: 1/2, step 1286/7134 completed (loss: 0.05031895637512207, acc: 0.9885550737380981)
[2025-02-13 02:35:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:28][root][INFO] - Training Epoch: 1/2, step 1287/7134 completed (loss: 0.09786124527454376, acc: 0.9767441749572754)
[2025-02-13 02:35:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:28][root][INFO] - Training Epoch: 1/2, step 1288/7134 completed (loss: 0.06393076479434967, acc: 0.9822335243225098)
[2025-02-13 02:35:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:28][root][INFO] - Training Epoch: 1/2, step 1289/7134 completed (loss: 0.07803759723901749, acc: 0.9782244563102722)
[2025-02-13 02:35:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:29][root][INFO] - Training Epoch: 1/2, step 1290/7134 completed (loss: 0.06059901788830757, acc: 0.9873239398002625)
[2025-02-13 02:35:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:29][root][INFO] - Training Epoch: 1/2, step 1291/7134 completed (loss: 0.04320593550801277, acc: 0.9888424277305603)
[2025-02-13 02:35:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:30][root][INFO] - Training Epoch: 1/2, step 1292/7134 completed (loss: 0.06553773581981659, acc: 0.983116865158081)
[2025-02-13 02:35:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:30][root][INFO] - Training Epoch: 1/2, step 1293/7134 completed (loss: 0.03313421085476875, acc: 0.9918509721755981)
[2025-02-13 02:35:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:31][root][INFO] - Training Epoch: 1/2, step 1294/7134 completed (loss: 0.04780735448002815, acc: 0.9895969033241272)
[2025-02-13 02:35:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:31][root][INFO] - Training Epoch: 1/2, step 1295/7134 completed (loss: 0.05068293213844299, acc: 0.9830220937728882)
[2025-02-13 02:35:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:32][root][INFO] - Training Epoch: 1/2, step 1296/7134 completed (loss: 0.018776003271341324, acc: 0.9955752491950989)
[2025-02-13 02:35:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:32][root][INFO] - Training Epoch: 1/2, step 1297/7134 completed (loss: 0.07341546565294266, acc: 0.9771309494972229)
[2025-02-13 02:35:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:33][root][INFO] - Training Epoch: 1/2, step 1298/7134 completed (loss: 0.1080201044678688, acc: 0.9784172773361206)
[2025-02-13 02:35:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:33][root][INFO] - Training Epoch: 1/2, step 1299/7134 completed (loss: 0.05208157002925873, acc: 0.9817708134651184)
[2025-02-13 02:35:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:33][root][INFO] - Training Epoch: 1/2, step 1300/7134 completed (loss: 0.038771189749240875, acc: 0.9902557730674744)
[2025-02-13 02:35:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:34][root][INFO] - Training Epoch: 1/2, step 1301/7134 completed (loss: 0.024359026923775673, acc: 0.9940652847290039)
[2025-02-13 02:35:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:34][root][INFO] - Training Epoch: 1/2, step 1302/7134 completed (loss: 0.06705976277589798, acc: 0.9854910969734192)
[2025-02-13 02:35:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:35][root][INFO] - Training Epoch: 1/2, step 1303/7134 completed (loss: 0.08684121817350388, acc: 0.9783653616905212)
[2025-02-13 02:35:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:35][root][INFO] - Training Epoch: 1/2, step 1304/7134 completed (loss: 0.10119825601577759, acc: 0.9723320007324219)
[2025-02-13 02:35:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:36][root][INFO] - Training Epoch: 1/2, step 1305/7134 completed (loss: 0.0719696506857872, acc: 0.9796954393386841)
[2025-02-13 02:35:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:36][root][INFO] - Training Epoch: 1/2, step 1306/7134 completed (loss: 0.07972834259271622, acc: 0.9833080172538757)
[2025-02-13 02:35:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:37][root][INFO] - Training Epoch: 1/2, step 1307/7134 completed (loss: 0.039576850831508636, acc: 0.9873617887496948)
[2025-02-13 02:35:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:37][root][INFO] - Training Epoch: 1/2, step 1308/7134 completed (loss: 0.032335493713617325, acc: 0.9864661693572998)
[2025-02-13 02:35:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:38][root][INFO] - Training Epoch: 1/2, step 1309/7134 completed (loss: 0.07204335182905197, acc: 0.980861246585846)
[2025-02-13 02:35:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:38][root][INFO] - Training Epoch: 1/2, step 1310/7134 completed (loss: 0.04773610457777977, acc: 0.9836734533309937)
[2025-02-13 02:35:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:38][root][INFO] - Training Epoch: 1/2, step 1311/7134 completed (loss: 0.06787315011024475, acc: 0.9864253401756287)
[2025-02-13 02:35:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:39][root][INFO] - Training Epoch: 1/2, step 1312/7134 completed (loss: 0.0935545340180397, acc: 0.973045825958252)
[2025-02-13 02:35:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:39][root][INFO] - Training Epoch: 1/2, step 1313/7134 completed (loss: 0.08370500802993774, acc: 0.9759206771850586)
[2025-02-13 02:35:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:40][root][INFO] - Training Epoch: 1/2, step 1314/7134 completed (loss: 0.0555603913962841, acc: 0.9842105507850647)
[2025-02-13 02:35:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:40][root][INFO] - Training Epoch: 1/2, step 1315/7134 completed (loss: 0.18668192625045776, acc: 0.9599999785423279)
[2025-02-13 02:35:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:41][root][INFO] - Training Epoch: 1/2, step 1316/7134 completed (loss: 0.08079398423433304, acc: 0.9811676144599915)
[2025-02-13 02:35:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:41][root][INFO] - Training Epoch: 1/2, step 1317/7134 completed (loss: 0.12423912435770035, acc: 0.9674796462059021)
[2025-02-13 02:35:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:42][root][INFO] - Training Epoch: 1/2, step 1318/7134 completed (loss: 0.09567168354988098, acc: 0.9809264540672302)
[2025-02-13 02:35:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:42][root][INFO] - Training Epoch: 1/2, step 1319/7134 completed (loss: 0.11786390841007233, acc: 0.970251739025116)
[2025-02-13 02:35:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:42][root][INFO] - Training Epoch: 1/2, step 1320/7134 completed (loss: 0.13803604245185852, acc: 0.9641693830490112)
[2025-02-13 02:35:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:43][root][INFO] - Training Epoch: 1/2, step 1321/7134 completed (loss: 0.03756542131304741, acc: 0.9914215803146362)
[2025-02-13 02:35:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:44][root][INFO] - Training Epoch: 1/2, step 1322/7134 completed (loss: 0.07074069231748581, acc: 0.9754500985145569)
[2025-02-13 02:35:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:44][root][INFO] - Training Epoch: 1/2, step 1323/7134 completed (loss: 0.09044129401445389, acc: 0.9750000238418579)
[2025-02-13 02:35:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:44][root][INFO] - Training Epoch: 1/2, step 1324/7134 completed (loss: 0.0764608383178711, acc: 0.9802631735801697)
[2025-02-13 02:35:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:45][root][INFO] - Training Epoch: 1/2, step 1325/7134 completed (loss: 0.06294431537389755, acc: 0.9795918464660645)
[2025-02-13 02:35:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:45][root][INFO] - Training Epoch: 1/2, step 1326/7134 completed (loss: 0.06994526833295822, acc: 0.9846322536468506)
[2025-02-13 02:35:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:46][root][INFO] - Training Epoch: 1/2, step 1327/7134 completed (loss: 0.0771261528134346, acc: 0.9775112271308899)
[2025-02-13 02:35:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:46][root][INFO] - Training Epoch: 1/2, step 1328/7134 completed (loss: 0.059531405568122864, acc: 0.9837209582328796)
[2025-02-13 02:35:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:47][root][INFO] - Training Epoch: 1/2, step 1329/7134 completed (loss: 0.08986871689558029, acc: 0.980663001537323)
[2025-02-13 02:35:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:47][root][INFO] - Training Epoch: 1/2, step 1330/7134 completed (loss: 0.0391879677772522, acc: 0.9914236664772034)
[2025-02-13 02:35:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:48][root][INFO] - Training Epoch: 1/2, step 1331/7134 completed (loss: 0.0285640861839056, acc: 0.9885993599891663)
[2025-02-13 02:35:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:48][root][INFO] - Training Epoch: 1/2, step 1332/7134 completed (loss: 0.02760576270520687, acc: 0.9912663698196411)
[2025-02-13 02:35:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:49][root][INFO] - Training Epoch: 1/2, step 1333/7134 completed (loss: 0.04734347015619278, acc: 0.9812925457954407)
[2025-02-13 02:35:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:49][root][INFO] - Training Epoch: 1/2, step 1334/7134 completed (loss: 0.11521433293819427, acc: 0.9751552939414978)
[2025-02-13 02:35:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:50][root][INFO] - Training Epoch: 1/2, step 1335/7134 completed (loss: 0.11905303597450256, acc: 0.9760319590568542)
[2025-02-13 02:35:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:50][root][INFO] - Training Epoch: 1/2, step 1336/7134 completed (loss: 0.03235451877117157, acc: 0.9918808937072754)
[2025-02-13 02:35:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:50][root][INFO] - Training Epoch: 1/2, step 1337/7134 completed (loss: 0.043712981045246124, acc: 0.9861963391304016)
[2025-02-13 02:35:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:51][root][INFO] - Training Epoch: 1/2, step 1338/7134 completed (loss: 0.12257962673902512, acc: 0.9638554453849792)
[2025-02-13 02:35:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:51][root][INFO] - Training Epoch: 1/2, step 1339/7134 completed (loss: 0.08576135337352753, acc: 0.9719101190567017)
[2025-02-13 02:35:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:52][root][INFO] - Training Epoch: 1/2, step 1340/7134 completed (loss: 0.08799181133508682, acc: 0.9776452779769897)
[2025-02-13 02:35:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:52][root][INFO] - Training Epoch: 1/2, step 1341/7134 completed (loss: 0.06468906998634338, acc: 0.9858757257461548)
[2025-02-13 02:35:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:53][root][INFO] - Training Epoch: 1/2, step 1342/7134 completed (loss: 0.036708395928144455, acc: 0.9871175289154053)
[2025-02-13 02:35:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:53][root][INFO] - Training Epoch: 1/2, step 1343/7134 completed (loss: 0.07851657271385193, acc: 0.980322003364563)
[2025-02-13 02:35:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:53][root][INFO] - Training Epoch: 1/2, step 1344/7134 completed (loss: 0.03167541325092316, acc: 0.9841628670692444)
[2025-02-13 02:35:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:54][root][INFO] - Training Epoch: 1/2, step 1345/7134 completed (loss: 0.04779152572154999, acc: 0.9840849041938782)
[2025-02-13 02:35:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:54][root][INFO] - Training Epoch: 1/2, step 1346/7134 completed (loss: 0.08530355989933014, acc: 0.9839786291122437)
[2025-02-13 02:35:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:55][root][INFO] - Training Epoch: 1/2, step 1347/7134 completed (loss: 0.08119932562112808, acc: 0.9808362126350403)
[2025-02-13 02:35:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:55][root][INFO] - Training Epoch: 1/2, step 1348/7134 completed (loss: 0.04400843381881714, acc: 0.9908257126808167)
[2025-02-13 02:35:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:56][root][INFO] - Training Epoch: 1/2, step 1349/7134 completed (loss: 0.056244175881147385, acc: 0.9846153855323792)
[2025-02-13 02:35:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:56][root][INFO] - Training Epoch: 1/2, step 1350/7134 completed (loss: 0.06182233244180679, acc: 0.9855282306671143)
[2025-02-13 02:35:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:57][root][INFO] - Training Epoch: 1/2, step 1351/7134 completed (loss: 0.03839040920138359, acc: 0.990176796913147)
[2025-02-13 02:35:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:57][root][INFO] - Training Epoch: 1/2, step 1352/7134 completed (loss: 0.05658657103776932, acc: 0.988095223903656)
[2025-02-13 02:35:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:57][root][INFO] - Training Epoch: 1/2, step 1353/7134 completed (loss: 0.05152024328708649, acc: 0.982758641242981)
[2025-02-13 02:35:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:58][root][INFO] - Training Epoch: 1/2, step 1354/7134 completed (loss: 0.02779470384120941, acc: 0.9951377511024475)
[2025-02-13 02:35:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:58][root][INFO] - Training Epoch: 1/2, step 1355/7134 completed (loss: 0.02728492207825184, acc: 0.9922118186950684)
[2025-02-13 02:35:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:59][root][INFO] - Training Epoch: 1/2, step 1356/7134 completed (loss: 0.028683360666036606, acc: 0.9934210777282715)
[2025-02-13 02:35:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:35:59][root][INFO] - Training Epoch: 1/2, step 1357/7134 completed (loss: 0.01463006716221571, acc: 0.9974779486656189)
[2025-02-13 02:35:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:00][root][INFO] - Training Epoch: 1/2, step 1358/7134 completed (loss: 0.02625340223312378, acc: 0.9909090995788574)
[2025-02-13 02:36:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:00][root][INFO] - Training Epoch: 1/2, step 1359/7134 completed (loss: 0.016463110223412514, acc: 0.9959514141082764)
[2025-02-13 02:36:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:01][root][INFO] - Training Epoch: 1/2, step 1360/7134 completed (loss: 0.01758943870663643, acc: 0.9946236610412598)
[2025-02-13 02:36:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:01][root][INFO] - Training Epoch: 1/2, step 1361/7134 completed (loss: 0.030712367966771126, acc: 0.9927007555961609)
[2025-02-13 02:36:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:02][root][INFO] - Training Epoch: 1/2, step 1362/7134 completed (loss: 0.020433293655514717, acc: 0.9931694269180298)
[2025-02-13 02:36:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:02][root][INFO] - Training Epoch: 1/2, step 1363/7134 completed (loss: 0.02757648192346096, acc: 0.9932975769042969)
[2025-02-13 02:36:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:02][root][INFO] - Training Epoch: 1/2, step 1364/7134 completed (loss: 0.015360964462161064, acc: 0.9971428513526917)
[2025-02-13 02:36:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:03][root][INFO] - Training Epoch: 1/2, step 1365/7134 completed (loss: 0.02098168060183525, acc: 0.9945054650306702)
[2025-02-13 02:36:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:03][root][INFO] - Training Epoch: 1/2, step 1366/7134 completed (loss: 0.011070044711232185, acc: 0.9982143044471741)
[2025-02-13 02:36:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:04][root][INFO] - Training Epoch: 1/2, step 1367/7134 completed (loss: 0.008905225433409214, acc: 0.998275876045227)
[2025-02-13 02:36:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:04][root][INFO] - Training Epoch: 1/2, step 1368/7134 completed (loss: 0.050735484808683395, acc: 0.9832317233085632)
[2025-02-13 02:36:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:05][root][INFO] - Training Epoch: 1/2, step 1369/7134 completed (loss: 0.06995340436697006, acc: 0.985989511013031)
[2025-02-13 02:36:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:05][root][INFO] - Training Epoch: 1/2, step 1370/7134 completed (loss: 0.041614044457674026, acc: 0.9898862242698669)
[2025-02-13 02:36:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:06][root][INFO] - Training Epoch: 1/2, step 1371/7134 completed (loss: 0.02201773226261139, acc: 0.9956331849098206)
[2025-02-13 02:36:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:06][root][INFO] - Training Epoch: 1/2, step 1372/7134 completed (loss: 0.1022464707493782, acc: 0.9775280952453613)
[2025-02-13 02:36:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:06][root][INFO] - Training Epoch: 1/2, step 1373/7134 completed (loss: 0.03915572911500931, acc: 0.9803149700164795)
[2025-02-13 02:36:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:07][root][INFO] - Training Epoch: 1/2, step 1374/7134 completed (loss: 0.04145219922065735, acc: 0.986138641834259)
[2025-02-13 02:36:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:08][root][INFO] - Training Epoch: 1/2, step 1375/7134 completed (loss: 0.06452584266662598, acc: 0.9807692170143127)
[2025-02-13 02:36:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:08][root][INFO] - Training Epoch: 1/2, step 1376/7134 completed (loss: 0.07251400500535965, acc: 0.9832985401153564)
[2025-02-13 02:36:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:08][root][INFO] - Training Epoch: 1/2, step 1377/7134 completed (loss: 0.04946596175432205, acc: 0.9858823418617249)
[2025-02-13 02:36:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:09][root][INFO] - Training Epoch: 1/2, step 1378/7134 completed (loss: 0.07698796689510345, acc: 0.9844054579734802)
[2025-02-13 02:36:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:09][root][INFO] - Training Epoch: 1/2, step 1379/7134 completed (loss: 0.0738551914691925, acc: 0.976344108581543)
[2025-02-13 02:36:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:10][root][INFO] - Training Epoch: 1/2, step 1380/7134 completed (loss: 0.07037849724292755, acc: 0.9831775426864624)
[2025-02-13 02:36:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:10][root][INFO] - Training Epoch: 1/2, step 1381/7134 completed (loss: 0.05058358609676361, acc: 0.9850746393203735)
[2025-02-13 02:36:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:11][root][INFO] - Training Epoch: 1/2, step 1382/7134 completed (loss: 0.0627543032169342, acc: 0.9809523820877075)
[2025-02-13 02:36:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:11][root][INFO] - Training Epoch: 1/2, step 1383/7134 completed (loss: 0.0417918860912323, acc: 0.9879518151283264)
[2025-02-13 02:36:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:11][root][INFO] - Training Epoch: 1/2, step 1384/7134 completed (loss: 0.11946599930524826, acc: 0.9633943438529968)
[2025-02-13 02:36:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:12][root][INFO] - Training Epoch: 1/2, step 1385/7134 completed (loss: 0.09503502398729324, acc: 0.9724770784378052)
[2025-02-13 02:36:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:12][root][INFO] - Training Epoch: 1/2, step 1386/7134 completed (loss: 0.070077084004879, acc: 0.983775794506073)
[2025-02-13 02:36:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:13][root][INFO] - Training Epoch: 1/2, step 1387/7134 completed (loss: 0.08595477044582367, acc: 0.9742120504379272)
[2025-02-13 02:36:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:13][root][INFO] - Training Epoch: 1/2, step 1388/7134 completed (loss: 0.07484008371829987, acc: 0.9880749583244324)
[2025-02-13 02:36:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:14][root][INFO] - Training Epoch: 1/2, step 1389/7134 completed (loss: 0.06434505432844162, acc: 0.9821717739105225)
[2025-02-13 02:36:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:14][root][INFO] - Training Epoch: 1/2, step 1390/7134 completed (loss: 0.0614112988114357, acc: 0.9812792539596558)
[2025-02-13 02:36:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:15][root][INFO] - Training Epoch: 1/2, step 1391/7134 completed (loss: 0.09102876484394073, acc: 0.9753954410552979)
[2025-02-13 02:36:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:15][root][INFO] - Training Epoch: 1/2, step 1392/7134 completed (loss: 0.06682106852531433, acc: 0.9826989769935608)
[2025-02-13 02:36:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:16][root][INFO] - Training Epoch: 1/2, step 1393/7134 completed (loss: 0.0709126815199852, acc: 0.981566846370697)
[2025-02-13 02:36:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:16][root][INFO] - Training Epoch: 1/2, step 1394/7134 completed (loss: 0.047816917300224304, acc: 0.9940476417541504)
[2025-02-13 02:36:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:17][root][INFO] - Training Epoch: 1/2, step 1395/7134 completed (loss: 0.0721740871667862, acc: 0.9802761077880859)
[2025-02-13 02:36:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:17][root][INFO] - Training Epoch: 1/2, step 1396/7134 completed (loss: 0.06199778616428375, acc: 0.9763513803482056)
[2025-02-13 02:36:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:18][root][INFO] - Training Epoch: 1/2, step 1397/7134 completed (loss: 0.08374840021133423, acc: 0.9747048616409302)
[2025-02-13 02:36:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:18][root][INFO] - Training Epoch: 1/2, step 1398/7134 completed (loss: 0.0642508715391159, acc: 0.9837728142738342)
[2025-02-13 02:36:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:18][root][INFO] - Training Epoch: 1/2, step 1399/7134 completed (loss: 0.14220364391803741, acc: 0.9728260636329651)
[2025-02-13 02:36:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:19][root][INFO] - Training Epoch: 1/2, step 1400/7134 completed (loss: 0.04102429375052452, acc: 0.9912739992141724)
[2025-02-13 02:36:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:19][root][INFO] - Training Epoch: 1/2, step 1401/7134 completed (loss: 0.02959408238530159, acc: 0.9917627573013306)
[2025-02-13 02:36:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:20][root][INFO] - Training Epoch: 1/2, step 1402/7134 completed (loss: 0.03374867141246796, acc: 0.9896373152732849)
[2025-02-13 02:36:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:20][root][INFO] - Training Epoch: 1/2, step 1403/7134 completed (loss: 0.04817742109298706, acc: 0.9825970530509949)
[2025-02-13 02:36:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:21][root][INFO] - Training Epoch: 1/2, step 1404/7134 completed (loss: 0.044670213013887405, acc: 0.988063633441925)
[2025-02-13 02:36:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:21][root][INFO] - Training Epoch: 1/2, step 1405/7134 completed (loss: 0.035406097769737244, acc: 0.9843304753303528)
[2025-02-13 02:36:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:22][root][INFO] - Training Epoch: 1/2, step 1406/7134 completed (loss: 0.022991089150309563, acc: 0.9913793206214905)
[2025-02-13 02:36:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:22][root][INFO] - Training Epoch: 1/2, step 1407/7134 completed (loss: 0.031238969415426254, acc: 0.9921773076057434)
[2025-02-13 02:36:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:22][root][INFO] - Training Epoch: 1/2, step 1408/7134 completed (loss: 0.03212789073586464, acc: 0.9855769276618958)
[2025-02-13 02:36:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:23][root][INFO] - Training Epoch: 1/2, step 1409/7134 completed (loss: 0.014065464958548546, acc: 0.9940387606620789)
[2025-02-13 02:36:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:23][root][INFO] - Training Epoch: 1/2, step 1410/7134 completed (loss: 0.041901845484972, acc: 0.9921875)
[2025-02-13 02:36:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:24][root][INFO] - Training Epoch: 1/2, step 1411/7134 completed (loss: 0.0132426293566823, acc: 0.9960106611251831)
[2025-02-13 02:36:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:24][root][INFO] - Training Epoch: 1/2, step 1412/7134 completed (loss: 0.02440601773560047, acc: 0.9940617680549622)
[2025-02-13 02:36:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:25][root][INFO] - Training Epoch: 1/2, step 1413/7134 completed (loss: 0.030503833666443825, acc: 0.9899280667304993)
[2025-02-13 02:36:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:25][root][INFO] - Training Epoch: 1/2, step 1414/7134 completed (loss: 0.02612408623099327, acc: 0.9953271150588989)
[2025-02-13 02:36:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:26][root][INFO] - Training Epoch: 1/2, step 1415/7134 completed (loss: 0.03973131254315376, acc: 0.9917218685150146)
[2025-02-13 02:36:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:26][root][INFO] - Training Epoch: 1/2, step 1416/7134 completed (loss: 0.01934957504272461, acc: 0.9957864880561829)
[2025-02-13 02:36:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:27][root][INFO] - Training Epoch: 1/2, step 1417/7134 completed (loss: 0.018241897225379944, acc: 0.9965986609458923)
[2025-02-13 02:36:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:27][root][INFO] - Training Epoch: 1/2, step 1418/7134 completed (loss: 0.02096170373260975, acc: 0.9942747950553894)
[2025-02-13 02:36:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:27][root][INFO] - Training Epoch: 1/2, step 1419/7134 completed (loss: 0.02903100475668907, acc: 0.9932523369789124)
[2025-02-13 02:36:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:28][root][INFO] - Training Epoch: 1/2, step 1420/7134 completed (loss: 0.028815405443310738, acc: 0.9919137358665466)
[2025-02-13 02:36:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:28][root][INFO] - Training Epoch: 1/2, step 1421/7134 completed (loss: 0.034612856805324554, acc: 0.9855769276618958)
[2025-02-13 02:36:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:29][root][INFO] - Training Epoch: 1/2, step 1422/7134 completed (loss: 0.06812191009521484, acc: 0.9829059839248657)
[2025-02-13 02:36:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:29][root][INFO] - Training Epoch: 1/2, step 1423/7134 completed (loss: 0.05698537081480026, acc: 0.9877551198005676)
[2025-02-13 02:36:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:30][root][INFO] - Training Epoch: 1/2, step 1424/7134 completed (loss: 0.09136587381362915, acc: 0.9752851724624634)
[2025-02-13 02:36:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:30][root][INFO] - Training Epoch: 1/2, step 1425/7134 completed (loss: 0.08014368265867233, acc: 0.9776452779769897)
[2025-02-13 02:36:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:31][root][INFO] - Training Epoch: 1/2, step 1426/7134 completed (loss: 0.07425007969141006, acc: 0.9794344305992126)
[2025-02-13 02:36:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:31][root][INFO] - Training Epoch: 1/2, step 1427/7134 completed (loss: 0.08630050718784332, acc: 0.9744572043418884)
[2025-02-13 02:36:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:32][root][INFO] - Training Epoch: 1/2, step 1428/7134 completed (loss: 0.31939199566841125, acc: 0.9184290170669556)
[2025-02-13 02:36:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:32][root][INFO] - Training Epoch: 1/2, step 1429/7134 completed (loss: 0.03658278286457062, acc: 0.9916840195655823)
[2025-02-13 02:36:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:33][root][INFO] - Training Epoch: 1/2, step 1430/7134 completed (loss: 0.07655749469995499, acc: 0.9833948612213135)
[2025-02-13 02:36:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:33][root][INFO] - Training Epoch: 1/2, step 1431/7134 completed (loss: 0.04240676015615463, acc: 0.9891451597213745)
[2025-02-13 02:36:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:33][root][INFO] - Training Epoch: 1/2, step 1432/7134 completed (loss: 0.03283295780420303, acc: 0.992277979850769)
[2025-02-13 02:36:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:34][root][INFO] - Training Epoch: 1/2, step 1433/7134 completed (loss: 0.04286465048789978, acc: 0.9842932224273682)
[2025-02-13 02:36:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:34][root][INFO] - Training Epoch: 1/2, step 1434/7134 completed (loss: 0.07437723875045776, acc: 0.9785810112953186)
[2025-02-13 02:36:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:35][root][INFO] - Training Epoch: 1/2, step 1435/7134 completed (loss: 0.05311928316950798, acc: 0.9794721603393555)
[2025-02-13 02:36:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:35][root][INFO] - Training Epoch: 1/2, step 1436/7134 completed (loss: 0.08821449428796768, acc: 0.9657360315322876)
[2025-02-13 02:36:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:36][root][INFO] - Training Epoch: 1/2, step 1437/7134 completed (loss: 0.05040636286139488, acc: 0.976323127746582)
[2025-02-13 02:36:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:36][root][INFO] - Training Epoch: 1/2, step 1438/7134 completed (loss: 0.07041849195957184, acc: 0.9824817776679993)
[2025-02-13 02:36:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:37][root][INFO] - Training Epoch: 1/2, step 1439/7134 completed (loss: 0.04894397407770157, acc: 0.9814471006393433)
[2025-02-13 02:36:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:37][root][INFO] - Training Epoch: 1/2, step 1440/7134 completed (loss: 0.10373391211032867, acc: 0.9672977328300476)
[2025-02-13 02:36:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:38][root][INFO] - Training Epoch: 1/2, step 1441/7134 completed (loss: 0.05941712111234665, acc: 0.9824047088623047)
[2025-02-13 02:36:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:38][root][INFO] - Training Epoch: 1/2, step 1442/7134 completed (loss: 0.037937063723802567, acc: 0.9880383014678955)
[2025-02-13 02:36:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:38][root][INFO] - Training Epoch: 1/2, step 1443/7134 completed (loss: 0.06916452944278717, acc: 0.9771528840065002)
[2025-02-13 02:36:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:39][root][INFO] - Training Epoch: 1/2, step 1444/7134 completed (loss: 0.056536223739385605, acc: 0.9824903011322021)
[2025-02-13 02:36:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:39][root][INFO] - Training Epoch: 1/2, step 1445/7134 completed (loss: 0.07620605826377869, acc: 0.9750415682792664)
[2025-02-13 02:36:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:40][root][INFO] - Training Epoch: 1/2, step 1446/7134 completed (loss: 0.03208057954907417, acc: 0.9928876161575317)
[2025-02-13 02:36:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:40][root][INFO] - Training Epoch: 1/2, step 1447/7134 completed (loss: 0.06278809159994125, acc: 0.9773049354553223)
[2025-02-13 02:36:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:41][root][INFO] - Training Epoch: 1/2, step 1448/7134 completed (loss: 0.10122301429510117, acc: 0.9724576473236084)
[2025-02-13 02:36:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:41][root][INFO] - Training Epoch: 1/2, step 1449/7134 completed (loss: 0.0968809649348259, acc: 0.9733145833015442)
[2025-02-13 02:36:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:41][root][INFO] - Training Epoch: 1/2, step 1450/7134 completed (loss: 0.0337325856089592, acc: 0.9898403286933899)
[2025-02-13 02:36:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:42][root][INFO] - Training Epoch: 1/2, step 1451/7134 completed (loss: 0.037088099867105484, acc: 0.9926793575286865)
[2025-02-13 02:36:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:42][root][INFO] - Training Epoch: 1/2, step 1452/7134 completed (loss: 0.07987931370735168, acc: 0.9878472089767456)
[2025-02-13 02:36:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:43][root][INFO] - Training Epoch: 1/2, step 1453/7134 completed (loss: 0.08333925902843475, acc: 0.9793977737426758)
[2025-02-13 02:36:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:43][root][INFO] - Training Epoch: 1/2, step 1454/7134 completed (loss: 0.08647415786981583, acc: 0.9798115491867065)
[2025-02-13 02:36:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:44][root][INFO] - Training Epoch: 1/2, step 1455/7134 completed (loss: 0.07187552005052567, acc: 0.9798164963722229)
[2025-02-13 02:36:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:44][root][INFO] - Training Epoch: 1/2, step 1456/7134 completed (loss: 0.03331681340932846, acc: 0.9912663698196411)
[2025-02-13 02:36:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:45][root][INFO] - Training Epoch: 1/2, step 1457/7134 completed (loss: 0.04671946540474892, acc: 0.9860464930534363)
[2025-02-13 02:36:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:45][root][INFO] - Training Epoch: 1/2, step 1458/7134 completed (loss: 0.027162855491042137, acc: 0.9926362037658691)
[2025-02-13 02:36:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:45][root][INFO] - Training Epoch: 1/2, step 1459/7134 completed (loss: 0.07141134142875671, acc: 0.9857904314994812)
[2025-02-13 02:36:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:46][root][INFO] - Training Epoch: 1/2, step 1460/7134 completed (loss: 0.10502906888723373, acc: 0.9736841917037964)
[2025-02-13 02:36:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:46][root][INFO] - Training Epoch: 1/2, step 1461/7134 completed (loss: 0.06438181549310684, acc: 0.979266345500946)
[2025-02-13 02:36:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:47][root][INFO] - Training Epoch: 1/2, step 1462/7134 completed (loss: 0.08505617827177048, acc: 0.9742063283920288)
[2025-02-13 02:36:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:47][root][INFO] - Training Epoch: 1/2, step 1463/7134 completed (loss: 0.07428167760372162, acc: 0.9804741740226746)
[2025-02-13 02:36:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:48][root][INFO] - Training Epoch: 1/2, step 1464/7134 completed (loss: 0.10885102301836014, acc: 0.9714714884757996)
[2025-02-13 02:36:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:48][root][INFO] - Training Epoch: 1/2, step 1465/7134 completed (loss: 0.07651649415493011, acc: 0.9780564308166504)
[2025-02-13 02:36:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:48][root][INFO] - Training Epoch: 1/2, step 1466/7134 completed (loss: 0.035028859972953796, acc: 0.9872521162033081)
[2025-02-13 02:36:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:49][root][INFO] - Training Epoch: 1/2, step 1467/7134 completed (loss: 0.028735248371958733, acc: 0.9899135231971741)
[2025-02-13 02:36:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:49][root][INFO] - Training Epoch: 1/2, step 1468/7134 completed (loss: 0.04493042081594467, acc: 0.9881109595298767)
[2025-02-13 02:36:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:50][root][INFO] - Training Epoch: 1/2, step 1469/7134 completed (loss: 0.03138439357280731, acc: 0.9932705163955688)
[2025-02-13 02:36:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:50][root][INFO] - Training Epoch: 1/2, step 1470/7134 completed (loss: 0.05203995853662491, acc: 0.9826589822769165)
[2025-02-13 02:36:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:51][root][INFO] - Training Epoch: 1/2, step 1471/7134 completed (loss: 0.061487384140491486, acc: 0.9822379946708679)
[2025-02-13 02:36:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:51][root][INFO] - Training Epoch: 1/2, step 1472/7134 completed (loss: 0.0350777693092823, acc: 0.9886792302131653)
[2025-02-13 02:36:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:52][root][INFO] - Training Epoch: 1/2, step 1473/7134 completed (loss: 0.0497405044734478, acc: 0.9866468906402588)
[2025-02-13 02:36:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:52][root][INFO] - Training Epoch: 1/2, step 1474/7134 completed (loss: 0.0642509013414383, acc: 0.9832214713096619)
[2025-02-13 02:36:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:53][root][INFO] - Training Epoch: 1/2, step 1475/7134 completed (loss: 0.07318640500307083, acc: 0.9854881167411804)
[2025-02-13 02:36:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:53][root][INFO] - Training Epoch: 1/2, step 1476/7134 completed (loss: 0.05242118611931801, acc: 0.9844720363616943)
[2025-02-13 02:36:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:53][root][INFO] - Training Epoch: 1/2, step 1477/7134 completed (loss: 0.04663985222578049, acc: 0.9895287752151489)
[2025-02-13 02:36:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:54][root][INFO] - Training Epoch: 1/2, step 1478/7134 completed (loss: 0.03807729110121727, acc: 0.985401451587677)
[2025-02-13 02:36:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:54][root][INFO] - Training Epoch: 1/2, step 1479/7134 completed (loss: 0.03734457865357399, acc: 0.9870689511299133)
[2025-02-13 02:36:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:55][root][INFO] - Training Epoch: 1/2, step 1480/7134 completed (loss: 0.024024488404393196, acc: 0.9922077655792236)
[2025-02-13 02:36:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:55][root][INFO] - Training Epoch: 1/2, step 1481/7134 completed (loss: 0.04036492481827736, acc: 0.9907894730567932)
[2025-02-13 02:36:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:56][root][INFO] - Training Epoch: 1/2, step 1482/7134 completed (loss: 0.05323953926563263, acc: 0.9872159361839294)
[2025-02-13 02:36:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:56][root][INFO] - Training Epoch: 1/2, step 1483/7134 completed (loss: 0.037778642028570175, acc: 0.9873617887496948)
[2025-02-13 02:36:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:57][root][INFO] - Training Epoch: 1/2, step 1484/7134 completed (loss: 0.07856008410453796, acc: 0.9783236980438232)
[2025-02-13 02:36:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:57][root][INFO] - Training Epoch: 1/2, step 1485/7134 completed (loss: 0.04042113199830055, acc: 0.9886220097541809)
[2025-02-13 02:36:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:58][root][INFO] - Training Epoch: 1/2, step 1486/7134 completed (loss: 0.06523002684116364, acc: 0.9837618470191956)
[2025-02-13 02:36:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:58][root][INFO] - Training Epoch: 1/2, step 1487/7134 completed (loss: 0.03336935490369797, acc: 0.9895424842834473)
[2025-02-13 02:36:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:58][root][INFO] - Training Epoch: 1/2, step 1488/7134 completed (loss: 0.05544664338231087, acc: 0.9850267171859741)
[2025-02-13 02:36:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:59][root][INFO] - Training Epoch: 1/2, step 1489/7134 completed (loss: 0.03846448287367821, acc: 0.9883177280426025)
[2025-02-13 02:36:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:36:59][root][INFO] - Training Epoch: 1/2, step 1490/7134 completed (loss: 0.028107115998864174, acc: 0.9936507940292358)
[2025-02-13 02:37:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:00][root][INFO] - Training Epoch: 1/2, step 1491/7134 completed (loss: 0.024767663329839706, acc: 0.9931034445762634)
[2025-02-13 02:37:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:00][root][INFO] - Training Epoch: 1/2, step 1492/7134 completed (loss: 0.055896859616041183, acc: 0.9849187731742859)
[2025-02-13 02:37:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:01][root][INFO] - Training Epoch: 1/2, step 1493/7134 completed (loss: 0.041098956018686295, acc: 0.9884792566299438)
[2025-02-13 02:37:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:01][root][INFO] - Training Epoch: 1/2, step 1494/7134 completed (loss: 0.02985842153429985, acc: 0.9902676343917847)
[2025-02-13 02:37:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:02][root][INFO] - Training Epoch: 1/2, step 1495/7134 completed (loss: 0.026946863159537315, acc: 0.9952095746994019)
[2025-02-13 02:37:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:02][root][INFO] - Training Epoch: 1/2, step 1496/7134 completed (loss: 0.05569081008434296, acc: 0.9871794581413269)
[2025-02-13 02:37:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:03][root][INFO] - Training Epoch: 1/2, step 1497/7134 completed (loss: 0.032664742320775986, acc: 0.9932126402854919)
[2025-02-13 02:37:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:03][root][INFO] - Training Epoch: 1/2, step 1498/7134 completed (loss: 0.028224052861332893, acc: 0.9894598126411438)
[2025-02-13 02:37:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:04][root][INFO] - Training Epoch: 1/2, step 1499/7134 completed (loss: 0.033537041395902634, acc: 0.991696298122406)
[2025-02-13 02:37:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:04][root][INFO] - Training Epoch: 1/2, step 1500/7134 completed (loss: 0.03426067903637886, acc: 0.9867549538612366)
[2025-02-13 02:37:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:05][root][INFO] - Training Epoch: 1/2, step 1501/7134 completed (loss: 0.04788706451654434, acc: 0.991150438785553)
[2025-02-13 02:37:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:05][root][INFO] - Training Epoch: 1/2, step 1502/7134 completed (loss: 0.05323871225118637, acc: 0.9864712357521057)
[2025-02-13 02:37:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:05][root][INFO] - Training Epoch: 1/2, step 1503/7134 completed (loss: 0.09897790104150772, acc: 0.9786931872367859)
[2025-02-13 02:37:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:06][root][INFO] - Training Epoch: 1/2, step 1504/7134 completed (loss: 0.03994749113917351, acc: 0.9874213933944702)
[2025-02-13 02:37:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:06][root][INFO] - Training Epoch: 1/2, step 1505/7134 completed (loss: 0.04052850231528282, acc: 0.9875389337539673)
[2025-02-13 02:37:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:07][root][INFO] - Training Epoch: 1/2, step 1506/7134 completed (loss: 0.06432393193244934, acc: 0.9817629456520081)
[2025-02-13 02:37:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:07][root][INFO] - Training Epoch: 1/2, step 1507/7134 completed (loss: 0.031233377754688263, acc: 0.9921466112136841)
[2025-02-13 02:37:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:08][root][INFO] - Training Epoch: 1/2, step 1508/7134 completed (loss: 0.03308641538023949, acc: 0.9890590906143188)
[2025-02-13 02:37:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:08][root][INFO] - Training Epoch: 1/2, step 1509/7134 completed (loss: 0.027106305584311485, acc: 0.9929971694946289)
[2025-02-13 02:37:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:09][root][INFO] - Training Epoch: 1/2, step 1510/7134 completed (loss: 0.20772729814052582, acc: 0.9666666388511658)
[2025-02-13 02:37:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:09][root][INFO] - Training Epoch: 1/2, step 1511/7134 completed (loss: 0.1066039502620697, acc: 0.9730496406555176)
[2025-02-13 02:37:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:09][root][INFO] - Training Epoch: 1/2, step 1512/7134 completed (loss: 0.05562516674399376, acc: 0.9817351698875427)
[2025-02-13 02:37:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:10][root][INFO] - Training Epoch: 1/2, step 1513/7134 completed (loss: 0.030036361888051033, acc: 0.9907407164573669)
[2025-02-13 02:37:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:10][root][INFO] - Training Epoch: 1/2, step 1514/7134 completed (loss: 0.07023066282272339, acc: 0.9875776171684265)
[2025-02-13 02:37:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:11][root][INFO] - Training Epoch: 1/2, step 1515/7134 completed (loss: 0.07593271136283875, acc: 0.9766355156898499)
[2025-02-13 02:37:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:11][root][INFO] - Training Epoch: 1/2, step 1516/7134 completed (loss: 0.0601172037422657, acc: 0.9772079586982727)
[2025-02-13 02:37:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:12][root][INFO] - Training Epoch: 1/2, step 1517/7134 completed (loss: 0.04948462173342705, acc: 0.9810924530029297)
[2025-02-13 02:37:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:12][root][INFO] - Training Epoch: 1/2, step 1518/7134 completed (loss: 0.031933221966028214, acc: 0.9855421781539917)
[2025-02-13 02:37:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:12][root][INFO] - Training Epoch: 1/2, step 1519/7134 completed (loss: 0.03424890711903572, acc: 0.9907833933830261)
[2025-02-13 02:37:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:13][root][INFO] - Training Epoch: 1/2, step 1520/7134 completed (loss: 0.03671425208449364, acc: 0.9921996593475342)
[2025-02-13 02:37:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:13][root][INFO] - Training Epoch: 1/2, step 1521/7134 completed (loss: 0.12213382124900818, acc: 0.9646182656288147)
[2025-02-13 02:37:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:14][root][INFO] - Training Epoch: 1/2, step 1522/7134 completed (loss: 0.12733091413974762, acc: 0.9576107859611511)
[2025-02-13 02:37:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:14][root][INFO] - Training Epoch: 1/2, step 1523/7134 completed (loss: 0.15752990543842316, acc: 0.9490255117416382)
[2025-02-13 02:37:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:15][root][INFO] - Training Epoch: 1/2, step 1524/7134 completed (loss: 0.04858457297086716, acc: 0.9932885766029358)
[2025-02-13 02:37:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:15][root][INFO] - Training Epoch: 1/2, step 1525/7134 completed (loss: 0.05011749267578125, acc: 0.9841628670692444)
[2025-02-13 02:37:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:16][root][INFO] - Training Epoch: 1/2, step 1526/7134 completed (loss: 0.10533533990383148, acc: 0.9782293438911438)
[2025-02-13 02:37:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:16][root][INFO] - Training Epoch: 1/2, step 1527/7134 completed (loss: 0.14176857471466064, acc: 0.9649415612220764)
[2025-02-13 02:37:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:17][root][INFO] - Training Epoch: 1/2, step 1528/7134 completed (loss: 0.0849541425704956, acc: 0.9710144996643066)
[2025-02-13 02:37:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:17][root][INFO] - Training Epoch: 1/2, step 1529/7134 completed (loss: 0.03699098899960518, acc: 0.9881094098091125)
[2025-02-13 02:37:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:17][root][INFO] - Training Epoch: 1/2, step 1530/7134 completed (loss: 0.07293907552957535, acc: 0.9789674878120422)
[2025-02-13 02:37:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:18][root][INFO] - Training Epoch: 1/2, step 1531/7134 completed (loss: 0.196566641330719, acc: 0.945035457611084)
[2025-02-13 02:37:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:19][root][INFO] - Training Epoch: 1/2, step 1532/7134 completed (loss: 0.060871221125125885, acc: 0.9826589822769165)
[2025-02-13 02:37:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:19][root][INFO] - Training Epoch: 1/2, step 1533/7134 completed (loss: 0.059237562119960785, acc: 0.9792531132698059)
[2025-02-13 02:37:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:19][root][INFO] - Training Epoch: 1/2, step 1534/7134 completed (loss: 0.06607941538095474, acc: 0.9819944500923157)
[2025-02-13 02:37:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:20][root][INFO] - Training Epoch: 1/2, step 1535/7134 completed (loss: 0.05722760781645775, acc: 0.9796407222747803)
[2025-02-13 02:37:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:20][root][INFO] - Training Epoch: 1/2, step 1536/7134 completed (loss: 0.046463340520858765, acc: 0.983565092086792)
[2025-02-13 02:37:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:21][root][INFO] - Training Epoch: 1/2, step 1537/7134 completed (loss: 0.06293351203203201, acc: 0.981675386428833)
[2025-02-13 02:37:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:21][root][INFO] - Training Epoch: 1/2, step 1538/7134 completed (loss: 0.08076484501361847, acc: 0.9795134663581848)
[2025-02-13 02:37:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:22][root][INFO] - Training Epoch: 1/2, step 1539/7134 completed (loss: 0.053726308047771454, acc: 0.9828721880912781)
[2025-02-13 02:37:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:22][root][INFO] - Training Epoch: 1/2, step 1540/7134 completed (loss: 0.04532631114125252, acc: 0.9839141964912415)
[2025-02-13 02:37:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:23][root][INFO] - Training Epoch: 1/2, step 1541/7134 completed (loss: 0.0746108889579773, acc: 0.9786163568496704)
[2025-02-13 02:37:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:23][root][INFO] - Training Epoch: 1/2, step 1542/7134 completed (loss: 0.05271238088607788, acc: 0.986522912979126)
[2025-02-13 02:37:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:24][root][INFO] - Training Epoch: 1/2, step 1543/7134 completed (loss: 0.03413238748908043, acc: 0.9934425950050354)
[2025-02-13 02:37:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:24][root][INFO] - Training Epoch: 1/2, step 1544/7134 completed (loss: 0.036951884627342224, acc: 0.987864077091217)
[2025-02-13 02:37:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:25][root][INFO] - Training Epoch: 1/2, step 1545/7134 completed (loss: 0.040369100868701935, acc: 0.9870874881744385)
[2025-02-13 02:37:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:25][root][INFO] - Training Epoch: 1/2, step 1546/7134 completed (loss: 0.08598583936691284, acc: 0.9803646802902222)
[2025-02-13 02:37:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:25][root][INFO] - Training Epoch: 1/2, step 1547/7134 completed (loss: 0.04817616939544678, acc: 0.983582079410553)
[2025-02-13 02:37:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:26][root][INFO] - Training Epoch: 1/2, step 1548/7134 completed (loss: 0.03261493146419525, acc: 0.9920364022254944)
[2025-02-13 02:37:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:26][root][INFO] - Training Epoch: 1/2, step 1549/7134 completed (loss: 0.06093839183449745, acc: 0.9800664186477661)
[2025-02-13 02:37:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:27][root][INFO] - Training Epoch: 1/2, step 1550/7134 completed (loss: 0.03474965691566467, acc: 0.9873257279396057)
[2025-02-13 02:37:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:27][root][INFO] - Training Epoch: 1/2, step 1551/7134 completed (loss: 0.048353590071201324, acc: 0.984415590763092)
[2025-02-13 02:37:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:28][root][INFO] - Training Epoch: 1/2, step 1552/7134 completed (loss: 0.04420923441648483, acc: 0.9892241358757019)
[2025-02-13 02:37:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:28][root][INFO] - Training Epoch: 1/2, step 1553/7134 completed (loss: 0.05247899517416954, acc: 0.9869961142539978)
[2025-02-13 02:37:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:29][root][INFO] - Training Epoch: 1/2, step 1554/7134 completed (loss: 0.05741383135318756, acc: 0.979706883430481)
[2025-02-13 02:37:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:29][root][INFO] - Training Epoch: 1/2, step 1555/7134 completed (loss: 0.05217704549431801, acc: 0.9854423403739929)
[2025-02-13 02:37:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:30][root][INFO] - Training Epoch: 1/2, step 1556/7134 completed (loss: 0.05493931099772453, acc: 0.9870740175247192)
[2025-02-13 02:37:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:30][root][INFO] - Training Epoch: 1/2, step 1557/7134 completed (loss: 0.03275910019874573, acc: 0.9899328947067261)
[2025-02-13 02:37:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:31][root][INFO] - Training Epoch: 1/2, step 1558/7134 completed (loss: 0.0444168858230114, acc: 0.9845288395881653)
[2025-02-13 02:37:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:31][root][INFO] - Training Epoch: 1/2, step 1559/7134 completed (loss: 0.01789475604891777, acc: 0.9919678568840027)
[2025-02-13 02:37:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:31][root][INFO] - Training Epoch: 1/2, step 1560/7134 completed (loss: 0.05184004828333855, acc: 0.9860228896141052)
[2025-02-13 02:37:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:32][root][INFO] - Training Epoch: 1/2, step 1561/7134 completed (loss: 0.10015705227851868, acc: 0.9739978313446045)
[2025-02-13 02:37:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:32][root][INFO] - Training Epoch: 1/2, step 1562/7134 completed (loss: 0.058125752955675125, acc: 0.9878970980644226)
[2025-02-13 02:37:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:33][root][INFO] - Training Epoch: 1/2, step 1563/7134 completed (loss: 0.05398138612508774, acc: 0.9862856864929199)
[2025-02-13 02:37:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:33][root][INFO] - Training Epoch: 1/2, step 1564/7134 completed (loss: 0.0938352644443512, acc: 0.9764851331710815)
[2025-02-13 02:37:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:34][root][INFO] - Training Epoch: 1/2, step 1565/7134 completed (loss: 0.07286088913679123, acc: 0.9815126061439514)
[2025-02-13 02:37:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:34][root][INFO] - Training Epoch: 1/2, step 1566/7134 completed (loss: 0.04834955558180809, acc: 0.9822161197662354)
[2025-02-13 02:37:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:35][root][INFO] - Training Epoch: 1/2, step 1567/7134 completed (loss: 0.06420587748289108, acc: 0.9788029789924622)
[2025-02-13 02:37:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:35][root][INFO] - Training Epoch: 1/2, step 1568/7134 completed (loss: 0.07910846173763275, acc: 0.9798816442489624)
[2025-02-13 02:37:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:36][root][INFO] - Training Epoch: 1/2, step 1569/7134 completed (loss: 0.06714130192995071, acc: 0.9795321822166443)
[2025-02-13 02:37:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:36][root][INFO] - Training Epoch: 1/2, step 1570/7134 completed (loss: 0.09083578735589981, acc: 0.9801543354988098)
[2025-02-13 02:37:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:36][root][INFO] - Training Epoch: 1/2, step 1571/7134 completed (loss: 0.06139209493994713, acc: 0.9845789074897766)
[2025-02-13 02:37:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:37][root][INFO] - Training Epoch: 1/2, step 1572/7134 completed (loss: 0.054102830588817596, acc: 0.982332170009613)
[2025-02-13 02:37:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:37][root][INFO] - Training Epoch: 1/2, step 1573/7134 completed (loss: 0.03500053286552429, acc: 0.9927431344985962)
[2025-02-13 02:37:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:38][root][INFO] - Training Epoch: 1/2, step 1574/7134 completed (loss: 0.06730733811855316, acc: 0.97921222448349)
[2025-02-13 02:37:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:38][root][INFO] - Training Epoch: 1/2, step 1575/7134 completed (loss: 0.053515758365392685, acc: 0.9855875968933105)
[2025-02-13 02:37:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:39][root][INFO] - Training Epoch: 1/2, step 1576/7134 completed (loss: 0.0560854971408844, acc: 0.985981285572052)
[2025-02-13 02:37:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:39][root][INFO] - Training Epoch: 1/2, step 1577/7134 completed (loss: 0.047696832567453384, acc: 0.987043559551239)
[2025-02-13 02:37:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:40][root][INFO] - Training Epoch: 1/2, step 1578/7134 completed (loss: 0.04595642164349556, acc: 0.9851149916648865)
[2025-02-13 02:37:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:40][root][INFO] - Training Epoch: 1/2, step 1579/7134 completed (loss: 0.04756254330277443, acc: 0.9884792566299438)
[2025-02-13 02:37:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:41][root][INFO] - Training Epoch: 1/2, step 1580/7134 completed (loss: 0.1047629714012146, acc: 0.9735682606697083)
[2025-02-13 02:37:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:41][root][INFO] - Training Epoch: 1/2, step 1581/7134 completed (loss: 0.12793302536010742, acc: 0.9629629850387573)
[2025-02-13 02:37:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:42][root][INFO] - Training Epoch: 1/2, step 1582/7134 completed (loss: 0.08061138540506363, acc: 0.9765258431434631)
[2025-02-13 02:37:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:42][root][INFO] - Training Epoch: 1/2, step 1583/7134 completed (loss: 0.046786464750766754, acc: 0.9871060252189636)
[2025-02-13 02:37:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:43][root][INFO] - Training Epoch: 1/2, step 1584/7134 completed (loss: 0.048818089067935944, acc: 0.9850106835365295)
[2025-02-13 02:37:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:43][root][INFO] - Training Epoch: 1/2, step 1585/7134 completed (loss: 0.045491207391023636, acc: 0.9893993139266968)
[2025-02-13 02:37:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:44][root][INFO] - Training Epoch: 1/2, step 1586/7134 completed (loss: 0.07211674004793167, acc: 0.978622317314148)
[2025-02-13 02:37:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:44][root][INFO] - Training Epoch: 1/2, step 1587/7134 completed (loss: 0.05310414731502533, acc: 0.9905808568000793)
[2025-02-13 02:37:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:45][root][INFO] - Training Epoch: 1/2, step 1588/7134 completed (loss: 0.07037258893251419, acc: 0.9840213060379028)
[2025-02-13 02:37:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:45][root][INFO] - Training Epoch: 1/2, step 1589/7134 completed (loss: 0.04439394921064377, acc: 0.99370276927948)
[2025-02-13 02:37:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:45][root][INFO] - Training Epoch: 1/2, step 1590/7134 completed (loss: 0.04530354216694832, acc: 0.9864048361778259)
[2025-02-13 02:37:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:46][root][INFO] - Training Epoch: 1/2, step 1591/7134 completed (loss: 0.05681540444493294, acc: 0.9830795526504517)
[2025-02-13 02:37:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:46][root][INFO] - Training Epoch: 1/2, step 1592/7134 completed (loss: 0.04845109581947327, acc: 0.9814550876617432)
[2025-02-13 02:37:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:47][root][INFO] - Training Epoch: 1/2, step 1593/7134 completed (loss: 0.0581836961209774, acc: 0.9850574731826782)
[2025-02-13 02:37:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:47][root][INFO] - Training Epoch: 1/2, step 1594/7134 completed (loss: 0.03765618056058884, acc: 0.9900826215744019)
[2025-02-13 02:37:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:48][root][INFO] - Training Epoch: 1/2, step 1595/7134 completed (loss: 0.05349159613251686, acc: 0.9928315281867981)
[2025-02-13 02:37:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:48][root][INFO] - Training Epoch: 1/2, step 1596/7134 completed (loss: 0.07462222874164581, acc: 0.9795275330543518)
[2025-02-13 02:37:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:49][root][INFO] - Training Epoch: 1/2, step 1597/7134 completed (loss: 0.019035495817661285, acc: 0.9942693114280701)
[2025-02-13 02:37:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:49][root][INFO] - Training Epoch: 1/2, step 1598/7134 completed (loss: 0.09152818471193314, acc: 0.9740518927574158)
[2025-02-13 02:37:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:50][root][INFO] - Training Epoch: 1/2, step 1599/7134 completed (loss: 0.11356326937675476, acc: 0.9735537171363831)
[2025-02-13 02:37:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:50][root][INFO] - Training Epoch: 1/2, step 1600/7134 completed (loss: 0.059580378234386444, acc: 0.988452672958374)
[2025-02-13 02:37:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:50][root][INFO] - Training Epoch: 1/2, step 1601/7134 completed (loss: 0.032491013407707214, acc: 0.993122398853302)
[2025-02-13 02:37:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:51][root][INFO] - Training Epoch: 1/2, step 1602/7134 completed (loss: 0.06376691907644272, acc: 0.9834087491035461)
[2025-02-13 02:37:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:51][root][INFO] - Training Epoch: 1/2, step 1603/7134 completed (loss: 0.10448654741048813, acc: 0.9795396327972412)
[2025-02-13 02:37:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:52][root][INFO] - Training Epoch: 1/2, step 1604/7134 completed (loss: 0.03833838179707527, acc: 0.9907161593437195)
[2025-02-13 02:37:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:52][root][INFO] - Training Epoch: 1/2, step 1605/7134 completed (loss: 0.05256364494562149, acc: 0.9869621992111206)
[2025-02-13 02:37:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:53][root][INFO] - Training Epoch: 1/2, step 1606/7134 completed (loss: 0.032036613672971725, acc: 0.992343008518219)
[2025-02-13 02:37:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:53][root][INFO] - Training Epoch: 1/2, step 1607/7134 completed (loss: 0.05240198224782944, acc: 0.9877800345420837)
[2025-02-13 02:37:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:54][root][INFO] - Training Epoch: 1/2, step 1608/7134 completed (loss: 0.033347371965646744, acc: 0.9888476133346558)
[2025-02-13 02:37:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:54][root][INFO] - Training Epoch: 1/2, step 1609/7134 completed (loss: 0.06193344667553902, acc: 0.985981285572052)
[2025-02-13 02:37:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:54][root][INFO] - Training Epoch: 1/2, step 1610/7134 completed (loss: 0.044636838138103485, acc: 0.989983320236206)
[2025-02-13 02:37:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:55][root][INFO] - Training Epoch: 1/2, step 1611/7134 completed (loss: 0.07533327490091324, acc: 0.9828042387962341)
[2025-02-13 02:37:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:55][root][INFO] - Training Epoch: 1/2, step 1612/7134 completed (loss: 0.06711366027593613, acc: 0.9821882843971252)
[2025-02-13 02:37:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:56][root][INFO] - Training Epoch: 1/2, step 1613/7134 completed (loss: 0.06968715786933899, acc: 0.9841897487640381)
[2025-02-13 02:37:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:56][root][INFO] - Training Epoch: 1/2, step 1614/7134 completed (loss: 0.05493684485554695, acc: 0.9882044792175293)
[2025-02-13 02:37:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:57][root][INFO] - Training Epoch: 1/2, step 1615/7134 completed (loss: 0.05686767399311066, acc: 0.9798657894134521)
[2025-02-13 02:37:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:57][root][INFO] - Training Epoch: 1/2, step 1616/7134 completed (loss: 0.03044893778860569, acc: 0.9913420081138611)
[2025-02-13 02:37:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:58][root][INFO] - Training Epoch: 1/2, step 1617/7134 completed (loss: 0.06421592086553574, acc: 0.9890795350074768)
[2025-02-13 02:37:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:58][root][INFO] - Training Epoch: 1/2, step 1618/7134 completed (loss: 0.08935116976499557, acc: 0.9754385948181152)
[2025-02-13 02:37:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:59][root][INFO] - Training Epoch: 1/2, step 1619/7134 completed (loss: 0.07112613320350647, acc: 0.9809264540672302)
[2025-02-13 02:37:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:37:59][root][INFO] - Training Epoch: 1/2, step 1620/7134 completed (loss: 0.08694099634885788, acc: 0.974281370639801)
[2025-02-13 02:37:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:00][root][INFO] - Training Epoch: 1/2, step 1621/7134 completed (loss: 0.09190884977579117, acc: 0.9756757020950317)
[2025-02-13 02:38:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:00][root][INFO] - Training Epoch: 1/2, step 1622/7134 completed (loss: 0.07025031000375748, acc: 0.9791086316108704)
[2025-02-13 02:38:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:00][root][INFO] - Training Epoch: 1/2, step 1623/7134 completed (loss: 0.06749457120895386, acc: 0.9803370833396912)
[2025-02-13 02:38:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:01][root][INFO] - Training Epoch: 1/2, step 1624/7134 completed (loss: 0.06083526089787483, acc: 0.98531574010849)
[2025-02-13 02:38:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:01][root][INFO] - Training Epoch: 1/2, step 1625/7134 completed (loss: 0.0380067378282547, acc: 0.9900285005569458)
[2025-02-13 02:38:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:02][root][INFO] - Training Epoch: 1/2, step 1626/7134 completed (loss: 0.09745806455612183, acc: 0.9745370149612427)
[2025-02-13 02:38:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:02][root][INFO] - Training Epoch: 1/2, step 1627/7134 completed (loss: 0.036889951676130295, acc: 0.989051103591919)
[2025-02-13 02:38:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:03][root][INFO] - Training Epoch: 1/2, step 1628/7134 completed (loss: 0.014657176099717617, acc: 0.9956268072128296)
[2025-02-13 02:38:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:03][root][INFO] - Training Epoch: 1/2, step 1629/7134 completed (loss: 0.03533163294196129, acc: 0.9872727394104004)
[2025-02-13 02:38:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:03][root][INFO] - Training Epoch: 1/2, step 1630/7134 completed (loss: 0.059485480189323425, acc: 0.9896296262741089)
[2025-02-13 02:38:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:04][root][INFO] - Training Epoch: 1/2, step 1631/7134 completed (loss: 0.08489973843097687, acc: 0.9758842587471008)
[2025-02-13 02:38:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:04][root][INFO] - Training Epoch: 1/2, step 1632/7134 completed (loss: 0.018989920616149902, acc: 0.9917080998420715)
[2025-02-13 02:38:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:05][root][INFO] - Training Epoch: 1/2, step 1633/7134 completed (loss: 0.03495495021343231, acc: 0.9874411225318909)
[2025-02-13 02:38:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:05][root][INFO] - Training Epoch: 1/2, step 1634/7134 completed (loss: 0.02518153004348278, acc: 0.992343008518219)
[2025-02-13 02:38:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:06][root][INFO] - Training Epoch: 1/2, step 1635/7134 completed (loss: 0.024544475600123405, acc: 0.9918533563613892)
[2025-02-13 02:38:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:06][root][INFO] - Training Epoch: 1/2, step 1636/7134 completed (loss: 0.06004487723112106, acc: 0.9866666793823242)
[2025-02-13 02:38:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:07][root][INFO] - Training Epoch: 1/2, step 1637/7134 completed (loss: 0.021179530769586563, acc: 0.9929378628730774)
[2025-02-13 02:38:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:07][root][INFO] - Training Epoch: 1/2, step 1638/7134 completed (loss: 0.07400437444448471, acc: 0.979626476764679)
[2025-02-13 02:38:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:07][root][INFO] - Training Epoch: 1/2, step 1639/7134 completed (loss: 0.139547660946846, acc: 0.9675036668777466)
[2025-02-13 02:38:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:08][root][INFO] - Training Epoch: 1/2, step 1640/7134 completed (loss: 0.0649079903960228, acc: 0.9771341681480408)
[2025-02-13 02:38:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:08][root][INFO] - Training Epoch: 1/2, step 1641/7134 completed (loss: 0.03625863417983055, acc: 0.9870689511299133)
[2025-02-13 02:38:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:09][root][INFO] - Training Epoch: 1/2, step 1642/7134 completed (loss: 0.1037086620926857, acc: 0.9723684191703796)
[2025-02-13 02:38:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:09][root][INFO] - Training Epoch: 1/2, step 1643/7134 completed (loss: 0.09542517364025116, acc: 0.9670710563659668)
[2025-02-13 02:38:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:10][root][INFO] - Training Epoch: 1/2, step 1644/7134 completed (loss: 0.06801605224609375, acc: 0.9817470908164978)
[2025-02-13 02:38:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:10][root][INFO] - Training Epoch: 1/2, step 1645/7134 completed (loss: 0.08294417709112167, acc: 0.9813218116760254)
[2025-02-13 02:38:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:11][root][INFO] - Training Epoch: 1/2, step 1646/7134 completed (loss: 0.10258422046899796, acc: 0.97444087266922)
[2025-02-13 02:38:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:11][root][INFO] - Training Epoch: 1/2, step 1647/7134 completed (loss: 0.06900759041309357, acc: 0.978723406791687)
[2025-02-13 02:38:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:12][root][INFO] - Training Epoch: 1/2, step 1648/7134 completed (loss: 0.10993918776512146, acc: 0.9734042286872864)
[2025-02-13 02:38:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:12][root][INFO] - Training Epoch: 1/2, step 1649/7134 completed (loss: 0.04311151057481766, acc: 0.9877913594245911)
[2025-02-13 02:38:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:12][root][INFO] - Training Epoch: 1/2, step 1650/7134 completed (loss: 0.053888995200395584, acc: 0.9845361113548279)
[2025-02-13 02:38:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:13][root][INFO] - Training Epoch: 1/2, step 1651/7134 completed (loss: 0.07460922747850418, acc: 0.9811946749687195)
[2025-02-13 02:38:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:14][root][INFO] - Training Epoch: 1/2, step 1652/7134 completed (loss: 0.03884269669651985, acc: 0.9901853799819946)
[2025-02-13 02:38:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:14][root][INFO] - Training Epoch: 1/2, step 1653/7134 completed (loss: 0.06663147360086441, acc: 0.9775840640068054)
[2025-02-13 02:38:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:15][root][INFO] - Training Epoch: 1/2, step 1654/7134 completed (loss: 0.050924111157655716, acc: 0.9847095012664795)
[2025-02-13 02:38:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:15][root][INFO] - Training Epoch: 1/2, step 1655/7134 completed (loss: 0.06537269800901413, acc: 0.9851301312446594)
[2025-02-13 02:38:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:15][root][INFO] - Training Epoch: 1/2, step 1656/7134 completed (loss: 0.05743872746825218, acc: 0.9831081032752991)
[2025-02-13 02:38:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:16][root][INFO] - Training Epoch: 1/2, step 1657/7134 completed (loss: 0.03569192439317703, acc: 0.990867555141449)
[2025-02-13 02:38:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:16][root][INFO] - Training Epoch: 1/2, step 1658/7134 completed (loss: 0.05024755373597145, acc: 0.9836734533309937)
[2025-02-13 02:38:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:17][root][INFO] - Training Epoch: 1/2, step 1659/7134 completed (loss: 0.05515908822417259, acc: 0.9848341345787048)
[2025-02-13 02:38:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:17][root][INFO] - Training Epoch: 1/2, step 1660/7134 completed (loss: 0.05142473429441452, acc: 0.9842995405197144)
[2025-02-13 02:38:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:18][root][INFO] - Training Epoch: 1/2, step 1661/7134 completed (loss: 0.05509371683001518, acc: 0.9835886359214783)
[2025-02-13 02:38:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:18][root][INFO] - Training Epoch: 1/2, step 1662/7134 completed (loss: 0.053598418831825256, acc: 0.9893955588340759)
[2025-02-13 02:38:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:19][root][INFO] - Training Epoch: 1/2, step 1663/7134 completed (loss: 0.0328887477517128, acc: 0.9938334822654724)
[2025-02-13 02:38:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:19][root][INFO] - Training Epoch: 1/2, step 1664/7134 completed (loss: 0.028838882222771645, acc: 0.9928486347198486)
[2025-02-13 02:38:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:20][root][INFO] - Training Epoch: 1/2, step 1665/7134 completed (loss: 0.03848185017704964, acc: 0.9870503544807434)
[2025-02-13 02:38:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:20][root][INFO] - Training Epoch: 1/2, step 1666/7134 completed (loss: 0.03782348334789276, acc: 0.9860279560089111)
[2025-02-13 02:38:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:21][root][INFO] - Training Epoch: 1/2, step 1667/7134 completed (loss: 0.049204546958208084, acc: 0.9869358539581299)
[2025-02-13 02:38:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:21][root][INFO] - Training Epoch: 1/2, step 1668/7134 completed (loss: 0.0460214801132679, acc: 0.9873949289321899)
[2025-02-13 02:38:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:22][root][INFO] - Training Epoch: 1/2, step 1669/7134 completed (loss: 0.03608567640185356, acc: 0.9905837774276733)
[2025-02-13 02:38:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:22][root][INFO] - Training Epoch: 1/2, step 1670/7134 completed (loss: 0.05951358377933502, acc: 0.9847328066825867)
[2025-02-13 02:38:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:23][root][INFO] - Training Epoch: 1/2, step 1671/7134 completed (loss: 0.06659847497940063, acc: 0.9820788502693176)
[2025-02-13 02:38:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:23][root][INFO] - Training Epoch: 1/2, step 1672/7134 completed (loss: 0.06924232095479965, acc: 0.9814077019691467)
[2025-02-13 02:38:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:24][root][INFO] - Training Epoch: 1/2, step 1673/7134 completed (loss: 0.06286189705133438, acc: 0.9865591526031494)
[2025-02-13 02:38:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:24][root][INFO] - Training Epoch: 1/2, step 1674/7134 completed (loss: 0.05298812687397003, acc: 0.9874301552772522)
[2025-02-13 02:38:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:25][root][INFO] - Training Epoch: 1/2, step 1675/7134 completed (loss: 0.08790691196918488, acc: 0.9823129177093506)
[2025-02-13 02:38:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:25][root][INFO] - Training Epoch: 1/2, step 1676/7134 completed (loss: 0.046000853180885315, acc: 0.9917126893997192)
[2025-02-13 02:38:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:26][root][INFO] - Training Epoch: 1/2, step 1677/7134 completed (loss: 0.09408318251371384, acc: 0.9738134145736694)
[2025-02-13 02:38:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:26][root][INFO] - Training Epoch: 1/2, step 1678/7134 completed (loss: 0.03229500353336334, acc: 0.9917808175086975)
[2025-02-13 02:38:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:27][root][INFO] - Training Epoch: 1/2, step 1679/7134 completed (loss: 0.05555642396211624, acc: 0.985358715057373)
[2025-02-13 02:38:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:27][root][INFO] - Training Epoch: 1/2, step 1680/7134 completed (loss: 0.0449843630194664, acc: 0.9865951538085938)
[2025-02-13 02:38:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:28][root][INFO] - Training Epoch: 1/2, step 1681/7134 completed (loss: 0.040525004267692566, acc: 0.9857954382896423)
[2025-02-13 02:38:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:28][root][INFO] - Training Epoch: 1/2, step 1682/7134 completed (loss: 0.08324006199836731, acc: 0.9769452214241028)
[2025-02-13 02:38:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:28][root][INFO] - Training Epoch: 1/2, step 1683/7134 completed (loss: 0.058800626546144485, acc: 0.9822006225585938)
[2025-02-13 02:38:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:29][root][INFO] - Training Epoch: 1/2, step 1684/7134 completed (loss: 0.04989946633577347, acc: 0.9880136847496033)
[2025-02-13 02:38:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:29][root][INFO] - Training Epoch: 1/2, step 1685/7134 completed (loss: 0.06513852626085281, acc: 0.9833101630210876)
[2025-02-13 02:38:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:30][root][INFO] - Training Epoch: 1/2, step 1686/7134 completed (loss: 0.05662309750914574, acc: 0.9847972989082336)
[2025-02-13 02:38:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:30][root][INFO] - Training Epoch: 1/2, step 1687/7134 completed (loss: 0.03581120818853378, acc: 0.9885931611061096)
[2025-02-13 02:38:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:31][root][INFO] - Training Epoch: 1/2, step 1688/7134 completed (loss: 0.07198072224855423, acc: 0.9860896468162537)
[2025-02-13 02:38:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:31][root][INFO] - Training Epoch: 1/2, step 1689/7134 completed (loss: 0.05272705852985382, acc: 0.9870129823684692)
[2025-02-13 02:38:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:31][root][INFO] - Training Epoch: 1/2, step 1690/7134 completed (loss: 0.029981011524796486, acc: 0.9959349632263184)
[2025-02-13 02:38:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:32][root][INFO] - Training Epoch: 1/2, step 1691/7134 completed (loss: 0.016005676239728928, acc: 0.9950330853462219)
[2025-02-13 02:38:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:32][root][INFO] - Training Epoch: 1/2, step 1692/7134 completed (loss: 0.07553441822528839, acc: 0.9780775904655457)
[2025-02-13 02:38:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:33][root][INFO] - Training Epoch: 1/2, step 1693/7134 completed (loss: 0.048238541930913925, acc: 0.985029935836792)
[2025-02-13 02:38:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:33][root][INFO] - Training Epoch: 1/2, step 1694/7134 completed (loss: 0.046048082411289215, acc: 0.98591548204422)
[2025-02-13 02:38:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:34][root][INFO] - Training Epoch: 1/2, step 1695/7134 completed (loss: 0.023531515151262283, acc: 0.9891892075538635)
[2025-02-13 02:38:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:34][root][INFO] - Training Epoch: 1/2, step 1696/7134 completed (loss: 0.0629824548959732, acc: 0.981574535369873)
[2025-02-13 02:38:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:35][root][INFO] - Training Epoch: 1/2, step 1697/7134 completed (loss: 0.0486588291823864, acc: 0.9874804615974426)
[2025-02-13 02:38:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:35][root][INFO] - Training Epoch: 1/2, step 1698/7134 completed (loss: 0.028303785249590874, acc: 0.9921630024909973)
[2025-02-13 02:38:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:35][root][INFO] - Training Epoch: 1/2, step 1699/7134 completed (loss: 0.04616418853402138, acc: 0.9809688329696655)
[2025-02-13 02:38:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:36][root][INFO] - Training Epoch: 1/2, step 1700/7134 completed (loss: 0.05941338464617729, acc: 0.980322003364563)
[2025-02-13 02:38:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:36][root][INFO] - Training Epoch: 1/2, step 1701/7134 completed (loss: 0.0705316811800003, acc: 0.977746844291687)
[2025-02-13 02:38:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:37][root][INFO] - Training Epoch: 1/2, step 1702/7134 completed (loss: 0.07143081724643707, acc: 0.9763975143432617)
[2025-02-13 02:38:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:37][root][INFO] - Training Epoch: 1/2, step 1703/7134 completed (loss: 0.057069387286901474, acc: 0.9861878156661987)
[2025-02-13 02:38:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:38][root][INFO] - Training Epoch: 1/2, step 1704/7134 completed (loss: 0.14592063426971436, acc: 0.9633151888847351)
[2025-02-13 02:38:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:38][root][INFO] - Training Epoch: 1/2, step 1705/7134 completed (loss: 0.06318076699972153, acc: 0.9842209219932556)
[2025-02-13 02:38:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:39][root][INFO] - Training Epoch: 1/2, step 1706/7134 completed (loss: 0.07955334335565567, acc: 0.9848484992980957)
[2025-02-13 02:38:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:39][root][INFO] - Training Epoch: 1/2, step 1707/7134 completed (loss: 0.042999591678380966, acc: 0.9894291758537292)
[2025-02-13 02:38:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:39][root][INFO] - Training Epoch: 1/2, step 1708/7134 completed (loss: 0.05514136701822281, acc: 0.9853658676147461)
[2025-02-13 02:38:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:40][root][INFO] - Training Epoch: 1/2, step 1709/7134 completed (loss: 0.07286407053470612, acc: 0.9793103337287903)
[2025-02-13 02:38:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:40][root][INFO] - Training Epoch: 1/2, step 1710/7134 completed (loss: 0.07674336433410645, acc: 0.9784482717514038)
[2025-02-13 02:38:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:41][root][INFO] - Training Epoch: 1/2, step 1711/7134 completed (loss: 0.05758596584200859, acc: 0.984455943107605)
[2025-02-13 02:38:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:41][root][INFO] - Training Epoch: 1/2, step 1712/7134 completed (loss: 0.10012543946504593, acc: 0.9701754450798035)
[2025-02-13 02:38:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:42][root][INFO] - Training Epoch: 1/2, step 1713/7134 completed (loss: 0.045333415269851685, acc: 0.9880239367485046)
[2025-02-13 02:38:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:42][root][INFO] - Training Epoch: 1/2, step 1714/7134 completed (loss: 0.07651067525148392, acc: 0.9802631735801697)
[2025-02-13 02:38:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:43][root][INFO] - Training Epoch: 1/2, step 1715/7134 completed (loss: 0.05312632396817207, acc: 0.9817444086074829)
[2025-02-13 02:38:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:43][root][INFO] - Training Epoch: 1/2, step 1716/7134 completed (loss: 0.09223628789186478, acc: 0.9733333587646484)
[2025-02-13 02:38:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:43][root][INFO] - Training Epoch: 1/2, step 1717/7134 completed (loss: 0.05753163620829582, acc: 0.9824903011322021)
[2025-02-13 02:38:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:44][root][INFO] - Training Epoch: 1/2, step 1718/7134 completed (loss: 0.08398954570293427, acc: 0.9755011200904846)
[2025-02-13 02:38:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:44][root][INFO] - Training Epoch: 1/2, step 1719/7134 completed (loss: 0.08543455600738525, acc: 0.9863013625144958)
[2025-02-13 02:38:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:45][root][INFO] - Training Epoch: 1/2, step 1720/7134 completed (loss: 0.09856191277503967, acc: 0.9668737053871155)
[2025-02-13 02:38:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:45][root][INFO] - Training Epoch: 1/2, step 1721/7134 completed (loss: 0.04944281652569771, acc: 0.9831365942955017)
[2025-02-13 02:38:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:46][root][INFO] - Training Epoch: 1/2, step 1722/7134 completed (loss: 0.06506802141666412, acc: 0.9842105507850647)
[2025-02-13 02:38:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:46][root][INFO] - Training Epoch: 1/2, step 1723/7134 completed (loss: 0.10820277780294418, acc: 0.9687034487724304)
[2025-02-13 02:38:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:46][root][INFO] - Training Epoch: 1/2, step 1724/7134 completed (loss: 0.09642719477415085, acc: 0.9686028361320496)
[2025-02-13 02:38:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:47][root][INFO] - Training Epoch: 1/2, step 1725/7134 completed (loss: 0.02681639790534973, acc: 0.9903537034988403)
[2025-02-13 02:38:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:47][root][INFO] - Training Epoch: 1/2, step 1726/7134 completed (loss: 0.0750027447938919, acc: 0.9730941653251648)
[2025-02-13 02:38:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:48][root][INFO] - Training Epoch: 1/2, step 1727/7134 completed (loss: 0.04520006477832794, acc: 0.989051103591919)
[2025-02-13 02:38:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:48][root][INFO] - Training Epoch: 1/2, step 1728/7134 completed (loss: 0.03084014356136322, acc: 0.9917218685150146)
[2025-02-13 02:38:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:49][root][INFO] - Training Epoch: 1/2, step 1729/7134 completed (loss: 0.07994566112756729, acc: 0.9796333909034729)
[2025-02-13 02:38:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:49][root][INFO] - Training Epoch: 1/2, step 1730/7134 completed (loss: 0.03773147985339165, acc: 0.9871520400047302)
[2025-02-13 02:38:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:49][root][INFO] - Training Epoch: 1/2, step 1731/7134 completed (loss: 0.08906228095293045, acc: 0.9826589822769165)
[2025-02-13 02:38:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:50][root][INFO] - Training Epoch: 1/2, step 1732/7134 completed (loss: 0.05108587443828583, acc: 0.9856262803077698)
[2025-02-13 02:38:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:50][root][INFO] - Training Epoch: 1/2, step 1733/7134 completed (loss: 0.10289531201124191, acc: 0.9773662686347961)
[2025-02-13 02:38:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:51][root][INFO] - Training Epoch: 1/2, step 1734/7134 completed (loss: 0.08809206634759903, acc: 0.9729729890823364)
[2025-02-13 02:38:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:51][root][INFO] - Training Epoch: 1/2, step 1735/7134 completed (loss: 0.025849750265479088, acc: 0.991584837436676)
[2025-02-13 02:38:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:52][root][INFO] - Training Epoch: 1/2, step 1736/7134 completed (loss: 0.03985118120908737, acc: 0.9866666793823242)
[2025-02-13 02:38:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:52][root][INFO] - Training Epoch: 1/2, step 1737/7134 completed (loss: 0.046196043491363525, acc: 0.9915730357170105)
[2025-02-13 02:38:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:53][root][INFO] - Training Epoch: 1/2, step 1738/7134 completed (loss: 0.03785674646496773, acc: 0.9879372715950012)
[2025-02-13 02:38:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:53][root][INFO] - Training Epoch: 1/2, step 1739/7134 completed (loss: 0.07104665786027908, acc: 0.9745509028434753)
[2025-02-13 02:38:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:54][root][INFO] - Training Epoch: 1/2, step 1740/7134 completed (loss: 0.04383181035518646, acc: 0.9839181303977966)
[2025-02-13 02:38:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:54][root][INFO] - Training Epoch: 1/2, step 1741/7134 completed (loss: 0.06648562848567963, acc: 0.9788359999656677)
[2025-02-13 02:38:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:54][root][INFO] - Training Epoch: 1/2, step 1742/7134 completed (loss: 0.06471838057041168, acc: 0.9815602898597717)
[2025-02-13 02:38:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:55][root][INFO] - Training Epoch: 1/2, step 1743/7134 completed (loss: 0.06877182424068451, acc: 0.9789915680885315)
[2025-02-13 02:38:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:55][root][INFO] - Training Epoch: 1/2, step 1744/7134 completed (loss: 0.04824255779385567, acc: 0.9951140284538269)
[2025-02-13 02:38:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:56][root][INFO] - Training Epoch: 1/2, step 1745/7134 completed (loss: 0.038979094475507736, acc: 0.9899749159812927)
[2025-02-13 02:38:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:56][root][INFO] - Training Epoch: 1/2, step 1746/7134 completed (loss: 0.032229091972112656, acc: 0.9869186282157898)
[2025-02-13 02:38:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:57][root][INFO] - Training Epoch: 1/2, step 1747/7134 completed (loss: 0.05025072023272514, acc: 0.9837296605110168)
[2025-02-13 02:38:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:57][root][INFO] - Training Epoch: 1/2, step 1748/7134 completed (loss: 0.0544104240834713, acc: 0.981389582157135)
[2025-02-13 02:38:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:58][root][INFO] - Training Epoch: 1/2, step 1749/7134 completed (loss: 0.04657460004091263, acc: 0.9875141978263855)
[2025-02-13 02:38:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:58][root][INFO] - Training Epoch: 1/2, step 1750/7134 completed (loss: 0.05404500290751457, acc: 0.9820144176483154)
[2025-02-13 02:38:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:59][root][INFO] - Training Epoch: 1/2, step 1751/7134 completed (loss: 0.051216449588537216, acc: 0.9842424392700195)
[2025-02-13 02:38:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:38:59][root][INFO] - Training Epoch: 1/2, step 1752/7134 completed (loss: 0.08053066581487656, acc: 0.9768041372299194)
[2025-02-13 02:38:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:00][root][INFO] - Training Epoch: 1/2, step 1753/7134 completed (loss: 0.07488544285297394, acc: 0.9843953251838684)
[2025-02-13 02:39:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:00][root][INFO] - Training Epoch: 1/2, step 1754/7134 completed (loss: 0.05835530906915665, acc: 0.9827089309692383)
[2025-02-13 02:39:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:01][root][INFO] - Training Epoch: 1/2, step 1755/7134 completed (loss: 0.03850382938981056, acc: 0.988095223903656)
[2025-02-13 02:39:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:01][root][INFO] - Training Epoch: 1/2, step 1756/7134 completed (loss: 0.06509331613779068, acc: 0.981794536113739)
[2025-02-13 02:39:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:01][root][INFO] - Training Epoch: 1/2, step 1757/7134 completed (loss: 0.07273872941732407, acc: 0.9783693552017212)
[2025-02-13 02:39:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:02][root][INFO] - Training Epoch: 1/2, step 1758/7134 completed (loss: 0.04189924895763397, acc: 0.9886363744735718)
[2025-02-13 02:39:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:02][root][INFO] - Training Epoch: 1/2, step 1759/7134 completed (loss: 0.04768456891179085, acc: 0.9833101630210876)
[2025-02-13 02:39:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:03][root][INFO] - Training Epoch: 1/2, step 1760/7134 completed (loss: 0.05073101818561554, acc: 0.9865546226501465)
[2025-02-13 02:39:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:03][root][INFO] - Training Epoch: 1/2, step 1761/7134 completed (loss: 0.04109415039420128, acc: 0.9872159361839294)
[2025-02-13 02:39:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:04][root][INFO] - Training Epoch: 1/2, step 1762/7134 completed (loss: 0.03120206855237484, acc: 0.9876288771629333)
[2025-02-13 02:39:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:04][root][INFO] - Training Epoch: 1/2, step 1763/7134 completed (loss: 0.04419490694999695, acc: 0.9839743375778198)
[2025-02-13 02:39:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:05][root][INFO] - Training Epoch: 1/2, step 1764/7134 completed (loss: 0.02370048314332962, acc: 0.9907063245773315)
[2025-02-13 02:39:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:05][root][INFO] - Training Epoch: 1/2, step 1765/7134 completed (loss: 0.027572423219680786, acc: 0.9891975522041321)
[2025-02-13 02:39:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:05][root][INFO] - Training Epoch: 1/2, step 1766/7134 completed (loss: 0.026642220094799995, acc: 0.992668628692627)
[2025-02-13 02:39:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:06][root][INFO] - Training Epoch: 1/2, step 1767/7134 completed (loss: 0.0916251465678215, acc: 0.9787836074829102)
[2025-02-13 02:39:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:06][root][INFO] - Training Epoch: 1/2, step 1768/7134 completed (loss: 0.03980156034231186, acc: 0.9826589822769165)
[2025-02-13 02:39:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:07][root][INFO] - Training Epoch: 1/2, step 1769/7134 completed (loss: 0.04295317456126213, acc: 0.9838235378265381)
[2025-02-13 02:39:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:07][root][INFO] - Training Epoch: 1/2, step 1770/7134 completed (loss: 0.03875330835580826, acc: 0.9908952713012695)
[2025-02-13 02:39:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:08][root][INFO] - Training Epoch: 1/2, step 1771/7134 completed (loss: 0.01577948033809662, acc: 0.994413435459137)
[2025-02-13 02:39:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:08][root][INFO] - Training Epoch: 1/2, step 1772/7134 completed (loss: 0.027041316032409668, acc: 0.9865996837615967)
[2025-02-13 02:39:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:09][root][INFO] - Training Epoch: 1/2, step 1773/7134 completed (loss: 0.023614104837179184, acc: 0.9956395626068115)
[2025-02-13 02:39:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:09][root][INFO] - Training Epoch: 1/2, step 1774/7134 completed (loss: 0.01292169839143753, acc: 0.9983388781547546)
[2025-02-13 02:39:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:09][root][INFO] - Training Epoch: 1/2, step 1775/7134 completed (loss: 0.017012929543852806, acc: 0.9984375238418579)
[2025-02-13 02:39:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:10][root][INFO] - Training Epoch: 1/2, step 1776/7134 completed (loss: 0.0250408798456192, acc: 0.992514967918396)
[2025-02-13 02:39:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:10][root][INFO] - Training Epoch: 1/2, step 1777/7134 completed (loss: 0.020685190334916115, acc: 0.9926035404205322)
[2025-02-13 02:39:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:11][root][INFO] - Training Epoch: 1/2, step 1778/7134 completed (loss: 0.06279955804347992, acc: 0.9881656765937805)
[2025-02-13 02:39:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:11][root][INFO] - Training Epoch: 1/2, step 1779/7134 completed (loss: 0.044998809695243835, acc: 0.9898989796638489)
[2025-02-13 02:39:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:12][root][INFO] - Training Epoch: 1/2, step 1780/7134 completed (loss: 0.05235214903950691, acc: 0.9900990128517151)
[2025-02-13 02:39:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:12][root][INFO] - Training Epoch: 1/2, step 1781/7134 completed (loss: 0.022602036595344543, acc: 0.9908397197723389)
[2025-02-13 02:39:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:12][root][INFO] - Training Epoch: 1/2, step 1782/7134 completed (loss: 0.029617583379149437, acc: 0.9923896789550781)
[2025-02-13 02:39:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:39:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:40:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:41:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:42:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:32][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.0661, device='cuda:0') eval_epoch_loss=tensor(0.0641, device='cuda:0') eval_epoch_acc=tensor(0.9830, device='cuda:0')
[2025-02-13 02:43:32][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-13 02:43:32][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-13 02:43:33][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme/asr_epoch_1_step_1783_loss_0.06405235826969147/model.pt
[2025-02-13 02:43:33][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme directory
[2025-02-13 02:43:33][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.06405235826969147
[2025-02-13 02:43:33][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.983024537563324
[2025-02-13 02:43:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:33][root][INFO] - Training Epoch: 1/2, step 1783/7134 completed (loss: 0.03917103260755539, acc: 0.9891892075538635)
[2025-02-13 02:43:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:34][root][INFO] - Training Epoch: 1/2, step 1784/7134 completed (loss: 0.033108629286289215, acc: 0.992668628692627)
[2025-02-13 02:43:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:34][root][INFO] - Training Epoch: 1/2, step 1785/7134 completed (loss: 0.030250409618020058, acc: 0.9903714060783386)
[2025-02-13 02:43:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:34][root][INFO] - Training Epoch: 1/2, step 1786/7134 completed (loss: 0.03336949646472931, acc: 0.9906396269798279)
[2025-02-13 02:43:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:35][root][INFO] - Training Epoch: 1/2, step 1787/7134 completed (loss: 0.034934040158987045, acc: 0.9892857074737549)
[2025-02-13 02:43:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:35][root][INFO] - Training Epoch: 1/2, step 1788/7134 completed (loss: 0.03452668711543083, acc: 0.9918166995048523)
[2025-02-13 02:43:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:36][root][INFO] - Training Epoch: 1/2, step 1789/7134 completed (loss: 0.019600002095103264, acc: 0.9953917264938354)
[2025-02-13 02:43:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:36][root][INFO] - Training Epoch: 1/2, step 1790/7134 completed (loss: 0.13561232388019562, acc: 0.977455735206604)
[2025-02-13 02:43:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:37][root][INFO] - Training Epoch: 1/2, step 1791/7134 completed (loss: 0.13026884198188782, acc: 0.9743223786354065)
[2025-02-13 02:43:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:37][root][INFO] - Training Epoch: 1/2, step 1792/7134 completed (loss: 0.19742706418037415, acc: 0.9566395878791809)
[2025-02-13 02:43:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:38][root][INFO] - Training Epoch: 1/2, step 1793/7134 completed (loss: 0.12722846865653992, acc: 0.9702702760696411)
[2025-02-13 02:43:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:38][root][INFO] - Training Epoch: 1/2, step 1794/7134 completed (loss: 0.09909047186374664, acc: 0.9716981053352356)
[2025-02-13 02:43:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:39][root][INFO] - Training Epoch: 1/2, step 1795/7134 completed (loss: 0.19888968765735626, acc: 0.9592198729515076)
[2025-02-13 02:43:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:39][root][INFO] - Training Epoch: 1/2, step 1796/7134 completed (loss: 0.1541188359260559, acc: 0.9592476487159729)
[2025-02-13 02:43:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:39][root][INFO] - Training Epoch: 1/2, step 1797/7134 completed (loss: 0.08910897374153137, acc: 0.9719887971878052)
[2025-02-13 02:43:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:40][root][INFO] - Training Epoch: 1/2, step 1798/7134 completed (loss: 0.08799076080322266, acc: 0.969111979007721)
[2025-02-13 02:43:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:40][root][INFO] - Training Epoch: 1/2, step 1799/7134 completed (loss: 0.13871236145496368, acc: 0.9685929417610168)
[2025-02-13 02:43:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:41][root][INFO] - Training Epoch: 1/2, step 1800/7134 completed (loss: 0.127532958984375, acc: 0.9649532437324524)
[2025-02-13 02:43:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:41][root][INFO] - Training Epoch: 1/2, step 1801/7134 completed (loss: 0.09216386824846268, acc: 0.9788135886192322)
[2025-02-13 02:43:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:42][root][INFO] - Training Epoch: 1/2, step 1802/7134 completed (loss: 0.07692426443099976, acc: 0.9799749851226807)
[2025-02-13 02:43:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:42][root][INFO] - Training Epoch: 1/2, step 1803/7134 completed (loss: 0.07155071198940277, acc: 0.9799692034721375)
[2025-02-13 02:43:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:43][root][INFO] - Training Epoch: 1/2, step 1804/7134 completed (loss: 0.039173148572444916, acc: 0.9877150058746338)
[2025-02-13 02:43:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:43][root][INFO] - Training Epoch: 1/2, step 1805/7134 completed (loss: 0.08431676775217056, acc: 0.9783549904823303)
[2025-02-13 02:43:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:43][root][INFO] - Training Epoch: 1/2, step 1806/7134 completed (loss: 0.05632699653506279, acc: 0.9815789461135864)
[2025-02-13 02:43:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:44][root][INFO] - Training Epoch: 1/2, step 1807/7134 completed (loss: 0.06858924776315689, acc: 0.9787836074829102)
[2025-02-13 02:43:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:44][root][INFO] - Training Epoch: 1/2, step 1808/7134 completed (loss: 0.04011144861578941, acc: 0.9908972978591919)
[2025-02-13 02:43:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:45][root][INFO] - Training Epoch: 1/2, step 1809/7134 completed (loss: 0.05470694974064827, acc: 0.9864864945411682)
[2025-02-13 02:43:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:45][root][INFO] - Training Epoch: 1/2, step 1810/7134 completed (loss: 0.018400272354483604, acc: 0.9974226951599121)
[2025-02-13 02:43:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:46][root][INFO] - Training Epoch: 1/2, step 1811/7134 completed (loss: 0.09309802204370499, acc: 0.975576639175415)
[2025-02-13 02:43:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:46][root][INFO] - Training Epoch: 1/2, step 1812/7134 completed (loss: 0.02820432372391224, acc: 0.9910714030265808)
[2025-02-13 02:43:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:46][root][INFO] - Training Epoch: 1/2, step 1813/7134 completed (loss: 0.07082836329936981, acc: 0.9820359349250793)
[2025-02-13 02:43:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:47][root][INFO] - Training Epoch: 1/2, step 1814/7134 completed (loss: 0.03968703746795654, acc: 0.9895104765892029)
[2025-02-13 02:43:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:47][root][INFO] - Training Epoch: 1/2, step 1815/7134 completed (loss: 0.04286744073033333, acc: 0.9885057210922241)
[2025-02-13 02:43:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:48][root][INFO] - Training Epoch: 1/2, step 1816/7134 completed (loss: 0.060361362993717194, acc: 0.98591548204422)
[2025-02-13 02:43:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:48][root][INFO] - Training Epoch: 1/2, step 1817/7134 completed (loss: 0.05569595843553543, acc: 0.9904534816741943)
[2025-02-13 02:43:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:49][root][INFO] - Training Epoch: 1/2, step 1818/7134 completed (loss: 0.06245201453566551, acc: 0.9888476133346558)
[2025-02-13 02:43:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:49][root][INFO] - Training Epoch: 1/2, step 1819/7134 completed (loss: 0.030001183971762657, acc: 0.9925187230110168)
[2025-02-13 02:43:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:50][root][INFO] - Training Epoch: 1/2, step 1820/7134 completed (loss: 0.04237937182188034, acc: 0.9901746511459351)
[2025-02-13 02:43:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:50][root][INFO] - Training Epoch: 1/2, step 1821/7134 completed (loss: 0.05685741826891899, acc: 0.9879649877548218)
[2025-02-13 02:43:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:51][root][INFO] - Training Epoch: 1/2, step 1822/7134 completed (loss: 0.0180638637393713, acc: 0.9941520690917969)
[2025-02-13 02:43:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:51][root][INFO] - Training Epoch: 1/2, step 1823/7134 completed (loss: 0.024695105850696564, acc: 0.9931787252426147)
[2025-02-13 02:43:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:51][root][INFO] - Training Epoch: 1/2, step 1824/7134 completed (loss: 0.028744956478476524, acc: 0.990111231803894)
[2025-02-13 02:43:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:52][root][INFO] - Training Epoch: 1/2, step 1825/7134 completed (loss: 0.022466283291578293, acc: 0.9943946003913879)
[2025-02-13 02:43:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:52][root][INFO] - Training Epoch: 1/2, step 1826/7134 completed (loss: 0.03090507537126541, acc: 0.9878869652748108)
[2025-02-13 02:43:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:53][root][INFO] - Training Epoch: 1/2, step 1827/7134 completed (loss: 0.0548437274992466, acc: 0.984564483165741)
[2025-02-13 02:43:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:53][root][INFO] - Training Epoch: 1/2, step 1828/7134 completed (loss: 0.00794393103569746, acc: 0.9987130165100098)
[2025-02-13 02:43:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:54][root][INFO] - Training Epoch: 1/2, step 1829/7134 completed (loss: 0.031368523836135864, acc: 0.991304337978363)
[2025-02-13 02:43:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:54][root][INFO] - Training Epoch: 1/2, step 1830/7134 completed (loss: 0.06138371676206589, acc: 0.9854721426963806)
[2025-02-13 02:43:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:55][root][INFO] - Training Epoch: 1/2, step 1831/7134 completed (loss: 0.024629369378089905, acc: 0.9906250238418579)
[2025-02-13 02:43:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:55][root][INFO] - Training Epoch: 1/2, step 1832/7134 completed (loss: 0.07473985105752945, acc: 0.9876325130462646)
[2025-02-13 02:43:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:55][root][INFO] - Training Epoch: 1/2, step 1833/7134 completed (loss: 0.210834339261055, acc: 0.9556025266647339)
[2025-02-13 02:43:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:56][root][INFO] - Training Epoch: 1/2, step 1834/7134 completed (loss: 0.06179564446210861, acc: 0.9774647951126099)
[2025-02-13 02:43:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:56][root][INFO] - Training Epoch: 1/2, step 1835/7134 completed (loss: 0.0739043802022934, acc: 0.9869186282157898)
[2025-02-13 02:43:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:57][root][INFO] - Training Epoch: 1/2, step 1836/7134 completed (loss: 0.062329262495040894, acc: 0.9810874462127686)
[2025-02-13 02:43:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:57][root][INFO] - Training Epoch: 1/2, step 1837/7134 completed (loss: 0.04506732523441315, acc: 0.9828009605407715)
[2025-02-13 02:43:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:58][root][INFO] - Training Epoch: 1/2, step 1838/7134 completed (loss: 0.025206033140420914, acc: 0.9918367266654968)
[2025-02-13 02:43:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:58][root][INFO] - Training Epoch: 1/2, step 1839/7134 completed (loss: 0.031889382749795914, acc: 0.9935232996940613)
[2025-02-13 02:43:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:59][root][INFO] - Training Epoch: 1/2, step 1840/7134 completed (loss: 0.05321196839213371, acc: 0.9818689227104187)
[2025-02-13 02:43:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:59][root][INFO] - Training Epoch: 1/2, step 1841/7134 completed (loss: 0.05697818845510483, acc: 0.978723406791687)
[2025-02-13 02:43:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:43:59][root][INFO] - Training Epoch: 1/2, step 1842/7134 completed (loss: 0.05308324471116066, acc: 0.9877551198005676)
[2025-02-13 02:44:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:00][root][INFO] - Training Epoch: 1/2, step 1843/7134 completed (loss: 0.07175937294960022, acc: 0.981249988079071)
[2025-02-13 02:44:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:00][root][INFO] - Training Epoch: 1/2, step 1844/7134 completed (loss: 0.04812804237008095, acc: 0.9886178970336914)
[2025-02-13 02:44:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:01][root][INFO] - Training Epoch: 1/2, step 1845/7134 completed (loss: 0.07465719431638718, acc: 0.9791231751441956)
[2025-02-13 02:44:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:01][root][INFO] - Training Epoch: 1/2, step 1846/7134 completed (loss: 0.034111473709344864, acc: 0.9900285005569458)
[2025-02-13 02:44:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:02][root][INFO] - Training Epoch: 1/2, step 1847/7134 completed (loss: 0.050634268671274185, acc: 0.9769585132598877)
[2025-02-13 02:44:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:02][root][INFO] - Training Epoch: 1/2, step 1848/7134 completed (loss: 0.04993728548288345, acc: 0.9858757257461548)
[2025-02-13 02:44:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:03][root][INFO] - Training Epoch: 1/2, step 1849/7134 completed (loss: 0.09584912657737732, acc: 0.9789103865623474)
[2025-02-13 02:44:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:03][root][INFO] - Training Epoch: 1/2, step 1850/7134 completed (loss: 0.033087510615587234, acc: 0.9914425611495972)
[2025-02-13 02:44:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:03][root][INFO] - Training Epoch: 1/2, step 1851/7134 completed (loss: 0.060960203409194946, acc: 0.9865689873695374)
[2025-02-13 02:44:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:04][root][INFO] - Training Epoch: 1/2, step 1852/7134 completed (loss: 0.03874310851097107, acc: 0.9875776171684265)
[2025-02-13 02:44:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:04][root][INFO] - Training Epoch: 1/2, step 1853/7134 completed (loss: 0.05110657587647438, acc: 0.989234447479248)
[2025-02-13 02:44:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:05][root][INFO] - Training Epoch: 1/2, step 1854/7134 completed (loss: 0.05423321947455406, acc: 0.9797859787940979)
[2025-02-13 02:44:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:05][root][INFO] - Training Epoch: 1/2, step 1855/7134 completed (loss: 0.056805141270160675, acc: 0.9838150143623352)
[2025-02-13 02:44:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:06][root][INFO] - Training Epoch: 1/2, step 1856/7134 completed (loss: 0.04726537689566612, acc: 0.9852398633956909)
[2025-02-13 02:44:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:06][root][INFO] - Training Epoch: 1/2, step 1857/7134 completed (loss: 0.08264566957950592, acc: 0.9802955389022827)
[2025-02-13 02:44:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:07][root][INFO] - Training Epoch: 1/2, step 1858/7134 completed (loss: 0.026357561349868774, acc: 0.9916267991065979)
[2025-02-13 02:44:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:07][root][INFO] - Training Epoch: 1/2, step 1859/7134 completed (loss: 0.0230399202555418, acc: 0.9931740760803223)
[2025-02-13 02:44:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:08][root][INFO] - Training Epoch: 1/2, step 1860/7134 completed (loss: 0.04784472659230232, acc: 0.9873417615890503)
[2025-02-13 02:44:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:08][root][INFO] - Training Epoch: 1/2, step 1861/7134 completed (loss: 0.05672229453921318, acc: 0.9870800971984863)
[2025-02-13 02:44:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:08][root][INFO] - Training Epoch: 1/2, step 1862/7134 completed (loss: 0.03015662357211113, acc: 0.9918588995933533)
[2025-02-13 02:44:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:09][root][INFO] - Training Epoch: 1/2, step 1863/7134 completed (loss: 0.05546760559082031, acc: 0.9880095720291138)
[2025-02-13 02:44:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:09][root][INFO] - Training Epoch: 1/2, step 1864/7134 completed (loss: 0.15969723463058472, acc: 0.9687075018882751)
[2025-02-13 02:44:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:10][root][INFO] - Training Epoch: 1/2, step 1865/7134 completed (loss: 0.06295106559991837, acc: 0.9830795526504517)
[2025-02-13 02:44:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:10][root][INFO] - Training Epoch: 1/2, step 1866/7134 completed (loss: 0.019234435632824898, acc: 0.9929971694946289)
[2025-02-13 02:44:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:11][root][INFO] - Training Epoch: 1/2, step 1867/7134 completed (loss: 0.03250257298350334, acc: 0.9899117350578308)
[2025-02-13 02:44:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:11][root][INFO] - Training Epoch: 1/2, step 1868/7134 completed (loss: 0.07564518600702286, acc: 0.9818481802940369)
[2025-02-13 02:44:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:11][root][INFO] - Training Epoch: 1/2, step 1869/7134 completed (loss: 0.05421850457787514, acc: 0.980424165725708)
[2025-02-13 02:44:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:12][root][INFO] - Training Epoch: 1/2, step 1870/7134 completed (loss: 0.0363461934030056, acc: 0.9869791865348816)
[2025-02-13 02:44:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:12][root][INFO] - Training Epoch: 1/2, step 1871/7134 completed (loss: 0.06420504301786423, acc: 0.9918434023857117)
[2025-02-13 02:44:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:13][root][INFO] - Training Epoch: 1/2, step 1872/7134 completed (loss: 0.5565366744995117, acc: 0.8950276374816895)
[2025-02-13 02:44:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:13][root][INFO] - Training Epoch: 1/2, step 1873/7134 completed (loss: 0.15716871619224548, acc: 0.9548693299293518)
[2025-02-13 02:44:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:14][root][INFO] - Training Epoch: 1/2, step 1874/7134 completed (loss: 0.058768149465322495, acc: 0.9882044792175293)
[2025-02-13 02:44:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:14][root][INFO] - Training Epoch: 1/2, step 1875/7134 completed (loss: 0.040198568254709244, acc: 0.9912663698196411)
[2025-02-13 02:44:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:14][root][INFO] - Training Epoch: 1/2, step 1876/7134 completed (loss: 0.16374237835407257, acc: 0.9558573961257935)
[2025-02-13 02:44:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:15][root][INFO] - Training Epoch: 1/2, step 1877/7134 completed (loss: 0.06812914460897446, acc: 0.9748110771179199)
[2025-02-13 02:44:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:15][root][INFO] - Training Epoch: 1/2, step 1878/7134 completed (loss: 0.08057888597249985, acc: 0.9815100431442261)
[2025-02-13 02:44:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:16][root][INFO] - Training Epoch: 1/2, step 1879/7134 completed (loss: 0.10814172774553299, acc: 0.9575856328010559)
[2025-02-13 02:44:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:16][root][INFO] - Training Epoch: 1/2, step 1880/7134 completed (loss: 0.08113221824169159, acc: 0.9818913340568542)
[2025-02-13 02:44:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:17][root][INFO] - Training Epoch: 1/2, step 1881/7134 completed (loss: 0.07816208153963089, acc: 0.9820261597633362)
[2025-02-13 02:44:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:17][root][INFO] - Training Epoch: 1/2, step 1882/7134 completed (loss: 0.1187560111284256, acc: 0.9704433679580688)
[2025-02-13 02:44:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:17][root][INFO] - Training Epoch: 1/2, step 1883/7134 completed (loss: 0.050902530550956726, acc: 0.9921259880065918)
[2025-02-13 02:44:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:18][root][INFO] - Training Epoch: 1/2, step 1884/7134 completed (loss: 0.1075344830751419, acc: 0.9699841737747192)
[2025-02-13 02:44:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:18][root][INFO] - Training Epoch: 1/2, step 1885/7134 completed (loss: 0.08604232221841812, acc: 0.9793621301651001)
[2025-02-13 02:44:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:19][root][INFO] - Training Epoch: 1/2, step 1886/7134 completed (loss: 0.09896494448184967, acc: 0.9770354628562927)
[2025-02-13 02:44:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:19][root][INFO] - Training Epoch: 1/2, step 1887/7134 completed (loss: 0.0915437564253807, acc: 0.9734345078468323)
[2025-02-13 02:44:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:20][root][INFO] - Training Epoch: 1/2, step 1888/7134 completed (loss: 0.10184597969055176, acc: 0.973128616809845)
[2025-02-13 02:44:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:20][root][INFO] - Training Epoch: 1/2, step 1889/7134 completed (loss: 0.057844746857881546, acc: 0.9812138676643372)
[2025-02-13 02:44:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:20][root][INFO] - Training Epoch: 1/2, step 1890/7134 completed (loss: 0.0352352000772953, acc: 0.9891641139984131)
[2025-02-13 02:44:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:21][root][INFO] - Training Epoch: 1/2, step 1891/7134 completed (loss: 0.10880909860134125, acc: 0.9724025726318359)
[2025-02-13 02:44:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:21][root][INFO] - Training Epoch: 1/2, step 1892/7134 completed (loss: 0.0851266011595726, acc: 0.9783616662025452)
[2025-02-13 02:44:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:22][root][INFO] - Training Epoch: 1/2, step 1893/7134 completed (loss: 0.05207188054919243, acc: 0.9812792539596558)
[2025-02-13 02:44:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:22][root][INFO] - Training Epoch: 1/2, step 1894/7134 completed (loss: 0.10952632874250412, acc: 0.9809027910232544)
[2025-02-13 02:44:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:23][root][INFO] - Training Epoch: 1/2, step 1895/7134 completed (loss: 0.29131022095680237, acc: 0.9444444179534912)
[2025-02-13 02:44:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:23][root][INFO] - Training Epoch: 1/2, step 1896/7134 completed (loss: 0.07865390181541443, acc: 0.9764705896377563)
[2025-02-13 02:44:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:23][root][INFO] - Training Epoch: 1/2, step 1897/7134 completed (loss: 0.028649482876062393, acc: 0.9925925731658936)
[2025-02-13 02:44:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:24][root][INFO] - Training Epoch: 1/2, step 1898/7134 completed (loss: 0.04991314560174942, acc: 0.9800994992256165)
[2025-02-13 02:44:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:24][root][INFO] - Training Epoch: 1/2, step 1899/7134 completed (loss: 0.06179133430123329, acc: 0.9891501069068909)
[2025-02-13 02:44:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:25][root][INFO] - Training Epoch: 1/2, step 1900/7134 completed (loss: 0.027470242232084274, acc: 0.9951456189155579)
[2025-02-13 02:44:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:25][root][INFO] - Training Epoch: 1/2, step 1901/7134 completed (loss: 0.07533466815948486, acc: 0.9817184805870056)
[2025-02-13 02:44:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:25][root][INFO] - Training Epoch: 1/2, step 1902/7134 completed (loss: 0.03694884106516838, acc: 0.9876977205276489)
[2025-02-13 02:44:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:26][root][INFO] - Training Epoch: 1/2, step 1903/7134 completed (loss: 0.042556460946798325, acc: 0.9861111044883728)
[2025-02-13 02:44:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:26][root][INFO] - Training Epoch: 1/2, step 1904/7134 completed (loss: 0.05923609063029289, acc: 0.9872881174087524)
[2025-02-13 02:44:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:27][root][INFO] - Training Epoch: 1/2, step 1905/7134 completed (loss: 0.060904864221811295, acc: 0.9856850504875183)
[2025-02-13 02:44:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:27][root][INFO] - Training Epoch: 1/2, step 1906/7134 completed (loss: 0.05504865199327469, acc: 0.9805068373680115)
[2025-02-13 02:44:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:28][root][INFO] - Training Epoch: 1/2, step 1907/7134 completed (loss: 0.055746857076883316, acc: 0.9848739504814148)
[2025-02-13 02:44:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:28][root][INFO] - Training Epoch: 1/2, step 1908/7134 completed (loss: 0.05088045075535774, acc: 0.9881847500801086)
[2025-02-13 02:44:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:29][root][INFO] - Training Epoch: 1/2, step 1909/7134 completed (loss: 0.05550067126750946, acc: 0.9841059446334839)
[2025-02-13 02:44:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:29][root][INFO] - Training Epoch: 1/2, step 1910/7134 completed (loss: 0.05229191109538078, acc: 0.9868420958518982)
[2025-02-13 02:44:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:30][root][INFO] - Training Epoch: 1/2, step 1911/7134 completed (loss: 0.060080286115407944, acc: 0.9854369163513184)
[2025-02-13 02:44:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:30][root][INFO] - Training Epoch: 1/2, step 1912/7134 completed (loss: 0.01742175593972206, acc: 0.9946737885475159)
[2025-02-13 02:44:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:30][root][INFO] - Training Epoch: 1/2, step 1913/7134 completed (loss: 0.06701767444610596, acc: 0.9851577281951904)
[2025-02-13 02:44:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:31][root][INFO] - Training Epoch: 1/2, step 1914/7134 completed (loss: 0.06799589842557907, acc: 0.9837618470191956)
[2025-02-13 02:44:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:31][root][INFO] - Training Epoch: 1/2, step 1915/7134 completed (loss: 0.04737154021859169, acc: 0.985318124294281)
[2025-02-13 02:44:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:32][root][INFO] - Training Epoch: 1/2, step 1916/7134 completed (loss: 0.053674016147851944, acc: 0.986601710319519)
[2025-02-13 02:44:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:32][root][INFO] - Training Epoch: 1/2, step 1917/7134 completed (loss: 0.056382544338703156, acc: 0.9859943985939026)
[2025-02-13 02:44:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:33][root][INFO] - Training Epoch: 1/2, step 1918/7134 completed (loss: 0.03924984112381935, acc: 0.9921630024909973)
[2025-02-13 02:44:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:33][root][INFO] - Training Epoch: 1/2, step 1919/7134 completed (loss: 0.059122759848833084, acc: 0.9856528043746948)
[2025-02-13 02:44:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:34][root][INFO] - Training Epoch: 1/2, step 1920/7134 completed (loss: 0.03702688589692116, acc: 0.9922839403152466)
[2025-02-13 02:44:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:34][root][INFO] - Training Epoch: 1/2, step 1921/7134 completed (loss: 0.07591324299573898, acc: 0.9778393507003784)
[2025-02-13 02:44:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:35][root][INFO] - Training Epoch: 1/2, step 1922/7134 completed (loss: 0.04556954279541969, acc: 0.987077534198761)
[2025-02-13 02:44:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:35][root][INFO] - Training Epoch: 1/2, step 1923/7134 completed (loss: 0.04427260532975197, acc: 0.9882746934890747)
[2025-02-13 02:44:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:35][root][INFO] - Training Epoch: 1/2, step 1924/7134 completed (loss: 0.04970213398337364, acc: 0.9785407781600952)
[2025-02-13 02:44:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:36][root][INFO] - Training Epoch: 1/2, step 1925/7134 completed (loss: 0.07984775304794312, acc: 0.9777777791023254)
[2025-02-13 02:44:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:36][root][INFO] - Training Epoch: 1/2, step 1926/7134 completed (loss: 0.02995307743549347, acc: 0.9908069372177124)
[2025-02-13 02:44:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:37][root][INFO] - Training Epoch: 1/2, step 1927/7134 completed (loss: 0.06731972098350525, acc: 0.9878453016281128)
[2025-02-13 02:44:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:37][root][INFO] - Training Epoch: 1/2, step 1928/7134 completed (loss: 0.04369775578379631, acc: 0.9908814430236816)
[2025-02-13 02:44:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:38][root][INFO] - Training Epoch: 1/2, step 1929/7134 completed (loss: 0.02209208719432354, acc: 0.9957627058029175)
[2025-02-13 02:44:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:38][root][INFO] - Training Epoch: 1/2, step 1930/7134 completed (loss: 0.07143706828355789, acc: 0.9763779640197754)
[2025-02-13 02:44:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:39][root][INFO] - Training Epoch: 1/2, step 1931/7134 completed (loss: 0.11157495528459549, acc: 0.9721871018409729)
[2025-02-13 02:44:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:39][root][INFO] - Training Epoch: 1/2, step 1932/7134 completed (loss: 0.10484059154987335, acc: 0.9776119589805603)
[2025-02-13 02:44:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:40][root][INFO] - Training Epoch: 1/2, step 1933/7134 completed (loss: 0.1640743464231491, acc: 0.970588207244873)
[2025-02-13 02:44:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:40][root][INFO] - Training Epoch: 1/2, step 1934/7134 completed (loss: 0.04248974099755287, acc: 0.9887005686759949)
[2025-02-13 02:44:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:40][root][INFO] - Training Epoch: 1/2, step 1935/7134 completed (loss: 0.11596512049436569, acc: 0.9694322943687439)
[2025-02-13 02:44:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:41][root][INFO] - Training Epoch: 1/2, step 1936/7134 completed (loss: 0.03198736160993576, acc: 0.9936808943748474)
[2025-02-13 02:44:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:41][root][INFO] - Training Epoch: 1/2, step 1937/7134 completed (loss: 0.11054356396198273, acc: 0.9724025726318359)
[2025-02-13 02:44:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:42][root][INFO] - Training Epoch: 1/2, step 1938/7134 completed (loss: 0.08336945623159409, acc: 0.9772079586982727)
[2025-02-13 02:44:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:42][root][INFO] - Training Epoch: 1/2, step 1939/7134 completed (loss: 0.06993072479963303, acc: 0.9809321761131287)
[2025-02-13 02:44:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:43][root][INFO] - Training Epoch: 1/2, step 1940/7134 completed (loss: 0.0953715592622757, acc: 0.9741496443748474)
[2025-02-13 02:44:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:43][root][INFO] - Training Epoch: 1/2, step 1941/7134 completed (loss: 0.11043532192707062, acc: 0.9692513346672058)
[2025-02-13 02:44:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:44][root][INFO] - Training Epoch: 1/2, step 1942/7134 completed (loss: 0.05954306945204735, acc: 0.9837398529052734)
[2025-02-13 02:44:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:44][root][INFO] - Training Epoch: 1/2, step 1943/7134 completed (loss: 0.12303361296653748, acc: 0.962195098400116)
[2025-02-13 02:44:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:44][root][INFO] - Training Epoch: 1/2, step 1944/7134 completed (loss: 0.0748855322599411, acc: 0.9762340188026428)
[2025-02-13 02:44:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:45][root][INFO] - Training Epoch: 1/2, step 1945/7134 completed (loss: 0.06353391706943512, acc: 0.9763779640197754)
[2025-02-13 02:44:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:45][root][INFO] - Training Epoch: 1/2, step 1946/7134 completed (loss: 0.09758022427558899, acc: 0.9677419066429138)
[2025-02-13 02:44:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:46][root][INFO] - Training Epoch: 1/2, step 1947/7134 completed (loss: 0.08506901562213898, acc: 0.9763157963752747)
[2025-02-13 02:44:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:46][root][INFO] - Training Epoch: 1/2, step 1948/7134 completed (loss: 0.16318248212337494, acc: 0.9620493650436401)
[2025-02-13 02:44:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:47][root][INFO] - Training Epoch: 1/2, step 1949/7134 completed (loss: 0.07566957175731659, acc: 0.9765886068344116)
[2025-02-13 02:44:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:47][root][INFO] - Training Epoch: 1/2, step 1950/7134 completed (loss: 0.12243463099002838, acc: 0.9689781069755554)
[2025-02-13 02:44:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:48][root][INFO] - Training Epoch: 1/2, step 1951/7134 completed (loss: 0.07270490378141403, acc: 0.9799330830574036)
[2025-02-13 02:44:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:48][root][INFO] - Training Epoch: 1/2, step 1952/7134 completed (loss: 0.11789285391569138, acc: 0.9683859944343567)
[2025-02-13 02:44:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:48][root][INFO] - Training Epoch: 1/2, step 1953/7134 completed (loss: 0.05140313506126404, acc: 0.9845201373100281)
[2025-02-13 02:44:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:49][root][INFO] - Training Epoch: 1/2, step 1954/7134 completed (loss: 0.061994463205337524, acc: 0.9853658676147461)
[2025-02-13 02:44:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:49][root][INFO] - Training Epoch: 1/2, step 1955/7134 completed (loss: 0.060760315507650375, acc: 0.9841827750205994)
[2025-02-13 02:44:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:50][root][INFO] - Training Epoch: 1/2, step 1956/7134 completed (loss: 0.029261929914355278, acc: 0.9892473220825195)
[2025-02-13 02:44:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:50][root][INFO] - Training Epoch: 1/2, step 1957/7134 completed (loss: 0.04791739955544472, acc: 0.9907407164573669)
[2025-02-13 02:44:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:50][root][INFO] - Training Epoch: 1/2, step 1958/7134 completed (loss: 0.07157693803310394, acc: 0.9856733679771423)
[2025-02-13 02:44:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:51][root][INFO] - Training Epoch: 1/2, step 1959/7134 completed (loss: 0.0342484787106514, acc: 0.9936034083366394)
[2025-02-13 02:44:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:51][root][INFO] - Training Epoch: 1/2, step 1960/7134 completed (loss: 0.024881189689040184, acc: 0.9943609237670898)
[2025-02-13 02:44:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:52][root][INFO] - Training Epoch: 1/2, step 1961/7134 completed (loss: 0.03917139396071434, acc: 0.9866220951080322)
[2025-02-13 02:44:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:52][root][INFO] - Training Epoch: 1/2, step 1962/7134 completed (loss: 0.06988207250833511, acc: 0.9804469347000122)
[2025-02-13 02:44:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:53][root][INFO] - Training Epoch: 1/2, step 1963/7134 completed (loss: 0.06290049850940704, acc: 0.9740932583808899)
[2025-02-13 02:44:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:53][root][INFO] - Training Epoch: 1/2, step 1964/7134 completed (loss: 0.0647524893283844, acc: 0.9816849827766418)
[2025-02-13 02:44:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:53][root][INFO] - Training Epoch: 1/2, step 1965/7134 completed (loss: 0.04397791624069214, acc: 0.9902371168136597)
[2025-02-13 02:44:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:54][root][INFO] - Training Epoch: 1/2, step 1966/7134 completed (loss: 0.048711925745010376, acc: 0.9877675771713257)
[2025-02-13 02:44:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:54][root][INFO] - Training Epoch: 1/2, step 1967/7134 completed (loss: 0.08129909634590149, acc: 0.9834395051002502)
[2025-02-13 02:44:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:55][root][INFO] - Training Epoch: 1/2, step 1968/7134 completed (loss: 0.02850385196506977, acc: 0.990123450756073)
[2025-02-13 02:44:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:55][root][INFO] - Training Epoch: 1/2, step 1969/7134 completed (loss: 0.06961025297641754, acc: 0.9853333234786987)
[2025-02-13 02:44:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:56][root][INFO] - Training Epoch: 1/2, step 1970/7134 completed (loss: 0.03407968953251839, acc: 0.9907975196838379)
[2025-02-13 02:44:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:56][root][INFO] - Training Epoch: 1/2, step 1971/7134 completed (loss: 0.034611765295267105, acc: 0.9907161593437195)
[2025-02-13 02:44:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:57][root][INFO] - Training Epoch: 1/2, step 1972/7134 completed (loss: 0.19254647195339203, acc: 0.9459459185600281)
[2025-02-13 02:44:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:57][root][INFO] - Training Epoch: 1/2, step 1973/7134 completed (loss: 0.1130775585770607, acc: 0.9733333587646484)
[2025-02-13 02:44:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:57][root][INFO] - Training Epoch: 1/2, step 1974/7134 completed (loss: 0.040426403284072876, acc: 0.9912060499191284)
[2025-02-13 02:44:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:58][root][INFO] - Training Epoch: 1/2, step 1975/7134 completed (loss: 0.039728693664073944, acc: 0.9939393997192383)
[2025-02-13 02:44:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:58][root][INFO] - Training Epoch: 1/2, step 1976/7134 completed (loss: 0.04895784333348274, acc: 0.9841449856758118)
[2025-02-13 02:44:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:59][root][INFO] - Training Epoch: 1/2, step 1977/7134 completed (loss: 0.06825987994670868, acc: 0.9840579628944397)
[2025-02-13 02:44:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:44:59][root][INFO] - Training Epoch: 1/2, step 1978/7134 completed (loss: 0.029868608340620995, acc: 0.991946280002594)
[2025-02-13 02:44:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:00][root][INFO] - Training Epoch: 1/2, step 1979/7134 completed (loss: 0.04366961121559143, acc: 0.9830729365348816)
[2025-02-13 02:45:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:00][root][INFO] - Training Epoch: 1/2, step 1980/7134 completed (loss: 0.10939868539571762, acc: 0.9778357148170471)
[2025-02-13 02:45:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:01][root][INFO] - Training Epoch: 1/2, step 1981/7134 completed (loss: 0.05353354662656784, acc: 0.9832167625427246)
[2025-02-13 02:45:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:01][root][INFO] - Training Epoch: 1/2, step 1982/7134 completed (loss: 0.0370907299220562, acc: 0.9845559597015381)
[2025-02-13 02:45:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:01][root][INFO] - Training Epoch: 1/2, step 1983/7134 completed (loss: 0.03425648435950279, acc: 0.9923664331436157)
[2025-02-13 02:45:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:02][root][INFO] - Training Epoch: 1/2, step 1984/7134 completed (loss: 0.01975623518228531, acc: 0.9932705163955688)
[2025-02-13 02:45:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:02][root][INFO] - Training Epoch: 1/2, step 1985/7134 completed (loss: 0.03851749747991562, acc: 0.984679639339447)
[2025-02-13 02:45:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:03][root][INFO] - Training Epoch: 1/2, step 1986/7134 completed (loss: 0.060455042868852615, acc: 0.9858430027961731)
[2025-02-13 02:45:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:03][root][INFO] - Training Epoch: 1/2, step 1987/7134 completed (loss: 0.06161179021000862, acc: 0.9858356714248657)
[2025-02-13 02:45:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:04][root][INFO] - Training Epoch: 1/2, step 1988/7134 completed (loss: 0.06348460912704468, acc: 0.9857346415519714)
[2025-02-13 02:45:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:04][root][INFO] - Training Epoch: 1/2, step 1989/7134 completed (loss: 0.06529751420021057, acc: 0.9824561476707458)
[2025-02-13 02:45:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:05][root][INFO] - Training Epoch: 1/2, step 1990/7134 completed (loss: 0.042910099029541016, acc: 0.9854369163513184)
[2025-02-13 02:45:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:05][root][INFO] - Training Epoch: 1/2, step 1991/7134 completed (loss: 0.02918531745672226, acc: 0.9867109656333923)
[2025-02-13 02:45:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:05][root][INFO] - Training Epoch: 1/2, step 1992/7134 completed (loss: 0.07931000739336014, acc: 0.9829457402229309)
[2025-02-13 02:45:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:06][root][INFO] - Training Epoch: 1/2, step 1993/7134 completed (loss: 0.01965508610010147, acc: 0.994727611541748)
[2025-02-13 02:45:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:06][root][INFO] - Training Epoch: 1/2, step 1994/7134 completed (loss: 0.022064439952373505, acc: 0.9862068891525269)
[2025-02-13 02:45:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:07][root][INFO] - Training Epoch: 1/2, step 1995/7134 completed (loss: 0.04956286400556564, acc: 0.9860896468162537)
[2025-02-13 02:45:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:07][root][INFO] - Training Epoch: 1/2, step 1996/7134 completed (loss: 0.037522539496421814, acc: 0.9900826215744019)
[2025-02-13 02:45:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:08][root][INFO] - Training Epoch: 1/2, step 1997/7134 completed (loss: 0.02700650691986084, acc: 0.9912663698196411)
[2025-02-13 02:45:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:08][root][INFO] - Training Epoch: 1/2, step 1998/7134 completed (loss: 0.030139919370412827, acc: 0.9905808568000793)
[2025-02-13 02:45:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:08][root][INFO] - Training Epoch: 1/2, step 1999/7134 completed (loss: 0.0419839583337307, acc: 0.9864176511764526)
[2025-02-13 02:45:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:09][root][INFO] - Training Epoch: 1/2, step 2000/7134 completed (loss: 0.04571715369820595, acc: 0.9873816967010498)
[2025-02-13 02:45:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:09][root][INFO] - Training Epoch: 1/2, step 2001/7134 completed (loss: 0.027528410777449608, acc: 0.9939393997192383)
[2025-02-13 02:45:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:10][root][INFO] - Training Epoch: 1/2, step 2002/7134 completed (loss: 0.033298980444669724, acc: 0.9930434823036194)
[2025-02-13 02:45:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:10][root][INFO] - Training Epoch: 1/2, step 2003/7134 completed (loss: 0.030015556141734123, acc: 0.987500011920929)
[2025-02-13 02:45:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:10][root][INFO] - Training Epoch: 1/2, step 2004/7134 completed (loss: 0.2652452886104584, acc: 0.9388185739517212)
[2025-02-13 02:45:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:11][root][INFO] - Training Epoch: 1/2, step 2005/7134 completed (loss: 0.17699386179447174, acc: 0.9698629975318909)
[2025-02-13 02:45:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:11][root][INFO] - Training Epoch: 1/2, step 2006/7134 completed (loss: 0.02288590930402279, acc: 0.9913544654846191)
[2025-02-13 02:45:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:12][root][INFO] - Training Epoch: 1/2, step 2007/7134 completed (loss: 0.05289536714553833, acc: 0.9879518151283264)
[2025-02-13 02:45:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:12][root][INFO] - Training Epoch: 1/2, step 2008/7134 completed (loss: 0.07559748739004135, acc: 0.9786477088928223)
[2025-02-13 02:45:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:13][root][INFO] - Training Epoch: 1/2, step 2009/7134 completed (loss: 0.0482562854886055, acc: 0.9891696572303772)
[2025-02-13 02:45:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:13][root][INFO] - Training Epoch: 1/2, step 2010/7134 completed (loss: 0.030609052628278732, acc: 0.9914236664772034)
[2025-02-13 02:45:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:14][root][INFO] - Training Epoch: 1/2, step 2011/7134 completed (loss: 0.036277320235967636, acc: 0.9823874831199646)
[2025-02-13 02:45:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:14][root][INFO] - Training Epoch: 1/2, step 2012/7134 completed (loss: 0.05773916095495224, acc: 0.9873149991035461)
[2025-02-13 02:45:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:14][root][INFO] - Training Epoch: 1/2, step 2013/7134 completed (loss: 0.03686827793717384, acc: 0.9879999756813049)
[2025-02-13 02:45:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:15][root][INFO] - Training Epoch: 1/2, step 2014/7134 completed (loss: 0.025624414905905724, acc: 0.9937008023262024)
[2025-02-13 02:45:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:15][root][INFO] - Training Epoch: 1/2, step 2015/7134 completed (loss: 0.03679944947361946, acc: 0.9868420958518982)
[2025-02-13 02:45:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:16][root][INFO] - Training Epoch: 1/2, step 2016/7134 completed (loss: 0.07457131147384644, acc: 0.9815384745597839)
[2025-02-13 02:45:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:16][root][INFO] - Training Epoch: 1/2, step 2017/7134 completed (loss: 0.08192343264818192, acc: 0.9810810685157776)
[2025-02-13 02:45:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:17][root][INFO] - Training Epoch: 1/2, step 2018/7134 completed (loss: 0.03889433294534683, acc: 0.985897421836853)
[2025-02-13 02:45:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:17][root][INFO] - Training Epoch: 1/2, step 2019/7134 completed (loss: 0.05857762694358826, acc: 0.977931022644043)
[2025-02-13 02:45:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:17][root][INFO] - Training Epoch: 1/2, step 2020/7134 completed (loss: 0.02968825027346611, acc: 0.9949238300323486)
[2025-02-13 02:45:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:18][root][INFO] - Training Epoch: 1/2, step 2021/7134 completed (loss: 0.06174664571881294, acc: 0.984415590763092)
[2025-02-13 02:45:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:18][root][INFO] - Training Epoch: 1/2, step 2022/7134 completed (loss: 0.08859214931726456, acc: 0.9774696826934814)
[2025-02-13 02:45:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:19][root][INFO] - Training Epoch: 1/2, step 2023/7134 completed (loss: 0.04842989146709442, acc: 0.985401451587677)
[2025-02-13 02:45:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:19][root][INFO] - Training Epoch: 1/2, step 2024/7134 completed (loss: 0.05438660457730293, acc: 0.9833333492279053)
[2025-02-13 02:45:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:19][root][INFO] - Training Epoch: 1/2, step 2025/7134 completed (loss: 0.05462725833058357, acc: 0.9866666793823242)
[2025-02-13 02:45:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:20][root][INFO] - Training Epoch: 1/2, step 2026/7134 completed (loss: 0.08329746872186661, acc: 0.974397599697113)
[2025-02-13 02:45:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:20][root][INFO] - Training Epoch: 1/2, step 2027/7134 completed (loss: 0.055695079267024994, acc: 0.985049843788147)
[2025-02-13 02:45:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:21][root][INFO] - Training Epoch: 1/2, step 2028/7134 completed (loss: 0.04926614463329315, acc: 0.9853801131248474)
[2025-02-13 02:45:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:21][root][INFO] - Training Epoch: 1/2, step 2029/7134 completed (loss: 0.04406123235821724, acc: 0.9857954382896423)
[2025-02-13 02:45:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:22][root][INFO] - Training Epoch: 1/2, step 2030/7134 completed (loss: 0.07428772747516632, acc: 0.9723926186561584)
[2025-02-13 02:45:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:22][root][INFO] - Training Epoch: 1/2, step 2031/7134 completed (loss: 0.05626634508371353, acc: 0.9865030646324158)
[2025-02-13 02:45:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:22][root][INFO] - Training Epoch: 1/2, step 2032/7134 completed (loss: 0.04507611319422722, acc: 0.9905914068222046)
[2025-02-13 02:45:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:23][root][INFO] - Training Epoch: 1/2, step 2033/7134 completed (loss: 0.04698717221617699, acc: 0.9862778782844543)
[2025-02-13 02:45:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:23][root][INFO] - Training Epoch: 1/2, step 2034/7134 completed (loss: 0.06524881720542908, acc: 0.9816232919692993)
[2025-02-13 02:45:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:24][root][INFO] - Training Epoch: 1/2, step 2035/7134 completed (loss: 0.029892105609178543, acc: 0.9900373816490173)
[2025-02-13 02:45:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:24][root][INFO] - Training Epoch: 1/2, step 2036/7134 completed (loss: 0.050066396594047546, acc: 0.9861591458320618)
[2025-02-13 02:45:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:25][root][INFO] - Training Epoch: 1/2, step 2037/7134 completed (loss: 0.02706591971218586, acc: 0.9925816059112549)
[2025-02-13 02:45:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:25][root][INFO] - Training Epoch: 1/2, step 2038/7134 completed (loss: 0.04489752650260925, acc: 0.9858155846595764)
[2025-02-13 02:45:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:25][root][INFO] - Training Epoch: 1/2, step 2039/7134 completed (loss: 0.014838720671832561, acc: 0.9981982111930847)
[2025-02-13 02:45:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:26][root][INFO] - Training Epoch: 1/2, step 2040/7134 completed (loss: 0.06086192652583122, acc: 0.9814814925193787)
[2025-02-13 02:45:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:26][root][INFO] - Training Epoch: 1/2, step 2041/7134 completed (loss: 0.01619863323867321, acc: 0.9958791136741638)
[2025-02-13 02:45:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:27][root][INFO] - Training Epoch: 1/2, step 2042/7134 completed (loss: 0.0648498386144638, acc: 0.9798387289047241)
[2025-02-13 02:45:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:27][root][INFO] - Training Epoch: 1/2, step 2043/7134 completed (loss: 0.03996381163597107, acc: 0.9906250238418579)
[2025-02-13 02:45:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:28][root][INFO] - Training Epoch: 1/2, step 2044/7134 completed (loss: 0.037665970623493195, acc: 0.9928443431854248)
[2025-02-13 02:45:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:28][root][INFO] - Training Epoch: 1/2, step 2045/7134 completed (loss: 0.024654630571603775, acc: 0.9909583926200867)
[2025-02-13 02:45:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:28][root][INFO] - Training Epoch: 1/2, step 2046/7134 completed (loss: 0.036493945866823196, acc: 0.9898374080657959)
[2025-02-13 02:45:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:29][root][INFO] - Training Epoch: 1/2, step 2047/7134 completed (loss: 0.03973713144659996, acc: 0.989983320236206)
[2025-02-13 02:45:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:29][root][INFO] - Training Epoch: 1/2, step 2048/7134 completed (loss: 0.08768083155155182, acc: 0.9820627570152283)
[2025-02-13 02:45:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:30][root][INFO] - Training Epoch: 1/2, step 2049/7134 completed (loss: 0.1137431412935257, acc: 0.9717646837234497)
[2025-02-13 02:45:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:30][root][INFO] - Training Epoch: 1/2, step 2050/7134 completed (loss: 0.10569176822900772, acc: 0.9794871807098389)
[2025-02-13 02:45:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:30][root][INFO] - Training Epoch: 1/2, step 2051/7134 completed (loss: 0.06538033485412598, acc: 0.984375)
[2025-02-13 02:45:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:31][root][INFO] - Training Epoch: 1/2, step 2052/7134 completed (loss: 0.11161182820796967, acc: 0.9726962447166443)
[2025-02-13 02:45:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:31][root][INFO] - Training Epoch: 1/2, step 2053/7134 completed (loss: 0.058922842144966125, acc: 0.9800000190734863)
[2025-02-13 02:45:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:32][root][INFO] - Training Epoch: 1/2, step 2054/7134 completed (loss: 0.11363790929317474, acc: 0.9713375568389893)
[2025-02-13 02:45:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:32][root][INFO] - Training Epoch: 1/2, step 2055/7134 completed (loss: 0.09980949014425278, acc: 0.977952778339386)
[2025-02-13 02:45:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:33][root][INFO] - Training Epoch: 1/2, step 2056/7134 completed (loss: 0.06576322019100189, acc: 0.9805447459220886)
[2025-02-13 02:45:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:33][root][INFO] - Training Epoch: 1/2, step 2057/7134 completed (loss: 0.07199197262525558, acc: 0.9810040593147278)
[2025-02-13 02:45:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:33][root][INFO] - Training Epoch: 1/2, step 2058/7134 completed (loss: 0.11347448080778122, acc: 0.9684361815452576)
[2025-02-13 02:45:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:34][root][INFO] - Training Epoch: 1/2, step 2059/7134 completed (loss: 0.12976635992527008, acc: 0.9740034937858582)
[2025-02-13 02:45:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:34][root][INFO] - Training Epoch: 1/2, step 2060/7134 completed (loss: 0.049526117742061615, acc: 0.9842632412910461)
[2025-02-13 02:45:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:35][root][INFO] - Training Epoch: 1/2, step 2061/7134 completed (loss: 0.07783573865890503, acc: 0.9844192862510681)
[2025-02-13 02:45:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:35][root][INFO] - Training Epoch: 1/2, step 2062/7134 completed (loss: 0.0972970575094223, acc: 0.9772295951843262)
[2025-02-13 02:45:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:35][root][INFO] - Training Epoch: 1/2, step 2063/7134 completed (loss: 0.08874410390853882, acc: 0.9776119589805603)
[2025-02-13 02:45:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:36][root][INFO] - Training Epoch: 1/2, step 2064/7134 completed (loss: 0.03667397052049637, acc: 0.9906250238418579)
[2025-02-13 02:45:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:36][root][INFO] - Training Epoch: 1/2, step 2065/7134 completed (loss: 0.1162634789943695, acc: 0.9763779640197754)
[2025-02-13 02:45:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:37][root][INFO] - Training Epoch: 1/2, step 2066/7134 completed (loss: 0.05166052281856537, acc: 0.9879759550094604)
[2025-02-13 02:45:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:37][root][INFO] - Training Epoch: 1/2, step 2067/7134 completed (loss: 0.047449223697185516, acc: 0.9829221963882446)
[2025-02-13 02:45:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:38][root][INFO] - Training Epoch: 1/2, step 2068/7134 completed (loss: 0.08668424189090729, acc: 0.976580798625946)
[2025-02-13 02:45:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:38][root][INFO] - Training Epoch: 1/2, step 2069/7134 completed (loss: 0.053283415734767914, acc: 0.9784615635871887)
[2025-02-13 02:45:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:38][root][INFO] - Training Epoch: 1/2, step 2070/7134 completed (loss: 0.05142592266201973, acc: 0.9847908616065979)
[2025-02-13 02:45:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:39][root][INFO] - Training Epoch: 1/2, step 2071/7134 completed (loss: 0.026933442801237106, acc: 0.9976580739021301)
[2025-02-13 02:45:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:39][root][INFO] - Training Epoch: 1/2, step 2072/7134 completed (loss: 0.032030217349529266, acc: 0.9950494766235352)
[2025-02-13 02:45:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:40][root][INFO] - Training Epoch: 1/2, step 2073/7134 completed (loss: 0.03488833084702492, acc: 0.9923076629638672)
[2025-02-13 02:45:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:40][root][INFO] - Training Epoch: 1/2, step 2074/7134 completed (loss: 0.05727316066622734, acc: 0.9823943376541138)
[2025-02-13 02:45:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:40][root][INFO] - Training Epoch: 1/2, step 2075/7134 completed (loss: 0.06929153949022293, acc: 0.9799666404724121)
[2025-02-13 02:45:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:41][root][INFO] - Training Epoch: 1/2, step 2076/7134 completed (loss: 0.04882775992155075, acc: 0.9876922965049744)
[2025-02-13 02:45:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:41][root][INFO] - Training Epoch: 1/2, step 2077/7134 completed (loss: 0.03207169473171234, acc: 0.9893758296966553)
[2025-02-13 02:45:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:42][root][INFO] - Training Epoch: 1/2, step 2078/7134 completed (loss: 0.03211016207933426, acc: 0.9941860437393188)
[2025-02-13 02:45:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:42][root][INFO] - Training Epoch: 1/2, step 2079/7134 completed (loss: 0.03281126916408539, acc: 0.9907407164573669)
[2025-02-13 02:45:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:43][root][INFO] - Training Epoch: 1/2, step 2080/7134 completed (loss: 0.044899288564920425, acc: 0.9897058606147766)
[2025-02-13 02:45:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:43][root][INFO] - Training Epoch: 1/2, step 2081/7134 completed (loss: 0.018397243693470955, acc: 0.9956458806991577)
[2025-02-13 02:45:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:44][root][INFO] - Training Epoch: 1/2, step 2082/7134 completed (loss: 0.10384386777877808, acc: 0.9706498980522156)
[2025-02-13 02:45:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:44][root][INFO] - Training Epoch: 1/2, step 2083/7134 completed (loss: 0.04637783020734787, acc: 0.9925925731658936)
[2025-02-13 02:45:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:44][root][INFO] - Training Epoch: 1/2, step 2084/7134 completed (loss: 0.03299117833375931, acc: 0.9906832575798035)
[2025-02-13 02:45:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:45][root][INFO] - Training Epoch: 1/2, step 2085/7134 completed (loss: 0.029593253508210182, acc: 0.9907578825950623)
[2025-02-13 02:45:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:45][root][INFO] - Training Epoch: 1/2, step 2086/7134 completed (loss: 0.040746789425611496, acc: 0.9885844588279724)
[2025-02-13 02:45:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:46][root][INFO] - Training Epoch: 1/2, step 2087/7134 completed (loss: 0.01242759358137846, acc: 0.9957325458526611)
[2025-02-13 02:45:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:46][root][INFO] - Training Epoch: 1/2, step 2088/7134 completed (loss: 0.04476785287261009, acc: 0.9884726405143738)
[2025-02-13 02:45:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:46][root][INFO] - Training Epoch: 1/2, step 2089/7134 completed (loss: 0.027286475524306297, acc: 0.9921135902404785)
[2025-02-13 02:45:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:47][root][INFO] - Training Epoch: 1/2, step 2090/7134 completed (loss: 0.01956336200237274, acc: 0.9952606558799744)
[2025-02-13 02:45:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:47][root][INFO] - Training Epoch: 1/2, step 2091/7134 completed (loss: 0.03879791870713234, acc: 0.9915540814399719)
[2025-02-13 02:45:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:48][root][INFO] - Training Epoch: 1/2, step 2092/7134 completed (loss: 0.01214997936040163, acc: 0.9968454241752625)
[2025-02-13 02:45:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:48][root][INFO] - Training Epoch: 1/2, step 2093/7134 completed (loss: 0.03257416933774948, acc: 0.9905020594596863)
[2025-02-13 02:45:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:49][root][INFO] - Training Epoch: 1/2, step 2094/7134 completed (loss: 0.08608850091695786, acc: 0.9776875972747803)
[2025-02-13 02:45:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:49][root][INFO] - Training Epoch: 1/2, step 2095/7134 completed (loss: 0.050006765872240067, acc: 0.9868074059486389)
[2025-02-13 02:45:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:49][root][INFO] - Training Epoch: 1/2, step 2096/7134 completed (loss: 0.04178640618920326, acc: 0.9931034445762634)
[2025-02-13 02:45:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:50][root][INFO] - Training Epoch: 1/2, step 2097/7134 completed (loss: 0.0326656848192215, acc: 0.9901823401451111)
[2025-02-13 02:45:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:50][root][INFO] - Training Epoch: 1/2, step 2098/7134 completed (loss: 0.029776733368635178, acc: 0.9903537034988403)
[2025-02-13 02:45:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:51][root][INFO] - Training Epoch: 1/2, step 2099/7134 completed (loss: 0.03561757504940033, acc: 0.988990843296051)
[2025-02-13 02:45:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:51][root][INFO] - Training Epoch: 1/2, step 2100/7134 completed (loss: 0.04350206255912781, acc: 0.9893758296966553)
[2025-02-13 02:45:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:52][root][INFO] - Training Epoch: 1/2, step 2101/7134 completed (loss: 0.025762265548110008, acc: 0.989924430847168)
[2025-02-13 02:45:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:52][root][INFO] - Training Epoch: 1/2, step 2102/7134 completed (loss: 0.035503558814525604, acc: 0.9885203838348389)
[2025-02-13 02:45:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:52][root][INFO] - Training Epoch: 1/2, step 2103/7134 completed (loss: 0.03511602431535721, acc: 0.9907975196838379)
[2025-02-13 02:45:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:53][root][INFO] - Training Epoch: 1/2, step 2104/7134 completed (loss: 0.03041495941579342, acc: 0.9884020686149597)
[2025-02-13 02:45:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:53][root][INFO] - Training Epoch: 1/2, step 2105/7134 completed (loss: 0.0755942314863205, acc: 0.9796556830406189)
[2025-02-13 02:45:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:54][root][INFO] - Training Epoch: 1/2, step 2106/7134 completed (loss: 0.0638435110449791, acc: 0.9813218116760254)
[2025-02-13 02:45:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:54][root][INFO] - Training Epoch: 1/2, step 2107/7134 completed (loss: 0.041359011083841324, acc: 0.9932523369789124)
[2025-02-13 02:45:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:55][root][INFO] - Training Epoch: 1/2, step 2108/7134 completed (loss: 0.039853110909461975, acc: 0.9865471124649048)
[2025-02-13 02:45:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:55][root][INFO] - Training Epoch: 1/2, step 2109/7134 completed (loss: 0.09330683201551437, acc: 0.9841269850730896)
[2025-02-13 02:45:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:55][root][INFO] - Training Epoch: 1/2, step 2110/7134 completed (loss: 0.09452132135629654, acc: 0.9825581312179565)
[2025-02-13 02:45:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:56][root][INFO] - Training Epoch: 1/2, step 2111/7134 completed (loss: 0.05870319530367851, acc: 0.9792477488517761)
[2025-02-13 02:45:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:56][root][INFO] - Training Epoch: 1/2, step 2112/7134 completed (loss: 0.12345018982887268, acc: 0.9681528806686401)
[2025-02-13 02:45:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:57][root][INFO] - Training Epoch: 1/2, step 2113/7134 completed (loss: 0.03925958648324013, acc: 0.9895678162574768)
[2025-02-13 02:45:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:57][root][INFO] - Training Epoch: 1/2, step 2114/7134 completed (loss: 0.03875201940536499, acc: 0.9882352948188782)
[2025-02-13 02:45:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:58][root][INFO] - Training Epoch: 1/2, step 2115/7134 completed (loss: 0.048349469900131226, acc: 0.9824780821800232)
[2025-02-13 02:45:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:58][root][INFO] - Training Epoch: 1/2, step 2116/7134 completed (loss: 0.06078428030014038, acc: 0.9861963391304016)
[2025-02-13 02:45:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:58][root][INFO] - Training Epoch: 1/2, step 2117/7134 completed (loss: 0.030929356813430786, acc: 0.9888059496879578)
[2025-02-13 02:45:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:59][root][INFO] - Training Epoch: 1/2, step 2118/7134 completed (loss: 0.043791741132736206, acc: 0.9918144345283508)
[2025-02-13 02:45:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:45:59][root][INFO] - Training Epoch: 1/2, step 2119/7134 completed (loss: 0.04718061536550522, acc: 0.9855642914772034)
[2025-02-13 02:45:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:00][root][INFO] - Training Epoch: 1/2, step 2120/7134 completed (loss: 0.05915611982345581, acc: 0.9847792983055115)
[2025-02-13 02:46:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:00][root][INFO] - Training Epoch: 1/2, step 2121/7134 completed (loss: 0.05167637765407562, acc: 0.9857142567634583)
[2025-02-13 02:46:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:00][root][INFO] - Training Epoch: 1/2, step 2122/7134 completed (loss: 0.03566877171397209, acc: 0.9921875)
[2025-02-13 02:46:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:01][root][INFO] - Training Epoch: 1/2, step 2123/7134 completed (loss: 0.047880642116069794, acc: 0.9858490824699402)
[2025-02-13 02:46:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:01][root][INFO] - Training Epoch: 1/2, step 2124/7134 completed (loss: 0.029518213123083115, acc: 0.9922118186950684)
[2025-02-13 02:46:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:02][root][INFO] - Training Epoch: 1/2, step 2125/7134 completed (loss: 0.02164885215461254, acc: 0.9954545497894287)
[2025-02-13 02:46:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:02][root][INFO] - Training Epoch: 1/2, step 2126/7134 completed (loss: 0.030622228980064392, acc: 0.9915966391563416)
[2025-02-13 02:46:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:03][root][INFO] - Training Epoch: 1/2, step 2127/7134 completed (loss: 0.0348142646253109, acc: 0.99262535572052)
[2025-02-13 02:46:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:03][root][INFO] - Training Epoch: 1/2, step 2128/7134 completed (loss: 0.04043702036142349, acc: 0.9895012974739075)
[2025-02-13 02:46:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:03][root][INFO] - Training Epoch: 1/2, step 2129/7134 completed (loss: 0.04600362479686737, acc: 0.9864253401756287)
[2025-02-13 02:46:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:04][root][INFO] - Training Epoch: 1/2, step 2130/7134 completed (loss: 0.055702000856399536, acc: 0.9851484894752502)
[2025-02-13 02:46:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:04][root][INFO] - Training Epoch: 1/2, step 2131/7134 completed (loss: 0.02666197530925274, acc: 0.9891641139984131)
[2025-02-13 02:46:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:05][root][INFO] - Training Epoch: 1/2, step 2132/7134 completed (loss: 0.023345859721302986, acc: 0.9905660152435303)
[2025-02-13 02:46:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:05][root][INFO] - Training Epoch: 1/2, step 2133/7134 completed (loss: 0.03414013981819153, acc: 0.9881109595298767)
[2025-02-13 02:46:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:05][root][INFO] - Training Epoch: 1/2, step 2134/7134 completed (loss: 0.03234301880002022, acc: 0.9879518151283264)
[2025-02-13 02:46:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:06][root][INFO] - Training Epoch: 1/2, step 2135/7134 completed (loss: 0.04601648449897766, acc: 0.9804216623306274)
[2025-02-13 02:46:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:06][root][INFO] - Training Epoch: 1/2, step 2136/7134 completed (loss: 0.01804126240313053, acc: 0.9959946870803833)
[2025-02-13 02:46:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:07][root][INFO] - Training Epoch: 1/2, step 2137/7134 completed (loss: 0.027430614456534386, acc: 0.9883871078491211)
[2025-02-13 02:46:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:07][root][INFO] - Training Epoch: 1/2, step 2138/7134 completed (loss: 0.012121135368943214, acc: 0.9984756112098694)
[2025-02-13 02:46:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:08][root][INFO] - Training Epoch: 1/2, step 2139/7134 completed (loss: 0.01863471232354641, acc: 0.994452178478241)
[2025-02-13 02:46:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:08][root][INFO] - Training Epoch: 1/2, step 2140/7134 completed (loss: 0.07007569819688797, acc: 0.9820936918258667)
[2025-02-13 02:46:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:08][root][INFO] - Training Epoch: 1/2, step 2141/7134 completed (loss: 0.023915210738778114, acc: 0.9912280440330505)
[2025-02-13 02:46:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:09][root][INFO] - Training Epoch: 1/2, step 2142/7134 completed (loss: 0.03762262314558029, acc: 0.9872521162033081)
[2025-02-13 02:46:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:09][root][INFO] - Training Epoch: 1/2, step 2143/7134 completed (loss: 0.02439008094370365, acc: 0.9912280440330505)
[2025-02-13 02:46:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:10][root][INFO] - Training Epoch: 1/2, step 2144/7134 completed (loss: 0.008101575076580048, acc: 0.9986720085144043)
[2025-02-13 02:46:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:10][root][INFO] - Training Epoch: 1/2, step 2145/7134 completed (loss: 0.055830396711826324, acc: 0.9828326106071472)
[2025-02-13 02:46:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:11][root][INFO] - Training Epoch: 1/2, step 2146/7134 completed (loss: 0.047341108322143555, acc: 0.98591548204422)
[2025-02-13 02:46:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:11][root][INFO] - Training Epoch: 1/2, step 2147/7134 completed (loss: 0.04975664243102074, acc: 0.9862843155860901)
[2025-02-13 02:46:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:12][root][INFO] - Training Epoch: 1/2, step 2148/7134 completed (loss: 0.05828065052628517, acc: 0.983988344669342)
[2025-02-13 02:46:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:12][root][INFO] - Training Epoch: 1/2, step 2149/7134 completed (loss: 0.013905631378293037, acc: 0.9957746267318726)
[2025-02-13 02:46:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:12][root][INFO] - Training Epoch: 1/2, step 2150/7134 completed (loss: 0.04599911347031593, acc: 0.9879999756813049)
[2025-02-13 02:46:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:13][root][INFO] - Training Epoch: 1/2, step 2151/7134 completed (loss: 0.031160561367869377, acc: 0.991428554058075)
[2025-02-13 02:46:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:13][root][INFO] - Training Epoch: 1/2, step 2152/7134 completed (loss: 0.11621006578207016, acc: 0.966911792755127)
[2025-02-13 02:46:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:14][root][INFO] - Training Epoch: 1/2, step 2153/7134 completed (loss: 0.06797833740711212, acc: 0.9833518266677856)
[2025-02-13 02:46:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:14][root][INFO] - Training Epoch: 1/2, step 2154/7134 completed (loss: 0.06155452877283096, acc: 0.983565092086792)
[2025-02-13 02:46:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:15][root][INFO] - Training Epoch: 1/2, step 2155/7134 completed (loss: 0.09008103609085083, acc: 0.9862637519836426)
[2025-02-13 02:46:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:15][root][INFO] - Training Epoch: 1/2, step 2156/7134 completed (loss: 0.05070992186665535, acc: 0.9878472089767456)
[2025-02-13 02:46:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:16][root][INFO] - Training Epoch: 1/2, step 2157/7134 completed (loss: 0.03199080750346184, acc: 0.9877095222473145)
[2025-02-13 02:46:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:16][root][INFO] - Training Epoch: 1/2, step 2158/7134 completed (loss: 0.06829198449850082, acc: 0.9841269850730896)
[2025-02-13 02:46:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:17][root][INFO] - Training Epoch: 1/2, step 2159/7134 completed (loss: 0.03416473791003227, acc: 0.9948914647102356)
[2025-02-13 02:46:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:17][root][INFO] - Training Epoch: 1/2, step 2160/7134 completed (loss: 0.04825013875961304, acc: 0.9876203536987305)
[2025-02-13 02:46:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:17][root][INFO] - Training Epoch: 1/2, step 2161/7134 completed (loss: 0.06300776451826096, acc: 0.984635055065155)
[2025-02-13 02:46:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:18][root][INFO] - Training Epoch: 1/2, step 2162/7134 completed (loss: 0.0505959615111351, acc: 0.9848713874816895)
[2025-02-13 02:46:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:18][root][INFO] - Training Epoch: 1/2, step 2163/7134 completed (loss: 0.04724161699414253, acc: 0.98740154504776)
[2025-02-13 02:46:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:19][root][INFO] - Training Epoch: 1/2, step 2164/7134 completed (loss: 0.03596087172627449, acc: 0.989708423614502)
[2025-02-13 02:46:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:19][root][INFO] - Training Epoch: 1/2, step 2165/7134 completed (loss: 0.04043617099523544, acc: 0.9870874881744385)
[2025-02-13 02:46:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:20][root][INFO] - Training Epoch: 1/2, step 2166/7134 completed (loss: 0.04172953590750694, acc: 0.98828125)
[2025-02-13 02:46:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:20][root][INFO] - Training Epoch: 1/2, step 2167/7134 completed (loss: 0.037562351673841476, acc: 0.9888641238212585)
[2025-02-13 02:46:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:20][root][INFO] - Training Epoch: 1/2, step 2168/7134 completed (loss: 0.0193795133382082, acc: 0.9968253970146179)
[2025-02-13 02:46:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:21][root][INFO] - Training Epoch: 1/2, step 2169/7134 completed (loss: 0.044130392372608185, acc: 0.988399088382721)
[2025-02-13 02:46:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:21][root][INFO] - Training Epoch: 1/2, step 2170/7134 completed (loss: 0.049780670553445816, acc: 0.9883551597595215)
[2025-02-13 02:46:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:22][root][INFO] - Training Epoch: 1/2, step 2171/7134 completed (loss: 0.0357232540845871, acc: 0.9909399747848511)
[2025-02-13 02:46:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:22][root][INFO] - Training Epoch: 1/2, step 2172/7134 completed (loss: 0.040519051253795624, acc: 0.9886363744735718)
[2025-02-13 02:46:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:23][root][INFO] - Training Epoch: 1/2, step 2173/7134 completed (loss: 0.04874343052506447, acc: 0.9894259572029114)
[2025-02-13 02:46:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:23][root][INFO] - Training Epoch: 1/2, step 2174/7134 completed (loss: 0.03315160796046257, acc: 0.9858155846595764)
[2025-02-13 02:46:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:24][root][INFO] - Training Epoch: 1/2, step 2175/7134 completed (loss: 0.09410054236650467, acc: 0.9806094169616699)
[2025-02-13 02:46:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:24][root][INFO] - Training Epoch: 1/2, step 2176/7134 completed (loss: 0.040035784244537354, acc: 0.9869668483734131)
[2025-02-13 02:46:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:25][root][INFO] - Training Epoch: 1/2, step 2177/7134 completed (loss: 0.04670150578022003, acc: 0.9883117079734802)
[2025-02-13 02:46:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:25][root][INFO] - Training Epoch: 1/2, step 2178/7134 completed (loss: 0.0309004969894886, acc: 0.9933775067329407)
[2025-02-13 02:46:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:25][root][INFO] - Training Epoch: 1/2, step 2179/7134 completed (loss: 0.036454569548368454, acc: 0.9924050569534302)
[2025-02-13 02:46:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:26][root][INFO] - Training Epoch: 1/2, step 2180/7134 completed (loss: 0.03585398942232132, acc: 0.9940898418426514)
[2025-02-13 02:46:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:26][root][INFO] - Training Epoch: 1/2, step 2181/7134 completed (loss: 0.020478880032896996, acc: 0.9928469061851501)
[2025-02-13 02:46:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:27][root][INFO] - Training Epoch: 1/2, step 2182/7134 completed (loss: 0.05185888335108757, acc: 0.985005795955658)
[2025-02-13 02:46:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:27][root][INFO] - Training Epoch: 1/2, step 2183/7134 completed (loss: 0.03015444241464138, acc: 0.9930151104927063)
[2025-02-13 02:46:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:28][root][INFO] - Training Epoch: 1/2, step 2184/7134 completed (loss: 0.033658217638731, acc: 0.9920127987861633)
[2025-02-13 02:46:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:28][root][INFO] - Training Epoch: 1/2, step 2185/7134 completed (loss: 0.08163493871688843, acc: 0.9787535667419434)
[2025-02-13 02:46:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:29][root][INFO] - Training Epoch: 1/2, step 2186/7134 completed (loss: 0.09809589385986328, acc: 0.9740680456161499)
[2025-02-13 02:46:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:29][root][INFO] - Training Epoch: 1/2, step 2187/7134 completed (loss: 0.13876616954803467, acc: 0.9648000001907349)
[2025-02-13 02:46:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:30][root][INFO] - Training Epoch: 1/2, step 2188/7134 completed (loss: 0.039737358689308167, acc: 0.9835841059684753)
[2025-02-13 02:46:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:30][root][INFO] - Training Epoch: 1/2, step 2189/7134 completed (loss: 0.06871441006660461, acc: 0.9785932898521423)
[2025-02-13 02:46:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:30][root][INFO] - Training Epoch: 1/2, step 2190/7134 completed (loss: 0.04533780738711357, acc: 0.9894459247589111)
[2025-02-13 02:46:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:31][root][INFO] - Training Epoch: 1/2, step 2191/7134 completed (loss: 0.06423680484294891, acc: 0.9756097793579102)
[2025-02-13 02:46:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:31][root][INFO] - Training Epoch: 1/2, step 2192/7134 completed (loss: 0.02288929373025894, acc: 0.9905837774276733)
[2025-02-13 02:46:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:32][root][INFO] - Training Epoch: 1/2, step 2193/7134 completed (loss: 0.0378388911485672, acc: 0.9840425252914429)
[2025-02-13 02:46:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:32][root][INFO] - Training Epoch: 1/2, step 2194/7134 completed (loss: 0.04250684008002281, acc: 0.979899525642395)
[2025-02-13 02:46:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:32][root][INFO] - Training Epoch: 1/2, step 2195/7134 completed (loss: 0.030460568144917488, acc: 0.9919999837875366)
[2025-02-13 02:46:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:33][root][INFO] - Training Epoch: 1/2, step 2196/7134 completed (loss: 0.04035510495305061, acc: 0.9870466589927673)
[2025-02-13 02:46:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:33][root][INFO] - Training Epoch: 1/2, step 2197/7134 completed (loss: 0.027417240664362907, acc: 0.9908925294876099)
[2025-02-13 02:46:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:34][root][INFO] - Training Epoch: 1/2, step 2198/7134 completed (loss: 0.06411474198102951, acc: 0.9834087491035461)
[2025-02-13 02:46:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:34][root][INFO] - Training Epoch: 1/2, step 2199/7134 completed (loss: 0.028918854892253876, acc: 0.9894259572029114)
[2025-02-13 02:46:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:35][root][INFO] - Training Epoch: 1/2, step 2200/7134 completed (loss: 0.04629005491733551, acc: 0.9801084995269775)
[2025-02-13 02:46:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:35][root][INFO] - Training Epoch: 1/2, step 2201/7134 completed (loss: 0.02478676661849022, acc: 0.9909090995788574)
[2025-02-13 02:46:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:35][root][INFO] - Training Epoch: 1/2, step 2202/7134 completed (loss: 0.052939023822546005, acc: 0.9866443872451782)
[2025-02-13 02:46:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:36][root][INFO] - Training Epoch: 1/2, step 2203/7134 completed (loss: 0.022336792200803757, acc: 0.9890410900115967)
[2025-02-13 02:46:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:36][root][INFO] - Training Epoch: 1/2, step 2204/7134 completed (loss: 0.029357558116316795, acc: 0.9868420958518982)
[2025-02-13 02:46:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:37][root][INFO] - Training Epoch: 1/2, step 2205/7134 completed (loss: 0.02899027429521084, acc: 0.9947368502616882)
[2025-02-13 02:46:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:37][root][INFO] - Training Epoch: 1/2, step 2206/7134 completed (loss: 0.07454781234264374, acc: 0.9841827750205994)
[2025-02-13 02:46:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:38][root][INFO] - Training Epoch: 1/2, step 2207/7134 completed (loss: 0.02797536365687847, acc: 0.993537962436676)
[2025-02-13 02:46:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:38][root][INFO] - Training Epoch: 1/2, step 2208/7134 completed (loss: 0.053940631449222565, acc: 0.9917355179786682)
[2025-02-13 02:46:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:38][root][INFO] - Training Epoch: 1/2, step 2209/7134 completed (loss: 0.041096463799476624, acc: 0.9886845946311951)
[2025-02-13 02:46:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:39][root][INFO] - Training Epoch: 1/2, step 2210/7134 completed (loss: 0.05048587918281555, acc: 0.987860381603241)
[2025-02-13 02:46:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:39][root][INFO] - Training Epoch: 1/2, step 2211/7134 completed (loss: 0.03454155847430229, acc: 0.9930747747421265)
[2025-02-13 02:46:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:40][root][INFO] - Training Epoch: 1/2, step 2212/7134 completed (loss: 0.01253929827362299, acc: 0.9958333373069763)
[2025-02-13 02:46:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:40][root][INFO] - Training Epoch: 1/2, step 2213/7134 completed (loss: 0.02880026586353779, acc: 0.991525411605835)
[2025-02-13 02:46:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:41][root][INFO] - Training Epoch: 1/2, step 2214/7134 completed (loss: 0.043023426085710526, acc: 0.9850746393203735)
[2025-02-13 02:46:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:41][root][INFO] - Training Epoch: 1/2, step 2215/7134 completed (loss: 0.02950301580131054, acc: 0.992977499961853)
[2025-02-13 02:46:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:41][root][INFO] - Training Epoch: 1/2, step 2216/7134 completed (loss: 0.023517651483416557, acc: 0.9906542301177979)
[2025-02-13 02:46:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:42][root][INFO] - Training Epoch: 1/2, step 2217/7134 completed (loss: 0.03559967502951622, acc: 0.9912827014923096)
[2025-02-13 02:46:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:42][root][INFO] - Training Epoch: 1/2, step 2218/7134 completed (loss: 0.04232209175825119, acc: 0.9885057210922241)
[2025-02-13 02:46:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:43][root][INFO] - Training Epoch: 1/2, step 2219/7134 completed (loss: 0.018442686647176743, acc: 0.9938176274299622)
[2025-02-13 02:46:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:43][root][INFO] - Training Epoch: 1/2, step 2220/7134 completed (loss: 0.027801744639873505, acc: 0.9905660152435303)
[2025-02-13 02:46:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:44][root][INFO] - Training Epoch: 1/2, step 2221/7134 completed (loss: 0.03443893790245056, acc: 0.9935064911842346)
[2025-02-13 02:46:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:44][root][INFO] - Training Epoch: 1/2, step 2222/7134 completed (loss: 0.03670237585902214, acc: 0.9841040372848511)
[2025-02-13 02:46:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:45][root][INFO] - Training Epoch: 1/2, step 2223/7134 completed (loss: 0.04444316402077675, acc: 0.9860529899597168)
[2025-02-13 02:46:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:45][root][INFO] - Training Epoch: 1/2, step 2224/7134 completed (loss: 0.03309641033411026, acc: 0.9890260696411133)
[2025-02-13 02:46:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:45][root][INFO] - Training Epoch: 1/2, step 2225/7134 completed (loss: 0.03639734163880348, acc: 0.9913169145584106)
[2025-02-13 02:46:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:46][root][INFO] - Training Epoch: 1/2, step 2226/7134 completed (loss: 0.028497956693172455, acc: 0.990867555141449)
[2025-02-13 02:46:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:46][root][INFO] - Training Epoch: 1/2, step 2227/7134 completed (loss: 0.02688547782599926, acc: 0.9912152290344238)
[2025-02-13 02:46:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:47][root][INFO] - Training Epoch: 1/2, step 2228/7134 completed (loss: 0.017831603065133095, acc: 0.9918699264526367)
[2025-02-13 02:46:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:47][root][INFO] - Training Epoch: 1/2, step 2229/7134 completed (loss: 0.03121713548898697, acc: 0.9940000176429749)
[2025-02-13 02:46:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:48][root][INFO] - Training Epoch: 1/2, step 2230/7134 completed (loss: 0.021642662584781647, acc: 0.9967637658119202)
[2025-02-13 02:46:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:48][root][INFO] - Training Epoch: 1/2, step 2231/7134 completed (loss: 0.053083259612321854, acc: 0.9840637445449829)
[2025-02-13 02:46:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:48][root][INFO] - Training Epoch: 1/2, step 2232/7134 completed (loss: 0.01106733176857233, acc: 0.9973261952400208)
[2025-02-13 02:46:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:49][root][INFO] - Training Epoch: 1/2, step 2233/7134 completed (loss: 0.03394263610243797, acc: 0.9886202216148376)
[2025-02-13 02:46:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:49][root][INFO] - Training Epoch: 1/2, step 2234/7134 completed (loss: 0.014321732334792614, acc: 0.9972677826881409)
[2025-02-13 02:46:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:50][root][INFO] - Training Epoch: 1/2, step 2235/7134 completed (loss: 0.05668579041957855, acc: 0.9874607920646667)
[2025-02-13 02:46:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:50][root][INFO] - Training Epoch: 1/2, step 2236/7134 completed (loss: 0.023029306903481483, acc: 0.9925037622451782)
[2025-02-13 02:46:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:51][root][INFO] - Training Epoch: 1/2, step 2237/7134 completed (loss: 0.05519605800509453, acc: 0.9886845946311951)
[2025-02-13 02:46:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:51][root][INFO] - Training Epoch: 1/2, step 2238/7134 completed (loss: 0.07702978700399399, acc: 0.9797394871711731)
[2025-02-13 02:46:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:51][root][INFO] - Training Epoch: 1/2, step 2239/7134 completed (loss: 0.10043925791978836, acc: 0.9740259647369385)
[2025-02-13 02:46:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:52][root][INFO] - Training Epoch: 1/2, step 2240/7134 completed (loss: 0.06130305677652359, acc: 0.983116865158081)
[2025-02-13 02:46:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:52][root][INFO] - Training Epoch: 1/2, step 2241/7134 completed (loss: 0.08801676332950592, acc: 0.9716312289237976)
[2025-02-13 02:46:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:53][root][INFO] - Training Epoch: 1/2, step 2242/7134 completed (loss: 0.1311378926038742, acc: 0.9691211581230164)
[2025-02-13 02:46:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:53][root][INFO] - Training Epoch: 1/2, step 2243/7134 completed (loss: 0.13244695961475372, acc: 0.9557251930236816)
[2025-02-13 02:46:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:54][root][INFO] - Training Epoch: 1/2, step 2244/7134 completed (loss: 0.043273068964481354, acc: 0.9811320900917053)
[2025-02-13 02:46:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:54][root][INFO] - Training Epoch: 1/2, step 2245/7134 completed (loss: 0.10331141203641891, acc: 0.9676375389099121)
[2025-02-13 02:46:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:55][root][INFO] - Training Epoch: 1/2, step 2246/7134 completed (loss: 0.14520883560180664, acc: 0.958108127117157)
[2025-02-13 02:46:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:55][root][INFO] - Training Epoch: 1/2, step 2247/7134 completed (loss: 0.1339532434940338, acc: 0.9741379022598267)
[2025-02-13 02:46:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:56][root][INFO] - Training Epoch: 1/2, step 2248/7134 completed (loss: 0.07299397140741348, acc: 0.9806362390518188)
[2025-02-13 02:46:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:56][root][INFO] - Training Epoch: 1/2, step 2249/7134 completed (loss: 0.0716136023402214, acc: 0.9790502786636353)
[2025-02-13 02:46:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:57][root][INFO] - Training Epoch: 1/2, step 2250/7134 completed (loss: 0.07295899838209152, acc: 0.9799465537071228)
[2025-02-13 02:46:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:57][root][INFO] - Training Epoch: 1/2, step 2251/7134 completed (loss: 0.05684234946966171, acc: 0.9841269850730896)
[2025-02-13 02:46:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:57][root][INFO] - Training Epoch: 1/2, step 2252/7134 completed (loss: 0.04094753414392471, acc: 0.9887955188751221)
[2025-02-13 02:46:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:58][root][INFO] - Training Epoch: 1/2, step 2253/7134 completed (loss: 0.05042503401637077, acc: 0.9882943034172058)
[2025-02-13 02:46:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:58][root][INFO] - Training Epoch: 1/2, step 2254/7134 completed (loss: 0.13317915797233582, acc: 0.9561753273010254)
[2025-02-13 02:46:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:59][root][INFO] - Training Epoch: 1/2, step 2255/7134 completed (loss: 0.04831325635313988, acc: 0.9863429665565491)
[2025-02-13 02:46:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:46:59][root][INFO] - Training Epoch: 1/2, step 2256/7134 completed (loss: 0.1357857584953308, acc: 0.9557521939277649)
[2025-02-13 02:46:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:00][root][INFO] - Training Epoch: 1/2, step 2257/7134 completed (loss: 0.05235990136861801, acc: 0.9833837151527405)
[2025-02-13 02:47:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:00][root][INFO] - Training Epoch: 1/2, step 2258/7134 completed (loss: 0.05585143342614174, acc: 0.9884615540504456)
[2025-02-13 02:47:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:00][root][INFO] - Training Epoch: 1/2, step 2259/7134 completed (loss: 0.042846813797950745, acc: 0.9844444394111633)
[2025-02-13 02:47:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:01][root][INFO] - Training Epoch: 1/2, step 2260/7134 completed (loss: 0.07169868797063828, acc: 0.978787899017334)
[2025-02-13 02:47:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:01][root][INFO] - Training Epoch: 1/2, step 2261/7134 completed (loss: 0.10698316991329193, acc: 0.9635036587715149)
[2025-02-13 02:47:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:02][root][INFO] - Training Epoch: 1/2, step 2262/7134 completed (loss: 0.0309138223528862, acc: 0.9871086478233337)
[2025-02-13 02:47:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:02][root][INFO] - Training Epoch: 1/2, step 2263/7134 completed (loss: 0.062262099236249924, acc: 0.9819819927215576)
[2025-02-13 02:47:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:03][root][INFO] - Training Epoch: 1/2, step 2264/7134 completed (loss: 0.09134138375520706, acc: 0.9718706011772156)
[2025-02-13 02:47:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:03][root][INFO] - Training Epoch: 1/2, step 2265/7134 completed (loss: 0.10186424106359482, acc: 0.9707903861999512)
[2025-02-13 02:47:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:03][root][INFO] - Training Epoch: 1/2, step 2266/7134 completed (loss: 0.052134282886981964, acc: 0.9839141964912415)
[2025-02-13 02:47:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:04][root][INFO] - Training Epoch: 1/2, step 2267/7134 completed (loss: 0.022924678400158882, acc: 0.9958847761154175)
[2025-02-13 02:47:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:04][root][INFO] - Training Epoch: 1/2, step 2268/7134 completed (loss: 0.04563520848751068, acc: 0.9899569749832153)
[2025-02-13 02:47:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:05][root][INFO] - Training Epoch: 1/2, step 2269/7134 completed (loss: 0.024012520909309387, acc: 0.9911949634552002)
[2025-02-13 02:47:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:05][root][INFO] - Training Epoch: 1/2, step 2270/7134 completed (loss: 0.07625731825828552, acc: 0.9751243591308594)
[2025-02-13 02:47:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:06][root][INFO] - Training Epoch: 1/2, step 2271/7134 completed (loss: 0.04941019043326378, acc: 0.9873417615890503)
[2025-02-13 02:47:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:06][root][INFO] - Training Epoch: 1/2, step 2272/7134 completed (loss: 0.03853672742843628, acc: 0.9938875436782837)
[2025-02-13 02:47:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:07][root][INFO] - Training Epoch: 1/2, step 2273/7134 completed (loss: 0.025088032707571983, acc: 0.9940119981765747)
[2025-02-13 02:47:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:07][root][INFO] - Training Epoch: 1/2, step 2274/7134 completed (loss: 0.116716668009758, acc: 0.9719745516777039)
[2025-02-13 02:47:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:08][root][INFO] - Training Epoch: 1/2, step 2275/7134 completed (loss: 0.04008728265762329, acc: 0.98591548204422)
[2025-02-13 02:47:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:08][root][INFO] - Training Epoch: 1/2, step 2276/7134 completed (loss: 0.04591410607099533, acc: 0.9881516695022583)
[2025-02-13 02:47:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:08][root][INFO] - Training Epoch: 1/2, step 2277/7134 completed (loss: 0.05202566087245941, acc: 0.986143171787262)
[2025-02-13 02:47:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:09][root][INFO] - Training Epoch: 1/2, step 2278/7134 completed (loss: 0.12072071433067322, acc: 0.9760383367538452)
[2025-02-13 02:47:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:09][root][INFO] - Training Epoch: 1/2, step 2279/7134 completed (loss: 0.03901532292366028, acc: 0.9881936311721802)
[2025-02-13 02:47:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:10][root][INFO] - Training Epoch: 1/2, step 2280/7134 completed (loss: 0.054598622024059296, acc: 0.9822904467582703)
[2025-02-13 02:47:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:10][root][INFO] - Training Epoch: 1/2, step 2281/7134 completed (loss: 0.11418819427490234, acc: 0.9760000109672546)
[2025-02-13 02:47:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:11][root][INFO] - Training Epoch: 1/2, step 2282/7134 completed (loss: 0.014422394335269928, acc: 0.9975278377532959)
[2025-02-13 02:47:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:11][root][INFO] - Training Epoch: 1/2, step 2283/7134 completed (loss: 0.034504350274801254, acc: 0.9877750873565674)
[2025-02-13 02:47:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:12][root][INFO] - Training Epoch: 1/2, step 2284/7134 completed (loss: 0.04599771648645401, acc: 0.9896602630615234)
[2025-02-13 02:47:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:12][root][INFO] - Training Epoch: 1/2, step 2285/7134 completed (loss: 0.034223027527332306, acc: 0.9877368807792664)
[2025-02-13 02:47:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:13][root][INFO] - Training Epoch: 1/2, step 2286/7134 completed (loss: 0.07985018193721771, acc: 0.9766536951065063)
[2025-02-13 02:47:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:13][root][INFO] - Training Epoch: 1/2, step 2287/7134 completed (loss: 0.03819909319281578, acc: 0.9825737476348877)
[2025-02-13 02:47:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:14][root][INFO] - Training Epoch: 1/2, step 2288/7134 completed (loss: 0.04058625549077988, acc: 0.9917550086975098)
[2025-02-13 02:47:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:14][root][INFO] - Training Epoch: 1/2, step 2289/7134 completed (loss: 0.0410618893802166, acc: 0.9845956563949585)
[2025-02-13 02:47:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:14][root][INFO] - Training Epoch: 1/2, step 2290/7134 completed (loss: 0.03759025037288666, acc: 0.9910827875137329)
[2025-02-13 02:47:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:15][root][INFO] - Training Epoch: 1/2, step 2291/7134 completed (loss: 0.03347349911928177, acc: 0.9865047335624695)
[2025-02-13 02:47:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:15][root][INFO] - Training Epoch: 1/2, step 2292/7134 completed (loss: 0.10790896415710449, acc: 0.9819355010986328)
[2025-02-13 02:47:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:16][root][INFO] - Training Epoch: 1/2, step 2293/7134 completed (loss: 0.058636073023080826, acc: 0.9855234026908875)
[2025-02-13 02:47:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:16][root][INFO] - Training Epoch: 1/2, step 2294/7134 completed (loss: 0.050033267587423325, acc: 0.9853768348693848)
[2025-02-13 02:47:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:17][root][INFO] - Training Epoch: 1/2, step 2295/7134 completed (loss: 0.05392105132341385, acc: 0.983855664730072)
[2025-02-13 02:47:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:17][root][INFO] - Training Epoch: 1/2, step 2296/7134 completed (loss: 0.028821386396884918, acc: 0.9914893507957458)
[2025-02-13 02:47:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:18][root][INFO] - Training Epoch: 1/2, step 2297/7134 completed (loss: 0.041489411145448685, acc: 0.9888211488723755)
[2025-02-13 02:47:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:18][root][INFO] - Training Epoch: 1/2, step 2298/7134 completed (loss: 0.06728053092956543, acc: 0.9810725450515747)
[2025-02-13 02:47:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:19][root][INFO] - Training Epoch: 1/2, step 2299/7134 completed (loss: 0.03893989324569702, acc: 0.9840728044509888)
[2025-02-13 02:47:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:19][root][INFO] - Training Epoch: 1/2, step 2300/7134 completed (loss: 0.0192425400018692, acc: 0.994363009929657)
[2025-02-13 02:47:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:20][root][INFO] - Training Epoch: 1/2, step 2301/7134 completed (loss: 0.03877398744225502, acc: 0.9867549538612366)
[2025-02-13 02:47:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:20][root][INFO] - Training Epoch: 1/2, step 2302/7134 completed (loss: 0.056620121002197266, acc: 0.9791666865348816)
[2025-02-13 02:47:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:21][root][INFO] - Training Epoch: 1/2, step 2303/7134 completed (loss: 0.05263667553663254, acc: 0.9851936101913452)
[2025-02-13 02:47:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:21][root][INFO] - Training Epoch: 1/2, step 2304/7134 completed (loss: 0.06395730376243591, acc: 0.9784172773361206)
[2025-02-13 02:47:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:22][root][INFO] - Training Epoch: 1/2, step 2305/7134 completed (loss: 0.04435306414961815, acc: 0.9882854223251343)
[2025-02-13 02:47:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:22][root][INFO] - Training Epoch: 1/2, step 2306/7134 completed (loss: 0.02909194864332676, acc: 0.9922394752502441)
[2025-02-13 02:47:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:23][root][INFO] - Training Epoch: 1/2, step 2307/7134 completed (loss: 0.045474641025066376, acc: 0.9896694421768188)
[2025-02-13 02:47:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:23][root][INFO] - Training Epoch: 1/2, step 2308/7134 completed (loss: 0.05382966622710228, acc: 0.9826353192329407)
[2025-02-13 02:47:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:24][root][INFO] - Training Epoch: 1/2, step 2309/7134 completed (loss: 0.03300565853714943, acc: 0.9892086386680603)
[2025-02-13 02:47:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:24][root][INFO] - Training Epoch: 1/2, step 2310/7134 completed (loss: 0.029104672372341156, acc: 0.9909604787826538)
[2025-02-13 02:47:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:24][root][INFO] - Training Epoch: 1/2, step 2311/7134 completed (loss: 0.04629582166671753, acc: 0.9931192398071289)
[2025-02-13 02:47:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:25][root][INFO] - Training Epoch: 1/2, step 2312/7134 completed (loss: 0.025406453758478165, acc: 0.9967602491378784)
[2025-02-13 02:47:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:25][root][INFO] - Training Epoch: 1/2, step 2313/7134 completed (loss: 0.056701887398958206, acc: 0.9817351698875427)
[2025-02-13 02:47:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:26][root][INFO] - Training Epoch: 1/2, step 2314/7134 completed (loss: 0.03721127659082413, acc: 0.9900110960006714)
[2025-02-13 02:47:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:26][root][INFO] - Training Epoch: 1/2, step 2315/7134 completed (loss: 0.030443554744124413, acc: 0.9929412007331848)
[2025-02-13 02:47:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:27][root][INFO] - Training Epoch: 1/2, step 2316/7134 completed (loss: 0.026529500260949135, acc: 0.9907940030097961)
[2025-02-13 02:47:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:27][root][INFO] - Training Epoch: 1/2, step 2317/7134 completed (loss: 0.030205996707081795, acc: 0.9925925731658936)
[2025-02-13 02:47:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:28][root][INFO] - Training Epoch: 1/2, step 2318/7134 completed (loss: 0.028605887666344643, acc: 0.9905857443809509)
[2025-02-13 02:47:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:28][root][INFO] - Training Epoch: 1/2, step 2319/7134 completed (loss: 0.022764841094613075, acc: 0.9934354424476624)
[2025-02-13 02:47:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:29][root][INFO] - Training Epoch: 1/2, step 2320/7134 completed (loss: 0.023991642519831657, acc: 0.9920454621315002)
[2025-02-13 02:47:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:29][root][INFO] - Training Epoch: 1/2, step 2321/7134 completed (loss: 0.02837897650897503, acc: 0.9930555820465088)
[2025-02-13 02:47:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:30][root][INFO] - Training Epoch: 1/2, step 2322/7134 completed (loss: 0.035016510635614395, acc: 0.9881481528282166)
[2025-02-13 02:47:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:30][root][INFO] - Training Epoch: 1/2, step 2323/7134 completed (loss: 0.05627293512225151, acc: 0.9779179692268372)
[2025-02-13 02:47:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:30][root][INFO] - Training Epoch: 1/2, step 2324/7134 completed (loss: 0.08729155361652374, acc: 0.9829059839248657)
[2025-02-13 02:47:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:31][root][INFO] - Training Epoch: 1/2, step 2325/7134 completed (loss: 0.05321507528424263, acc: 0.9794871807098389)
[2025-02-13 02:47:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:31][root][INFO] - Training Epoch: 1/2, step 2326/7134 completed (loss: 0.016575530171394348, acc: 0.9944853186607361)
[2025-02-13 02:47:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:32][root][INFO] - Training Epoch: 1/2, step 2327/7134 completed (loss: 0.017587458714842796, acc: 0.9901639223098755)
[2025-02-13 02:47:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:32][root][INFO] - Training Epoch: 1/2, step 2328/7134 completed (loss: 0.08328403532505035, acc: 0.9767025113105774)
[2025-02-13 02:47:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:33][root][INFO] - Training Epoch: 1/2, step 2329/7134 completed (loss: 0.049489863216876984, acc: 0.9922480583190918)
[2025-02-13 02:47:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:33][root][INFO] - Training Epoch: 1/2, step 2330/7134 completed (loss: 0.02467091754078865, acc: 0.9938931465148926)
[2025-02-13 02:47:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:33][root][INFO] - Training Epoch: 1/2, step 2331/7134 completed (loss: 0.019835807383060455, acc: 0.9938650131225586)
[2025-02-13 02:47:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:34][root][INFO] - Training Epoch: 1/2, step 2332/7134 completed (loss: 0.048688896000385284, acc: 0.9925705790519714)
[2025-02-13 02:47:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:34][root][INFO] - Training Epoch: 1/2, step 2333/7134 completed (loss: 0.01608661748468876, acc: 0.9940476417541504)
[2025-02-13 02:47:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:35][root][INFO] - Training Epoch: 1/2, step 2334/7134 completed (loss: 0.031172748655080795, acc: 0.9880596995353699)
[2025-02-13 02:47:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:35][root][INFO] - Training Epoch: 1/2, step 2335/7134 completed (loss: 0.05500861629843712, acc: 0.9854838848114014)
[2025-02-13 02:47:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:36][root][INFO] - Training Epoch: 1/2, step 2336/7134 completed (loss: 0.012862122617661953, acc: 0.9969087839126587)
[2025-02-13 02:47:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:36][root][INFO] - Training Epoch: 1/2, step 2337/7134 completed (loss: 0.009582060389220715, acc: 0.9983525276184082)
[2025-02-13 02:47:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:37][root][INFO] - Training Epoch: 1/2, step 2338/7134 completed (loss: 0.030997997149825096, acc: 0.9938176274299622)
[2025-02-13 02:47:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:37][root][INFO] - Training Epoch: 1/2, step 2339/7134 completed (loss: 0.047682005912065506, acc: 0.9869494438171387)
[2025-02-13 02:47:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:37][root][INFO] - Training Epoch: 1/2, step 2340/7134 completed (loss: 0.016450637951493263, acc: 0.993966817855835)
[2025-02-13 02:47:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:38][root][INFO] - Training Epoch: 1/2, step 2341/7134 completed (loss: 0.015825536102056503, acc: 0.9943609237670898)
[2025-02-13 02:47:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:38][root][INFO] - Training Epoch: 1/2, step 2342/7134 completed (loss: 0.01965474709868431, acc: 0.9911373853683472)
[2025-02-13 02:47:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:39][root][INFO] - Training Epoch: 1/2, step 2343/7134 completed (loss: 0.01782124489545822, acc: 0.995199978351593)
[2025-02-13 02:47:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:39][root][INFO] - Training Epoch: 1/2, step 2344/7134 completed (loss: 0.02855820581316948, acc: 0.9906103014945984)
[2025-02-13 02:47:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:40][root][INFO] - Training Epoch: 1/2, step 2345/7134 completed (loss: 0.016176991164684296, acc: 0.9916083812713623)
[2025-02-13 02:47:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:40][root][INFO] - Training Epoch: 1/2, step 2346/7134 completed (loss: 0.022440338507294655, acc: 0.9929178357124329)
[2025-02-13 02:47:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:40][root][INFO] - Training Epoch: 1/2, step 2347/7134 completed (loss: 0.03898100182414055, acc: 0.9934210777282715)
[2025-02-13 02:47:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:41][root][INFO] - Training Epoch: 1/2, step 2348/7134 completed (loss: 0.07743779569864273, acc: 0.9785624146461487)
[2025-02-13 02:47:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:41][root][INFO] - Training Epoch: 1/2, step 2349/7134 completed (loss: 0.058785486966371536, acc: 0.9819444417953491)
[2025-02-13 02:47:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:42][root][INFO] - Training Epoch: 1/2, step 2350/7134 completed (loss: 0.10234744101762772, acc: 0.9725610017776489)
[2025-02-13 02:47:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:42][root][INFO] - Training Epoch: 1/2, step 2351/7134 completed (loss: 0.06579363346099854, acc: 0.9822404384613037)
[2025-02-13 02:47:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:43][root][INFO] - Training Epoch: 1/2, step 2352/7134 completed (loss: 0.0461106151342392, acc: 0.9871794581413269)
[2025-02-13 02:47:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:43][root][INFO] - Training Epoch: 1/2, step 2353/7134 completed (loss: 0.059435755014419556, acc: 0.9886524677276611)
[2025-02-13 02:47:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:44][root][INFO] - Training Epoch: 1/2, step 2354/7134 completed (loss: 0.035742178559303284, acc: 0.9848101139068604)
[2025-02-13 02:47:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:44][root][INFO] - Training Epoch: 1/2, step 2355/7134 completed (loss: 0.05409471318125725, acc: 0.9876543283462524)
[2025-02-13 02:47:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:44][root][INFO] - Training Epoch: 1/2, step 2356/7134 completed (loss: 0.05822088196873665, acc: 0.9805653691291809)
[2025-02-13 02:47:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:45][root][INFO] - Training Epoch: 1/2, step 2357/7134 completed (loss: 0.05462155118584633, acc: 0.9861111044883728)
[2025-02-13 02:47:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:45][root][INFO] - Training Epoch: 1/2, step 2358/7134 completed (loss: 0.053242914378643036, acc: 0.9807229042053223)
[2025-02-13 02:47:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:46][root][INFO] - Training Epoch: 1/2, step 2359/7134 completed (loss: 0.059067681431770325, acc: 0.9838472604751587)
[2025-02-13 02:47:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:46][root][INFO] - Training Epoch: 1/2, step 2360/7134 completed (loss: 0.04547029733657837, acc: 0.9868228435516357)
[2025-02-13 02:47:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:47][root][INFO] - Training Epoch: 1/2, step 2361/7134 completed (loss: 0.05228639766573906, acc: 0.9837278127670288)
[2025-02-13 02:47:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:47][root][INFO] - Training Epoch: 1/2, step 2362/7134 completed (loss: 0.04123793914914131, acc: 0.9887892603874207)
[2025-02-13 02:47:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:48][root][INFO] - Training Epoch: 1/2, step 2363/7134 completed (loss: 0.041862957179546356, acc: 0.9872537851333618)
[2025-02-13 02:47:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:48][root][INFO] - Training Epoch: 1/2, step 2364/7134 completed (loss: 0.04024074971675873, acc: 0.9893454909324646)
[2025-02-13 02:47:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:48][root][INFO] - Training Epoch: 1/2, step 2365/7134 completed (loss: 0.0208671186119318, acc: 0.994490385055542)
[2025-02-13 02:47:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:49][root][INFO] - Training Epoch: 1/2, step 2366/7134 completed (loss: 0.024241948500275612, acc: 0.9907407164573669)
[2025-02-13 02:47:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:49][root][INFO] - Training Epoch: 1/2, step 2367/7134 completed (loss: 0.08148148655891418, acc: 0.9849246144294739)
[2025-02-13 02:47:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:50][root][INFO] - Training Epoch: 1/2, step 2368/7134 completed (loss: 0.02406536415219307, acc: 0.9939393997192383)
[2025-02-13 02:47:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:50][root][INFO] - Training Epoch: 1/2, step 2369/7134 completed (loss: 0.020693786442279816, acc: 0.9905882477760315)
[2025-02-13 02:47:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:51][root][INFO] - Training Epoch: 1/2, step 2370/7134 completed (loss: 0.043634843081235886, acc: 0.9907407164573669)
[2025-02-13 02:47:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:51][root][INFO] - Training Epoch: 1/2, step 2371/7134 completed (loss: 0.02843559719622135, acc: 0.9927954077720642)
[2025-02-13 02:47:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:52][root][INFO] - Training Epoch: 1/2, step 2372/7134 completed (loss: 0.031198110431432724, acc: 0.9920634627342224)
[2025-02-13 02:47:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:52][root][INFO] - Training Epoch: 1/2, step 2373/7134 completed (loss: 0.02262159436941147, acc: 0.9937965273857117)
[2025-02-13 02:47:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:53][root][INFO] - Training Epoch: 1/2, step 2374/7134 completed (loss: 0.02206673100590706, acc: 0.9946452379226685)
[2025-02-13 02:47:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:53][root][INFO] - Training Epoch: 1/2, step 2375/7134 completed (loss: 0.06379631161689758, acc: 0.989393949508667)
[2025-02-13 02:47:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:53][root][INFO] - Training Epoch: 1/2, step 2376/7134 completed (loss: 0.1369021087884903, acc: 0.9727427363395691)
[2025-02-13 02:47:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:54][root][INFO] - Training Epoch: 1/2, step 2377/7134 completed (loss: 0.07518317550420761, acc: 0.979689359664917)
[2025-02-13 02:47:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:54][root][INFO] - Training Epoch: 1/2, step 2378/7134 completed (loss: 0.059329625219106674, acc: 0.98591548204422)
[2025-02-13 02:47:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:55][root][INFO] - Training Epoch: 1/2, step 2379/7134 completed (loss: 0.09469249099493027, acc: 0.9806896448135376)
[2025-02-13 02:47:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:55][root][INFO] - Training Epoch: 1/2, step 2380/7134 completed (loss: 0.06077100709080696, acc: 0.9794437885284424)
[2025-02-13 02:47:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:56][root][INFO] - Training Epoch: 1/2, step 2381/7134 completed (loss: 0.15485969185829163, acc: 0.952622652053833)
[2025-02-13 02:47:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:56][root][INFO] - Training Epoch: 1/2, step 2382/7134 completed (loss: 0.05867255851626396, acc: 0.9831804037094116)
[2025-02-13 02:47:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:56][root][INFO] - Training Epoch: 1/2, step 2383/7134 completed (loss: 0.09441131353378296, acc: 0.9773755669593811)
[2025-02-13 02:47:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:57][root][INFO] - Training Epoch: 1/2, step 2384/7134 completed (loss: 0.08048325777053833, acc: 0.9803921580314636)
[2025-02-13 02:47:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:57][root][INFO] - Training Epoch: 1/2, step 2385/7134 completed (loss: 0.055889297276735306, acc: 0.9834254384040833)
[2025-02-13 02:47:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:58][root][INFO] - Training Epoch: 1/2, step 2386/7134 completed (loss: 0.05907674878835678, acc: 0.9869109988212585)
[2025-02-13 02:47:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:58][root][INFO] - Training Epoch: 1/2, step 2387/7134 completed (loss: 0.05197509005665779, acc: 0.9895591735839844)
[2025-02-13 02:47:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:59][root][INFO] - Training Epoch: 1/2, step 2388/7134 completed (loss: 0.10209255665540695, acc: 0.9737783074378967)
[2025-02-13 02:47:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:47:59][root][INFO] - Training Epoch: 1/2, step 2389/7134 completed (loss: 0.06678921729326248, acc: 0.9770379066467285)
[2025-02-13 02:47:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:00][root][INFO] - Training Epoch: 1/2, step 2390/7134 completed (loss: 0.07390352338552475, acc: 0.9772998690605164)
[2025-02-13 02:48:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:00][root][INFO] - Training Epoch: 1/2, step 2391/7134 completed (loss: 0.08623189479112625, acc: 0.9783989787101746)
[2025-02-13 02:48:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:01][root][INFO] - Training Epoch: 1/2, step 2392/7134 completed (loss: 0.05751664191484451, acc: 0.9893190860748291)
[2025-02-13 02:48:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:01][root][INFO] - Training Epoch: 1/2, step 2393/7134 completed (loss: 0.10282202064990997, acc: 0.9779506921768188)
[2025-02-13 02:48:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:02][root][INFO] - Training Epoch: 1/2, step 2394/7134 completed (loss: 0.057646553963422775, acc: 0.9842424392700195)
[2025-02-13 02:48:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:02][root][INFO] - Training Epoch: 1/2, step 2395/7134 completed (loss: 0.18909142911434174, acc: 0.9576547145843506)
[2025-02-13 02:48:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:02][root][INFO] - Training Epoch: 1/2, step 2396/7134 completed (loss: 0.08586356043815613, acc: 0.9724518060684204)
[2025-02-13 02:48:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:03][root][INFO] - Training Epoch: 1/2, step 2397/7134 completed (loss: 0.0979028195142746, acc: 0.9737991094589233)
[2025-02-13 02:48:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:03][root][INFO] - Training Epoch: 1/2, step 2398/7134 completed (loss: 0.04204995185136795, acc: 0.9876203536987305)
[2025-02-13 02:48:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:04][root][INFO] - Training Epoch: 1/2, step 2399/7134 completed (loss: 0.07784456014633179, acc: 0.9799714088439941)
[2025-02-13 02:48:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:04][root][INFO] - Training Epoch: 1/2, step 2400/7134 completed (loss: 0.03887014463543892, acc: 0.9889655113220215)
[2025-02-13 02:48:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:05][root][INFO] - Training Epoch: 1/2, step 2401/7134 completed (loss: 0.12531213462352753, acc: 0.9620253443717957)
[2025-02-13 02:48:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:05][root][INFO] - Training Epoch: 1/2, step 2402/7134 completed (loss: 0.08943336457014084, acc: 0.970783531665802)
[2025-02-13 02:48:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:05][root][INFO] - Training Epoch: 1/2, step 2403/7134 completed (loss: 0.059381578117609024, acc: 0.978723406791687)
[2025-02-13 02:48:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:06][root][INFO] - Training Epoch: 1/2, step 2404/7134 completed (loss: 0.035334452986717224, acc: 0.9877049326896667)
[2025-02-13 02:48:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:06][root][INFO] - Training Epoch: 1/2, step 2405/7134 completed (loss: 0.06009458377957344, acc: 0.97826087474823)
[2025-02-13 02:48:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:07][root][INFO] - Training Epoch: 1/2, step 2406/7134 completed (loss: 0.03536483645439148, acc: 0.9827127456665039)
[2025-02-13 02:48:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:07][root][INFO] - Training Epoch: 1/2, step 2407/7134 completed (loss: 0.06004688888788223, acc: 0.9802431464195251)
[2025-02-13 02:48:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:08][root][INFO] - Training Epoch: 1/2, step 2408/7134 completed (loss: 0.1150849312543869, acc: 0.9710526466369629)
[2025-02-13 02:48:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:08][root][INFO] - Training Epoch: 1/2, step 2409/7134 completed (loss: 0.04012599587440491, acc: 0.9863387942314148)
[2025-02-13 02:48:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:08][root][INFO] - Training Epoch: 1/2, step 2410/7134 completed (loss: 0.03418697789311409, acc: 0.9885222315788269)
[2025-02-13 02:48:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:09][root][INFO] - Training Epoch: 1/2, step 2411/7134 completed (loss: 0.040435515344142914, acc: 0.9909443855285645)
[2025-02-13 02:48:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:09][root][INFO] - Training Epoch: 1/2, step 2412/7134 completed (loss: 0.04781372845172882, acc: 0.9846153855323792)
[2025-02-13 02:48:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:10][root][INFO] - Training Epoch: 1/2, step 2413/7134 completed (loss: 0.03411848098039627, acc: 0.9834815859794617)
[2025-02-13 02:48:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:10][root][INFO] - Training Epoch: 1/2, step 2414/7134 completed (loss: 0.04404284060001373, acc: 0.9875621795654297)
[2025-02-13 02:48:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:11][root][INFO] - Training Epoch: 1/2, step 2415/7134 completed (loss: 0.06281539797782898, acc: 0.9835391044616699)
[2025-02-13 02:48:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:11][root][INFO] - Training Epoch: 1/2, step 2416/7134 completed (loss: 0.0442056804895401, acc: 0.9847856163978577)
[2025-02-13 02:48:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:12][root][INFO] - Training Epoch: 1/2, step 2417/7134 completed (loss: 0.07668724656105042, acc: 0.9810479283332825)
[2025-02-13 02:48:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:12][root][INFO] - Training Epoch: 1/2, step 2418/7134 completed (loss: 0.055685438215732574, acc: 0.9831223487854004)
[2025-02-13 02:48:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:12][root][INFO] - Training Epoch: 1/2, step 2419/7134 completed (loss: 0.035973601043224335, acc: 0.9871244430541992)
[2025-02-13 02:48:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:13][root][INFO] - Training Epoch: 1/2, step 2420/7134 completed (loss: 0.030761554837226868, acc: 0.9883913993835449)
[2025-02-13 02:48:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:13][root][INFO] - Training Epoch: 1/2, step 2421/7134 completed (loss: 0.058990370482206345, acc: 0.9776119589805603)
[2025-02-13 02:48:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:14][root][INFO] - Training Epoch: 1/2, step 2422/7134 completed (loss: 0.026519861072301865, acc: 0.9940898418426514)
[2025-02-13 02:48:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:14][root][INFO] - Training Epoch: 1/2, step 2423/7134 completed (loss: 0.039385490119457245, acc: 0.9874476790428162)
[2025-02-13 02:48:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:15][root][INFO] - Training Epoch: 1/2, step 2424/7134 completed (loss: 0.03206424042582512, acc: 0.9901685118675232)
[2025-02-13 02:48:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:15][root][INFO] - Training Epoch: 1/2, step 2425/7134 completed (loss: 0.041974734514951706, acc: 0.9866666793823242)
[2025-02-13 02:48:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:16][root][INFO] - Training Epoch: 1/2, step 2426/7134 completed (loss: 0.022969534620642662, acc: 0.9928656220436096)
[2025-02-13 02:48:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:16][root][INFO] - Training Epoch: 1/2, step 2427/7134 completed (loss: 0.013970697298645973, acc: 0.9953595995903015)
[2025-02-13 02:48:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:16][root][INFO] - Training Epoch: 1/2, step 2428/7134 completed (loss: 0.03152884170413017, acc: 0.993630588054657)
[2025-02-13 02:48:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:17][root][INFO] - Training Epoch: 1/2, step 2429/7134 completed (loss: 0.033905111253261566, acc: 0.9937694668769836)
[2025-02-13 02:48:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:17][root][INFO] - Training Epoch: 1/2, step 2430/7134 completed (loss: 0.052212104201316833, acc: 0.9880775213241577)
[2025-02-13 02:48:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:18][root][INFO] - Training Epoch: 1/2, step 2431/7134 completed (loss: 0.07161947339773178, acc: 0.9809384346008301)
[2025-02-13 02:48:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:18][root][INFO] - Training Epoch: 1/2, step 2432/7134 completed (loss: 0.16030091047286987, acc: 0.9626485705375671)
[2025-02-13 02:48:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:19][root][INFO] - Training Epoch: 1/2, step 2433/7134 completed (loss: 0.09138071537017822, acc: 0.9815340638160706)
[2025-02-13 02:48:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:19][root][INFO] - Training Epoch: 1/2, step 2434/7134 completed (loss: 0.10316582024097443, acc: 0.9720497131347656)
[2025-02-13 02:48:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:20][root][INFO] - Training Epoch: 1/2, step 2435/7134 completed (loss: 0.0494241863489151, acc: 0.9841897487640381)
[2025-02-13 02:48:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:20][root][INFO] - Training Epoch: 1/2, step 2436/7134 completed (loss: 0.04511426016688347, acc: 0.9837020039558411)
[2025-02-13 02:48:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:20][root][INFO] - Training Epoch: 1/2, step 2437/7134 completed (loss: 0.04257285222411156, acc: 0.9819004535675049)
[2025-02-13 02:48:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:21][root][INFO] - Training Epoch: 1/2, step 2438/7134 completed (loss: 0.05669832602143288, acc: 0.989234447479248)
[2025-02-13 02:48:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:21][root][INFO] - Training Epoch: 1/2, step 2439/7134 completed (loss: 0.06271737068891525, acc: 0.979547917842865)
[2025-02-13 02:48:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:22][root][INFO] - Training Epoch: 1/2, step 2440/7134 completed (loss: 0.060031693428754807, acc: 0.9865471124649048)
[2025-02-13 02:48:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:22][root][INFO] - Training Epoch: 1/2, step 2441/7134 completed (loss: 0.13610635697841644, acc: 0.9673704504966736)
[2025-02-13 02:48:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:23][root][INFO] - Training Epoch: 1/2, step 2442/7134 completed (loss: 0.07519007474184036, acc: 0.977622389793396)
[2025-02-13 02:48:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:23][root][INFO] - Training Epoch: 1/2, step 2443/7134 completed (loss: 0.04757528007030487, acc: 0.9888734221458435)
[2025-02-13 02:48:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:24][root][INFO] - Training Epoch: 1/2, step 2444/7134 completed (loss: 0.06214190274477005, acc: 0.9753788113594055)
[2025-02-13 02:48:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:24][root][INFO] - Training Epoch: 1/2, step 2445/7134 completed (loss: 0.06516829133033752, acc: 0.9758883118629456)
[2025-02-13 02:48:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:25][root][INFO] - Training Epoch: 1/2, step 2446/7134 completed (loss: 0.13708174228668213, acc: 0.9597222208976746)
[2025-02-13 02:48:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:25][root][INFO] - Training Epoch: 1/2, step 2447/7134 completed (loss: 0.05613480880856514, acc: 0.9811320900917053)
[2025-02-13 02:48:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:25][root][INFO] - Training Epoch: 1/2, step 2448/7134 completed (loss: 0.04405168443918228, acc: 0.9839572310447693)
[2025-02-13 02:48:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:26][root][INFO] - Training Epoch: 1/2, step 2449/7134 completed (loss: 0.07747255265712738, acc: 0.9761273264884949)
[2025-02-13 02:48:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:26][root][INFO] - Training Epoch: 1/2, step 2450/7134 completed (loss: 0.06297256052494049, acc: 0.9834815859794617)
[2025-02-13 02:48:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:27][root][INFO] - Training Epoch: 1/2, step 2451/7134 completed (loss: 0.024260252714157104, acc: 0.9949044585227966)
[2025-02-13 02:48:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:27][root][INFO] - Training Epoch: 1/2, step 2452/7134 completed (loss: 0.04735757037997246, acc: 0.981794536113739)
[2025-02-13 02:48:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:28][root][INFO] - Training Epoch: 1/2, step 2453/7134 completed (loss: 0.048769086599349976, acc: 0.9858064651489258)
[2025-02-13 02:48:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:28][root][INFO] - Training Epoch: 1/2, step 2454/7134 completed (loss: 0.03801073133945465, acc: 0.9849498271942139)
[2025-02-13 02:48:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:29][root][INFO] - Training Epoch: 1/2, step 2455/7134 completed (loss: 0.04875228926539421, acc: 0.9858356714248657)
[2025-02-13 02:48:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:29][root][INFO] - Training Epoch: 1/2, step 2456/7134 completed (loss: 0.060587991029024124, acc: 0.9818417429924011)
[2025-02-13 02:48:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:30][root][INFO] - Training Epoch: 1/2, step 2457/7134 completed (loss: 0.0744468942284584, acc: 0.9767054915428162)
[2025-02-13 02:48:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:30][root][INFO] - Training Epoch: 1/2, step 2458/7134 completed (loss: 0.08394446223974228, acc: 0.9858490824699402)
[2025-02-13 02:48:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:31][root][INFO] - Training Epoch: 1/2, step 2459/7134 completed (loss: 0.024527575820684433, acc: 0.9926035404205322)
[2025-02-13 02:48:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:31][root][INFO] - Training Epoch: 1/2, step 2460/7134 completed (loss: 0.04242238774895668, acc: 0.9903181195259094)
[2025-02-13 02:48:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:31][root][INFO] - Training Epoch: 1/2, step 2461/7134 completed (loss: 0.03224579244852066, acc: 0.9884058237075806)
[2025-02-13 02:48:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:32][root][INFO] - Training Epoch: 1/2, step 2462/7134 completed (loss: 0.05662430822849274, acc: 0.9848275780677795)
[2025-02-13 02:48:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:32][root][INFO] - Training Epoch: 1/2, step 2463/7134 completed (loss: 0.02673940546810627, acc: 0.9909228682518005)
[2025-02-13 02:48:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:33][root][INFO] - Training Epoch: 1/2, step 2464/7134 completed (loss: 0.06457610428333282, acc: 0.9812734127044678)
[2025-02-13 02:48:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:33][root][INFO] - Training Epoch: 1/2, step 2465/7134 completed (loss: 0.04776736721396446, acc: 0.9827814698219299)
[2025-02-13 02:48:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:34][root][INFO] - Training Epoch: 1/2, step 2466/7134 completed (loss: 0.026962026953697205, acc: 0.9880794882774353)
[2025-02-13 02:48:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:34][root][INFO] - Training Epoch: 1/2, step 2467/7134 completed (loss: 0.03978424146771431, acc: 0.9858611822128296)
[2025-02-13 02:48:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:34][root][INFO] - Training Epoch: 1/2, step 2468/7134 completed (loss: 0.024848727509379387, acc: 0.9902098178863525)
[2025-02-13 02:48:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:35][root][INFO] - Training Epoch: 1/2, step 2469/7134 completed (loss: 0.02791745588183403, acc: 0.9918144345283508)
[2025-02-13 02:48:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:35][root][INFO] - Training Epoch: 1/2, step 2470/7134 completed (loss: 0.03849870711565018, acc: 0.9898989796638489)
[2025-02-13 02:48:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:36][root][INFO] - Training Epoch: 1/2, step 2471/7134 completed (loss: 0.02144361101090908, acc: 0.9916387796401978)
[2025-02-13 02:48:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:36][root][INFO] - Training Epoch: 1/2, step 2472/7134 completed (loss: 0.020470714196562767, acc: 0.9969696998596191)
[2025-02-13 02:48:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:37][root][INFO] - Training Epoch: 1/2, step 2473/7134 completed (loss: 0.04030371084809303, acc: 0.9897540807723999)
[2025-02-13 02:48:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:37][root][INFO] - Training Epoch: 1/2, step 2474/7134 completed (loss: 0.024501562118530273, acc: 0.9943661689758301)
[2025-02-13 02:48:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:37][root][INFO] - Training Epoch: 1/2, step 2475/7134 completed (loss: 0.021733831614255905, acc: 0.9914236664772034)
[2025-02-13 02:48:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:38][root][INFO] - Training Epoch: 1/2, step 2476/7134 completed (loss: 0.05798447132110596, acc: 0.9888579249382019)
[2025-02-13 02:48:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:38][root][INFO] - Training Epoch: 1/2, step 2477/7134 completed (loss: 0.043984804302453995, acc: 0.9875690340995789)
[2025-02-13 02:48:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:39][root][INFO] - Training Epoch: 1/2, step 2478/7134 completed (loss: 0.022158563137054443, acc: 0.9906542301177979)
[2025-02-13 02:48:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:39][root][INFO] - Training Epoch: 1/2, step 2479/7134 completed (loss: 0.07555101811885834, acc: 0.9809221029281616)
[2025-02-13 02:48:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:40][root][INFO] - Training Epoch: 1/2, step 2480/7134 completed (loss: 0.005705705378204584, acc: 1.0)
[2025-02-13 02:48:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:40][root][INFO] - Training Epoch: 1/2, step 2481/7134 completed (loss: 0.031991809606552124, acc: 0.9875621795654297)
[2025-02-13 02:48:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:40][root][INFO] - Training Epoch: 1/2, step 2482/7134 completed (loss: 0.043740563094615936, acc: 0.9807383418083191)
[2025-02-13 02:48:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:41][root][INFO] - Training Epoch: 1/2, step 2483/7134 completed (loss: 0.024741634726524353, acc: 0.9889042973518372)
[2025-02-13 02:48:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:41][root][INFO] - Training Epoch: 1/2, step 2484/7134 completed (loss: 0.03416771814227104, acc: 0.9902777671813965)
[2025-02-13 02:48:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:42][root][INFO] - Training Epoch: 1/2, step 2485/7134 completed (loss: 0.021007616072893143, acc: 0.9968253970146179)
[2025-02-13 02:48:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:42][root][INFO] - Training Epoch: 1/2, step 2486/7134 completed (loss: 0.026982227340340614, acc: 0.9956896305084229)
[2025-02-13 02:48:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:43][root][INFO] - Training Epoch: 1/2, step 2487/7134 completed (loss: 0.07333101332187653, acc: 0.9795275330543518)
[2025-02-13 02:48:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:43][root][INFO] - Training Epoch: 1/2, step 2488/7134 completed (loss: 0.03155653178691864, acc: 0.9917126893997192)
[2025-02-13 02:48:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:44][root][INFO] - Training Epoch: 1/2, step 2489/7134 completed (loss: 0.023942062631249428, acc: 0.9958217144012451)
[2025-02-13 02:48:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:44][root][INFO] - Training Epoch: 1/2, step 2490/7134 completed (loss: 0.02084692381322384, acc: 0.9942938685417175)
[2025-02-13 02:48:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:44][root][INFO] - Training Epoch: 1/2, step 2491/7134 completed (loss: 0.12695148587226868, acc: 0.976190447807312)
[2025-02-13 02:48:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:45][root][INFO] - Training Epoch: 1/2, step 2492/7134 completed (loss: 0.04683161526918411, acc: 0.9863523840904236)
[2025-02-13 02:48:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:45][root][INFO] - Training Epoch: 1/2, step 2493/7134 completed (loss: 0.05938013643026352, acc: 0.9753521084785461)
[2025-02-13 02:48:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:46][root][INFO] - Training Epoch: 1/2, step 2494/7134 completed (loss: 0.047035589814186096, acc: 0.9899874925613403)
[2025-02-13 02:48:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:46][root][INFO] - Training Epoch: 1/2, step 2495/7134 completed (loss: 0.06223752722144127, acc: 0.975576639175415)
[2025-02-13 02:48:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:47][root][INFO] - Training Epoch: 1/2, step 2496/7134 completed (loss: 0.03439522534608841, acc: 0.9817415475845337)
[2025-02-13 02:48:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:47][root][INFO] - Training Epoch: 1/2, step 2497/7134 completed (loss: 0.04502299055457115, acc: 0.9914841651916504)
[2025-02-13 02:48:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:48][root][INFO] - Training Epoch: 1/2, step 2498/7134 completed (loss: 0.047458574175834656, acc: 0.9863481521606445)
[2025-02-13 02:48:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:48][root][INFO] - Training Epoch: 1/2, step 2499/7134 completed (loss: 0.03694981709122658, acc: 0.987922728061676)
[2025-02-13 02:48:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:49][root][INFO] - Training Epoch: 1/2, step 2500/7134 completed (loss: 0.02582395076751709, acc: 0.9948520064353943)
[2025-02-13 02:48:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:49][root][INFO] - Training Epoch: 1/2, step 2501/7134 completed (loss: 0.044380780309438705, acc: 0.9851258397102356)
[2025-02-13 02:48:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:49][root][INFO] - Training Epoch: 1/2, step 2502/7134 completed (loss: 0.01925375498831272, acc: 0.9964664578437805)
[2025-02-13 02:48:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:50][root][INFO] - Training Epoch: 1/2, step 2503/7134 completed (loss: 0.0400359146296978, acc: 0.9900285005569458)
[2025-02-13 02:48:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:50][root][INFO] - Training Epoch: 1/2, step 2504/7134 completed (loss: 0.034549035131931305, acc: 0.9879999756813049)
[2025-02-13 02:48:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:51][root][INFO] - Training Epoch: 1/2, step 2505/7134 completed (loss: 0.033758729696273804, acc: 0.9874857664108276)
[2025-02-13 02:48:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:51][root][INFO] - Training Epoch: 1/2, step 2506/7134 completed (loss: 0.04193805903196335, acc: 0.9839743375778198)
[2025-02-13 02:48:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:52][root][INFO] - Training Epoch: 1/2, step 2507/7134 completed (loss: 0.012613365426659584, acc: 0.996515691280365)
[2025-02-13 02:48:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:52][root][INFO] - Training Epoch: 1/2, step 2508/7134 completed (loss: 0.04965459927916527, acc: 0.9861751198768616)
[2025-02-13 02:48:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:53][root][INFO] - Training Epoch: 1/2, step 2509/7134 completed (loss: 0.024498140439391136, acc: 0.9963280558586121)
[2025-02-13 02:48:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:53][root][INFO] - Training Epoch: 1/2, step 2510/7134 completed (loss: 0.027968252077698708, acc: 0.9893778562545776)
[2025-02-13 02:48:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:53][root][INFO] - Training Epoch: 1/2, step 2511/7134 completed (loss: 0.0531734935939312, acc: 0.9876681566238403)
[2025-02-13 02:48:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:54][root][INFO] - Training Epoch: 1/2, step 2512/7134 completed (loss: 0.08120781183242798, acc: 0.9824561476707458)
[2025-02-13 02:48:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:54][root][INFO] - Training Epoch: 1/2, step 2513/7134 completed (loss: 0.0403994619846344, acc: 0.9905213117599487)
[2025-02-13 02:48:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:55][root][INFO] - Training Epoch: 1/2, step 2514/7134 completed (loss: 0.03070877678692341, acc: 0.9894982576370239)
[2025-02-13 02:48:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:55][root][INFO] - Training Epoch: 1/2, step 2515/7134 completed (loss: 0.016456477344036102, acc: 0.9973992109298706)
[2025-02-13 02:48:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:56][root][INFO] - Training Epoch: 1/2, step 2516/7134 completed (loss: 0.023498043417930603, acc: 0.9954545497894287)
[2025-02-13 02:48:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:56][root][INFO] - Training Epoch: 1/2, step 2517/7134 completed (loss: 0.037654511630535126, acc: 0.9896670579910278)
[2025-02-13 02:48:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:57][root][INFO] - Training Epoch: 1/2, step 2518/7134 completed (loss: 0.03050883486866951, acc: 0.9928469061851501)
[2025-02-13 02:48:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:57][root][INFO] - Training Epoch: 1/2, step 2519/7134 completed (loss: 0.01727566309273243, acc: 0.9944674968719482)
[2025-02-13 02:48:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:58][root][INFO] - Training Epoch: 1/2, step 2520/7134 completed (loss: 0.030420467257499695, acc: 0.9886040091514587)
[2025-02-13 02:48:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:58][root][INFO] - Training Epoch: 1/2, step 2521/7134 completed (loss: 0.08749233186244965, acc: 0.9832689762115479)
[2025-02-13 02:48:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:58][root][INFO] - Training Epoch: 1/2, step 2522/7134 completed (loss: 0.05251722037792206, acc: 0.9871060252189636)
[2025-02-13 02:48:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:59][root][INFO] - Training Epoch: 1/2, step 2523/7134 completed (loss: 0.06047289818525314, acc: 0.9869109988212585)
[2025-02-13 02:48:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:48:59][root][INFO] - Training Epoch: 1/2, step 2524/7134 completed (loss: 0.01183986570686102, acc: 0.9972375631332397)
[2025-02-13 02:49:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:00][root][INFO] - Training Epoch: 1/2, step 2525/7134 completed (loss: 0.04299749806523323, acc: 0.9849749803543091)
[2025-02-13 02:49:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:00][root][INFO] - Training Epoch: 1/2, step 2526/7134 completed (loss: 0.00992064829915762, acc: 0.9976470470428467)
[2025-02-13 02:49:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:01][root][INFO] - Training Epoch: 1/2, step 2527/7134 completed (loss: 0.03531825914978981, acc: 0.9890244007110596)
[2025-02-13 02:49:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:01][root][INFO] - Training Epoch: 1/2, step 2528/7134 completed (loss: 0.01436362974345684, acc: 0.99589604139328)
[2025-02-13 02:49:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:01][root][INFO] - Training Epoch: 1/2, step 2529/7134 completed (loss: 0.014741642400622368, acc: 0.9920212626457214)
[2025-02-13 02:49:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:02][root][INFO] - Training Epoch: 1/2, step 2530/7134 completed (loss: 0.018721476197242737, acc: 0.9961038827896118)
[2025-02-13 02:49:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:02][root][INFO] - Training Epoch: 1/2, step 2531/7134 completed (loss: 0.012593447230756283, acc: 0.9944751262664795)
[2025-02-13 02:49:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:03][root][INFO] - Training Epoch: 1/2, step 2532/7134 completed (loss: 0.011384207755327225, acc: 0.9938367009162903)
[2025-02-13 02:49:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:03][root][INFO] - Training Epoch: 1/2, step 2533/7134 completed (loss: 0.029829571023583412, acc: 0.9924585223197937)
[2025-02-13 02:49:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:04][root][INFO] - Training Epoch: 1/2, step 2534/7134 completed (loss: 0.042180437594652176, acc: 0.9898089170455933)
[2025-02-13 02:49:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:04][root][INFO] - Training Epoch: 1/2, step 2535/7134 completed (loss: 0.047832027077674866, acc: 0.9865125417709351)
[2025-02-13 02:49:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:05][root][INFO] - Training Epoch: 1/2, step 2536/7134 completed (loss: 0.03760279342532158, acc: 0.9876998662948608)
[2025-02-13 02:49:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:05][root][INFO] - Training Epoch: 1/2, step 2537/7134 completed (loss: 0.08363064378499985, acc: 0.9770641922950745)
[2025-02-13 02:49:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:05][root][INFO] - Training Epoch: 1/2, step 2538/7134 completed (loss: 0.0788445919752121, acc: 0.977337121963501)
[2025-02-13 02:49:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:06][root][INFO] - Training Epoch: 1/2, step 2539/7134 completed (loss: 0.0188100878149271, acc: 0.9928571581840515)
[2025-02-13 02:49:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:06][root][INFO] - Training Epoch: 1/2, step 2540/7134 completed (loss: 0.08986318111419678, acc: 0.9816513657569885)
[2025-02-13 02:49:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:07][root][INFO] - Training Epoch: 1/2, step 2541/7134 completed (loss: 0.018421834334731102, acc: 0.9933775067329407)
[2025-02-13 02:49:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:07][root][INFO] - Training Epoch: 1/2, step 2542/7134 completed (loss: 0.06930233538150787, acc: 0.9896449446678162)
[2025-02-13 02:49:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:08][root][INFO] - Training Epoch: 1/2, step 2543/7134 completed (loss: 0.0830770954489708, acc: 0.9795082211494446)
[2025-02-13 02:49:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:08][root][INFO] - Training Epoch: 1/2, step 2544/7134 completed (loss: 0.0475749634206295, acc: 0.9858247637748718)
[2025-02-13 02:49:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:09][root][INFO] - Training Epoch: 1/2, step 2545/7134 completed (loss: 0.04922914132475853, acc: 0.9860383868217468)
[2025-02-13 02:49:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:09][root][INFO] - Training Epoch: 1/2, step 2546/7134 completed (loss: 0.15208186209201813, acc: 0.9651860594749451)
[2025-02-13 02:49:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:09][root][INFO] - Training Epoch: 1/2, step 2547/7134 completed (loss: 0.06837143748998642, acc: 0.9806763529777527)
[2025-02-13 02:49:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:10][root][INFO] - Training Epoch: 1/2, step 2548/7134 completed (loss: 0.03833632543683052, acc: 0.9901574850082397)
[2025-02-13 02:49:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:10][root][INFO] - Training Epoch: 1/2, step 2549/7134 completed (loss: 0.05232248455286026, acc: 0.9834254384040833)
[2025-02-13 02:49:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:11][root][INFO] - Training Epoch: 1/2, step 2550/7134 completed (loss: 0.033065907657146454, acc: 0.9869494438171387)
[2025-02-13 02:49:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:11][root][INFO] - Training Epoch: 1/2, step 2551/7134 completed (loss: 0.04761435464024544, acc: 0.9884488582611084)
[2025-02-13 02:49:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:12][root][INFO] - Training Epoch: 1/2, step 2552/7134 completed (loss: 0.017149632796645164, acc: 0.9936908483505249)
[2025-02-13 02:49:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:12][root][INFO] - Training Epoch: 1/2, step 2553/7134 completed (loss: 0.06643866747617722, acc: 0.981873095035553)
[2025-02-13 02:49:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:13][root][INFO] - Training Epoch: 1/2, step 2554/7134 completed (loss: 0.08257576823234558, acc: 0.9849246144294739)
[2025-02-13 02:49:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:13][root][INFO] - Training Epoch: 1/2, step 2555/7134 completed (loss: 0.039208900183439255, acc: 0.9834710955619812)
[2025-02-13 02:49:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:13][root][INFO] - Training Epoch: 1/2, step 2556/7134 completed (loss: 0.04966089129447937, acc: 0.9853249192237854)
[2025-02-13 02:49:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:14][root][INFO] - Training Epoch: 1/2, step 2557/7134 completed (loss: 0.06401894241571426, acc: 0.9836065769195557)
[2025-02-13 02:49:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:14][root][INFO] - Training Epoch: 1/2, step 2558/7134 completed (loss: 0.027070051059126854, acc: 0.9951456189155579)
[2025-02-13 02:49:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:15][root][INFO] - Training Epoch: 1/2, step 2559/7134 completed (loss: 0.02763661928474903, acc: 0.9932340979576111)
[2025-02-13 02:49:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:15][root][INFO] - Training Epoch: 1/2, step 2560/7134 completed (loss: 0.03841931000351906, acc: 0.9886178970336914)
[2025-02-13 02:49:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:16][root][INFO] - Training Epoch: 1/2, step 2561/7134 completed (loss: 0.06223210319876671, acc: 0.9825708270072937)
[2025-02-13 02:49:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:16][root][INFO] - Training Epoch: 1/2, step 2562/7134 completed (loss: 0.037948351353406906, acc: 0.988034188747406)
[2025-02-13 02:49:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:17][root][INFO] - Training Epoch: 1/2, step 2563/7134 completed (loss: 0.047320034354925156, acc: 0.9897360801696777)
[2025-02-13 02:49:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:17][root][INFO] - Training Epoch: 1/2, step 2564/7134 completed (loss: 0.1076163500547409, acc: 0.9781879186630249)
[2025-02-13 02:49:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:17][root][INFO] - Training Epoch: 1/2, step 2565/7134 completed (loss: 0.02674986980855465, acc: 0.9909365773200989)
[2025-02-13 02:49:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:18][root][INFO] - Training Epoch: 1/2, step 2566/7134 completed (loss: 0.04115765541791916, acc: 0.9873595237731934)
[2025-02-13 02:49:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:18][root][INFO] - Training Epoch: 1/2, step 2567/7134 completed (loss: 0.03638768568634987, acc: 0.9876543283462524)
[2025-02-13 02:49:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:19][root][INFO] - Training Epoch: 1/2, step 2568/7134 completed (loss: 0.0528135672211647, acc: 0.9826589822769165)
[2025-02-13 02:49:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:19][root][INFO] - Training Epoch: 1/2, step 2569/7134 completed (loss: 0.05163896456360817, acc: 0.9864457845687866)
[2025-02-13 02:49:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:20][root][INFO] - Training Epoch: 1/2, step 2570/7134 completed (loss: 0.0477728508412838, acc: 0.9910045266151428)
[2025-02-13 02:49:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:20][root][INFO] - Training Epoch: 1/2, step 2571/7134 completed (loss: 0.028777699917554855, acc: 0.994727611541748)
[2025-02-13 02:49:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:21][root][INFO] - Training Epoch: 1/2, step 2572/7134 completed (loss: 0.05491633340716362, acc: 0.9873816967010498)
[2025-02-13 02:49:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:21][root][INFO] - Training Epoch: 1/2, step 2573/7134 completed (loss: 0.057115472853183746, acc: 0.9852458834648132)
[2025-02-13 02:49:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:21][root][INFO] - Training Epoch: 1/2, step 2574/7134 completed (loss: 0.033254172652959824, acc: 0.9890829920768738)
[2025-02-13 02:49:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:22][root][INFO] - Training Epoch: 1/2, step 2575/7134 completed (loss: 0.04002911597490311, acc: 0.9888476133346558)
[2025-02-13 02:49:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:22][root][INFO] - Training Epoch: 1/2, step 2576/7134 completed (loss: 0.04058859869837761, acc: 0.9896551966667175)
[2025-02-13 02:49:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:23][root][INFO] - Training Epoch: 1/2, step 2577/7134 completed (loss: 0.06377271562814713, acc: 0.9846153855323792)
[2025-02-13 02:49:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:23][root][INFO] - Training Epoch: 1/2, step 2578/7134 completed (loss: 0.0665481835603714, acc: 0.9800918698310852)
[2025-02-13 02:49:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:24][root][INFO] - Training Epoch: 1/2, step 2579/7134 completed (loss: 0.07654399424791336, acc: 0.9810126423835754)
[2025-02-13 02:49:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:24][root][INFO] - Training Epoch: 1/2, step 2580/7134 completed (loss: 0.05515966936945915, acc: 0.977806806564331)
[2025-02-13 02:49:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:25][root][INFO] - Training Epoch: 1/2, step 2581/7134 completed (loss: 0.043213944882154465, acc: 0.9932705163955688)
[2025-02-13 02:49:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:25][root][INFO] - Training Epoch: 1/2, step 2582/7134 completed (loss: 0.0412105955183506, acc: 0.9878048896789551)
[2025-02-13 02:49:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:25][root][INFO] - Training Epoch: 1/2, step 2583/7134 completed (loss: 0.06414023041725159, acc: 0.9772403836250305)
[2025-02-13 02:49:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:26][root][INFO] - Training Epoch: 1/2, step 2584/7134 completed (loss: 0.09411181509494781, acc: 0.9755671620368958)
[2025-02-13 02:49:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:26][root][INFO] - Training Epoch: 1/2, step 2585/7134 completed (loss: 0.055031634867191315, acc: 0.9899280667304993)
[2025-02-13 02:49:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:27][root][INFO] - Training Epoch: 1/2, step 2586/7134 completed (loss: 0.0656152293086052, acc: 0.98591548204422)
[2025-02-13 02:49:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:27][root][INFO] - Training Epoch: 1/2, step 2587/7134 completed (loss: 0.06466422230005264, acc: 0.9901719689369202)
[2025-02-13 02:49:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:28][root][INFO] - Training Epoch: 1/2, step 2588/7134 completed (loss: 0.02693679742515087, acc: 0.9890859723091125)
[2025-02-13 02:49:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:28][root][INFO] - Training Epoch: 1/2, step 2589/7134 completed (loss: 0.04667427018284798, acc: 0.9858657121658325)
[2025-02-13 02:49:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:29][root][INFO] - Training Epoch: 1/2, step 2590/7134 completed (loss: 0.04539046436548233, acc: 0.9901960492134094)
[2025-02-13 02:49:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:29][root][INFO] - Training Epoch: 1/2, step 2591/7134 completed (loss: 0.06838013976812363, acc: 0.9795396327972412)
[2025-02-13 02:49:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:29][root][INFO] - Training Epoch: 1/2, step 2592/7134 completed (loss: 0.05831662192940712, acc: 0.9803197979927063)
[2025-02-13 02:49:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:30][root][INFO] - Training Epoch: 1/2, step 2593/7134 completed (loss: 0.05800611525774002, acc: 0.9895287752151489)
[2025-02-13 02:49:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:30][root][INFO] - Training Epoch: 1/2, step 2594/7134 completed (loss: 0.04736146330833435, acc: 0.9847095012664795)
[2025-02-13 02:49:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:31][root][INFO] - Training Epoch: 1/2, step 2595/7134 completed (loss: 0.027765238657593727, acc: 0.9905914068222046)
[2025-02-13 02:49:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:31][root][INFO] - Training Epoch: 1/2, step 2596/7134 completed (loss: 0.05751772224903107, acc: 0.9887217879295349)
[2025-02-13 02:49:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:32][root][INFO] - Training Epoch: 1/2, step 2597/7134 completed (loss: 0.04748429358005524, acc: 0.9878378510475159)
[2025-02-13 02:49:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:32][root][INFO] - Training Epoch: 1/2, step 2598/7134 completed (loss: 0.08626990765333176, acc: 0.9752321839332581)
[2025-02-13 02:49:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:33][root][INFO] - Training Epoch: 1/2, step 2599/7134 completed (loss: 0.01139550469815731, acc: 0.9973614811897278)
[2025-02-13 02:49:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:33][root][INFO] - Training Epoch: 1/2, step 2600/7134 completed (loss: 0.05115832760930061, acc: 0.9833546876907349)
[2025-02-13 02:49:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:34][root][INFO] - Training Epoch: 1/2, step 2601/7134 completed (loss: 0.06434297561645508, acc: 0.981055498123169)
[2025-02-13 02:49:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:34][root][INFO] - Training Epoch: 1/2, step 2602/7134 completed (loss: 0.057583995163440704, acc: 0.980629563331604)
[2025-02-13 02:49:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:35][root][INFO] - Training Epoch: 1/2, step 2603/7134 completed (loss: 0.047710102051496506, acc: 0.9829457402229309)
[2025-02-13 02:49:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:35][root][INFO] - Training Epoch: 1/2, step 2604/7134 completed (loss: 0.05676603317260742, acc: 0.9802631735801697)
[2025-02-13 02:49:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:36][root][INFO] - Training Epoch: 1/2, step 2605/7134 completed (loss: 0.10074448585510254, acc: 0.9729241728782654)
[2025-02-13 02:49:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:36][root][INFO] - Training Epoch: 1/2, step 2606/7134 completed (loss: 0.05078687146306038, acc: 0.9856114983558655)
[2025-02-13 02:49:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:36][root][INFO] - Training Epoch: 1/2, step 2607/7134 completed (loss: 0.051328130066394806, acc: 0.977647066116333)
[2025-02-13 02:49:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:37][root][INFO] - Training Epoch: 1/2, step 2608/7134 completed (loss: 0.0905950516462326, acc: 0.9752781391143799)
[2025-02-13 02:49:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:37][root][INFO] - Training Epoch: 1/2, step 2609/7134 completed (loss: 0.07991186529397964, acc: 0.9818941354751587)
[2025-02-13 02:49:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:38][root][INFO] - Training Epoch: 1/2, step 2610/7134 completed (loss: 0.08879510313272476, acc: 0.981679379940033)
[2025-02-13 02:49:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:38][root][INFO] - Training Epoch: 1/2, step 2611/7134 completed (loss: 0.09292800724506378, acc: 0.9758898019790649)
[2025-02-13 02:49:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:39][root][INFO] - Training Epoch: 1/2, step 2612/7134 completed (loss: 0.06949827820062637, acc: 0.9783599376678467)
[2025-02-13 02:49:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:39][root][INFO] - Training Epoch: 1/2, step 2613/7134 completed (loss: 0.041614726185798645, acc: 0.9848101139068604)
[2025-02-13 02:49:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:40][root][INFO] - Training Epoch: 1/2, step 2614/7134 completed (loss: 0.10003634542226791, acc: 0.9738041162490845)
[2025-02-13 02:49:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:40][root][INFO] - Training Epoch: 1/2, step 2615/7134 completed (loss: 0.0602198988199234, acc: 0.9781491160392761)
[2025-02-13 02:49:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:41][root][INFO] - Training Epoch: 1/2, step 2616/7134 completed (loss: 0.07617690414190292, acc: 0.9794238805770874)
[2025-02-13 02:49:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:41][root][INFO] - Training Epoch: 1/2, step 2617/7134 completed (loss: 0.09345769137144089, acc: 0.9743589758872986)
[2025-02-13 02:49:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:41][root][INFO] - Training Epoch: 1/2, step 2618/7134 completed (loss: 0.06291192770004272, acc: 0.9794661402702332)
[2025-02-13 02:49:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:42][root][INFO] - Training Epoch: 1/2, step 2619/7134 completed (loss: 0.05775470286607742, acc: 0.9815436005592346)
[2025-02-13 02:49:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:42][root][INFO] - Training Epoch: 1/2, step 2620/7134 completed (loss: 0.06701938807964325, acc: 0.9801734685897827)
[2025-02-13 02:49:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:43][root][INFO] - Training Epoch: 1/2, step 2621/7134 completed (loss: 0.04930226877331734, acc: 0.9869668483734131)
[2025-02-13 02:49:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:43][root][INFO] - Training Epoch: 1/2, step 2622/7134 completed (loss: 0.05567809194326401, acc: 0.9836333990097046)
[2025-02-13 02:49:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:44][root][INFO] - Training Epoch: 1/2, step 2623/7134 completed (loss: 0.06768230348825455, acc: 0.977011501789093)
[2025-02-13 02:49:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:44][root][INFO] - Training Epoch: 1/2, step 2624/7134 completed (loss: 0.05476799234747887, acc: 0.9865016937255859)
[2025-02-13 02:49:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:45][root][INFO] - Training Epoch: 1/2, step 2625/7134 completed (loss: 0.05913389474153519, acc: 0.9833333492279053)
[2025-02-13 02:49:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:45][root][INFO] - Training Epoch: 1/2, step 2626/7134 completed (loss: 0.032107140868902206, acc: 0.9876237511634827)
[2025-02-13 02:49:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:46][root][INFO] - Training Epoch: 1/2, step 2627/7134 completed (loss: 0.047723911702632904, acc: 0.9851149916648865)
[2025-02-13 02:49:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:46][root][INFO] - Training Epoch: 1/2, step 2628/7134 completed (loss: 0.06421098858118057, acc: 0.9810606241226196)
[2025-02-13 02:49:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:47][root][INFO] - Training Epoch: 1/2, step 2629/7134 completed (loss: 0.03347416967153549, acc: 0.991150438785553)
[2025-02-13 02:49:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:47][root][INFO] - Training Epoch: 1/2, step 2630/7134 completed (loss: 0.03654664754867554, acc: 0.9904153347015381)
[2025-02-13 02:49:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:48][root][INFO] - Training Epoch: 1/2, step 2631/7134 completed (loss: 0.04133995994925499, acc: 0.9871345162391663)
[2025-02-13 02:49:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:48][root][INFO] - Training Epoch: 1/2, step 2632/7134 completed (loss: 0.019900957122445107, acc: 0.994106113910675)
[2025-02-13 02:49:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:49][root][INFO] - Training Epoch: 1/2, step 2633/7134 completed (loss: 0.09942758083343506, acc: 0.9759206771850586)
[2025-02-13 02:49:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:49][root][INFO] - Training Epoch: 1/2, step 2634/7134 completed (loss: 0.0199971292167902, acc: 0.9925037622451782)
[2025-02-13 02:49:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:49][root][INFO] - Training Epoch: 1/2, step 2635/7134 completed (loss: 0.0888158306479454, acc: 0.9791666865348816)
[2025-02-13 02:49:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:50][root][INFO] - Training Epoch: 1/2, step 2636/7134 completed (loss: 0.06576953828334808, acc: 0.9829303026199341)
[2025-02-13 02:49:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:50][root][INFO] - Training Epoch: 1/2, step 2637/7134 completed (loss: 0.21660985052585602, acc: 0.9490662217140198)
[2025-02-13 02:49:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:51][root][INFO] - Training Epoch: 1/2, step 2638/7134 completed (loss: 0.10674639791250229, acc: 0.9713168144226074)
[2025-02-13 02:49:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:51][root][INFO] - Training Epoch: 1/2, step 2639/7134 completed (loss: 0.10491659492254257, acc: 0.9730290174484253)
[2025-02-13 02:49:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:52][root][INFO] - Training Epoch: 1/2, step 2640/7134 completed (loss: 0.0909053236246109, acc: 0.9825218319892883)
[2025-02-13 02:49:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:52][root][INFO] - Training Epoch: 1/2, step 2641/7134 completed (loss: 0.09228482842445374, acc: 0.9749631881713867)
[2025-02-13 02:49:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:53][root][INFO] - Training Epoch: 1/2, step 2642/7134 completed (loss: 0.11864428222179413, acc: 0.9679715037345886)
[2025-02-13 02:49:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:53][root][INFO] - Training Epoch: 1/2, step 2643/7134 completed (loss: 0.08165701478719711, acc: 0.97508305311203)
[2025-02-13 02:49:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:53][root][INFO] - Training Epoch: 1/2, step 2644/7134 completed (loss: 0.07441762089729309, acc: 0.9842233061790466)
[2025-02-13 02:49:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:54][root][INFO] - Training Epoch: 1/2, step 2645/7134 completed (loss: 0.06068246811628342, acc: 0.9825072884559631)
[2025-02-13 02:49:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:54][root][INFO] - Training Epoch: 1/2, step 2646/7134 completed (loss: 0.13097815215587616, acc: 0.9601910710334778)
[2025-02-13 02:49:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:55][root][INFO] - Training Epoch: 1/2, step 2647/7134 completed (loss: 0.05283093824982643, acc: 0.981794536113739)
[2025-02-13 02:49:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:55][root][INFO] - Training Epoch: 1/2, step 2648/7134 completed (loss: 0.030077869072556496, acc: 0.9883871078491211)
[2025-02-13 02:49:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:56][root][INFO] - Training Epoch: 1/2, step 2649/7134 completed (loss: 0.1454371213912964, acc: 0.9567809104919434)
[2025-02-13 02:49:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:56][root][INFO] - Training Epoch: 1/2, step 2650/7134 completed (loss: 0.12105948477983475, acc: 0.9621785283088684)
[2025-02-13 02:49:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:57][root][INFO] - Training Epoch: 1/2, step 2651/7134 completed (loss: 0.04456091672182083, acc: 0.9880810379981995)
[2025-02-13 02:49:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:57][root][INFO] - Training Epoch: 1/2, step 2652/7134 completed (loss: 0.05472954735159874, acc: 0.9836065769195557)
[2025-02-13 02:49:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:57][root][INFO] - Training Epoch: 1/2, step 2653/7134 completed (loss: 0.06769652664661407, acc: 0.9753289222717285)
[2025-02-13 02:49:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:58][root][INFO] - Training Epoch: 1/2, step 2654/7134 completed (loss: 0.05413374304771423, acc: 0.9868852496147156)
[2025-02-13 02:49:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:58][root][INFO] - Training Epoch: 1/2, step 2655/7134 completed (loss: 0.02679511159658432, acc: 0.9939485788345337)
[2025-02-13 02:49:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:59][root][INFO] - Training Epoch: 1/2, step 2656/7134 completed (loss: 0.06272508203983307, acc: 0.9847009778022766)
[2025-02-13 02:49:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:49:59][root][INFO] - Training Epoch: 1/2, step 2657/7134 completed (loss: 0.045825082808732986, acc: 0.9887096881866455)
[2025-02-13 02:49:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:00][root][INFO] - Training Epoch: 1/2, step 2658/7134 completed (loss: 0.12246540188789368, acc: 0.97062748670578)
[2025-02-13 02:50:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:00][root][INFO] - Training Epoch: 1/2, step 2659/7134 completed (loss: 0.055401839315891266, acc: 0.9865900278091431)
[2025-02-13 02:50:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:00][root][INFO] - Training Epoch: 1/2, step 2660/7134 completed (loss: 0.042469970881938934, acc: 0.9879032373428345)
[2025-02-13 02:50:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:01][root][INFO] - Training Epoch: 1/2, step 2661/7134 completed (loss: 0.05256235599517822, acc: 0.991584837436676)
[2025-02-13 02:50:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:01][root][INFO] - Training Epoch: 1/2, step 2662/7134 completed (loss: 0.021192628890275955, acc: 0.9943100810050964)
[2025-02-13 02:50:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:02][root][INFO] - Training Epoch: 1/2, step 2663/7134 completed (loss: 0.03736644983291626, acc: 0.9894737005233765)
[2025-02-13 02:50:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:02][root][INFO] - Training Epoch: 1/2, step 2664/7134 completed (loss: 0.020813141018152237, acc: 0.9935483932495117)
[2025-02-13 02:50:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:03][root][INFO] - Training Epoch: 1/2, step 2665/7134 completed (loss: 0.024229809641838074, acc: 0.9895833134651184)
[2025-02-13 02:50:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:03][root][INFO] - Training Epoch: 1/2, step 2666/7134 completed (loss: 0.04036008194088936, acc: 0.984088122844696)
[2025-02-13 02:50:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:04][root][INFO] - Training Epoch: 1/2, step 2667/7134 completed (loss: 0.02226700820028782, acc: 0.9943342804908752)
[2025-02-13 02:50:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:04][root][INFO] - Training Epoch: 1/2, step 2668/7134 completed (loss: 0.020269982516765594, acc: 0.9959623217582703)
[2025-02-13 02:50:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:05][root][INFO] - Training Epoch: 1/2, step 2669/7134 completed (loss: 0.019725315272808075, acc: 0.9965477585792542)
[2025-02-13 02:50:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:05][root][INFO] - Training Epoch: 1/2, step 2670/7134 completed (loss: 0.04467945545911789, acc: 0.9882044792175293)
[2025-02-13 02:50:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:05][root][INFO] - Training Epoch: 1/2, step 2671/7134 completed (loss: 0.04019789397716522, acc: 0.9879679083824158)
[2025-02-13 02:50:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:06][root][INFO] - Training Epoch: 1/2, step 2672/7134 completed (loss: 0.01992824859917164, acc: 0.9970845580101013)
[2025-02-13 02:50:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:06][root][INFO] - Training Epoch: 1/2, step 2673/7134 completed (loss: 0.02312229387462139, acc: 0.9911616444587708)
[2025-02-13 02:50:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:07][root][INFO] - Training Epoch: 1/2, step 2674/7134 completed (loss: 0.02235282212495804, acc: 0.991631805896759)
[2025-02-13 02:50:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:07][root][INFO] - Training Epoch: 1/2, step 2675/7134 completed (loss: 0.0088254539296031, acc: 0.998630166053772)
[2025-02-13 02:50:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:08][root][INFO] - Training Epoch: 1/2, step 2676/7134 completed (loss: 0.02535027079284191, acc: 0.9922077655792236)
[2025-02-13 02:50:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:08][root][INFO] - Training Epoch: 1/2, step 2677/7134 completed (loss: 0.01825166679918766, acc: 0.992559552192688)
[2025-02-13 02:50:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:09][root][INFO] - Training Epoch: 1/2, step 2678/7134 completed (loss: 0.00608544098213315, acc: 1.0)
[2025-02-13 02:50:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:09][root][INFO] - Training Epoch: 1/2, step 2679/7134 completed (loss: 0.014421438798308372, acc: 0.9943883419036865)
[2025-02-13 02:50:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:10][root][INFO] - Training Epoch: 1/2, step 2680/7134 completed (loss: 0.024815615266561508, acc: 0.9938347935676575)
[2025-02-13 02:50:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:10][root][INFO] - Training Epoch: 1/2, step 2681/7134 completed (loss: 0.008854784071445465, acc: 0.9969651103019714)
[2025-02-13 02:50:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:10][root][INFO] - Training Epoch: 1/2, step 2682/7134 completed (loss: 0.0103208152577281, acc: 0.9984050989151001)
[2025-02-13 02:50:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:11][root][INFO] - Training Epoch: 1/2, step 2683/7134 completed (loss: 0.009218117222189903, acc: 0.9939302206039429)
[2025-02-13 02:50:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:11][root][INFO] - Training Epoch: 1/2, step 2684/7134 completed (loss: 0.04115951806306839, acc: 0.9946236610412598)
[2025-02-13 02:50:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:12][root][INFO] - Training Epoch: 1/2, step 2685/7134 completed (loss: 0.01861828938126564, acc: 0.9947984218597412)
[2025-02-13 02:50:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:12][root][INFO] - Training Epoch: 1/2, step 2686/7134 completed (loss: 0.059801820665597916, acc: 0.9814814925193787)
[2025-02-13 02:50:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:13][root][INFO] - Training Epoch: 1/2, step 2687/7134 completed (loss: 0.06606008857488632, acc: 0.9805653691291809)
[2025-02-13 02:50:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:13][root][INFO] - Training Epoch: 1/2, step 2688/7134 completed (loss: 0.05487675219774246, acc: 0.9858490824699402)
[2025-02-13 02:50:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:14][root][INFO] - Training Epoch: 1/2, step 2689/7134 completed (loss: 0.045459069311618805, acc: 0.9873417615890503)
[2025-02-13 02:50:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:14][root][INFO] - Training Epoch: 1/2, step 2690/7134 completed (loss: 0.07740945369005203, acc: 0.9768160581588745)
[2025-02-13 02:50:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:14][root][INFO] - Training Epoch: 1/2, step 2691/7134 completed (loss: 0.09410975873470306, acc: 0.98046875)
[2025-02-13 02:50:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:15][root][INFO] - Training Epoch: 1/2, step 2692/7134 completed (loss: 0.12134307622909546, acc: 0.9757914543151855)
[2025-02-13 02:50:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:15][root][INFO] - Training Epoch: 1/2, step 2693/7134 completed (loss: 0.12011642754077911, acc: 0.9730769395828247)
[2025-02-13 02:50:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:16][root][INFO] - Training Epoch: 1/2, step 2694/7134 completed (loss: 0.09906668961048126, acc: 0.9823529124259949)
[2025-02-13 02:50:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:16][root][INFO] - Training Epoch: 1/2, step 2695/7134 completed (loss: 0.048305388540029526, acc: 0.9865996837615967)
[2025-02-13 02:50:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:17][root][INFO] - Training Epoch: 1/2, step 2696/7134 completed (loss: 0.07751485705375671, acc: 0.9809221029281616)
[2025-02-13 02:50:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:17][root][INFO] - Training Epoch: 1/2, step 2697/7134 completed (loss: 0.10212043672800064, acc: 0.9722650051116943)
[2025-02-13 02:50:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:18][root][INFO] - Training Epoch: 1/2, step 2698/7134 completed (loss: 0.05732261389493942, acc: 0.976068377494812)
[2025-02-13 02:50:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:18][root][INFO] - Training Epoch: 1/2, step 2699/7134 completed (loss: 0.059600118547677994, acc: 0.9845474362373352)
[2025-02-13 02:50:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:18][root][INFO] - Training Epoch: 1/2, step 2700/7134 completed (loss: 0.06149347871541977, acc: 0.9821958541870117)
[2025-02-13 02:50:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:19][root][INFO] - Training Epoch: 1/2, step 2701/7134 completed (loss: 0.0569797083735466, acc: 0.9831932783126831)
[2025-02-13 02:50:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:19][root][INFO] - Training Epoch: 1/2, step 2702/7134 completed (loss: 0.10513945668935776, acc: 0.9791666865348816)
[2025-02-13 02:50:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:20][root][INFO] - Training Epoch: 1/2, step 2703/7134 completed (loss: 0.030301406979560852, acc: 0.993630588054657)
[2025-02-13 02:50:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:20][root][INFO] - Training Epoch: 1/2, step 2704/7134 completed (loss: 0.06829666346311569, acc: 0.986522912979126)
[2025-02-13 02:50:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:21][root][INFO] - Training Epoch: 1/2, step 2705/7134 completed (loss: 0.07486389577388763, acc: 0.9783950448036194)
[2025-02-13 02:50:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:21][root][INFO] - Training Epoch: 1/2, step 2706/7134 completed (loss: 0.06981297582387924, acc: 0.9825834631919861)
[2025-02-13 02:50:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:22][root][INFO] - Training Epoch: 1/2, step 2707/7134 completed (loss: 0.037410054355859756, acc: 0.9948275685310364)
[2025-02-13 02:50:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:22][root][INFO] - Training Epoch: 1/2, step 2708/7134 completed (loss: 0.07745305448770523, acc: 0.9719495177268982)
[2025-02-13 02:50:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:23][root][INFO] - Training Epoch: 1/2, step 2709/7134 completed (loss: 0.029379554092884064, acc: 0.9967532753944397)
[2025-02-13 02:50:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:23][root][INFO] - Training Epoch: 1/2, step 2710/7134 completed (loss: 0.03636473789811134, acc: 0.9883449673652649)
[2025-02-13 02:50:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:23][root][INFO] - Training Epoch: 1/2, step 2711/7134 completed (loss: 0.0514899305999279, acc: 0.9858490824699402)
[2025-02-13 02:50:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:24][root][INFO] - Training Epoch: 1/2, step 2712/7134 completed (loss: 0.061727870255708694, acc: 0.9821428656578064)
[2025-02-13 02:50:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:24][root][INFO] - Training Epoch: 1/2, step 2713/7134 completed (loss: 0.06336261332035065, acc: 0.9832869172096252)
[2025-02-13 02:50:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:25][root][INFO] - Training Epoch: 1/2, step 2714/7134 completed (loss: 0.04561593011021614, acc: 0.9900166392326355)
[2025-02-13 02:50:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:25][root][INFO] - Training Epoch: 1/2, step 2715/7134 completed (loss: 0.03914925083518028, acc: 0.9879699349403381)
[2025-02-13 02:50:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:26][root][INFO] - Training Epoch: 1/2, step 2716/7134 completed (loss: 0.06094641983509064, acc: 0.9829620122909546)
[2025-02-13 02:50:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:26][root][INFO] - Training Epoch: 1/2, step 2717/7134 completed (loss: 0.09322772175073624, acc: 0.9733059406280518)
[2025-02-13 02:50:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:27][root][INFO] - Training Epoch: 1/2, step 2718/7134 completed (loss: 0.1084119975566864, acc: 0.9647887349128723)
[2025-02-13 02:50:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:27][root][INFO] - Training Epoch: 1/2, step 2719/7134 completed (loss: 0.09180156141519547, acc: 0.9695431590080261)
[2025-02-13 02:50:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:27][root][INFO] - Training Epoch: 1/2, step 2720/7134 completed (loss: 0.04362785071134567, acc: 0.9842271208763123)
[2025-02-13 02:50:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:28][root][INFO] - Training Epoch: 1/2, step 2721/7134 completed (loss: 0.0657060295343399, acc: 0.9884058237075806)
[2025-02-13 02:50:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:28][root][INFO] - Training Epoch: 1/2, step 2722/7134 completed (loss: 0.14484292268753052, acc: 0.969565212726593)
[2025-02-13 02:50:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:29][root][INFO] - Training Epoch: 1/2, step 2723/7134 completed (loss: 0.11540945619344711, acc: 0.9753997325897217)
[2025-02-13 02:50:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:29][root][INFO] - Training Epoch: 1/2, step 2724/7134 completed (loss: 0.06402144581079483, acc: 0.9878493547439575)
[2025-02-13 02:50:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:29][root][INFO] - Training Epoch: 1/2, step 2725/7134 completed (loss: 0.03218407928943634, acc: 0.9885057210922241)
[2025-02-13 02:50:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:30][root][INFO] - Training Epoch: 1/2, step 2726/7134 completed (loss: 0.09169311821460724, acc: 0.9833948612213135)
[2025-02-13 02:50:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:30][root][INFO] - Training Epoch: 1/2, step 2727/7134 completed (loss: 0.05151550844311714, acc: 0.9870370626449585)
[2025-02-13 02:50:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:31][root][INFO] - Training Epoch: 1/2, step 2728/7134 completed (loss: 0.12582474946975708, acc: 0.9655963182449341)
[2025-02-13 02:50:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:31][root][INFO] - Training Epoch: 1/2, step 2729/7134 completed (loss: 0.07561324536800385, acc: 0.9756097793579102)
[2025-02-13 02:50:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:32][root][INFO] - Training Epoch: 1/2, step 2730/7134 completed (loss: 0.08972744643688202, acc: 0.981249988079071)
[2025-02-13 02:50:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:32][root][INFO] - Training Epoch: 1/2, step 2731/7134 completed (loss: 0.14587491750717163, acc: 0.9635036587715149)
[2025-02-13 02:50:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:33][root][INFO] - Training Epoch: 1/2, step 2732/7134 completed (loss: 0.06614294648170471, acc: 0.9746192693710327)
[2025-02-13 02:50:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:33][root][INFO] - Training Epoch: 1/2, step 2733/7134 completed (loss: 0.07265318930149078, acc: 0.9782903790473938)
[2025-02-13 02:50:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:33][root][INFO] - Training Epoch: 1/2, step 2734/7134 completed (loss: 0.0709233433008194, acc: 0.9798792600631714)
[2025-02-13 02:50:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:34][root][INFO] - Training Epoch: 1/2, step 2735/7134 completed (loss: 0.05913684889674187, acc: 0.9837398529052734)
[2025-02-13 02:50:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:34][root][INFO] - Training Epoch: 1/2, step 2736/7134 completed (loss: 0.09540534019470215, acc: 0.9701492786407471)
[2025-02-13 02:50:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:35][root][INFO] - Training Epoch: 1/2, step 2737/7134 completed (loss: 0.05135937035083771, acc: 0.9884488582611084)
[2025-02-13 02:50:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:35][root][INFO] - Training Epoch: 1/2, step 2738/7134 completed (loss: 0.05677763372659683, acc: 0.977142870426178)
[2025-02-13 02:50:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:36][root][INFO] - Training Epoch: 1/2, step 2739/7134 completed (loss: 0.07862122356891632, acc: 0.9763636589050293)
[2025-02-13 02:50:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:36][root][INFO] - Training Epoch: 1/2, step 2740/7134 completed (loss: 0.035921860486269, acc: 0.9912917017936707)
[2025-02-13 02:50:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:36][root][INFO] - Training Epoch: 1/2, step 2741/7134 completed (loss: 0.0826626569032669, acc: 0.9719626307487488)
[2025-02-13 02:50:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:37][root][INFO] - Training Epoch: 1/2, step 2742/7134 completed (loss: 0.03150032460689545, acc: 0.9854166507720947)
[2025-02-13 02:50:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:37][root][INFO] - Training Epoch: 1/2, step 2743/7134 completed (loss: 0.0257297083735466, acc: 0.99210524559021)
[2025-02-13 02:50:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:38][root][INFO] - Training Epoch: 1/2, step 2744/7134 completed (loss: 0.05045585334300995, acc: 0.984674334526062)
[2025-02-13 02:50:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:38][root][INFO] - Training Epoch: 1/2, step 2745/7134 completed (loss: 0.08086642622947693, acc: 0.9721254110336304)
[2025-02-13 02:50:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:38][root][INFO] - Training Epoch: 1/2, step 2746/7134 completed (loss: 0.06422988325357437, acc: 0.9807692170143127)
[2025-02-13 02:50:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:39][root][INFO] - Training Epoch: 1/2, step 2747/7134 completed (loss: 0.03454072028398514, acc: 0.9852579832077026)
[2025-02-13 02:50:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:39][root][INFO] - Training Epoch: 1/2, step 2748/7134 completed (loss: 0.020498210564255714, acc: 0.9968152642250061)
[2025-02-13 02:50:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:40][root][INFO] - Training Epoch: 1/2, step 2749/7134 completed (loss: 0.05007182061672211, acc: 0.9863387942314148)
[2025-02-13 02:50:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:40][root][INFO] - Training Epoch: 1/2, step 2750/7134 completed (loss: 0.02375815249979496, acc: 0.9909774661064148)
[2025-02-13 02:50:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:40][root][INFO] - Training Epoch: 1/2, step 2751/7134 completed (loss: 0.0647205039858818, acc: 0.9797468185424805)
[2025-02-13 02:50:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:41][root][INFO] - Training Epoch: 1/2, step 2752/7134 completed (loss: 0.0429496094584465, acc: 0.9847908616065979)
[2025-02-13 02:50:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:41][root][INFO] - Training Epoch: 1/2, step 2753/7134 completed (loss: 0.08965187519788742, acc: 0.9649122953414917)
[2025-02-13 02:50:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:42][root][INFO] - Training Epoch: 1/2, step 2754/7134 completed (loss: 0.08137105405330658, acc: 0.9716024398803711)
[2025-02-13 02:50:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:42][root][INFO] - Training Epoch: 1/2, step 2755/7134 completed (loss: 0.041129663586616516, acc: 0.9883720874786377)
[2025-02-13 02:50:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:42][root][INFO] - Training Epoch: 1/2, step 2756/7134 completed (loss: 0.07943513244390488, acc: 0.9809523820877075)
[2025-02-13 02:50:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:43][root][INFO] - Training Epoch: 1/2, step 2757/7134 completed (loss: 0.07516597956418991, acc: 0.9699453711509705)
[2025-02-13 02:50:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:43][root][INFO] - Training Epoch: 1/2, step 2758/7134 completed (loss: 0.07109029591083527, acc: 0.9840182662010193)
[2025-02-13 02:50:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:44][root][INFO] - Training Epoch: 1/2, step 2759/7134 completed (loss: 0.041300270706415176, acc: 0.9866220951080322)
[2025-02-13 02:50:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:44][root][INFO] - Training Epoch: 1/2, step 2760/7134 completed (loss: 0.06860531866550446, acc: 0.9746031761169434)
[2025-02-13 02:50:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:44][root][INFO] - Training Epoch: 1/2, step 2761/7134 completed (loss: 0.07834266126155853, acc: 0.9785353541374207)
[2025-02-13 02:50:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:45][root][INFO] - Training Epoch: 1/2, step 2762/7134 completed (loss: 0.0712505578994751, acc: 0.9827387928962708)
[2025-02-13 02:50:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:45][root][INFO] - Training Epoch: 1/2, step 2763/7134 completed (loss: 0.059507668018341064, acc: 0.9849246144294739)
[2025-02-13 02:50:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:46][root][INFO] - Training Epoch: 1/2, step 2764/7134 completed (loss: 0.08123015612363815, acc: 0.9789196252822876)
[2025-02-13 02:50:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:46][root][INFO] - Training Epoch: 1/2, step 2765/7134 completed (loss: 0.03575878217816353, acc: 0.9956896305084229)
[2025-02-13 02:50:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:47][root][INFO] - Training Epoch: 1/2, step 2766/7134 completed (loss: 0.03116443380713463, acc: 0.9919224381446838)
[2025-02-13 02:50:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:47][root][INFO] - Training Epoch: 1/2, step 2767/7134 completed (loss: 0.08901423960924149, acc: 0.9814814925193787)
[2025-02-13 02:50:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:48][root][INFO] - Training Epoch: 1/2, step 2768/7134 completed (loss: 0.06950642913579941, acc: 0.9773691892623901)
[2025-02-13 02:50:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:48][root][INFO] - Training Epoch: 1/2, step 2769/7134 completed (loss: 0.05045700818300247, acc: 0.9831081032752991)
[2025-02-13 02:50:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:49][root][INFO] - Training Epoch: 1/2, step 2770/7134 completed (loss: 0.023664429783821106, acc: 0.9930459260940552)
[2025-02-13 02:50:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:49][root][INFO] - Training Epoch: 1/2, step 2771/7134 completed (loss: 0.05482364073395729, acc: 0.9807074069976807)
[2025-02-13 02:50:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:49][root][INFO] - Training Epoch: 1/2, step 2772/7134 completed (loss: 0.04467013478279114, acc: 0.9876957535743713)
[2025-02-13 02:50:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:50][root][INFO] - Training Epoch: 1/2, step 2773/7134 completed (loss: 0.045150239020586014, acc: 0.9828571677207947)
[2025-02-13 02:50:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:50][root][INFO] - Training Epoch: 1/2, step 2774/7134 completed (loss: 0.06669069826602936, acc: 0.9868420958518982)
[2025-02-13 02:50:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:51][root][INFO] - Training Epoch: 1/2, step 2775/7134 completed (loss: 0.05091675743460655, acc: 0.9865030646324158)
[2025-02-13 02:50:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:51][root][INFO] - Training Epoch: 1/2, step 2776/7134 completed (loss: 0.03281664475798607, acc: 0.987730085849762)
[2025-02-13 02:50:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:52][root][INFO] - Training Epoch: 1/2, step 2777/7134 completed (loss: 0.0363992340862751, acc: 0.9878048896789551)
[2025-02-13 02:50:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:52][root][INFO] - Training Epoch: 1/2, step 2778/7134 completed (loss: 0.04818185418844223, acc: 0.9884124994277954)
[2025-02-13 02:50:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:53][root][INFO] - Training Epoch: 1/2, step 2779/7134 completed (loss: 0.07254484295845032, acc: 0.9789029359817505)
[2025-02-13 02:50:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:53][root][INFO] - Training Epoch: 1/2, step 2780/7134 completed (loss: 0.06137968972325325, acc: 0.9852320551872253)
[2025-02-13 02:50:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:54][root][INFO] - Training Epoch: 1/2, step 2781/7134 completed (loss: 0.032735638320446014, acc: 0.9922279715538025)
[2025-02-13 02:50:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:54][root][INFO] - Training Epoch: 1/2, step 2782/7134 completed (loss: 0.049437686800956726, acc: 0.9884454011917114)
[2025-02-13 02:50:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:55][root][INFO] - Training Epoch: 1/2, step 2783/7134 completed (loss: 0.03769691661000252, acc: 0.9924924969673157)
[2025-02-13 02:50:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:55][root][INFO] - Training Epoch: 1/2, step 2784/7134 completed (loss: 0.04102057218551636, acc: 0.9905362725257874)
[2025-02-13 02:50:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:56][root][INFO] - Training Epoch: 1/2, step 2785/7134 completed (loss: 0.031175367534160614, acc: 0.9934498071670532)
[2025-02-13 02:50:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:56][root][INFO] - Training Epoch: 1/2, step 2786/7134 completed (loss: 0.05700407922267914, acc: 0.9833852648735046)
[2025-02-13 02:50:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:56][root][INFO] - Training Epoch: 1/2, step 2787/7134 completed (loss: 0.043754249811172485, acc: 0.9867197871208191)
[2025-02-13 02:50:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:57][root][INFO] - Training Epoch: 1/2, step 2788/7134 completed (loss: 0.04720664769411087, acc: 0.9803664684295654)
[2025-02-13 02:50:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:57][root][INFO] - Training Epoch: 1/2, step 2789/7134 completed (loss: 0.17640063166618347, acc: 0.9510086178779602)
[2025-02-13 02:50:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:58][root][INFO] - Training Epoch: 1/2, step 2790/7134 completed (loss: 0.16692200303077698, acc: 0.9580574035644531)
[2025-02-13 02:50:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:58][root][INFO] - Training Epoch: 1/2, step 2791/7134 completed (loss: 0.050085145980119705, acc: 0.9865471124649048)
[2025-02-13 02:50:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:59][root][INFO] - Training Epoch: 1/2, step 2792/7134 completed (loss: 0.03619426116347313, acc: 0.99068683385849)
[2025-02-13 02:50:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:59][root][INFO] - Training Epoch: 1/2, step 2793/7134 completed (loss: 0.06510289013385773, acc: 0.9838056564331055)
[2025-02-13 02:50:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:50:59][root][INFO] - Training Epoch: 1/2, step 2794/7134 completed (loss: 0.034095145761966705, acc: 0.990227997303009)
[2025-02-13 02:51:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:00][root][INFO] - Training Epoch: 1/2, step 2795/7134 completed (loss: 0.16011762619018555, acc: 0.9605568647384644)
[2025-02-13 02:51:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:00][root][INFO] - Training Epoch: 1/2, step 2796/7134 completed (loss: 0.14171123504638672, acc: 0.9539170265197754)
[2025-02-13 02:51:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:01][root][INFO] - Training Epoch: 1/2, step 2797/7134 completed (loss: 0.12540748715400696, acc: 0.9683544039726257)
[2025-02-13 02:51:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:01][root][INFO] - Training Epoch: 1/2, step 2798/7134 completed (loss: 0.11676644533872604, acc: 0.970588207244873)
[2025-02-13 02:51:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:02][root][INFO] - Training Epoch: 1/2, step 2799/7134 completed (loss: 0.08753667771816254, acc: 0.9789842367172241)
[2025-02-13 02:51:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:02][root][INFO] - Training Epoch: 1/2, step 2800/7134 completed (loss: 0.09699653834104538, acc: 0.9771528840065002)
[2025-02-13 02:51:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:02][root][INFO] - Training Epoch: 1/2, step 2801/7134 completed (loss: 0.05586893856525421, acc: 0.9846547245979309)
[2025-02-13 02:51:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:03][root][INFO] - Training Epoch: 1/2, step 2802/7134 completed (loss: 0.05962032824754715, acc: 0.9812734127044678)
[2025-02-13 02:51:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:03][root][INFO] - Training Epoch: 1/2, step 2803/7134 completed (loss: 0.019849533215165138, acc: 0.9957567453384399)
[2025-02-13 02:51:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:04][root][INFO] - Training Epoch: 1/2, step 2804/7134 completed (loss: 0.046524785459041595, acc: 0.9888613820075989)
[2025-02-13 02:51:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:04][root][INFO] - Training Epoch: 1/2, step 2805/7134 completed (loss: 0.030369458720088005, acc: 0.9894598126411438)
[2025-02-13 02:51:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:05][root][INFO] - Training Epoch: 1/2, step 2806/7134 completed (loss: 0.05572123825550079, acc: 0.9819672107696533)
[2025-02-13 02:51:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:05][root][INFO] - Training Epoch: 1/2, step 2807/7134 completed (loss: 0.08505024015903473, acc: 0.9771309494972229)
[2025-02-13 02:51:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:05][root][INFO] - Training Epoch: 1/2, step 2808/7134 completed (loss: 0.02683713659644127, acc: 0.9908571243286133)
[2025-02-13 02:51:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:06][root][INFO] - Training Epoch: 1/2, step 2809/7134 completed (loss: 0.054814405739307404, acc: 0.9839572310447693)
[2025-02-13 02:51:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:06][root][INFO] - Training Epoch: 1/2, step 2810/7134 completed (loss: 0.04952964931726456, acc: 0.9857482314109802)
[2025-02-13 02:51:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:07][root][INFO] - Training Epoch: 1/2, step 2811/7134 completed (loss: 0.013561989180743694, acc: 0.9988066554069519)
[2025-02-13 02:51:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:07][root][INFO] - Training Epoch: 1/2, step 2812/7134 completed (loss: 0.03657637536525726, acc: 0.9876543283462524)
[2025-02-13 02:51:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:08][root][INFO] - Training Epoch: 1/2, step 2813/7134 completed (loss: 0.019179726019501686, acc: 0.99370276927948)
[2025-02-13 02:51:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:08][root][INFO] - Training Epoch: 1/2, step 2814/7134 completed (loss: 0.045553047209978104, acc: 0.9922077655792236)
[2025-02-13 02:51:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:09][root][INFO] - Training Epoch: 1/2, step 2815/7134 completed (loss: 0.05707297474145889, acc: 0.9869513511657715)
[2025-02-13 02:51:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:09][root][INFO] - Training Epoch: 1/2, step 2816/7134 completed (loss: 0.07622039318084717, acc: 0.9828721880912781)
[2025-02-13 02:51:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:10][root][INFO] - Training Epoch: 1/2, step 2817/7134 completed (loss: 0.059190865606069565, acc: 0.9853249192237854)
[2025-02-13 02:51:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:10][root][INFO] - Training Epoch: 1/2, step 2818/7134 completed (loss: 0.048520926386117935, acc: 0.984649121761322)
[2025-02-13 02:51:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:10][root][INFO] - Training Epoch: 1/2, step 2819/7134 completed (loss: 0.019250433892011642, acc: 0.993537962436676)
[2025-02-13 02:51:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:11][root][INFO] - Training Epoch: 1/2, step 2820/7134 completed (loss: 0.10490866005420685, acc: 0.9779086709022522)
[2025-02-13 02:51:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:11][root][INFO] - Training Epoch: 1/2, step 2821/7134 completed (loss: 0.062393758445978165, acc: 0.9808841347694397)
[2025-02-13 02:51:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:12][root][INFO] - Training Epoch: 1/2, step 2822/7134 completed (loss: 0.07063107192516327, acc: 0.983565092086792)
[2025-02-13 02:51:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:12][root][INFO] - Training Epoch: 1/2, step 2823/7134 completed (loss: 0.08136671781539917, acc: 0.9744681119918823)
[2025-02-13 02:51:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:13][root][INFO] - Training Epoch: 1/2, step 2824/7134 completed (loss: 0.12874563038349152, acc: 0.9659090638160706)
[2025-02-13 02:51:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:13][root][INFO] - Training Epoch: 1/2, step 2825/7134 completed (loss: 0.06828803569078445, acc: 0.9892617464065552)
[2025-02-13 02:51:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:14][root][INFO] - Training Epoch: 1/2, step 2826/7134 completed (loss: 0.09624844044446945, acc: 0.9789156913757324)
[2025-02-13 02:51:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:14][root][INFO] - Training Epoch: 1/2, step 2827/7134 completed (loss: 0.06780416518449783, acc: 0.9821200370788574)
[2025-02-13 02:51:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:15][root][INFO] - Training Epoch: 1/2, step 2828/7134 completed (loss: 0.053569402545690536, acc: 0.984000027179718)
[2025-02-13 02:51:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:15][root][INFO] - Training Epoch: 1/2, step 2829/7134 completed (loss: 0.06817711889743805, acc: 0.9809004068374634)
[2025-02-13 02:51:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:15][root][INFO] - Training Epoch: 1/2, step 2830/7134 completed (loss: 0.01876957155764103, acc: 0.9981784820556641)
[2025-02-13 02:51:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:16][root][INFO] - Training Epoch: 1/2, step 2831/7134 completed (loss: 0.03879453241825104, acc: 0.9889415502548218)
[2025-02-13 02:51:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:16][root][INFO] - Training Epoch: 1/2, step 2832/7134 completed (loss: 0.054438769817352295, acc: 0.9890109896659851)
[2025-02-13 02:51:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:17][root][INFO] - Training Epoch: 1/2, step 2833/7134 completed (loss: 0.05244910717010498, acc: 0.984795331954956)
[2025-02-13 02:51:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:17][root][INFO] - Training Epoch: 1/2, step 2834/7134 completed (loss: 0.09031344950199127, acc: 0.9796954393386841)
[2025-02-13 02:51:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:18][root][INFO] - Training Epoch: 1/2, step 2835/7134 completed (loss: 0.07080981880426407, acc: 0.9806138873100281)
[2025-02-13 02:51:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:18][root][INFO] - Training Epoch: 1/2, step 2836/7134 completed (loss: 0.06023087725043297, acc: 0.9810426831245422)
[2025-02-13 02:51:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:19][root][INFO] - Training Epoch: 1/2, step 2837/7134 completed (loss: 0.03661644458770752, acc: 0.9891892075538635)
[2025-02-13 02:51:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:19][root][INFO] - Training Epoch: 1/2, step 2838/7134 completed (loss: 0.08295990526676178, acc: 0.9780334830284119)
[2025-02-13 02:51:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:19][root][INFO] - Training Epoch: 1/2, step 2839/7134 completed (loss: 0.06387756019830704, acc: 0.9817906022071838)
[2025-02-13 02:51:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:20][root][INFO] - Training Epoch: 1/2, step 2840/7134 completed (loss: 0.03287506103515625, acc: 0.988950252532959)
[2025-02-13 02:51:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:20][root][INFO] - Training Epoch: 1/2, step 2841/7134 completed (loss: 0.03490443900227547, acc: 0.9838709831237793)
[2025-02-13 02:51:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:21][root][INFO] - Training Epoch: 1/2, step 2842/7134 completed (loss: 0.03340713679790497, acc: 0.9870129823684692)
[2025-02-13 02:51:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:21][root][INFO] - Training Epoch: 1/2, step 2843/7134 completed (loss: 0.034233588725328445, acc: 0.9926470518112183)
[2025-02-13 02:51:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:22][root][INFO] - Training Epoch: 1/2, step 2844/7134 completed (loss: 0.12952682375907898, acc: 0.9699042439460754)
[2025-02-13 02:51:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:22][root][INFO] - Training Epoch: 1/2, step 2845/7134 completed (loss: 0.042717769742012024, acc: 0.984402060508728)
[2025-02-13 02:51:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:23][root][INFO] - Training Epoch: 1/2, step 2846/7134 completed (loss: 0.08069469779729843, acc: 0.9785832166671753)
[2025-02-13 02:51:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:23][root][INFO] - Training Epoch: 1/2, step 2847/7134 completed (loss: 0.06904858350753784, acc: 0.9868035316467285)
[2025-02-13 02:51:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:23][root][INFO] - Training Epoch: 1/2, step 2848/7134 completed (loss: 0.0710444450378418, acc: 0.9766899943351746)
[2025-02-13 02:51:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:24][root][INFO] - Training Epoch: 1/2, step 2849/7134 completed (loss: 0.05327710509300232, acc: 0.9861687421798706)
[2025-02-13 02:51:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:24][root][INFO] - Training Epoch: 1/2, step 2850/7134 completed (loss: 0.04099975526332855, acc: 0.9870967864990234)
[2025-02-13 02:51:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:25][root][INFO] - Training Epoch: 1/2, step 2851/7134 completed (loss: 0.03529883921146393, acc: 0.9871086478233337)
[2025-02-13 02:51:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:25][root][INFO] - Training Epoch: 1/2, step 2852/7134 completed (loss: 0.027961362153291702, acc: 0.9900596141815186)
[2025-02-13 02:51:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:25][root][INFO] - Training Epoch: 1/2, step 2853/7134 completed (loss: 0.03946826234459877, acc: 0.989847719669342)
[2025-02-13 02:51:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:26][root][INFO] - Training Epoch: 1/2, step 2854/7134 completed (loss: 0.05273110046982765, acc: 0.9851411581039429)
[2025-02-13 02:51:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:26][root][INFO] - Training Epoch: 1/2, step 2855/7134 completed (loss: 0.04228712618350983, acc: 0.993537962436676)
[2025-02-13 02:51:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:27][root][INFO] - Training Epoch: 1/2, step 2856/7134 completed (loss: 0.012303031049668789, acc: 0.9971791505813599)
[2025-02-13 02:51:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:27][root][INFO] - Training Epoch: 1/2, step 2857/7134 completed (loss: 0.057119254022836685, acc: 0.9841040372848511)
[2025-02-13 02:51:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:28][root][INFO] - Training Epoch: 1/2, step 2858/7134 completed (loss: 0.012865752913057804, acc: 0.9984227418899536)
[2025-02-13 02:51:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:28][root][INFO] - Training Epoch: 1/2, step 2859/7134 completed (loss: 0.07562758028507233, acc: 0.9721059799194336)
[2025-02-13 02:51:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:28][root][INFO] - Training Epoch: 1/2, step 2860/7134 completed (loss: 0.049135029315948486, acc: 0.9779816269874573)
[2025-02-13 02:51:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:29][root][INFO] - Training Epoch: 1/2, step 2861/7134 completed (loss: 0.017167838290333748, acc: 0.9959431886672974)
[2025-02-13 02:51:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:29][root][INFO] - Training Epoch: 1/2, step 2862/7134 completed (loss: 0.021569930016994476, acc: 0.9965694546699524)
[2025-02-13 02:51:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:30][root][INFO] - Training Epoch: 1/2, step 2863/7134 completed (loss: 0.015788910910487175, acc: 0.9949044585227966)
[2025-02-13 02:51:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:30][root][INFO] - Training Epoch: 1/2, step 2864/7134 completed (loss: 0.02679312601685524, acc: 0.9922077655792236)
[2025-02-13 02:51:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:31][root][INFO] - Training Epoch: 1/2, step 2865/7134 completed (loss: 0.03168463334441185, acc: 0.9934782385826111)
[2025-02-13 02:51:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:31][root][INFO] - Training Epoch: 1/2, step 2866/7134 completed (loss: 0.04927746579051018, acc: 0.9865471124649048)
[2025-02-13 02:51:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:31][root][INFO] - Training Epoch: 1/2, step 2867/7134 completed (loss: 0.03303096815943718, acc: 0.9938650131225586)
[2025-02-13 02:51:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:32][root][INFO] - Training Epoch: 1/2, step 2868/7134 completed (loss: 0.013120261020958424, acc: 0.9943820238113403)
[2025-02-13 02:51:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:32][root][INFO] - Training Epoch: 1/2, step 2869/7134 completed (loss: 0.05414805933833122, acc: 0.9941434860229492)
[2025-02-13 02:51:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:33][root][INFO] - Training Epoch: 1/2, step 2870/7134 completed (loss: 0.0797387957572937, acc: 0.9832285046577454)
[2025-02-13 02:51:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:33][root][INFO] - Training Epoch: 1/2, step 2871/7134 completed (loss: 0.06684157252311707, acc: 0.9859594106674194)
[2025-02-13 02:51:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:33][root][INFO] - Training Epoch: 1/2, step 2872/7134 completed (loss: 0.017395393922924995, acc: 0.99609375)
[2025-02-13 02:51:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:34][root][INFO] - Training Epoch: 1/2, step 2873/7134 completed (loss: 0.01644340716302395, acc: 0.9927007555961609)
[2025-02-13 02:51:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:34][root][INFO] - Training Epoch: 1/2, step 2874/7134 completed (loss: 0.041475504636764526, acc: 0.9875195026397705)
[2025-02-13 02:51:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:35][root][INFO] - Training Epoch: 1/2, step 2875/7134 completed (loss: 0.03351184353232384, acc: 0.9900426864624023)
[2025-02-13 02:51:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:35][root][INFO] - Training Epoch: 1/2, step 2876/7134 completed (loss: 0.12399304658174515, acc: 0.9723502397537231)
[2025-02-13 02:51:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:36][root][INFO] - Training Epoch: 1/2, step 2877/7134 completed (loss: 0.02428724244236946, acc: 0.9890829920768738)
[2025-02-13 02:51:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:36][root][INFO] - Training Epoch: 1/2, step 2878/7134 completed (loss: 0.031156696379184723, acc: 0.9978813529014587)
[2025-02-13 02:51:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:36][root][INFO] - Training Epoch: 1/2, step 2879/7134 completed (loss: 0.017074860632419586, acc: 0.9968652129173279)
[2025-02-13 02:51:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:37][root][INFO] - Training Epoch: 1/2, step 2880/7134 completed (loss: 0.018408693373203278, acc: 0.9921875)
[2025-02-13 02:51:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:37][root][INFO] - Training Epoch: 1/2, step 2881/7134 completed (loss: 0.0453203059732914, acc: 0.9838998317718506)
[2025-02-13 02:51:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:38][root][INFO] - Training Epoch: 1/2, step 2882/7134 completed (loss: 0.07614961266517639, acc: 0.9790382385253906)
[2025-02-13 02:51:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:38][root][INFO] - Training Epoch: 1/2, step 2883/7134 completed (loss: 0.039897751063108444, acc: 0.9854497313499451)
[2025-02-13 02:51:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:38][root][INFO] - Training Epoch: 1/2, step 2884/7134 completed (loss: 0.09364916384220123, acc: 0.9758672714233398)
[2025-02-13 02:51:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:39][root][INFO] - Training Epoch: 1/2, step 2885/7134 completed (loss: 0.06564327329397202, acc: 0.9772382378578186)
[2025-02-13 02:51:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:39][root][INFO] - Training Epoch: 1/2, step 2886/7134 completed (loss: 0.10629747062921524, acc: 0.9731343388557434)
[2025-02-13 02:51:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:40][root][INFO] - Training Epoch: 1/2, step 2887/7134 completed (loss: 0.047968536615371704, acc: 0.9893491268157959)
[2025-02-13 02:51:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:40][root][INFO] - Training Epoch: 1/2, step 2888/7134 completed (loss: 0.07391694188117981, acc: 0.9805825352668762)
[2025-02-13 02:51:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:41][root][INFO] - Training Epoch: 1/2, step 2889/7134 completed (loss: 0.025571685284376144, acc: 0.9916897416114807)
[2025-02-13 02:51:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:41][root][INFO] - Training Epoch: 1/2, step 2890/7134 completed (loss: 0.030212247744202614, acc: 0.9933422207832336)
[2025-02-13 02:51:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:41][root][INFO] - Training Epoch: 1/2, step 2891/7134 completed (loss: 0.059401173144578934, acc: 0.9830769300460815)
[2025-02-13 02:51:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:42][root][INFO] - Training Epoch: 1/2, step 2892/7134 completed (loss: 0.08060067892074585, acc: 0.9798488616943359)
[2025-02-13 02:51:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:42][root][INFO] - Training Epoch: 1/2, step 2893/7134 completed (loss: 0.06646367162466049, acc: 0.9798234701156616)
[2025-02-13 02:51:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:43][root][INFO] - Training Epoch: 1/2, step 2894/7134 completed (loss: 0.05051026493310928, acc: 0.9831932783126831)
[2025-02-13 02:51:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:43][root][INFO] - Training Epoch: 1/2, step 2895/7134 completed (loss: 0.02572629600763321, acc: 0.992438554763794)
[2025-02-13 02:51:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:44][root][INFO] - Training Epoch: 1/2, step 2896/7134 completed (loss: 0.027278469875454903, acc: 0.991094172000885)
[2025-02-13 02:51:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:44][root][INFO] - Training Epoch: 1/2, step 2897/7134 completed (loss: 0.05252093821763992, acc: 0.9822379946708679)
[2025-02-13 02:51:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:44][root][INFO] - Training Epoch: 1/2, step 2898/7134 completed (loss: 0.03581808879971504, acc: 0.9934640526771545)
[2025-02-13 02:51:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:45][root][INFO] - Training Epoch: 1/2, step 2899/7134 completed (loss: 0.048011086881160736, acc: 0.9898256063461304)
[2025-02-13 02:51:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:45][root][INFO] - Training Epoch: 1/2, step 2900/7134 completed (loss: 0.05101890489459038, acc: 0.990123450756073)
[2025-02-13 02:51:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:46][root][INFO] - Training Epoch: 1/2, step 2901/7134 completed (loss: 0.036667123436927795, acc: 0.9888535141944885)
[2025-02-13 02:51:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:46][root][INFO] - Training Epoch: 1/2, step 2902/7134 completed (loss: 0.050999294966459274, acc: 0.9849624037742615)
[2025-02-13 02:51:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:47][root][INFO] - Training Epoch: 1/2, step 2903/7134 completed (loss: 0.06444963812828064, acc: 0.9797570705413818)
[2025-02-13 02:51:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:47][root][INFO] - Training Epoch: 1/2, step 2904/7134 completed (loss: 0.024299973621964455, acc: 0.9925093650817871)
[2025-02-13 02:51:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:48][root][INFO] - Training Epoch: 1/2, step 2905/7134 completed (loss: 0.04606388881802559, acc: 0.9849812388420105)
[2025-02-13 02:51:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:48][root][INFO] - Training Epoch: 1/2, step 2906/7134 completed (loss: 0.014520172029733658, acc: 0.9976047873497009)
[2025-02-13 02:51:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:48][root][INFO] - Training Epoch: 1/2, step 2907/7134 completed (loss: 0.0540885366499424, acc: 0.9865410327911377)
[2025-02-13 02:51:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:49][root][INFO] - Training Epoch: 1/2, step 2908/7134 completed (loss: 0.025637555867433548, acc: 0.9948253631591797)
[2025-02-13 02:51:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:49][root][INFO] - Training Epoch: 1/2, step 2909/7134 completed (loss: 0.03678265959024429, acc: 0.9856114983558655)
[2025-02-13 02:51:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:50][root][INFO] - Training Epoch: 1/2, step 2910/7134 completed (loss: 0.08645310252904892, acc: 0.980728030204773)
[2025-02-13 02:51:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:50][root][INFO] - Training Epoch: 1/2, step 2911/7134 completed (loss: 0.027278009802103043, acc: 0.991946280002594)
[2025-02-13 02:51:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:51][root][INFO] - Training Epoch: 1/2, step 2912/7134 completed (loss: 0.03958386555314064, acc: 0.9889655113220215)
[2025-02-13 02:51:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:51][root][INFO] - Training Epoch: 1/2, step 2913/7134 completed (loss: 0.044068872928619385, acc: 0.9843304753303528)
[2025-02-13 02:51:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:52][root][INFO] - Training Epoch: 1/2, step 2914/7134 completed (loss: 0.0429120697081089, acc: 0.9879372715950012)
[2025-02-13 02:51:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:52][root][INFO] - Training Epoch: 1/2, step 2915/7134 completed (loss: 0.09946995973587036, acc: 0.9769452214241028)
[2025-02-13 02:51:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:52][root][INFO] - Training Epoch: 1/2, step 2916/7134 completed (loss: 0.06683533638715744, acc: 0.9896907210350037)
[2025-02-13 02:51:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:53][root][INFO] - Training Epoch: 1/2, step 2917/7134 completed (loss: 0.042128775268793106, acc: 0.9899497628211975)
[2025-02-13 02:51:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:53][root][INFO] - Training Epoch: 1/2, step 2918/7134 completed (loss: 0.03193243592977524, acc: 0.9918414950370789)
[2025-02-13 02:51:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:54][root][INFO] - Training Epoch: 1/2, step 2919/7134 completed (loss: 0.06519263237714767, acc: 0.9823736548423767)
[2025-02-13 02:51:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:54][root][INFO] - Training Epoch: 1/2, step 2920/7134 completed (loss: 0.04178469255566597, acc: 0.9880794882774353)
[2025-02-13 02:51:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:55][root][INFO] - Training Epoch: 1/2, step 2921/7134 completed (loss: 0.028607124462723732, acc: 0.9913793206214905)
[2025-02-13 02:51:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:55][root][INFO] - Training Epoch: 1/2, step 2922/7134 completed (loss: 0.05426841601729393, acc: 0.9849246144294739)
[2025-02-13 02:51:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:56][root][INFO] - Training Epoch: 1/2, step 2923/7134 completed (loss: 0.1770850569009781, acc: 0.9677419066429138)
[2025-02-13 02:51:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:56][root][INFO] - Training Epoch: 1/2, step 2924/7134 completed (loss: 0.02626694366335869, acc: 0.9929245114326477)
[2025-02-13 02:51:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:56][root][INFO] - Training Epoch: 1/2, step 2925/7134 completed (loss: 0.05664260312914848, acc: 0.9840294718742371)
[2025-02-13 02:51:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:57][root][INFO] - Training Epoch: 1/2, step 2926/7134 completed (loss: 0.15256312489509583, acc: 0.973557710647583)
[2025-02-13 02:51:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:57][root][INFO] - Training Epoch: 1/2, step 2927/7134 completed (loss: 0.030056647956371307, acc: 0.990326464176178)
[2025-02-13 02:51:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:58][root][INFO] - Training Epoch: 1/2, step 2928/7134 completed (loss: 0.03649386391043663, acc: 0.9847561120986938)
[2025-02-13 02:51:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:58][root][INFO] - Training Epoch: 1/2, step 2929/7134 completed (loss: 0.045644521713256836, acc: 0.9895561337471008)
[2025-02-13 02:51:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:59][root][INFO] - Training Epoch: 1/2, step 2930/7134 completed (loss: 0.03304417058825493, acc: 0.988252580165863)
[2025-02-13 02:51:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:51:59][root][INFO] - Training Epoch: 1/2, step 2931/7134 completed (loss: 0.06796400994062424, acc: 0.9775784611701965)
[2025-02-13 02:51:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:00][root][INFO] - Training Epoch: 1/2, step 2932/7134 completed (loss: 0.040964383631944656, acc: 0.9895712733268738)
[2025-02-13 02:52:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:00][root][INFO] - Training Epoch: 1/2, step 2933/7134 completed (loss: 0.025404347106814384, acc: 0.9913420081138611)
[2025-02-13 02:52:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:01][root][INFO] - Training Epoch: 1/2, step 2934/7134 completed (loss: 0.02570800855755806, acc: 0.9929478168487549)
[2025-02-13 02:52:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:01][root][INFO] - Training Epoch: 1/2, step 2935/7134 completed (loss: 0.035899631679058075, acc: 0.992290735244751)
[2025-02-13 02:52:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:01][root][INFO] - Training Epoch: 1/2, step 2936/7134 completed (loss: 0.05398106575012207, acc: 0.9847406148910522)
[2025-02-13 02:52:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:02][root][INFO] - Training Epoch: 1/2, step 2937/7134 completed (loss: 0.0589437372982502, acc: 0.9842180609703064)
[2025-02-13 02:52:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:02][root][INFO] - Training Epoch: 1/2, step 2938/7134 completed (loss: 0.042078860104084015, acc: 0.9888268113136292)
[2025-02-13 02:52:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:03][root][INFO] - Training Epoch: 1/2, step 2939/7134 completed (loss: 0.032193515449762344, acc: 0.9935897588729858)
[2025-02-13 02:52:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:03][root][INFO] - Training Epoch: 1/2, step 2940/7134 completed (loss: 0.07630268484354019, acc: 0.9849520921707153)
[2025-02-13 02:52:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:04][root][INFO] - Training Epoch: 1/2, step 2941/7134 completed (loss: 0.04136405512690544, acc: 0.9869375824928284)
[2025-02-13 02:52:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:04][root][INFO] - Training Epoch: 1/2, step 2942/7134 completed (loss: 0.04600820317864418, acc: 0.98959881067276)
[2025-02-13 02:52:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:04][root][INFO] - Training Epoch: 1/2, step 2943/7134 completed (loss: 0.024834850803017616, acc: 0.9932546615600586)
[2025-02-13 02:52:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:05][root][INFO] - Training Epoch: 1/2, step 2944/7134 completed (loss: 0.06200240179896355, acc: 0.98531574010849)
[2025-02-13 02:52:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:05][root][INFO] - Training Epoch: 1/2, step 2945/7134 completed (loss: 0.030168574303388596, acc: 0.9891135096549988)
[2025-02-13 02:52:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:06][root][INFO] - Training Epoch: 1/2, step 2946/7134 completed (loss: 0.03065800666809082, acc: 0.992732584476471)
[2025-02-13 02:52:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:06][root][INFO] - Training Epoch: 1/2, step 2947/7134 completed (loss: 0.046941451728343964, acc: 0.9827044010162354)
[2025-02-13 02:52:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:07][root][INFO] - Training Epoch: 1/2, step 2948/7134 completed (loss: 0.05192120745778084, acc: 0.9818781018257141)
[2025-02-13 02:52:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:07][root][INFO] - Training Epoch: 1/2, step 2949/7134 completed (loss: 0.008510362356901169, acc: 0.9984709620475769)
[2025-02-13 02:52:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:08][root][INFO] - Training Epoch: 1/2, step 2950/7134 completed (loss: 0.024685358628630638, acc: 0.9917241334915161)
[2025-02-13 02:52:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:08][root][INFO] - Training Epoch: 1/2, step 2951/7134 completed (loss: 0.013204873539507389, acc: 0.9971014261245728)
[2025-02-13 02:52:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:08][root][INFO] - Training Epoch: 1/2, step 2952/7134 completed (loss: 0.02206525206565857, acc: 0.9925816059112549)
[2025-02-13 02:52:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:09][root][INFO] - Training Epoch: 1/2, step 2953/7134 completed (loss: 0.03531991317868233, acc: 0.9864864945411682)
[2025-02-13 02:52:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:09][root][INFO] - Training Epoch: 1/2, step 2954/7134 completed (loss: 0.03585854917764664, acc: 0.9897040128707886)
[2025-02-13 02:52:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:10][root][INFO] - Training Epoch: 1/2, step 2955/7134 completed (loss: 0.030825531110167503, acc: 0.9928977489471436)
[2025-02-13 02:52:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:10][root][INFO] - Training Epoch: 1/2, step 2956/7134 completed (loss: 0.021962041035294533, acc: 0.9956076145172119)
[2025-02-13 02:52:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:10][root][INFO] - Training Epoch: 1/2, step 2957/7134 completed (loss: 0.025603411719202995, acc: 0.990641713142395)
[2025-02-13 02:52:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:11][root][INFO] - Training Epoch: 1/2, step 2958/7134 completed (loss: 0.027985436841845512, acc: 0.9914529919624329)
[2025-02-13 02:52:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:11][root][INFO] - Training Epoch: 1/2, step 2959/7134 completed (loss: 0.019895877689123154, acc: 0.9926900863647461)
[2025-02-13 02:52:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:12][root][INFO] - Training Epoch: 1/2, step 2960/7134 completed (loss: 0.023115556687116623, acc: 0.9917355179786682)
[2025-02-13 02:52:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:12][root][INFO] - Training Epoch: 1/2, step 2961/7134 completed (loss: 0.028308730572462082, acc: 0.9891156554222107)
[2025-02-13 02:52:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:13][root][INFO] - Training Epoch: 1/2, step 2962/7134 completed (loss: 0.02532576210796833, acc: 0.9909443855285645)
[2025-02-13 02:52:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:13][root][INFO] - Training Epoch: 1/2, step 2963/7134 completed (loss: 0.02276594191789627, acc: 0.9921466112136841)
[2025-02-13 02:52:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:14][root][INFO] - Training Epoch: 1/2, step 2964/7134 completed (loss: 0.02547735534608364, acc: 0.9912152290344238)
[2025-02-13 02:52:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:14][root][INFO] - Training Epoch: 1/2, step 2965/7134 completed (loss: 0.0589480847120285, acc: 0.9850746393203735)
[2025-02-13 02:52:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:14][root][INFO] - Training Epoch: 1/2, step 2966/7134 completed (loss: 0.01260099746286869, acc: 0.9957355856895447)
[2025-02-13 02:52:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:15][root][INFO] - Training Epoch: 1/2, step 2967/7134 completed (loss: 0.0432417169213295, acc: 0.9874301552772522)
[2025-02-13 02:52:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:15][root][INFO] - Training Epoch: 1/2, step 2968/7134 completed (loss: 0.15633971989154816, acc: 0.9630225300788879)
[2025-02-13 02:52:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:16][root][INFO] - Training Epoch: 1/2, step 2969/7134 completed (loss: 0.2734265923500061, acc: 0.9408283829689026)
[2025-02-13 02:52:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:16][root][INFO] - Training Epoch: 1/2, step 2970/7134 completed (loss: 0.3584642708301544, acc: 0.93028324842453)
[2025-02-13 02:52:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:17][root][INFO] - Training Epoch: 1/2, step 2971/7134 completed (loss: 0.06397709250450134, acc: 0.9842312932014465)
[2025-02-13 02:52:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:17][root][INFO] - Training Epoch: 1/2, step 2972/7134 completed (loss: 0.043351463973522186, acc: 0.9866468906402588)
[2025-02-13 02:52:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:17][root][INFO] - Training Epoch: 1/2, step 2973/7134 completed (loss: 0.0750790685415268, acc: 0.9784809947013855)
[2025-02-13 02:52:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:18][root][INFO] - Training Epoch: 1/2, step 2974/7134 completed (loss: 0.05740104988217354, acc: 0.9831387996673584)
[2025-02-13 02:52:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:18][root][INFO] - Training Epoch: 1/2, step 2975/7134 completed (loss: 0.059021543711423874, acc: 0.9834254384040833)
[2025-02-13 02:52:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:19][root][INFO] - Training Epoch: 1/2, step 2976/7134 completed (loss: 0.053565338253974915, acc: 0.9872340559959412)
[2025-02-13 02:52:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:19][root][INFO] - Training Epoch: 1/2, step 2977/7134 completed (loss: 0.05547238886356354, acc: 0.9884393215179443)
[2025-02-13 02:52:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:20][root][INFO] - Training Epoch: 1/2, step 2978/7134 completed (loss: 0.05531065911054611, acc: 0.9872340559959412)
[2025-02-13 02:52:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:20][root][INFO] - Training Epoch: 1/2, step 2979/7134 completed (loss: 0.020209750160574913, acc: 0.9953271150588989)
[2025-02-13 02:52:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:21][root][INFO] - Training Epoch: 1/2, step 2980/7134 completed (loss: 0.05849766731262207, acc: 0.9859872460365295)
[2025-02-13 02:52:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:21][root][INFO] - Training Epoch: 1/2, step 2981/7134 completed (loss: 0.08987164497375488, acc: 0.9823608994483948)
[2025-02-13 02:52:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:21][root][INFO] - Training Epoch: 1/2, step 2982/7134 completed (loss: 0.06065407022833824, acc: 0.9842932224273682)
[2025-02-13 02:52:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:22][root][INFO] - Training Epoch: 1/2, step 2983/7134 completed (loss: 0.08689581602811813, acc: 0.9801587462425232)
[2025-02-13 02:52:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:22][root][INFO] - Training Epoch: 1/2, step 2984/7134 completed (loss: 0.03388127684593201, acc: 0.99245285987854)
[2025-02-13 02:52:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:23][root][INFO] - Training Epoch: 1/2, step 2985/7134 completed (loss: 0.05709477514028549, acc: 0.9833134412765503)
[2025-02-13 02:52:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:23][root][INFO] - Training Epoch: 1/2, step 2986/7134 completed (loss: 0.030887668952345848, acc: 0.9893617033958435)
[2025-02-13 02:52:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:24][root][INFO] - Training Epoch: 1/2, step 2987/7134 completed (loss: 0.038644518703222275, acc: 0.9941107034683228)
[2025-02-13 02:52:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:24][root][INFO] - Training Epoch: 1/2, step 2988/7134 completed (loss: 0.03447747603058815, acc: 0.9927007555961609)
[2025-02-13 02:52:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:25][root][INFO] - Training Epoch: 1/2, step 2989/7134 completed (loss: 0.04005017131567001, acc: 0.9882352948188782)
[2025-02-13 02:52:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:25][root][INFO] - Training Epoch: 1/2, step 2990/7134 completed (loss: 0.13710494339466095, acc: 0.9667832255363464)
[2025-02-13 02:52:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:25][root][INFO] - Training Epoch: 1/2, step 2991/7134 completed (loss: 0.1510605812072754, acc: 0.962552011013031)
[2025-02-13 02:52:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:26][root][INFO] - Training Epoch: 1/2, step 2992/7134 completed (loss: 0.07874514162540436, acc: 0.9834087491035461)
[2025-02-13 02:52:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:26][root][INFO] - Training Epoch: 1/2, step 2993/7134 completed (loss: 0.05055464059114456, acc: 0.9865525960922241)
[2025-02-13 02:52:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:27][root][INFO] - Training Epoch: 1/2, step 2994/7134 completed (loss: 0.028943432494997978, acc: 0.9910072088241577)
[2025-02-13 02:52:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:27][root][INFO] - Training Epoch: 1/2, step 2995/7134 completed (loss: 0.03524764999747276, acc: 0.9920904040336609)
[2025-02-13 02:52:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:28][root][INFO] - Training Epoch: 1/2, step 2996/7134 completed (loss: 0.05044431984424591, acc: 0.9871345162391663)
[2025-02-13 02:52:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:28][root][INFO] - Training Epoch: 1/2, step 2997/7134 completed (loss: 0.021390371024608612, acc: 0.9958419799804688)
[2025-02-13 02:52:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:29][root][INFO] - Training Epoch: 1/2, step 2998/7134 completed (loss: 0.04321691393852234, acc: 0.9890909194946289)
[2025-02-13 02:52:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:29][root][INFO] - Training Epoch: 1/2, step 2999/7134 completed (loss: 0.02890446037054062, acc: 0.9956584572792053)
[2025-02-13 02:52:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:30][root][INFO] - Training Epoch: 1/2, step 3000/7134 completed (loss: 0.041473228484392166, acc: 0.9903640151023865)
[2025-02-13 02:52:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:30][root][INFO] - Training Epoch: 1/2, step 3001/7134 completed (loss: 0.03751005977392197, acc: 0.9857819676399231)
[2025-02-13 02:52:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:31][root][INFO] - Training Epoch: 1/2, step 3002/7134 completed (loss: 0.03900422155857086, acc: 0.9852607846260071)
[2025-02-13 02:52:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:31][root][INFO] - Training Epoch: 1/2, step 3003/7134 completed (loss: 0.020106520503759384, acc: 0.9930394291877747)
[2025-02-13 02:52:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:31][root][INFO] - Training Epoch: 1/2, step 3004/7134 completed (loss: 0.016384253278374672, acc: 0.9932960867881775)
[2025-02-13 02:52:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:32][root][INFO] - Training Epoch: 1/2, step 3005/7134 completed (loss: 0.015002946369349957, acc: 0.9977195262908936)
[2025-02-13 02:52:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:32][root][INFO] - Training Epoch: 1/2, step 3006/7134 completed (loss: 0.018929092213511467, acc: 0.9936143159866333)
[2025-02-13 02:52:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:33][root][INFO] - Training Epoch: 1/2, step 3007/7134 completed (loss: 0.03553682565689087, acc: 0.989461362361908)
[2025-02-13 02:52:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:33][root][INFO] - Training Epoch: 1/2, step 3008/7134 completed (loss: 0.06284740567207336, acc: 0.9818181991577148)
[2025-02-13 02:52:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:34][root][INFO] - Training Epoch: 1/2, step 3009/7134 completed (loss: 0.08234885334968567, acc: 0.9772151708602905)
[2025-02-13 02:52:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:34][root][INFO] - Training Epoch: 1/2, step 3010/7134 completed (loss: 0.05938323587179184, acc: 0.9867403507232666)
[2025-02-13 02:52:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:35][root][INFO] - Training Epoch: 1/2, step 3011/7134 completed (loss: 0.11543835699558258, acc: 0.9657632112503052)
[2025-02-13 02:52:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:35][root][INFO] - Training Epoch: 1/2, step 3012/7134 completed (loss: 0.06919413805007935, acc: 0.9730586409568787)
[2025-02-13 02:52:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:36][root][INFO] - Training Epoch: 1/2, step 3013/7134 completed (loss: 0.01997442916035652, acc: 0.9919354915618896)
[2025-02-13 02:52:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:36][root][INFO] - Training Epoch: 1/2, step 3014/7134 completed (loss: 0.03843967244029045, acc: 0.9871043562889099)
[2025-02-13 02:52:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:36][root][INFO] - Training Epoch: 1/2, step 3015/7134 completed (loss: 0.030167847871780396, acc: 0.9876033067703247)
[2025-02-13 02:52:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:37][root][INFO] - Training Epoch: 1/2, step 3016/7134 completed (loss: 0.020751414820551872, acc: 0.9915611743927002)
[2025-02-13 02:52:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:37][root][INFO] - Training Epoch: 1/2, step 3017/7134 completed (loss: 0.057152848690748215, acc: 0.9813559055328369)
[2025-02-13 02:52:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:38][root][INFO] - Training Epoch: 1/2, step 3018/7134 completed (loss: 0.03634120523929596, acc: 0.9933884143829346)
[2025-02-13 02:52:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:38][root][INFO] - Training Epoch: 1/2, step 3019/7134 completed (loss: 0.03928174078464508, acc: 0.9885621070861816)
[2025-02-13 02:52:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:39][root][INFO] - Training Epoch: 1/2, step 3020/7134 completed (loss: 0.04955582693219185, acc: 0.9857397675514221)
[2025-02-13 02:52:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:39][root][INFO] - Training Epoch: 1/2, step 3021/7134 completed (loss: 0.021249888464808464, acc: 0.9945454597473145)
[2025-02-13 02:52:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:39][root][INFO] - Training Epoch: 1/2, step 3022/7134 completed (loss: 0.047330789268016815, acc: 0.9871382713317871)
[2025-02-13 02:52:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:40][root][INFO] - Training Epoch: 1/2, step 3023/7134 completed (loss: 0.05633671581745148, acc: 0.9801192879676819)
[2025-02-13 02:52:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:40][root][INFO] - Training Epoch: 1/2, step 3024/7134 completed (loss: 0.018355973064899445, acc: 0.992337167263031)
[2025-02-13 02:52:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:41][root][INFO] - Training Epoch: 1/2, step 3025/7134 completed (loss: 0.07079203426837921, acc: 0.9810344576835632)
[2025-02-13 02:52:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:41][root][INFO] - Training Epoch: 1/2, step 3026/7134 completed (loss: 0.03946791961789131, acc: 0.9892280101776123)
[2025-02-13 02:52:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:42][root][INFO] - Training Epoch: 1/2, step 3027/7134 completed (loss: 0.05744904652237892, acc: 0.9898989796638489)
[2025-02-13 02:52:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:42][root][INFO] - Training Epoch: 1/2, step 3028/7134 completed (loss: 0.0331694595515728, acc: 0.9909297227859497)
[2025-02-13 02:52:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:42][root][INFO] - Training Epoch: 1/2, step 3029/7134 completed (loss: 0.04512692987918854, acc: 0.9907264113426208)
[2025-02-13 02:52:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:43][root][INFO] - Training Epoch: 1/2, step 3030/7134 completed (loss: 0.04228799417614937, acc: 0.9923664331436157)
[2025-02-13 02:52:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:43][root][INFO] - Training Epoch: 1/2, step 3031/7134 completed (loss: 0.01808765158057213, acc: 0.9936440587043762)
[2025-02-13 02:52:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:44][root][INFO] - Training Epoch: 1/2, step 3032/7134 completed (loss: 0.0588570199906826, acc: 0.9891107082366943)
[2025-02-13 02:52:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:44][root][INFO] - Training Epoch: 1/2, step 3033/7134 completed (loss: 0.032306548207998276, acc: 0.9876543283462524)
[2025-02-13 02:52:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:45][root][INFO] - Training Epoch: 1/2, step 3034/7134 completed (loss: 0.0478971041738987, acc: 0.9872340559959412)
[2025-02-13 02:52:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:45][root][INFO] - Training Epoch: 1/2, step 3035/7134 completed (loss: 0.03086252324283123, acc: 0.9905511736869812)
[2025-02-13 02:52:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:45][root][INFO] - Training Epoch: 1/2, step 3036/7134 completed (loss: 0.07561643421649933, acc: 0.9832776188850403)
[2025-02-13 02:52:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:46][root][INFO] - Training Epoch: 1/2, step 3037/7134 completed (loss: 0.03709488734602928, acc: 0.9892141819000244)
[2025-02-13 02:52:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:46][root][INFO] - Training Epoch: 1/2, step 3038/7134 completed (loss: 0.022352084517478943, acc: 0.9947826266288757)
[2025-02-13 02:52:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:47][root][INFO] - Training Epoch: 1/2, step 3039/7134 completed (loss: 0.03771268576383591, acc: 0.992343008518219)
[2025-02-13 02:52:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:47][root][INFO] - Training Epoch: 1/2, step 3040/7134 completed (loss: 0.04691215232014656, acc: 0.9870129823684692)
[2025-02-13 02:52:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:48][root][INFO] - Training Epoch: 1/2, step 3041/7134 completed (loss: 0.07976868003606796, acc: 0.9856459498405457)
[2025-02-13 02:52:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:48][root][INFO] - Training Epoch: 1/2, step 3042/7134 completed (loss: 0.03389979898929596, acc: 0.9940915703773499)
[2025-02-13 02:52:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:48][root][INFO] - Training Epoch: 1/2, step 3043/7134 completed (loss: 0.020099233835935593, acc: 0.9923954606056213)
[2025-02-13 02:52:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:49][root][INFO] - Training Epoch: 1/2, step 3044/7134 completed (loss: 0.031254060566425323, acc: 0.9913644194602966)
[2025-02-13 02:52:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:49][root][INFO] - Training Epoch: 1/2, step 3045/7134 completed (loss: 0.02137620560824871, acc: 0.993220329284668)
[2025-02-13 02:52:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:50][root][INFO] - Training Epoch: 1/2, step 3046/7134 completed (loss: 0.04765744507312775, acc: 0.9890710115432739)
[2025-02-13 02:52:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:50][root][INFO] - Training Epoch: 1/2, step 3047/7134 completed (loss: 0.011835956014692783, acc: 0.99622642993927)
[2025-02-13 02:52:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:51][root][INFO] - Training Epoch: 1/2, step 3048/7134 completed (loss: 0.012134088203310966, acc: 0.996221661567688)
[2025-02-13 02:52:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:51][root][INFO] - Training Epoch: 1/2, step 3049/7134 completed (loss: 0.024168385192751884, acc: 0.992732584476471)
[2025-02-13 02:52:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:51][root][INFO] - Training Epoch: 1/2, step 3050/7134 completed (loss: 0.03769239783287048, acc: 0.9881235361099243)
[2025-02-13 02:52:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:52][root][INFO] - Training Epoch: 1/2, step 3051/7134 completed (loss: 0.041514769196510315, acc: 0.9894737005233765)
[2025-02-13 02:52:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:52][root][INFO] - Training Epoch: 1/2, step 3052/7134 completed (loss: 0.034433603286743164, acc: 0.9896907210350037)
[2025-02-13 02:52:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:53][root][INFO] - Training Epoch: 1/2, step 3053/7134 completed (loss: 0.029229044914245605, acc: 0.9915459156036377)
[2025-02-13 02:52:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:53][root][INFO] - Training Epoch: 1/2, step 3054/7134 completed (loss: 0.0489511713385582, acc: 0.9883117079734802)
[2025-02-13 02:52:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:54][root][INFO] - Training Epoch: 1/2, step 3055/7134 completed (loss: 0.047375600785017014, acc: 0.9863247871398926)
[2025-02-13 02:52:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:54][root][INFO] - Training Epoch: 1/2, step 3056/7134 completed (loss: 0.02167625166475773, acc: 0.9948586225509644)
[2025-02-13 02:52:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:55][root][INFO] - Training Epoch: 1/2, step 3057/7134 completed (loss: 0.020952438935637474, acc: 0.9902912378311157)
[2025-02-13 02:52:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:55][root][INFO] - Training Epoch: 1/2, step 3058/7134 completed (loss: 0.06810256838798523, acc: 0.9766839146614075)
[2025-02-13 02:52:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:56][root][INFO] - Training Epoch: 1/2, step 3059/7134 completed (loss: 0.03252194821834564, acc: 0.9904109835624695)
[2025-02-13 02:52:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:56][root][INFO] - Training Epoch: 1/2, step 3060/7134 completed (loss: 0.02494613081216812, acc: 0.9916467666625977)
[2025-02-13 02:52:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:56][root][INFO] - Training Epoch: 1/2, step 3061/7134 completed (loss: 0.019633861258625984, acc: 0.9939393997192383)
[2025-02-13 02:52:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:57][root][INFO] - Training Epoch: 1/2, step 3062/7134 completed (loss: 0.011190320365130901, acc: 0.9973045587539673)
[2025-02-13 02:52:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:57][root][INFO] - Training Epoch: 1/2, step 3063/7134 completed (loss: 0.027759121730923653, acc: 0.9910141229629517)
[2025-02-13 02:52:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:58][root][INFO] - Training Epoch: 1/2, step 3064/7134 completed (loss: 0.016946008428931236, acc: 0.9943661689758301)
[2025-02-13 02:52:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:58][root][INFO] - Training Epoch: 1/2, step 3065/7134 completed (loss: 0.02303071692585945, acc: 0.9932795763015747)
[2025-02-13 02:52:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:59][root][INFO] - Training Epoch: 1/2, step 3066/7134 completed (loss: 0.014849881641566753, acc: 0.9967741966247559)
[2025-02-13 02:52:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:52:59][root][INFO] - Training Epoch: 1/2, step 3067/7134 completed (loss: 0.024745376780629158, acc: 0.9927272796630859)
[2025-02-13 02:52:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:00][root][INFO] - Training Epoch: 1/2, step 3068/7134 completed (loss: 0.04349854961037636, acc: 0.9837618470191956)
[2025-02-13 02:53:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:00][root][INFO] - Training Epoch: 1/2, step 3069/7134 completed (loss: 0.023264115676283836, acc: 0.9930875301361084)
[2025-02-13 02:53:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:00][root][INFO] - Training Epoch: 1/2, step 3070/7134 completed (loss: 0.018061788752675056, acc: 0.9936373233795166)
[2025-02-13 02:53:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:01][root][INFO] - Training Epoch: 1/2, step 3071/7134 completed (loss: 0.03635338693857193, acc: 0.992277979850769)
[2025-02-13 02:53:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:01][root][INFO] - Training Epoch: 1/2, step 3072/7134 completed (loss: 0.022640768438577652, acc: 0.9941657185554504)
[2025-02-13 02:53:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:02][root][INFO] - Training Epoch: 1/2, step 3073/7134 completed (loss: 0.011638317257165909, acc: 0.9961832165718079)
[2025-02-13 02:53:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:02][root][INFO] - Training Epoch: 1/2, step 3074/7134 completed (loss: 0.02346496284008026, acc: 0.9933444261550903)
[2025-02-13 02:53:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:03][root][INFO] - Training Epoch: 1/2, step 3075/7134 completed (loss: 0.012503099627792835, acc: 0.9986110925674438)
[2025-02-13 02:53:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:03][root][INFO] - Training Epoch: 1/2, step 3076/7134 completed (loss: 0.034953873604536057, acc: 0.9866814613342285)
[2025-02-13 02:53:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:04][root][INFO] - Training Epoch: 1/2, step 3077/7134 completed (loss: 0.09090033173561096, acc: 0.9781420826911926)
[2025-02-13 02:53:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:04][root][INFO] - Training Epoch: 1/2, step 3078/7134 completed (loss: 0.06759322434663773, acc: 0.9759562611579895)
[2025-02-13 02:53:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:05][root][INFO] - Training Epoch: 1/2, step 3079/7134 completed (loss: 0.038665227591991425, acc: 0.990123450756073)
[2025-02-13 02:53:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:05][root][INFO] - Training Epoch: 1/2, step 3080/7134 completed (loss: 0.05892586335539818, acc: 0.9803707599639893)
[2025-02-13 02:53:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:05][root][INFO] - Training Epoch: 1/2, step 3081/7134 completed (loss: 0.012009159661829472, acc: 0.9978448152542114)
[2025-02-13 02:53:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:06][root][INFO] - Training Epoch: 1/2, step 3082/7134 completed (loss: 0.06204180046916008, acc: 0.9807460904121399)
[2025-02-13 02:53:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:06][root][INFO] - Training Epoch: 1/2, step 3083/7134 completed (loss: 0.07129180431365967, acc: 0.9751309156417847)
[2025-02-13 02:53:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:07][root][INFO] - Training Epoch: 1/2, step 3084/7134 completed (loss: 0.06396744400262833, acc: 0.9821428656578064)
[2025-02-13 02:53:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:07][root][INFO] - Training Epoch: 1/2, step 3085/7134 completed (loss: 0.08110848069190979, acc: 0.9766423106193542)
[2025-02-13 02:53:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:08][root][INFO] - Training Epoch: 1/2, step 3086/7134 completed (loss: 0.06786545366048813, acc: 0.9797979593276978)
[2025-02-13 02:53:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:08][root][INFO] - Training Epoch: 1/2, step 3087/7134 completed (loss: 0.04974848031997681, acc: 0.9856528043746948)
[2025-02-13 02:53:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:09][root][INFO] - Training Epoch: 1/2, step 3088/7134 completed (loss: 0.048841651529073715, acc: 0.9842164516448975)
[2025-02-13 02:53:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:09][root][INFO] - Training Epoch: 1/2, step 3089/7134 completed (loss: 0.04010418802499771, acc: 0.983818769454956)
[2025-02-13 02:53:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:10][root][INFO] - Training Epoch: 1/2, step 3090/7134 completed (loss: 0.0860566645860672, acc: 0.9755434989929199)
[2025-02-13 02:53:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:10][root][INFO] - Training Epoch: 1/2, step 3091/7134 completed (loss: 0.05065256729722023, acc: 0.9838709831237793)
[2025-02-13 02:53:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:10][root][INFO] - Training Epoch: 1/2, step 3092/7134 completed (loss: 0.07601507753133774, acc: 0.975970447063446)
[2025-02-13 02:53:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:11][root][INFO] - Training Epoch: 1/2, step 3093/7134 completed (loss: 0.05857129395008087, acc: 0.9834437370300293)
[2025-02-13 02:53:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:11][root][INFO] - Training Epoch: 1/2, step 3094/7134 completed (loss: 0.029098575934767723, acc: 0.9918256402015686)
[2025-02-13 02:53:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:12][root][INFO] - Training Epoch: 1/2, step 3095/7134 completed (loss: 0.06503952294588089, acc: 0.9833333492279053)
[2025-02-13 02:53:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:12][root][INFO] - Training Epoch: 1/2, step 3096/7134 completed (loss: 0.07186029106378555, acc: 0.9781771302223206)
[2025-02-13 02:53:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:13][root][INFO] - Training Epoch: 1/2, step 3097/7134 completed (loss: 0.08867491781711578, acc: 0.9800570011138916)
[2025-02-13 02:53:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:13][root][INFO] - Training Epoch: 1/2, step 3098/7134 completed (loss: 0.037174370139837265, acc: 0.9874213933944702)
[2025-02-13 02:53:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:14][root][INFO] - Training Epoch: 1/2, step 3099/7134 completed (loss: 0.03992420807480812, acc: 0.9886547923088074)
[2025-02-13 02:53:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:14][root][INFO] - Training Epoch: 1/2, step 3100/7134 completed (loss: 0.041992396116256714, acc: 0.9882978796958923)
[2025-02-13 02:53:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:15][root][INFO] - Training Epoch: 1/2, step 3101/7134 completed (loss: 0.0315183624625206, acc: 0.9878345727920532)
[2025-02-13 02:53:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:15][root][INFO] - Training Epoch: 1/2, step 3102/7134 completed (loss: 0.027995232492685318, acc: 0.9896296262741089)
[2025-02-13 02:53:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:15][root][INFO] - Training Epoch: 1/2, step 3103/7134 completed (loss: 0.04337320476770401, acc: 0.9872204661369324)
[2025-02-13 02:53:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:16][root][INFO] - Training Epoch: 1/2, step 3104/7134 completed (loss: 0.0322774276137352, acc: 0.9885495901107788)
[2025-02-13 02:53:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:16][root][INFO] - Training Epoch: 1/2, step 3105/7134 completed (loss: 0.05528474226593971, acc: 0.9846153855323792)
[2025-02-13 02:53:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:17][root][INFO] - Training Epoch: 1/2, step 3106/7134 completed (loss: 0.03209640085697174, acc: 0.9949832558631897)
[2025-02-13 02:53:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:17][root][INFO] - Training Epoch: 1/2, step 3107/7134 completed (loss: 0.03186425194144249, acc: 0.9908496737480164)
[2025-02-13 02:53:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:18][root][INFO] - Training Epoch: 1/2, step 3108/7134 completed (loss: 0.04361836984753609, acc: 0.989230751991272)
[2025-02-13 02:53:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:18][root][INFO] - Training Epoch: 1/2, step 3109/7134 completed (loss: 0.04264553263783455, acc: 0.990774929523468)
[2025-02-13 02:53:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:19][root][INFO] - Training Epoch: 1/2, step 3110/7134 completed (loss: 0.022889869287610054, acc: 0.992682933807373)
[2025-02-13 02:53:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:19][root][INFO] - Training Epoch: 1/2, step 3111/7134 completed (loss: 0.10886435210704803, acc: 0.975944995880127)
[2025-02-13 02:53:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:19][root][INFO] - Training Epoch: 1/2, step 3112/7134 completed (loss: 0.044851917773485184, acc: 0.9877192974090576)
[2025-02-13 02:53:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:20][root][INFO] - Training Epoch: 1/2, step 3113/7134 completed (loss: 0.028606779873371124, acc: 0.9943820238113403)
[2025-02-13 02:53:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:20][root][INFO] - Training Epoch: 1/2, step 3114/7134 completed (loss: 0.03927246853709221, acc: 0.9844827651977539)
[2025-02-13 02:53:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:21][root][INFO] - Training Epoch: 1/2, step 3115/7134 completed (loss: 0.05771978572010994, acc: 0.9891975522041321)
[2025-02-13 02:53:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:21][root][INFO] - Training Epoch: 1/2, step 3116/7134 completed (loss: 0.038712356239557266, acc: 0.9910256266593933)
[2025-02-13 02:53:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:22][root][INFO] - Training Epoch: 1/2, step 3117/7134 completed (loss: 0.09930706024169922, acc: 0.97826087474823)
[2025-02-13 02:53:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:22][root][INFO] - Training Epoch: 1/2, step 3118/7134 completed (loss: 0.05721624568104744, acc: 0.9894598126411438)
[2025-02-13 02:53:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:22][root][INFO] - Training Epoch: 1/2, step 3119/7134 completed (loss: 0.0876704677939415, acc: 0.9779506921768188)
[2025-02-13 02:53:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:23][root][INFO] - Training Epoch: 1/2, step 3120/7134 completed (loss: 0.05968448519706726, acc: 0.9887164831161499)
[2025-02-13 02:53:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:23][root][INFO] - Training Epoch: 1/2, step 3121/7134 completed (loss: 0.03767471760511398, acc: 0.9897172451019287)
[2025-02-13 02:53:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:24][root][INFO] - Training Epoch: 1/2, step 3122/7134 completed (loss: 0.047455623745918274, acc: 0.9872093200683594)
[2025-02-13 02:53:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:24][root][INFO] - Training Epoch: 1/2, step 3123/7134 completed (loss: 0.058833640068769455, acc: 0.9838308691978455)
[2025-02-13 02:53:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:25][root][INFO] - Training Epoch: 1/2, step 3124/7134 completed (loss: 0.05471792444586754, acc: 0.9888613820075989)
[2025-02-13 02:53:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:25][root][INFO] - Training Epoch: 1/2, step 3125/7134 completed (loss: 0.02015722543001175, acc: 0.9954614043235779)
[2025-02-13 02:53:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:26][root][INFO] - Training Epoch: 1/2, step 3126/7134 completed (loss: 0.04439012333750725, acc: 0.9834815859794617)
[2025-02-13 02:53:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:26][root][INFO] - Training Epoch: 1/2, step 3127/7134 completed (loss: 0.039431776851415634, acc: 0.9860464930534363)
[2025-02-13 02:53:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:26][root][INFO] - Training Epoch: 1/2, step 3128/7134 completed (loss: 0.03544288128614426, acc: 0.9909677505493164)
[2025-02-13 02:53:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:27][root][INFO] - Training Epoch: 1/2, step 3129/7134 completed (loss: 0.03691181540489197, acc: 0.990111231803894)
[2025-02-13 02:53:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:27][root][INFO] - Training Epoch: 1/2, step 3130/7134 completed (loss: 0.026795363053679466, acc: 0.9942660331726074)
[2025-02-13 02:53:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:28][root][INFO] - Training Epoch: 1/2, step 3131/7134 completed (loss: 0.05365777388215065, acc: 0.9797101616859436)
[2025-02-13 02:53:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:28][root][INFO] - Training Epoch: 1/2, step 3132/7134 completed (loss: 0.025931082665920258, acc: 0.9934924244880676)
[2025-02-13 02:53:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:29][root][INFO] - Training Epoch: 1/2, step 3133/7134 completed (loss: 0.09470415860414505, acc: 0.9793956279754639)
[2025-02-13 02:53:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:29][root][INFO] - Training Epoch: 1/2, step 3134/7134 completed (loss: 0.034844689071178436, acc: 0.9890109896659851)
[2025-02-13 02:53:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:30][root][INFO] - Training Epoch: 1/2, step 3135/7134 completed (loss: 0.07891445606946945, acc: 0.9792147874832153)
[2025-02-13 02:53:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:30][root][INFO] - Training Epoch: 1/2, step 3136/7134 completed (loss: 0.02040093205869198, acc: 0.9930434823036194)
[2025-02-13 02:53:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:30][root][INFO] - Training Epoch: 1/2, step 3137/7134 completed (loss: 0.0713551715016365, acc: 0.9742063283920288)
[2025-02-13 02:53:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:31][root][INFO] - Training Epoch: 1/2, step 3138/7134 completed (loss: 0.09397473186254501, acc: 0.9911764860153198)
[2025-02-13 02:53:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:31][root][INFO] - Training Epoch: 1/2, step 3139/7134 completed (loss: 0.10503951460123062, acc: 0.9678111672401428)
[2025-02-13 02:53:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:32][root][INFO] - Training Epoch: 1/2, step 3140/7134 completed (loss: 0.04124465957283974, acc: 0.9883268475532532)
[2025-02-13 02:53:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:32][root][INFO] - Training Epoch: 1/2, step 3141/7134 completed (loss: 0.0739917978644371, acc: 0.9807692170143127)
[2025-02-13 02:53:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:32][root][INFO] - Training Epoch: 1/2, step 3142/7134 completed (loss: 0.026230791583657265, acc: 0.9955157041549683)
[2025-02-13 02:53:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:33][root][INFO] - Training Epoch: 1/2, step 3143/7134 completed (loss: 0.029407605528831482, acc: 0.9903537034988403)
[2025-02-13 02:53:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:33][root][INFO] - Training Epoch: 1/2, step 3144/7134 completed (loss: 0.03631782531738281, acc: 0.9905660152435303)
[2025-02-13 02:53:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:34][root][INFO] - Training Epoch: 1/2, step 3145/7134 completed (loss: 0.057126276195049286, acc: 0.9803063273429871)
[2025-02-13 02:53:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:34][root][INFO] - Training Epoch: 1/2, step 3146/7134 completed (loss: 0.03715435788035393, acc: 0.9856770634651184)
[2025-02-13 02:53:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:35][root][INFO] - Training Epoch: 1/2, step 3147/7134 completed (loss: 0.04246259108185768, acc: 0.9855538010597229)
[2025-02-13 02:53:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:35][root][INFO] - Training Epoch: 1/2, step 3148/7134 completed (loss: 0.029609940946102142, acc: 0.9910714030265808)
[2025-02-13 02:53:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:35][root][INFO] - Training Epoch: 1/2, step 3149/7134 completed (loss: 0.01282488089054823, acc: 0.9985358715057373)
[2025-02-13 02:53:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:36][root][INFO] - Training Epoch: 1/2, step 3150/7134 completed (loss: 0.13558991253376007, acc: 0.9625246524810791)
[2025-02-13 02:53:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:36][root][INFO] - Training Epoch: 1/2, step 3151/7134 completed (loss: 0.09979073703289032, acc: 0.975095808506012)
[2025-02-13 02:53:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:37][root][INFO] - Training Epoch: 1/2, step 3152/7134 completed (loss: 0.055723872035741806, acc: 0.9818181991577148)
[2025-02-13 02:53:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:37][root][INFO] - Training Epoch: 1/2, step 3153/7134 completed (loss: 0.052018601447343826, acc: 0.9824561476707458)
[2025-02-13 02:53:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:37][root][INFO] - Training Epoch: 1/2, step 3154/7134 completed (loss: 0.06955339759588242, acc: 0.9840142130851746)
[2025-02-13 02:53:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:38][root][INFO] - Training Epoch: 1/2, step 3155/7134 completed (loss: 0.12283141165971756, acc: 0.9634888172149658)
[2025-02-13 02:53:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:38][root][INFO] - Training Epoch: 1/2, step 3156/7134 completed (loss: 0.10517042130231857, acc: 0.9777117371559143)
[2025-02-13 02:53:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:39][root][INFO] - Training Epoch: 1/2, step 3157/7134 completed (loss: 0.033340826630592346, acc: 0.9935483932495117)
[2025-02-13 02:53:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:39][root][INFO] - Training Epoch: 1/2, step 3158/7134 completed (loss: 0.026534372940659523, acc: 0.9885621070861816)
[2025-02-13 02:53:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:40][root][INFO] - Training Epoch: 1/2, step 3159/7134 completed (loss: 0.08273250609636307, acc: 0.9730392098426819)
[2025-02-13 02:53:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:40][root][INFO] - Training Epoch: 1/2, step 3160/7134 completed (loss: 0.014306996017694473, acc: 0.9958506226539612)
[2025-02-13 02:53:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:40][root][INFO] - Training Epoch: 1/2, step 3161/7134 completed (loss: 0.04096388444304466, acc: 0.9859943985939026)
[2025-02-13 02:53:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:41][root][INFO] - Training Epoch: 1/2, step 3162/7134 completed (loss: 0.025519123300909996, acc: 0.9941691160202026)
[2025-02-13 02:53:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:41][root][INFO] - Training Epoch: 1/2, step 3163/7134 completed (loss: 0.06391880661249161, acc: 0.9799666404724121)
[2025-02-13 02:53:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:42][root][INFO] - Training Epoch: 1/2, step 3164/7134 completed (loss: 0.06650727242231369, acc: 0.981574535369873)
[2025-02-13 02:53:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:42][root][INFO] - Training Epoch: 1/2, step 3165/7134 completed (loss: 0.025268223136663437, acc: 0.994991660118103)
[2025-02-13 02:53:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:43][root][INFO] - Training Epoch: 1/2, step 3166/7134 completed (loss: 0.08460792154073715, acc: 0.9744361042976379)
[2025-02-13 02:53:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:43][root][INFO] - Training Epoch: 1/2, step 3167/7134 completed (loss: 0.11489821970462799, acc: 0.957812488079071)
[2025-02-13 02:53:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:43][root][INFO] - Training Epoch: 1/2, step 3168/7134 completed (loss: 0.10109124332666397, acc: 0.9740853905677795)
[2025-02-13 02:53:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:44][root][INFO] - Training Epoch: 1/2, step 3169/7134 completed (loss: 0.07688693702220917, acc: 0.9774590134620667)
[2025-02-13 02:53:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:44][root][INFO] - Training Epoch: 1/2, step 3170/7134 completed (loss: 0.03338675573468208, acc: 0.9872727394104004)
[2025-02-13 02:53:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:45][root][INFO] - Training Epoch: 1/2, step 3171/7134 completed (loss: 0.054160308092832565, acc: 0.9842725992202759)
[2025-02-13 02:53:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:45][root][INFO] - Training Epoch: 1/2, step 3172/7134 completed (loss: 0.020273111760616302, acc: 0.9961685538291931)
[2025-02-13 02:53:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:45][root][INFO] - Training Epoch: 1/2, step 3173/7134 completed (loss: 0.03927112743258476, acc: 0.9872449040412903)
[2025-02-13 02:53:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:46][root][INFO] - Training Epoch: 1/2, step 3174/7134 completed (loss: 0.08301202952861786, acc: 0.976331353187561)
[2025-02-13 02:53:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:46][root][INFO] - Training Epoch: 1/2, step 3175/7134 completed (loss: 0.05411181226372719, acc: 0.9845070242881775)
[2025-02-13 02:53:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:47][root][INFO] - Training Epoch: 1/2, step 3176/7134 completed (loss: 0.07865520566701889, acc: 0.9789473414421082)
[2025-02-13 02:53:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:47][root][INFO] - Training Epoch: 1/2, step 3177/7134 completed (loss: 0.06583066284656525, acc: 0.9826388955116272)
[2025-02-13 02:53:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:48][root][INFO] - Training Epoch: 1/2, step 3178/7134 completed (loss: 0.02221406064927578, acc: 0.9934318661689758)
[2025-02-13 02:53:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:48][root][INFO] - Training Epoch: 1/2, step 3179/7134 completed (loss: 0.07603174448013306, acc: 0.9817578792572021)
[2025-02-13 02:53:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:48][root][INFO] - Training Epoch: 1/2, step 3180/7134 completed (loss: 0.05004281550645828, acc: 0.9900000095367432)
[2025-02-13 02:53:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:49][root][INFO] - Training Epoch: 1/2, step 3181/7134 completed (loss: 0.05990101397037506, acc: 0.9812792539596558)
[2025-02-13 02:53:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:49][root][INFO] - Training Epoch: 1/2, step 3182/7134 completed (loss: 0.03636450693011284, acc: 0.9940387606620789)
[2025-02-13 02:53:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:50][root][INFO] - Training Epoch: 1/2, step 3183/7134 completed (loss: 0.05155320093035698, acc: 0.9847715497016907)
[2025-02-13 02:53:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:50][root][INFO] - Training Epoch: 1/2, step 3184/7134 completed (loss: 0.04991801455616951, acc: 0.9895833134651184)
[2025-02-13 02:53:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:51][root][INFO] - Training Epoch: 1/2, step 3185/7134 completed (loss: 0.009053169749677181, acc: 0.9985755085945129)
[2025-02-13 02:53:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:51][root][INFO] - Training Epoch: 1/2, step 3186/7134 completed (loss: 0.027422847226262093, acc: 0.9911110997200012)
[2025-02-13 02:53:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:51][root][INFO] - Training Epoch: 1/2, step 3187/7134 completed (loss: 0.062353845685720444, acc: 0.9857594966888428)
[2025-02-13 02:53:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:52][root][INFO] - Training Epoch: 1/2, step 3188/7134 completed (loss: 0.04717430844902992, acc: 0.9883570671081543)
[2025-02-13 02:53:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:52][root][INFO] - Training Epoch: 1/2, step 3189/7134 completed (loss: 0.059435728937387466, acc: 0.9826338887214661)
[2025-02-13 02:53:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:53][root][INFO] - Training Epoch: 1/2, step 3190/7134 completed (loss: 0.05932154506444931, acc: 0.9835442900657654)
[2025-02-13 02:53:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:53][root][INFO] - Training Epoch: 1/2, step 3191/7134 completed (loss: 0.028645452111959457, acc: 0.991428554058075)
[2025-02-13 02:53:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:54][root][INFO] - Training Epoch: 1/2, step 3192/7134 completed (loss: 0.05298887938261032, acc: 0.9886578321456909)
[2025-02-13 02:53:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:54][root][INFO] - Training Epoch: 1/2, step 3193/7134 completed (loss: 0.06453681737184525, acc: 0.9838945865631104)
[2025-02-13 02:53:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:54][root][INFO] - Training Epoch: 1/2, step 3194/7134 completed (loss: 0.05359843373298645, acc: 0.983660101890564)
[2025-02-13 02:53:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:55][root][INFO] - Training Epoch: 1/2, step 3195/7134 completed (loss: 0.05001085624098778, acc: 0.9914236664772034)
[2025-02-13 02:53:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:55][root][INFO] - Training Epoch: 1/2, step 3196/7134 completed (loss: 0.03357677534222603, acc: 0.9869158864021301)
[2025-02-13 02:53:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:56][root][INFO] - Training Epoch: 1/2, step 3197/7134 completed (loss: 0.04418094456195831, acc: 0.9887323975563049)
[2025-02-13 02:53:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:56][root][INFO] - Training Epoch: 1/2, step 3198/7134 completed (loss: 0.06972567737102509, acc: 0.9818456768989563)
[2025-02-13 02:53:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:56][root][INFO] - Training Epoch: 1/2, step 3199/7134 completed (loss: 0.03346782550215721, acc: 0.9897210001945496)
[2025-02-13 02:53:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:57][root][INFO] - Training Epoch: 1/2, step 3200/7134 completed (loss: 0.04473493620753288, acc: 0.9859402179718018)
[2025-02-13 02:53:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:57][root][INFO] - Training Epoch: 1/2, step 3201/7134 completed (loss: 0.11616112291812897, acc: 0.9739696383476257)
[2025-02-13 02:53:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:58][root][INFO] - Training Epoch: 1/2, step 3202/7134 completed (loss: 0.031051920726895332, acc: 0.9917012453079224)
[2025-02-13 02:53:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:58][root][INFO] - Training Epoch: 1/2, step 3203/7134 completed (loss: 0.04319816827774048, acc: 0.9857819676399231)
[2025-02-13 02:53:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:58][root][INFO] - Training Epoch: 1/2, step 3204/7134 completed (loss: 0.09512688964605331, acc: 0.969924807548523)
[2025-02-13 02:53:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:59][root][INFO] - Training Epoch: 1/2, step 3205/7134 completed (loss: 0.014598863199353218, acc: 0.9964664578437805)
[2025-02-13 02:53:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:53:59][root][INFO] - Training Epoch: 1/2, step 3206/7134 completed (loss: 0.05083610117435455, acc: 0.9884225726127625)
[2025-02-13 02:53:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:00][root][INFO] - Training Epoch: 1/2, step 3207/7134 completed (loss: 0.043551668524742126, acc: 0.9846860766410828)
[2025-02-13 02:54:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:00][root][INFO] - Training Epoch: 1/2, step 3208/7134 completed (loss: 0.07914087176322937, acc: 0.9787581562995911)
[2025-02-13 02:54:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:01][root][INFO] - Training Epoch: 1/2, step 3209/7134 completed (loss: 0.027778126299381256, acc: 0.9940476417541504)
[2025-02-13 02:54:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:01][root][INFO] - Training Epoch: 1/2, step 3210/7134 completed (loss: 0.013406858779489994, acc: 0.9969879388809204)
[2025-02-13 02:54:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:01][root][INFO] - Training Epoch: 1/2, step 3211/7134 completed (loss: 0.05827583000063896, acc: 0.9896050095558167)
[2025-02-13 02:54:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:02][root][INFO] - Training Epoch: 1/2, step 3212/7134 completed (loss: 0.010146083310246468, acc: 0.9986013770103455)
[2025-02-13 02:54:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:02][root][INFO] - Training Epoch: 1/2, step 3213/7134 completed (loss: 0.03314899280667305, acc: 0.991391658782959)
[2025-02-13 02:54:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:03][root][INFO] - Training Epoch: 1/2, step 3214/7134 completed (loss: 0.034893542528152466, acc: 0.9906291961669922)
[2025-02-13 02:54:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:03][root][INFO] - Training Epoch: 1/2, step 3215/7134 completed (loss: 0.0405825711786747, acc: 0.9929971694946289)
[2025-02-13 02:54:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:04][root][INFO] - Training Epoch: 1/2, step 3216/7134 completed (loss: 0.017521608620882034, acc: 0.9942693114280701)
[2025-02-13 02:54:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:04][root][INFO] - Training Epoch: 1/2, step 3217/7134 completed (loss: 0.021863551810383797, acc: 0.9922580718994141)
[2025-02-13 02:54:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:04][root][INFO] - Training Epoch: 1/2, step 3218/7134 completed (loss: 0.06835174560546875, acc: 0.9827855825424194)
[2025-02-13 02:54:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:05][root][INFO] - Training Epoch: 1/2, step 3219/7134 completed (loss: 0.043951086699962616, acc: 0.9837518334388733)
[2025-02-13 02:54:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:05][root][INFO] - Training Epoch: 1/2, step 3220/7134 completed (loss: 0.04845477268099785, acc: 0.9866488575935364)
[2025-02-13 02:54:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:06][root][INFO] - Training Epoch: 1/2, step 3221/7134 completed (loss: 0.04336978867650032, acc: 0.9863013625144958)
[2025-02-13 02:54:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:06][root][INFO] - Training Epoch: 1/2, step 3222/7134 completed (loss: 0.06660492718219757, acc: 0.9837296605110168)
[2025-02-13 02:54:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:07][root][INFO] - Training Epoch: 1/2, step 3223/7134 completed (loss: 0.07838376611471176, acc: 0.9857142567634583)
[2025-02-13 02:54:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:07][root][INFO] - Training Epoch: 1/2, step 3224/7134 completed (loss: 0.06668776273727417, acc: 0.9811866879463196)
[2025-02-13 02:54:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:07][root][INFO] - Training Epoch: 1/2, step 3225/7134 completed (loss: 0.03575462847948074, acc: 0.9898862242698669)
[2025-02-13 02:54:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:08][root][INFO] - Training Epoch: 1/2, step 3226/7134 completed (loss: 0.045830320566892624, acc: 0.9820466637611389)
[2025-02-13 02:54:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:08][root][INFO] - Training Epoch: 1/2, step 3227/7134 completed (loss: 0.07864319533109665, acc: 0.9769230484962463)
[2025-02-13 02:54:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:09][root][INFO] - Training Epoch: 1/2, step 3228/7134 completed (loss: 0.02113606035709381, acc: 0.9921156167984009)
[2025-02-13 02:54:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:09][root][INFO] - Training Epoch: 1/2, step 3229/7134 completed (loss: 0.04581059142947197, acc: 0.9828641414642334)
[2025-02-13 02:54:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:10][root][INFO] - Training Epoch: 1/2, step 3230/7134 completed (loss: 0.06505945324897766, acc: 0.9807692170143127)
[2025-02-13 02:54:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:10][root][INFO] - Training Epoch: 1/2, step 3231/7134 completed (loss: 0.054310645908117294, acc: 0.9878419637680054)
[2025-02-13 02:54:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:10][root][INFO] - Training Epoch: 1/2, step 3232/7134 completed (loss: 0.029019664973020554, acc: 0.9898989796638489)
[2025-02-13 02:54:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:11][root][INFO] - Training Epoch: 1/2, step 3233/7134 completed (loss: 0.054110877215862274, acc: 0.9881656765937805)
[2025-02-13 02:54:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:11][root][INFO] - Training Epoch: 1/2, step 3234/7134 completed (loss: 0.02084474079310894, acc: 0.9952606558799744)
[2025-02-13 02:54:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:12][root][INFO] - Training Epoch: 1/2, step 3235/7134 completed (loss: 0.058239541947841644, acc: 0.9828571677207947)
[2025-02-13 02:54:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:12][root][INFO] - Training Epoch: 1/2, step 3236/7134 completed (loss: 0.10664838552474976, acc: 0.9793814420700073)
[2025-02-13 02:54:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:13][root][INFO] - Training Epoch: 1/2, step 3237/7134 completed (loss: 0.025321774184703827, acc: 0.9935587644577026)
[2025-02-13 02:54:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:13][root][INFO] - Training Epoch: 1/2, step 3238/7134 completed (loss: 0.055005259811878204, acc: 0.9800613522529602)
[2025-02-13 02:54:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:13][root][INFO] - Training Epoch: 1/2, step 3239/7134 completed (loss: 0.028315436094999313, acc: 0.9921135902404785)
[2025-02-13 02:54:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:14][root][INFO] - Training Epoch: 1/2, step 3240/7134 completed (loss: 0.03404993191361427, acc: 0.9895366430282593)
[2025-02-13 02:54:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:14][root][INFO] - Training Epoch: 1/2, step 3241/7134 completed (loss: 0.026345284655690193, acc: 0.9910979270935059)
[2025-02-13 02:54:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:15][root][INFO] - Training Epoch: 1/2, step 3242/7134 completed (loss: 0.030903413891792297, acc: 0.991830050945282)
[2025-02-13 02:54:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:15][root][INFO] - Training Epoch: 1/2, step 3243/7134 completed (loss: 0.02748776227235794, acc: 0.9918256402015686)
[2025-02-13 02:54:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:15][root][INFO] - Training Epoch: 1/2, step 3244/7134 completed (loss: 0.0401604063808918, acc: 0.9942611455917358)
[2025-02-13 02:54:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:16][root][INFO] - Training Epoch: 1/2, step 3245/7134 completed (loss: 0.017004622146487236, acc: 0.9948453903198242)
[2025-02-13 02:54:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:16][root][INFO] - Training Epoch: 1/2, step 3246/7134 completed (loss: 0.050678860396146774, acc: 0.9846153855323792)
[2025-02-13 02:54:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:17][root][INFO] - Training Epoch: 1/2, step 3247/7134 completed (loss: 0.04576129838824272, acc: 0.9886547923088074)
[2025-02-13 02:54:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:17][root][INFO] - Training Epoch: 1/2, step 3248/7134 completed (loss: 0.024949749931693077, acc: 0.9934318661689758)
[2025-02-13 02:54:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:18][root][INFO] - Training Epoch: 1/2, step 3249/7134 completed (loss: 0.029753709211945534, acc: 0.9902912378311157)
[2025-02-13 02:54:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:18][root][INFO] - Training Epoch: 1/2, step 3250/7134 completed (loss: 0.049639925360679626, acc: 0.9812949895858765)
[2025-02-13 02:54:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:18][root][INFO] - Training Epoch: 1/2, step 3251/7134 completed (loss: 0.015660550445318222, acc: 0.9928673505783081)
[2025-02-13 02:54:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:19][root][INFO] - Training Epoch: 1/2, step 3252/7134 completed (loss: 0.02053673379123211, acc: 0.9929078221321106)
[2025-02-13 02:54:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:19][root][INFO] - Training Epoch: 1/2, step 3253/7134 completed (loss: 0.024166403338313103, acc: 0.9935794472694397)
[2025-02-13 02:54:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:20][root][INFO] - Training Epoch: 1/2, step 3254/7134 completed (loss: 0.01011254359036684, acc: 0.9983136653900146)
[2025-02-13 02:54:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:20][root][INFO] - Training Epoch: 1/2, step 3255/7134 completed (loss: 0.012670294381678104, acc: 0.9933422207832336)
[2025-02-13 02:54:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:21][root][INFO] - Training Epoch: 1/2, step 3256/7134 completed (loss: 0.023511433973908424, acc: 0.995555579662323)
[2025-02-13 02:54:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:21][root][INFO] - Training Epoch: 1/2, step 3257/7134 completed (loss: 0.018306566402316093, acc: 0.994452178478241)
[2025-02-13 02:54:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:21][root][INFO] - Training Epoch: 1/2, step 3258/7134 completed (loss: 0.0402156263589859, acc: 0.9884615540504456)
[2025-02-13 02:54:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:22][root][INFO] - Training Epoch: 1/2, step 3259/7134 completed (loss: 0.02846870943903923, acc: 0.9911764860153198)
[2025-02-13 02:54:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:22][root][INFO] - Training Epoch: 1/2, step 3260/7134 completed (loss: 0.07474970817565918, acc: 0.9851632118225098)
[2025-02-13 02:54:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:23][root][INFO] - Training Epoch: 1/2, step 3261/7134 completed (loss: 0.05573176220059395, acc: 0.9815436005592346)
[2025-02-13 02:54:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:23][root][INFO] - Training Epoch: 1/2, step 3262/7134 completed (loss: 0.03410341218113899, acc: 0.9884225726127625)
[2025-02-13 02:54:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:24][root][INFO] - Training Epoch: 1/2, step 3263/7134 completed (loss: 0.0704779401421547, acc: 0.9867841601371765)
[2025-02-13 02:54:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:24][root][INFO] - Training Epoch: 1/2, step 3264/7134 completed (loss: 0.02329856902360916, acc: 0.9902777671813965)
[2025-02-13 02:54:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:24][root][INFO] - Training Epoch: 1/2, step 3265/7134 completed (loss: 0.08062200248241425, acc: 0.9772079586982727)
[2025-02-13 02:54:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:25][root][INFO] - Training Epoch: 1/2, step 3266/7134 completed (loss: 0.05924259126186371, acc: 0.9879879951477051)
[2025-02-13 02:54:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:25][root][INFO] - Training Epoch: 1/2, step 3267/7134 completed (loss: 0.06578046828508377, acc: 0.9813753366470337)
[2025-02-13 02:54:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:26][root][INFO] - Training Epoch: 1/2, step 3268/7134 completed (loss: 0.05226113647222519, acc: 0.990755021572113)
[2025-02-13 02:54:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:26][root][INFO] - Training Epoch: 1/2, step 3269/7134 completed (loss: 0.057123493403196335, acc: 0.9848484992980957)
[2025-02-13 02:54:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:27][root][INFO] - Training Epoch: 1/2, step 3270/7134 completed (loss: 0.02379227615892887, acc: 0.9928229451179504)
[2025-02-13 02:54:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:27][root][INFO] - Training Epoch: 1/2, step 3271/7134 completed (loss: 0.007589130662381649, acc: 1.0)
[2025-02-13 02:54:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:27][root][INFO] - Training Epoch: 1/2, step 3272/7134 completed (loss: 0.029428085312247276, acc: 0.9969465732574463)
[2025-02-13 02:54:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:28][root][INFO] - Training Epoch: 1/2, step 3273/7134 completed (loss: 0.02432912401854992, acc: 0.9936143159866333)
[2025-02-13 02:54:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:28][root][INFO] - Training Epoch: 1/2, step 3274/7134 completed (loss: 0.05107908695936203, acc: 0.98531574010849)
[2025-02-13 02:54:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:29][root][INFO] - Training Epoch: 1/2, step 3275/7134 completed (loss: 0.1505257785320282, acc: 0.9707602262496948)
[2025-02-13 02:54:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:29][root][INFO] - Training Epoch: 1/2, step 3276/7134 completed (loss: 0.07204429060220718, acc: 0.9807162284851074)
[2025-02-13 02:54:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:30][root][INFO] - Training Epoch: 1/2, step 3277/7134 completed (loss: 0.04481741413474083, acc: 0.9862805008888245)
[2025-02-13 02:54:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:30][root][INFO] - Training Epoch: 1/2, step 3278/7134 completed (loss: 0.04595343396067619, acc: 0.9847036600112915)
[2025-02-13 02:54:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:30][root][INFO] - Training Epoch: 1/2, step 3279/7134 completed (loss: 0.03968137130141258, acc: 0.9888198971748352)
[2025-02-13 02:54:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:31][root][INFO] - Training Epoch: 1/2, step 3280/7134 completed (loss: 0.14653564989566803, acc: 0.9647302627563477)
[2025-02-13 02:54:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:31][root][INFO] - Training Epoch: 1/2, step 3281/7134 completed (loss: 0.05098233371973038, acc: 0.9884058237075806)
[2025-02-13 02:54:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:32][root][INFO] - Training Epoch: 1/2, step 3282/7134 completed (loss: 0.0408589132130146, acc: 0.9873417615890503)
[2025-02-13 02:54:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:32][root][INFO] - Training Epoch: 1/2, step 3283/7134 completed (loss: 0.06289699673652649, acc: 0.9876288771629333)
[2025-02-13 02:54:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:32][root][INFO] - Training Epoch: 1/2, step 3284/7134 completed (loss: 0.07349248230457306, acc: 0.9802131056785583)
[2025-02-13 02:54:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:33][root][INFO] - Training Epoch: 1/2, step 3285/7134 completed (loss: 0.03910831734538078, acc: 0.9829721450805664)
[2025-02-13 02:54:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:33][root][INFO] - Training Epoch: 1/2, step 3286/7134 completed (loss: 0.036763668060302734, acc: 0.9939849376678467)
[2025-02-13 02:54:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:34][root][INFO] - Training Epoch: 1/2, step 3287/7134 completed (loss: 0.10774720460176468, acc: 0.9753289222717285)
[2025-02-13 02:54:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:34][root][INFO] - Training Epoch: 1/2, step 3288/7134 completed (loss: 0.04954088479280472, acc: 0.9835329055786133)
[2025-02-13 02:54:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:35][root][INFO] - Training Epoch: 1/2, step 3289/7134 completed (loss: 0.0784885436296463, acc: 0.9754385948181152)
[2025-02-13 02:54:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:35][root][INFO] - Training Epoch: 1/2, step 3290/7134 completed (loss: 0.08261450380086899, acc: 0.9791666865348816)
[2025-02-13 02:54:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:35][root][INFO] - Training Epoch: 1/2, step 3291/7134 completed (loss: 0.07143794000148773, acc: 0.9710467457771301)
[2025-02-13 02:54:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:36][root][INFO] - Training Epoch: 1/2, step 3292/7134 completed (loss: 0.0591316819190979, acc: 0.9876543283462524)
[2025-02-13 02:54:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:36][root][INFO] - Training Epoch: 1/2, step 3293/7134 completed (loss: 0.035665012896060944, acc: 0.9867256879806519)
[2025-02-13 02:54:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:37][root][INFO] - Training Epoch: 1/2, step 3294/7134 completed (loss: 0.07063671201467514, acc: 0.9831546545028687)
[2025-02-13 02:54:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:37][root][INFO] - Training Epoch: 1/2, step 3295/7134 completed (loss: 0.021229444071650505, acc: 0.9965870380401611)
[2025-02-13 02:54:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:37][root][INFO] - Training Epoch: 1/2, step 3296/7134 completed (loss: 0.06322559714317322, acc: 0.9798902869224548)
[2025-02-13 02:54:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:38][root][INFO] - Training Epoch: 1/2, step 3297/7134 completed (loss: 0.04397166892886162, acc: 0.9812206625938416)
[2025-02-13 02:54:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:38][root][INFO] - Training Epoch: 1/2, step 3298/7134 completed (loss: 0.10415185987949371, acc: 0.9760383367538452)
[2025-02-13 02:54:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:39][root][INFO] - Training Epoch: 1/2, step 3299/7134 completed (loss: 0.04768270254135132, acc: 0.9798657894134521)
[2025-02-13 02:54:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:39][root][INFO] - Training Epoch: 1/2, step 3300/7134 completed (loss: 0.06549666076898575, acc: 0.9821746945381165)
[2025-02-13 02:54:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:39][root][INFO] - Training Epoch: 1/2, step 3301/7134 completed (loss: 0.05893838033080101, acc: 0.9875518679618835)
[2025-02-13 02:54:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:40][root][INFO] - Training Epoch: 1/2, step 3302/7134 completed (loss: 0.0640123188495636, acc: 0.9898989796638489)
[2025-02-13 02:54:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:40][root][INFO] - Training Epoch: 1/2, step 3303/7134 completed (loss: 0.046134691685438156, acc: 0.9867841601371765)
[2025-02-13 02:54:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:41][root][INFO] - Training Epoch: 1/2, step 3304/7134 completed (loss: 0.024064820259809494, acc: 0.9940333962440491)
[2025-02-13 02:54:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:41][root][INFO] - Training Epoch: 1/2, step 3305/7134 completed (loss: 0.04271227493882179, acc: 0.9894419312477112)
[2025-02-13 02:54:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:41][root][INFO] - Training Epoch: 1/2, step 3306/7134 completed (loss: 0.041231051087379456, acc: 0.9870298504829407)
[2025-02-13 02:54:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:42][root][INFO] - Training Epoch: 1/2, step 3307/7134 completed (loss: 0.05345439165830612, acc: 0.9830795526504517)
[2025-02-13 02:54:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:42][root][INFO] - Training Epoch: 1/2, step 3308/7134 completed (loss: 0.03841886296868324, acc: 0.9841040372848511)
[2025-02-13 02:54:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:43][root][INFO] - Training Epoch: 1/2, step 3309/7134 completed (loss: 0.06884109228849411, acc: 0.9793510437011719)
[2025-02-13 02:54:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:43][root][INFO] - Training Epoch: 1/2, step 3310/7134 completed (loss: 0.05744422972202301, acc: 0.9893389940261841)
[2025-02-13 02:54:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:44][root][INFO] - Training Epoch: 1/2, step 3311/7134 completed (loss: 0.04234929382801056, acc: 0.9849462509155273)
[2025-02-13 02:54:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:44][root][INFO] - Training Epoch: 1/2, step 3312/7134 completed (loss: 0.050903018563985825, acc: 0.9854545593261719)
[2025-02-13 02:54:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:44][root][INFO] - Training Epoch: 1/2, step 3313/7134 completed (loss: 0.019756002351641655, acc: 0.9952267408370972)
[2025-02-13 02:54:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:45][root][INFO] - Training Epoch: 1/2, step 3314/7134 completed (loss: 0.03830359876155853, acc: 0.9927536249160767)
[2025-02-13 02:54:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:45][root][INFO] - Training Epoch: 1/2, step 3315/7134 completed (loss: 0.08054586499929428, acc: 0.9782903790473938)
[2025-02-13 02:54:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:46][root][INFO] - Training Epoch: 1/2, step 3316/7134 completed (loss: 0.01679120399057865, acc: 0.9949579834938049)
[2025-02-13 02:54:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:46][root][INFO] - Training Epoch: 1/2, step 3317/7134 completed (loss: 0.0761621817946434, acc: 0.982692301273346)
[2025-02-13 02:54:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:46][root][INFO] - Training Epoch: 1/2, step 3318/7134 completed (loss: 0.04774576425552368, acc: 0.9747292399406433)
[2025-02-13 02:54:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:47][root][INFO] - Training Epoch: 1/2, step 3319/7134 completed (loss: 0.048325225710868835, acc: 0.9918566942214966)
[2025-02-13 02:54:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:47][root][INFO] - Training Epoch: 1/2, step 3320/7134 completed (loss: 0.08639516681432724, acc: 0.9803664684295654)
[2025-02-13 02:54:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:48][root][INFO] - Training Epoch: 1/2, step 3321/7134 completed (loss: 0.10201030224561691, acc: 0.9779816269874573)
[2025-02-13 02:54:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:48][root][INFO] - Training Epoch: 1/2, step 3322/7134 completed (loss: 0.037862613797187805, acc: 0.9894894957542419)
[2025-02-13 02:54:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:49][root][INFO] - Training Epoch: 1/2, step 3323/7134 completed (loss: 0.048865191638469696, acc: 0.9875862002372742)
[2025-02-13 02:54:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:49][root][INFO] - Training Epoch: 1/2, step 3324/7134 completed (loss: 0.06696709245443344, acc: 0.9752650260925293)
[2025-02-13 02:54:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:50][root][INFO] - Training Epoch: 1/2, step 3325/7134 completed (loss: 0.04357822611927986, acc: 0.9892473220825195)
[2025-02-13 02:54:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:50][root][INFO] - Training Epoch: 1/2, step 3326/7134 completed (loss: 0.0749177560210228, acc: 0.9811023473739624)
[2025-02-13 02:54:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:50][root][INFO] - Training Epoch: 1/2, step 3327/7134 completed (loss: 0.05363435298204422, acc: 0.9851149916648865)
[2025-02-13 02:54:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:51][root][INFO] - Training Epoch: 1/2, step 3328/7134 completed (loss: 0.04280790314078331, acc: 0.9912280440330505)
[2025-02-13 02:54:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:51][root][INFO] - Training Epoch: 1/2, step 3329/7134 completed (loss: 0.021159429103136063, acc: 0.9951279163360596)
[2025-02-13 02:54:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:52][root][INFO] - Training Epoch: 1/2, step 3330/7134 completed (loss: 0.05398938059806824, acc: 0.9873272180557251)
[2025-02-13 02:54:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:52][root][INFO] - Training Epoch: 1/2, step 3331/7134 completed (loss: 0.09149160981178284, acc: 0.9723837375640869)
[2025-02-13 02:54:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:53][root][INFO] - Training Epoch: 1/2, step 3332/7134 completed (loss: 0.13396939635276794, acc: 0.9653978943824768)
[2025-02-13 02:54:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:53][root][INFO] - Training Epoch: 1/2, step 3333/7134 completed (loss: 0.0864747017621994, acc: 0.9795082211494446)
[2025-02-13 02:54:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:53][root][INFO] - Training Epoch: 1/2, step 3334/7134 completed (loss: 0.09229638427495956, acc: 0.9743243455886841)
[2025-02-13 02:54:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:54][root][INFO] - Training Epoch: 1/2, step 3335/7134 completed (loss: 0.04514317214488983, acc: 0.9873417615890503)
[2025-02-13 02:54:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:54][root][INFO] - Training Epoch: 1/2, step 3336/7134 completed (loss: 0.026228222995996475, acc: 0.9927113652229309)
[2025-02-13 02:54:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:55][root][INFO] - Training Epoch: 1/2, step 3337/7134 completed (loss: 0.020373353734612465, acc: 0.9921104311943054)
[2025-02-13 02:54:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:55][root][INFO] - Training Epoch: 1/2, step 3338/7134 completed (loss: 0.014742225408554077, acc: 0.9950310587882996)
[2025-02-13 02:54:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:56][root][INFO] - Training Epoch: 1/2, step 3339/7134 completed (loss: 0.06440843641757965, acc: 0.9856801629066467)
[2025-02-13 02:54:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:56][root][INFO] - Training Epoch: 1/2, step 3340/7134 completed (loss: 0.10841213911771774, acc: 0.9698340892791748)
[2025-02-13 02:54:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:56][root][INFO] - Training Epoch: 1/2, step 3341/7134 completed (loss: 0.060391489416360855, acc: 0.9822221994400024)
[2025-02-13 02:54:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:57][root][INFO] - Training Epoch: 1/2, step 3342/7134 completed (loss: 0.044074635952711105, acc: 0.9898648858070374)
[2025-02-13 02:54:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:57][root][INFO] - Training Epoch: 1/2, step 3343/7134 completed (loss: 0.04170972853899002, acc: 0.9858871102333069)
[2025-02-13 02:54:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:58][root][INFO] - Training Epoch: 1/2, step 3344/7134 completed (loss: 0.03220071271061897, acc: 0.989708423614502)
[2025-02-13 02:54:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:58][root][INFO] - Training Epoch: 1/2, step 3345/7134 completed (loss: 0.07247816771268845, acc: 0.987034022808075)
[2025-02-13 02:54:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:59][root][INFO] - Training Epoch: 1/2, step 3346/7134 completed (loss: 0.036992769688367844, acc: 0.988304078578949)
[2025-02-13 02:54:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:59][root][INFO] - Training Epoch: 1/2, step 3347/7134 completed (loss: 0.08186660706996918, acc: 0.9804560542106628)
[2025-02-13 02:54:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:54:59][root][INFO] - Training Epoch: 1/2, step 3348/7134 completed (loss: 0.04400704801082611, acc: 0.9896296262741089)
[2025-02-13 02:55:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:00][root][INFO] - Training Epoch: 1/2, step 3349/7134 completed (loss: 0.0238493625074625, acc: 0.9947090148925781)
[2025-02-13 02:55:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:00][root][INFO] - Training Epoch: 1/2, step 3350/7134 completed (loss: 0.033891987055540085, acc: 0.9878048896789551)
[2025-02-13 02:55:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:01][root][INFO] - Training Epoch: 1/2, step 3351/7134 completed (loss: 0.0721534937620163, acc: 0.9867374300956726)
[2025-02-13 02:55:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:01][root][INFO] - Training Epoch: 1/2, step 3352/7134 completed (loss: 0.024594934657216072, acc: 0.9947826266288757)
[2025-02-13 02:55:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:02][root][INFO] - Training Epoch: 1/2, step 3353/7134 completed (loss: 0.04137926548719406, acc: 0.9921104311943054)
[2025-02-13 02:55:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:02][root][INFO] - Training Epoch: 1/2, step 3354/7134 completed (loss: 0.022007938474416733, acc: 0.9953488111495972)
[2025-02-13 02:55:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:02][root][INFO] - Training Epoch: 1/2, step 3355/7134 completed (loss: 0.017073066905140877, acc: 0.994358241558075)
[2025-02-13 02:55:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:03][root][INFO] - Training Epoch: 1/2, step 3356/7134 completed (loss: 0.013728322461247444, acc: 0.9965277910232544)
[2025-02-13 02:55:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:03][root][INFO] - Training Epoch: 1/2, step 3357/7134 completed (loss: 0.035913076251745224, acc: 0.9913420081138611)
[2025-02-13 02:55:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:04][root][INFO] - Training Epoch: 1/2, step 3358/7134 completed (loss: 0.05854550376534462, acc: 0.9930796027183533)
[2025-02-13 02:55:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:04][root][INFO] - Training Epoch: 1/2, step 3359/7134 completed (loss: 0.024089591577649117, acc: 0.9951456189155579)
[2025-02-13 02:55:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:04][root][INFO] - Training Epoch: 1/2, step 3360/7134 completed (loss: 0.1207297146320343, acc: 0.9763948321342468)
[2025-02-13 02:55:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:05][root][INFO] - Training Epoch: 1/2, step 3361/7134 completed (loss: 0.0780906081199646, acc: 0.9709172248840332)
[2025-02-13 02:55:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:05][root][INFO] - Training Epoch: 1/2, step 3362/7134 completed (loss: 0.050177689641714096, acc: 0.9863945841789246)
[2025-02-13 02:55:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:06][root][INFO] - Training Epoch: 1/2, step 3363/7134 completed (loss: 0.018934614956378937, acc: 0.9953051805496216)
[2025-02-13 02:55:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:06][root][INFO] - Training Epoch: 1/2, step 3364/7134 completed (loss: 0.10298357158899307, acc: 0.9692780375480652)
[2025-02-13 02:55:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:06][root][INFO] - Training Epoch: 1/2, step 3365/7134 completed (loss: 0.09482937306165695, acc: 0.9783132672309875)
[2025-02-13 02:55:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:07][root][INFO] - Training Epoch: 1/2, step 3366/7134 completed (loss: 0.05511455237865448, acc: 0.9865471124649048)
[2025-02-13 02:55:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:07][root][INFO] - Training Epoch: 1/2, step 3367/7134 completed (loss: 0.03584272414445877, acc: 0.9903581142425537)
[2025-02-13 02:55:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:08][root][INFO] - Training Epoch: 1/2, step 3368/7134 completed (loss: 0.042337026447057724, acc: 0.9890795350074768)
[2025-02-13 02:55:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:08][root][INFO] - Training Epoch: 1/2, step 3369/7134 completed (loss: 0.053643397986888885, acc: 0.9884892106056213)
[2025-02-13 02:55:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:09][root][INFO] - Training Epoch: 1/2, step 3370/7134 completed (loss: 0.03145492821931839, acc: 0.991416335105896)
[2025-02-13 02:55:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:09][root][INFO] - Training Epoch: 1/2, step 3371/7134 completed (loss: 0.028976524248719215, acc: 0.9917762875556946)
[2025-02-13 02:55:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:09][root][INFO] - Training Epoch: 1/2, step 3372/7134 completed (loss: 0.03676329925656319, acc: 0.9905956387519836)
[2025-02-13 02:55:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:10][root][INFO] - Training Epoch: 1/2, step 3373/7134 completed (loss: 0.02230324223637581, acc: 0.9953703880310059)
[2025-02-13 02:55:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:10][root][INFO] - Training Epoch: 1/2, step 3374/7134 completed (loss: 0.021182850003242493, acc: 0.9958217144012451)
[2025-02-13 02:55:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:11][root][INFO] - Training Epoch: 1/2, step 3375/7134 completed (loss: 0.02775588072836399, acc: 0.9956584572792053)
[2025-02-13 02:55:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:11][root][INFO] - Training Epoch: 1/2, step 3376/7134 completed (loss: 0.021250184625387192, acc: 0.9948914647102356)
[2025-02-13 02:55:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:12][root][INFO] - Training Epoch: 1/2, step 3377/7134 completed (loss: 0.02078525349497795, acc: 0.9940405488014221)
[2025-02-13 02:55:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:12][root][INFO] - Training Epoch: 1/2, step 3378/7134 completed (loss: 0.01884954608976841, acc: 0.9947916865348816)
[2025-02-13 02:55:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:13][root][INFO] - Training Epoch: 1/2, step 3379/7134 completed (loss: 0.029884934425354004, acc: 0.9909774661064148)
[2025-02-13 02:55:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:13][root][INFO] - Training Epoch: 1/2, step 3380/7134 completed (loss: 0.04192879796028137, acc: 0.9907692074775696)
[2025-02-13 02:55:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:13][root][INFO] - Training Epoch: 1/2, step 3381/7134 completed (loss: 0.0667521059513092, acc: 0.9806201457977295)
[2025-02-13 02:55:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:14][root][INFO] - Training Epoch: 1/2, step 3382/7134 completed (loss: 0.008488433435559273, acc: 1.0)
[2025-02-13 02:55:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:14][root][INFO] - Training Epoch: 1/2, step 3383/7134 completed (loss: 0.042095087468624115, acc: 0.9942611455917358)
[2025-02-13 02:55:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:15][root][INFO] - Training Epoch: 1/2, step 3384/7134 completed (loss: 0.04181108623743057, acc: 0.9892966151237488)
[2025-02-13 02:55:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:15][root][INFO] - Training Epoch: 1/2, step 3385/7134 completed (loss: 0.03653280809521675, acc: 0.9928057789802551)
[2025-02-13 02:55:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:16][root][INFO] - Training Epoch: 1/2, step 3386/7134 completed (loss: 0.024543756619095802, acc: 0.991525411605835)
[2025-02-13 02:55:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:16][root][INFO] - Training Epoch: 1/2, step 3387/7134 completed (loss: 0.056639473885297775, acc: 0.9863945841789246)
[2025-02-13 02:55:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:16][root][INFO] - Training Epoch: 1/2, step 3388/7134 completed (loss: 0.065941222012043, acc: 0.9801084995269775)
[2025-02-13 02:55:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:17][root][INFO] - Training Epoch: 1/2, step 3389/7134 completed (loss: 0.03279021009802818, acc: 0.9934123754501343)
[2025-02-13 02:55:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:17][root][INFO] - Training Epoch: 1/2, step 3390/7134 completed (loss: 0.03661924600601196, acc: 0.9857512712478638)
[2025-02-13 02:55:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:18][root][INFO] - Training Epoch: 1/2, step 3391/7134 completed (loss: 0.006070888135582209, acc: 0.9986263513565063)
[2025-02-13 02:55:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:18][root][INFO] - Training Epoch: 1/2, step 3392/7134 completed (loss: 0.019794471561908722, acc: 0.9921383857727051)
[2025-02-13 02:55:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:18][root][INFO] - Training Epoch: 1/2, step 3393/7134 completed (loss: 0.028646642342209816, acc: 0.9891975522041321)
[2025-02-13 02:55:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:19][root][INFO] - Training Epoch: 1/2, step 3394/7134 completed (loss: 0.03491859510540962, acc: 0.9873417615890503)
[2025-02-13 02:55:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:19][root][INFO] - Training Epoch: 1/2, step 3395/7134 completed (loss: 0.06892024725675583, acc: 0.975649356842041)
[2025-02-13 02:55:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:20][root][INFO] - Training Epoch: 1/2, step 3396/7134 completed (loss: 0.03622637689113617, acc: 0.9928143620491028)
[2025-02-13 02:55:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:20][root][INFO] - Training Epoch: 1/2, step 3397/7134 completed (loss: 0.042574185878038406, acc: 0.9926650524139404)
[2025-02-13 02:55:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:21][root][INFO] - Training Epoch: 1/2, step 3398/7134 completed (loss: 0.029843313619494438, acc: 0.9946236610412598)
[2025-02-13 02:55:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:21][root][INFO] - Training Epoch: 1/2, step 3399/7134 completed (loss: 0.11894091218709946, acc: 0.9724264740943909)
[2025-02-13 02:55:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:21][root][INFO] - Training Epoch: 1/2, step 3400/7134 completed (loss: 0.08271905779838562, acc: 0.9776875972747803)
[2025-02-13 02:55:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:22][root][INFO] - Training Epoch: 1/2, step 3401/7134 completed (loss: 0.05731207877397537, acc: 0.9825174808502197)
[2025-02-13 02:55:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:22][root][INFO] - Training Epoch: 1/2, step 3402/7134 completed (loss: 0.0663844645023346, acc: 0.9776021242141724)
[2025-02-13 02:55:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:23][root][INFO] - Training Epoch: 1/2, step 3403/7134 completed (loss: 0.0995703712105751, acc: 0.9765625)
[2025-02-13 02:55:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:23][root][INFO] - Training Epoch: 1/2, step 3404/7134 completed (loss: 0.01815173029899597, acc: 0.994020938873291)
[2025-02-13 02:55:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:24][root][INFO] - Training Epoch: 1/2, step 3405/7134 completed (loss: 0.10701275616884232, acc: 0.9748201370239258)
[2025-02-13 02:55:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:24][root][INFO] - Training Epoch: 1/2, step 3406/7134 completed (loss: 0.05836980789899826, acc: 0.9832473993301392)
[2025-02-13 02:55:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:25][root][INFO] - Training Epoch: 1/2, step 3407/7134 completed (loss: 0.05445993319153786, acc: 0.9906396269798279)
[2025-02-13 02:55:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:25][root][INFO] - Training Epoch: 1/2, step 3408/7134 completed (loss: 0.06205909326672554, acc: 0.9838709831237793)
[2025-02-13 02:55:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:25][root][INFO] - Training Epoch: 1/2, step 3409/7134 completed (loss: 0.039162881672382355, acc: 0.9891745448112488)
[2025-02-13 02:55:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:26][root][INFO] - Training Epoch: 1/2, step 3410/7134 completed (loss: 0.04334785416722298, acc: 0.9889807105064392)
[2025-02-13 02:55:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:26][root][INFO] - Training Epoch: 1/2, step 3411/7134 completed (loss: 0.04617222398519516, acc: 0.9883570671081543)
[2025-02-13 02:55:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:27][root][INFO] - Training Epoch: 1/2, step 3412/7134 completed (loss: 0.0575709231197834, acc: 0.9754253029823303)
[2025-02-13 02:55:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:27][root][INFO] - Training Epoch: 1/2, step 3413/7134 completed (loss: 0.038933608680963516, acc: 0.9900000095367432)
[2025-02-13 02:55:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:28][root][INFO] - Training Epoch: 1/2, step 3414/7134 completed (loss: 0.05745565891265869, acc: 0.9816360473632812)
[2025-02-13 02:55:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:28][root][INFO] - Training Epoch: 1/2, step 3415/7134 completed (loss: 0.04563915356993675, acc: 0.9903846383094788)
[2025-02-13 02:55:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:28][root][INFO] - Training Epoch: 1/2, step 3416/7134 completed (loss: 0.058504555374383926, acc: 0.9831932783126831)
[2025-02-13 02:55:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:29][root][INFO] - Training Epoch: 1/2, step 3417/7134 completed (loss: 0.021901652216911316, acc: 0.9959514141082764)
[2025-02-13 02:55:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:29][root][INFO] - Training Epoch: 1/2, step 3418/7134 completed (loss: 0.02942597307264805, acc: 0.9896907210350037)
[2025-02-13 02:55:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:30][root][INFO] - Training Epoch: 1/2, step 3419/7134 completed (loss: 0.017844611778855324, acc: 0.9942362904548645)
[2025-02-13 02:55:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:30][root][INFO] - Training Epoch: 1/2, step 3420/7134 completed (loss: 0.010020156390964985, acc: 0.998420238494873)
[2025-02-13 02:55:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:31][root][INFO] - Training Epoch: 1/2, step 3421/7134 completed (loss: 0.025505345314741135, acc: 0.993779182434082)
[2025-02-13 02:55:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:31][root][INFO] - Training Epoch: 1/2, step 3422/7134 completed (loss: 0.013709581457078457, acc: 0.9947460889816284)
[2025-02-13 02:55:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:31][root][INFO] - Training Epoch: 1/2, step 3423/7134 completed (loss: 0.016264967620372772, acc: 0.9954545497894287)
[2025-02-13 02:55:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:32][root][INFO] - Training Epoch: 1/2, step 3424/7134 completed (loss: 0.07558877021074295, acc: 0.9759036302566528)
[2025-02-13 02:55:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:32][root][INFO] - Training Epoch: 1/2, step 3425/7134 completed (loss: 0.037957657128572464, acc: 0.9948717951774597)
[2025-02-13 02:55:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:33][root][INFO] - Training Epoch: 1/2, step 3426/7134 completed (loss: 0.03648039326071739, acc: 0.9944827556610107)
[2025-02-13 02:55:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:33][root][INFO] - Training Epoch: 1/2, step 3427/7134 completed (loss: 0.03765397146344185, acc: 0.9921011328697205)
[2025-02-13 02:55:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:34][root][INFO] - Training Epoch: 1/2, step 3428/7134 completed (loss: 0.02717355266213417, acc: 0.9914039969444275)
[2025-02-13 02:55:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:34][root][INFO] - Training Epoch: 1/2, step 3429/7134 completed (loss: 0.0552661195397377, acc: 0.9854369163513184)
[2025-02-13 02:55:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:34][root][INFO] - Training Epoch: 1/2, step 3430/7134 completed (loss: 0.08701144903898239, acc: 0.9837925434112549)
[2025-02-13 02:55:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:35][root][INFO] - Training Epoch: 1/2, step 3431/7134 completed (loss: 0.03954723849892616, acc: 0.9932088255882263)
[2025-02-13 02:55:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:35][root][INFO] - Training Epoch: 1/2, step 3432/7134 completed (loss: 0.04024670273065567, acc: 0.991769552230835)
[2025-02-13 02:55:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:36][root][INFO] - Training Epoch: 1/2, step 3433/7134 completed (loss: 0.07450074702501297, acc: 0.970588207244873)
[2025-02-13 02:55:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:36][root][INFO] - Training Epoch: 1/2, step 3434/7134 completed (loss: 0.046144817024469376, acc: 0.9856915473937988)
[2025-02-13 02:55:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:37][root][INFO] - Training Epoch: 1/2, step 3435/7134 completed (loss: 0.04448245093226433, acc: 0.9887459874153137)
[2025-02-13 02:55:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:37][root][INFO] - Training Epoch: 1/2, step 3436/7134 completed (loss: 0.041008707135915756, acc: 0.9853658676147461)
[2025-02-13 02:55:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:37][root][INFO] - Training Epoch: 1/2, step 3437/7134 completed (loss: 0.026355871930718422, acc: 0.994140625)
[2025-02-13 02:55:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:38][root][INFO] - Training Epoch: 1/2, step 3438/7134 completed (loss: 0.05853985622525215, acc: 0.9858585596084595)
[2025-02-13 02:55:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:38][root][INFO] - Training Epoch: 1/2, step 3439/7134 completed (loss: 0.0525476336479187, acc: 0.9855334758758545)
[2025-02-13 02:55:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:39][root][INFO] - Training Epoch: 1/2, step 3440/7134 completed (loss: 0.09790168702602386, acc: 0.9670782089233398)
[2025-02-13 02:55:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:39][root][INFO] - Training Epoch: 1/2, step 3441/7134 completed (loss: 0.05987129732966423, acc: 0.9797688126564026)
[2025-02-13 02:55:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:39][root][INFO] - Training Epoch: 1/2, step 3442/7134 completed (loss: 0.018868140876293182, acc: 0.9927007555961609)
[2025-02-13 02:55:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:40][root][INFO] - Training Epoch: 1/2, step 3443/7134 completed (loss: 0.06857537478208542, acc: 0.9803370833396912)
[2025-02-13 02:55:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:40][root][INFO] - Training Epoch: 1/2, step 3444/7134 completed (loss: 0.07169833034276962, acc: 0.9857594966888428)
[2025-02-13 02:55:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:41][root][INFO] - Training Epoch: 1/2, step 3445/7134 completed (loss: 0.11498241126537323, acc: 0.9720430374145508)
[2025-02-13 02:55:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:41][root][INFO] - Training Epoch: 1/2, step 3446/7134 completed (loss: 0.09439515322446823, acc: 0.9732441306114197)
[2025-02-13 02:55:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:42][root][INFO] - Training Epoch: 1/2, step 3447/7134 completed (loss: 0.06011536717414856, acc: 0.9804560542106628)
[2025-02-13 02:55:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:42][root][INFO] - Training Epoch: 1/2, step 3448/7134 completed (loss: 0.024633124470710754, acc: 0.9894894957542419)
[2025-02-13 02:55:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:42][root][INFO] - Training Epoch: 1/2, step 3449/7134 completed (loss: 0.062310658395290375, acc: 0.9821428656578064)
[2025-02-13 02:55:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:43][root][INFO] - Training Epoch: 1/2, step 3450/7134 completed (loss: 0.04865574464201927, acc: 0.9848101139068604)
[2025-02-13 02:55:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:43][root][INFO] - Training Epoch: 1/2, step 3451/7134 completed (loss: 0.0481279194355011, acc: 0.9858657121658325)
[2025-02-13 02:55:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:44][root][INFO] - Training Epoch: 1/2, step 3452/7134 completed (loss: 0.05527159199118614, acc: 0.9869847893714905)
[2025-02-13 02:55:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:44][root][INFO] - Training Epoch: 1/2, step 3453/7134 completed (loss: 0.05633953586220741, acc: 0.9898132681846619)
[2025-02-13 02:55:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:45][root][INFO] - Training Epoch: 1/2, step 3454/7134 completed (loss: 0.04877428710460663, acc: 0.9901269674301147)
[2025-02-13 02:55:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:45][root][INFO] - Training Epoch: 1/2, step 3455/7134 completed (loss: 0.013791058212518692, acc: 0.9971014261245728)
[2025-02-13 02:55:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:45][root][INFO] - Training Epoch: 1/2, step 3456/7134 completed (loss: 0.034764960408210754, acc: 0.9917647242546082)
[2025-02-13 02:55:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:46][root][INFO] - Training Epoch: 1/2, step 3457/7134 completed (loss: 0.02187560684978962, acc: 0.993220329284668)
[2025-02-13 02:55:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:46][root][INFO] - Training Epoch: 1/2, step 3458/7134 completed (loss: 0.06214432418346405, acc: 0.9864681959152222)
[2025-02-13 02:55:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:47][root][INFO] - Training Epoch: 1/2, step 3459/7134 completed (loss: 0.034810733050107956, acc: 0.9893454909324646)
[2025-02-13 02:55:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:47][root][INFO] - Training Epoch: 1/2, step 3460/7134 completed (loss: 0.20152388513088226, acc: 0.9583333134651184)
[2025-02-13 02:55:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:47][root][INFO] - Training Epoch: 1/2, step 3461/7134 completed (loss: 0.2490958273410797, acc: 0.951724112033844)
[2025-02-13 02:55:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:48][root][INFO] - Training Epoch: 1/2, step 3462/7134 completed (loss: 0.10252833366394043, acc: 0.9780219793319702)
[2025-02-13 02:55:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:48][root][INFO] - Training Epoch: 1/2, step 3463/7134 completed (loss: 0.019146565347909927, acc: 0.9924585223197937)
[2025-02-13 02:55:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:49][root][INFO] - Training Epoch: 1/2, step 3464/7134 completed (loss: 0.054580334573984146, acc: 0.9798927903175354)
[2025-02-13 02:55:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:49][root][INFO] - Training Epoch: 1/2, step 3465/7134 completed (loss: 0.08579342812299728, acc: 0.9837067127227783)
[2025-02-13 02:55:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:50][root][INFO] - Training Epoch: 1/2, step 3466/7134 completed (loss: 0.024186130613088608, acc: 0.9920760989189148)
[2025-02-13 02:55:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:50][root][INFO] - Training Epoch: 1/2, step 3467/7134 completed (loss: 0.049576323479413986, acc: 0.9851379990577698)
[2025-02-13 02:55:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:50][root][INFO] - Training Epoch: 1/2, step 3468/7134 completed (loss: 0.013507301919162273, acc: 0.9971056580543518)
[2025-02-13 02:55:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:51][root][INFO] - Training Epoch: 1/2, step 3469/7134 completed (loss: 0.05198611319065094, acc: 0.9867924451828003)
[2025-02-13 02:55:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:51][root][INFO] - Training Epoch: 1/2, step 3470/7134 completed (loss: 0.015947287902235985, acc: 0.9953917264938354)
[2025-02-13 02:55:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:52][root][INFO] - Training Epoch: 1/2, step 3471/7134 completed (loss: 0.07633205503225327, acc: 0.988950252532959)
[2025-02-13 02:55:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:52][root][INFO] - Training Epoch: 1/2, step 3472/7134 completed (loss: 0.023708881810307503, acc: 0.9940387606620789)
[2025-02-13 02:55:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:52][root][INFO] - Training Epoch: 1/2, step 3473/7134 completed (loss: 0.029940713196992874, acc: 0.9917218685150146)
[2025-02-13 02:55:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:53][root][INFO] - Training Epoch: 1/2, step 3474/7134 completed (loss: 0.06456630676984787, acc: 0.9893993139266968)
[2025-02-13 02:55:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:53][root][INFO] - Training Epoch: 1/2, step 3475/7134 completed (loss: 0.03637261688709259, acc: 0.9946714043617249)
[2025-02-13 02:55:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:54][root][INFO] - Training Epoch: 1/2, step 3476/7134 completed (loss: 0.06662662327289581, acc: 0.9811643958091736)
[2025-02-13 02:55:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:54][root][INFO] - Training Epoch: 1/2, step 3477/7134 completed (loss: 0.040284663438797, acc: 0.9901315569877625)
[2025-02-13 02:55:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:55][root][INFO] - Training Epoch: 1/2, step 3478/7134 completed (loss: 0.027637718245387077, acc: 0.9955089688301086)
[2025-02-13 02:55:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:55][root][INFO] - Training Epoch: 1/2, step 3479/7134 completed (loss: 0.007671000435948372, acc: 0.998487114906311)
[2025-02-13 02:55:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:55][root][INFO] - Training Epoch: 1/2, step 3480/7134 completed (loss: 0.039754536002874374, acc: 0.9904631972312927)
[2025-02-13 02:55:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:56][root][INFO] - Training Epoch: 1/2, step 3481/7134 completed (loss: 0.03514499589800835, acc: 0.9929078221321106)
[2025-02-13 02:55:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:56][root][INFO] - Training Epoch: 1/2, step 3482/7134 completed (loss: 0.04291156306862831, acc: 0.9874100685119629)
[2025-02-13 02:55:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:57][root][INFO] - Training Epoch: 1/2, step 3483/7134 completed (loss: 0.011665171012282372, acc: 0.9969183206558228)
[2025-02-13 02:55:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:57][root][INFO] - Training Epoch: 1/2, step 3484/7134 completed (loss: 0.03231550380587578, acc: 0.99262535572052)
[2025-02-13 02:55:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:58][root][INFO] - Training Epoch: 1/2, step 3485/7134 completed (loss: 0.03665345162153244, acc: 0.9898989796638489)
[2025-02-13 02:55:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:58][root][INFO] - Training Epoch: 1/2, step 3486/7134 completed (loss: 0.022767966613173485, acc: 0.9946042895317078)
[2025-02-13 02:55:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:58][root][INFO] - Training Epoch: 1/2, step 3487/7134 completed (loss: 0.04214517027139664, acc: 0.9852724671363831)
[2025-02-13 02:55:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:59][root][INFO] - Training Epoch: 1/2, step 3488/7134 completed (loss: 0.028527401387691498, acc: 0.9950658082962036)
[2025-02-13 02:55:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:55:59][root][INFO] - Training Epoch: 1/2, step 3489/7134 completed (loss: 0.07545971870422363, acc: 0.9815837740898132)
[2025-02-13 02:56:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:00][root][INFO] - Training Epoch: 1/2, step 3490/7134 completed (loss: 0.08797681331634521, acc: 0.9837518334388733)
[2025-02-13 02:56:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:00][root][INFO] - Training Epoch: 1/2, step 3491/7134 completed (loss: 0.054009728133678436, acc: 0.9843971729278564)
[2025-02-13 02:56:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:01][root][INFO] - Training Epoch: 1/2, step 3492/7134 completed (loss: 0.04177285358309746, acc: 0.987679660320282)
[2025-02-13 02:56:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:01][root][INFO] - Training Epoch: 1/2, step 3493/7134 completed (loss: 0.019960958510637283, acc: 0.9915966391563416)
[2025-02-13 02:56:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:01][root][INFO] - Training Epoch: 1/2, step 3494/7134 completed (loss: 0.029202619567513466, acc: 0.9948275685310364)
[2025-02-13 02:56:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:02][root][INFO] - Training Epoch: 1/2, step 3495/7134 completed (loss: 0.014641827903687954, acc: 0.9952038526535034)
[2025-02-13 02:56:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:02][root][INFO] - Training Epoch: 1/2, step 3496/7134 completed (loss: 0.025781936943531036, acc: 0.9887387156486511)
[2025-02-13 02:56:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:03][root][INFO] - Training Epoch: 1/2, step 3497/7134 completed (loss: 0.04532000422477722, acc: 0.9881305694580078)
[2025-02-13 02:56:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:03][root][INFO] - Training Epoch: 1/2, step 3498/7134 completed (loss: 0.042271293699741364, acc: 0.9878048896789551)
[2025-02-13 02:56:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:04][root][INFO] - Training Epoch: 1/2, step 3499/7134 completed (loss: 0.026076868176460266, acc: 0.9951377511024475)
[2025-02-13 02:56:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:04][root][INFO] - Training Epoch: 1/2, step 3500/7134 completed (loss: 0.04484891518950462, acc: 0.98531574010849)
[2025-02-13 02:56:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:04][root][INFO] - Training Epoch: 1/2, step 3501/7134 completed (loss: 0.08949302136898041, acc: 0.9718309640884399)
[2025-02-13 02:56:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:05][root][INFO] - Training Epoch: 1/2, step 3502/7134 completed (loss: 0.0950680747628212, acc: 0.9736841917037964)
[2025-02-13 02:56:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:05][root][INFO] - Training Epoch: 1/2, step 3503/7134 completed (loss: 0.06733588129281998, acc: 0.9757365584373474)
[2025-02-13 02:56:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:06][root][INFO] - Training Epoch: 1/2, step 3504/7134 completed (loss: 0.017302105203270912, acc: 0.9969834089279175)
[2025-02-13 02:56:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:06][root][INFO] - Training Epoch: 1/2, step 3505/7134 completed (loss: 0.0277432631701231, acc: 0.9874776601791382)
[2025-02-13 02:56:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:07][root][INFO] - Training Epoch: 1/2, step 3506/7134 completed (loss: 0.03785588592290878, acc: 0.9905481934547424)
[2025-02-13 02:56:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:07][root][INFO] - Training Epoch: 1/2, step 3507/7134 completed (loss: 0.01285844761878252, acc: 0.9933775067329407)
[2025-02-13 02:56:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:07][root][INFO] - Training Epoch: 1/2, step 3508/7134 completed (loss: 0.037051934748888016, acc: 0.9926900863647461)
[2025-02-13 02:56:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:08][root][INFO] - Training Epoch: 1/2, step 3509/7134 completed (loss: 0.02419031783938408, acc: 0.9903314709663391)
[2025-02-13 02:56:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:08][root][INFO] - Training Epoch: 1/2, step 3510/7134 completed (loss: 0.058015014976263046, acc: 0.9879931211471558)
[2025-02-13 02:56:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:09][root][INFO] - Training Epoch: 1/2, step 3511/7134 completed (loss: 0.00580488471314311, acc: 0.9981982111930847)
[2025-02-13 02:56:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:09][root][INFO] - Training Epoch: 1/2, step 3512/7134 completed (loss: 0.04541026055812836, acc: 0.9857369065284729)
[2025-02-13 02:56:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:10][root][INFO] - Training Epoch: 1/2, step 3513/7134 completed (loss: 0.04510721191763878, acc: 0.9893805384635925)
[2025-02-13 02:56:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:10][root][INFO] - Training Epoch: 1/2, step 3514/7134 completed (loss: 0.02621600404381752, acc: 0.9942396283149719)
[2025-02-13 02:56:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:10][root][INFO] - Training Epoch: 1/2, step 3515/7134 completed (loss: 0.013013295829296112, acc: 0.994413435459137)
[2025-02-13 02:56:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:11][root][INFO] - Training Epoch: 1/2, step 3516/7134 completed (loss: 0.014500847086310387, acc: 0.9945828914642334)
[2025-02-13 02:56:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:11][root][INFO] - Training Epoch: 1/2, step 3517/7134 completed (loss: 0.011660045944154263, acc: 0.9976162314414978)
[2025-02-13 02:56:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:12][root][INFO] - Training Epoch: 1/2, step 3518/7134 completed (loss: 0.043725356459617615, acc: 0.9889094233512878)
[2025-02-13 02:56:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:12][root][INFO] - Training Epoch: 1/2, step 3519/7134 completed (loss: 0.018573658540844917, acc: 0.9955056309700012)
[2025-02-13 02:56:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:13][root][INFO] - Training Epoch: 1/2, step 3520/7134 completed (loss: 0.057483166456222534, acc: 0.9870588183403015)
[2025-02-13 02:56:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:13][root][INFO] - Training Epoch: 1/2, step 3521/7134 completed (loss: 0.011816009879112244, acc: 0.9943100810050964)
[2025-02-13 02:56:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:14][root][INFO] - Training Epoch: 1/2, step 3522/7134 completed (loss: 0.04194901883602142, acc: 0.9866310358047485)
[2025-02-13 02:56:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:14][root][INFO] - Training Epoch: 1/2, step 3523/7134 completed (loss: 0.00701692933216691, acc: 1.0)
[2025-02-13 02:56:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:14][root][INFO] - Training Epoch: 1/2, step 3524/7134 completed (loss: 0.03001645766198635, acc: 0.9908854365348816)
[2025-02-13 02:56:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:15][root][INFO] - Training Epoch: 1/2, step 3525/7134 completed (loss: 0.018592216074466705, acc: 0.9950576424598694)
[2025-02-13 02:56:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:15][root][INFO] - Training Epoch: 1/2, step 3526/7134 completed (loss: 0.029019296169281006, acc: 0.9949302673339844)
[2025-02-13 02:56:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:16][root][INFO] - Training Epoch: 1/2, step 3527/7134 completed (loss: 0.0221039280295372, acc: 0.99615877866745)
[2025-02-13 02:56:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:16][root][INFO] - Training Epoch: 1/2, step 3528/7134 completed (loss: 0.008282299153506756, acc: 0.9976931810379028)
[2025-02-13 02:56:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:17][root][INFO] - Training Epoch: 1/2, step 3529/7134 completed (loss: 0.014091606251895428, acc: 0.9952940940856934)
[2025-02-13 02:56:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:17][root][INFO] - Training Epoch: 1/2, step 3530/7134 completed (loss: 0.021739952266216278, acc: 0.9931192398071289)
[2025-02-13 02:56:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:18][root][INFO] - Training Epoch: 1/2, step 3531/7134 completed (loss: 0.020881667733192444, acc: 0.9954819083213806)
[2025-02-13 02:56:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:18][root][INFO] - Training Epoch: 1/2, step 3532/7134 completed (loss: 0.005190655589103699, acc: 1.0)
[2025-02-13 02:56:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:19][root][INFO] - Training Epoch: 1/2, step 3533/7134 completed (loss: 0.018045587465167046, acc: 0.9941792488098145)
[2025-02-13 02:56:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:19][root][INFO] - Training Epoch: 1/2, step 3534/7134 completed (loss: 0.03311343491077423, acc: 0.9930459260940552)
[2025-02-13 02:56:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:19][root][INFO] - Training Epoch: 1/2, step 3535/7134 completed (loss: 0.036931801587343216, acc: 0.9938555955886841)
[2025-02-13 02:56:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:20][root][INFO] - Training Epoch: 1/2, step 3536/7134 completed (loss: 0.05202232301235199, acc: 0.9863387942314148)
[2025-02-13 02:56:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:20][root][INFO] - Training Epoch: 1/2, step 3537/7134 completed (loss: 0.03261581063270569, acc: 0.9899328947067261)
[2025-02-13 02:56:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:21][root][INFO] - Training Epoch: 1/2, step 3538/7134 completed (loss: 0.06357898563146591, acc: 0.9885931611061096)
[2025-02-13 02:56:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:21][root][INFO] - Training Epoch: 1/2, step 3539/7134 completed (loss: 0.024384992197155952, acc: 0.987864077091217)
[2025-02-13 02:56:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:21][root][INFO] - Training Epoch: 1/2, step 3540/7134 completed (loss: 0.05695519596338272, acc: 0.9787557125091553)
[2025-02-13 02:56:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:22][root][INFO] - Training Epoch: 1/2, step 3541/7134 completed (loss: 0.042927470058202744, acc: 0.9838472604751587)
[2025-02-13 02:56:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:22][root][INFO] - Training Epoch: 1/2, step 3542/7134 completed (loss: 0.033271756023168564, acc: 0.9876922965049744)
[2025-02-13 02:56:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:23][root][INFO] - Training Epoch: 1/2, step 3543/7134 completed (loss: 0.04936832934617996, acc: 0.9841549396514893)
[2025-02-13 02:56:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:23][root][INFO] - Training Epoch: 1/2, step 3544/7134 completed (loss: 0.053501781076192856, acc: 0.9830769300460815)
[2025-02-13 02:56:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:24][root][INFO] - Training Epoch: 1/2, step 3545/7134 completed (loss: 0.03377611190080643, acc: 0.9922360181808472)
[2025-02-13 02:56:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:24][root][INFO] - Training Epoch: 1/2, step 3546/7134 completed (loss: 0.03281884267926216, acc: 0.9884488582611084)
[2025-02-13 02:56:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:24][root][INFO] - Training Epoch: 1/2, step 3547/7134 completed (loss: 0.05505578592419624, acc: 0.9818181991577148)
[2025-02-13 02:56:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:25][root][INFO] - Training Epoch: 1/2, step 3548/7134 completed (loss: 0.03548668697476387, acc: 0.9933628439903259)
[2025-02-13 02:56:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:25][root][INFO] - Training Epoch: 1/2, step 3549/7134 completed (loss: 0.05140853300690651, acc: 0.9823633432388306)
[2025-02-13 02:56:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:26][root][INFO] - Training Epoch: 1/2, step 3550/7134 completed (loss: 0.029742984101176262, acc: 0.985989511013031)
[2025-02-13 02:56:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:26][root][INFO] - Training Epoch: 1/2, step 3551/7134 completed (loss: 0.02635926567018032, acc: 0.9898989796638489)
[2025-02-13 02:56:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:27][root][INFO] - Training Epoch: 1/2, step 3552/7134 completed (loss: 0.12816563248634338, acc: 0.9644194841384888)
[2025-02-13 02:56:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:27][root][INFO] - Training Epoch: 1/2, step 3553/7134 completed (loss: 0.07535354793071747, acc: 0.9835526347160339)
[2025-02-13 02:56:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:27][root][INFO] - Training Epoch: 1/2, step 3554/7134 completed (loss: 0.059087470173835754, acc: 0.9879518151283264)
[2025-02-13 02:56:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:28][root][INFO] - Training Epoch: 1/2, step 3555/7134 completed (loss: 0.036756061017513275, acc: 0.9905808568000793)
[2025-02-13 02:56:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:28][root][INFO] - Training Epoch: 1/2, step 3556/7134 completed (loss: 0.0372554287314415, acc: 0.9942418336868286)
[2025-02-13 02:56:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:29][root][INFO] - Training Epoch: 1/2, step 3557/7134 completed (loss: 0.04457036778330803, acc: 0.9909256100654602)
[2025-02-13 02:56:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:29][root][INFO] - Training Epoch: 1/2, step 3558/7134 completed (loss: 0.055905453860759735, acc: 0.9896907210350037)
[2025-02-13 02:56:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:30][root][INFO] - Training Epoch: 1/2, step 3559/7134 completed (loss: 0.02303023263812065, acc: 0.992732584476471)
[2025-02-13 02:56:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:30][root][INFO] - Training Epoch: 1/2, step 3560/7134 completed (loss: 0.017846351489424706, acc: 0.9931507110595703)
[2025-02-13 02:56:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:30][root][INFO] - Training Epoch: 1/2, step 3561/7134 completed (loss: 0.041949689388275146, acc: 0.9859943985939026)
[2025-02-13 02:56:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:31][root][INFO] - Training Epoch: 1/2, step 3562/7134 completed (loss: 0.0379183255136013, acc: 0.9857142567634583)
[2025-02-13 02:56:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:31][root][INFO] - Training Epoch: 1/2, step 3563/7134 completed (loss: 0.02655443176627159, acc: 0.9904610514640808)
[2025-02-13 02:56:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:32][root][INFO] - Training Epoch: 1/2, step 3564/7134 completed (loss: 0.06709017604589462, acc: 0.9868420958518982)
[2025-02-13 02:56:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:32][root][INFO] - Training Epoch: 1/2, step 3565/7134 completed (loss: 0.08972163498401642, acc: 0.9718044996261597)
[2025-02-13 02:56:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:56:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:57:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:58:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 02:59:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:31][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.0531, device='cuda:0') eval_epoch_loss=tensor(0.0518, device='cuda:0') eval_epoch_acc=tensor(0.9854, device='cuda:0')
[2025-02-13 03:00:31][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-13 03:00:31][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-13 03:00:31][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme/asr_epoch_1_step_3566_loss_0.05175735428929329/model.pt
[2025-02-13 03:00:31][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme directory
[2025-02-13 03:00:31][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.05175735428929329
[2025-02-13 03:00:31][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.985428512096405
[2025-02-13 03:00:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:32][root][INFO] - Training Epoch: 1/2, step 3566/7134 completed (loss: 0.061600372195243835, acc: 0.9820716977119446)
[2025-02-13 03:00:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:32][root][INFO] - Training Epoch: 1/2, step 3567/7134 completed (loss: 0.09185537695884705, acc: 0.9793014526367188)
[2025-02-13 03:00:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:33][root][INFO] - Training Epoch: 1/2, step 3568/7134 completed (loss: 0.10055478662252426, acc: 0.9738134145736694)
[2025-02-13 03:00:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:33][root][INFO] - Training Epoch: 1/2, step 3569/7134 completed (loss: 0.05439218878746033, acc: 0.9856630563735962)
[2025-02-13 03:00:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:34][root][INFO] - Training Epoch: 1/2, step 3570/7134 completed (loss: 0.08952944725751877, acc: 0.9818181991577148)
[2025-02-13 03:00:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:34][root][INFO] - Training Epoch: 1/2, step 3571/7134 completed (loss: 0.08294256031513214, acc: 0.9801061153411865)
[2025-02-13 03:00:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:34][root][INFO] - Training Epoch: 1/2, step 3572/7134 completed (loss: 0.040416374802589417, acc: 0.9881423115730286)
[2025-02-13 03:00:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:35][root][INFO] - Training Epoch: 1/2, step 3573/7134 completed (loss: 0.059756774455308914, acc: 0.9837037324905396)
[2025-02-13 03:00:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:35][root][INFO] - Training Epoch: 1/2, step 3574/7134 completed (loss: 0.05572284385561943, acc: 0.977011501789093)
[2025-02-13 03:00:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:36][root][INFO] - Training Epoch: 1/2, step 3575/7134 completed (loss: 0.12064532935619354, acc: 0.9639794230461121)
[2025-02-13 03:00:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:36][root][INFO] - Training Epoch: 1/2, step 3576/7134 completed (loss: 0.10158439725637436, acc: 0.9668508172035217)
[2025-02-13 03:00:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:37][root][INFO] - Training Epoch: 1/2, step 3577/7134 completed (loss: 0.05179795250296593, acc: 0.984829306602478)
[2025-02-13 03:00:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:37][root][INFO] - Training Epoch: 1/2, step 3578/7134 completed (loss: 0.04125375300645828, acc: 0.983988344669342)
[2025-02-13 03:00:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:38][root][INFO] - Training Epoch: 1/2, step 3579/7134 completed (loss: 0.055810876190662384, acc: 0.9853333234786987)
[2025-02-13 03:00:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:38][root][INFO] - Training Epoch: 1/2, step 3580/7134 completed (loss: 0.08017236739397049, acc: 0.9748031497001648)
[2025-02-13 03:00:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:38][root][INFO] - Training Epoch: 1/2, step 3581/7134 completed (loss: 0.07365764677524567, acc: 0.9779086709022522)
[2025-02-13 03:00:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:39][root][INFO] - Training Epoch: 1/2, step 3582/7134 completed (loss: 0.06471223384141922, acc: 0.9857819676399231)
[2025-02-13 03:00:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:39][root][INFO] - Training Epoch: 1/2, step 3583/7134 completed (loss: 0.06951281428337097, acc: 0.9802431464195251)
[2025-02-13 03:00:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:40][root][INFO] - Training Epoch: 1/2, step 3584/7134 completed (loss: 0.08203922212123871, acc: 0.9814814925193787)
[2025-02-13 03:00:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:40][root][INFO] - Training Epoch: 1/2, step 3585/7134 completed (loss: 0.06248319894075394, acc: 0.984375)
[2025-02-13 03:00:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:40][root][INFO] - Training Epoch: 1/2, step 3586/7134 completed (loss: 0.0783986747264862, acc: 0.9775280952453613)
[2025-02-13 03:00:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:41][root][INFO] - Training Epoch: 1/2, step 3587/7134 completed (loss: 0.061927177011966705, acc: 0.9813874959945679)
[2025-02-13 03:00:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:41][root][INFO] - Training Epoch: 1/2, step 3588/7134 completed (loss: 0.08476515859365463, acc: 0.973372757434845)
[2025-02-13 03:00:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:42][root][INFO] - Training Epoch: 1/2, step 3589/7134 completed (loss: 0.07489269226789474, acc: 0.9833585619926453)
[2025-02-13 03:00:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:42][root][INFO] - Training Epoch: 1/2, step 3590/7134 completed (loss: 0.024380896240472794, acc: 0.9943820238113403)
[2025-02-13 03:00:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:43][root][INFO] - Training Epoch: 1/2, step 3591/7134 completed (loss: 0.04193847253918648, acc: 0.9864864945411682)
[2025-02-13 03:00:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:43][root][INFO] - Training Epoch: 1/2, step 3592/7134 completed (loss: 0.024801891297101974, acc: 0.9933884143829346)
[2025-02-13 03:00:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:44][root][INFO] - Training Epoch: 1/2, step 3593/7134 completed (loss: 0.06106075644493103, acc: 0.985989511013031)
[2025-02-13 03:00:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:44][root][INFO] - Training Epoch: 1/2, step 3594/7134 completed (loss: 0.04001230001449585, acc: 0.9853895902633667)
[2025-02-13 03:00:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:44][root][INFO] - Training Epoch: 1/2, step 3595/7134 completed (loss: 0.043886397033929825, acc: 0.9877488613128662)
[2025-02-13 03:00:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:45][root][INFO] - Training Epoch: 1/2, step 3596/7134 completed (loss: 0.040343500673770905, acc: 0.9917469024658203)
[2025-02-13 03:00:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:45][root][INFO] - Training Epoch: 1/2, step 3597/7134 completed (loss: 0.04673605039715767, acc: 0.9884892106056213)
[2025-02-13 03:00:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:46][root][INFO] - Training Epoch: 1/2, step 3598/7134 completed (loss: 0.06590759754180908, acc: 0.9828375577926636)
[2025-02-13 03:00:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:46][root][INFO] - Training Epoch: 1/2, step 3599/7134 completed (loss: 0.06730223447084427, acc: 0.9810844659805298)
[2025-02-13 03:00:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:47][root][INFO] - Training Epoch: 1/2, step 3600/7134 completed (loss: 0.04733038321137428, acc: 0.9876543283462524)
[2025-02-13 03:00:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:47][root][INFO] - Training Epoch: 1/2, step 3601/7134 completed (loss: 0.12639018893241882, acc: 0.9627586007118225)
[2025-02-13 03:00:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:48][root][INFO] - Training Epoch: 1/2, step 3602/7134 completed (loss: 0.09619683772325516, acc: 0.9743290543556213)
[2025-02-13 03:00:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:48][root][INFO] - Training Epoch: 1/2, step 3603/7134 completed (loss: 0.04135477542877197, acc: 0.984375)
[2025-02-13 03:00:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:48][root][INFO] - Training Epoch: 1/2, step 3604/7134 completed (loss: 0.0391826331615448, acc: 0.9812382459640503)
[2025-02-13 03:00:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:49][root][INFO] - Training Epoch: 1/2, step 3605/7134 completed (loss: 0.055784814059734344, acc: 0.9875776171684265)
[2025-02-13 03:00:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:49][root][INFO] - Training Epoch: 1/2, step 3606/7134 completed (loss: 0.08788325637578964, acc: 0.980461835861206)
[2025-02-13 03:00:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:50][root][INFO] - Training Epoch: 1/2, step 3607/7134 completed (loss: 0.04335857927799225, acc: 0.9882550239562988)
[2025-02-13 03:00:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:50][root][INFO] - Training Epoch: 1/2, step 3608/7134 completed (loss: 0.088116854429245, acc: 0.982300877571106)
[2025-02-13 03:00:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:51][root][INFO] - Training Epoch: 1/2, step 3609/7134 completed (loss: 0.056225553154945374, acc: 0.9873096346855164)
[2025-02-13 03:00:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:51][root][INFO] - Training Epoch: 1/2, step 3610/7134 completed (loss: 0.08166173100471497, acc: 0.9727272987365723)
[2025-02-13 03:00:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:52][root][INFO] - Training Epoch: 1/2, step 3611/7134 completed (loss: 0.06939377635717392, acc: 0.975806474685669)
[2025-02-13 03:00:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:52][root][INFO] - Training Epoch: 1/2, step 3612/7134 completed (loss: 0.09469567239284515, acc: 0.9730290174484253)
[2025-02-13 03:00:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:53][root][INFO] - Training Epoch: 1/2, step 3613/7134 completed (loss: 0.05802435055375099, acc: 0.9821882843971252)
[2025-02-13 03:00:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:53][root][INFO] - Training Epoch: 1/2, step 3614/7134 completed (loss: 0.04899809509515762, acc: 0.9825970530509949)
[2025-02-13 03:00:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:54][root][INFO] - Training Epoch: 1/2, step 3615/7134 completed (loss: 0.03849881887435913, acc: 0.9862385392189026)
[2025-02-13 03:00:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:54][root][INFO] - Training Epoch: 1/2, step 3616/7134 completed (loss: 0.05078502744436264, acc: 0.9811617136001587)
[2025-02-13 03:00:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:54][root][INFO] - Training Epoch: 1/2, step 3617/7134 completed (loss: 0.0676722377538681, acc: 0.9851484894752502)
[2025-02-13 03:00:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:55][root][INFO] - Training Epoch: 1/2, step 3618/7134 completed (loss: 0.11833538860082626, acc: 0.9624183177947998)
[2025-02-13 03:00:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:55][root][INFO] - Training Epoch: 1/2, step 3619/7134 completed (loss: 0.09555761516094208, acc: 0.9702380895614624)
[2025-02-13 03:00:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:56][root][INFO] - Training Epoch: 1/2, step 3620/7134 completed (loss: 0.07596025615930557, acc: 0.9803094267845154)
[2025-02-13 03:00:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:56][root][INFO] - Training Epoch: 1/2, step 3621/7134 completed (loss: 0.04265698418021202, acc: 0.9833564758300781)
[2025-02-13 03:00:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:57][root][INFO] - Training Epoch: 1/2, step 3622/7134 completed (loss: 0.04120548814535141, acc: 0.9879879951477051)
[2025-02-13 03:00:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:57][root][INFO] - Training Epoch: 1/2, step 3623/7134 completed (loss: 0.03858631104230881, acc: 0.9814126491546631)
[2025-02-13 03:00:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:57][root][INFO] - Training Epoch: 1/2, step 3624/7134 completed (loss: 0.043697554618120193, acc: 0.9883333444595337)
[2025-02-13 03:00:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:58][root][INFO] - Training Epoch: 1/2, step 3625/7134 completed (loss: 0.06980817764997482, acc: 0.9798792600631714)
[2025-02-13 03:00:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:58][root][INFO] - Training Epoch: 1/2, step 3626/7134 completed (loss: 0.20812784135341644, acc: 0.9546666741371155)
[2025-02-13 03:00:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:59][root][INFO] - Training Epoch: 1/2, step 3627/7134 completed (loss: 0.08820214867591858, acc: 0.9739952683448792)
[2025-02-13 03:00:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:00:59][root][INFO] - Training Epoch: 1/2, step 3628/7134 completed (loss: 0.14571541547775269, acc: 0.9630681872367859)
[2025-02-13 03:00:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:00][root][INFO] - Training Epoch: 1/2, step 3629/7134 completed (loss: 0.11816107481718063, acc: 0.9674796462059021)
[2025-02-13 03:01:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:00][root][INFO] - Training Epoch: 1/2, step 3630/7134 completed (loss: 0.09849502891302109, acc: 0.9741601943969727)
[2025-02-13 03:01:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:00][root][INFO] - Training Epoch: 1/2, step 3631/7134 completed (loss: 0.07805673032999039, acc: 0.9786885380744934)
[2025-02-13 03:01:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:01][root][INFO] - Training Epoch: 1/2, step 3632/7134 completed (loss: 0.0724121481180191, acc: 0.9778671860694885)
[2025-02-13 03:01:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:01][root][INFO] - Training Epoch: 1/2, step 3633/7134 completed (loss: 0.08965884149074554, acc: 0.9799138903617859)
[2025-02-13 03:01:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:02][root][INFO] - Training Epoch: 1/2, step 3634/7134 completed (loss: 0.10255751758813858, acc: 0.9858299493789673)
[2025-02-13 03:01:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:02][root][INFO] - Training Epoch: 1/2, step 3635/7134 completed (loss: 0.03129160776734352, acc: 0.9892473220825195)
[2025-02-13 03:01:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:02][root][INFO] - Training Epoch: 1/2, step 3636/7134 completed (loss: 0.03734087198972702, acc: 0.9881889820098877)
[2025-02-13 03:01:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:03][root][INFO] - Training Epoch: 1/2, step 3637/7134 completed (loss: 0.0454009585082531, acc: 0.9919517040252686)
[2025-02-13 03:01:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:03][root][INFO] - Training Epoch: 1/2, step 3638/7134 completed (loss: 0.03037860617041588, acc: 0.9905882477760315)
[2025-02-13 03:01:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:04][root][INFO] - Training Epoch: 1/2, step 3639/7134 completed (loss: 0.021350178867578506, acc: 0.9932432174682617)
[2025-02-13 03:01:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:04][root][INFO] - Training Epoch: 1/2, step 3640/7134 completed (loss: 0.05014006793498993, acc: 0.9847328066825867)
[2025-02-13 03:01:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:04][root][INFO] - Training Epoch: 1/2, step 3641/7134 completed (loss: 0.04290192946791649, acc: 0.9922178983688354)
[2025-02-13 03:01:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:05][root][INFO] - Training Epoch: 1/2, step 3642/7134 completed (loss: 0.04851813241839409, acc: 0.989180862903595)
[2025-02-13 03:01:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:05][root][INFO] - Training Epoch: 1/2, step 3643/7134 completed (loss: 0.03747611865401268, acc: 0.993127167224884)
[2025-02-13 03:01:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:06][root][INFO] - Training Epoch: 1/2, step 3644/7134 completed (loss: 0.06035903841257095, acc: 0.9853658676147461)
[2025-02-13 03:01:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:06][root][INFO] - Training Epoch: 1/2, step 3645/7134 completed (loss: 0.052913323044776917, acc: 0.9911971688270569)
[2025-02-13 03:01:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:07][root][INFO] - Training Epoch: 1/2, step 3646/7134 completed (loss: 0.13011804223060608, acc: 0.9704861044883728)
[2025-02-13 03:01:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:07][root][INFO] - Training Epoch: 1/2, step 3647/7134 completed (loss: 0.021482964977622032, acc: 0.9948320388793945)
[2025-02-13 03:01:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:07][root][INFO] - Training Epoch: 1/2, step 3648/7134 completed (loss: 0.06618990749120712, acc: 0.9794238805770874)
[2025-02-13 03:01:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:08][root][INFO] - Training Epoch: 1/2, step 3649/7134 completed (loss: 0.08389502018690109, acc: 0.976190447807312)
[2025-02-13 03:01:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:08][root][INFO] - Training Epoch: 1/2, step 3650/7134 completed (loss: 0.0581844300031662, acc: 0.9869281053543091)
[2025-02-13 03:01:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:09][root][INFO] - Training Epoch: 1/2, step 3651/7134 completed (loss: 0.07358264923095703, acc: 0.9825737476348877)
[2025-02-13 03:01:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:09][root][INFO] - Training Epoch: 1/2, step 3652/7134 completed (loss: 0.0690673440694809, acc: 0.9803921580314636)
[2025-02-13 03:01:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:10][root][INFO] - Training Epoch: 1/2, step 3653/7134 completed (loss: 0.10193515568971634, acc: 0.9773755669593811)
[2025-02-13 03:01:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:10][root][INFO] - Training Epoch: 1/2, step 3654/7134 completed (loss: 0.024431563913822174, acc: 0.9896373152732849)
[2025-02-13 03:01:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:10][root][INFO] - Training Epoch: 1/2, step 3655/7134 completed (loss: 0.06426580250263214, acc: 0.9835164546966553)
[2025-02-13 03:01:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:11][root][INFO] - Training Epoch: 1/2, step 3656/7134 completed (loss: 0.09382393956184387, acc: 0.9818181991577148)
[2025-02-13 03:01:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:11][root][INFO] - Training Epoch: 1/2, step 3657/7134 completed (loss: 0.02424001321196556, acc: 0.9919871687889099)
[2025-02-13 03:01:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:12][root][INFO] - Training Epoch: 1/2, step 3658/7134 completed (loss: 0.0586429089307785, acc: 0.985981285572052)
[2025-02-13 03:01:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:12][root][INFO] - Training Epoch: 1/2, step 3659/7134 completed (loss: 0.04396762326359749, acc: 0.9835575222969055)
[2025-02-13 03:01:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:12][root][INFO] - Training Epoch: 1/2, step 3660/7134 completed (loss: 0.07326162606477737, acc: 0.9836065769195557)
[2025-02-13 03:01:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:13][root][INFO] - Training Epoch: 1/2, step 3661/7134 completed (loss: 0.06865391880273819, acc: 0.9785714149475098)
[2025-02-13 03:01:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:13][root][INFO] - Training Epoch: 1/2, step 3662/7134 completed (loss: 0.08357951045036316, acc: 0.9771341681480408)
[2025-02-13 03:01:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:14][root][INFO] - Training Epoch: 1/2, step 3663/7134 completed (loss: 0.049671661108732224, acc: 0.9853479862213135)
[2025-02-13 03:01:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:14][root][INFO] - Training Epoch: 1/2, step 3664/7134 completed (loss: 0.05999300628900528, acc: 0.9863221645355225)
[2025-02-13 03:01:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:15][root][INFO] - Training Epoch: 1/2, step 3665/7134 completed (loss: 0.03660585731267929, acc: 0.9934318661689758)
[2025-02-13 03:01:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:15][root][INFO] - Training Epoch: 1/2, step 3666/7134 completed (loss: 0.061353690922260284, acc: 0.9799270033836365)
[2025-02-13 03:01:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:15][root][INFO] - Training Epoch: 1/2, step 3667/7134 completed (loss: 0.1725168526172638, acc: 0.9589977264404297)
[2025-02-13 03:01:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:16][root][INFO] - Training Epoch: 1/2, step 3668/7134 completed (loss: 0.05319076403975487, acc: 0.9877836108207703)
[2025-02-13 03:01:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:16][root][INFO] - Training Epoch: 1/2, step 3669/7134 completed (loss: 0.033393047749996185, acc: 0.9867674708366394)
[2025-02-13 03:01:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:17][root][INFO] - Training Epoch: 1/2, step 3670/7134 completed (loss: 0.03465561196208, acc: 0.9917582273483276)
[2025-02-13 03:01:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:17][root][INFO] - Training Epoch: 1/2, step 3671/7134 completed (loss: 0.032545316964387894, acc: 0.9898843765258789)
[2025-02-13 03:01:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:17][root][INFO] - Training Epoch: 1/2, step 3672/7134 completed (loss: 0.024937937036156654, acc: 0.993686854839325)
[2025-02-13 03:01:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:18][root][INFO] - Training Epoch: 1/2, step 3673/7134 completed (loss: 0.045129451900720596, acc: 0.9849498271942139)
[2025-02-13 03:01:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:18][root][INFO] - Training Epoch: 1/2, step 3674/7134 completed (loss: 0.01921854168176651, acc: 0.9968503713607788)
[2025-02-13 03:01:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:19][root][INFO] - Training Epoch: 1/2, step 3675/7134 completed (loss: 0.01977965608239174, acc: 0.9948979616165161)
[2025-02-13 03:01:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:19][root][INFO] - Training Epoch: 1/2, step 3676/7134 completed (loss: 0.03573068603873253, acc: 0.991253674030304)
[2025-02-13 03:01:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:20][root][INFO] - Training Epoch: 1/2, step 3677/7134 completed (loss: 0.03654615953564644, acc: 0.9863013625144958)
[2025-02-13 03:01:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:20][root][INFO] - Training Epoch: 1/2, step 3678/7134 completed (loss: 0.04182972386479378, acc: 0.9851552248001099)
[2025-02-13 03:01:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:20][root][INFO] - Training Epoch: 1/2, step 3679/7134 completed (loss: 0.017408300191164017, acc: 0.9933884143829346)
[2025-02-13 03:01:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:21][root][INFO] - Training Epoch: 1/2, step 3680/7134 completed (loss: 0.057649165391922, acc: 0.9821428656578064)
[2025-02-13 03:01:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:21][root][INFO] - Training Epoch: 1/2, step 3681/7134 completed (loss: 0.029233738780021667, acc: 0.9902234673500061)
[2025-02-13 03:01:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:22][root][INFO] - Training Epoch: 1/2, step 3682/7134 completed (loss: 0.02494981326162815, acc: 0.992094874382019)
[2025-02-13 03:01:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:22][root][INFO] - Training Epoch: 1/2, step 3683/7134 completed (loss: 0.020963259041309357, acc: 0.9937008023262024)
[2025-02-13 03:01:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:23][root][INFO] - Training Epoch: 1/2, step 3684/7134 completed (loss: 0.038784995675086975, acc: 0.9898989796638489)
[2025-02-13 03:01:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:23][root][INFO] - Training Epoch: 1/2, step 3685/7134 completed (loss: 0.03873797878623009, acc: 0.9865125417709351)
[2025-02-13 03:01:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:23][root][INFO] - Training Epoch: 1/2, step 3686/7134 completed (loss: 0.025894032791256905, acc: 0.9895052313804626)
[2025-02-13 03:01:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:24][root][INFO] - Training Epoch: 1/2, step 3687/7134 completed (loss: 0.017353622242808342, acc: 0.9953846335411072)
[2025-02-13 03:01:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:24][root][INFO] - Training Epoch: 1/2, step 3688/7134 completed (loss: 0.01828230917453766, acc: 0.9918864369392395)
[2025-02-13 03:01:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:25][root][INFO] - Training Epoch: 1/2, step 3689/7134 completed (loss: 0.019186358898878098, acc: 0.9950980544090271)
[2025-02-13 03:01:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:25][root][INFO] - Training Epoch: 1/2, step 3690/7134 completed (loss: 0.02070160023868084, acc: 0.9928469061851501)
[2025-02-13 03:01:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:26][root][INFO] - Training Epoch: 1/2, step 3691/7134 completed (loss: 0.03840494528412819, acc: 0.9900744557380676)
[2025-02-13 03:01:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:26][root][INFO] - Training Epoch: 1/2, step 3692/7134 completed (loss: 0.04305790737271309, acc: 0.9817351698875427)
[2025-02-13 03:01:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:26][root][INFO] - Training Epoch: 1/2, step 3693/7134 completed (loss: 0.026525072753429413, acc: 0.988959014415741)
[2025-02-13 03:01:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:27][root][INFO] - Training Epoch: 1/2, step 3694/7134 completed (loss: 0.029239537194371223, acc: 0.991465151309967)
[2025-02-13 03:01:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:27][root][INFO] - Training Epoch: 1/2, step 3695/7134 completed (loss: 0.02909069135785103, acc: 0.9900850057601929)
[2025-02-13 03:01:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:28][root][INFO] - Training Epoch: 1/2, step 3696/7134 completed (loss: 0.02509472519159317, acc: 0.9910025596618652)
[2025-02-13 03:01:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:28][root][INFO] - Training Epoch: 1/2, step 3697/7134 completed (loss: 0.01783202774822712, acc: 0.991253674030304)
[2025-02-13 03:01:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:29][root][INFO] - Training Epoch: 1/2, step 3698/7134 completed (loss: 0.03367488831281662, acc: 0.9887640476226807)
[2025-02-13 03:01:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:29][root][INFO] - Training Epoch: 1/2, step 3699/7134 completed (loss: 0.020287666469812393, acc: 0.9932432174682617)
[2025-02-13 03:01:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:29][root][INFO] - Training Epoch: 1/2, step 3700/7134 completed (loss: 0.020059330388903618, acc: 0.9941691160202026)
[2025-02-13 03:01:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:30][root][INFO] - Training Epoch: 1/2, step 3701/7134 completed (loss: 0.038385167717933655, acc: 0.9867549538612366)
[2025-02-13 03:01:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:30][root][INFO] - Training Epoch: 1/2, step 3702/7134 completed (loss: 0.046522870659828186, acc: 0.9889807105064392)
[2025-02-13 03:01:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:31][root][INFO] - Training Epoch: 1/2, step 3703/7134 completed (loss: 0.025074372068047523, acc: 0.9948914647102356)
[2025-02-13 03:01:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:31][root][INFO] - Training Epoch: 1/2, step 3704/7134 completed (loss: 0.01712094247341156, acc: 0.991631805896759)
[2025-02-13 03:01:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:32][root][INFO] - Training Epoch: 1/2, step 3705/7134 completed (loss: 0.059631917625665665, acc: 0.9824841022491455)
[2025-02-13 03:01:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:32][root][INFO] - Training Epoch: 1/2, step 3706/7134 completed (loss: 0.09541400521993637, acc: 0.9778597950935364)
[2025-02-13 03:01:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:32][root][INFO] - Training Epoch: 1/2, step 3707/7134 completed (loss: 0.02083798125386238, acc: 0.991584837436676)
[2025-02-13 03:01:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:33][root][INFO] - Training Epoch: 1/2, step 3708/7134 completed (loss: 0.05661064013838768, acc: 0.9845238327980042)
[2025-02-13 03:01:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:33][root][INFO] - Training Epoch: 1/2, step 3709/7134 completed (loss: 0.031074753031134605, acc: 0.9929006099700928)
[2025-02-13 03:01:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:34][root][INFO] - Training Epoch: 1/2, step 3710/7134 completed (loss: 0.03418905287981033, acc: 0.9887295365333557)
[2025-02-13 03:01:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:34][root][INFO] - Training Epoch: 1/2, step 3711/7134 completed (loss: 0.03876788541674614, acc: 0.9863523840904236)
[2025-02-13 03:01:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:35][root][INFO] - Training Epoch: 1/2, step 3712/7134 completed (loss: 0.02368231490254402, acc: 0.9940263032913208)
[2025-02-13 03:01:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:35][root][INFO] - Training Epoch: 1/2, step 3713/7134 completed (loss: 0.04544397071003914, acc: 0.9917898178100586)
[2025-02-13 03:01:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:36][root][INFO] - Training Epoch: 1/2, step 3714/7134 completed (loss: 0.04286261275410652, acc: 0.9878453016281128)
[2025-02-13 03:01:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:36][root][INFO] - Training Epoch: 1/2, step 3715/7134 completed (loss: 0.021618250757455826, acc: 0.9927448630332947)
[2025-02-13 03:01:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:37][root][INFO] - Training Epoch: 1/2, step 3716/7134 completed (loss: 0.04533405229449272, acc: 0.9852034449577332)
[2025-02-13 03:01:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:37][root][INFO] - Training Epoch: 1/2, step 3717/7134 completed (loss: 0.042120859026908875, acc: 0.9890710115432739)
[2025-02-13 03:01:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:37][root][INFO] - Training Epoch: 1/2, step 3718/7134 completed (loss: 0.052911169826984406, acc: 0.9835873246192932)
[2025-02-13 03:01:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:38][root][INFO] - Training Epoch: 1/2, step 3719/7134 completed (loss: 0.060785528272390366, acc: 0.9852941036224365)
[2025-02-13 03:01:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:38][root][INFO] - Training Epoch: 1/2, step 3720/7134 completed (loss: 0.023162564262747765, acc: 0.992668628692627)
[2025-02-13 03:01:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:39][root][INFO] - Training Epoch: 1/2, step 3721/7134 completed (loss: 0.04965928569436073, acc: 0.9866844415664673)
[2025-02-13 03:01:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:39][root][INFO] - Training Epoch: 1/2, step 3722/7134 completed (loss: 0.025519154965877533, acc: 0.9926108121871948)
[2025-02-13 03:01:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:40][root][INFO] - Training Epoch: 1/2, step 3723/7134 completed (loss: 0.051936253905296326, acc: 0.9913669228553772)
[2025-02-13 03:01:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:40][root][INFO] - Training Epoch: 1/2, step 3724/7134 completed (loss: 0.03946428373456001, acc: 0.9870689511299133)
[2025-02-13 03:01:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:41][root][INFO] - Training Epoch: 1/2, step 3725/7134 completed (loss: 0.10996458679437637, acc: 0.9793510437011719)
[2025-02-13 03:01:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:41][root][INFO] - Training Epoch: 1/2, step 3726/7134 completed (loss: 0.026484327390789986, acc: 0.9925611019134521)
[2025-02-13 03:01:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:42][root][INFO] - Training Epoch: 1/2, step 3727/7134 completed (loss: 0.028166016563773155, acc: 0.9973297715187073)
[2025-02-13 03:01:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:42][root][INFO] - Training Epoch: 1/2, step 3728/7134 completed (loss: 0.013504068367183208, acc: 0.9954904317855835)
[2025-02-13 03:01:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:42][root][INFO] - Training Epoch: 1/2, step 3729/7134 completed (loss: 0.03245474025607109, acc: 0.9909090995788574)
[2025-02-13 03:01:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:43][root][INFO] - Training Epoch: 1/2, step 3730/7134 completed (loss: 0.0216352641582489, acc: 0.9932157397270203)
[2025-02-13 03:01:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:43][root][INFO] - Training Epoch: 1/2, step 3731/7134 completed (loss: 0.02230258472263813, acc: 0.9912827014923096)
[2025-02-13 03:01:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:44][root][INFO] - Training Epoch: 1/2, step 3732/7134 completed (loss: 0.026746327057480812, acc: 0.9909194111824036)
[2025-02-13 03:01:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:44][root][INFO] - Training Epoch: 1/2, step 3733/7134 completed (loss: 0.05327650532126427, acc: 0.9859747290611267)
[2025-02-13 03:01:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:45][root][INFO] - Training Epoch: 1/2, step 3734/7134 completed (loss: 0.04971161112189293, acc: 0.9822006225585938)
[2025-02-13 03:01:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:45][root][INFO] - Training Epoch: 1/2, step 3735/7134 completed (loss: 0.04838871210813522, acc: 0.9849397540092468)
[2025-02-13 03:01:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:46][root][INFO] - Training Epoch: 1/2, step 3736/7134 completed (loss: 0.10036614537239075, acc: 0.9773913025856018)
[2025-02-13 03:01:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:46][root][INFO] - Training Epoch: 1/2, step 3737/7134 completed (loss: 0.07485449314117432, acc: 0.9743589758872986)
[2025-02-13 03:01:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:46][root][INFO] - Training Epoch: 1/2, step 3738/7134 completed (loss: 0.02405812405049801, acc: 0.9933993220329285)
[2025-02-13 03:01:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:47][root][INFO] - Training Epoch: 1/2, step 3739/7134 completed (loss: 0.06560587882995605, acc: 0.983146071434021)
[2025-02-13 03:01:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:47][root][INFO] - Training Epoch: 1/2, step 3740/7134 completed (loss: 0.043019749224185944, acc: 0.9902597665786743)
[2025-02-13 03:01:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:48][root][INFO] - Training Epoch: 1/2, step 3741/7134 completed (loss: 0.04538598284125328, acc: 0.9914407730102539)
[2025-02-13 03:01:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:48][root][INFO] - Training Epoch: 1/2, step 3742/7134 completed (loss: 0.05682472139596939, acc: 0.9863247871398926)
[2025-02-13 03:01:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:48][root][INFO] - Training Epoch: 1/2, step 3743/7134 completed (loss: 0.055554259568452835, acc: 0.98525071144104)
[2025-02-13 03:01:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:49][root][INFO] - Training Epoch: 1/2, step 3744/7134 completed (loss: 0.07653122395277023, acc: 0.9815546870231628)
[2025-02-13 03:01:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:49][root][INFO] - Training Epoch: 1/2, step 3745/7134 completed (loss: 0.08009030669927597, acc: 0.9744779467582703)
[2025-02-13 03:01:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:50][root][INFO] - Training Epoch: 1/2, step 3746/7134 completed (loss: 0.06040608137845993, acc: 0.9829843044281006)
[2025-02-13 03:01:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:50][root][INFO] - Training Epoch: 1/2, step 3747/7134 completed (loss: 0.052563101053237915, acc: 0.9839650392532349)
[2025-02-13 03:01:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:50][root][INFO] - Training Epoch: 1/2, step 3748/7134 completed (loss: 0.07026463001966476, acc: 0.98591548204422)
[2025-02-13 03:01:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:51][root][INFO] - Training Epoch: 1/2, step 3749/7134 completed (loss: 0.08994246274232864, acc: 0.9730135202407837)
[2025-02-13 03:01:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:51][root][INFO] - Training Epoch: 1/2, step 3750/7134 completed (loss: 0.020534344017505646, acc: 0.9958158731460571)
[2025-02-13 03:01:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:52][root][INFO] - Training Epoch: 1/2, step 3751/7134 completed (loss: 0.05280131474137306, acc: 0.9843527674674988)
[2025-02-13 03:01:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:52][root][INFO] - Training Epoch: 1/2, step 3752/7134 completed (loss: 0.03401900455355644, acc: 0.9910447597503662)
[2025-02-13 03:01:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:53][root][INFO] - Training Epoch: 1/2, step 3753/7134 completed (loss: 0.10329408943653107, acc: 0.9730941653251648)
[2025-02-13 03:01:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:53][root][INFO] - Training Epoch: 1/2, step 3754/7134 completed (loss: 0.0275834072381258, acc: 0.9939302206039429)
[2025-02-13 03:01:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:53][root][INFO] - Training Epoch: 1/2, step 3755/7134 completed (loss: 0.04384995624423027, acc: 0.9907235503196716)
[2025-02-13 03:01:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:54][root][INFO] - Training Epoch: 1/2, step 3756/7134 completed (loss: 0.04544408619403839, acc: 0.990304708480835)
[2025-02-13 03:01:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:54][root][INFO] - Training Epoch: 1/2, step 3757/7134 completed (loss: 0.08512461185455322, acc: 0.977011501789093)
[2025-02-13 03:01:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:55][root][INFO] - Training Epoch: 1/2, step 3758/7134 completed (loss: 0.05528418719768524, acc: 0.9855855703353882)
[2025-02-13 03:01:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:55][root][INFO] - Training Epoch: 1/2, step 3759/7134 completed (loss: 0.05656712129712105, acc: 0.9847972989082336)
[2025-02-13 03:01:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:56][root][INFO] - Training Epoch: 1/2, step 3760/7134 completed (loss: 0.06984589248895645, acc: 0.9844632744789124)
[2025-02-13 03:01:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:56][root][INFO] - Training Epoch: 1/2, step 3761/7134 completed (loss: 0.029691917821764946, acc: 0.989180862903595)
[2025-02-13 03:01:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:56][root][INFO] - Training Epoch: 1/2, step 3762/7134 completed (loss: 0.03554806858301163, acc: 0.9862448573112488)
[2025-02-13 03:01:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:57][root][INFO] - Training Epoch: 1/2, step 3763/7134 completed (loss: 0.023364795371890068, acc: 0.9922839403152466)
[2025-02-13 03:01:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:57][root][INFO] - Training Epoch: 1/2, step 3764/7134 completed (loss: 0.055890653282403946, acc: 0.9863201379776001)
[2025-02-13 03:01:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:58][root][INFO] - Training Epoch: 1/2, step 3765/7134 completed (loss: 0.032422490417957306, acc: 0.9903961420059204)
[2025-02-13 03:01:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:58][root][INFO] - Training Epoch: 1/2, step 3766/7134 completed (loss: 0.07463625073432922, acc: 0.9843304753303528)
[2025-02-13 03:01:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:59][root][INFO] - Training Epoch: 1/2, step 3767/7134 completed (loss: 0.01946321316063404, acc: 0.9925187230110168)
[2025-02-13 03:01:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:01:59][root][INFO] - Training Epoch: 1/2, step 3768/7134 completed (loss: 0.02931240200996399, acc: 0.9937759041786194)
[2025-02-13 03:01:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:00][root][INFO] - Training Epoch: 1/2, step 3769/7134 completed (loss: 0.04600853845477104, acc: 0.983627200126648)
[2025-02-13 03:02:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:00][root][INFO] - Training Epoch: 1/2, step 3770/7134 completed (loss: 0.05038376897573471, acc: 0.9858757257461548)
[2025-02-13 03:02:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:01][root][INFO] - Training Epoch: 1/2, step 3771/7134 completed (loss: 0.028000574558973312, acc: 0.9935064911842346)
[2025-02-13 03:02:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:01][root][INFO] - Training Epoch: 1/2, step 3772/7134 completed (loss: 0.058490656316280365, acc: 0.980567991733551)
[2025-02-13 03:02:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:02][root][INFO] - Training Epoch: 1/2, step 3773/7134 completed (loss: 0.03915279358625412, acc: 0.9899553656578064)
[2025-02-13 03:02:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:02][root][INFO] - Training Epoch: 1/2, step 3774/7134 completed (loss: 0.04319372400641441, acc: 0.9918793439865112)
[2025-02-13 03:02:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:03][root][INFO] - Training Epoch: 1/2, step 3775/7134 completed (loss: 0.02349552884697914, acc: 0.9942693114280701)
[2025-02-13 03:02:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:03][root][INFO] - Training Epoch: 1/2, step 3776/7134 completed (loss: 0.03860852122306824, acc: 0.9866666793823242)
[2025-02-13 03:02:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:03][root][INFO] - Training Epoch: 1/2, step 3777/7134 completed (loss: 0.028450241312384605, acc: 0.9937343597412109)
[2025-02-13 03:02:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:04][root][INFO] - Training Epoch: 1/2, step 3778/7134 completed (loss: 0.06031683832406998, acc: 0.9843527674674988)
[2025-02-13 03:02:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:04][root][INFO] - Training Epoch: 1/2, step 3779/7134 completed (loss: 0.05273917317390442, acc: 0.9857549667358398)
[2025-02-13 03:02:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:05][root][INFO] - Training Epoch: 1/2, step 3780/7134 completed (loss: 0.02815934456884861, acc: 0.9899135231971741)
[2025-02-13 03:02:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:05][root][INFO] - Training Epoch: 1/2, step 3781/7134 completed (loss: 0.0712340921163559, acc: 0.9794238805770874)
[2025-02-13 03:02:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:05][root][INFO] - Training Epoch: 1/2, step 3782/7134 completed (loss: 0.0554250068962574, acc: 0.9829059839248657)
[2025-02-13 03:02:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:06][root][INFO] - Training Epoch: 1/2, step 3783/7134 completed (loss: 0.047589417546987534, acc: 0.9858064651489258)
[2025-02-13 03:02:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:06][root][INFO] - Training Epoch: 1/2, step 3784/7134 completed (loss: 0.04576822742819786, acc: 0.9876352548599243)
[2025-02-13 03:02:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:07][root][INFO] - Training Epoch: 1/2, step 3785/7134 completed (loss: 0.018016323447227478, acc: 0.9937597513198853)
[2025-02-13 03:02:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:07][root][INFO] - Training Epoch: 1/2, step 3786/7134 completed (loss: 0.04350719600915909, acc: 0.9870634078979492)
[2025-02-13 03:02:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:08][root][INFO] - Training Epoch: 1/2, step 3787/7134 completed (loss: 0.04030060023069382, acc: 0.9872408509254456)
[2025-02-13 03:02:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:08][root][INFO] - Training Epoch: 1/2, step 3788/7134 completed (loss: 0.04637701064348221, acc: 0.9865047335624695)
[2025-02-13 03:02:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:08][root][INFO] - Training Epoch: 1/2, step 3789/7134 completed (loss: 0.05664512515068054, acc: 0.9789029359817505)
[2025-02-13 03:02:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:09][root][INFO] - Training Epoch: 1/2, step 3790/7134 completed (loss: 0.09079805016517639, acc: 0.980988621711731)
[2025-02-13 03:02:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:09][root][INFO] - Training Epoch: 1/2, step 3791/7134 completed (loss: 0.026195429265499115, acc: 0.9948387145996094)
[2025-02-13 03:02:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:10][root][INFO] - Training Epoch: 1/2, step 3792/7134 completed (loss: 0.03798408806324005, acc: 0.9852216839790344)
[2025-02-13 03:02:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:10][root][INFO] - Training Epoch: 1/2, step 3793/7134 completed (loss: 0.043790318071842194, acc: 0.9891892075538635)
[2025-02-13 03:02:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:11][root][INFO] - Training Epoch: 1/2, step 3794/7134 completed (loss: 0.046755749732255936, acc: 0.9870298504829407)
[2025-02-13 03:02:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:11][root][INFO] - Training Epoch: 1/2, step 3795/7134 completed (loss: 0.0441751591861248, acc: 0.9867469668388367)
[2025-02-13 03:02:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:11][root][INFO] - Training Epoch: 1/2, step 3796/7134 completed (loss: 0.04974980652332306, acc: 0.983668327331543)
[2025-02-13 03:02:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:12][root][INFO] - Training Epoch: 1/2, step 3797/7134 completed (loss: 0.02216634340584278, acc: 0.9921773076057434)
[2025-02-13 03:02:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:12][root][INFO] - Training Epoch: 1/2, step 3798/7134 completed (loss: 0.07771003991365433, acc: 0.9798850417137146)
[2025-02-13 03:02:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:13][root][INFO] - Training Epoch: 1/2, step 3799/7134 completed (loss: 0.06644933670759201, acc: 0.9794079661369324)
[2025-02-13 03:02:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:13][root][INFO] - Training Epoch: 1/2, step 3800/7134 completed (loss: 0.05368300899863243, acc: 0.9892966151237488)
[2025-02-13 03:02:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:14][root][INFO] - Training Epoch: 1/2, step 3801/7134 completed (loss: 0.028719013556838036, acc: 0.9938271641731262)
[2025-02-13 03:02:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:14][root][INFO] - Training Epoch: 1/2, step 3802/7134 completed (loss: 0.030666686594486237, acc: 0.9929078221321106)
[2025-02-13 03:02:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:14][root][INFO] - Training Epoch: 1/2, step 3803/7134 completed (loss: 0.03888906538486481, acc: 0.9887955188751221)
[2025-02-13 03:02:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:15][root][INFO] - Training Epoch: 1/2, step 3804/7134 completed (loss: 0.04204733297228813, acc: 0.9875583052635193)
[2025-02-13 03:02:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:15][root][INFO] - Training Epoch: 1/2, step 3805/7134 completed (loss: 0.023157987743616104, acc: 0.9934554696083069)
[2025-02-13 03:02:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:16][root][INFO] - Training Epoch: 1/2, step 3806/7134 completed (loss: 0.06119254231452942, acc: 0.9822485446929932)
[2025-02-13 03:02:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:16][root][INFO] - Training Epoch: 1/2, step 3807/7134 completed (loss: 0.013344509527087212, acc: 0.9985380172729492)
[2025-02-13 03:02:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:17][root][INFO] - Training Epoch: 1/2, step 3808/7134 completed (loss: 0.04586397483944893, acc: 0.9886075854301453)
[2025-02-13 03:02:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:17][root][INFO] - Training Epoch: 1/2, step 3809/7134 completed (loss: 0.06179031729698181, acc: 0.9835025668144226)
[2025-02-13 03:02:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:17][root][INFO] - Training Epoch: 1/2, step 3810/7134 completed (loss: 0.038218218833208084, acc: 0.9910714030265808)
[2025-02-13 03:02:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:18][root][INFO] - Training Epoch: 1/2, step 3811/7134 completed (loss: 0.05363915115594864, acc: 0.9862068891525269)
[2025-02-13 03:02:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:18][root][INFO] - Training Epoch: 1/2, step 3812/7134 completed (loss: 0.049136657267808914, acc: 0.9777015447616577)
[2025-02-13 03:02:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:19][root][INFO] - Training Epoch: 1/2, step 3813/7134 completed (loss: 0.06389981508255005, acc: 0.980211079120636)
[2025-02-13 03:02:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:19][root][INFO] - Training Epoch: 1/2, step 3814/7134 completed (loss: 0.03080681711435318, acc: 0.9879356622695923)
[2025-02-13 03:02:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:20][root][INFO] - Training Epoch: 1/2, step 3815/7134 completed (loss: 0.037834957242012024, acc: 0.9896296262741089)
[2025-02-13 03:02:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:20][root][INFO] - Training Epoch: 1/2, step 3816/7134 completed (loss: 0.030312424525618553, acc: 0.9905362725257874)
[2025-02-13 03:02:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:21][root][INFO] - Training Epoch: 1/2, step 3817/7134 completed (loss: 0.055488504469394684, acc: 0.9832134246826172)
[2025-02-13 03:02:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:21][root][INFO] - Training Epoch: 1/2, step 3818/7134 completed (loss: 0.07958804816007614, acc: 0.9878787994384766)
[2025-02-13 03:02:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:21][root][INFO] - Training Epoch: 1/2, step 3819/7134 completed (loss: 0.10194573551416397, acc: 0.9718875288963318)
[2025-02-13 03:02:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:22][root][INFO] - Training Epoch: 1/2, step 3820/7134 completed (loss: 0.06456484645605087, acc: 0.9738717079162598)
[2025-02-13 03:02:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:22][root][INFO] - Training Epoch: 1/2, step 3821/7134 completed (loss: 0.03835359215736389, acc: 0.9864636063575745)
[2025-02-13 03:02:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:23][root][INFO] - Training Epoch: 1/2, step 3822/7134 completed (loss: 0.05443965271115303, acc: 0.9823182821273804)
[2025-02-13 03:02:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:23][root][INFO] - Training Epoch: 1/2, step 3823/7134 completed (loss: 0.020617948845028877, acc: 0.9944674968719482)
[2025-02-13 03:02:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:23][root][INFO] - Training Epoch: 1/2, step 3824/7134 completed (loss: 0.08236359804868698, acc: 0.9780775904655457)
[2025-02-13 03:02:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:24][root][INFO] - Training Epoch: 1/2, step 3825/7134 completed (loss: 0.03577430918812752, acc: 0.9860279560089111)
[2025-02-13 03:02:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:24][root][INFO] - Training Epoch: 1/2, step 3826/7134 completed (loss: 0.050487276166677475, acc: 0.9838235378265381)
[2025-02-13 03:02:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:25][root][INFO] - Training Epoch: 1/2, step 3827/7134 completed (loss: 0.044732317328453064, acc: 0.9887955188751221)
[2025-02-13 03:02:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:25][root][INFO] - Training Epoch: 1/2, step 3828/7134 completed (loss: 0.05548195540904999, acc: 0.9840849041938782)
[2025-02-13 03:02:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:26][root][INFO] - Training Epoch: 1/2, step 3829/7134 completed (loss: 0.025225762277841568, acc: 0.993966817855835)
[2025-02-13 03:02:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:26][root][INFO] - Training Epoch: 1/2, step 3830/7134 completed (loss: 0.05229414626955986, acc: 0.9882746934890747)
[2025-02-13 03:02:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:26][root][INFO] - Training Epoch: 1/2, step 3831/7134 completed (loss: 0.037915732711553574, acc: 0.9909909963607788)
[2025-02-13 03:02:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:27][root][INFO] - Training Epoch: 1/2, step 3832/7134 completed (loss: 0.06712664663791656, acc: 0.9882121682167053)
[2025-02-13 03:02:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:27][root][INFO] - Training Epoch: 1/2, step 3833/7134 completed (loss: 0.01414234098047018, acc: 0.9938119053840637)
[2025-02-13 03:02:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:28][root][INFO] - Training Epoch: 1/2, step 3834/7134 completed (loss: 0.008418416604399681, acc: 0.9948052167892456)
[2025-02-13 03:02:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:28][root][INFO] - Training Epoch: 1/2, step 3835/7134 completed (loss: 0.01516459509730339, acc: 0.9951456189155579)
[2025-02-13 03:02:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:28][root][INFO] - Training Epoch: 1/2, step 3836/7134 completed (loss: 0.05383233726024628, acc: 0.9884058237075806)
[2025-02-13 03:02:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:29][root][INFO] - Training Epoch: 1/2, step 3837/7134 completed (loss: 0.025854388251900673, acc: 0.9956896305084229)
[2025-02-13 03:02:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:29][root][INFO] - Training Epoch: 1/2, step 3838/7134 completed (loss: 0.015022005885839462, acc: 0.9933599233627319)
[2025-02-13 03:02:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:30][root][INFO] - Training Epoch: 1/2, step 3839/7134 completed (loss: 0.025489946827292442, acc: 0.9930939078330994)
[2025-02-13 03:02:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:30][root][INFO] - Training Epoch: 1/2, step 3840/7134 completed (loss: 0.02552042156457901, acc: 0.9906291961669922)
[2025-02-13 03:02:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:31][root][INFO] - Training Epoch: 1/2, step 3841/7134 completed (loss: 0.06489966064691544, acc: 0.9844617247581482)
[2025-02-13 03:02:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:31][root][INFO] - Training Epoch: 1/2, step 3842/7134 completed (loss: 0.006779555231332779, acc: 0.9975489974021912)
[2025-02-13 03:02:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:31][root][INFO] - Training Epoch: 1/2, step 3843/7134 completed (loss: 0.03213971480727196, acc: 0.9906152486801147)
[2025-02-13 03:02:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:32][root][INFO] - Training Epoch: 1/2, step 3844/7134 completed (loss: 0.028503291308879852, acc: 0.9961880445480347)
[2025-02-13 03:02:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:32][root][INFO] - Training Epoch: 1/2, step 3845/7134 completed (loss: 0.03658277541399002, acc: 0.9915825128555298)
[2025-02-13 03:02:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:33][root][INFO] - Training Epoch: 1/2, step 3846/7134 completed (loss: 0.023343773558735847, acc: 0.9949066042900085)
[2025-02-13 03:02:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:33][root][INFO] - Training Epoch: 1/2, step 3847/7134 completed (loss: 0.0229965727776289, acc: 0.9936000108718872)
[2025-02-13 03:02:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:33][root][INFO] - Training Epoch: 1/2, step 3848/7134 completed (loss: 0.03536275029182434, acc: 0.9923664331436157)
[2025-02-13 03:02:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:34][root][INFO] - Training Epoch: 1/2, step 3849/7134 completed (loss: 0.055119480937719345, acc: 0.9839572310447693)
[2025-02-13 03:02:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:34][root][INFO] - Training Epoch: 1/2, step 3850/7134 completed (loss: 0.07495252788066864, acc: 0.980440080165863)
[2025-02-13 03:02:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:35][root][INFO] - Training Epoch: 1/2, step 3851/7134 completed (loss: 0.0500611811876297, acc: 0.9846583008766174)
[2025-02-13 03:02:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:35][root][INFO] - Training Epoch: 1/2, step 3852/7134 completed (loss: 0.03455887362360954, acc: 0.9919614195823669)
[2025-02-13 03:02:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:36][root][INFO] - Training Epoch: 1/2, step 3853/7134 completed (loss: 0.05108200013637543, acc: 0.9831223487854004)
[2025-02-13 03:02:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:36][root][INFO] - Training Epoch: 1/2, step 3854/7134 completed (loss: 0.01659104786813259, acc: 0.9943820238113403)
[2025-02-13 03:02:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:36][root][INFO] - Training Epoch: 1/2, step 3855/7134 completed (loss: 0.05815880745649338, acc: 0.9862843155860901)
[2025-02-13 03:02:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:37][root][INFO] - Training Epoch: 1/2, step 3856/7134 completed (loss: 0.045503199100494385, acc: 0.9874476790428162)
[2025-02-13 03:02:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:37][root][INFO] - Training Epoch: 1/2, step 3857/7134 completed (loss: 0.03763648122549057, acc: 0.9871382713317871)
[2025-02-13 03:02:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:38][root][INFO] - Training Epoch: 1/2, step 3858/7134 completed (loss: 0.06301192939281464, acc: 0.980169951915741)
[2025-02-13 03:02:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:38][root][INFO] - Training Epoch: 1/2, step 3859/7134 completed (loss: 0.04458010196685791, acc: 0.9885495901107788)
[2025-02-13 03:02:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:39][root][INFO] - Training Epoch: 1/2, step 3860/7134 completed (loss: 0.038091935217380524, acc: 0.9899441599845886)
[2025-02-13 03:02:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:39][root][INFO] - Training Epoch: 1/2, step 3861/7134 completed (loss: 0.04612092673778534, acc: 0.9850339889526367)
[2025-02-13 03:02:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:39][root][INFO] - Training Epoch: 1/2, step 3862/7134 completed (loss: 0.0526273138821125, acc: 0.9852216839790344)
[2025-02-13 03:02:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:40][root][INFO] - Training Epoch: 1/2, step 3863/7134 completed (loss: 0.025712382048368454, acc: 0.9913580417633057)
[2025-02-13 03:02:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:40][root][INFO] - Training Epoch: 1/2, step 3864/7134 completed (loss: 0.0424514003098011, acc: 0.9934725761413574)
[2025-02-13 03:02:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:41][root][INFO] - Training Epoch: 1/2, step 3865/7134 completed (loss: 0.030530249699950218, acc: 0.9860557913780212)
[2025-02-13 03:02:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:41][root][INFO] - Training Epoch: 1/2, step 3866/7134 completed (loss: 0.04749884083867073, acc: 0.9877899885177612)
[2025-02-13 03:02:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:42][root][INFO] - Training Epoch: 1/2, step 3867/7134 completed (loss: 0.03496503084897995, acc: 0.9867424368858337)
[2025-02-13 03:02:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:42][root][INFO] - Training Epoch: 1/2, step 3868/7134 completed (loss: 0.034360289573669434, acc: 0.9881556630134583)
[2025-02-13 03:02:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:42][root][INFO] - Training Epoch: 1/2, step 3869/7134 completed (loss: 0.030063839629292488, acc: 0.9909228682518005)
[2025-02-13 03:02:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:43][root][INFO] - Training Epoch: 1/2, step 3870/7134 completed (loss: 0.09044300019741058, acc: 0.9791666865348816)
[2025-02-13 03:02:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:43][root][INFO] - Training Epoch: 1/2, step 3871/7134 completed (loss: 0.04362816363573074, acc: 0.9862843155860901)
[2025-02-13 03:02:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:44][root][INFO] - Training Epoch: 1/2, step 3872/7134 completed (loss: 0.05474051460623741, acc: 0.9882227182388306)
[2025-02-13 03:02:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:44][root][INFO] - Training Epoch: 1/2, step 3873/7134 completed (loss: 0.04581465944647789, acc: 0.9884910583496094)
[2025-02-13 03:02:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:45][root][INFO] - Training Epoch: 1/2, step 3874/7134 completed (loss: 0.05775434151291847, acc: 0.9834024906158447)
[2025-02-13 03:02:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:45][root][INFO] - Training Epoch: 1/2, step 3875/7134 completed (loss: 0.0671236589550972, acc: 0.9840425252914429)
[2025-02-13 03:02:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:45][root][INFO] - Training Epoch: 1/2, step 3876/7134 completed (loss: 0.031281448900699615, acc: 0.9887429475784302)
[2025-02-13 03:02:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:46][root][INFO] - Training Epoch: 1/2, step 3877/7134 completed (loss: 0.06365173310041428, acc: 0.9895561337471008)
[2025-02-13 03:02:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:46][root][INFO] - Training Epoch: 1/2, step 3878/7134 completed (loss: 0.0712476298213005, acc: 0.9838274717330933)
[2025-02-13 03:02:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:47][root][INFO] - Training Epoch: 1/2, step 3879/7134 completed (loss: 0.026868803426623344, acc: 0.9907578825950623)
[2025-02-13 03:02:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:47][root][INFO] - Training Epoch: 1/2, step 3880/7134 completed (loss: 0.12008753418922424, acc: 0.9750367403030396)
[2025-02-13 03:02:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:48][root][INFO] - Training Epoch: 1/2, step 3881/7134 completed (loss: 0.12558035552501678, acc: 0.972000002861023)
[2025-02-13 03:02:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:48][root][INFO] - Training Epoch: 1/2, step 3882/7134 completed (loss: 0.07070194184780121, acc: 0.981249988079071)
[2025-02-13 03:02:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:48][root][INFO] - Training Epoch: 1/2, step 3883/7134 completed (loss: 0.024202605709433556, acc: 0.9916083812713623)
[2025-02-13 03:02:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:49][root][INFO] - Training Epoch: 1/2, step 3884/7134 completed (loss: 0.030590737238526344, acc: 0.9918367266654968)
[2025-02-13 03:02:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:49][root][INFO] - Training Epoch: 1/2, step 3885/7134 completed (loss: 0.033705614507198334, acc: 0.9921671152114868)
[2025-02-13 03:02:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:50][root][INFO] - Training Epoch: 1/2, step 3886/7134 completed (loss: 0.03682791069149971, acc: 0.987922728061676)
[2025-02-13 03:02:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:50][root][INFO] - Training Epoch: 1/2, step 3887/7134 completed (loss: 0.03399151563644409, acc: 0.9924585223197937)
[2025-02-13 03:02:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:51][root][INFO] - Training Epoch: 1/2, step 3888/7134 completed (loss: 0.047934867441654205, acc: 0.9862499833106995)
[2025-02-13 03:02:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:51][root][INFO] - Training Epoch: 1/2, step 3889/7134 completed (loss: 0.02613784559071064, acc: 0.992337167263031)
[2025-02-13 03:02:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:52][root][INFO] - Training Epoch: 1/2, step 3890/7134 completed (loss: 0.02293824590742588, acc: 0.9927797913551331)
[2025-02-13 03:02:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:52][root][INFO] - Training Epoch: 1/2, step 3891/7134 completed (loss: 0.02564082108438015, acc: 0.9895424842834473)
[2025-02-13 03:02:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:53][root][INFO] - Training Epoch: 1/2, step 3892/7134 completed (loss: 0.029599664732813835, acc: 0.9876922965049744)
[2025-02-13 03:02:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:53][root][INFO] - Training Epoch: 1/2, step 3893/7134 completed (loss: 0.08002591878175735, acc: 0.9780927896499634)
[2025-02-13 03:02:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:53][root][INFO] - Training Epoch: 1/2, step 3894/7134 completed (loss: 0.024780096486210823, acc: 0.9915730357170105)
[2025-02-13 03:02:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:54][root][INFO] - Training Epoch: 1/2, step 3895/7134 completed (loss: 0.033288631588220596, acc: 0.9870967864990234)
[2025-02-13 03:02:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:54][root][INFO] - Training Epoch: 1/2, step 3896/7134 completed (loss: 0.01690068654716015, acc: 0.9966139793395996)
[2025-02-13 03:02:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:55][root][INFO] - Training Epoch: 1/2, step 3897/7134 completed (loss: 0.06679104268550873, acc: 0.9873239398002625)
[2025-02-13 03:02:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:55][root][INFO] - Training Epoch: 1/2, step 3898/7134 completed (loss: 0.053623247891664505, acc: 0.983460545539856)
[2025-02-13 03:02:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:56][root][INFO] - Training Epoch: 1/2, step 3899/7134 completed (loss: 0.03677631542086601, acc: 0.9922380447387695)
[2025-02-13 03:02:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:56][root][INFO] - Training Epoch: 1/2, step 3900/7134 completed (loss: 0.03914455696940422, acc: 0.9849362969398499)
[2025-02-13 03:02:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:56][root][INFO] - Training Epoch: 1/2, step 3901/7134 completed (loss: 0.09557126462459564, acc: 0.9824798107147217)
[2025-02-13 03:02:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:57][root][INFO] - Training Epoch: 1/2, step 3902/7134 completed (loss: 0.08769061416387558, acc: 0.9761589169502258)
[2025-02-13 03:02:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:57][root][INFO] - Training Epoch: 1/2, step 3903/7134 completed (loss: 0.0981513187289238, acc: 0.97050940990448)
[2025-02-13 03:02:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:58][root][INFO] - Training Epoch: 1/2, step 3904/7134 completed (loss: 0.051262035965919495, acc: 0.9847792983055115)
[2025-02-13 03:02:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:58][root][INFO] - Training Epoch: 1/2, step 3905/7134 completed (loss: 0.030908631160855293, acc: 0.9901269674301147)
[2025-02-13 03:02:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:59][root][INFO] - Training Epoch: 1/2, step 3906/7134 completed (loss: 0.029421616345643997, acc: 0.9885057210922241)
[2025-02-13 03:02:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:02:59][root][INFO] - Training Epoch: 1/2, step 3907/7134 completed (loss: 0.0358019657433033, acc: 0.9910025596618652)
[2025-02-13 03:02:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:00][root][INFO] - Training Epoch: 1/2, step 3908/7134 completed (loss: 0.07987694442272186, acc: 0.9785605072975159)
[2025-02-13 03:03:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:00][root][INFO] - Training Epoch: 1/2, step 3909/7134 completed (loss: 0.04034588485956192, acc: 0.9854604005813599)
[2025-02-13 03:03:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:00][root][INFO] - Training Epoch: 1/2, step 3910/7134 completed (loss: 0.054499927908182144, acc: 0.9874826073646545)
[2025-02-13 03:03:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:01][root][INFO] - Training Epoch: 1/2, step 3911/7134 completed (loss: 0.03731468692421913, acc: 0.9886731505393982)
[2025-02-13 03:03:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:01][root][INFO] - Training Epoch: 1/2, step 3912/7134 completed (loss: 0.014848148450255394, acc: 0.992559552192688)
[2025-02-13 03:03:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:02][root][INFO] - Training Epoch: 1/2, step 3913/7134 completed (loss: 0.04902692884206772, acc: 0.9897435903549194)
[2025-02-13 03:03:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:02][root][INFO] - Training Epoch: 1/2, step 3914/7134 completed (loss: 0.04106605798006058, acc: 0.9882155060768127)
[2025-02-13 03:03:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:03][root][INFO] - Training Epoch: 1/2, step 3915/7134 completed (loss: 0.022484781220555305, acc: 0.9929478168487549)
[2025-02-13 03:03:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:03][root][INFO] - Training Epoch: 1/2, step 3916/7134 completed (loss: 0.024388518184423447, acc: 0.9910827875137329)
[2025-02-13 03:03:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:03][root][INFO] - Training Epoch: 1/2, step 3917/7134 completed (loss: 0.014412323012948036, acc: 0.9927797913551331)
[2025-02-13 03:03:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:04][root][INFO] - Training Epoch: 1/2, step 3918/7134 completed (loss: 0.04840750992298126, acc: 0.984375)
[2025-02-13 03:03:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:04][root][INFO] - Training Epoch: 1/2, step 3919/7134 completed (loss: 0.08682143688201904, acc: 0.9853420257568359)
[2025-02-13 03:03:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:05][root][INFO] - Training Epoch: 1/2, step 3920/7134 completed (loss: 0.05557089298963547, acc: 0.9813084006309509)
[2025-02-13 03:03:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:05][root][INFO] - Training Epoch: 1/2, step 3921/7134 completed (loss: 0.04868131875991821, acc: 0.984000027179718)
[2025-02-13 03:03:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:06][root][INFO] - Training Epoch: 1/2, step 3922/7134 completed (loss: 0.12560804188251495, acc: 0.9622641801834106)
[2025-02-13 03:03:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:06][root][INFO] - Training Epoch: 1/2, step 3923/7134 completed (loss: 0.14562946557998657, acc: 0.965753436088562)
[2025-02-13 03:03:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:06][root][INFO] - Training Epoch: 1/2, step 3924/7134 completed (loss: 0.057857852429151535, acc: 0.9856321811676025)
[2025-02-13 03:03:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:07][root][INFO] - Training Epoch: 1/2, step 3925/7134 completed (loss: 0.06278915703296661, acc: 0.9838945865631104)
[2025-02-13 03:03:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:07][root][INFO] - Training Epoch: 1/2, step 3926/7134 completed (loss: 0.06273756176233292, acc: 0.9798561334609985)
[2025-02-13 03:03:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:08][root][INFO] - Training Epoch: 1/2, step 3927/7134 completed (loss: 0.03822774067521095, acc: 0.9889958500862122)
[2025-02-13 03:03:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:08][root][INFO] - Training Epoch: 1/2, step 3928/7134 completed (loss: 0.06256779283285141, acc: 0.9856321811676025)
[2025-02-13 03:03:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:09][root][INFO] - Training Epoch: 1/2, step 3929/7134 completed (loss: 0.030391890555620193, acc: 0.9920508861541748)
[2025-02-13 03:03:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:09][root][INFO] - Training Epoch: 1/2, step 3930/7134 completed (loss: 0.0576668456196785, acc: 0.9747899174690247)
[2025-02-13 03:03:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:09][root][INFO] - Training Epoch: 1/2, step 3931/7134 completed (loss: 0.03046184591948986, acc: 0.9895366430282593)
[2025-02-13 03:03:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:10][root][INFO] - Training Epoch: 1/2, step 3932/7134 completed (loss: 0.0507480725646019, acc: 0.97826087474823)
[2025-02-13 03:03:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:10][root][INFO] - Training Epoch: 1/2, step 3933/7134 completed (loss: 0.027612946927547455, acc: 0.9934210777282715)
[2025-02-13 03:03:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:11][root][INFO] - Training Epoch: 1/2, step 3934/7134 completed (loss: 0.036689192056655884, acc: 0.9882903695106506)
[2025-02-13 03:03:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:11][root][INFO] - Training Epoch: 1/2, step 3935/7134 completed (loss: 0.007900560274720192, acc: 1.0)
[2025-02-13 03:03:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:11][root][INFO] - Training Epoch: 1/2, step 3936/7134 completed (loss: 0.05351211130619049, acc: 0.9886792302131653)
[2025-02-13 03:03:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:12][root][INFO] - Training Epoch: 1/2, step 3937/7134 completed (loss: 0.055584657937288284, acc: 0.981632649898529)
[2025-02-13 03:03:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:12][root][INFO] - Training Epoch: 1/2, step 3938/7134 completed (loss: 0.021241402253508568, acc: 0.9909502267837524)
[2025-02-13 03:03:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:13][root][INFO] - Training Epoch: 1/2, step 3939/7134 completed (loss: 0.07562538236379623, acc: 0.9813874959945679)
[2025-02-13 03:03:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:13][root][INFO] - Training Epoch: 1/2, step 3940/7134 completed (loss: 0.07800035923719406, acc: 0.9824120402336121)
[2025-02-13 03:03:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:13][root][INFO] - Training Epoch: 1/2, step 3941/7134 completed (loss: 0.0350785069167614, acc: 0.9939024448394775)
[2025-02-13 03:03:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:14][root][INFO] - Training Epoch: 1/2, step 3942/7134 completed (loss: 0.029699860140681267, acc: 0.9897540807723999)
[2025-02-13 03:03:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:14][root][INFO] - Training Epoch: 1/2, step 3943/7134 completed (loss: 0.021098585799336433, acc: 0.9884169697761536)
[2025-02-13 03:03:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:15][root][INFO] - Training Epoch: 1/2, step 3944/7134 completed (loss: 0.0445479154586792, acc: 0.9926199316978455)
[2025-02-13 03:03:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:15][root][INFO] - Training Epoch: 1/2, step 3945/7134 completed (loss: 0.02222343161702156, acc: 0.9893292784690857)
[2025-02-13 03:03:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:15][root][INFO] - Training Epoch: 1/2, step 3946/7134 completed (loss: 0.02821153774857521, acc: 0.9916492700576782)
[2025-02-13 03:03:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:16][root][INFO] - Training Epoch: 1/2, step 3947/7134 completed (loss: 0.03714071586728096, acc: 0.9922279715538025)
[2025-02-13 03:03:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:16][root][INFO] - Training Epoch: 1/2, step 3948/7134 completed (loss: 0.026598971337080002, acc: 0.9924623370170593)
[2025-02-13 03:03:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:17][root][INFO] - Training Epoch: 1/2, step 3949/7134 completed (loss: 0.0436701774597168, acc: 0.9925373196601868)
[2025-02-13 03:03:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:17][root][INFO] - Training Epoch: 1/2, step 3950/7134 completed (loss: 0.04052016884088516, acc: 0.9955357313156128)
[2025-02-13 03:03:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:17][root][INFO] - Training Epoch: 1/2, step 3951/7134 completed (loss: 0.049712903797626495, acc: 0.9874686598777771)
[2025-02-13 03:03:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:18][root][INFO] - Training Epoch: 1/2, step 3952/7134 completed (loss: 0.026390761137008667, acc: 0.9923076629638672)
[2025-02-13 03:03:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:18][root][INFO] - Training Epoch: 1/2, step 3953/7134 completed (loss: 0.0458366833627224, acc: 0.9905123114585876)
[2025-02-13 03:03:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:19][root][INFO] - Training Epoch: 1/2, step 3954/7134 completed (loss: 0.050793182104825974, acc: 0.9819587469100952)
[2025-02-13 03:03:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:19][root][INFO] - Training Epoch: 1/2, step 3955/7134 completed (loss: 0.04104798287153244, acc: 0.9881955981254578)
[2025-02-13 03:03:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:19][root][INFO] - Training Epoch: 1/2, step 3956/7134 completed (loss: 0.02789672464132309, acc: 0.9919999837875366)
[2025-02-13 03:03:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:20][root][INFO] - Training Epoch: 1/2, step 3957/7134 completed (loss: 0.04585863649845123, acc: 0.9864864945411682)
[2025-02-13 03:03:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:20][root][INFO] - Training Epoch: 1/2, step 3958/7134 completed (loss: 0.094989113509655, acc: 0.9679487347602844)
[2025-02-13 03:03:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:21][root][INFO] - Training Epoch: 1/2, step 3959/7134 completed (loss: 0.02027566358447075, acc: 0.997802197933197)
[2025-02-13 03:03:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:21][root][INFO] - Training Epoch: 1/2, step 3960/7134 completed (loss: 0.03886371850967407, acc: 0.9871794581413269)
[2025-02-13 03:03:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:21][root][INFO] - Training Epoch: 1/2, step 3961/7134 completed (loss: 0.013541075401008129, acc: 0.9925037622451782)
[2025-02-13 03:03:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:22][root][INFO] - Training Epoch: 1/2, step 3962/7134 completed (loss: 0.017638307064771652, acc: 0.9935275316238403)
[2025-02-13 03:03:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:22][root][INFO] - Training Epoch: 1/2, step 3963/7134 completed (loss: 0.07370243221521378, acc: 0.9868035316467285)
[2025-02-13 03:03:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:23][root][INFO] - Training Epoch: 1/2, step 3964/7134 completed (loss: 0.06209240481257439, acc: 0.9870370626449585)
[2025-02-13 03:03:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:23][root][INFO] - Training Epoch: 1/2, step 3965/7134 completed (loss: 0.03598733991384506, acc: 0.991416335105896)
[2025-02-13 03:03:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:23][root][INFO] - Training Epoch: 1/2, step 3966/7134 completed (loss: 0.01804523728787899, acc: 0.9963898658752441)
[2025-02-13 03:03:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:24][root][INFO] - Training Epoch: 1/2, step 3967/7134 completed (loss: 0.1139124408364296, acc: 0.9722703695297241)
[2025-02-13 03:03:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:24][root][INFO] - Training Epoch: 1/2, step 3968/7134 completed (loss: 0.02132149413228035, acc: 0.9921976327896118)
[2025-02-13 03:03:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:25][root][INFO] - Training Epoch: 1/2, step 3969/7134 completed (loss: 0.06739002466201782, acc: 0.981675386428833)
[2025-02-13 03:03:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:25][root][INFO] - Training Epoch: 1/2, step 3970/7134 completed (loss: 0.08329121023416519, acc: 0.9884892106056213)
[2025-02-13 03:03:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:26][root][INFO] - Training Epoch: 1/2, step 3971/7134 completed (loss: 0.14040759205818176, acc: 0.971867024898529)
[2025-02-13 03:03:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:26][root][INFO] - Training Epoch: 1/2, step 3972/7134 completed (loss: 0.1709548979997635, acc: 0.9510309100151062)
[2025-02-13 03:03:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:26][root][INFO] - Training Epoch: 1/2, step 3973/7134 completed (loss: 0.04569040611386299, acc: 0.984455943107605)
[2025-02-13 03:03:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:27][root][INFO] - Training Epoch: 1/2, step 3974/7134 completed (loss: 0.1160023882985115, acc: 0.9661246538162231)
[2025-02-13 03:03:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:27][root][INFO] - Training Epoch: 1/2, step 3975/7134 completed (loss: 0.03418976813554764, acc: 0.9894259572029114)
[2025-02-13 03:03:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:28][root][INFO] - Training Epoch: 1/2, step 3976/7134 completed (loss: 0.12033697962760925, acc: 0.974926233291626)
[2025-02-13 03:03:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:28][root][INFO] - Training Epoch: 1/2, step 3977/7134 completed (loss: 0.01872159168124199, acc: 0.9982608556747437)
[2025-02-13 03:03:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:29][root][INFO] - Training Epoch: 1/2, step 3978/7134 completed (loss: 0.03912559896707535, acc: 0.9922380447387695)
[2025-02-13 03:03:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:29][root][INFO] - Training Epoch: 1/2, step 3979/7134 completed (loss: 0.05665528401732445, acc: 0.9854280352592468)
[2025-02-13 03:03:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:29][root][INFO] - Training Epoch: 1/2, step 3980/7134 completed (loss: 0.03203611820936203, acc: 0.9939024448394775)
[2025-02-13 03:03:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:30][root][INFO] - Training Epoch: 1/2, step 3981/7134 completed (loss: 0.06102682650089264, acc: 0.9831804037094116)
[2025-02-13 03:03:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:30][root][INFO] - Training Epoch: 1/2, step 3982/7134 completed (loss: 0.09893156588077545, acc: 0.9784792065620422)
[2025-02-13 03:03:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:31][root][INFO] - Training Epoch: 1/2, step 3983/7134 completed (loss: 0.2876526415348053, acc: 0.9340425729751587)
[2025-02-13 03:03:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:31][root][INFO] - Training Epoch: 1/2, step 3984/7134 completed (loss: 0.07907663285732269, acc: 0.9792208075523376)
[2025-02-13 03:03:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:32][root][INFO] - Training Epoch: 1/2, step 3985/7134 completed (loss: 0.02965281903743744, acc: 0.9928698539733887)
[2025-02-13 03:03:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:32][root][INFO] - Training Epoch: 1/2, step 3986/7134 completed (loss: 0.17810092866420746, acc: 0.9590908885002136)
[2025-02-13 03:03:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:32][root][INFO] - Training Epoch: 1/2, step 3987/7134 completed (loss: 0.06812703609466553, acc: 0.9808061122894287)
[2025-02-13 03:03:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:33][root][INFO] - Training Epoch: 1/2, step 3988/7134 completed (loss: 0.09289994090795517, acc: 0.9734513163566589)
[2025-02-13 03:03:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:33][root][INFO] - Training Epoch: 1/2, step 3989/7134 completed (loss: 0.206752210855484, acc: 0.9573560953140259)
[2025-02-13 03:03:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:34][root][INFO] - Training Epoch: 1/2, step 3990/7134 completed (loss: 0.04595237970352173, acc: 0.9867330193519592)
[2025-02-13 03:03:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:34][root][INFO] - Training Epoch: 1/2, step 3991/7134 completed (loss: 0.05271946266293526, acc: 0.9867987036705017)
[2025-02-13 03:03:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:34][root][INFO] - Training Epoch: 1/2, step 3992/7134 completed (loss: 0.07181353121995926, acc: 0.9864864945411682)
[2025-02-13 03:03:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:35][root][INFO] - Training Epoch: 1/2, step 3993/7134 completed (loss: 0.06954021006822586, acc: 0.9770641922950745)
[2025-02-13 03:03:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:35][root][INFO] - Training Epoch: 1/2, step 3994/7134 completed (loss: 0.07396931201219559, acc: 0.9833333492279053)
[2025-02-13 03:03:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:36][root][INFO] - Training Epoch: 1/2, step 3995/7134 completed (loss: 0.12161551415920258, acc: 0.9728682041168213)
[2025-02-13 03:03:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:36][root][INFO] - Training Epoch: 1/2, step 3996/7134 completed (loss: 0.05347292870283127, acc: 0.9897330403327942)
[2025-02-13 03:03:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:36][root][INFO] - Training Epoch: 1/2, step 3997/7134 completed (loss: 0.1847405731678009, acc: 0.9626373648643494)
[2025-02-13 03:03:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:37][root][INFO] - Training Epoch: 1/2, step 3998/7134 completed (loss: 0.1096716970205307, acc: 0.9671772718429565)
[2025-02-13 03:03:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:37][root][INFO] - Training Epoch: 1/2, step 3999/7134 completed (loss: 0.05123952776193619, acc: 0.9860279560089111)
[2025-02-13 03:03:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:37][root][INFO] - Training Epoch: 1/2, step 4000/7134 completed (loss: 0.06652267277240753, acc: 0.9864864945411682)
[2025-02-13 03:03:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:38][root][INFO] - Training Epoch: 1/2, step 4001/7134 completed (loss: 0.08314087986946106, acc: 0.9721670150756836)
[2025-02-13 03:03:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:38][root][INFO] - Training Epoch: 1/2, step 4002/7134 completed (loss: 0.09035247564315796, acc: 0.9807229042053223)
[2025-02-13 03:03:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:39][root][INFO] - Training Epoch: 1/2, step 4003/7134 completed (loss: 0.10757482051849365, acc: 0.9760147333145142)
[2025-02-13 03:03:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:39][root][INFO] - Training Epoch: 1/2, step 4004/7134 completed (loss: 0.07843017578125, acc: 0.9790209531784058)
[2025-02-13 03:03:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:39][root][INFO] - Training Epoch: 1/2, step 4005/7134 completed (loss: 0.023495834320783615, acc: 0.9937205910682678)
[2025-02-13 03:03:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:40][root][INFO] - Training Epoch: 1/2, step 4006/7134 completed (loss: 0.02273338846862316, acc: 0.9968782663345337)
[2025-02-13 03:03:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:40][root][INFO] - Training Epoch: 1/2, step 4007/7134 completed (loss: 0.053604017943143845, acc: 0.9865067601203918)
[2025-02-13 03:03:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:41][root][INFO] - Training Epoch: 1/2, step 4008/7134 completed (loss: 0.028711698949337006, acc: 0.9915433526039124)
[2025-02-13 03:03:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:41][root][INFO] - Training Epoch: 1/2, step 4009/7134 completed (loss: 0.019074641168117523, acc: 0.9946737885475159)
[2025-02-13 03:03:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:42][root][INFO] - Training Epoch: 1/2, step 4010/7134 completed (loss: 0.01957133039832115, acc: 0.9944071769714355)
[2025-02-13 03:03:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:42][root][INFO] - Training Epoch: 1/2, step 4011/7134 completed (loss: 0.0421820767223835, acc: 0.9933599233627319)
[2025-02-13 03:03:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:43][root][INFO] - Training Epoch: 1/2, step 4012/7134 completed (loss: 0.024599643424153328, acc: 0.9929378628730774)
[2025-02-13 03:03:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:43][root][INFO] - Training Epoch: 1/2, step 4013/7134 completed (loss: 0.009505447000265121, acc: 0.9983922839164734)
[2025-02-13 03:03:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:43][root][INFO] - Training Epoch: 1/2, step 4014/7134 completed (loss: 0.017193155363202095, acc: 0.9952977895736694)
[2025-02-13 03:03:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:44][root][INFO] - Training Epoch: 1/2, step 4015/7134 completed (loss: 0.023760952055454254, acc: 0.9938555955886841)
[2025-02-13 03:03:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:44][root][INFO] - Training Epoch: 1/2, step 4016/7134 completed (loss: 0.03881944343447685, acc: 0.9910714030265808)
[2025-02-13 03:03:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:45][root][INFO] - Training Epoch: 1/2, step 4017/7134 completed (loss: 0.04539496824145317, acc: 0.9907621145248413)
[2025-02-13 03:03:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:45][root][INFO] - Training Epoch: 1/2, step 4018/7134 completed (loss: 0.03306204453110695, acc: 0.9939393997192383)
[2025-02-13 03:03:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:46][root][INFO] - Training Epoch: 1/2, step 4019/7134 completed (loss: 0.051085423678159714, acc: 0.9873096346855164)
[2025-02-13 03:03:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:46][root][INFO] - Training Epoch: 1/2, step 4020/7134 completed (loss: 0.05350188538432121, acc: 0.9808743000030518)
[2025-02-13 03:03:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:46][root][INFO] - Training Epoch: 1/2, step 4021/7134 completed (loss: 0.03126678243279457, acc: 0.9857397675514221)
[2025-02-13 03:03:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:47][root][INFO] - Training Epoch: 1/2, step 4022/7134 completed (loss: 0.02845146134495735, acc: 0.9926380515098572)
[2025-02-13 03:03:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:47][root][INFO] - Training Epoch: 1/2, step 4023/7134 completed (loss: 0.053486417979002, acc: 0.9886792302131653)
[2025-02-13 03:03:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:48][root][INFO] - Training Epoch: 1/2, step 4024/7134 completed (loss: 0.049292296171188354, acc: 0.9855642914772034)
[2025-02-13 03:03:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:48][root][INFO] - Training Epoch: 1/2, step 4025/7134 completed (loss: 0.06755049526691437, acc: 0.9852774739265442)
[2025-02-13 03:03:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:49][root][INFO] - Training Epoch: 1/2, step 4026/7134 completed (loss: 0.0898558720946312, acc: 0.983775794506073)
[2025-02-13 03:03:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:49][root][INFO] - Training Epoch: 1/2, step 4027/7134 completed (loss: 0.05738896504044533, acc: 0.9856114983558655)
[2025-02-13 03:03:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:50][root][INFO] - Training Epoch: 1/2, step 4028/7134 completed (loss: 0.06938743591308594, acc: 0.977624773979187)
[2025-02-13 03:03:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:50][root][INFO] - Training Epoch: 1/2, step 4029/7134 completed (loss: 0.022138960659503937, acc: 0.9913899302482605)
[2025-02-13 03:03:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:50][root][INFO] - Training Epoch: 1/2, step 4030/7134 completed (loss: 0.01914709247648716, acc: 0.9938398599624634)
[2025-02-13 03:03:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:51][root][INFO] - Training Epoch: 1/2, step 4031/7134 completed (loss: 0.026285221800208092, acc: 0.9882698059082031)
[2025-02-13 03:03:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:51][root][INFO] - Training Epoch: 1/2, step 4032/7134 completed (loss: 0.07480642199516296, acc: 0.9852744340896606)
[2025-02-13 03:03:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:52][root][INFO] - Training Epoch: 1/2, step 4033/7134 completed (loss: 0.017455805093050003, acc: 0.9949685335159302)
[2025-02-13 03:03:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:52][root][INFO] - Training Epoch: 1/2, step 4034/7134 completed (loss: 0.010788795538246632, acc: 0.9975639581680298)
[2025-02-13 03:03:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:53][root][INFO] - Training Epoch: 1/2, step 4035/7134 completed (loss: 0.04988030716776848, acc: 0.9870129823684692)
[2025-02-13 03:03:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:53][root][INFO] - Training Epoch: 1/2, step 4036/7134 completed (loss: 0.040069784969091415, acc: 0.9853479862213135)
[2025-02-13 03:03:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:53][root][INFO] - Training Epoch: 1/2, step 4037/7134 completed (loss: 0.021938776597380638, acc: 0.9927797913551331)
[2025-02-13 03:03:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:54][root][INFO] - Training Epoch: 1/2, step 4038/7134 completed (loss: 0.034217413514852524, acc: 0.9889958500862122)
[2025-02-13 03:03:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:54][root][INFO] - Training Epoch: 1/2, step 4039/7134 completed (loss: 0.04840463772416115, acc: 0.9843971729278564)
[2025-02-13 03:03:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:55][root][INFO] - Training Epoch: 1/2, step 4040/7134 completed (loss: 0.016968291252851486, acc: 0.9950739145278931)
[2025-02-13 03:03:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:55][root][INFO] - Training Epoch: 1/2, step 4041/7134 completed (loss: 0.0471859835088253, acc: 0.9905914068222046)
[2025-02-13 03:03:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:56][root][INFO] - Training Epoch: 1/2, step 4042/7134 completed (loss: 0.02940055914223194, acc: 0.9941176176071167)
[2025-02-13 03:03:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:56][root][INFO] - Training Epoch: 1/2, step 4043/7134 completed (loss: 0.019769953563809395, acc: 0.9911894202232361)
[2025-02-13 03:03:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:56][root][INFO] - Training Epoch: 1/2, step 4044/7134 completed (loss: 0.01359130721539259, acc: 0.9962121248245239)
[2025-02-13 03:03:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:57][root][INFO] - Training Epoch: 1/2, step 4045/7134 completed (loss: 0.0471733994781971, acc: 0.987500011920929)
[2025-02-13 03:03:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:57][root][INFO] - Training Epoch: 1/2, step 4046/7134 completed (loss: 0.03330591693520546, acc: 0.9917582273483276)
[2025-02-13 03:03:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:58][root][INFO] - Training Epoch: 1/2, step 4047/7134 completed (loss: 0.04606379568576813, acc: 0.9916467666625977)
[2025-02-13 03:03:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:58][root][INFO] - Training Epoch: 1/2, step 4048/7134 completed (loss: 0.024901213124394417, acc: 0.9930070042610168)
[2025-02-13 03:03:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:59][root][INFO] - Training Epoch: 1/2, step 4049/7134 completed (loss: 0.029268905520439148, acc: 0.9919540286064148)
[2025-02-13 03:03:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:03:59][root][INFO] - Training Epoch: 1/2, step 4050/7134 completed (loss: 0.04735918343067169, acc: 0.9886524677276611)
[2025-02-13 03:03:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:00][root][INFO] - Training Epoch: 1/2, step 4051/7134 completed (loss: 0.08167970180511475, acc: 0.9868420958518982)
[2025-02-13 03:04:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:00][root][INFO] - Training Epoch: 1/2, step 4052/7134 completed (loss: 0.0260025504976511, acc: 0.9909560680389404)
[2025-02-13 03:04:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:00][root][INFO] - Training Epoch: 1/2, step 4053/7134 completed (loss: 0.04293964430689812, acc: 0.9867374300956726)
[2025-02-13 03:04:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:01][root][INFO] - Training Epoch: 1/2, step 4054/7134 completed (loss: 0.0353284627199173, acc: 0.9900000095367432)
[2025-02-13 03:04:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:01][root][INFO] - Training Epoch: 1/2, step 4055/7134 completed (loss: 0.03506633639335632, acc: 0.987864077091217)
[2025-02-13 03:04:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:02][root][INFO] - Training Epoch: 1/2, step 4056/7134 completed (loss: 0.02808874100446701, acc: 0.9903030395507812)
[2025-02-13 03:04:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:02][root][INFO] - Training Epoch: 1/2, step 4057/7134 completed (loss: 0.06308332830667496, acc: 0.9874826073646545)
[2025-02-13 03:04:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:03][root][INFO] - Training Epoch: 1/2, step 4058/7134 completed (loss: 0.09802050143480301, acc: 0.9770554304122925)
[2025-02-13 03:04:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:03][root][INFO] - Training Epoch: 1/2, step 4059/7134 completed (loss: 0.053237710148096085, acc: 0.985401451587677)
[2025-02-13 03:04:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:03][root][INFO] - Training Epoch: 1/2, step 4060/7134 completed (loss: 0.04814367741346359, acc: 0.9860896468162537)
[2025-02-13 03:04:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:04][root][INFO] - Training Epoch: 1/2, step 4061/7134 completed (loss: 0.017452536150813103, acc: 0.995726466178894)
[2025-02-13 03:04:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:04][root][INFO] - Training Epoch: 1/2, step 4062/7134 completed (loss: 0.012594803236424923, acc: 0.9937008023262024)
[2025-02-13 03:04:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:05][root][INFO] - Training Epoch: 1/2, step 4063/7134 completed (loss: 0.0218195877969265, acc: 0.9929328560829163)
[2025-02-13 03:04:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:05][root][INFO] - Training Epoch: 1/2, step 4064/7134 completed (loss: 0.027616824954748154, acc: 0.9954614043235779)
[2025-02-13 03:04:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:05][root][INFO] - Training Epoch: 1/2, step 4065/7134 completed (loss: 0.02895963191986084, acc: 0.9944827556610107)
[2025-02-13 03:04:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:06][root][INFO] - Training Epoch: 1/2, step 4066/7134 completed (loss: 0.01873646304011345, acc: 0.9930555820465088)
[2025-02-13 03:04:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:06][root][INFO] - Training Epoch: 1/2, step 4067/7134 completed (loss: 0.046489764004945755, acc: 0.9885621070861816)
[2025-02-13 03:04:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:07][root][INFO] - Training Epoch: 1/2, step 4068/7134 completed (loss: 0.04478410258889198, acc: 0.9851852059364319)
[2025-02-13 03:04:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:07][root][INFO] - Training Epoch: 1/2, step 4069/7134 completed (loss: 0.030537256971001625, acc: 0.992175281047821)
[2025-02-13 03:04:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:08][root][INFO] - Training Epoch: 1/2, step 4070/7134 completed (loss: 0.03567364811897278, acc: 0.9893454909324646)
[2025-02-13 03:04:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:08][root][INFO] - Training Epoch: 1/2, step 4071/7134 completed (loss: 0.04358544945716858, acc: 0.9884678721427917)
[2025-02-13 03:04:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:08][root][INFO] - Training Epoch: 1/2, step 4072/7134 completed (loss: 0.02014455571770668, acc: 0.9942280054092407)
[2025-02-13 03:04:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:09][root][INFO] - Training Epoch: 1/2, step 4073/7134 completed (loss: 0.018108561635017395, acc: 0.9942528605461121)
[2025-02-13 03:04:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:09][root][INFO] - Training Epoch: 1/2, step 4074/7134 completed (loss: 0.0309690460562706, acc: 0.9942775368690491)
[2025-02-13 03:04:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:10][root][INFO] - Training Epoch: 1/2, step 4075/7134 completed (loss: 0.018501687794923782, acc: 0.9925037622451782)
[2025-02-13 03:04:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:10][root][INFO] - Training Epoch: 1/2, step 4076/7134 completed (loss: 0.01699822209775448, acc: 0.9971056580543518)
[2025-02-13 03:04:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:11][root][INFO] - Training Epoch: 1/2, step 4077/7134 completed (loss: 0.04465803876519203, acc: 0.9881556630134583)
[2025-02-13 03:04:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:11][root][INFO] - Training Epoch: 1/2, step 4078/7134 completed (loss: 0.021599920466542244, acc: 0.9928774833679199)
[2025-02-13 03:04:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:11][root][INFO] - Training Epoch: 1/2, step 4079/7134 completed (loss: 0.026972411200404167, acc: 0.9928698539733887)
[2025-02-13 03:04:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:12][root][INFO] - Training Epoch: 1/2, step 4080/7134 completed (loss: 0.014565899036824703, acc: 0.9952531456947327)
[2025-02-13 03:04:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:12][root][INFO] - Training Epoch: 1/2, step 4081/7134 completed (loss: 0.00987277552485466, acc: 0.9966996908187866)
[2025-02-13 03:04:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:13][root][INFO] - Training Epoch: 1/2, step 4082/7134 completed (loss: 0.07237862795591354, acc: 0.9815157055854797)
[2025-02-13 03:04:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:13][root][INFO] - Training Epoch: 1/2, step 4083/7134 completed (loss: 0.05100413039326668, acc: 0.985029935836792)
[2025-02-13 03:04:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:13][root][INFO] - Training Epoch: 1/2, step 4084/7134 completed (loss: 0.09235800057649612, acc: 0.970588207244873)
[2025-02-13 03:04:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:14][root][INFO] - Training Epoch: 1/2, step 4085/7134 completed (loss: 0.04315393790602684, acc: 0.986522912979126)
[2025-02-13 03:04:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:14][root][INFO] - Training Epoch: 1/2, step 4086/7134 completed (loss: 0.06343776732683182, acc: 0.9819967150688171)
[2025-02-13 03:04:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:15][root][INFO] - Training Epoch: 1/2, step 4087/7134 completed (loss: 0.023939097300171852, acc: 0.9974160194396973)
[2025-02-13 03:04:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:15][root][INFO] - Training Epoch: 1/2, step 4088/7134 completed (loss: 0.08316455781459808, acc: 0.9788557291030884)
[2025-02-13 03:04:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:16][root][INFO] - Training Epoch: 1/2, step 4089/7134 completed (loss: 0.08154755085706711, acc: 0.980461835861206)
[2025-02-13 03:04:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:16][root][INFO] - Training Epoch: 1/2, step 4090/7134 completed (loss: 0.11172137409448624, acc: 0.9775725603103638)
[2025-02-13 03:04:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:16][root][INFO] - Training Epoch: 1/2, step 4091/7134 completed (loss: 0.11399652063846588, acc: 0.9696458578109741)
[2025-02-13 03:04:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:17][root][INFO] - Training Epoch: 1/2, step 4092/7134 completed (loss: 0.057736288756132126, acc: 0.9897040128707886)
[2025-02-13 03:04:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:17][root][INFO] - Training Epoch: 1/2, step 4093/7134 completed (loss: 0.03490357846021652, acc: 0.9908925294876099)
[2025-02-13 03:04:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:18][root][INFO] - Training Epoch: 1/2, step 4094/7134 completed (loss: 0.08115603774785995, acc: 0.9759759902954102)
[2025-02-13 03:04:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:18][root][INFO] - Training Epoch: 1/2, step 4095/7134 completed (loss: 0.03511542081832886, acc: 0.9927641153335571)
[2025-02-13 03:04:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:18][root][INFO] - Training Epoch: 1/2, step 4096/7134 completed (loss: 0.07900306582450867, acc: 0.9880596995353699)
[2025-02-13 03:04:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:19][root][INFO] - Training Epoch: 1/2, step 4097/7134 completed (loss: 0.06068366393446922, acc: 0.9887096881866455)
[2025-02-13 03:04:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:19][root][INFO] - Training Epoch: 1/2, step 4098/7134 completed (loss: 0.04360789805650711, acc: 0.989159882068634)
[2025-02-13 03:04:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:20][root][INFO] - Training Epoch: 1/2, step 4099/7134 completed (loss: 0.041444867849349976, acc: 0.9911971688270569)
[2025-02-13 03:04:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:20][root][INFO] - Training Epoch: 1/2, step 4100/7134 completed (loss: 0.01359228603541851, acc: 0.9939485788345337)
[2025-02-13 03:04:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:21][root][INFO] - Training Epoch: 1/2, step 4101/7134 completed (loss: 0.020100576803088188, acc: 0.9923954606056213)
[2025-02-13 03:04:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:21][root][INFO] - Training Epoch: 1/2, step 4102/7134 completed (loss: 0.033956125378608704, acc: 0.9890310764312744)
[2025-02-13 03:04:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:21][root][INFO] - Training Epoch: 1/2, step 4103/7134 completed (loss: 0.009463435970246792, acc: 0.9970104694366455)
[2025-02-13 03:04:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:22][root][INFO] - Training Epoch: 1/2, step 4104/7134 completed (loss: 0.017725082114338875, acc: 0.9966158866882324)
[2025-02-13 03:04:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:22][root][INFO] - Training Epoch: 1/2, step 4105/7134 completed (loss: 0.032739557325839996, acc: 0.995468258857727)
[2025-02-13 03:04:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:23][root][INFO] - Training Epoch: 1/2, step 4106/7134 completed (loss: 0.031243938952684402, acc: 0.9907975196838379)
[2025-02-13 03:04:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:23][root][INFO] - Training Epoch: 1/2, step 4107/7134 completed (loss: 0.038163475692272186, acc: 0.9932773113250732)
[2025-02-13 03:04:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:24][root][INFO] - Training Epoch: 1/2, step 4108/7134 completed (loss: 0.026996180415153503, acc: 0.9902912378311157)
[2025-02-13 03:04:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:24][root][INFO] - Training Epoch: 1/2, step 4109/7134 completed (loss: 0.013233896344900131, acc: 0.9967897534370422)
[2025-02-13 03:04:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:24][root][INFO] - Training Epoch: 1/2, step 4110/7134 completed (loss: 0.0065337130799889565, acc: 0.9982993006706238)
[2025-02-13 03:04:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:25][root][INFO] - Training Epoch: 1/2, step 4111/7134 completed (loss: 0.013043014332652092, acc: 0.9936908483505249)
[2025-02-13 03:04:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:25][root][INFO] - Training Epoch: 1/2, step 4112/7134 completed (loss: 0.04244660586118698, acc: 0.9928315281867981)
[2025-02-13 03:04:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:26][root][INFO] - Training Epoch: 1/2, step 4113/7134 completed (loss: 0.035499610006809235, acc: 0.9906976819038391)
[2025-02-13 03:04:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:26][root][INFO] - Training Epoch: 1/2, step 4114/7134 completed (loss: 0.10924559831619263, acc: 0.9695122241973877)
[2025-02-13 03:04:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:27][root][INFO] - Training Epoch: 1/2, step 4115/7134 completed (loss: 0.09680575877428055, acc: 0.9813432693481445)
[2025-02-13 03:04:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:27][root][INFO] - Training Epoch: 1/2, step 4116/7134 completed (loss: 0.020813612267374992, acc: 0.9967426657676697)
[2025-02-13 03:04:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:28][root][INFO] - Training Epoch: 1/2, step 4117/7134 completed (loss: 0.02567056566476822, acc: 0.9936908483505249)
[2025-02-13 03:04:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:28][root][INFO] - Training Epoch: 1/2, step 4118/7134 completed (loss: 0.022263459861278534, acc: 0.9926289916038513)
[2025-02-13 03:04:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:28][root][INFO] - Training Epoch: 1/2, step 4119/7134 completed (loss: 0.042302727699279785, acc: 0.987075924873352)
[2025-02-13 03:04:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:29][root][INFO] - Training Epoch: 1/2, step 4120/7134 completed (loss: 0.03476547449827194, acc: 0.9934102296829224)
[2025-02-13 03:04:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:29][root][INFO] - Training Epoch: 1/2, step 4121/7134 completed (loss: 0.05252718925476074, acc: 0.9872159361839294)
[2025-02-13 03:04:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:30][root][INFO] - Training Epoch: 1/2, step 4122/7134 completed (loss: 0.0225684717297554, acc: 0.9904580116271973)
[2025-02-13 03:04:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:30][root][INFO] - Training Epoch: 1/2, step 4123/7134 completed (loss: 0.034432053565979004, acc: 0.9864253401756287)
[2025-02-13 03:04:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:30][root][INFO] - Training Epoch: 1/2, step 4124/7134 completed (loss: 0.030514227226376534, acc: 0.9915134310722351)
[2025-02-13 03:04:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:31][root][INFO] - Training Epoch: 1/2, step 4125/7134 completed (loss: 0.027765734121203423, acc: 0.9911764860153198)
[2025-02-13 03:04:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:31][root][INFO] - Training Epoch: 1/2, step 4126/7134 completed (loss: 0.02240213192999363, acc: 0.9915013909339905)
[2025-02-13 03:04:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:32][root][INFO] - Training Epoch: 1/2, step 4127/7134 completed (loss: 0.0305104348808527, acc: 0.9921875)
[2025-02-13 03:04:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:32][root][INFO] - Training Epoch: 1/2, step 4128/7134 completed (loss: 0.0180418249219656, acc: 0.9984350800514221)
[2025-02-13 03:04:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:32][root][INFO] - Training Epoch: 1/2, step 4129/7134 completed (loss: 0.03071105293929577, acc: 0.9910979270935059)
[2025-02-13 03:04:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:33][root][INFO] - Training Epoch: 1/2, step 4130/7134 completed (loss: 0.029057424515485764, acc: 0.9942196607589722)
[2025-02-13 03:04:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:33][root][INFO] - Training Epoch: 1/2, step 4131/7134 completed (loss: 0.018736576661467552, acc: 0.996303141117096)
[2025-02-13 03:04:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:34][root][INFO] - Training Epoch: 1/2, step 4132/7134 completed (loss: 0.03412393853068352, acc: 0.984375)
[2025-02-13 03:04:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:34][root][INFO] - Training Epoch: 1/2, step 4133/7134 completed (loss: 0.051411524415016174, acc: 0.979629635810852)
[2025-02-13 03:04:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:34][root][INFO] - Training Epoch: 1/2, step 4134/7134 completed (loss: 0.0848582312464714, acc: 0.9739336371421814)
[2025-02-13 03:04:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:35][root][INFO] - Training Epoch: 1/2, step 4135/7134 completed (loss: 0.020585546270012856, acc: 0.9928951859474182)
[2025-02-13 03:04:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:35][root][INFO] - Training Epoch: 1/2, step 4136/7134 completed (loss: 0.012428362853825092, acc: 0.9982300996780396)
[2025-02-13 03:04:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:36][root][INFO] - Training Epoch: 1/2, step 4137/7134 completed (loss: 0.03173230588436127, acc: 0.990234375)
[2025-02-13 03:04:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:36][root][INFO] - Training Epoch: 1/2, step 4138/7134 completed (loss: 0.049478378146886826, acc: 0.982758641242981)
[2025-02-13 03:04:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:37][root][INFO] - Training Epoch: 1/2, step 4139/7134 completed (loss: 0.029493607580661774, acc: 0.991416335105896)
[2025-02-13 03:04:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:37][root][INFO] - Training Epoch: 1/2, step 4140/7134 completed (loss: 0.05958669260144234, acc: 0.9862068891525269)
[2025-02-13 03:04:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:37][root][INFO] - Training Epoch: 1/2, step 4141/7134 completed (loss: 0.06304462999105453, acc: 0.982807993888855)
[2025-02-13 03:04:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:38][root][INFO] - Training Epoch: 1/2, step 4142/7134 completed (loss: 0.0908501148223877, acc: 0.9759725332260132)
[2025-02-13 03:04:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:38][root][INFO] - Training Epoch: 1/2, step 4143/7134 completed (loss: 0.07166067510843277, acc: 0.9836734533309937)
[2025-02-13 03:04:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:39][root][INFO] - Training Epoch: 1/2, step 4144/7134 completed (loss: 0.10214823484420776, acc: 0.9681093096733093)
[2025-02-13 03:04:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:39][root][INFO] - Training Epoch: 1/2, step 4145/7134 completed (loss: 0.042780738323926926, acc: 0.9855595827102661)
[2025-02-13 03:04:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:40][root][INFO] - Training Epoch: 1/2, step 4146/7134 completed (loss: 0.08012372255325317, acc: 0.9797979593276978)
[2025-02-13 03:04:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:40][root][INFO] - Training Epoch: 1/2, step 4147/7134 completed (loss: 0.11272972077131271, acc: 0.969348669052124)
[2025-02-13 03:04:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:41][root][INFO] - Training Epoch: 1/2, step 4148/7134 completed (loss: 0.14867599308490753, acc: 0.9555984735488892)
[2025-02-13 03:04:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:41][root][INFO] - Training Epoch: 1/2, step 4149/7134 completed (loss: 0.14289575815200806, acc: 0.9599999785423279)
[2025-02-13 03:04:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:41][root][INFO] - Training Epoch: 1/2, step 4150/7134 completed (loss: 0.1585879921913147, acc: 0.9496982097625732)
[2025-02-13 03:04:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:42][root][INFO] - Training Epoch: 1/2, step 4151/7134 completed (loss: 0.144173726439476, acc: 0.9764492511749268)
[2025-02-13 03:04:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:42][root][INFO] - Training Epoch: 1/2, step 4152/7134 completed (loss: 0.07476689666509628, acc: 0.9799072742462158)
[2025-02-13 03:04:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:43][root][INFO] - Training Epoch: 1/2, step 4153/7134 completed (loss: 0.08459378778934479, acc: 0.9800613522529602)
[2025-02-13 03:04:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:43][root][INFO] - Training Epoch: 1/2, step 4154/7134 completed (loss: 0.06081075966358185, acc: 0.9814814925193787)
[2025-02-13 03:04:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:44][root][INFO] - Training Epoch: 1/2, step 4155/7134 completed (loss: 0.08196195214986801, acc: 0.9742709994316101)
[2025-02-13 03:04:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:44][root][INFO] - Training Epoch: 1/2, step 4156/7134 completed (loss: 0.08547917008399963, acc: 0.9735743999481201)
[2025-02-13 03:04:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:44][root][INFO] - Training Epoch: 1/2, step 4157/7134 completed (loss: 0.05974783003330231, acc: 0.9800398945808411)
[2025-02-13 03:04:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:45][root][INFO] - Training Epoch: 1/2, step 4158/7134 completed (loss: 0.1549922674894333, acc: 0.9614396095275879)
[2025-02-13 03:04:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:45][root][INFO] - Training Epoch: 1/2, step 4159/7134 completed (loss: 0.06313277781009674, acc: 0.9864176511764526)
[2025-02-13 03:04:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:46][root][INFO] - Training Epoch: 1/2, step 4160/7134 completed (loss: 0.10094822943210602, acc: 0.9679245352745056)
[2025-02-13 03:04:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:46][root][INFO] - Training Epoch: 1/2, step 4161/7134 completed (loss: 0.06410770118236542, acc: 0.9799196720123291)
[2025-02-13 03:04:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:47][root][INFO] - Training Epoch: 1/2, step 4162/7134 completed (loss: 0.07968231290578842, acc: 0.9806259274482727)
[2025-02-13 03:04:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:47][root][INFO] - Training Epoch: 1/2, step 4163/7134 completed (loss: 0.07377554476261139, acc: 0.9863842725753784)
[2025-02-13 03:04:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:47][root][INFO] - Training Epoch: 1/2, step 4164/7134 completed (loss: 0.05976840481162071, acc: 0.9825834631919861)
[2025-02-13 03:04:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:48][root][INFO] - Training Epoch: 1/2, step 4165/7134 completed (loss: 0.06330377608537674, acc: 0.9850187301635742)
[2025-02-13 03:04:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:48][root][INFO] - Training Epoch: 1/2, step 4166/7134 completed (loss: 0.0564124658703804, acc: 0.9823788404464722)
[2025-02-13 03:04:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:49][root][INFO] - Training Epoch: 1/2, step 4167/7134 completed (loss: 0.023309987038373947, acc: 0.9913194179534912)
[2025-02-13 03:04:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:49][root][INFO] - Training Epoch: 1/2, step 4168/7134 completed (loss: 0.048012133687734604, acc: 0.984375)
[2025-02-13 03:04:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:50][root][INFO] - Training Epoch: 1/2, step 4169/7134 completed (loss: 0.058979135006666183, acc: 0.9883843660354614)
[2025-02-13 03:04:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:50][root][INFO] - Training Epoch: 1/2, step 4170/7134 completed (loss: 0.08262963593006134, acc: 0.9723991751670837)
[2025-02-13 03:04:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:51][root][INFO] - Training Epoch: 1/2, step 4171/7134 completed (loss: 0.03451485559344292, acc: 0.9861591458320618)
[2025-02-13 03:04:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:51][root][INFO] - Training Epoch: 1/2, step 4172/7134 completed (loss: 0.0640135109424591, acc: 0.9850373864173889)
[2025-02-13 03:04:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:52][root][INFO] - Training Epoch: 1/2, step 4173/7134 completed (loss: 0.07617911696434021, acc: 0.9828326106071472)
[2025-02-13 03:04:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:52][root][INFO] - Training Epoch: 1/2, step 4174/7134 completed (loss: 0.009424474090337753, acc: 0.998084306716919)
[2025-02-13 03:04:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:52][root][INFO] - Training Epoch: 1/2, step 4175/7134 completed (loss: 0.029775191098451614, acc: 0.9948453903198242)
[2025-02-13 03:04:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:53][root][INFO] - Training Epoch: 1/2, step 4176/7134 completed (loss: 0.02568046748638153, acc: 0.992548406124115)
[2025-02-13 03:04:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:53][root][INFO] - Training Epoch: 1/2, step 4177/7134 completed (loss: 0.016113722696900368, acc: 0.9960578083992004)
[2025-02-13 03:04:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:54][root][INFO] - Training Epoch: 1/2, step 4178/7134 completed (loss: 0.02883673645555973, acc: 0.9894259572029114)
[2025-02-13 03:04:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:54][root][INFO] - Training Epoch: 1/2, step 4179/7134 completed (loss: 0.016443559899926186, acc: 0.9934383034706116)
[2025-02-13 03:04:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:55][root][INFO] - Training Epoch: 1/2, step 4180/7134 completed (loss: 0.014866351149976254, acc: 0.9957627058029175)
[2025-02-13 03:04:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:55][root][INFO] - Training Epoch: 1/2, step 4181/7134 completed (loss: 0.01184574794024229, acc: 0.9983686804771423)
[2025-02-13 03:04:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:55][root][INFO] - Training Epoch: 1/2, step 4182/7134 completed (loss: 0.008681376464664936, acc: 0.998609185218811)
[2025-02-13 03:04:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:56][root][INFO] - Training Epoch: 1/2, step 4183/7134 completed (loss: 0.022279221564531326, acc: 0.9944367408752441)
[2025-02-13 03:04:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:56][root][INFO] - Training Epoch: 1/2, step 4184/7134 completed (loss: 0.04093584790825844, acc: 0.9893898963928223)
[2025-02-13 03:04:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:57][root][INFO] - Training Epoch: 1/2, step 4185/7134 completed (loss: 0.013521585613489151, acc: 0.9954338073730469)
[2025-02-13 03:04:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:57][root][INFO] - Training Epoch: 1/2, step 4186/7134 completed (loss: 0.08302496373653412, acc: 0.9867629408836365)
[2025-02-13 03:04:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:58][root][INFO] - Training Epoch: 1/2, step 4187/7134 completed (loss: 0.011690415441989899, acc: 0.99609375)
[2025-02-13 03:04:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:58][root][INFO] - Training Epoch: 1/2, step 4188/7134 completed (loss: 0.008293751627206802, acc: 0.9975000023841858)
[2025-02-13 03:04:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:59][root][INFO] - Training Epoch: 1/2, step 4189/7134 completed (loss: 0.018826905637979507, acc: 0.9986013770103455)
[2025-02-13 03:04:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:59][root][INFO] - Training Epoch: 1/2, step 4190/7134 completed (loss: 0.02017705887556076, acc: 0.9943181872367859)
[2025-02-13 03:04:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:04:59][root][INFO] - Training Epoch: 1/2, step 4191/7134 completed (loss: 0.04017775133252144, acc: 0.9930875301361084)
[2025-02-13 03:04:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:00][root][INFO] - Training Epoch: 1/2, step 4192/7134 completed (loss: 0.013744941912591457, acc: 0.9920127987861633)
[2025-02-13 03:05:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:00][root][INFO] - Training Epoch: 1/2, step 4193/7134 completed (loss: 0.02292626164853573, acc: 0.9959893226623535)
[2025-02-13 03:05:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:01][root][INFO] - Training Epoch: 1/2, step 4194/7134 completed (loss: 0.010383201763033867, acc: 0.9969742894172668)
[2025-02-13 03:05:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:01][root][INFO] - Training Epoch: 1/2, step 4195/7134 completed (loss: 0.0450996458530426, acc: 0.9850948452949524)
[2025-02-13 03:05:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:01][root][INFO] - Training Epoch: 1/2, step 4196/7134 completed (loss: 0.013309570960700512, acc: 0.9965397715568542)
[2025-02-13 03:05:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:02][root][INFO] - Training Epoch: 1/2, step 4197/7134 completed (loss: 0.03517844155430794, acc: 0.9922380447387695)
[2025-02-13 03:05:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:02][root][INFO] - Training Epoch: 1/2, step 4198/7134 completed (loss: 0.02868540771305561, acc: 0.9916666746139526)
[2025-02-13 03:05:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:03][root][INFO] - Training Epoch: 1/2, step 4199/7134 completed (loss: 0.046444009989500046, acc: 0.989051103591919)
[2025-02-13 03:05:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:03][root][INFO] - Training Epoch: 1/2, step 4200/7134 completed (loss: 0.0353998988866806, acc: 0.9910394549369812)
[2025-02-13 03:05:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:04][root][INFO] - Training Epoch: 1/2, step 4201/7134 completed (loss: 0.016972290351986885, acc: 0.9965753555297852)
[2025-02-13 03:05:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:04][root][INFO] - Training Epoch: 1/2, step 4202/7134 completed (loss: 0.06541118770837784, acc: 0.9883720874786377)
[2025-02-13 03:05:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:04][root][INFO] - Training Epoch: 1/2, step 4203/7134 completed (loss: 0.04622998461127281, acc: 0.9875195026397705)
[2025-02-13 03:05:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:05][root][INFO] - Training Epoch: 1/2, step 4204/7134 completed (loss: 0.10687308013439178, acc: 0.9766082167625427)
[2025-02-13 03:05:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:05][root][INFO] - Training Epoch: 1/2, step 4205/7134 completed (loss: 0.05550286918878555, acc: 0.9870503544807434)
[2025-02-13 03:05:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:06][root][INFO] - Training Epoch: 1/2, step 4206/7134 completed (loss: 0.07272350043058395, acc: 0.9830747246742249)
[2025-02-13 03:05:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:06][root][INFO] - Training Epoch: 1/2, step 4207/7134 completed (loss: 0.051810652017593384, acc: 0.9888535141944885)
[2025-02-13 03:05:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:07][root][INFO] - Training Epoch: 1/2, step 4208/7134 completed (loss: 0.09096598625183105, acc: 0.9856459498405457)
[2025-02-13 03:05:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:07][root][INFO] - Training Epoch: 1/2, step 4209/7134 completed (loss: 0.08368440717458725, acc: 0.9830028414726257)
[2025-02-13 03:05:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:07][root][INFO] - Training Epoch: 1/2, step 4210/7134 completed (loss: 0.01174287311732769, acc: 0.9979550242424011)
[2025-02-13 03:05:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:08][root][INFO] - Training Epoch: 1/2, step 4211/7134 completed (loss: 0.08326109498739243, acc: 0.9769585132598877)
[2025-02-13 03:05:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:08][root][INFO] - Training Epoch: 1/2, step 4212/7134 completed (loss: 0.028854144737124443, acc: 0.9918166995048523)
[2025-02-13 03:05:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:08][root][INFO] - Training Epoch: 1/2, step 4213/7134 completed (loss: 0.015218092128634453, acc: 0.995708167552948)
[2025-02-13 03:05:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:09][root][INFO] - Training Epoch: 1/2, step 4214/7134 completed (loss: 0.06617209315299988, acc: 0.9805970191955566)
[2025-02-13 03:05:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:09][root][INFO] - Training Epoch: 1/2, step 4215/7134 completed (loss: 0.05619875714182854, acc: 0.9868131875991821)
[2025-02-13 03:05:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:10][root][INFO] - Training Epoch: 1/2, step 4216/7134 completed (loss: 0.04890308901667595, acc: 0.9908592104911804)
[2025-02-13 03:05:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:10][root][INFO] - Training Epoch: 1/2, step 4217/7134 completed (loss: 0.025505511090159416, acc: 0.9930070042610168)
[2025-02-13 03:05:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:11][root][INFO] - Training Epoch: 1/2, step 4218/7134 completed (loss: 0.05271760746836662, acc: 0.9876288771629333)
[2025-02-13 03:05:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:11][root][INFO] - Training Epoch: 1/2, step 4219/7134 completed (loss: 0.0762307196855545, acc: 0.9839357137680054)
[2025-02-13 03:05:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:11][root][INFO] - Training Epoch: 1/2, step 4220/7134 completed (loss: 0.04729127883911133, acc: 0.9913669228553772)
[2025-02-13 03:05:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:12][root][INFO] - Training Epoch: 1/2, step 4221/7134 completed (loss: 0.03644265979528427, acc: 0.9929203391075134)
[2025-02-13 03:05:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:12][root][INFO] - Training Epoch: 1/2, step 4222/7134 completed (loss: 0.14004845917224884, acc: 0.9800000190734863)
[2025-02-13 03:05:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:13][root][INFO] - Training Epoch: 1/2, step 4223/7134 completed (loss: 0.0472942590713501, acc: 0.9880794882774353)
[2025-02-13 03:05:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:13][root][INFO] - Training Epoch: 1/2, step 4224/7134 completed (loss: 0.07871950417757034, acc: 0.9827213883399963)
[2025-02-13 03:05:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:13][root][INFO] - Training Epoch: 1/2, step 4225/7134 completed (loss: 0.1093866154551506, acc: 0.979626476764679)
[2025-02-13 03:05:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:14][root][INFO] - Training Epoch: 1/2, step 4226/7134 completed (loss: 0.055718544870615005, acc: 0.9859514832496643)
[2025-02-13 03:05:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:14][root][INFO] - Training Epoch: 1/2, step 4227/7134 completed (loss: 0.10688301175832748, acc: 0.9729729890823364)
[2025-02-13 03:05:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:15][root][INFO] - Training Epoch: 1/2, step 4228/7134 completed (loss: 0.14039377868175507, acc: 0.9655172228813171)
[2025-02-13 03:05:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:15][root][INFO] - Training Epoch: 1/2, step 4229/7134 completed (loss: 0.09809233248233795, acc: 0.9769392013549805)
[2025-02-13 03:05:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:15][root][INFO] - Training Epoch: 1/2, step 4230/7134 completed (loss: 0.08520706743001938, acc: 0.9764705896377563)
[2025-02-13 03:05:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:16][root][INFO] - Training Epoch: 1/2, step 4231/7134 completed (loss: 0.1590263843536377, acc: 0.9668508172035217)
[2025-02-13 03:05:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:16][root][INFO] - Training Epoch: 1/2, step 4232/7134 completed (loss: 0.10579197108745575, acc: 0.9755725264549255)
[2025-02-13 03:05:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:17][root][INFO] - Training Epoch: 1/2, step 4233/7134 completed (loss: 0.09316140413284302, acc: 0.9816360473632812)
[2025-02-13 03:05:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:17][root][INFO] - Training Epoch: 1/2, step 4234/7134 completed (loss: 0.06663142889738083, acc: 0.9809221029281616)
[2025-02-13 03:05:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:17][root][INFO] - Training Epoch: 1/2, step 4235/7134 completed (loss: 0.03674442321062088, acc: 0.9857819676399231)
[2025-02-13 03:05:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:18][root][INFO] - Training Epoch: 1/2, step 4236/7134 completed (loss: 0.02382715977728367, acc: 0.9949495196342468)
[2025-02-13 03:05:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:18][root][INFO] - Training Epoch: 1/2, step 4237/7134 completed (loss: 0.04354013875126839, acc: 0.9876288771629333)
[2025-02-13 03:05:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:19][root][INFO] - Training Epoch: 1/2, step 4238/7134 completed (loss: 0.061136506497859955, acc: 0.9885433912277222)
[2025-02-13 03:05:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:19][root][INFO] - Training Epoch: 1/2, step 4239/7134 completed (loss: 0.03303462266921997, acc: 0.9919999837875366)
[2025-02-13 03:05:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:19][root][INFO] - Training Epoch: 1/2, step 4240/7134 completed (loss: 0.05815417319536209, acc: 0.987075924873352)
[2025-02-13 03:05:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:20][root][INFO] - Training Epoch: 1/2, step 4241/7134 completed (loss: 0.016822636127471924, acc: 0.9956616163253784)
[2025-02-13 03:05:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:20][root][INFO] - Training Epoch: 1/2, step 4242/7134 completed (loss: 0.06031254678964615, acc: 0.9844098091125488)
[2025-02-13 03:05:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:21][root][INFO] - Training Epoch: 1/2, step 4243/7134 completed (loss: 0.06327352672815323, acc: 0.9909583926200867)
[2025-02-13 03:05:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:21][root][INFO] - Training Epoch: 1/2, step 4244/7134 completed (loss: 0.045852404087781906, acc: 0.9866962432861328)
[2025-02-13 03:05:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:21][root][INFO] - Training Epoch: 1/2, step 4245/7134 completed (loss: 0.04460534453392029, acc: 0.9877750873565674)
[2025-02-13 03:05:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:22][root][INFO] - Training Epoch: 1/2, step 4246/7134 completed (loss: 0.046111710369586945, acc: 0.9889065027236938)
[2025-02-13 03:05:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:22][root][INFO] - Training Epoch: 1/2, step 4247/7134 completed (loss: 0.04268826171755791, acc: 0.9821746945381165)
[2025-02-13 03:05:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:23][root][INFO] - Training Epoch: 1/2, step 4248/7134 completed (loss: 0.06118409335613251, acc: 0.9807692170143127)
[2025-02-13 03:05:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:23][root][INFO] - Training Epoch: 1/2, step 4249/7134 completed (loss: 0.06530442833900452, acc: 0.9873015880584717)
[2025-02-13 03:05:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:24][root][INFO] - Training Epoch: 1/2, step 4250/7134 completed (loss: 0.07537996768951416, acc: 0.9808917045593262)
[2025-02-13 03:05:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:24][root][INFO] - Training Epoch: 1/2, step 4251/7134 completed (loss: 0.041351743042469025, acc: 0.9901315569877625)
[2025-02-13 03:05:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:24][root][INFO] - Training Epoch: 1/2, step 4252/7134 completed (loss: 0.04539814963936806, acc: 0.9872813820838928)
[2025-02-13 03:05:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:25][root][INFO] - Training Epoch: 1/2, step 4253/7134 completed (loss: 0.0490703284740448, acc: 0.989154040813446)
[2025-02-13 03:05:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:25][root][INFO] - Training Epoch: 1/2, step 4254/7134 completed (loss: 0.028997762128710747, acc: 0.9925187230110168)
[2025-02-13 03:05:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:26][root][INFO] - Training Epoch: 1/2, step 4255/7134 completed (loss: 0.04049478843808174, acc: 0.9880136847496033)
[2025-02-13 03:05:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:26][root][INFO] - Training Epoch: 1/2, step 4256/7134 completed (loss: 0.046983812004327774, acc: 0.9864253401756287)
[2025-02-13 03:05:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:26][root][INFO] - Training Epoch: 1/2, step 4257/7134 completed (loss: 0.03381361812353134, acc: 0.9893048405647278)
[2025-02-13 03:05:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:27][root][INFO] - Training Epoch: 1/2, step 4258/7134 completed (loss: 0.012586167082190514, acc: 0.9942747950553894)
[2025-02-13 03:05:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:27][root][INFO] - Training Epoch: 1/2, step 4259/7134 completed (loss: 0.10108441859483719, acc: 0.9770773649215698)
[2025-02-13 03:05:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:28][root][INFO] - Training Epoch: 1/2, step 4260/7134 completed (loss: 0.03599996492266655, acc: 0.9875776171684265)
[2025-02-13 03:05:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:28][root][INFO] - Training Epoch: 1/2, step 4261/7134 completed (loss: 0.03477819263935089, acc: 0.990439772605896)
[2025-02-13 03:05:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:29][root][INFO] - Training Epoch: 1/2, step 4262/7134 completed (loss: 0.03389808535575867, acc: 0.9843505620956421)
[2025-02-13 03:05:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:29][root][INFO] - Training Epoch: 1/2, step 4263/7134 completed (loss: 0.0874032974243164, acc: 0.9864864945411682)
[2025-02-13 03:05:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:29][root][INFO] - Training Epoch: 1/2, step 4264/7134 completed (loss: 0.08345577120780945, acc: 0.978723406791687)
[2025-02-13 03:05:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:30][root][INFO] - Training Epoch: 1/2, step 4265/7134 completed (loss: 0.026448141783475876, acc: 0.993686854839325)
[2025-02-13 03:05:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:30][root][INFO] - Training Epoch: 1/2, step 4266/7134 completed (loss: 0.034693680703639984, acc: 0.9940119981765747)
[2025-02-13 03:05:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:31][root][INFO] - Training Epoch: 1/2, step 4267/7134 completed (loss: 0.03967990726232529, acc: 0.9903030395507812)
[2025-02-13 03:05:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:31][root][INFO] - Training Epoch: 1/2, step 4268/7134 completed (loss: 0.08518531918525696, acc: 0.9759519100189209)
[2025-02-13 03:05:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:32][root][INFO] - Training Epoch: 1/2, step 4269/7134 completed (loss: 0.09698537737131119, acc: 0.971222996711731)
[2025-02-13 03:05:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:32][root][INFO] - Training Epoch: 1/2, step 4270/7134 completed (loss: 0.05355710908770561, acc: 0.9817708134651184)
[2025-02-13 03:05:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:33][root][INFO] - Training Epoch: 1/2, step 4271/7134 completed (loss: 0.03969693183898926, acc: 0.9863013625144958)
[2025-02-13 03:05:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:33][root][INFO] - Training Epoch: 1/2, step 4272/7134 completed (loss: 0.05097312107682228, acc: 0.9823943376541138)
[2025-02-13 03:05:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:33][root][INFO] - Training Epoch: 1/2, step 4273/7134 completed (loss: 0.03545982018113136, acc: 0.9887076616287231)
[2025-02-13 03:05:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:34][root][INFO] - Training Epoch: 1/2, step 4274/7134 completed (loss: 0.06934292614459991, acc: 0.9789719581604004)
[2025-02-13 03:05:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:34][root][INFO] - Training Epoch: 1/2, step 4275/7134 completed (loss: 0.02770654298365116, acc: 0.9924242496490479)
[2025-02-13 03:05:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:35][root][INFO] - Training Epoch: 1/2, step 4276/7134 completed (loss: 0.06799596548080444, acc: 0.9832473993301392)
[2025-02-13 03:05:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:35][root][INFO] - Training Epoch: 1/2, step 4277/7134 completed (loss: 0.031565409153699875, acc: 0.9904761910438538)
[2025-02-13 03:05:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:36][root][INFO] - Training Epoch: 1/2, step 4278/7134 completed (loss: 0.061123523861169815, acc: 0.980246901512146)
[2025-02-13 03:05:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:36][root][INFO] - Training Epoch: 1/2, step 4279/7134 completed (loss: 0.054828643798828125, acc: 0.9895833134651184)
[2025-02-13 03:05:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:36][root][INFO] - Training Epoch: 1/2, step 4280/7134 completed (loss: 0.01185726560652256, acc: 0.9952380657196045)
[2025-02-13 03:05:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:37][root][INFO] - Training Epoch: 1/2, step 4281/7134 completed (loss: 0.035925183445215225, acc: 0.9851484894752502)
[2025-02-13 03:05:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:37][root][INFO] - Training Epoch: 1/2, step 4282/7134 completed (loss: 0.017836252227425575, acc: 0.9944444298744202)
[2025-02-13 03:05:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:38][root][INFO] - Training Epoch: 1/2, step 4283/7134 completed (loss: 0.024717828258872032, acc: 0.9919725060462952)
[2025-02-13 03:05:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:38][root][INFO] - Training Epoch: 1/2, step 4284/7134 completed (loss: 0.06481548398733139, acc: 0.9873188138008118)
[2025-02-13 03:05:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:39][root][INFO] - Training Epoch: 1/2, step 4285/7134 completed (loss: 0.04456596076488495, acc: 0.9863013625144958)
[2025-02-13 03:05:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:39][root][INFO] - Training Epoch: 1/2, step 4286/7134 completed (loss: 0.053079795092344284, acc: 0.9892086386680603)
[2025-02-13 03:05:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:40][root][INFO] - Training Epoch: 1/2, step 4287/7134 completed (loss: 0.04181107133626938, acc: 0.9871134161949158)
[2025-02-13 03:05:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:40][root][INFO] - Training Epoch: 1/2, step 4288/7134 completed (loss: 0.053239401429891586, acc: 0.9872340559959412)
[2025-02-13 03:05:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:40][root][INFO] - Training Epoch: 1/2, step 4289/7134 completed (loss: 0.05796888470649719, acc: 0.9804511070251465)
[2025-02-13 03:05:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:41][root][INFO] - Training Epoch: 1/2, step 4290/7134 completed (loss: 0.027662359178066254, acc: 0.9918808937072754)
[2025-02-13 03:05:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:41][root][INFO] - Training Epoch: 1/2, step 4291/7134 completed (loss: 0.03568251430988312, acc: 0.9904648661613464)
[2025-02-13 03:05:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:42][root][INFO] - Training Epoch: 1/2, step 4292/7134 completed (loss: 0.036512959748506546, acc: 0.990963876247406)
[2025-02-13 03:05:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:42][root][INFO] - Training Epoch: 1/2, step 4293/7134 completed (loss: 0.026494218036532402, acc: 0.9919354915618896)
[2025-02-13 03:05:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:43][root][INFO] - Training Epoch: 1/2, step 4294/7134 completed (loss: 0.022218827158212662, acc: 0.9970887899398804)
[2025-02-13 03:05:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:43][root][INFO] - Training Epoch: 1/2, step 4295/7134 completed (loss: 0.04200802743434906, acc: 0.989051103591919)
[2025-02-13 03:05:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:43][root][INFO] - Training Epoch: 1/2, step 4296/7134 completed (loss: 0.02675480768084526, acc: 0.9951338171958923)
[2025-02-13 03:05:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:44][root][INFO] - Training Epoch: 1/2, step 4297/7134 completed (loss: 0.022854192182421684, acc: 0.9953051805496216)
[2025-02-13 03:05:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:44][root][INFO] - Training Epoch: 1/2, step 4298/7134 completed (loss: 0.04051141440868378, acc: 0.9917469024658203)
[2025-02-13 03:05:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:45][root][INFO] - Training Epoch: 1/2, step 4299/7134 completed (loss: 0.02569502778351307, acc: 0.9914039969444275)
[2025-02-13 03:05:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:45][root][INFO] - Training Epoch: 1/2, step 4300/7134 completed (loss: 0.015329359099268913, acc: 0.9956140518188477)
[2025-02-13 03:05:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:46][root][INFO] - Training Epoch: 1/2, step 4301/7134 completed (loss: 0.041335441172122955, acc: 0.9901639223098755)
[2025-02-13 03:05:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:46][root][INFO] - Training Epoch: 1/2, step 4302/7134 completed (loss: 0.02394106797873974, acc: 0.9910447597503662)
[2025-02-13 03:05:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:46][root][INFO] - Training Epoch: 1/2, step 4303/7134 completed (loss: 0.03345143049955368, acc: 0.9904534816741943)
[2025-02-13 03:05:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:47][root][INFO] - Training Epoch: 1/2, step 4304/7134 completed (loss: 0.03748071566224098, acc: 0.98959881067276)
[2025-02-13 03:05:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:47][root][INFO] - Training Epoch: 1/2, step 4305/7134 completed (loss: 0.029822079464793205, acc: 0.9929577708244324)
[2025-02-13 03:05:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:48][root][INFO] - Training Epoch: 1/2, step 4306/7134 completed (loss: 0.07143377512693405, acc: 0.9842312932014465)
[2025-02-13 03:05:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:48][root][INFO] - Training Epoch: 1/2, step 4307/7134 completed (loss: 0.09707295894622803, acc: 0.9743177890777588)
[2025-02-13 03:05:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:49][root][INFO] - Training Epoch: 1/2, step 4308/7134 completed (loss: 0.04996109753847122, acc: 0.9849187731742859)
[2025-02-13 03:05:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:49][root][INFO] - Training Epoch: 1/2, step 4309/7134 completed (loss: 0.04129786044359207, acc: 0.9901356101036072)
[2025-02-13 03:05:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:50][root][INFO] - Training Epoch: 1/2, step 4310/7134 completed (loss: 0.022298019379377365, acc: 0.9941245317459106)
[2025-02-13 03:05:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:50][root][INFO] - Training Epoch: 1/2, step 4311/7134 completed (loss: 0.021109309047460556, acc: 0.9927536249160767)
[2025-02-13 03:05:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:50][root][INFO] - Training Epoch: 1/2, step 4312/7134 completed (loss: 0.01741180755198002, acc: 0.9931880235671997)
[2025-02-13 03:05:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:51][root][INFO] - Training Epoch: 1/2, step 4313/7134 completed (loss: 0.02024613879621029, acc: 0.9922077655792236)
[2025-02-13 03:05:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:51][root][INFO] - Training Epoch: 1/2, step 4314/7134 completed (loss: 0.031326472759246826, acc: 0.9879153966903687)
[2025-02-13 03:05:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:52][root][INFO] - Training Epoch: 1/2, step 4315/7134 completed (loss: 0.03838156536221504, acc: 0.9869281053543091)
[2025-02-13 03:05:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:52][root][INFO] - Training Epoch: 1/2, step 4316/7134 completed (loss: 0.04240261763334274, acc: 0.9885203838348389)
[2025-02-13 03:05:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:53][root][INFO] - Training Epoch: 1/2, step 4317/7134 completed (loss: 0.04718722403049469, acc: 0.9851301312446594)
[2025-02-13 03:05:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:53][root][INFO] - Training Epoch: 1/2, step 4318/7134 completed (loss: 0.03251853585243225, acc: 0.9921524524688721)
[2025-02-13 03:05:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:54][root][INFO] - Training Epoch: 1/2, step 4319/7134 completed (loss: 0.013325504958629608, acc: 0.9955406785011292)
[2025-02-13 03:05:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:54][root][INFO] - Training Epoch: 1/2, step 4320/7134 completed (loss: 0.03368665277957916, acc: 0.9905405640602112)
[2025-02-13 03:05:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:54][root][INFO] - Training Epoch: 1/2, step 4321/7134 completed (loss: 0.05065755546092987, acc: 0.986143171787262)
[2025-02-13 03:05:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:55][root][INFO] - Training Epoch: 1/2, step 4322/7134 completed (loss: 0.037802137434482574, acc: 0.9905660152435303)
[2025-02-13 03:05:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:55][root][INFO] - Training Epoch: 1/2, step 4323/7134 completed (loss: 0.06566248834133148, acc: 0.9826589822769165)
[2025-02-13 03:05:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:56][root][INFO] - Training Epoch: 1/2, step 4324/7134 completed (loss: 0.02352459542453289, acc: 0.9903100728988647)
[2025-02-13 03:05:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:56][root][INFO] - Training Epoch: 1/2, step 4325/7134 completed (loss: 0.03812863677740097, acc: 0.9872408509254456)
[2025-02-13 03:05:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:56][root][INFO] - Training Epoch: 1/2, step 4326/7134 completed (loss: 0.024949438869953156, acc: 0.9894366264343262)
[2025-02-13 03:05:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:57][root][INFO] - Training Epoch: 1/2, step 4327/7134 completed (loss: 0.03592842072248459, acc: 0.9860140085220337)
[2025-02-13 03:05:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:57][root][INFO] - Training Epoch: 1/2, step 4328/7134 completed (loss: 0.06842024624347687, acc: 0.9849624037742615)
[2025-02-13 03:05:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:58][root][INFO] - Training Epoch: 1/2, step 4329/7134 completed (loss: 0.02649635262787342, acc: 0.993446946144104)
[2025-02-13 03:05:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:58][root][INFO] - Training Epoch: 1/2, step 4330/7134 completed (loss: 0.05524368956685066, acc: 0.9893389940261841)
[2025-02-13 03:05:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:58][root][INFO] - Training Epoch: 1/2, step 4331/7134 completed (loss: 0.014407715760171413, acc: 0.9943342804908752)
[2025-02-13 03:05:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:59][root][INFO] - Training Epoch: 1/2, step 4332/7134 completed (loss: 0.04764861986041069, acc: 0.980988621711731)
[2025-02-13 03:05:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:05:59][root][INFO] - Training Epoch: 1/2, step 4333/7134 completed (loss: 0.03710836172103882, acc: 0.9924127459526062)
[2025-02-13 03:05:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:00][root][INFO] - Training Epoch: 1/2, step 4334/7134 completed (loss: 0.05173420533537865, acc: 0.9835329055786133)
[2025-02-13 03:06:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:00][root][INFO] - Training Epoch: 1/2, step 4335/7134 completed (loss: 0.04614473506808281, acc: 0.9879032373428345)
[2025-02-13 03:06:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:01][root][INFO] - Training Epoch: 1/2, step 4336/7134 completed (loss: 0.009713724255561829, acc: 0.9983818531036377)
[2025-02-13 03:06:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:01][root][INFO] - Training Epoch: 1/2, step 4337/7134 completed (loss: 0.0217612124979496, acc: 0.9905063509941101)
[2025-02-13 03:06:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:01][root][INFO] - Training Epoch: 1/2, step 4338/7134 completed (loss: 0.019663801416754723, acc: 0.9905213117599487)
[2025-02-13 03:06:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:02][root][INFO] - Training Epoch: 1/2, step 4339/7134 completed (loss: 0.046552516520023346, acc: 0.9883381724357605)
[2025-02-13 03:06:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:02][root][INFO] - Training Epoch: 1/2, step 4340/7134 completed (loss: 0.01904325745999813, acc: 0.9966216087341309)
[2025-02-13 03:06:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:03][root][INFO] - Training Epoch: 1/2, step 4341/7134 completed (loss: 0.047389086335897446, acc: 0.9881129264831543)
[2025-02-13 03:06:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:03][root][INFO] - Training Epoch: 1/2, step 4342/7134 completed (loss: 0.041909366846084595, acc: 0.9871794581413269)
[2025-02-13 03:06:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:04][root][INFO] - Training Epoch: 1/2, step 4343/7134 completed (loss: 0.032613903284072876, acc: 0.9916805028915405)
[2025-02-13 03:06:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:04][root][INFO] - Training Epoch: 1/2, step 4344/7134 completed (loss: 0.014733495190739632, acc: 0.9958419799804688)
[2025-02-13 03:06:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:04][root][INFO] - Training Epoch: 1/2, step 4345/7134 completed (loss: 0.023665865883231163, acc: 0.9896480441093445)
[2025-02-13 03:06:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:05][root][INFO] - Training Epoch: 1/2, step 4346/7134 completed (loss: 0.009519626386463642, acc: 1.0)
[2025-02-13 03:06:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:05][root][INFO] - Training Epoch: 1/2, step 4347/7134 completed (loss: 0.05489914119243622, acc: 0.987270176410675)
[2025-02-13 03:06:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:06][root][INFO] - Training Epoch: 1/2, step 4348/7134 completed (loss: 0.023006442934274673, acc: 0.995199978351593)
[2025-02-13 03:06:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:06][root][INFO] - Training Epoch: 1/2, step 4349/7134 completed (loss: 0.013174016959965229, acc: 0.9976905584335327)
[2025-02-13 03:06:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:06][root][INFO] - Training Epoch: 1/2, step 4350/7134 completed (loss: 0.046199847012758255, acc: 0.9934554696083069)
[2025-02-13 03:06:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:07][root][INFO] - Training Epoch: 1/2, step 4351/7134 completed (loss: 0.049003250896930695, acc: 0.9849108457565308)
[2025-02-13 03:06:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:07][root][INFO] - Training Epoch: 1/2, step 4352/7134 completed (loss: 0.04874221608042717, acc: 0.9873257279396057)
[2025-02-13 03:06:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:08][root][INFO] - Training Epoch: 1/2, step 4353/7134 completed (loss: 0.04657577723264694, acc: 0.9880794882774353)
[2025-02-13 03:06:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:08][root][INFO] - Training Epoch: 1/2, step 4354/7134 completed (loss: 0.029767509549856186, acc: 0.9919999837875366)
[2025-02-13 03:06:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:09][root][INFO] - Training Epoch: 1/2, step 4355/7134 completed (loss: 0.03794911503791809, acc: 0.987484335899353)
[2025-02-13 03:06:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:09][root][INFO] - Training Epoch: 1/2, step 4356/7134 completed (loss: 0.03135846555233002, acc: 0.9916368126869202)
[2025-02-13 03:06:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:09][root][INFO] - Training Epoch: 1/2, step 4357/7134 completed (loss: 0.05523589253425598, acc: 0.982677161693573)
[2025-02-13 03:06:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:10][root][INFO] - Training Epoch: 1/2, step 4358/7134 completed (loss: 0.041226424276828766, acc: 0.9832335114479065)
[2025-02-13 03:06:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:10][root][INFO] - Training Epoch: 1/2, step 4359/7134 completed (loss: 0.05790478363633156, acc: 0.9873217344284058)
[2025-02-13 03:06:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:11][root][INFO] - Training Epoch: 1/2, step 4360/7134 completed (loss: 0.07836123555898666, acc: 0.9776536226272583)
[2025-02-13 03:06:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:11][root][INFO] - Training Epoch: 1/2, step 4361/7134 completed (loss: 0.03091069869697094, acc: 0.986994206905365)
[2025-02-13 03:06:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:12][root][INFO] - Training Epoch: 1/2, step 4362/7134 completed (loss: 0.03419353812932968, acc: 0.9864364862442017)
[2025-02-13 03:06:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:12][root][INFO] - Training Epoch: 1/2, step 4363/7134 completed (loss: 0.036468297243118286, acc: 0.9900826215744019)
[2025-02-13 03:06:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:12][root][INFO] - Training Epoch: 1/2, step 4364/7134 completed (loss: 0.01853855513036251, acc: 0.9906250238418579)
[2025-02-13 03:06:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:13][root][INFO] - Training Epoch: 1/2, step 4365/7134 completed (loss: 0.03808542713522911, acc: 0.9921383857727051)
[2025-02-13 03:06:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:13][root][INFO] - Training Epoch: 1/2, step 4366/7134 completed (loss: 0.06800682097673416, acc: 0.9793814420700073)
[2025-02-13 03:06:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:14][root][INFO] - Training Epoch: 1/2, step 4367/7134 completed (loss: 0.03655702620744705, acc: 0.984240710735321)
[2025-02-13 03:06:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:14][root][INFO] - Training Epoch: 1/2, step 4368/7134 completed (loss: 0.022681089118123055, acc: 0.992443323135376)
[2025-02-13 03:06:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:15][root][INFO] - Training Epoch: 1/2, step 4369/7134 completed (loss: 0.03870057687163353, acc: 0.9916666746139526)
[2025-02-13 03:06:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:15][root][INFO] - Training Epoch: 1/2, step 4370/7134 completed (loss: 0.04872973635792732, acc: 0.9924337863922119)
[2025-02-13 03:06:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:16][root][INFO] - Training Epoch: 1/2, step 4371/7134 completed (loss: 0.04231187701225281, acc: 0.9868074059486389)
[2025-02-13 03:06:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:16][root][INFO] - Training Epoch: 1/2, step 4372/7134 completed (loss: 0.043266117572784424, acc: 0.9896729588508606)
[2025-02-13 03:06:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:16][root][INFO] - Training Epoch: 1/2, step 4373/7134 completed (loss: 0.029179837554693222, acc: 0.9881094098091125)
[2025-02-13 03:06:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:17][root][INFO] - Training Epoch: 1/2, step 4374/7134 completed (loss: 0.0377226285636425, acc: 0.9856114983558655)
[2025-02-13 03:06:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:17][root][INFO] - Training Epoch: 1/2, step 4375/7134 completed (loss: 0.02563091740012169, acc: 0.9897040128707886)
[2025-02-13 03:06:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:18][root][INFO] - Training Epoch: 1/2, step 4376/7134 completed (loss: 0.03960801288485527, acc: 0.9886877536773682)
[2025-02-13 03:06:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:18][root][INFO] - Training Epoch: 1/2, step 4377/7134 completed (loss: 0.056862108409404755, acc: 0.9876390695571899)
[2025-02-13 03:06:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:19][root][INFO] - Training Epoch: 1/2, step 4378/7134 completed (loss: 0.03835182636976242, acc: 0.991830050945282)
[2025-02-13 03:06:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:19][root][INFO] - Training Epoch: 1/2, step 4379/7134 completed (loss: 0.07752721011638641, acc: 0.9753788113594055)
[2025-02-13 03:06:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:19][root][INFO] - Training Epoch: 1/2, step 4380/7134 completed (loss: 0.07088573276996613, acc: 0.9842382073402405)
[2025-02-13 03:06:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:20][root][INFO] - Training Epoch: 1/2, step 4381/7134 completed (loss: 0.0320710688829422, acc: 0.9889655113220215)
[2025-02-13 03:06:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:20][root][INFO] - Training Epoch: 1/2, step 4382/7134 completed (loss: 0.01777642033994198, acc: 0.995230495929718)
[2025-02-13 03:06:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:21][root][INFO] - Training Epoch: 1/2, step 4383/7134 completed (loss: 0.043331559747457504, acc: 0.9862448573112488)
[2025-02-13 03:06:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:21][root][INFO] - Training Epoch: 1/2, step 4384/7134 completed (loss: 0.02644922025501728, acc: 0.9874371886253357)
[2025-02-13 03:06:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:22][root][INFO] - Training Epoch: 1/2, step 4385/7134 completed (loss: 0.05166636407375336, acc: 0.9830508232116699)
[2025-02-13 03:06:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:22][root][INFO] - Training Epoch: 1/2, step 4386/7134 completed (loss: 0.06679427623748779, acc: 0.9822275042533875)
[2025-02-13 03:06:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:23][root][INFO] - Training Epoch: 1/2, step 4387/7134 completed (loss: 0.055875688791275024, acc: 0.9806763529777527)
[2025-02-13 03:06:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:23][root][INFO] - Training Epoch: 1/2, step 4388/7134 completed (loss: 0.0468449667096138, acc: 0.9829192757606506)
[2025-02-13 03:06:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:23][root][INFO] - Training Epoch: 1/2, step 4389/7134 completed (loss: 0.07614920288324356, acc: 0.9836868047714233)
[2025-02-13 03:06:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:24][root][INFO] - Training Epoch: 1/2, step 4390/7134 completed (loss: 0.08632971346378326, acc: 0.9761570692062378)
[2025-02-13 03:06:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:24][root][INFO] - Training Epoch: 1/2, step 4391/7134 completed (loss: 0.10779426246881485, acc: 0.9690553545951843)
[2025-02-13 03:06:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:25][root][INFO] - Training Epoch: 1/2, step 4392/7134 completed (loss: 0.02838560938835144, acc: 0.9938725233078003)
[2025-02-13 03:06:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:25][root][INFO] - Training Epoch: 1/2, step 4393/7134 completed (loss: 0.024431008845567703, acc: 0.9929676651954651)
[2025-02-13 03:06:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:26][root][INFO] - Training Epoch: 1/2, step 4394/7134 completed (loss: 0.030984684824943542, acc: 0.9929478168487549)
[2025-02-13 03:06:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:26][root][INFO] - Training Epoch: 1/2, step 4395/7134 completed (loss: 0.05491609871387482, acc: 0.9811617136001587)
[2025-02-13 03:06:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:27][root][INFO] - Training Epoch: 1/2, step 4396/7134 completed (loss: 0.09477067738771439, acc: 0.9701896905899048)
[2025-02-13 03:06:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:27][root][INFO] - Training Epoch: 1/2, step 4397/7134 completed (loss: 0.07386234402656555, acc: 0.9764705896377563)
[2025-02-13 03:06:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:27][root][INFO] - Training Epoch: 1/2, step 4398/7134 completed (loss: 0.013250347226858139, acc: 0.9949495196342468)
[2025-02-13 03:06:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:28][root][INFO] - Training Epoch: 1/2, step 4399/7134 completed (loss: 0.055783506482839584, acc: 0.9839228391647339)
[2025-02-13 03:06:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:28][root][INFO] - Training Epoch: 1/2, step 4400/7134 completed (loss: 0.10249219089746475, acc: 0.9814126491546631)
[2025-02-13 03:06:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:29][root][INFO] - Training Epoch: 1/2, step 4401/7134 completed (loss: 0.08586785942316055, acc: 0.9797101616859436)
[2025-02-13 03:06:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:29][root][INFO] - Training Epoch: 1/2, step 4402/7134 completed (loss: 0.11591096222400665, acc: 0.9636628031730652)
[2025-02-13 03:06:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:30][root][INFO] - Training Epoch: 1/2, step 4403/7134 completed (loss: 0.15734420716762543, acc: 0.9559585452079773)
[2025-02-13 03:06:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:30][root][INFO] - Training Epoch: 1/2, step 4404/7134 completed (loss: 0.1360183209180832, acc: 0.958776593208313)
[2025-02-13 03:06:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:31][root][INFO] - Training Epoch: 1/2, step 4405/7134 completed (loss: 0.038073621690273285, acc: 0.9913606643676758)
[2025-02-13 03:06:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:31][root][INFO] - Training Epoch: 1/2, step 4406/7134 completed (loss: 0.036010079085826874, acc: 0.9910913109779358)
[2025-02-13 03:06:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:32][root][INFO] - Training Epoch: 1/2, step 4407/7134 completed (loss: 0.05308021605014801, acc: 0.9872832298278809)
[2025-02-13 03:06:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:32][root][INFO] - Training Epoch: 1/2, step 4408/7134 completed (loss: 0.09100645780563354, acc: 0.9790453910827637)
[2025-02-13 03:06:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:32][root][INFO] - Training Epoch: 1/2, step 4409/7134 completed (loss: 0.09434647113084793, acc: 0.97555011510849)
[2025-02-13 03:06:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:33][root][INFO] - Training Epoch: 1/2, step 4410/7134 completed (loss: 0.07202286273241043, acc: 0.9838709831237793)
[2025-02-13 03:06:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:33][root][INFO] - Training Epoch: 1/2, step 4411/7134 completed (loss: 0.11942101269960403, acc: 0.9683453440666199)
[2025-02-13 03:06:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:34][root][INFO] - Training Epoch: 1/2, step 4412/7134 completed (loss: 0.16936273872852325, acc: 0.9491018056869507)
[2025-02-13 03:06:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:34][root][INFO] - Training Epoch: 1/2, step 4413/7134 completed (loss: 0.11766315996646881, acc: 0.9700680375099182)
[2025-02-13 03:06:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:35][root][INFO] - Training Epoch: 1/2, step 4414/7134 completed (loss: 0.041581086814403534, acc: 0.9893364906311035)
[2025-02-13 03:06:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:35][root][INFO] - Training Epoch: 1/2, step 4415/7134 completed (loss: 0.07324502617120743, acc: 0.975093424320221)
[2025-02-13 03:06:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:35][root][INFO] - Training Epoch: 1/2, step 4416/7134 completed (loss: 0.06315162777900696, acc: 0.9848307967185974)
[2025-02-13 03:06:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:36][root][INFO] - Training Epoch: 1/2, step 4417/7134 completed (loss: 0.07705903798341751, acc: 0.984375)
[2025-02-13 03:06:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:36][root][INFO] - Training Epoch: 1/2, step 4418/7134 completed (loss: 0.07154488563537598, acc: 0.9817708134651184)
[2025-02-13 03:06:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:37][root][INFO] - Training Epoch: 1/2, step 4419/7134 completed (loss: 0.08166070282459259, acc: 0.979567289352417)
[2025-02-13 03:06:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:37][root][INFO] - Training Epoch: 1/2, step 4420/7134 completed (loss: 0.06980867683887482, acc: 0.97648686170578)
[2025-02-13 03:06:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:38][root][INFO] - Training Epoch: 1/2, step 4421/7134 completed (loss: 0.057212695479393005, acc: 0.9842519760131836)
[2025-02-13 03:06:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:38][root][INFO] - Training Epoch: 1/2, step 4422/7134 completed (loss: 0.09148475527763367, acc: 0.9733123779296875)
[2025-02-13 03:06:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:39][root][INFO] - Training Epoch: 1/2, step 4423/7134 completed (loss: 0.06248569115996361, acc: 0.9840182662010193)
[2025-02-13 03:06:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:39][root][INFO] - Training Epoch: 1/2, step 4424/7134 completed (loss: 0.04227316007018089, acc: 0.9924924969673157)
[2025-02-13 03:06:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:40][root][INFO] - Training Epoch: 1/2, step 4425/7134 completed (loss: 0.047766488045454025, acc: 0.9871345162391663)
[2025-02-13 03:06:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:40][root][INFO] - Training Epoch: 1/2, step 4426/7134 completed (loss: 0.06146379932761192, acc: 0.9834539890289307)
[2025-02-13 03:06:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:40][root][INFO] - Training Epoch: 1/2, step 4427/7134 completed (loss: 0.16186264157295227, acc: 0.9518072009086609)
[2025-02-13 03:06:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:41][root][INFO] - Training Epoch: 1/2, step 4428/7134 completed (loss: 0.03906392306089401, acc: 0.9887323975563049)
[2025-02-13 03:06:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:41][root][INFO] - Training Epoch: 1/2, step 4429/7134 completed (loss: 0.07520099729299545, acc: 0.969072163105011)
[2025-02-13 03:06:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:42][root][INFO] - Training Epoch: 1/2, step 4430/7134 completed (loss: 0.08556947112083435, acc: 0.980182945728302)
[2025-02-13 03:06:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:42][root][INFO] - Training Epoch: 1/2, step 4431/7134 completed (loss: 0.0729326456785202, acc: 0.9797822833061218)
[2025-02-13 03:06:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:43][root][INFO] - Training Epoch: 1/2, step 4432/7134 completed (loss: 0.014269931241869926, acc: 0.996927797794342)
[2025-02-13 03:06:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:43][root][INFO] - Training Epoch: 1/2, step 4433/7134 completed (loss: 0.03642043471336365, acc: 0.9906759858131409)
[2025-02-13 03:06:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:43][root][INFO] - Training Epoch: 1/2, step 4434/7134 completed (loss: 0.040591273456811905, acc: 0.9819819927215576)
[2025-02-13 03:06:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:44][root][INFO] - Training Epoch: 1/2, step 4435/7134 completed (loss: 0.0432334765791893, acc: 0.9833641648292542)
[2025-02-13 03:06:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:44][root][INFO] - Training Epoch: 1/2, step 4436/7134 completed (loss: 0.05757293850183487, acc: 0.9804469347000122)
[2025-02-13 03:06:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:45][root][INFO] - Training Epoch: 1/2, step 4437/7134 completed (loss: 0.019580505788326263, acc: 0.9934383034706116)
[2025-02-13 03:06:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:45][root][INFO] - Training Epoch: 1/2, step 4438/7134 completed (loss: 0.042211126536130905, acc: 0.988135576248169)
[2025-02-13 03:06:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:46][root][INFO] - Training Epoch: 1/2, step 4439/7134 completed (loss: 0.022255612537264824, acc: 0.9918699264526367)
[2025-02-13 03:06:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:46][root][INFO] - Training Epoch: 1/2, step 4440/7134 completed (loss: 0.038635604083538055, acc: 0.988950252532959)
[2025-02-13 03:06:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:47][root][INFO] - Training Epoch: 1/2, step 4441/7134 completed (loss: 0.06285613030195236, acc: 0.9831029176712036)
[2025-02-13 03:06:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:47][root][INFO] - Training Epoch: 1/2, step 4442/7134 completed (loss: 0.0701712965965271, acc: 0.9819819927215576)
[2025-02-13 03:06:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:48][root][INFO] - Training Epoch: 1/2, step 4443/7134 completed (loss: 0.047691598534584045, acc: 0.9820512533187866)
[2025-02-13 03:06:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:48][root][INFO] - Training Epoch: 1/2, step 4444/7134 completed (loss: 0.038576241582632065, acc: 0.990212082862854)
[2025-02-13 03:06:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:48][root][INFO] - Training Epoch: 1/2, step 4445/7134 completed (loss: 0.05315611883997917, acc: 0.9776536226272583)
[2025-02-13 03:06:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:49][root][INFO] - Training Epoch: 1/2, step 4446/7134 completed (loss: 0.038797248154878616, acc: 0.988727867603302)
[2025-02-13 03:06:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:49][root][INFO] - Training Epoch: 1/2, step 4447/7134 completed (loss: 0.04078793525695801, acc: 0.989386796951294)
[2025-02-13 03:06:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:50][root][INFO] - Training Epoch: 1/2, step 4448/7134 completed (loss: 0.050630439072847366, acc: 0.9780621528625488)
[2025-02-13 03:06:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:50][root][INFO] - Training Epoch: 1/2, step 4449/7134 completed (loss: 0.06398379802703857, acc: 0.975836455821991)
[2025-02-13 03:06:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:51][root][INFO] - Training Epoch: 1/2, step 4450/7134 completed (loss: 0.04006402939558029, acc: 0.9881481528282166)
[2025-02-13 03:06:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:51][root][INFO] - Training Epoch: 1/2, step 4451/7134 completed (loss: 0.10421529412269592, acc: 0.9800994992256165)
[2025-02-13 03:06:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:51][root][INFO] - Training Epoch: 1/2, step 4452/7134 completed (loss: 0.02143040858209133, acc: 0.995199978351593)
[2025-02-13 03:06:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:52][root][INFO] - Training Epoch: 1/2, step 4453/7134 completed (loss: 0.04382067173719406, acc: 0.9918032884597778)
[2025-02-13 03:06:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:52][root][INFO] - Training Epoch: 1/2, step 4454/7134 completed (loss: 0.04107074812054634, acc: 0.9900850057601929)
[2025-02-13 03:06:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:53][root][INFO] - Training Epoch: 1/2, step 4455/7134 completed (loss: 0.10366028547286987, acc: 0.9779005646705627)
[2025-02-13 03:06:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:53][root][INFO] - Training Epoch: 1/2, step 4456/7134 completed (loss: 0.018518520519137383, acc: 0.9935275316238403)
[2025-02-13 03:06:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:54][root][INFO] - Training Epoch: 1/2, step 4457/7134 completed (loss: 0.04710071533918381, acc: 0.9925093650817871)
[2025-02-13 03:06:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:54][root][INFO] - Training Epoch: 1/2, step 4458/7134 completed (loss: 0.01279964204877615, acc: 0.9967266917228699)
[2025-02-13 03:06:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:54][root][INFO] - Training Epoch: 1/2, step 4459/7134 completed (loss: 0.034939005970954895, acc: 0.9931318759918213)
[2025-02-13 03:06:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:55][root][INFO] - Training Epoch: 1/2, step 4460/7134 completed (loss: 0.028735186904668808, acc: 0.9938875436782837)
[2025-02-13 03:06:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:55][root][INFO] - Training Epoch: 1/2, step 4461/7134 completed (loss: 0.06047468259930611, acc: 0.9901599287986755)
[2025-02-13 03:06:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:56][root][INFO] - Training Epoch: 1/2, step 4462/7134 completed (loss: 0.014490214176476002, acc: 0.9945504069328308)
[2025-02-13 03:06:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:56][root][INFO] - Training Epoch: 1/2, step 4463/7134 completed (loss: 0.01899797096848488, acc: 0.9949685335159302)
[2025-02-13 03:06:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:57][root][INFO] - Training Epoch: 1/2, step 4464/7134 completed (loss: 0.036066729575395584, acc: 0.9881955981254578)
[2025-02-13 03:06:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:57][root][INFO] - Training Epoch: 1/2, step 4465/7134 completed (loss: 0.0190924983471632, acc: 0.9948453903198242)
[2025-02-13 03:06:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:58][root][INFO] - Training Epoch: 1/2, step 4466/7134 completed (loss: 0.032861411571502686, acc: 0.9950617551803589)
[2025-02-13 03:06:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:58][root][INFO] - Training Epoch: 1/2, step 4467/7134 completed (loss: 0.0470825619995594, acc: 0.9899497628211975)
[2025-02-13 03:06:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:59][root][INFO] - Training Epoch: 1/2, step 4468/7134 completed (loss: 0.01143459603190422, acc: 0.9976218938827515)
[2025-02-13 03:06:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:06:59][root][INFO] - Training Epoch: 1/2, step 4469/7134 completed (loss: 0.03098021075129509, acc: 0.9930635690689087)
[2025-02-13 03:06:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:00][root][INFO] - Training Epoch: 1/2, step 4470/7134 completed (loss: 0.024680109694600105, acc: 0.99005526304245)
[2025-02-13 03:07:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:00][root][INFO] - Training Epoch: 1/2, step 4471/7134 completed (loss: 0.012505940161645412, acc: 0.996314525604248)
[2025-02-13 03:07:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:00][root][INFO] - Training Epoch: 1/2, step 4472/7134 completed (loss: 0.039194073528051376, acc: 0.9894598126411438)
[2025-02-13 03:07:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:01][root][INFO] - Training Epoch: 1/2, step 4473/7134 completed (loss: 0.07738692313432693, acc: 0.976190447807312)
[2025-02-13 03:07:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:01][root][INFO] - Training Epoch: 1/2, step 4474/7134 completed (loss: 0.0712878629565239, acc: 0.9762901067733765)
[2025-02-13 03:07:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:02][root][INFO] - Training Epoch: 1/2, step 4475/7134 completed (loss: 0.13393615186214447, acc: 0.9640198349952698)
[2025-02-13 03:07:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:02][root][INFO] - Training Epoch: 1/2, step 4476/7134 completed (loss: 0.057284608483314514, acc: 0.9862825870513916)
[2025-02-13 03:07:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:03][root][INFO] - Training Epoch: 1/2, step 4477/7134 completed (loss: 0.008969019167125225, acc: 0.9975550174713135)
[2025-02-13 03:07:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:03][root][INFO] - Training Epoch: 1/2, step 4478/7134 completed (loss: 0.022609824314713478, acc: 0.9962916970252991)
[2025-02-13 03:07:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:03][root][INFO] - Training Epoch: 1/2, step 4479/7134 completed (loss: 0.014743410050868988, acc: 0.993261456489563)
[2025-02-13 03:07:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:04][root][INFO] - Training Epoch: 1/2, step 4480/7134 completed (loss: 0.024627117440104485, acc: 0.9929906725883484)
[2025-02-13 03:07:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:04][root][INFO] - Training Epoch: 1/2, step 4481/7134 completed (loss: 0.03572576865553856, acc: 0.9918224215507507)
[2025-02-13 03:07:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:05][root][INFO] - Training Epoch: 1/2, step 4482/7134 completed (loss: 0.010729393921792507, acc: 0.9964028596878052)
[2025-02-13 03:07:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:05][root][INFO] - Training Epoch: 1/2, step 4483/7134 completed (loss: 0.020629961043596268, acc: 0.9955423474311829)
[2025-02-13 03:07:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:06][root][INFO] - Training Epoch: 1/2, step 4484/7134 completed (loss: 0.020569853484630585, acc: 0.9920634627342224)
[2025-02-13 03:07:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:06][root][INFO] - Training Epoch: 1/2, step 4485/7134 completed (loss: 0.011812658049166203, acc: 0.9954057931900024)
[2025-02-13 03:07:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:07][root][INFO] - Training Epoch: 1/2, step 4486/7134 completed (loss: 0.007057126611471176, acc: 0.9985074400901794)
[2025-02-13 03:07:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:07][root][INFO] - Training Epoch: 1/2, step 4487/7134 completed (loss: 0.012669139541685581, acc: 0.9972413778305054)
[2025-02-13 03:07:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:07][root][INFO] - Training Epoch: 1/2, step 4488/7134 completed (loss: 0.01205172948539257, acc: 0.9946042895317078)
[2025-02-13 03:07:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:08][root][INFO] - Training Epoch: 1/2, step 4489/7134 completed (loss: 0.011757727712392807, acc: 0.9965635538101196)
[2025-02-13 03:07:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:08][root][INFO] - Training Epoch: 1/2, step 4490/7134 completed (loss: 0.01985355094075203, acc: 0.9944444298744202)
[2025-02-13 03:07:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:09][root][INFO] - Training Epoch: 1/2, step 4491/7134 completed (loss: 0.0039093829691410065, acc: 1.0)
[2025-02-13 03:07:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:09][root][INFO] - Training Epoch: 1/2, step 4492/7134 completed (loss: 0.018502499908208847, acc: 0.9908088445663452)
[2025-02-13 03:07:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:10][root][INFO] - Training Epoch: 1/2, step 4493/7134 completed (loss: 0.0086646294221282, acc: 0.9958734512329102)
[2025-02-13 03:07:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:10][root][INFO] - Training Epoch: 1/2, step 4494/7134 completed (loss: 0.028688402846455574, acc: 0.9913169145584106)
[2025-02-13 03:07:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:11][root][INFO] - Training Epoch: 1/2, step 4495/7134 completed (loss: 0.016303623095154762, acc: 0.9974457025527954)
[2025-02-13 03:07:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:11][root][INFO] - Training Epoch: 1/2, step 4496/7134 completed (loss: 0.03653254732489586, acc: 0.9886040091514587)
[2025-02-13 03:07:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:11][root][INFO] - Training Epoch: 1/2, step 4497/7134 completed (loss: 0.01359582133591175, acc: 0.9973509907722473)
[2025-02-13 03:07:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:12][root][INFO] - Training Epoch: 1/2, step 4498/7134 completed (loss: 0.05080695077776909, acc: 0.9912917017936707)
[2025-02-13 03:07:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:12][root][INFO] - Training Epoch: 1/2, step 4499/7134 completed (loss: 0.035880573093891144, acc: 0.991304337978363)
[2025-02-13 03:07:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:13][root][INFO] - Training Epoch: 1/2, step 4500/7134 completed (loss: 0.02228453941643238, acc: 0.9958041906356812)
[2025-02-13 03:07:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:13][root][INFO] - Training Epoch: 1/2, step 4501/7134 completed (loss: 0.018480585888028145, acc: 0.9917920827865601)
[2025-02-13 03:07:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:14][root][INFO] - Training Epoch: 1/2, step 4502/7134 completed (loss: 0.016547735780477524, acc: 0.9970015287399292)
[2025-02-13 03:07:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:14][root][INFO] - Training Epoch: 1/2, step 4503/7134 completed (loss: 0.008382260799407959, acc: 0.9983948469161987)
[2025-02-13 03:07:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:14][root][INFO] - Training Epoch: 1/2, step 4504/7134 completed (loss: 0.0530342236161232, acc: 0.988041877746582)
[2025-02-13 03:07:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:15][root][INFO] - Training Epoch: 1/2, step 4505/7134 completed (loss: 0.0408400222659111, acc: 0.9877150058746338)
[2025-02-13 03:07:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:15][root][INFO] - Training Epoch: 1/2, step 4506/7134 completed (loss: 0.023892764002084732, acc: 0.992277979850769)
[2025-02-13 03:07:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:16][root][INFO] - Training Epoch: 1/2, step 4507/7134 completed (loss: 0.021440977230668068, acc: 0.9943100810050964)
[2025-02-13 03:07:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:16][root][INFO] - Training Epoch: 1/2, step 4508/7134 completed (loss: 0.04028620570898056, acc: 0.9871794581413269)
[2025-02-13 03:07:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:17][root][INFO] - Training Epoch: 1/2, step 4509/7134 completed (loss: 0.04665137082338333, acc: 0.982550323009491)
[2025-02-13 03:07:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:17][root][INFO] - Training Epoch: 1/2, step 4510/7134 completed (loss: 0.04440411552786827, acc: 0.9858956336975098)
[2025-02-13 03:07:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:18][root][INFO] - Training Epoch: 1/2, step 4511/7134 completed (loss: 0.08019652217626572, acc: 0.9789029359817505)
[2025-02-13 03:07:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:18][root][INFO] - Training Epoch: 1/2, step 4512/7134 completed (loss: 0.04846489429473877, acc: 0.9784946441650391)
[2025-02-13 03:07:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:18][root][INFO] - Training Epoch: 1/2, step 4513/7134 completed (loss: 0.025931095704436302, acc: 0.9915730357170105)
[2025-02-13 03:07:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:19][root][INFO] - Training Epoch: 1/2, step 4514/7134 completed (loss: 0.07722370326519012, acc: 0.9824817776679993)
[2025-02-13 03:07:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:19][root][INFO] - Training Epoch: 1/2, step 4515/7134 completed (loss: 0.06149905174970627, acc: 0.983582079410553)
[2025-02-13 03:07:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:20][root][INFO] - Training Epoch: 1/2, step 4516/7134 completed (loss: 0.02411525696516037, acc: 0.9914320707321167)
[2025-02-13 03:07:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:20][root][INFO] - Training Epoch: 1/2, step 4517/7134 completed (loss: 0.03765176609158516, acc: 0.984635055065155)
[2025-02-13 03:07:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:21][root][INFO] - Training Epoch: 1/2, step 4518/7134 completed (loss: 0.05996356159448624, acc: 0.9846938848495483)
[2025-02-13 03:07:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:21][root][INFO] - Training Epoch: 1/2, step 4519/7134 completed (loss: 0.029484344646334648, acc: 0.990326464176178)
[2025-02-13 03:07:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:22][root][INFO] - Training Epoch: 1/2, step 4520/7134 completed (loss: 0.027778515592217445, acc: 0.9929378628730774)
[2025-02-13 03:07:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:22][root][INFO] - Training Epoch: 1/2, step 4521/7134 completed (loss: 0.04064825549721718, acc: 0.9855595827102661)
[2025-02-13 03:07:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:22][root][INFO] - Training Epoch: 1/2, step 4522/7134 completed (loss: 0.03939798101782799, acc: 0.988664984703064)
[2025-02-13 03:07:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:23][root][INFO] - Training Epoch: 1/2, step 4523/7134 completed (loss: 0.024928154423832893, acc: 0.9917159676551819)
[2025-02-13 03:07:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:23][root][INFO] - Training Epoch: 1/2, step 4524/7134 completed (loss: 0.04031985253095627, acc: 0.9919028282165527)
[2025-02-13 03:07:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:24][root][INFO] - Training Epoch: 1/2, step 4525/7134 completed (loss: 0.03310764580965042, acc: 0.9911110997200012)
[2025-02-13 03:07:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:24][root][INFO] - Training Epoch: 1/2, step 4526/7134 completed (loss: 0.05754006281495094, acc: 0.9806451797485352)
[2025-02-13 03:07:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:25][root][INFO] - Training Epoch: 1/2, step 4527/7134 completed (loss: 0.041320107877254486, acc: 0.9855072498321533)
[2025-02-13 03:07:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:25][root][INFO] - Training Epoch: 1/2, step 4528/7134 completed (loss: 0.04915107414126396, acc: 0.9858611822128296)
[2025-02-13 03:07:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:26][root][INFO] - Training Epoch: 1/2, step 4529/7134 completed (loss: 0.09808207303285599, acc: 0.9805699586868286)
[2025-02-13 03:07:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:26][root][INFO] - Training Epoch: 1/2, step 4530/7134 completed (loss: 0.03883896768093109, acc: 0.9859943985939026)
[2025-02-13 03:07:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:26][root][INFO] - Training Epoch: 1/2, step 4531/7134 completed (loss: 0.021319324150681496, acc: 0.9928186535835266)
[2025-02-13 03:07:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:27][root][INFO] - Training Epoch: 1/2, step 4532/7134 completed (loss: 0.04646555706858635, acc: 0.9890438318252563)
[2025-02-13 03:07:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:27][root][INFO] - Training Epoch: 1/2, step 4533/7134 completed (loss: 0.022311771288514137, acc: 0.991696298122406)
[2025-02-13 03:07:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:28][root][INFO] - Training Epoch: 1/2, step 4534/7134 completed (loss: 0.016659846529364586, acc: 0.9941314458847046)
[2025-02-13 03:07:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:28][root][INFO] - Training Epoch: 1/2, step 4535/7134 completed (loss: 0.037119414657354355, acc: 0.9863325953483582)
[2025-02-13 03:07:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:29][root][INFO] - Training Epoch: 1/2, step 4536/7134 completed (loss: 0.027062825858592987, acc: 0.992337167263031)
[2025-02-13 03:07:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:29][root][INFO] - Training Epoch: 1/2, step 4537/7134 completed (loss: 0.013441963121294975, acc: 0.9955307245254517)
[2025-02-13 03:07:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:30][root][INFO] - Training Epoch: 1/2, step 4538/7134 completed (loss: 0.035446617752313614, acc: 0.9885057210922241)
[2025-02-13 03:07:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:30][root][INFO] - Training Epoch: 1/2, step 4539/7134 completed (loss: 0.03960425406694412, acc: 0.9924487471580505)
[2025-02-13 03:07:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:31][root][INFO] - Training Epoch: 1/2, step 4540/7134 completed (loss: 0.04155939817428589, acc: 0.9884393215179443)
[2025-02-13 03:07:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:31][root][INFO] - Training Epoch: 1/2, step 4541/7134 completed (loss: 0.059077322483062744, acc: 0.9794661402702332)
[2025-02-13 03:07:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:31][root][INFO] - Training Epoch: 1/2, step 4542/7134 completed (loss: 0.06148692965507507, acc: 0.9891774654388428)
[2025-02-13 03:07:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:32][root][INFO] - Training Epoch: 1/2, step 4543/7134 completed (loss: 0.08353874832391739, acc: 0.9714794754981995)
[2025-02-13 03:07:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:32][root][INFO] - Training Epoch: 1/2, step 4544/7134 completed (loss: 0.04476243257522583, acc: 0.9862805008888245)
[2025-02-13 03:07:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:33][root][INFO] - Training Epoch: 1/2, step 4545/7134 completed (loss: 0.07606152445077896, acc: 0.9749518036842346)
[2025-02-13 03:07:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:33][root][INFO] - Training Epoch: 1/2, step 4546/7134 completed (loss: 0.06653787940740585, acc: 0.9848739504814148)
[2025-02-13 03:07:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:33][root][INFO] - Training Epoch: 1/2, step 4547/7134 completed (loss: 0.0340891033411026, acc: 0.9896193742752075)
[2025-02-13 03:07:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:34][root][INFO] - Training Epoch: 1/2, step 4548/7134 completed (loss: 0.0545734204351902, acc: 0.9867330193519592)
[2025-02-13 03:07:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:34][root][INFO] - Training Epoch: 1/2, step 4549/7134 completed (loss: 0.04444843903183937, acc: 0.9871244430541992)
[2025-02-13 03:07:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:35][root][INFO] - Training Epoch: 1/2, step 4550/7134 completed (loss: 0.058490294963121414, acc: 0.9882746934890747)
[2025-02-13 03:07:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:35][root][INFO] - Training Epoch: 1/2, step 4551/7134 completed (loss: 0.03897080197930336, acc: 0.9885714054107666)
[2025-02-13 03:07:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:36][root][INFO] - Training Epoch: 1/2, step 4552/7134 completed (loss: 0.02252575382590294, acc: 0.9923809766769409)
[2025-02-13 03:07:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:36][root][INFO] - Training Epoch: 1/2, step 4553/7134 completed (loss: 0.03630039468407631, acc: 0.9915397763252258)
[2025-02-13 03:07:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:36][root][INFO] - Training Epoch: 1/2, step 4554/7134 completed (loss: 0.054950304329395294, acc: 0.9801802039146423)
[2025-02-13 03:07:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:37][root][INFO] - Training Epoch: 1/2, step 4555/7134 completed (loss: 0.047490015625953674, acc: 0.9806094169616699)
[2025-02-13 03:07:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:37][root][INFO] - Training Epoch: 1/2, step 4556/7134 completed (loss: 0.06002219021320343, acc: 0.9819548726081848)
[2025-02-13 03:07:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:38][root][INFO] - Training Epoch: 1/2, step 4557/7134 completed (loss: 0.07884562015533447, acc: 0.985401451587677)
[2025-02-13 03:07:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:38][root][INFO] - Training Epoch: 1/2, step 4558/7134 completed (loss: 0.040497951209545135, acc: 0.9898989796638489)
[2025-02-13 03:07:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:38][root][INFO] - Training Epoch: 1/2, step 4559/7134 completed (loss: 0.0692872405052185, acc: 0.9798319339752197)
[2025-02-13 03:07:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:39][root][INFO] - Training Epoch: 1/2, step 4560/7134 completed (loss: 0.03802023082971573, acc: 0.989130437374115)
[2025-02-13 03:07:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:39][root][INFO] - Training Epoch: 1/2, step 4561/7134 completed (loss: 0.07165436446666718, acc: 0.9745097756385803)
[2025-02-13 03:07:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:40][root][INFO] - Training Epoch: 1/2, step 4562/7134 completed (loss: 0.055457618087530136, acc: 0.988950252532959)
[2025-02-13 03:07:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:40][root][INFO] - Training Epoch: 1/2, step 4563/7134 completed (loss: 0.043668728321790695, acc: 0.9862595200538635)
[2025-02-13 03:07:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:40][root][INFO] - Training Epoch: 1/2, step 4564/7134 completed (loss: 0.06106552481651306, acc: 0.9802538752555847)
[2025-02-13 03:07:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:41][root][INFO] - Training Epoch: 1/2, step 4565/7134 completed (loss: 0.04218267276883125, acc: 0.9866156578063965)
[2025-02-13 03:07:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:41][root][INFO] - Training Epoch: 1/2, step 4566/7134 completed (loss: 0.03874841704964638, acc: 0.9892473220825195)
[2025-02-13 03:07:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:42][root][INFO] - Training Epoch: 1/2, step 4567/7134 completed (loss: 0.023786280304193497, acc: 0.9944853186607361)
[2025-02-13 03:07:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:42][root][INFO] - Training Epoch: 1/2, step 4568/7134 completed (loss: 0.04738641902804375, acc: 0.9839449524879456)
[2025-02-13 03:07:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:43][root][INFO] - Training Epoch: 1/2, step 4569/7134 completed (loss: 0.02574947103857994, acc: 0.9941176176071167)
[2025-02-13 03:07:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:43][root][INFO] - Training Epoch: 1/2, step 4570/7134 completed (loss: 0.026275740936398506, acc: 0.9883381724357605)
[2025-02-13 03:07:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:43][root][INFO] - Training Epoch: 1/2, step 4571/7134 completed (loss: 0.04988710209727287, acc: 0.9884467124938965)
[2025-02-13 03:07:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:44][root][INFO] - Training Epoch: 1/2, step 4572/7134 completed (loss: 0.03910084813833237, acc: 0.9906542301177979)
[2025-02-13 03:07:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:44][root][INFO] - Training Epoch: 1/2, step 4573/7134 completed (loss: 0.038430579006671906, acc: 0.9915966391563416)
[2025-02-13 03:07:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:45][root][INFO] - Training Epoch: 1/2, step 4574/7134 completed (loss: 0.01199394091963768, acc: 0.9946996569633484)
[2025-02-13 03:07:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:45][root][INFO] - Training Epoch: 1/2, step 4575/7134 completed (loss: 0.04114691540598869, acc: 0.9880319237709045)
[2025-02-13 03:07:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:46][root][INFO] - Training Epoch: 1/2, step 4576/7134 completed (loss: 0.02024383656680584, acc: 0.9962359070777893)
[2025-02-13 03:07:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:46][root][INFO] - Training Epoch: 1/2, step 4577/7134 completed (loss: 0.04847406595945358, acc: 0.9887640476226807)
[2025-02-13 03:07:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:46][root][INFO] - Training Epoch: 1/2, step 4578/7134 completed (loss: 0.017705032601952553, acc: 0.9941775798797607)
[2025-02-13 03:07:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:47][root][INFO] - Training Epoch: 1/2, step 4579/7134 completed (loss: 0.05274420604109764, acc: 0.9889065027236938)
[2025-02-13 03:07:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:47][root][INFO] - Training Epoch: 1/2, step 4580/7134 completed (loss: 0.019871436059474945, acc: 0.9955223798751831)
[2025-02-13 03:07:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:48][root][INFO] - Training Epoch: 1/2, step 4581/7134 completed (loss: 0.017626581713557243, acc: 0.9947090148925781)
[2025-02-13 03:07:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:48][root][INFO] - Training Epoch: 1/2, step 4582/7134 completed (loss: 0.02293034829199314, acc: 0.9917762875556946)
[2025-02-13 03:07:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:48][root][INFO] - Training Epoch: 1/2, step 4583/7134 completed (loss: 0.020801007747650146, acc: 0.9934640526771545)
[2025-02-13 03:07:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:49][root][INFO] - Training Epoch: 1/2, step 4584/7134 completed (loss: 0.03132697567343712, acc: 0.990338146686554)
[2025-02-13 03:07:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:49][root][INFO] - Training Epoch: 1/2, step 4585/7134 completed (loss: 0.03262406587600708, acc: 0.9886731505393982)
[2025-02-13 03:07:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:50][root][INFO] - Training Epoch: 1/2, step 4586/7134 completed (loss: 0.02918195351958275, acc: 0.9908972978591919)
[2025-02-13 03:07:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:50][root][INFO] - Training Epoch: 1/2, step 4587/7134 completed (loss: 0.052186716347932816, acc: 0.9849315285682678)
[2025-02-13 03:07:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:51][root][INFO] - Training Epoch: 1/2, step 4588/7134 completed (loss: 0.04580904170870781, acc: 0.9811046719551086)
[2025-02-13 03:07:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:51][root][INFO] - Training Epoch: 1/2, step 4589/7134 completed (loss: 0.06398477405309677, acc: 0.9805389046669006)
[2025-02-13 03:07:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:51][root][INFO] - Training Epoch: 1/2, step 4590/7134 completed (loss: 0.04689406976103783, acc: 0.988950252532959)
[2025-02-13 03:07:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:52][root][INFO] - Training Epoch: 1/2, step 4591/7134 completed (loss: 0.035043101757764816, acc: 0.9894459247589111)
[2025-02-13 03:07:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:52][root][INFO] - Training Epoch: 1/2, step 4592/7134 completed (loss: 0.03475184366106987, acc: 0.9912917017936707)
[2025-02-13 03:07:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:53][root][INFO] - Training Epoch: 1/2, step 4593/7134 completed (loss: 0.042692430317401886, acc: 0.9896296262741089)
[2025-02-13 03:07:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:53][root][INFO] - Training Epoch: 1/2, step 4594/7134 completed (loss: 0.04686332866549492, acc: 0.9879999756813049)
[2025-02-13 03:07:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:54][root][INFO] - Training Epoch: 1/2, step 4595/7134 completed (loss: 0.02545795403420925, acc: 0.9910846948623657)
[2025-02-13 03:07:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:54][root][INFO] - Training Epoch: 1/2, step 4596/7134 completed (loss: 0.03742677718400955, acc: 0.9904761910438538)
[2025-02-13 03:07:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:55][root][INFO] - Training Epoch: 1/2, step 4597/7134 completed (loss: 0.027454638853669167, acc: 0.9914945363998413)
[2025-02-13 03:07:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:55][root][INFO] - Training Epoch: 1/2, step 4598/7134 completed (loss: 0.05515283718705177, acc: 0.9865030646324158)
[2025-02-13 03:07:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:55][root][INFO] - Training Epoch: 1/2, step 4599/7134 completed (loss: 0.01999410055577755, acc: 0.995230495929718)
[2025-02-13 03:07:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:56][root][INFO] - Training Epoch: 1/2, step 4600/7134 completed (loss: 0.027259202674031258, acc: 0.9932340979576111)
[2025-02-13 03:07:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:56][root][INFO] - Training Epoch: 1/2, step 4601/7134 completed (loss: 0.044828031212091446, acc: 0.9856584072113037)
[2025-02-13 03:07:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:57][root][INFO] - Training Epoch: 1/2, step 4602/7134 completed (loss: 0.03867463394999504, acc: 0.9877384305000305)
[2025-02-13 03:07:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:57][root][INFO] - Training Epoch: 1/2, step 4603/7134 completed (loss: 0.04817371070384979, acc: 0.9862068891525269)
[2025-02-13 03:07:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:58][root][INFO] - Training Epoch: 1/2, step 4604/7134 completed (loss: 0.028780611231923103, acc: 0.9871588945388794)
[2025-02-13 03:07:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:58][root][INFO] - Training Epoch: 1/2, step 4605/7134 completed (loss: 0.06824016571044922, acc: 0.9781659245491028)
[2025-02-13 03:07:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:59][root][INFO] - Training Epoch: 1/2, step 4606/7134 completed (loss: 0.017888328060507774, acc: 0.9954128265380859)
[2025-02-13 03:07:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:59][root][INFO] - Training Epoch: 1/2, step 4607/7134 completed (loss: 0.018577300012111664, acc: 0.9898989796638489)
[2025-02-13 03:07:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:07:59][root][INFO] - Training Epoch: 1/2, step 4608/7134 completed (loss: 0.015823092311620712, acc: 0.9940740466117859)
[2025-02-13 03:08:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:00][root][INFO] - Training Epoch: 1/2, step 4609/7134 completed (loss: 0.02980397455394268, acc: 0.9926605224609375)
[2025-02-13 03:08:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:00][root][INFO] - Training Epoch: 1/2, step 4610/7134 completed (loss: 0.02971254102885723, acc: 0.9893758296966553)
[2025-02-13 03:08:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:01][root][INFO] - Training Epoch: 1/2, step 4611/7134 completed (loss: 0.015192430466413498, acc: 0.9968701004981995)
[2025-02-13 03:08:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:01][root][INFO] - Training Epoch: 1/2, step 4612/7134 completed (loss: 0.02022656612098217, acc: 0.9925650358200073)
[2025-02-13 03:08:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:01][root][INFO] - Training Epoch: 1/2, step 4613/7134 completed (loss: 0.023863190785050392, acc: 0.9930555820465088)
[2025-02-13 03:08:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:02][root][INFO] - Training Epoch: 1/2, step 4614/7134 completed (loss: 0.04909440875053406, acc: 0.9848484992980957)
[2025-02-13 03:08:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:02][root][INFO] - Training Epoch: 1/2, step 4615/7134 completed (loss: 0.04358817636966705, acc: 0.9810040593147278)
[2025-02-13 03:08:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:03][root][INFO] - Training Epoch: 1/2, step 4616/7134 completed (loss: 0.06433766335248947, acc: 0.9792899489402771)
[2025-02-13 03:08:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:03][root][INFO] - Training Epoch: 1/2, step 4617/7134 completed (loss: 0.030270852148532867, acc: 0.9909909963607788)
[2025-02-13 03:08:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:04][root][INFO] - Training Epoch: 1/2, step 4618/7134 completed (loss: 0.04490499943494797, acc: 0.984466016292572)
[2025-02-13 03:08:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:04][root][INFO] - Training Epoch: 1/2, step 4619/7134 completed (loss: 0.01268374640494585, acc: 0.9969834089279175)
[2025-02-13 03:08:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:04][root][INFO] - Training Epoch: 1/2, step 4620/7134 completed (loss: 0.011681128293275833, acc: 0.9985507130622864)
[2025-02-13 03:08:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:05][root][INFO] - Training Epoch: 1/2, step 4621/7134 completed (loss: 0.03387461230158806, acc: 0.9845678806304932)
[2025-02-13 03:08:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:05][root][INFO] - Training Epoch: 1/2, step 4622/7134 completed (loss: 0.032110460102558136, acc: 0.9887640476226807)
[2025-02-13 03:08:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:06][root][INFO] - Training Epoch: 1/2, step 4623/7134 completed (loss: 0.04238282889127731, acc: 0.9843137264251709)
[2025-02-13 03:08:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:06][root][INFO] - Training Epoch: 1/2, step 4624/7134 completed (loss: 0.06622030586004257, acc: 0.9821673631668091)
[2025-02-13 03:08:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:07][root][INFO] - Training Epoch: 1/2, step 4625/7134 completed (loss: 0.09169415384531021, acc: 0.9688013195991516)
[2025-02-13 03:08:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:07][root][INFO] - Training Epoch: 1/2, step 4626/7134 completed (loss: 0.06165321543812752, acc: 0.9838926196098328)
[2025-02-13 03:08:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:08][root][INFO] - Training Epoch: 1/2, step 4627/7134 completed (loss: 0.08214204013347626, acc: 0.9787557125091553)
[2025-02-13 03:08:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:08][root][INFO] - Training Epoch: 1/2, step 4628/7134 completed (loss: 0.06494120508432388, acc: 0.9840579628944397)
[2025-02-13 03:08:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:08][root][INFO] - Training Epoch: 1/2, step 4629/7134 completed (loss: 0.05268780142068863, acc: 0.9829059839248657)
[2025-02-13 03:08:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:09][root][INFO] - Training Epoch: 1/2, step 4630/7134 completed (loss: 0.05335555598139763, acc: 0.9841479659080505)
[2025-02-13 03:08:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:09][root][INFO] - Training Epoch: 1/2, step 4631/7134 completed (loss: 0.045024458318948746, acc: 0.9856194853782654)
[2025-02-13 03:08:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:10][root][INFO] - Training Epoch: 1/2, step 4632/7134 completed (loss: 0.038092467933893204, acc: 0.9955882430076599)
[2025-02-13 03:08:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:10][root][INFO] - Training Epoch: 1/2, step 4633/7134 completed (loss: 0.06423725187778473, acc: 0.9838150143623352)
[2025-02-13 03:08:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:11][root][INFO] - Training Epoch: 1/2, step 4634/7134 completed (loss: 0.04569314792752266, acc: 0.9883494973182678)
[2025-02-13 03:08:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:11][root][INFO] - Training Epoch: 1/2, step 4635/7134 completed (loss: 0.08533865958452225, acc: 0.9862805008888245)
[2025-02-13 03:08:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:11][root][INFO] - Training Epoch: 1/2, step 4636/7134 completed (loss: 0.04345576465129852, acc: 0.9845361113548279)
[2025-02-13 03:08:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:12][root][INFO] - Training Epoch: 1/2, step 4637/7134 completed (loss: 0.05528988316655159, acc: 0.9838709831237793)
[2025-02-13 03:08:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:12][root][INFO] - Training Epoch: 1/2, step 4638/7134 completed (loss: 0.07235464453697205, acc: 0.9801849126815796)
[2025-02-13 03:08:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:13][root][INFO] - Training Epoch: 1/2, step 4639/7134 completed (loss: 0.06222383305430412, acc: 0.9878345727920532)
[2025-02-13 03:08:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:13][root][INFO] - Training Epoch: 1/2, step 4640/7134 completed (loss: 0.0650406926870346, acc: 0.985897421836853)
[2025-02-13 03:08:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:14][root][INFO] - Training Epoch: 1/2, step 4641/7134 completed (loss: 0.026851754635572433, acc: 0.9928977489471436)
[2025-02-13 03:08:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:14][root][INFO] - Training Epoch: 1/2, step 4642/7134 completed (loss: 0.022671878337860107, acc: 0.9960317611694336)
[2025-02-13 03:08:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:15][root][INFO] - Training Epoch: 1/2, step 4643/7134 completed (loss: 0.03330538049340248, acc: 0.9952662587165833)
[2025-02-13 03:08:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:15][root][INFO] - Training Epoch: 1/2, step 4644/7134 completed (loss: 0.03132762759923935, acc: 0.9905325174331665)
[2025-02-13 03:08:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:15][root][INFO] - Training Epoch: 1/2, step 4645/7134 completed (loss: 0.027076276019215584, acc: 0.9930459260940552)
[2025-02-13 03:08:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:16][root][INFO] - Training Epoch: 1/2, step 4646/7134 completed (loss: 0.027730921283364296, acc: 0.9921507239341736)
[2025-02-13 03:08:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:16][root][INFO] - Training Epoch: 1/2, step 4647/7134 completed (loss: 0.0733826607465744, acc: 0.98828125)
[2025-02-13 03:08:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:17][root][INFO] - Training Epoch: 1/2, step 4648/7134 completed (loss: 0.038246627897024155, acc: 0.9903978109359741)
[2025-02-13 03:08:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:17][root][INFO] - Training Epoch: 1/2, step 4649/7134 completed (loss: 0.040268898010253906, acc: 0.9824868440628052)
[2025-02-13 03:08:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:18][root][INFO] - Training Epoch: 1/2, step 4650/7134 completed (loss: 0.056769125163555145, acc: 0.9791666865348816)
[2025-02-13 03:08:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:18][root][INFO] - Training Epoch: 1/2, step 4651/7134 completed (loss: 0.09121392667293549, acc: 0.9759188890457153)
[2025-02-13 03:08:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:19][root][INFO] - Training Epoch: 1/2, step 4652/7134 completed (loss: 0.041561562567949295, acc: 0.9893292784690857)
[2025-02-13 03:08:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:19][root][INFO] - Training Epoch: 1/2, step 4653/7134 completed (loss: 0.04308779910206795, acc: 0.9881720542907715)
[2025-02-13 03:08:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:20][root][INFO] - Training Epoch: 1/2, step 4654/7134 completed (loss: 0.05901329219341278, acc: 0.9841656684875488)
[2025-02-13 03:08:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:20][root][INFO] - Training Epoch: 1/2, step 4655/7134 completed (loss: 0.0415680967271328, acc: 0.9876881241798401)
[2025-02-13 03:08:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:20][root][INFO] - Training Epoch: 1/2, step 4656/7134 completed (loss: 0.04135524854063988, acc: 0.9886363744735718)
[2025-02-13 03:08:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:21][root][INFO] - Training Epoch: 1/2, step 4657/7134 completed (loss: 0.06301308423280716, acc: 0.9844192862510681)
[2025-02-13 03:08:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:21][root][INFO] - Training Epoch: 1/2, step 4658/7134 completed (loss: 0.0680815652012825, acc: 0.981055498123169)
[2025-02-13 03:08:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:22][root][INFO] - Training Epoch: 1/2, step 4659/7134 completed (loss: 0.0724836140871048, acc: 0.9832904934883118)
[2025-02-13 03:08:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:22][root][INFO] - Training Epoch: 1/2, step 4660/7134 completed (loss: 0.07783824950456619, acc: 0.9811912178993225)
[2025-02-13 03:08:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:23][root][INFO] - Training Epoch: 1/2, step 4661/7134 completed (loss: 0.05279712378978729, acc: 0.9832317233085632)
[2025-02-13 03:08:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:23][root][INFO] - Training Epoch: 1/2, step 4662/7134 completed (loss: 0.07475584745407104, acc: 0.976190447807312)
[2025-02-13 03:08:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:23][root][INFO] - Training Epoch: 1/2, step 4663/7134 completed (loss: 0.06632764637470245, acc: 0.98128342628479)
[2025-02-13 03:08:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:24][root][INFO] - Training Epoch: 1/2, step 4664/7134 completed (loss: 0.038438763469457626, acc: 0.9907529950141907)
[2025-02-13 03:08:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:24][root][INFO] - Training Epoch: 1/2, step 4665/7134 completed (loss: 0.03937385976314545, acc: 0.9918144345283508)
[2025-02-13 03:08:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:25][root][INFO] - Training Epoch: 1/2, step 4666/7134 completed (loss: 0.05084141343832016, acc: 0.9869621992111206)
[2025-02-13 03:08:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:25][root][INFO] - Training Epoch: 1/2, step 4667/7134 completed (loss: 0.0667719915509224, acc: 0.9827883243560791)
[2025-02-13 03:08:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:26][root][INFO] - Training Epoch: 1/2, step 4668/7134 completed (loss: 0.028898043558001518, acc: 0.9910394549369812)
[2025-02-13 03:08:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:26][root][INFO] - Training Epoch: 1/2, step 4669/7134 completed (loss: 0.030044401064515114, acc: 0.9926035404205322)
[2025-02-13 03:08:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:26][root][INFO] - Training Epoch: 1/2, step 4670/7134 completed (loss: 0.056497227400541306, acc: 0.9844054579734802)
[2025-02-13 03:08:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:27][root][INFO] - Training Epoch: 1/2, step 4671/7134 completed (loss: 0.055281370878219604, acc: 0.9830747246742249)
[2025-02-13 03:08:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:27][root][INFO] - Training Epoch: 1/2, step 4672/7134 completed (loss: 0.04278560355305672, acc: 0.9860627055168152)
[2025-02-13 03:08:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:28][root][INFO] - Training Epoch: 1/2, step 4673/7134 completed (loss: 0.05298685282468796, acc: 0.9859594106674194)
[2025-02-13 03:08:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:28][root][INFO] - Training Epoch: 1/2, step 4674/7134 completed (loss: 0.042545754462480545, acc: 0.9857369065284729)
[2025-02-13 03:08:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:29][root][INFO] - Training Epoch: 1/2, step 4675/7134 completed (loss: 0.037017595022916794, acc: 0.9898089170455933)
[2025-02-13 03:08:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:29][root][INFO] - Training Epoch: 1/2, step 4676/7134 completed (loss: 0.022823994979262352, acc: 0.9954819083213806)
[2025-02-13 03:08:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:29][root][INFO] - Training Epoch: 1/2, step 4677/7134 completed (loss: 0.057473067194223404, acc: 0.984649121761322)
[2025-02-13 03:08:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:30][root][INFO] - Training Epoch: 1/2, step 4678/7134 completed (loss: 0.049936775118112564, acc: 0.9821693897247314)
[2025-02-13 03:08:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:30][root][INFO] - Training Epoch: 1/2, step 4679/7134 completed (loss: 0.035274818539619446, acc: 0.9917241334915161)
[2025-02-13 03:08:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:31][root][INFO] - Training Epoch: 1/2, step 4680/7134 completed (loss: 0.10082488507032394, acc: 0.9742990732192993)
[2025-02-13 03:08:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:31][root][INFO] - Training Epoch: 1/2, step 4681/7134 completed (loss: 0.039601124823093414, acc: 0.9938461780548096)
[2025-02-13 03:08:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:32][root][INFO] - Training Epoch: 1/2, step 4682/7134 completed (loss: 0.020793505012989044, acc: 0.9951338171958923)
[2025-02-13 03:08:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:32][root][INFO] - Training Epoch: 1/2, step 4683/7134 completed (loss: 0.053752779960632324, acc: 0.984308123588562)
[2025-02-13 03:08:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:32][root][INFO] - Training Epoch: 1/2, step 4684/7134 completed (loss: 0.07424323260784149, acc: 0.9800838828086853)
[2025-02-13 03:08:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:33][root][INFO] - Training Epoch: 1/2, step 4685/7134 completed (loss: 0.03458591550588608, acc: 0.9907786846160889)
[2025-02-13 03:08:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:33][root][INFO] - Training Epoch: 1/2, step 4686/7134 completed (loss: 0.03018064983189106, acc: 0.9930151104927063)
[2025-02-13 03:08:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:34][root][INFO] - Training Epoch: 1/2, step 4687/7134 completed (loss: 0.14221540093421936, acc: 0.9662576913833618)
[2025-02-13 03:08:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:34][root][INFO] - Training Epoch: 1/2, step 4688/7134 completed (loss: 0.24109406769275665, acc: 0.9445585012435913)
[2025-02-13 03:08:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:35][root][INFO] - Training Epoch: 1/2, step 4689/7134 completed (loss: 0.11684880405664444, acc: 0.969072163105011)
[2025-02-13 03:08:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:35][root][INFO] - Training Epoch: 1/2, step 4690/7134 completed (loss: 0.08837511390447617, acc: 0.9790356159210205)
[2025-02-13 03:08:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:36][root][INFO] - Training Epoch: 1/2, step 4691/7134 completed (loss: 0.03540942817926407, acc: 0.9875283241271973)
[2025-02-13 03:08:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:36][root][INFO] - Training Epoch: 1/2, step 4692/7134 completed (loss: 0.1542651653289795, acc: 0.9639065861701965)
[2025-02-13 03:08:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:36][root][INFO] - Training Epoch: 1/2, step 4693/7134 completed (loss: 0.15654583275318146, acc: 0.9624060392379761)
[2025-02-13 03:08:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:37][root][INFO] - Training Epoch: 1/2, step 4694/7134 completed (loss: 0.16294468939304352, acc: 0.9675456285476685)
[2025-02-13 03:08:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:37][root][INFO] - Training Epoch: 1/2, step 4695/7134 completed (loss: 0.0707230344414711, acc: 0.9835796356201172)
[2025-02-13 03:08:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:38][root][INFO] - Training Epoch: 1/2, step 4696/7134 completed (loss: 0.030274182558059692, acc: 0.9910846948623657)
[2025-02-13 03:08:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:38][root][INFO] - Training Epoch: 1/2, step 4697/7134 completed (loss: 0.042957305908203125, acc: 0.9830827116966248)
[2025-02-13 03:08:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:38][root][INFO] - Training Epoch: 1/2, step 4698/7134 completed (loss: 0.13226604461669922, acc: 0.9713740348815918)
[2025-02-13 03:08:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:39][root][INFO] - Training Epoch: 1/2, step 4699/7134 completed (loss: 0.06393681466579437, acc: 0.9780621528625488)
[2025-02-13 03:08:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:39][root][INFO] - Training Epoch: 1/2, step 4700/7134 completed (loss: 0.05915910005569458, acc: 0.9850968718528748)
[2025-02-13 03:08:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:40][root][INFO] - Training Epoch: 1/2, step 4701/7134 completed (loss: 0.08101367950439453, acc: 0.9776358008384705)
[2025-02-13 03:08:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:40][root][INFO] - Training Epoch: 1/2, step 4702/7134 completed (loss: 0.044024888426065445, acc: 0.9856687784194946)
[2025-02-13 03:08:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:41][root][INFO] - Training Epoch: 1/2, step 4703/7134 completed (loss: 0.05234122276306152, acc: 0.9905405640602112)
[2025-02-13 03:08:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:41][root][INFO] - Training Epoch: 1/2, step 4704/7134 completed (loss: 0.026514975354075432, acc: 0.9906213283538818)
[2025-02-13 03:08:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:41][root][INFO] - Training Epoch: 1/2, step 4705/7134 completed (loss: 0.03554737567901611, acc: 0.9894737005233765)
[2025-02-13 03:08:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:42][root][INFO] - Training Epoch: 1/2, step 4706/7134 completed (loss: 0.08192449063062668, acc: 0.9735202789306641)
[2025-02-13 03:08:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:42][root][INFO] - Training Epoch: 1/2, step 4707/7134 completed (loss: 0.19451938569545746, acc: 0.9489796161651611)
[2025-02-13 03:08:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:43][root][INFO] - Training Epoch: 1/2, step 4708/7134 completed (loss: 0.023726709187030792, acc: 0.9912717938423157)
[2025-02-13 03:08:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:43][root][INFO] - Training Epoch: 1/2, step 4709/7134 completed (loss: 0.03197668120265007, acc: 0.990777313709259)
[2025-02-13 03:08:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:44][root][INFO] - Training Epoch: 1/2, step 4710/7134 completed (loss: 0.03881193324923515, acc: 0.9883551597595215)
[2025-02-13 03:08:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:44][root][INFO] - Training Epoch: 1/2, step 4711/7134 completed (loss: 0.07672230899333954, acc: 0.9813242554664612)
[2025-02-13 03:08:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:45][root][INFO] - Training Epoch: 1/2, step 4712/7134 completed (loss: 0.08900205790996552, acc: 0.9772727489471436)
[2025-02-13 03:08:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:45][root][INFO] - Training Epoch: 1/2, step 4713/7134 completed (loss: 0.038941629230976105, acc: 0.987500011920929)
[2025-02-13 03:08:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:45][root][INFO] - Training Epoch: 1/2, step 4714/7134 completed (loss: 0.05783524736762047, acc: 0.9850467443466187)
[2025-02-13 03:08:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:46][root][INFO] - Training Epoch: 1/2, step 4715/7134 completed (loss: 0.12135784327983856, acc: 0.9722814559936523)
[2025-02-13 03:08:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:46][root][INFO] - Training Epoch: 1/2, step 4716/7134 completed (loss: 0.015570386312901974, acc: 0.9969512224197388)
[2025-02-13 03:08:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:47][root][INFO] - Training Epoch: 1/2, step 4717/7134 completed (loss: 0.046859804540872574, acc: 0.9911110997200012)
[2025-02-13 03:08:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:47][root][INFO] - Training Epoch: 1/2, step 4718/7134 completed (loss: 0.024486025795340538, acc: 0.9933775067329407)
[2025-02-13 03:08:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:47][root][INFO] - Training Epoch: 1/2, step 4719/7134 completed (loss: 0.032031163573265076, acc: 0.9941002726554871)
[2025-02-13 03:08:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:48][root][INFO] - Training Epoch: 1/2, step 4720/7134 completed (loss: 0.01678438112139702, acc: 0.9957924485206604)
[2025-02-13 03:08:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:48][root][INFO] - Training Epoch: 1/2, step 4721/7134 completed (loss: 0.005606016609817743, acc: 1.0)
[2025-02-13 03:08:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:49][root][INFO] - Training Epoch: 1/2, step 4722/7134 completed (loss: 0.019097192212939262, acc: 0.9955621361732483)
[2025-02-13 03:08:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:49][root][INFO] - Training Epoch: 1/2, step 4723/7134 completed (loss: 0.01307984720915556, acc: 0.9969512224197388)
[2025-02-13 03:08:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:49][root][INFO] - Training Epoch: 1/2, step 4724/7134 completed (loss: 0.025835087522864342, acc: 0.9926035404205322)
[2025-02-13 03:08:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:50][root][INFO] - Training Epoch: 1/2, step 4725/7134 completed (loss: 0.02106788195669651, acc: 0.9934036731719971)
[2025-02-13 03:08:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:50][root][INFO] - Training Epoch: 1/2, step 4726/7134 completed (loss: 0.06435029953718185, acc: 0.980141818523407)
[2025-02-13 03:08:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:51][root][INFO] - Training Epoch: 1/2, step 4727/7134 completed (loss: 0.029979681596159935, acc: 0.9921630024909973)
[2025-02-13 03:08:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:51][root][INFO] - Training Epoch: 1/2, step 4728/7134 completed (loss: 0.03703654557466507, acc: 0.9917627573013306)
[2025-02-13 03:08:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:52][root][INFO] - Training Epoch: 1/2, step 4729/7134 completed (loss: 0.016657523810863495, acc: 0.9957746267318726)
[2025-02-13 03:08:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:52][root][INFO] - Training Epoch: 1/2, step 4730/7134 completed (loss: 0.021489832550287247, acc: 0.992343008518219)
[2025-02-13 03:08:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:52][root][INFO] - Training Epoch: 1/2, step 4731/7134 completed (loss: 0.04552799090743065, acc: 0.9877862334251404)
[2025-02-13 03:08:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:53][root][INFO] - Training Epoch: 1/2, step 4732/7134 completed (loss: 0.08696524053812027, acc: 0.9782214164733887)
[2025-02-13 03:08:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:53][root][INFO] - Training Epoch: 1/2, step 4733/7134 completed (loss: 0.009551525115966797, acc: 0.9967690110206604)
[2025-02-13 03:08:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:54][root][INFO] - Training Epoch: 1/2, step 4734/7134 completed (loss: 0.050862621515989304, acc: 0.9856915473937988)
[2025-02-13 03:08:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:54][root][INFO] - Training Epoch: 1/2, step 4735/7134 completed (loss: 0.039382204413414, acc: 0.9894894957542419)
[2025-02-13 03:08:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:55][root][INFO] - Training Epoch: 1/2, step 4736/7134 completed (loss: 0.07323184609413147, acc: 0.9842209219932556)
[2025-02-13 03:08:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:55][root][INFO] - Training Epoch: 1/2, step 4737/7134 completed (loss: 0.034357063472270966, acc: 0.9921630024909973)
[2025-02-13 03:08:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:55][root][INFO] - Training Epoch: 1/2, step 4738/7134 completed (loss: 0.03357628360390663, acc: 0.9863429665565491)
[2025-02-13 03:08:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:56][root][INFO] - Training Epoch: 1/2, step 4739/7134 completed (loss: 0.08864608407020569, acc: 0.9728434681892395)
[2025-02-13 03:08:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:56][root][INFO] - Training Epoch: 1/2, step 4740/7134 completed (loss: 0.08110003918409348, acc: 0.9745509028434753)
[2025-02-13 03:08:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:57][root][INFO] - Training Epoch: 1/2, step 4741/7134 completed (loss: 0.0602702721953392, acc: 0.9796673059463501)
[2025-02-13 03:08:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:57][root][INFO] - Training Epoch: 1/2, step 4742/7134 completed (loss: 0.10545382648706436, acc: 0.9787610769271851)
[2025-02-13 03:08:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:57][root][INFO] - Training Epoch: 1/2, step 4743/7134 completed (loss: 0.05389135330915451, acc: 0.9839857816696167)
[2025-02-13 03:08:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:58][root][INFO] - Training Epoch: 1/2, step 4744/7134 completed (loss: 0.030420703813433647, acc: 0.9917355179786682)
[2025-02-13 03:08:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:58][root][INFO] - Training Epoch: 1/2, step 4745/7134 completed (loss: 0.061078060418367386, acc: 0.9866220951080322)
[2025-02-13 03:08:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:59][root][INFO] - Training Epoch: 1/2, step 4746/7134 completed (loss: 0.04513020068407059, acc: 0.9871323704719543)
[2025-02-13 03:08:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:08:59][root][INFO] - Training Epoch: 1/2, step 4747/7134 completed (loss: 0.036509811878204346, acc: 0.9905533194541931)
[2025-02-13 03:08:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:00][root][INFO] - Training Epoch: 1/2, step 4748/7134 completed (loss: 0.08305928111076355, acc: 0.9675516486167908)
[2025-02-13 03:09:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:00][root][INFO] - Training Epoch: 1/2, step 4749/7134 completed (loss: 0.048262909054756165, acc: 0.9866666793823242)
[2025-02-13 03:09:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:00][root][INFO] - Training Epoch: 1/2, step 4750/7134 completed (loss: 0.08895330876111984, acc: 0.978805422782898)
[2025-02-13 03:09:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:01][root][INFO] - Training Epoch: 1/2, step 4751/7134 completed (loss: 0.017055535688996315, acc: 0.9957567453384399)
[2025-02-13 03:09:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:01][root][INFO] - Training Epoch: 1/2, step 4752/7134 completed (loss: 0.06362903863191605, acc: 0.9877111911773682)
[2025-02-13 03:09:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:02][root][INFO] - Training Epoch: 1/2, step 4753/7134 completed (loss: 0.08112313598394394, acc: 0.9804216623306274)
[2025-02-13 03:09:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:02][root][INFO] - Training Epoch: 1/2, step 4754/7134 completed (loss: 0.022397005930542946, acc: 0.9955947399139404)
[2025-02-13 03:09:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:03][root][INFO] - Training Epoch: 1/2, step 4755/7134 completed (loss: 0.06143898516893387, acc: 0.9847434163093567)
[2025-02-13 03:09:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:03][root][INFO] - Training Epoch: 1/2, step 4756/7134 completed (loss: 0.08844452351331711, acc: 0.9811715483665466)
[2025-02-13 03:09:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:04][root][INFO] - Training Epoch: 1/2, step 4757/7134 completed (loss: 0.02477932535111904, acc: 0.991963267326355)
[2025-02-13 03:09:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:04][root][INFO] - Training Epoch: 1/2, step 4758/7134 completed (loss: 0.046271465718746185, acc: 0.9873949289321899)
[2025-02-13 03:09:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:04][root][INFO] - Training Epoch: 1/2, step 4759/7134 completed (loss: 0.020609544590115547, acc: 0.9948275685310364)
[2025-02-13 03:09:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:05][root][INFO] - Training Epoch: 1/2, step 4760/7134 completed (loss: 0.04718795791268349, acc: 0.9855072498321533)
[2025-02-13 03:09:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:05][root][INFO] - Training Epoch: 1/2, step 4761/7134 completed (loss: 0.06136399880051613, acc: 0.9908397197723389)
[2025-02-13 03:09:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:06][root][INFO] - Training Epoch: 1/2, step 4762/7134 completed (loss: 0.06150737404823303, acc: 0.9891892075538635)
[2025-02-13 03:09:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:06][root][INFO] - Training Epoch: 1/2, step 4763/7134 completed (loss: 0.05402415990829468, acc: 0.9848024249076843)
[2025-02-13 03:09:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:07][root][INFO] - Training Epoch: 1/2, step 4764/7134 completed (loss: 0.04283452406525612, acc: 0.9876390695571899)
[2025-02-13 03:09:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:07][root][INFO] - Training Epoch: 1/2, step 4765/7134 completed (loss: 0.08110957592725754, acc: 0.9856114983558655)
[2025-02-13 03:09:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:07][root][INFO] - Training Epoch: 1/2, step 4766/7134 completed (loss: 0.0402388758957386, acc: 0.9909909963607788)
[2025-02-13 03:09:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:08][root][INFO] - Training Epoch: 1/2, step 4767/7134 completed (loss: 0.06674113124608994, acc: 0.9770992398262024)
[2025-02-13 03:09:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:08][root][INFO] - Training Epoch: 1/2, step 4768/7134 completed (loss: 0.05942255258560181, acc: 0.9823848009109497)
[2025-02-13 03:09:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:09][root][INFO] - Training Epoch: 1/2, step 4769/7134 completed (loss: 0.06157507374882698, acc: 0.9855538010597229)
[2025-02-13 03:09:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:09][root][INFO] - Training Epoch: 1/2, step 4770/7134 completed (loss: 0.028574882075190544, acc: 0.9939393997192383)
[2025-02-13 03:09:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:10][root][INFO] - Training Epoch: 1/2, step 4771/7134 completed (loss: 0.0765119269490242, acc: 0.9781976938247681)
[2025-02-13 03:09:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:10][root][INFO] - Training Epoch: 1/2, step 4772/7134 completed (loss: 0.05607466772198677, acc: 0.9904305934906006)
[2025-02-13 03:09:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:10][root][INFO] - Training Epoch: 1/2, step 4773/7134 completed (loss: 0.039661116898059845, acc: 0.9890410900115967)
[2025-02-13 03:09:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:11][root][INFO] - Training Epoch: 1/2, step 4774/7134 completed (loss: 0.03325209394097328, acc: 0.992977499961853)
[2025-02-13 03:09:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:11][root][INFO] - Training Epoch: 1/2, step 4775/7134 completed (loss: 0.053029660135507584, acc: 0.988727867603302)
[2025-02-13 03:09:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:12][root][INFO] - Training Epoch: 1/2, step 4776/7134 completed (loss: 0.049700237810611725, acc: 0.9871588945388794)
[2025-02-13 03:09:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:12][root][INFO] - Training Epoch: 1/2, step 4777/7134 completed (loss: 0.0249808169901371, acc: 0.994991660118103)
[2025-02-13 03:09:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:12][root][INFO] - Training Epoch: 1/2, step 4778/7134 completed (loss: 0.03863180801272392, acc: 0.9934354424476624)
[2025-02-13 03:09:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:13][root][INFO] - Training Epoch: 1/2, step 4779/7134 completed (loss: 0.018911724910140038, acc: 0.9944751262664795)
[2025-02-13 03:09:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:13][root][INFO] - Training Epoch: 1/2, step 4780/7134 completed (loss: 0.009025785140693188, acc: 0.9968847632408142)
[2025-02-13 03:09:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:14][root][INFO] - Training Epoch: 1/2, step 4781/7134 completed (loss: 0.0254695862531662, acc: 0.9921721816062927)
[2025-02-13 03:09:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:14][root][INFO] - Training Epoch: 1/2, step 4782/7134 completed (loss: 0.018290143460035324, acc: 0.9918962717056274)
[2025-02-13 03:09:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:15][root][INFO] - Training Epoch: 1/2, step 4783/7134 completed (loss: 0.07278229296207428, acc: 0.982206404209137)
[2025-02-13 03:09:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:15][root][INFO] - Training Epoch: 1/2, step 4784/7134 completed (loss: 0.013785231858491898, acc: 0.9972714781761169)
[2025-02-13 03:09:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:15][root][INFO] - Training Epoch: 1/2, step 4785/7134 completed (loss: 0.047128304839134216, acc: 0.9943820238113403)
[2025-02-13 03:09:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:16][root][INFO] - Training Epoch: 1/2, step 4786/7134 completed (loss: 0.04232528805732727, acc: 0.9906832575798035)
[2025-02-13 03:09:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:16][root][INFO] - Training Epoch: 1/2, step 4787/7134 completed (loss: 0.008727620355784893, acc: 0.9984177350997925)
[2025-02-13 03:09:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:17][root][INFO] - Training Epoch: 1/2, step 4788/7134 completed (loss: 0.005368728190660477, acc: 1.0)
[2025-02-13 03:09:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:17][root][INFO] - Training Epoch: 1/2, step 4789/7134 completed (loss: 0.028701459988951683, acc: 0.993630588054657)
[2025-02-13 03:09:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:18][root][INFO] - Training Epoch: 1/2, step 4790/7134 completed (loss: 0.026221493259072304, acc: 0.9932065010070801)
[2025-02-13 03:09:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:18][root][INFO] - Training Epoch: 1/2, step 4791/7134 completed (loss: 0.017920149490237236, acc: 0.9962756037712097)
[2025-02-13 03:09:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:18][root][INFO] - Training Epoch: 1/2, step 4792/7134 completed (loss: 0.018295645713806152, acc: 0.9933110475540161)
[2025-02-13 03:09:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:19][root][INFO] - Training Epoch: 1/2, step 4793/7134 completed (loss: 0.07320285588502884, acc: 0.9753424525260925)
[2025-02-13 03:09:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:19][root][INFO] - Training Epoch: 1/2, step 4794/7134 completed (loss: 0.08478099852800369, acc: 0.9714285731315613)
[2025-02-13 03:09:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:20][root][INFO] - Training Epoch: 1/2, step 4795/7134 completed (loss: 0.02761041186749935, acc: 0.9887005686759949)
[2025-02-13 03:09:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:20][root][INFO] - Training Epoch: 1/2, step 4796/7134 completed (loss: 0.028923070058226585, acc: 0.9889937043190002)
[2025-02-13 03:09:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:21][root][INFO] - Training Epoch: 1/2, step 4797/7134 completed (loss: 0.029747532680630684, acc: 0.9937888383865356)
[2025-02-13 03:09:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:21][root][INFO] - Training Epoch: 1/2, step 4798/7134 completed (loss: 0.03802292048931122, acc: 0.9880159497261047)
[2025-02-13 03:09:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:21][root][INFO] - Training Epoch: 1/2, step 4799/7134 completed (loss: 0.02474268525838852, acc: 0.9900990128517151)
[2025-02-13 03:09:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:22][root][INFO] - Training Epoch: 1/2, step 4800/7134 completed (loss: 0.02731074020266533, acc: 0.992548406124115)
[2025-02-13 03:09:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:22][root][INFO] - Training Epoch: 1/2, step 4801/7134 completed (loss: 0.02663605473935604, acc: 0.9954751133918762)
[2025-02-13 03:09:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:23][root][INFO] - Training Epoch: 1/2, step 4802/7134 completed (loss: 0.05169001594185829, acc: 0.985602080821991)
[2025-02-13 03:09:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:23][root][INFO] - Training Epoch: 1/2, step 4803/7134 completed (loss: 0.06954650580883026, acc: 0.9838235378265381)
[2025-02-13 03:09:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:24][root][INFO] - Training Epoch: 1/2, step 4804/7134 completed (loss: 0.045351944863796234, acc: 0.9876922965049744)
[2025-02-13 03:09:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:24][root][INFO] - Training Epoch: 1/2, step 4805/7134 completed (loss: 0.037137966603040695, acc: 0.9902912378311157)
[2025-02-13 03:09:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:24][root][INFO] - Training Epoch: 1/2, step 4806/7134 completed (loss: 0.03399435430765152, acc: 0.9933444261550903)
[2025-02-13 03:09:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:25][root][INFO] - Training Epoch: 1/2, step 4807/7134 completed (loss: 0.08026571571826935, acc: 0.9908814430236816)
[2025-02-13 03:09:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:25][root][INFO] - Training Epoch: 1/2, step 4808/7134 completed (loss: 0.029615111649036407, acc: 0.9929078221321106)
[2025-02-13 03:09:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:26][root][INFO] - Training Epoch: 1/2, step 4809/7134 completed (loss: 0.03213154152035713, acc: 0.9888888597488403)
[2025-02-13 03:09:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:26][root][INFO] - Training Epoch: 1/2, step 4810/7134 completed (loss: 0.05720103904604912, acc: 0.9900142550468445)
[2025-02-13 03:09:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:27][root][INFO] - Training Epoch: 1/2, step 4811/7134 completed (loss: 0.05427378788590431, acc: 0.9903100728988647)
[2025-02-13 03:09:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:27][root][INFO] - Training Epoch: 1/2, step 4812/7134 completed (loss: 0.0805593729019165, acc: 0.9805285334587097)
[2025-02-13 03:09:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:27][root][INFO] - Training Epoch: 1/2, step 4813/7134 completed (loss: 0.01220146007835865, acc: 0.9974026083946228)
[2025-02-13 03:09:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:28][root][INFO] - Training Epoch: 1/2, step 4814/7134 completed (loss: 0.03381958603858948, acc: 0.9923469424247742)
[2025-02-13 03:09:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:28][root][INFO] - Training Epoch: 1/2, step 4815/7134 completed (loss: 0.006514975801110268, acc: 1.0)
[2025-02-13 03:09:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:29][root][INFO] - Training Epoch: 1/2, step 4816/7134 completed (loss: 0.012878747656941414, acc: 0.9975247383117676)
[2025-02-13 03:09:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:29][root][INFO] - Training Epoch: 1/2, step 4817/7134 completed (loss: 0.019881291314959526, acc: 0.9927710890769958)
[2025-02-13 03:09:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:29][root][INFO] - Training Epoch: 1/2, step 4818/7134 completed (loss: 0.0258522629737854, acc: 0.9953846335411072)
[2025-02-13 03:09:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:30][root][INFO] - Training Epoch: 1/2, step 4819/7134 completed (loss: 0.03691202029585838, acc: 0.9878296256065369)
[2025-02-13 03:09:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:30][root][INFO] - Training Epoch: 1/2, step 4820/7134 completed (loss: 0.01905633695423603, acc: 0.9961389899253845)
[2025-02-13 03:09:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:31][root][INFO] - Training Epoch: 1/2, step 4821/7134 completed (loss: 0.019006848335266113, acc: 0.994413435459137)
[2025-02-13 03:09:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:31][root][INFO] - Training Epoch: 1/2, step 4822/7134 completed (loss: 0.02561192959547043, acc: 0.996219277381897)
[2025-02-13 03:09:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:32][root][INFO] - Training Epoch: 1/2, step 4823/7134 completed (loss: 0.06219085305929184, acc: 0.9842022061347961)
[2025-02-13 03:09:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:32][root][INFO] - Training Epoch: 1/2, step 4824/7134 completed (loss: 0.05202798917889595, acc: 0.9863013625144958)
[2025-02-13 03:09:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:32][root][INFO] - Training Epoch: 1/2, step 4825/7134 completed (loss: 0.01846049726009369, acc: 0.9945255517959595)
[2025-02-13 03:09:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:33][root][INFO] - Training Epoch: 1/2, step 4826/7134 completed (loss: 0.02788757160305977, acc: 0.991416335105896)
[2025-02-13 03:09:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:33][root][INFO] - Training Epoch: 1/2, step 4827/7134 completed (loss: 0.03370651602745056, acc: 0.991349458694458)
[2025-02-13 03:09:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:34][root][INFO] - Training Epoch: 1/2, step 4828/7134 completed (loss: 0.018330732360482216, acc: 0.9951397180557251)
[2025-02-13 03:09:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:34][root][INFO] - Training Epoch: 1/2, step 4829/7134 completed (loss: 0.01584361121058464, acc: 0.9948717951774597)
[2025-02-13 03:09:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:35][root][INFO] - Training Epoch: 1/2, step 4830/7134 completed (loss: 0.012639128603041172, acc: 0.9955621361732483)
[2025-02-13 03:09:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:35][root][INFO] - Training Epoch: 1/2, step 4831/7134 completed (loss: 0.012117049656808376, acc: 0.9956140518188477)
[2025-02-13 03:09:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:36][root][INFO] - Training Epoch: 1/2, step 4832/7134 completed (loss: 0.012249676510691643, acc: 0.9983713626861572)
[2025-02-13 03:09:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:36][root][INFO] - Training Epoch: 1/2, step 4833/7134 completed (loss: 0.015705138444900513, acc: 0.9949173927307129)
[2025-02-13 03:09:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:36][root][INFO] - Training Epoch: 1/2, step 4834/7134 completed (loss: 0.019904756918549538, acc: 0.9940476417541504)
[2025-02-13 03:09:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:37][root][INFO] - Training Epoch: 1/2, step 4835/7134 completed (loss: 0.010772463865578175, acc: 0.9972972869873047)
[2025-02-13 03:09:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:37][root][INFO] - Training Epoch: 1/2, step 4836/7134 completed (loss: 0.012676463462412357, acc: 0.9969465732574463)
[2025-02-13 03:09:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:38][root][INFO] - Training Epoch: 1/2, step 4837/7134 completed (loss: 0.07958531379699707, acc: 0.9797794222831726)
[2025-02-13 03:09:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:38][root][INFO] - Training Epoch: 1/2, step 4838/7134 completed (loss: 0.03055938333272934, acc: 0.9935587644577026)
[2025-02-13 03:09:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:39][root][INFO] - Training Epoch: 1/2, step 4839/7134 completed (loss: 0.050592370331287384, acc: 0.9896729588508606)
[2025-02-13 03:09:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:39][root][INFO] - Training Epoch: 1/2, step 4840/7134 completed (loss: 0.038621049374341965, acc: 0.986994206905365)
[2025-02-13 03:09:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:39][root][INFO] - Training Epoch: 1/2, step 4841/7134 completed (loss: 0.03868352249264717, acc: 0.9865871667861938)
[2025-02-13 03:09:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:40][root][INFO] - Training Epoch: 1/2, step 4842/7134 completed (loss: 0.037286195904016495, acc: 0.9910714030265808)
[2025-02-13 03:09:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:40][root][INFO] - Training Epoch: 1/2, step 4843/7134 completed (loss: 0.043407149612903595, acc: 0.9881129264831543)
[2025-02-13 03:09:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:41][root][INFO] - Training Epoch: 1/2, step 4844/7134 completed (loss: 0.03805708885192871, acc: 0.9903069734573364)
[2025-02-13 03:09:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:41][root][INFO] - Training Epoch: 1/2, step 4845/7134 completed (loss: 0.030962564051151276, acc: 0.9931694269180298)
[2025-02-13 03:09:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:42][root][INFO] - Training Epoch: 1/2, step 4846/7134 completed (loss: 0.029852116480469704, acc: 0.9878378510475159)
[2025-02-13 03:09:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:42][root][INFO] - Training Epoch: 1/2, step 4847/7134 completed (loss: 0.023535335436463356, acc: 0.9919137358665466)
[2025-02-13 03:09:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:42][root][INFO] - Training Epoch: 1/2, step 4848/7134 completed (loss: 0.026956094428896904, acc: 0.9944751262664795)
[2025-02-13 03:09:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:43][root][INFO] - Training Epoch: 1/2, step 4849/7134 completed (loss: 0.028017444536089897, acc: 0.9940564632415771)
[2025-02-13 03:09:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:43][root][INFO] - Training Epoch: 1/2, step 4850/7134 completed (loss: 0.04726092889904976, acc: 0.9853121042251587)
[2025-02-13 03:09:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:44][root][INFO] - Training Epoch: 1/2, step 4851/7134 completed (loss: 0.042918622493743896, acc: 0.9871495366096497)
[2025-02-13 03:09:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:44][root][INFO] - Training Epoch: 1/2, step 4852/7134 completed (loss: 0.041264940053224564, acc: 0.9853528738021851)
[2025-02-13 03:09:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:45][root][INFO] - Training Epoch: 1/2, step 4853/7134 completed (loss: 0.0761750116944313, acc: 0.9828816056251526)
[2025-02-13 03:09:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:45][root][INFO] - Training Epoch: 1/2, step 4854/7134 completed (loss: 0.02468019165098667, acc: 0.9919999837875366)
[2025-02-13 03:09:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:45][root][INFO] - Training Epoch: 1/2, step 4855/7134 completed (loss: 0.02772095985710621, acc: 0.9891172647476196)
[2025-02-13 03:09:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:46][root][INFO] - Training Epoch: 1/2, step 4856/7134 completed (loss: 0.0348597876727581, acc: 0.9923664331436157)
[2025-02-13 03:09:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:46][root][INFO] - Training Epoch: 1/2, step 4857/7134 completed (loss: 0.013225389644503593, acc: 0.9956834316253662)
[2025-02-13 03:09:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:47][root][INFO] - Training Epoch: 1/2, step 4858/7134 completed (loss: 0.029026417061686516, acc: 0.9900285005569458)
[2025-02-13 03:09:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:47][root][INFO] - Training Epoch: 1/2, step 4859/7134 completed (loss: 0.018887558951973915, acc: 0.993565022945404)
[2025-02-13 03:09:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:48][root][INFO] - Training Epoch: 1/2, step 4860/7134 completed (loss: 0.01817443035542965, acc: 0.9945504069328308)
[2025-02-13 03:09:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:48][root][INFO] - Training Epoch: 1/2, step 4861/7134 completed (loss: 0.027537941932678223, acc: 0.9919137358665466)
[2025-02-13 03:09:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:48][root][INFO] - Training Epoch: 1/2, step 4862/7134 completed (loss: 0.021701034158468246, acc: 0.9928469061851501)
[2025-02-13 03:09:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:49][root][INFO] - Training Epoch: 1/2, step 4863/7134 completed (loss: 0.031152868643403053, acc: 0.989051103591919)
[2025-02-13 03:09:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:49][root][INFO] - Training Epoch: 1/2, step 4864/7134 completed (loss: 0.02955174446105957, acc: 0.9861538410186768)
[2025-02-13 03:09:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:50][root][INFO] - Training Epoch: 1/2, step 4865/7134 completed (loss: 0.03984149545431137, acc: 0.9890109896659851)
[2025-02-13 03:09:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:50][root][INFO] - Training Epoch: 1/2, step 4866/7134 completed (loss: 0.03373388573527336, acc: 0.990920901298523)
[2025-02-13 03:09:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:51][root][INFO] - Training Epoch: 1/2, step 4867/7134 completed (loss: 0.04149114340543747, acc: 0.9894179701805115)
[2025-02-13 03:09:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:51][root][INFO] - Training Epoch: 1/2, step 4868/7134 completed (loss: 0.06685111671686172, acc: 0.9786821603775024)
[2025-02-13 03:09:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:51][root][INFO] - Training Epoch: 1/2, step 4869/7134 completed (loss: 0.04729393869638443, acc: 0.9888888597488403)
[2025-02-13 03:09:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:52][root][INFO] - Training Epoch: 1/2, step 4870/7134 completed (loss: 0.0272582545876503, acc: 0.9953051805496216)
[2025-02-13 03:09:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:52][root][INFO] - Training Epoch: 1/2, step 4871/7134 completed (loss: 0.029506953433156013, acc: 0.9953703880310059)
[2025-02-13 03:09:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:53][root][INFO] - Training Epoch: 1/2, step 4872/7134 completed (loss: 0.033189814537763596, acc: 0.9930459260940552)
[2025-02-13 03:09:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:53][root][INFO] - Training Epoch: 1/2, step 4873/7134 completed (loss: 0.01651090569794178, acc: 0.9934895634651184)
[2025-02-13 03:09:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:54][root][INFO] - Training Epoch: 1/2, step 4874/7134 completed (loss: 0.02243822067975998, acc: 0.9908571243286133)
[2025-02-13 03:09:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:54][root][INFO] - Training Epoch: 1/2, step 4875/7134 completed (loss: 0.0389292873442173, acc: 0.9861303567886353)
[2025-02-13 03:09:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:54][root][INFO] - Training Epoch: 1/2, step 4876/7134 completed (loss: 0.09255249053239822, acc: 0.9788583517074585)
[2025-02-13 03:09:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:55][root][INFO] - Training Epoch: 1/2, step 4877/7134 completed (loss: 0.041723478585481644, acc: 0.9870967864990234)
[2025-02-13 03:09:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:55][root][INFO] - Training Epoch: 1/2, step 4878/7134 completed (loss: 0.039078228175640106, acc: 0.9837996959686279)
[2025-02-13 03:09:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:56][root][INFO] - Training Epoch: 1/2, step 4879/7134 completed (loss: 0.07031130790710449, acc: 0.983433723449707)
[2025-02-13 03:09:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:56][root][INFO] - Training Epoch: 1/2, step 4880/7134 completed (loss: 0.013778263702988625, acc: 0.993514895439148)
[2025-02-13 03:09:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:57][root][INFO] - Training Epoch: 1/2, step 4881/7134 completed (loss: 0.04149310663342476, acc: 0.985401451587677)
[2025-02-13 03:09:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:57][root][INFO] - Training Epoch: 1/2, step 4882/7134 completed (loss: 0.039353128522634506, acc: 0.9868420958518982)
[2025-02-13 03:09:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:58][root][INFO] - Training Epoch: 1/2, step 4883/7134 completed (loss: 0.04035293310880661, acc: 0.9824817776679993)
[2025-02-13 03:09:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:58][root][INFO] - Training Epoch: 1/2, step 4884/7134 completed (loss: 0.009064551442861557, acc: 0.9984639286994934)
[2025-02-13 03:09:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:58][root][INFO] - Training Epoch: 1/2, step 4885/7134 completed (loss: 0.01578962616622448, acc: 0.9973614811897278)
[2025-02-13 03:09:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:59][root][INFO] - Training Epoch: 1/2, step 4886/7134 completed (loss: 0.03295087069272995, acc: 0.9948717951774597)
[2025-02-13 03:09:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:09:59][root][INFO] - Training Epoch: 1/2, step 4887/7134 completed (loss: 0.017878061160445213, acc: 0.9928315281867981)
[2025-02-13 03:09:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:00][root][INFO] - Training Epoch: 1/2, step 4888/7134 completed (loss: 0.046307459473609924, acc: 0.9917582273483276)
[2025-02-13 03:10:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:00][root][INFO] - Training Epoch: 1/2, step 4889/7134 completed (loss: 0.016864357516169548, acc: 0.9951865077018738)
[2025-02-13 03:10:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:01][root][INFO] - Training Epoch: 1/2, step 4890/7134 completed (loss: 0.045346107333898544, acc: 0.9928571581840515)
[2025-02-13 03:10:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:01][root][INFO] - Training Epoch: 1/2, step 4891/7134 completed (loss: 0.0380813330411911, acc: 0.9875621795654297)
[2025-02-13 03:10:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:01][root][INFO] - Training Epoch: 1/2, step 4892/7134 completed (loss: 0.06334012746810913, acc: 0.9848993420600891)
[2025-02-13 03:10:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:02][root][INFO] - Training Epoch: 1/2, step 4893/7134 completed (loss: 0.1736651510000229, acc: 0.960422158241272)
[2025-02-13 03:10:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:02][root][INFO] - Training Epoch: 1/2, step 4894/7134 completed (loss: 0.013894309289753437, acc: 0.9985590577125549)
[2025-02-13 03:10:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:03][root][INFO] - Training Epoch: 1/2, step 4895/7134 completed (loss: 0.019142473116517067, acc: 0.9950000047683716)
[2025-02-13 03:10:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:03][root][INFO] - Training Epoch: 1/2, step 4896/7134 completed (loss: 0.013253126293420792, acc: 0.9957864880561829)
[2025-02-13 03:10:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:04][root][INFO] - Training Epoch: 1/2, step 4897/7134 completed (loss: 0.034376051276922226, acc: 0.9906432628631592)
[2025-02-13 03:10:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:04][root][INFO] - Training Epoch: 1/2, step 4898/7134 completed (loss: 0.040713608264923096, acc: 0.9861634969711304)
[2025-02-13 03:10:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:04][root][INFO] - Training Epoch: 1/2, step 4899/7134 completed (loss: 0.03351093456149101, acc: 0.9898107647895813)
[2025-02-13 03:10:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:05][root][INFO] - Training Epoch: 1/2, step 4900/7134 completed (loss: 0.037107810378074646, acc: 0.99210524559021)
[2025-02-13 03:10:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:05][root][INFO] - Training Epoch: 1/2, step 4901/7134 completed (loss: 0.02540474571287632, acc: 0.9939849376678467)
[2025-02-13 03:10:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:06][root][INFO] - Training Epoch: 1/2, step 4902/7134 completed (loss: 0.012782431207597256, acc: 0.9973474740982056)
[2025-02-13 03:10:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:06][root][INFO] - Training Epoch: 1/2, step 4903/7134 completed (loss: 0.0187675803899765, acc: 0.9946236610412598)
[2025-02-13 03:10:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:07][root][INFO] - Training Epoch: 1/2, step 4904/7134 completed (loss: 0.00977463461458683, acc: 0.9970149397850037)
[2025-02-13 03:10:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:07][root][INFO] - Training Epoch: 1/2, step 4905/7134 completed (loss: 0.005001845303922892, acc: 1.0)
[2025-02-13 03:10:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:07][root][INFO] - Training Epoch: 1/2, step 4906/7134 completed (loss: 0.024555310606956482, acc: 0.9915682673454285)
[2025-02-13 03:10:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:08][root][INFO] - Training Epoch: 1/2, step 4907/7134 completed (loss: 0.023931941017508507, acc: 0.9936708807945251)
[2025-02-13 03:10:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:08][root][INFO] - Training Epoch: 1/2, step 4908/7134 completed (loss: 0.011181685142219067, acc: 0.995199978351593)
[2025-02-13 03:10:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:09][root][INFO] - Training Epoch: 1/2, step 4909/7134 completed (loss: 0.017017904669046402, acc: 0.9967585206031799)
[2025-02-13 03:10:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:09][root][INFO] - Training Epoch: 1/2, step 4910/7134 completed (loss: 0.023737305775284767, acc: 0.9890282154083252)
[2025-02-13 03:10:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:10][root][INFO] - Training Epoch: 1/2, step 4911/7134 completed (loss: 0.020541558042168617, acc: 0.9920844435691833)
[2025-02-13 03:10:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:10][root][INFO] - Training Epoch: 1/2, step 4912/7134 completed (loss: 0.022217050194740295, acc: 0.9939467310905457)
[2025-02-13 03:10:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:11][root][INFO] - Training Epoch: 1/2, step 4913/7134 completed (loss: 0.023454144597053528, acc: 0.9949324131011963)
[2025-02-13 03:10:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:11][root][INFO] - Training Epoch: 1/2, step 4914/7134 completed (loss: 0.027064386755228043, acc: 0.9938837885856628)
[2025-02-13 03:10:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:11][root][INFO] - Training Epoch: 1/2, step 4915/7134 completed (loss: 0.004487238358706236, acc: 0.998410165309906)
[2025-02-13 03:10:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:12][root][INFO] - Training Epoch: 1/2, step 4916/7134 completed (loss: 0.06963393092155457, acc: 0.9805825352668762)
[2025-02-13 03:10:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:12][root][INFO] - Training Epoch: 1/2, step 4917/7134 completed (loss: 0.04044345021247864, acc: 0.9888734221458435)
[2025-02-13 03:10:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:13][root][INFO] - Training Epoch: 1/2, step 4918/7134 completed (loss: 0.020657747983932495, acc: 0.9972677826881409)
[2025-02-13 03:10:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:13][root][INFO] - Training Epoch: 1/2, step 4919/7134 completed (loss: 0.041435834020376205, acc: 0.9907894730567932)
[2025-02-13 03:10:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:13][root][INFO] - Training Epoch: 1/2, step 4920/7134 completed (loss: 0.06749822199344635, acc: 0.9832041263580322)
[2025-02-13 03:10:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:14][root][INFO] - Training Epoch: 1/2, step 4921/7134 completed (loss: 0.03775809705257416, acc: 0.991391658782959)
[2025-02-13 03:10:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:14][root][INFO] - Training Epoch: 1/2, step 4922/7134 completed (loss: 0.025705303996801376, acc: 0.994557797908783)
[2025-02-13 03:10:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:15][root][INFO] - Training Epoch: 1/2, step 4923/7134 completed (loss: 0.014963989146053791, acc: 0.9941262602806091)
[2025-02-13 03:10:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:15][root][INFO] - Training Epoch: 1/2, step 4924/7134 completed (loss: 0.026452751830220222, acc: 0.9900000095367432)
[2025-02-13 03:10:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:16][root][INFO] - Training Epoch: 1/2, step 4925/7134 completed (loss: 0.018560314550995827, acc: 0.9971387982368469)
[2025-02-13 03:10:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:16][root][INFO] - Training Epoch: 1/2, step 4926/7134 completed (loss: 0.016809586435556412, acc: 0.9956958293914795)
[2025-02-13 03:10:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:16][root][INFO] - Training Epoch: 1/2, step 4927/7134 completed (loss: 0.021647585555911064, acc: 0.994397759437561)
[2025-02-13 03:10:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:17][root][INFO] - Training Epoch: 1/2, step 4928/7134 completed (loss: 0.017554908990859985, acc: 0.9930475354194641)
[2025-02-13 03:10:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:17][root][INFO] - Training Epoch: 1/2, step 4929/7134 completed (loss: 0.021167142316699028, acc: 0.9937984347343445)
[2025-02-13 03:10:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:18][root][INFO] - Training Epoch: 1/2, step 4930/7134 completed (loss: 0.03292326256632805, acc: 0.993966817855835)
[2025-02-13 03:10:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:18][root][INFO] - Training Epoch: 1/2, step 4931/7134 completed (loss: 0.08570899814367294, acc: 0.9847792983055115)
[2025-02-13 03:10:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:18][root][INFO] - Training Epoch: 1/2, step 4932/7134 completed (loss: 0.02596810832619667, acc: 0.9932523369789124)
[2025-02-13 03:10:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:19][root][INFO] - Training Epoch: 1/2, step 4933/7134 completed (loss: 0.031449656933546066, acc: 0.993819534778595)
[2025-02-13 03:10:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:19][root][INFO] - Training Epoch: 1/2, step 4934/7134 completed (loss: 0.013602601364254951, acc: 0.9959404468536377)
[2025-02-13 03:10:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:20][root][INFO] - Training Epoch: 1/2, step 4935/7134 completed (loss: 0.02257619984447956, acc: 0.9917920827865601)
[2025-02-13 03:10:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:20][root][INFO] - Training Epoch: 1/2, step 4936/7134 completed (loss: 0.031192054972052574, acc: 0.9907407164573669)
[2025-02-13 03:10:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:21][root][INFO] - Training Epoch: 1/2, step 4937/7134 completed (loss: 0.027475664392113686, acc: 0.9910314083099365)
[2025-02-13 03:10:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:21][root][INFO] - Training Epoch: 1/2, step 4938/7134 completed (loss: 0.02782507613301277, acc: 0.9905533194541931)
[2025-02-13 03:10:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:21][root][INFO] - Training Epoch: 1/2, step 4939/7134 completed (loss: 0.03012625128030777, acc: 0.9869494438171387)
[2025-02-13 03:10:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:22][root][INFO] - Training Epoch: 1/2, step 4940/7134 completed (loss: 0.03362369164824486, acc: 0.9954545497894287)
[2025-02-13 03:10:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:22][root][INFO] - Training Epoch: 1/2, step 4941/7134 completed (loss: 0.03140614926815033, acc: 0.9905787110328674)
[2025-02-13 03:10:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:23][root][INFO] - Training Epoch: 1/2, step 4942/7134 completed (loss: 0.021041128784418106, acc: 0.9919571280479431)
[2025-02-13 03:10:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:23][root][INFO] - Training Epoch: 1/2, step 4943/7134 completed (loss: 0.05097312852740288, acc: 0.9863201379776001)
[2025-02-13 03:10:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:24][root][INFO] - Training Epoch: 1/2, step 4944/7134 completed (loss: 0.05131175369024277, acc: 0.9873684048652649)
[2025-02-13 03:10:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:24][root][INFO] - Training Epoch: 1/2, step 4945/7134 completed (loss: 0.0438852496445179, acc: 0.9838709831237793)
[2025-02-13 03:10:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:24][root][INFO] - Training Epoch: 1/2, step 4946/7134 completed (loss: 0.0663808137178421, acc: 0.9714285731315613)
[2025-02-13 03:10:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:25][root][INFO] - Training Epoch: 1/2, step 4947/7134 completed (loss: 0.036704253405332565, acc: 0.9889135360717773)
[2025-02-13 03:10:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:25][root][INFO] - Training Epoch: 1/2, step 4948/7134 completed (loss: 0.05091628432273865, acc: 0.9893048405647278)
[2025-02-13 03:10:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:26][root][INFO] - Training Epoch: 1/2, step 4949/7134 completed (loss: 0.031962793320417404, acc: 0.9968253970146179)
[2025-02-13 03:10:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:26][root][INFO] - Training Epoch: 1/2, step 4950/7134 completed (loss: 0.014648456126451492, acc: 1.0)
[2025-02-13 03:10:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:26][root][INFO] - Training Epoch: 1/2, step 4951/7134 completed (loss: 0.03333593159914017, acc: 0.9914529919624329)
[2025-02-13 03:10:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:27][root][INFO] - Training Epoch: 1/2, step 4952/7134 completed (loss: 0.02762478031218052, acc: 0.9933775067329407)
[2025-02-13 03:10:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:27][root][INFO] - Training Epoch: 1/2, step 4953/7134 completed (loss: 0.01568136177957058, acc: 0.9956427216529846)
[2025-02-13 03:10:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:28][root][INFO] - Training Epoch: 1/2, step 4954/7134 completed (loss: 0.01602759025990963, acc: 0.9942857027053833)
[2025-02-13 03:10:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:28][root][INFO] - Training Epoch: 1/2, step 4955/7134 completed (loss: 0.025112075731158257, acc: 0.9929742217063904)
[2025-02-13 03:10:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:28][root][INFO] - Training Epoch: 1/2, step 4956/7134 completed (loss: 0.01895625703036785, acc: 0.9943100810050964)
[2025-02-13 03:10:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:29][root][INFO] - Training Epoch: 1/2, step 4957/7134 completed (loss: 0.02594267576932907, acc: 0.9911816716194153)
[2025-02-13 03:10:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:29][root][INFO] - Training Epoch: 1/2, step 4958/7134 completed (loss: 0.013250800780951977, acc: 0.9956834316253662)
[2025-02-13 03:10:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:30][root][INFO] - Training Epoch: 1/2, step 4959/7134 completed (loss: 0.0163301732391119, acc: 0.9941176176071167)
[2025-02-13 03:10:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:30][root][INFO] - Training Epoch: 1/2, step 4960/7134 completed (loss: 0.013706156052649021, acc: 0.9948275685310364)
[2025-02-13 03:10:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:31][root][INFO] - Training Epoch: 1/2, step 4961/7134 completed (loss: 0.013691228814423084, acc: 0.9963570237159729)
[2025-02-13 03:10:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:31][root][INFO] - Training Epoch: 1/2, step 4962/7134 completed (loss: 0.005554272793233395, acc: 0.9984447956085205)
[2025-02-13 03:10:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:31][root][INFO] - Training Epoch: 1/2, step 4963/7134 completed (loss: 0.026444057002663612, acc: 0.9935064911842346)
[2025-02-13 03:10:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:32][root][INFO] - Training Epoch: 1/2, step 4964/7134 completed (loss: 0.024198101833462715, acc: 0.9934959411621094)
[2025-02-13 03:10:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:32][root][INFO] - Training Epoch: 1/2, step 4965/7134 completed (loss: 0.046620409935712814, acc: 0.9915966391563416)
[2025-02-13 03:10:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:33][root][INFO] - Training Epoch: 1/2, step 4966/7134 completed (loss: 0.06413788348436356, acc: 0.9806094169616699)
[2025-02-13 03:10:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:33][root][INFO] - Training Epoch: 1/2, step 4967/7134 completed (loss: 0.028763942420482635, acc: 0.990234375)
[2025-02-13 03:10:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:33][root][INFO] - Training Epoch: 1/2, step 4968/7134 completed (loss: 0.0833483561873436, acc: 0.9786477088928223)
[2025-02-13 03:10:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:34][root][INFO] - Training Epoch: 1/2, step 4969/7134 completed (loss: 0.06674600392580032, acc: 0.9845361113548279)
[2025-02-13 03:10:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:34][root][INFO] - Training Epoch: 1/2, step 4970/7134 completed (loss: 0.016595207154750824, acc: 0.9916387796401978)
[2025-02-13 03:10:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:35][root][INFO] - Training Epoch: 1/2, step 4971/7134 completed (loss: 0.02772172912955284, acc: 0.988135576248169)
[2025-02-13 03:10:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:35][root][INFO] - Training Epoch: 1/2, step 4972/7134 completed (loss: 0.015464718453586102, acc: 0.9971791505813599)
[2025-02-13 03:10:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:36][root][INFO] - Training Epoch: 1/2, step 4973/7134 completed (loss: 0.05030054971575737, acc: 0.9897119402885437)
[2025-02-13 03:10:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:36][root][INFO] - Training Epoch: 1/2, step 4974/7134 completed (loss: 0.05299046263098717, acc: 0.9859719276428223)
[2025-02-13 03:10:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:36][root][INFO] - Training Epoch: 1/2, step 4975/7134 completed (loss: 0.06697075068950653, acc: 0.9808306694030762)
[2025-02-13 03:10:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:37][root][INFO] - Training Epoch: 1/2, step 4976/7134 completed (loss: 0.029887881129980087, acc: 0.9929742217063904)
[2025-02-13 03:10:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:37][root][INFO] - Training Epoch: 1/2, step 4977/7134 completed (loss: 0.09090222418308258, acc: 0.9784946441650391)
[2025-02-13 03:10:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:37][root][INFO] - Training Epoch: 1/2, step 4978/7134 completed (loss: 0.026446467265486717, acc: 0.9863353967666626)
[2025-02-13 03:10:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:38][root][INFO] - Training Epoch: 1/2, step 4979/7134 completed (loss: 0.10048682987689972, acc: 0.9752883315086365)
[2025-02-13 03:10:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:38][root][INFO] - Training Epoch: 1/2, step 4980/7134 completed (loss: 0.06817980110645294, acc: 0.9822866320610046)
[2025-02-13 03:10:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:39][root][INFO] - Training Epoch: 1/2, step 4981/7134 completed (loss: 0.03232238069176674, acc: 0.9920254945755005)
[2025-02-13 03:10:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:39][root][INFO] - Training Epoch: 1/2, step 4982/7134 completed (loss: 0.09740039706230164, acc: 0.97773277759552)
[2025-02-13 03:10:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:40][root][INFO] - Training Epoch: 1/2, step 4983/7134 completed (loss: 0.03964722156524658, acc: 0.9937694668769836)
[2025-02-13 03:10:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:40][root][INFO] - Training Epoch: 1/2, step 4984/7134 completed (loss: 0.07804656773805618, acc: 0.987500011920929)
[2025-02-13 03:10:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:40][root][INFO] - Training Epoch: 1/2, step 4985/7134 completed (loss: 0.06698419153690338, acc: 0.9877675771713257)
[2025-02-13 03:10:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:41][root][INFO] - Training Epoch: 1/2, step 4986/7134 completed (loss: 0.06706348061561584, acc: 0.9881129264831543)
[2025-02-13 03:10:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:41][root][INFO] - Training Epoch: 1/2, step 4987/7134 completed (loss: 0.03637700527906418, acc: 0.9900596141815186)
[2025-02-13 03:10:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:42][root][INFO] - Training Epoch: 1/2, step 4988/7134 completed (loss: 0.03497912734746933, acc: 0.9876325130462646)
[2025-02-13 03:10:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:42][root][INFO] - Training Epoch: 1/2, step 4989/7134 completed (loss: 0.0401943102478981, acc: 0.9876237511634827)
[2025-02-13 03:10:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:42][root][INFO] - Training Epoch: 1/2, step 4990/7134 completed (loss: 0.04053459316492081, acc: 0.9898648858070374)
[2025-02-13 03:10:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:43][root][INFO] - Training Epoch: 1/2, step 4991/7134 completed (loss: 0.019474327564239502, acc: 0.9949748516082764)
[2025-02-13 03:10:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:43][root][INFO] - Training Epoch: 1/2, step 4992/7134 completed (loss: 0.1295699179172516, acc: 0.9715536236763)
[2025-02-13 03:10:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:44][root][INFO] - Training Epoch: 1/2, step 4993/7134 completed (loss: 0.04217888414859772, acc: 0.9864253401756287)
[2025-02-13 03:10:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:44][root][INFO] - Training Epoch: 1/2, step 4994/7134 completed (loss: 0.029334327206015587, acc: 0.9929078221321106)
[2025-02-13 03:10:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:44][root][INFO] - Training Epoch: 1/2, step 4995/7134 completed (loss: 0.053768470883369446, acc: 0.9836734533309937)
[2025-02-13 03:10:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:45][root][INFO] - Training Epoch: 1/2, step 4996/7134 completed (loss: 0.03586948662996292, acc: 0.9889298677444458)
[2025-02-13 03:10:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:45][root][INFO] - Training Epoch: 1/2, step 4997/7134 completed (loss: 0.04901031032204628, acc: 0.9863013625144958)
[2025-02-13 03:10:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:46][root][INFO] - Training Epoch: 1/2, step 4998/7134 completed (loss: 0.09039126336574554, acc: 0.9726775884628296)
[2025-02-13 03:10:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:46][root][INFO] - Training Epoch: 1/2, step 4999/7134 completed (loss: 0.016365541145205498, acc: 0.9965576529502869)
[2025-02-13 03:10:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:46][root][INFO] - Training Epoch: 1/2, step 5000/7134 completed (loss: 0.02341446839272976, acc: 0.9953488111495972)
[2025-02-13 03:10:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:47][root][INFO] - Training Epoch: 1/2, step 5001/7134 completed (loss: 0.03899834305047989, acc: 0.9941860437393188)
[2025-02-13 03:10:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:47][root][INFO] - Training Epoch: 1/2, step 5002/7134 completed (loss: 0.03506411239504814, acc: 0.9962825179100037)
[2025-02-13 03:10:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:48][root][INFO] - Training Epoch: 1/2, step 5003/7134 completed (loss: 0.01624158024787903, acc: 0.995199978351593)
[2025-02-13 03:10:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:48][root][INFO] - Training Epoch: 1/2, step 5004/7134 completed (loss: 0.022756129503250122, acc: 0.9933775067329407)
[2025-02-13 03:10:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:48][root][INFO] - Training Epoch: 1/2, step 5005/7134 completed (loss: 0.04209413006901741, acc: 0.992682933807373)
[2025-02-13 03:10:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:49][root][INFO] - Training Epoch: 1/2, step 5006/7134 completed (loss: 0.03267398104071617, acc: 0.9941262602806091)
[2025-02-13 03:10:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:49][root][INFO] - Training Epoch: 1/2, step 5007/7134 completed (loss: 0.025122378021478653, acc: 0.991525411605835)
[2025-02-13 03:10:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:50][root][INFO] - Training Epoch: 1/2, step 5008/7134 completed (loss: 0.06582560390233994, acc: 0.9822834730148315)
[2025-02-13 03:10:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:50][root][INFO] - Training Epoch: 1/2, step 5009/7134 completed (loss: 0.06580648571252823, acc: 0.9900744557380676)
[2025-02-13 03:10:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:50][root][INFO] - Training Epoch: 1/2, step 5010/7134 completed (loss: 0.03308999165892601, acc: 0.9878542423248291)
[2025-02-13 03:10:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:51][root][INFO] - Training Epoch: 1/2, step 5011/7134 completed (loss: 0.06242390349507332, acc: 0.9837398529052734)
[2025-02-13 03:10:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:51][root][INFO] - Training Epoch: 1/2, step 5012/7134 completed (loss: 0.10818155109882355, acc: 0.9712643623352051)
[2025-02-13 03:10:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:52][root][INFO] - Training Epoch: 1/2, step 5013/7134 completed (loss: 0.010797868482768536, acc: 0.9979959726333618)
[2025-02-13 03:10:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:52][root][INFO] - Training Epoch: 1/2, step 5014/7134 completed (loss: 0.0434623584151268, acc: 0.9918032884597778)
[2025-02-13 03:10:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:53][root][INFO] - Training Epoch: 1/2, step 5015/7134 completed (loss: 0.07950249314308167, acc: 0.9862068891525269)
[2025-02-13 03:10:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:53][root][INFO] - Training Epoch: 1/2, step 5016/7134 completed (loss: 0.018740905448794365, acc: 0.9927667379379272)
[2025-02-13 03:10:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:54][root][INFO] - Training Epoch: 1/2, step 5017/7134 completed (loss: 0.018452372401952744, acc: 0.9945945739746094)
[2025-02-13 03:10:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:54][root][INFO] - Training Epoch: 1/2, step 5018/7134 completed (loss: 0.03125103563070297, acc: 0.9858155846595764)
[2025-02-13 03:10:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:54][root][INFO] - Training Epoch: 1/2, step 5019/7134 completed (loss: 0.0236899945884943, acc: 0.994301974773407)
[2025-02-13 03:10:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:55][root][INFO] - Training Epoch: 1/2, step 5020/7134 completed (loss: 0.031711336225271225, acc: 0.9912499785423279)
[2025-02-13 03:10:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:55][root][INFO] - Training Epoch: 1/2, step 5021/7134 completed (loss: 0.03791402280330658, acc: 0.9908466935157776)
[2025-02-13 03:10:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:56][root][INFO] - Training Epoch: 1/2, step 5022/7134 completed (loss: 0.022908644750714302, acc: 0.9938271641731262)
[2025-02-13 03:10:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:56][root][INFO] - Training Epoch: 1/2, step 5023/7134 completed (loss: 0.05123140290379524, acc: 0.9817629456520081)
[2025-02-13 03:10:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:57][root][INFO] - Training Epoch: 1/2, step 5024/7134 completed (loss: 0.05076304450631142, acc: 0.9897040128707886)
[2025-02-13 03:10:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:57][root][INFO] - Training Epoch: 1/2, step 5025/7134 completed (loss: 0.019565800204873085, acc: 0.9963768124580383)
[2025-02-13 03:10:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:57][root][INFO] - Training Epoch: 1/2, step 5026/7134 completed (loss: 0.02629721909761429, acc: 0.9929947257041931)
[2025-02-13 03:10:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:58][root][INFO] - Training Epoch: 1/2, step 5027/7134 completed (loss: 0.029325738549232483, acc: 0.9927140474319458)
[2025-02-13 03:10:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:58][root][INFO] - Training Epoch: 1/2, step 5028/7134 completed (loss: 0.018046943470835686, acc: 0.9937304258346558)
[2025-02-13 03:10:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:59][root][INFO] - Training Epoch: 1/2, step 5029/7134 completed (loss: 0.02428531087934971, acc: 0.9924242496490479)
[2025-02-13 03:10:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:10:59][root][INFO] - Training Epoch: 1/2, step 5030/7134 completed (loss: 0.012843187898397446, acc: 0.9972714781761169)
[2025-02-13 03:10:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:00][root][INFO] - Training Epoch: 1/2, step 5031/7134 completed (loss: 0.0747610479593277, acc: 0.988095223903656)
[2025-02-13 03:11:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:00][root][INFO] - Training Epoch: 1/2, step 5032/7134 completed (loss: 0.021805353462696075, acc: 0.9967532753944397)
[2025-02-13 03:11:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:00][root][INFO] - Training Epoch: 1/2, step 5033/7134 completed (loss: 0.008198906667530537, acc: 0.9983974099159241)
[2025-02-13 03:11:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:01][root][INFO] - Training Epoch: 1/2, step 5034/7134 completed (loss: 0.011197283864021301, acc: 0.9964538812637329)
[2025-02-13 03:11:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:01][root][INFO] - Training Epoch: 1/2, step 5035/7134 completed (loss: 0.0025102635845541954, acc: 1.0)
[2025-02-13 03:11:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:02][root][INFO] - Training Epoch: 1/2, step 5036/7134 completed (loss: 0.0682956874370575, acc: 0.9866071343421936)
[2025-02-13 03:11:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:02][root][INFO] - Training Epoch: 1/2, step 5037/7134 completed (loss: 0.052849579602479935, acc: 0.9906166195869446)
[2025-02-13 03:11:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:02][root][INFO] - Training Epoch: 1/2, step 5038/7134 completed (loss: 0.031820185482501984, acc: 0.9910913109779358)
[2025-02-13 03:11:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:03][root][INFO] - Training Epoch: 1/2, step 5039/7134 completed (loss: 0.04220721870660782, acc: 0.9873417615890503)
[2025-02-13 03:11:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:03][root][INFO] - Training Epoch: 1/2, step 5040/7134 completed (loss: 0.018099257722496986, acc: 0.990439772605896)
[2025-02-13 03:11:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:04][root][INFO] - Training Epoch: 1/2, step 5041/7134 completed (loss: 0.009897314012050629, acc: 0.9960212111473083)
[2025-02-13 03:11:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:04][root][INFO] - Training Epoch: 1/2, step 5042/7134 completed (loss: 0.061975184828042984, acc: 0.9836829900741577)
[2025-02-13 03:11:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:04][root][INFO] - Training Epoch: 1/2, step 5043/7134 completed (loss: 0.038984671235084534, acc: 0.9861660003662109)
[2025-02-13 03:11:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:05][root][INFO] - Training Epoch: 1/2, step 5044/7134 completed (loss: 0.02671004645526409, acc: 0.9912280440330505)
[2025-02-13 03:11:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:05][root][INFO] - Training Epoch: 1/2, step 5045/7134 completed (loss: 0.05608275160193443, acc: 0.982503354549408)
[2025-02-13 03:11:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:06][root][INFO] - Training Epoch: 1/2, step 5046/7134 completed (loss: 0.06390982866287231, acc: 0.9859648942947388)
[2025-02-13 03:11:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:06][root][INFO] - Training Epoch: 1/2, step 5047/7134 completed (loss: 0.06930788606405258, acc: 0.9834710955619812)
[2025-02-13 03:11:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:07][root][INFO] - Training Epoch: 1/2, step 5048/7134 completed (loss: 0.04675466567277908, acc: 0.990867555141449)
[2025-02-13 03:11:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:07][root][INFO] - Training Epoch: 1/2, step 5049/7134 completed (loss: 0.05824239179491997, acc: 0.9850136041641235)
[2025-02-13 03:11:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:07][root][INFO] - Training Epoch: 1/2, step 5050/7134 completed (loss: 0.05401201546192169, acc: 0.9887482523918152)
[2025-02-13 03:11:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:08][root][INFO] - Training Epoch: 1/2, step 5051/7134 completed (loss: 0.057306598871946335, acc: 0.9855072498321533)
[2025-02-13 03:11:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:08][root][INFO] - Training Epoch: 1/2, step 5052/7134 completed (loss: 0.03179965913295746, acc: 0.9833333492279053)
[2025-02-13 03:11:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:09][root][INFO] - Training Epoch: 1/2, step 5053/7134 completed (loss: 0.08214407414197922, acc: 0.9792453050613403)
[2025-02-13 03:11:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:09][root][INFO] - Training Epoch: 1/2, step 5054/7134 completed (loss: 0.0803966224193573, acc: 0.9747633934020996)
[2025-02-13 03:11:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:09][root][INFO] - Training Epoch: 1/2, step 5055/7134 completed (loss: 0.03538423031568527, acc: 0.9857142567634583)
[2025-02-13 03:11:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:10][root][INFO] - Training Epoch: 1/2, step 5056/7134 completed (loss: 0.09549194574356079, acc: 0.9809523820877075)
[2025-02-13 03:11:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:10][root][INFO] - Training Epoch: 1/2, step 5057/7134 completed (loss: 0.08852425962686539, acc: 0.9772403836250305)
[2025-02-13 03:11:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:11][root][INFO] - Training Epoch: 1/2, step 5058/7134 completed (loss: 0.06222226843237877, acc: 0.9811594486236572)
[2025-02-13 03:11:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:11][root][INFO] - Training Epoch: 1/2, step 5059/7134 completed (loss: 0.033402327448129654, acc: 0.9908257126808167)
[2025-02-13 03:11:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:12][root][INFO] - Training Epoch: 1/2, step 5060/7134 completed (loss: 0.02317836508154869, acc: 0.9955423474311829)
[2025-02-13 03:11:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:12][root][INFO] - Training Epoch: 1/2, step 5061/7134 completed (loss: 0.049278441816568375, acc: 0.9848812222480774)
[2025-02-13 03:11:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:12][root][INFO] - Training Epoch: 1/2, step 5062/7134 completed (loss: 0.08558899164199829, acc: 0.9777015447616577)
[2025-02-13 03:11:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:13][root][INFO] - Training Epoch: 1/2, step 5063/7134 completed (loss: 0.0428524948656559, acc: 0.9916527271270752)
[2025-02-13 03:11:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:13][root][INFO] - Training Epoch: 1/2, step 5064/7134 completed (loss: 0.028529435396194458, acc: 0.9897260069847107)
[2025-02-13 03:11:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:14][root][INFO] - Training Epoch: 1/2, step 5065/7134 completed (loss: 0.046342406421899796, acc: 0.9887164831161499)
[2025-02-13 03:11:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:14][root][INFO] - Training Epoch: 1/2, step 5066/7134 completed (loss: 0.05825499817728996, acc: 0.9863842725753784)
[2025-02-13 03:11:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:15][root][INFO] - Training Epoch: 1/2, step 5067/7134 completed (loss: 0.02956126071512699, acc: 0.9941520690917969)
[2025-02-13 03:11:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:15][root][INFO] - Training Epoch: 1/2, step 5068/7134 completed (loss: 0.04229873791337013, acc: 0.9905213117599487)
[2025-02-13 03:11:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:15][root][INFO] - Training Epoch: 1/2, step 5069/7134 completed (loss: 0.03521624207496643, acc: 0.987500011920929)
[2025-02-13 03:11:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:16][root][INFO] - Training Epoch: 1/2, step 5070/7134 completed (loss: 0.026557138189673424, acc: 0.9942528605461121)
[2025-02-13 03:11:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:16][root][INFO] - Training Epoch: 1/2, step 5071/7134 completed (loss: 0.060717642307281494, acc: 0.9845722317695618)
[2025-02-13 03:11:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:17][root][INFO] - Training Epoch: 1/2, step 5072/7134 completed (loss: 0.05869091674685478, acc: 0.9836888313293457)
[2025-02-13 03:11:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:17][root][INFO] - Training Epoch: 1/2, step 5073/7134 completed (loss: 0.052048057317733765, acc: 0.9825918674468994)
[2025-02-13 03:11:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:18][root][INFO] - Training Epoch: 1/2, step 5074/7134 completed (loss: 0.059996794909238815, acc: 0.9800994992256165)
[2025-02-13 03:11:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:18][root][INFO] - Training Epoch: 1/2, step 5075/7134 completed (loss: 0.0476895309984684, acc: 0.9903100728988647)
[2025-02-13 03:11:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:19][root][INFO] - Training Epoch: 1/2, step 5076/7134 completed (loss: 0.04610298573970795, acc: 0.9868420958518982)
[2025-02-13 03:11:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:19][root][INFO] - Training Epoch: 1/2, step 5077/7134 completed (loss: 0.03425579145550728, acc: 0.9873417615890503)
[2025-02-13 03:11:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:19][root][INFO] - Training Epoch: 1/2, step 5078/7134 completed (loss: 0.08670591562986374, acc: 0.9769874215126038)
[2025-02-13 03:11:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:20][root][INFO] - Training Epoch: 1/2, step 5079/7134 completed (loss: 0.02605467103421688, acc: 0.9931972622871399)
[2025-02-13 03:11:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:20][root][INFO] - Training Epoch: 1/2, step 5080/7134 completed (loss: 0.024268772453069687, acc: 0.9940898418426514)
[2025-02-13 03:11:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:21][root][INFO] - Training Epoch: 1/2, step 5081/7134 completed (loss: 0.017642462626099586, acc: 0.9959239363670349)
[2025-02-13 03:11:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:21][root][INFO] - Training Epoch: 1/2, step 5082/7134 completed (loss: 0.03344720974564552, acc: 0.9914966225624084)
[2025-02-13 03:11:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:22][root][INFO] - Training Epoch: 1/2, step 5083/7134 completed (loss: 0.03941080719232559, acc: 0.9929676651954651)
[2025-02-13 03:11:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:22][root][INFO] - Training Epoch: 1/2, step 5084/7134 completed (loss: 0.017844052985310555, acc: 0.9930555820465088)
[2025-02-13 03:11:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:23][root][INFO] - Training Epoch: 1/2, step 5085/7134 completed (loss: 0.03845246881246567, acc: 0.9857988357543945)
[2025-02-13 03:11:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:23][root][INFO] - Training Epoch: 1/2, step 5086/7134 completed (loss: 0.04740508273243904, acc: 0.9885641932487488)
[2025-02-13 03:11:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:23][root][INFO] - Training Epoch: 1/2, step 5087/7134 completed (loss: 0.015123243443667889, acc: 0.9946452379226685)
[2025-02-13 03:11:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:24][root][INFO] - Training Epoch: 1/2, step 5088/7134 completed (loss: 0.010077102109789848, acc: 0.9970717430114746)
[2025-02-13 03:11:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:24][root][INFO] - Training Epoch: 1/2, step 5089/7134 completed (loss: 0.017156196758151054, acc: 0.996259331703186)
[2025-02-13 03:11:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:25][root][INFO] - Training Epoch: 1/2, step 5090/7134 completed (loss: 0.020009130239486694, acc: 0.9953325390815735)
[2025-02-13 03:11:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:25][root][INFO] - Training Epoch: 1/2, step 5091/7134 completed (loss: 0.011275487020611763, acc: 0.9986824989318848)
[2025-02-13 03:11:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:26][root][INFO] - Training Epoch: 1/2, step 5092/7134 completed (loss: 0.03736809268593788, acc: 0.9930843710899353)
[2025-02-13 03:11:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:26][root][INFO] - Training Epoch: 1/2, step 5093/7134 completed (loss: 0.03636050596833229, acc: 0.9875690340995789)
[2025-02-13 03:11:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:26][root][INFO] - Training Epoch: 1/2, step 5094/7134 completed (loss: 0.020276252180337906, acc: 0.9945454597473145)
[2025-02-13 03:11:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:27][root][INFO] - Training Epoch: 1/2, step 5095/7134 completed (loss: 0.1290755718946457, acc: 0.975039005279541)
[2025-02-13 03:11:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:27][root][INFO] - Training Epoch: 1/2, step 5096/7134 completed (loss: 0.03348112106323242, acc: 0.9861271381378174)
[2025-02-13 03:11:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:28][root][INFO] - Training Epoch: 1/2, step 5097/7134 completed (loss: 0.04696228727698326, acc: 0.9877899885177612)
[2025-02-13 03:11:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:28][root][INFO] - Training Epoch: 1/2, step 5098/7134 completed (loss: 0.038986288011074066, acc: 0.9853768348693848)
[2025-02-13 03:11:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:29][root][INFO] - Training Epoch: 1/2, step 5099/7134 completed (loss: 0.04201065003871918, acc: 0.9849884510040283)
[2025-02-13 03:11:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:29][root][INFO] - Training Epoch: 1/2, step 5100/7134 completed (loss: 0.035270240157842636, acc: 0.9900881052017212)
[2025-02-13 03:11:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:30][root][INFO] - Training Epoch: 1/2, step 5101/7134 completed (loss: 0.023588357493281364, acc: 0.9930070042610168)
[2025-02-13 03:11:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:30][root][INFO] - Training Epoch: 1/2, step 5102/7134 completed (loss: 0.05579964816570282, acc: 0.9849931597709656)
[2025-02-13 03:11:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:30][root][INFO] - Training Epoch: 1/2, step 5103/7134 completed (loss: 0.05446048825979233, acc: 0.9816724061965942)
[2025-02-13 03:11:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:31][root][INFO] - Training Epoch: 1/2, step 5104/7134 completed (loss: 0.024420088157057762, acc: 0.9900332093238831)
[2025-02-13 03:11:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:31][root][INFO] - Training Epoch: 1/2, step 5105/7134 completed (loss: 0.017466595396399498, acc: 0.995529055595398)
[2025-02-13 03:11:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:32][root][INFO] - Training Epoch: 1/2, step 5106/7134 completed (loss: 0.03560792654752731, acc: 0.9881481528282166)
[2025-02-13 03:11:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:32][root][INFO] - Training Epoch: 1/2, step 5107/7134 completed (loss: 0.09545706957578659, acc: 0.9825673699378967)
[2025-02-13 03:11:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:33][root][INFO] - Training Epoch: 1/2, step 5108/7134 completed (loss: 0.0764070600271225, acc: 0.9825783967971802)
[2025-02-13 03:11:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:33][root][INFO] - Training Epoch: 1/2, step 5109/7134 completed (loss: 0.06425061076879501, acc: 0.9834070801734924)
[2025-02-13 03:11:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:34][root][INFO] - Training Epoch: 1/2, step 5110/7134 completed (loss: 0.05223112180829048, acc: 0.9846153855323792)
[2025-02-13 03:11:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:34][root][INFO] - Training Epoch: 1/2, step 5111/7134 completed (loss: 0.028077028691768646, acc: 0.9904109835624695)
[2025-02-13 03:11:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:34][root][INFO] - Training Epoch: 1/2, step 5112/7134 completed (loss: 0.04770030826330185, acc: 0.9863013625144958)
[2025-02-13 03:11:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:35][root][INFO] - Training Epoch: 1/2, step 5113/7134 completed (loss: 0.04753521457314491, acc: 0.9880596995353699)
[2025-02-13 03:11:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:35][root][INFO] - Training Epoch: 1/2, step 5114/7134 completed (loss: 0.017529604956507683, acc: 0.9944367408752441)
[2025-02-13 03:11:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:36][root][INFO] - Training Epoch: 1/2, step 5115/7134 completed (loss: 0.021016797050833702, acc: 0.9935483932495117)
[2025-02-13 03:11:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:36][root][INFO] - Training Epoch: 1/2, step 5116/7134 completed (loss: 0.027813492342829704, acc: 0.9922077655792236)
[2025-02-13 03:11:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:37][root][INFO] - Training Epoch: 1/2, step 5117/7134 completed (loss: 0.02862660028040409, acc: 0.991631805896759)
[2025-02-13 03:11:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:37][root][INFO] - Training Epoch: 1/2, step 5118/7134 completed (loss: 0.062497034668922424, acc: 0.9893778562545776)
[2025-02-13 03:11:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:37][root][INFO] - Training Epoch: 1/2, step 5119/7134 completed (loss: 0.04507334157824516, acc: 0.9917469024658203)
[2025-02-13 03:11:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:38][root][INFO] - Training Epoch: 1/2, step 5120/7134 completed (loss: 0.07653611153364182, acc: 0.9842932224273682)
[2025-02-13 03:11:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:38][root][INFO] - Training Epoch: 1/2, step 5121/7134 completed (loss: 0.030983474105596542, acc: 0.9930651783943176)
[2025-02-13 03:11:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:39][root][INFO] - Training Epoch: 1/2, step 5122/7134 completed (loss: 0.0471864677965641, acc: 0.9859353303909302)
[2025-02-13 03:11:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:39][root][INFO] - Training Epoch: 1/2, step 5123/7134 completed (loss: 0.02046153135597706, acc: 0.9951748847961426)
[2025-02-13 03:11:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:40][root][INFO] - Training Epoch: 1/2, step 5124/7134 completed (loss: 0.051526766270399094, acc: 0.9863353967666626)
[2025-02-13 03:11:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:40][root][INFO] - Training Epoch: 1/2, step 5125/7134 completed (loss: 0.027754975482821465, acc: 0.9952606558799744)
[2025-02-13 03:11:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:40][root][INFO] - Training Epoch: 1/2, step 5126/7134 completed (loss: 0.015849648043513298, acc: 0.9931623935699463)
[2025-02-13 03:11:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:41][root][INFO] - Training Epoch: 1/2, step 5127/7134 completed (loss: 0.039766259491443634, acc: 0.9902098178863525)
[2025-02-13 03:11:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:41][root][INFO] - Training Epoch: 1/2, step 5128/7134 completed (loss: 0.021970802918076515, acc: 0.9934065937995911)
[2025-02-13 03:11:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:42][root][INFO] - Training Epoch: 1/2, step 5129/7134 completed (loss: 0.052326858043670654, acc: 0.9843137264251709)
[2025-02-13 03:11:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:42][root][INFO] - Training Epoch: 1/2, step 5130/7134 completed (loss: 0.022253578528761864, acc: 0.9937597513198853)
[2025-02-13 03:11:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:43][root][INFO] - Training Epoch: 1/2, step 5131/7134 completed (loss: 0.050280291587114334, acc: 0.9886363744735718)
[2025-02-13 03:11:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:43][root][INFO] - Training Epoch: 1/2, step 5132/7134 completed (loss: 0.01438047643750906, acc: 0.996039628982544)
[2025-02-13 03:11:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:43][root][INFO] - Training Epoch: 1/2, step 5133/7134 completed (loss: 0.04681441932916641, acc: 0.9875776171684265)
[2025-02-13 03:11:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:44][root][INFO] - Training Epoch: 1/2, step 5134/7134 completed (loss: 0.027247153222560883, acc: 0.9943714737892151)
[2025-02-13 03:11:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:44][root][INFO] - Training Epoch: 1/2, step 5135/7134 completed (loss: 0.049230094999074936, acc: 0.9885714054107666)
[2025-02-13 03:11:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:45][root][INFO] - Training Epoch: 1/2, step 5136/7134 completed (loss: 0.02608548291027546, acc: 0.994535505771637)
[2025-02-13 03:11:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:45][root][INFO] - Training Epoch: 1/2, step 5137/7134 completed (loss: 0.03063138574361801, acc: 0.9908257126808167)
[2025-02-13 03:11:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:45][root][INFO] - Training Epoch: 1/2, step 5138/7134 completed (loss: 0.021947961300611496, acc: 0.9912499785423279)
[2025-02-13 03:11:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:46][root][INFO] - Training Epoch: 1/2, step 5139/7134 completed (loss: 0.015670301392674446, acc: 0.995708167552948)
[2025-02-13 03:11:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:46][root][INFO] - Training Epoch: 1/2, step 5140/7134 completed (loss: 0.026308204978704453, acc: 0.9925768971443176)
[2025-02-13 03:11:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:47][root][INFO] - Training Epoch: 1/2, step 5141/7134 completed (loss: 0.06802517175674438, acc: 0.9806950092315674)
[2025-02-13 03:11:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:47][root][INFO] - Training Epoch: 1/2, step 5142/7134 completed (loss: 0.03457322716712952, acc: 0.9917647242546082)
[2025-02-13 03:11:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:48][root][INFO] - Training Epoch: 1/2, step 5143/7134 completed (loss: 0.03904842957854271, acc: 0.989847719669342)
[2025-02-13 03:11:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:48][root][INFO] - Training Epoch: 1/2, step 5144/7134 completed (loss: 0.019567696377635002, acc: 0.9964994192123413)
[2025-02-13 03:11:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:48][root][INFO] - Training Epoch: 1/2, step 5145/7134 completed (loss: 0.014981319196522236, acc: 0.9969293475151062)
[2025-02-13 03:11:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:49][root][INFO] - Training Epoch: 1/2, step 5146/7134 completed (loss: 0.05346054583787918, acc: 0.9843924045562744)
[2025-02-13 03:11:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:49][root][INFO] - Training Epoch: 1/2, step 5147/7134 completed (loss: 0.018973730504512787, acc: 0.994535505771637)
[2025-02-13 03:11:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:50][root][INFO] - Training Epoch: 1/2, step 5148/7134 completed (loss: 0.06781404465436935, acc: 0.9826897382736206)
[2025-02-13 03:11:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:50][root][INFO] - Training Epoch: 1/2, step 5149/7134 completed (loss: 0.05754302442073822, acc: 0.9840795993804932)
[2025-02-13 03:11:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:51][root][INFO] - Training Epoch: 1/2, step 5150/7134 completed (loss: 0.07402759045362473, acc: 0.979139506816864)
[2025-02-13 03:11:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:51][root][INFO] - Training Epoch: 1/2, step 5151/7134 completed (loss: 0.04649997502565384, acc: 0.9876237511634827)
[2025-02-13 03:11:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:52][root][INFO] - Training Epoch: 1/2, step 5152/7134 completed (loss: 0.038366060703992844, acc: 0.989276111125946)
[2025-02-13 03:11:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:52][root][INFO] - Training Epoch: 1/2, step 5153/7134 completed (loss: 0.023635361343622208, acc: 0.9931412935256958)
[2025-02-13 03:11:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:52][root][INFO] - Training Epoch: 1/2, step 5154/7134 completed (loss: 0.02863389626145363, acc: 0.9908987283706665)
[2025-02-13 03:11:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:53][root][INFO] - Training Epoch: 1/2, step 5155/7134 completed (loss: 0.06238999590277672, acc: 0.9894578456878662)
[2025-02-13 03:11:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:53][root][INFO] - Training Epoch: 1/2, step 5156/7134 completed (loss: 0.02768213115632534, acc: 0.9920555949211121)
[2025-02-13 03:11:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:54][root][INFO] - Training Epoch: 1/2, step 5157/7134 completed (loss: 0.06851493567228317, acc: 0.980440080165863)
[2025-02-13 03:11:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:54][root][INFO] - Training Epoch: 1/2, step 5158/7134 completed (loss: 0.09329673647880554, acc: 0.9732937812805176)
[2025-02-13 03:11:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:55][root][INFO] - Training Epoch: 1/2, step 5159/7134 completed (loss: 0.025115473195910454, acc: 0.9934980273246765)
[2025-02-13 03:11:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:55][root][INFO] - Training Epoch: 1/2, step 5160/7134 completed (loss: 0.07418464124202728, acc: 0.9827089309692383)
[2025-02-13 03:11:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:56][root][INFO] - Training Epoch: 1/2, step 5161/7134 completed (loss: 0.08504000306129456, acc: 0.9720279574394226)
[2025-02-13 03:11:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:56][root][INFO] - Training Epoch: 1/2, step 5162/7134 completed (loss: 0.027260497212409973, acc: 0.987034022808075)
[2025-02-13 03:11:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:56][root][INFO] - Training Epoch: 1/2, step 5163/7134 completed (loss: 0.025987811386585236, acc: 0.9915013909339905)
[2025-02-13 03:11:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:57][root][INFO] - Training Epoch: 1/2, step 5164/7134 completed (loss: 0.01936466060578823, acc: 0.9949579834938049)
[2025-02-13 03:11:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:57][root][INFO] - Training Epoch: 1/2, step 5165/7134 completed (loss: 0.047221891582012177, acc: 0.9849397540092468)
[2025-02-13 03:11:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:58][root][INFO] - Training Epoch: 1/2, step 5166/7134 completed (loss: 0.019818849861621857, acc: 0.9954338073730469)
[2025-02-13 03:11:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:58][root][INFO] - Training Epoch: 1/2, step 5167/7134 completed (loss: 0.054450519382953644, acc: 0.9823718070983887)
[2025-02-13 03:11:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:59][root][INFO] - Training Epoch: 1/2, step 5168/7134 completed (loss: 0.05260846018791199, acc: 0.9767080545425415)
[2025-02-13 03:11:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:59][root][INFO] - Training Epoch: 1/2, step 5169/7134 completed (loss: 0.0670510083436966, acc: 0.9813084006309509)
[2025-02-13 03:11:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:11:59][root][INFO] - Training Epoch: 1/2, step 5170/7134 completed (loss: 0.048196520656347275, acc: 0.9867549538612366)
[2025-02-13 03:12:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:00][root][INFO] - Training Epoch: 1/2, step 5171/7134 completed (loss: 0.027927102521061897, acc: 0.9893162250518799)
[2025-02-13 03:12:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:00][root][INFO] - Training Epoch: 1/2, step 5172/7134 completed (loss: 0.08216608315706253, acc: 0.9761499166488647)
[2025-02-13 03:12:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:01][root][INFO] - Training Epoch: 1/2, step 5173/7134 completed (loss: 0.038659028708934784, acc: 0.98740154504776)
[2025-02-13 03:12:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:01][root][INFO] - Training Epoch: 1/2, step 5174/7134 completed (loss: 0.012263653799891472, acc: 0.9974457025527954)
[2025-02-13 03:12:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:01][root][INFO] - Training Epoch: 1/2, step 5175/7134 completed (loss: 0.018231969326734543, acc: 0.9930939078330994)
[2025-02-13 03:12:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:02][root][INFO] - Training Epoch: 1/2, step 5176/7134 completed (loss: 0.02252846211194992, acc: 0.9924127459526062)
[2025-02-13 03:12:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:02][root][INFO] - Training Epoch: 1/2, step 5177/7134 completed (loss: 0.061890244483947754, acc: 0.9843478202819824)
[2025-02-13 03:12:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:03][root][INFO] - Training Epoch: 1/2, step 5178/7134 completed (loss: 0.13078898191452026, acc: 0.9756097793579102)
[2025-02-13 03:12:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:03][root][INFO] - Training Epoch: 1/2, step 5179/7134 completed (loss: 0.021311204880475998, acc: 0.9955621361732483)
[2025-02-13 03:12:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:04][root][INFO] - Training Epoch: 1/2, step 5180/7134 completed (loss: 0.04495543986558914, acc: 0.9831144213676453)
[2025-02-13 03:12:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:04][root][INFO] - Training Epoch: 1/2, step 5181/7134 completed (loss: 0.05250592902302742, acc: 0.9893617033958435)
[2025-02-13 03:12:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:04][root][INFO] - Training Epoch: 1/2, step 5182/7134 completed (loss: 0.15523763000965118, acc: 0.9645833373069763)
[2025-02-13 03:12:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:05][root][INFO] - Training Epoch: 1/2, step 5183/7134 completed (loss: 0.06292206794023514, acc: 0.9884488582611084)
[2025-02-13 03:12:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:05][root][INFO] - Training Epoch: 1/2, step 5184/7134 completed (loss: 0.0522661991417408, acc: 0.9828850626945496)
[2025-02-13 03:12:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:06][root][INFO] - Training Epoch: 1/2, step 5185/7134 completed (loss: 0.063713937997818, acc: 0.9879518151283264)
[2025-02-13 03:12:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:06][root][INFO] - Training Epoch: 1/2, step 5186/7134 completed (loss: 0.044802144169807434, acc: 0.9837067127227783)
[2025-02-13 03:12:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:06][root][INFO] - Training Epoch: 1/2, step 5187/7134 completed (loss: 0.02016555704176426, acc: 0.9942857027053833)
[2025-02-13 03:12:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:07][root][INFO] - Training Epoch: 1/2, step 5188/7134 completed (loss: 0.17354628443717957, acc: 0.9599999785423279)
[2025-02-13 03:12:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:07][root][INFO] - Training Epoch: 1/2, step 5189/7134 completed (loss: 0.13680605590343475, acc: 0.9693877696990967)
[2025-02-13 03:12:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:08][root][INFO] - Training Epoch: 1/2, step 5190/7134 completed (loss: 0.11714978516101837, acc: 0.9691211581230164)
[2025-02-13 03:12:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:08][root][INFO] - Training Epoch: 1/2, step 5191/7134 completed (loss: 0.11700054258108139, acc: 0.966325044631958)
[2025-02-13 03:12:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:08][root][INFO] - Training Epoch: 1/2, step 5192/7134 completed (loss: 0.06061943992972374, acc: 0.9892984628677368)
[2025-02-13 03:12:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:09][root][INFO] - Training Epoch: 1/2, step 5193/7134 completed (loss: 0.04698576405644417, acc: 0.9862204790115356)
[2025-02-13 03:12:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:09][root][INFO] - Training Epoch: 1/2, step 5194/7134 completed (loss: 0.09504710137844086, acc: 0.9722772240638733)
[2025-02-13 03:12:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:10][root][INFO] - Training Epoch: 1/2, step 5195/7134 completed (loss: 0.06431916356086731, acc: 0.9868420958518982)
[2025-02-13 03:12:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:10][root][INFO] - Training Epoch: 1/2, step 5196/7134 completed (loss: 0.05570608749985695, acc: 0.9856459498405457)
[2025-02-13 03:12:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:10][root][INFO] - Training Epoch: 1/2, step 5197/7134 completed (loss: 0.04238828644156456, acc: 0.9876543283462524)
[2025-02-13 03:12:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:11][root][INFO] - Training Epoch: 1/2, step 5198/7134 completed (loss: 0.050913840532302856, acc: 0.987525999546051)
[2025-02-13 03:12:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:11][root][INFO] - Training Epoch: 1/2, step 5199/7134 completed (loss: 0.12103888392448425, acc: 0.9651162624359131)
[2025-02-13 03:12:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:12][root][INFO] - Training Epoch: 1/2, step 5200/7134 completed (loss: 0.050066735595464706, acc: 0.984282910823822)
[2025-02-13 03:12:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:12][root][INFO] - Training Epoch: 1/2, step 5201/7134 completed (loss: 0.07816854864358902, acc: 0.9782871007919312)
[2025-02-13 03:12:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:12][root][INFO] - Training Epoch: 1/2, step 5202/7134 completed (loss: 0.06844092160463333, acc: 0.9793956279754639)
[2025-02-13 03:12:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:13][root][INFO] - Training Epoch: 1/2, step 5203/7134 completed (loss: 0.07253380864858627, acc: 0.9809523820877075)
[2025-02-13 03:12:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:13][root][INFO] - Training Epoch: 1/2, step 5204/7134 completed (loss: 0.03206615522503853, acc: 0.9896907210350037)
[2025-02-13 03:12:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:14][root][INFO] - Training Epoch: 1/2, step 5205/7134 completed (loss: 0.06907340884208679, acc: 0.9737827777862549)
[2025-02-13 03:12:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:14][root][INFO] - Training Epoch: 1/2, step 5206/7134 completed (loss: 0.05665181204676628, acc: 0.9855538010597229)
[2025-02-13 03:12:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:15][root][INFO] - Training Epoch: 1/2, step 5207/7134 completed (loss: 0.09777677059173584, acc: 0.9637826681137085)
[2025-02-13 03:12:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:15][root][INFO] - Training Epoch: 1/2, step 5208/7134 completed (loss: 0.037520259618759155, acc: 0.9920844435691833)
[2025-02-13 03:12:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:15][root][INFO] - Training Epoch: 1/2, step 5209/7134 completed (loss: 0.04441403970122337, acc: 0.9859437942504883)
[2025-02-13 03:12:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:16][root][INFO] - Training Epoch: 1/2, step 5210/7134 completed (loss: 0.03875366225838661, acc: 0.9919893145561218)
[2025-02-13 03:12:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:16][root][INFO] - Training Epoch: 1/2, step 5211/7134 completed (loss: 0.03864186629652977, acc: 0.9861111044883728)
[2025-02-13 03:12:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:17][root][INFO] - Training Epoch: 1/2, step 5212/7134 completed (loss: 0.07847259193658829, acc: 0.9692832827568054)
[2025-02-13 03:12:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:17][root][INFO] - Training Epoch: 1/2, step 5213/7134 completed (loss: 0.05269331485033035, acc: 0.9892086386680603)
[2025-02-13 03:12:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:18][root][INFO] - Training Epoch: 1/2, step 5214/7134 completed (loss: 0.13447058200836182, acc: 0.9570815563201904)
[2025-02-13 03:12:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:18][root][INFO] - Training Epoch: 1/2, step 5215/7134 completed (loss: 0.08297549188137054, acc: 0.9744572043418884)
[2025-02-13 03:12:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:18][root][INFO] - Training Epoch: 1/2, step 5216/7134 completed (loss: 0.06425326317548752, acc: 0.9771863222122192)
[2025-02-13 03:12:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:19][root][INFO] - Training Epoch: 1/2, step 5217/7134 completed (loss: 0.07362838834524155, acc: 0.9756097793579102)
[2025-02-13 03:12:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:19][root][INFO] - Training Epoch: 1/2, step 5218/7134 completed (loss: 0.06920258700847626, acc: 0.9842696785926819)
[2025-02-13 03:12:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:20][root][INFO] - Training Epoch: 1/2, step 5219/7134 completed (loss: 0.057871848344802856, acc: 0.980629563331604)
[2025-02-13 03:12:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:20][root][INFO] - Training Epoch: 1/2, step 5220/7134 completed (loss: 0.07208804786205292, acc: 0.9821109175682068)
[2025-02-13 03:12:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:20][root][INFO] - Training Epoch: 1/2, step 5221/7134 completed (loss: 0.05906488373875618, acc: 0.9875389337539673)
[2025-02-13 03:12:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:21][root][INFO] - Training Epoch: 1/2, step 5222/7134 completed (loss: 0.04057559743523598, acc: 0.9813084006309509)
[2025-02-13 03:12:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:21][root][INFO] - Training Epoch: 1/2, step 5223/7134 completed (loss: 0.032950509339571, acc: 0.9884792566299438)
[2025-02-13 03:12:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:22][root][INFO] - Training Epoch: 1/2, step 5224/7134 completed (loss: 0.06269581615924835, acc: 0.982425332069397)
[2025-02-13 03:12:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:22][root][INFO] - Training Epoch: 1/2, step 5225/7134 completed (loss: 0.07876238971948624, acc: 0.9806138873100281)
[2025-02-13 03:12:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:23][root][INFO] - Training Epoch: 1/2, step 5226/7134 completed (loss: 0.056220587342977524, acc: 0.9868203997612)
[2025-02-13 03:12:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:23][root][INFO] - Training Epoch: 1/2, step 5227/7134 completed (loss: 0.05983871966600418, acc: 0.9857142567634583)
[2025-02-13 03:12:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:23][root][INFO] - Training Epoch: 1/2, step 5228/7134 completed (loss: 0.04732264578342438, acc: 0.9883720874786377)
[2025-02-13 03:12:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:24][root][INFO] - Training Epoch: 1/2, step 5229/7134 completed (loss: 0.03285250812768936, acc: 0.98959881067276)
[2025-02-13 03:12:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:24][root][INFO] - Training Epoch: 1/2, step 5230/7134 completed (loss: 0.029604660347104073, acc: 0.9892183542251587)
[2025-02-13 03:12:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:25][root][INFO] - Training Epoch: 1/2, step 5231/7134 completed (loss: 0.029367873445153236, acc: 0.9936708807945251)
[2025-02-13 03:12:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:25][root][INFO] - Training Epoch: 1/2, step 5232/7134 completed (loss: 0.027903065085411072, acc: 0.9910314083099365)
[2025-02-13 03:12:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:26][root][INFO] - Training Epoch: 1/2, step 5233/7134 completed (loss: 0.02795175462961197, acc: 0.9932432174682617)
[2025-02-13 03:12:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:26][root][INFO] - Training Epoch: 1/2, step 5234/7134 completed (loss: 0.044734008610248566, acc: 0.9889841079711914)
[2025-02-13 03:12:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:27][root][INFO] - Training Epoch: 1/2, step 5235/7134 completed (loss: 0.022011984139680862, acc: 0.9949579834938049)
[2025-02-13 03:12:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:27][root][INFO] - Training Epoch: 1/2, step 5236/7134 completed (loss: 0.01512871216982603, acc: 0.9947916865348816)
[2025-02-13 03:12:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:27][root][INFO] - Training Epoch: 1/2, step 5237/7134 completed (loss: 0.009656081907451153, acc: 0.9982078671455383)
[2025-02-13 03:12:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:28][root][INFO] - Training Epoch: 1/2, step 5238/7134 completed (loss: 0.01664412021636963, acc: 0.9947183132171631)
[2025-02-13 03:12:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:28][root][INFO] - Training Epoch: 1/2, step 5239/7134 completed (loss: 0.021765336394309998, acc: 0.9961389899253845)
[2025-02-13 03:12:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:29][root][INFO] - Training Epoch: 1/2, step 5240/7134 completed (loss: 0.019361183047294617, acc: 0.9944674968719482)
[2025-02-13 03:12:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:29][root][INFO] - Training Epoch: 1/2, step 5241/7134 completed (loss: 0.03562786802649498, acc: 0.9952977895736694)
[2025-02-13 03:12:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:30][root][INFO] - Training Epoch: 1/2, step 5242/7134 completed (loss: 0.00878124125301838, acc: 0.9966942071914673)
[2025-02-13 03:12:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:30][root][INFO] - Training Epoch: 1/2, step 5243/7134 completed (loss: 0.024093180894851685, acc: 0.9965517520904541)
[2025-02-13 03:12:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:30][root][INFO] - Training Epoch: 1/2, step 5244/7134 completed (loss: 0.029254019260406494, acc: 0.9889655113220215)
[2025-02-13 03:12:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:31][root][INFO] - Training Epoch: 1/2, step 5245/7134 completed (loss: 0.025024358183145523, acc: 0.9965096116065979)
[2025-02-13 03:12:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:31][root][INFO] - Training Epoch: 1/2, step 5246/7134 completed (loss: 0.029193149879574776, acc: 0.9907975196838379)
[2025-02-13 03:12:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:32][root][INFO] - Training Epoch: 1/2, step 5247/7134 completed (loss: 0.02291412092745304, acc: 0.9907407164573669)
[2025-02-13 03:12:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:32][root][INFO] - Training Epoch: 1/2, step 5248/7134 completed (loss: 0.05777134373784065, acc: 0.9887640476226807)
[2025-02-13 03:12:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:33][root][INFO] - Training Epoch: 1/2, step 5249/7134 completed (loss: 0.021299757063388824, acc: 0.9952830076217651)
[2025-02-13 03:12:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:33][root][INFO] - Training Epoch: 1/2, step 5250/7134 completed (loss: 0.0226606335490942, acc: 0.9924337863922119)
[2025-02-13 03:12:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:33][root][INFO] - Training Epoch: 1/2, step 5251/7134 completed (loss: 0.037924058735370636, acc: 0.9890909194946289)
[2025-02-13 03:12:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:34][root][INFO] - Training Epoch: 1/2, step 5252/7134 completed (loss: 0.05747004225850105, acc: 0.9836448431015015)
[2025-02-13 03:12:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:34][root][INFO] - Training Epoch: 1/2, step 5253/7134 completed (loss: 0.02325216680765152, acc: 0.9950980544090271)
[2025-02-13 03:12:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:35][root][INFO] - Training Epoch: 1/2, step 5254/7134 completed (loss: 0.04278502240777016, acc: 0.9847198724746704)
[2025-02-13 03:12:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:35][root][INFO] - Training Epoch: 1/2, step 5255/7134 completed (loss: 0.04076462239027023, acc: 0.9917808175086975)
[2025-02-13 03:12:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:36][root][INFO] - Training Epoch: 1/2, step 5256/7134 completed (loss: 0.05842117965221405, acc: 0.9829984307289124)
[2025-02-13 03:12:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:36][root][INFO] - Training Epoch: 1/2, step 5257/7134 completed (loss: 0.06764459609985352, acc: 0.9863013625144958)
[2025-02-13 03:12:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:36][root][INFO] - Training Epoch: 1/2, step 5258/7134 completed (loss: 0.037623729556798935, acc: 0.9872449040412903)
[2025-02-13 03:12:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:37][root][INFO] - Training Epoch: 1/2, step 5259/7134 completed (loss: 0.06539984047412872, acc: 0.9873417615890503)
[2025-02-13 03:12:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:37][root][INFO] - Training Epoch: 1/2, step 5260/7134 completed (loss: 0.02643810398876667, acc: 0.9914634227752686)
[2025-02-13 03:12:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:38][root][INFO] - Training Epoch: 1/2, step 5261/7134 completed (loss: 0.05153275653719902, acc: 0.9807692170143127)
[2025-02-13 03:12:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:38][root][INFO] - Training Epoch: 1/2, step 5262/7134 completed (loss: 0.025590544566512108, acc: 0.9917241334915161)
[2025-02-13 03:12:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:39][root][INFO] - Training Epoch: 1/2, step 5263/7134 completed (loss: 0.05387449637055397, acc: 0.9894737005233765)
[2025-02-13 03:12:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:39][root][INFO] - Training Epoch: 1/2, step 5264/7134 completed (loss: 0.032169174402952194, acc: 0.9902794361114502)
[2025-02-13 03:12:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:40][root][INFO] - Training Epoch: 1/2, step 5265/7134 completed (loss: 0.04905501380562782, acc: 0.9860228896141052)
[2025-02-13 03:12:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:40][root][INFO] - Training Epoch: 1/2, step 5266/7134 completed (loss: 0.021369818598031998, acc: 0.9890410900115967)
[2025-02-13 03:12:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:40][root][INFO] - Training Epoch: 1/2, step 5267/7134 completed (loss: 0.09292930364608765, acc: 0.9735614061355591)
[2025-02-13 03:12:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:41][root][INFO] - Training Epoch: 1/2, step 5268/7134 completed (loss: 0.0553978867828846, acc: 0.984644889831543)
[2025-02-13 03:12:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:41][root][INFO] - Training Epoch: 1/2, step 5269/7134 completed (loss: 0.09018895775079727, acc: 0.9731663465499878)
[2025-02-13 03:12:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:42][root][INFO] - Training Epoch: 1/2, step 5270/7134 completed (loss: 0.035636551678180695, acc: 0.9879679083824158)
[2025-02-13 03:12:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:42][root][INFO] - Training Epoch: 1/2, step 5271/7134 completed (loss: 0.08681443333625793, acc: 0.971781313419342)
[2025-02-13 03:12:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:43][root][INFO] - Training Epoch: 1/2, step 5272/7134 completed (loss: 0.049235716462135315, acc: 0.9838709831237793)
[2025-02-13 03:12:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:43][root][INFO] - Training Epoch: 1/2, step 5273/7134 completed (loss: 0.042689379304647446, acc: 0.9881154298782349)
[2025-02-13 03:12:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:43][root][INFO] - Training Epoch: 1/2, step 5274/7134 completed (loss: 0.03238394483923912, acc: 0.9925373196601868)
[2025-02-13 03:12:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:44][root][INFO] - Training Epoch: 1/2, step 5275/7134 completed (loss: 0.02612055279314518, acc: 0.9890310764312744)
[2025-02-13 03:12:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:44][root][INFO] - Training Epoch: 1/2, step 5276/7134 completed (loss: 0.05113784223794937, acc: 0.9820627570152283)
[2025-02-13 03:12:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:45][root][INFO] - Training Epoch: 1/2, step 5277/7134 completed (loss: 0.03470693528652191, acc: 0.9884726405143738)
[2025-02-13 03:12:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:45][root][INFO] - Training Epoch: 1/2, step 5278/7134 completed (loss: 0.06478313356637955, acc: 0.9770641922950745)
[2025-02-13 03:12:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:46][root][INFO] - Training Epoch: 1/2, step 5279/7134 completed (loss: 0.018478544428944588, acc: 0.9961089491844177)
[2025-02-13 03:12:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:46][root][INFO] - Training Epoch: 1/2, step 5280/7134 completed (loss: 0.06397517025470734, acc: 0.9900497794151306)
[2025-02-13 03:12:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:46][root][INFO] - Training Epoch: 1/2, step 5281/7134 completed (loss: 0.03076944686472416, acc: 0.9866888523101807)
[2025-02-13 03:12:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:47][root][INFO] - Training Epoch: 1/2, step 5282/7134 completed (loss: 0.03156716004014015, acc: 0.9885877370834351)
[2025-02-13 03:12:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:47][root][INFO] - Training Epoch: 1/2, step 5283/7134 completed (loss: 0.029495559632778168, acc: 0.9931600689888)
[2025-02-13 03:12:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:48][root][INFO] - Training Epoch: 1/2, step 5284/7134 completed (loss: 0.04223211109638214, acc: 0.9889349937438965)
[2025-02-13 03:12:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:48][root][INFO] - Training Epoch: 1/2, step 5285/7134 completed (loss: 0.036627594381570816, acc: 0.9893491268157959)
[2025-02-13 03:12:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:49][root][INFO] - Training Epoch: 1/2, step 5286/7134 completed (loss: 0.06158097833395004, acc: 0.9926470518112183)
[2025-02-13 03:12:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:49][root][INFO] - Training Epoch: 1/2, step 5287/7134 completed (loss: 0.05952253192663193, acc: 0.9783197641372681)
[2025-02-13 03:12:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:49][root][INFO] - Training Epoch: 1/2, step 5288/7134 completed (loss: 0.024701645597815514, acc: 0.9923312664031982)
[2025-02-13 03:12:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:50][root][INFO] - Training Epoch: 1/2, step 5289/7134 completed (loss: 0.04449639096856117, acc: 0.9815863966941833)
[2025-02-13 03:12:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:50][root][INFO] - Training Epoch: 1/2, step 5290/7134 completed (loss: 0.037302836775779724, acc: 0.9873417615890503)
[2025-02-13 03:12:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:51][root][INFO] - Training Epoch: 1/2, step 5291/7134 completed (loss: 0.03864927962422371, acc: 0.9927272796630859)
[2025-02-13 03:12:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:51][root][INFO] - Training Epoch: 1/2, step 5292/7134 completed (loss: 0.033401135355234146, acc: 0.9923664331436157)
[2025-02-13 03:12:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:52][root][INFO] - Training Epoch: 1/2, step 5293/7134 completed (loss: 0.030052945017814636, acc: 0.9897810220718384)
[2025-02-13 03:12:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:52][root][INFO] - Training Epoch: 1/2, step 5294/7134 completed (loss: 0.04990163445472717, acc: 0.9888424277305603)
[2025-02-13 03:12:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:52][root][INFO] - Training Epoch: 1/2, step 5295/7134 completed (loss: 0.025991948321461678, acc: 0.9908088445663452)
[2025-02-13 03:12:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:53][root][INFO] - Training Epoch: 1/2, step 5296/7134 completed (loss: 0.047675345093011856, acc: 0.9888734221458435)
[2025-02-13 03:12:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:53][root][INFO] - Training Epoch: 1/2, step 5297/7134 completed (loss: 0.04518619924783707, acc: 0.9870689511299133)
[2025-02-13 03:12:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:54][root][INFO] - Training Epoch: 1/2, step 5298/7134 completed (loss: 0.025688398629426956, acc: 0.9914529919624329)
[2025-02-13 03:12:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:54][root][INFO] - Training Epoch: 1/2, step 5299/7134 completed (loss: 0.02798370085656643, acc: 0.9887640476226807)
[2025-02-13 03:12:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:55][root][INFO] - Training Epoch: 1/2, step 5300/7134 completed (loss: 0.03102056309580803, acc: 0.9897210001945496)
[2025-02-13 03:12:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:55][root][INFO] - Training Epoch: 1/2, step 5301/7134 completed (loss: 0.02266821824014187, acc: 0.9941262602806091)
[2025-02-13 03:12:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:55][root][INFO] - Training Epoch: 1/2, step 5302/7134 completed (loss: 0.07939104735851288, acc: 0.978723406791687)
[2025-02-13 03:12:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:56][root][INFO] - Training Epoch: 1/2, step 5303/7134 completed (loss: 0.05858834087848663, acc: 0.9858406782150269)
[2025-02-13 03:12:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:56][root][INFO] - Training Epoch: 1/2, step 5304/7134 completed (loss: 0.0721149817109108, acc: 0.9850249290466309)
[2025-02-13 03:12:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:57][root][INFO] - Training Epoch: 1/2, step 5305/7134 completed (loss: 0.06035958230495453, acc: 0.985401451587677)
[2025-02-13 03:12:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:57][root][INFO] - Training Epoch: 1/2, step 5306/7134 completed (loss: 0.016208520159125328, acc: 0.9972222447395325)
[2025-02-13 03:12:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:58][root][INFO] - Training Epoch: 1/2, step 5307/7134 completed (loss: 0.05885167047381401, acc: 0.9876543283462524)
[2025-02-13 03:12:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:58][root][INFO] - Training Epoch: 1/2, step 5308/7134 completed (loss: 0.05078829824924469, acc: 0.9848942756652832)
[2025-02-13 03:12:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:58][root][INFO] - Training Epoch: 1/2, step 5309/7134 completed (loss: 0.09827986359596252, acc: 0.9731663465499878)
[2025-02-13 03:12:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:59][root][INFO] - Training Epoch: 1/2, step 5310/7134 completed (loss: 0.05214374139904976, acc: 0.9860529899597168)
[2025-02-13 03:12:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:12:59][root][INFO] - Training Epoch: 1/2, step 5311/7134 completed (loss: 0.0532495491206646, acc: 0.9903692007064819)
[2025-02-13 03:12:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:00][root][INFO] - Training Epoch: 1/2, step 5312/7134 completed (loss: 0.02496154047548771, acc: 0.9906191229820251)
[2025-02-13 03:13:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:00][root][INFO] - Training Epoch: 1/2, step 5313/7134 completed (loss: 0.03838779404759407, acc: 0.9872495532035828)
[2025-02-13 03:13:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:01][root][INFO] - Training Epoch: 1/2, step 5314/7134 completed (loss: 0.036747802048921585, acc: 0.9912891983985901)
[2025-02-13 03:13:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:01][root][INFO] - Training Epoch: 1/2, step 5315/7134 completed (loss: 0.032354265451431274, acc: 0.9918166995048523)
[2025-02-13 03:13:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:01][root][INFO] - Training Epoch: 1/2, step 5316/7134 completed (loss: 0.04634212702512741, acc: 0.9899159669876099)
[2025-02-13 03:13:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:02][root][INFO] - Training Epoch: 1/2, step 5317/7134 completed (loss: 0.02508077584207058, acc: 0.9946236610412598)
[2025-02-13 03:13:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:02][root][INFO] - Training Epoch: 1/2, step 5318/7134 completed (loss: 0.02112286165356636, acc: 0.9930915236473083)
[2025-02-13 03:13:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:03][root][INFO] - Training Epoch: 1/2, step 5319/7134 completed (loss: 0.05323464795947075, acc: 0.9908758997917175)
[2025-02-13 03:13:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:03][root][INFO] - Training Epoch: 1/2, step 5320/7134 completed (loss: 0.032425425946712494, acc: 0.9937888383865356)
[2025-02-13 03:13:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:03][root][INFO] - Training Epoch: 1/2, step 5321/7134 completed (loss: 0.019971242174506187, acc: 0.9922480583190918)
[2025-02-13 03:13:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:04][root][INFO] - Training Epoch: 1/2, step 5322/7134 completed (loss: 0.021896280348300934, acc: 0.993966817855835)
[2025-02-13 03:13:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:04][root][INFO] - Training Epoch: 1/2, step 5323/7134 completed (loss: 0.036194056272506714, acc: 0.9878787994384766)
[2025-02-13 03:13:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:05][root][INFO] - Training Epoch: 1/2, step 5324/7134 completed (loss: 0.02693033218383789, acc: 0.9938176274299622)
[2025-02-13 03:13:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:05][root][INFO] - Training Epoch: 1/2, step 5325/7134 completed (loss: 0.02749725989997387, acc: 0.9931856989860535)
[2025-02-13 03:13:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:06][root][INFO] - Training Epoch: 1/2, step 5326/7134 completed (loss: 0.034816186875104904, acc: 0.9887096881866455)
[2025-02-13 03:13:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:06][root][INFO] - Training Epoch: 1/2, step 5327/7134 completed (loss: 0.029906470328569412, acc: 0.9912917017936707)
[2025-02-13 03:13:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:06][root][INFO] - Training Epoch: 1/2, step 5328/7134 completed (loss: 0.016346829012036324, acc: 0.9963302612304688)
[2025-02-13 03:13:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:07][root][INFO] - Training Epoch: 1/2, step 5329/7134 completed (loss: 0.04745757207274437, acc: 0.9886547923088074)
[2025-02-13 03:13:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:07][root][INFO] - Training Epoch: 1/2, step 5330/7134 completed (loss: 0.04721875488758087, acc: 0.9828125238418579)
[2025-02-13 03:13:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:08][root][INFO] - Training Epoch: 1/2, step 5331/7134 completed (loss: 0.05709170922636986, acc: 0.9842382073402405)
[2025-02-13 03:13:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:08][root][INFO] - Training Epoch: 1/2, step 5332/7134 completed (loss: 0.04577995091676712, acc: 0.9934959411621094)
[2025-02-13 03:13:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:09][root][INFO] - Training Epoch: 1/2, step 5333/7134 completed (loss: 0.024156713858246803, acc: 0.9922118186950684)
[2025-02-13 03:13:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:09][root][INFO] - Training Epoch: 1/2, step 5334/7134 completed (loss: 0.07138509303331375, acc: 0.9831546545028687)
[2025-02-13 03:13:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:09][root][INFO] - Training Epoch: 1/2, step 5335/7134 completed (loss: 0.04781701788306236, acc: 0.9888178706169128)
[2025-02-13 03:13:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:10][root][INFO] - Training Epoch: 1/2, step 5336/7134 completed (loss: 0.05042479187250137, acc: 0.989062488079071)
[2025-02-13 03:13:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:10][root][INFO] - Training Epoch: 1/2, step 5337/7134 completed (loss: 0.03806902468204498, acc: 0.9866369962692261)
[2025-02-13 03:13:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:11][root][INFO] - Training Epoch: 1/2, step 5338/7134 completed (loss: 0.018003743141889572, acc: 0.9965753555297852)
[2025-02-13 03:13:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:11][root][INFO] - Training Epoch: 1/2, step 5339/7134 completed (loss: 0.03726394847035408, acc: 0.9909326434135437)
[2025-02-13 03:13:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:12][root][INFO] - Training Epoch: 1/2, step 5340/7134 completed (loss: 0.09050250798463821, acc: 0.9740034937858582)
[2025-02-13 03:13:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:12][root][INFO] - Training Epoch: 1/2, step 5341/7134 completed (loss: 0.01915786601603031, acc: 0.993220329284668)
[2025-02-13 03:13:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:13][root][INFO] - Training Epoch: 1/2, step 5342/7134 completed (loss: 0.060442741960287094, acc: 0.9826897382736206)
[2025-02-13 03:13:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:13][root][INFO] - Training Epoch: 1/2, step 5343/7134 completed (loss: 0.05245917662978172, acc: 0.9868074059486389)
[2025-02-13 03:13:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:14][root][INFO] - Training Epoch: 1/2, step 5344/7134 completed (loss: 0.04872491955757141, acc: 0.9884210228919983)
[2025-02-13 03:13:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:14][root][INFO] - Training Epoch: 1/2, step 5345/7134 completed (loss: 0.034461088478565216, acc: 0.9908046126365662)
[2025-02-13 03:13:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:14][root][INFO] - Training Epoch: 1/2, step 5346/7134 completed (loss: 0.0555373840034008, acc: 0.9874081611633301)
[2025-02-13 03:13:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:15][root][INFO] - Training Epoch: 1/2, step 5347/7134 completed (loss: 0.028599165380001068, acc: 0.9907161593437195)
[2025-02-13 03:13:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:15][root][INFO] - Training Epoch: 1/2, step 5348/7134 completed (loss: 0.039956897497177124, acc: 0.990304708480835)
[2025-02-13 03:13:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:13:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:14:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:15:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:16:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:26][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.0489, device='cuda:0') eval_epoch_loss=tensor(0.0478, device='cuda:0') eval_epoch_acc=tensor(0.9867, device='cuda:0')
[2025-02-13 03:17:26][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-13 03:17:26][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-13 03:17:26][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme/asr_epoch_1_step_5349_loss_0.04778618738055229/model.pt
[2025-02-13 03:17:26][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme directory
[2025-02-13 03:17:26][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.04778618738055229
[2025-02-13 03:17:26][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.9866632223129272
[2025-02-13 03:17:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:27][root][INFO] - Training Epoch: 1/2, step 5349/7134 completed (loss: 0.057845473289489746, acc: 0.9819121360778809)
[2025-02-13 03:17:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:27][root][INFO] - Training Epoch: 1/2, step 5350/7134 completed (loss: 0.0436263345181942, acc: 0.9828269481658936)
[2025-02-13 03:17:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:27][root][INFO] - Training Epoch: 1/2, step 5351/7134 completed (loss: 0.05005030333995819, acc: 0.9863523840904236)
[2025-02-13 03:17:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:28][root][INFO] - Training Epoch: 1/2, step 5352/7134 completed (loss: 0.031114356592297554, acc: 0.9896507263183594)
[2025-02-13 03:17:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:28][root][INFO] - Training Epoch: 1/2, step 5353/7134 completed (loss: 0.015427552163600922, acc: 0.9966177940368652)
[2025-02-13 03:17:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:29][root][INFO] - Training Epoch: 1/2, step 5354/7134 completed (loss: 0.057726453989744186, acc: 0.9812775254249573)
[2025-02-13 03:17:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:29][root][INFO] - Training Epoch: 1/2, step 5355/7134 completed (loss: 0.07905751466751099, acc: 0.9751772880554199)
[2025-02-13 03:17:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:30][root][INFO] - Training Epoch: 1/2, step 5356/7134 completed (loss: 0.06587620824575424, acc: 0.9825654029846191)
[2025-02-13 03:17:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:30][root][INFO] - Training Epoch: 1/2, step 5357/7134 completed (loss: 0.04407971724867821, acc: 0.9860627055168152)
[2025-02-13 03:17:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:31][root][INFO] - Training Epoch: 1/2, step 5358/7134 completed (loss: 0.05279328301548958, acc: 0.9917241334915161)
[2025-02-13 03:17:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:31][root][INFO] - Training Epoch: 1/2, step 5359/7134 completed (loss: 0.024311138316988945, acc: 0.9941725134849548)
[2025-02-13 03:17:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:31][root][INFO] - Training Epoch: 1/2, step 5360/7134 completed (loss: 0.010779286734759808, acc: 0.9972527623176575)
[2025-02-13 03:17:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:32][root][INFO] - Training Epoch: 1/2, step 5361/7134 completed (loss: 0.07402104884386063, acc: 0.9808382987976074)
[2025-02-13 03:17:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:32][root][INFO] - Training Epoch: 1/2, step 5362/7134 completed (loss: 0.03544674068689346, acc: 0.9865671396255493)
[2025-02-13 03:17:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:33][root][INFO] - Training Epoch: 1/2, step 5363/7134 completed (loss: 0.07195569574832916, acc: 0.9811046719551086)
[2025-02-13 03:17:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:33][root][INFO] - Training Epoch: 1/2, step 5364/7134 completed (loss: 0.035857949405908585, acc: 0.9909090995788574)
[2025-02-13 03:17:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:34][root][INFO] - Training Epoch: 1/2, step 5365/7134 completed (loss: 0.025081101804971695, acc: 0.9911816716194153)
[2025-02-13 03:17:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:34][root][INFO] - Training Epoch: 1/2, step 5366/7134 completed (loss: 0.08248473703861237, acc: 0.9791666865348816)
[2025-02-13 03:17:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:35][root][INFO] - Training Epoch: 1/2, step 5367/7134 completed (loss: 0.042347975075244904, acc: 0.9886934757232666)
[2025-02-13 03:17:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:35][root][INFO] - Training Epoch: 1/2, step 5368/7134 completed (loss: 0.04921118542551994, acc: 0.9849498271942139)
[2025-02-13 03:17:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:35][root][INFO] - Training Epoch: 1/2, step 5369/7134 completed (loss: 0.03636278212070465, acc: 0.987261176109314)
[2025-02-13 03:17:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:36][root][INFO] - Training Epoch: 1/2, step 5370/7134 completed (loss: 0.06170353665947914, acc: 0.9843013882637024)
[2025-02-13 03:17:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:36][root][INFO] - Training Epoch: 1/2, step 5371/7134 completed (loss: 0.09283363819122314, acc: 0.9734219312667847)
[2025-02-13 03:17:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:37][root][INFO] - Training Epoch: 1/2, step 5372/7134 completed (loss: 0.009575005620718002, acc: 0.9985975027084351)
[2025-02-13 03:17:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:37][root][INFO] - Training Epoch: 1/2, step 5373/7134 completed (loss: 0.08493991941213608, acc: 0.9728033542633057)
[2025-02-13 03:17:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:38][root][INFO] - Training Epoch: 1/2, step 5374/7134 completed (loss: 0.060945939272642136, acc: 0.9807074069976807)
[2025-02-13 03:17:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:38][root][INFO] - Training Epoch: 1/2, step 5375/7134 completed (loss: 0.07806993275880814, acc: 0.977011501789093)
[2025-02-13 03:17:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:39][root][INFO] - Training Epoch: 1/2, step 5376/7134 completed (loss: 0.023313328623771667, acc: 0.9947735071182251)
[2025-02-13 03:17:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:39][root][INFO] - Training Epoch: 1/2, step 5377/7134 completed (loss: 0.04706171154975891, acc: 0.9893778562545776)
[2025-02-13 03:17:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:40][root][INFO] - Training Epoch: 1/2, step 5378/7134 completed (loss: 0.021439777687191963, acc: 0.9939320683479309)
[2025-02-13 03:17:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:40][root][INFO] - Training Epoch: 1/2, step 5379/7134 completed (loss: 0.04150912165641785, acc: 0.9870298504829407)
[2025-02-13 03:17:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:40][root][INFO] - Training Epoch: 1/2, step 5380/7134 completed (loss: 0.04868658259510994, acc: 0.9876543283462524)
[2025-02-13 03:17:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:41][root][INFO] - Training Epoch: 1/2, step 5381/7134 completed (loss: 0.029516078531742096, acc: 0.9874476790428162)
[2025-02-13 03:17:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:41][root][INFO] - Training Epoch: 1/2, step 5382/7134 completed (loss: 0.021683624014258385, acc: 0.9914675951004028)
[2025-02-13 03:17:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:42][root][INFO] - Training Epoch: 1/2, step 5383/7134 completed (loss: 0.027514910325407982, acc: 0.9924242496490479)
[2025-02-13 03:17:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:42][root][INFO] - Training Epoch: 1/2, step 5384/7134 completed (loss: 0.018272273242473602, acc: 0.9952606558799744)
[2025-02-13 03:17:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:43][root][INFO] - Training Epoch: 1/2, step 5385/7134 completed (loss: 0.0444907546043396, acc: 0.9854689836502075)
[2025-02-13 03:17:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:43][root][INFO] - Training Epoch: 1/2, step 5386/7134 completed (loss: 0.019451986998319626, acc: 0.9971910119056702)
[2025-02-13 03:17:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:43][root][INFO] - Training Epoch: 1/2, step 5387/7134 completed (loss: 0.01983644813299179, acc: 0.9942857027053833)
[2025-02-13 03:17:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:44][root][INFO] - Training Epoch: 1/2, step 5388/7134 completed (loss: 0.030258547514677048, acc: 0.9848484992980957)
[2025-02-13 03:17:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:44][root][INFO] - Training Epoch: 1/2, step 5389/7134 completed (loss: 0.020272325724363327, acc: 0.9920127987861633)
[2025-02-13 03:17:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:45][root][INFO] - Training Epoch: 1/2, step 5390/7134 completed (loss: 0.038803473114967346, acc: 0.9873217344284058)
[2025-02-13 03:17:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:45][root][INFO] - Training Epoch: 1/2, step 5391/7134 completed (loss: 0.07934637367725372, acc: 0.9879931211471558)
[2025-02-13 03:17:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:46][root][INFO] - Training Epoch: 1/2, step 5392/7134 completed (loss: 0.060295239090919495, acc: 0.980182945728302)
[2025-02-13 03:17:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:46][root][INFO] - Training Epoch: 1/2, step 5393/7134 completed (loss: 0.024622993543744087, acc: 0.990234375)
[2025-02-13 03:17:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:47][root][INFO] - Training Epoch: 1/2, step 5394/7134 completed (loss: 0.052435390651226044, acc: 0.9841269850730896)
[2025-02-13 03:17:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:47][root][INFO] - Training Epoch: 1/2, step 5395/7134 completed (loss: 0.026598546653985977, acc: 0.9894419312477112)
[2025-02-13 03:17:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:47][root][INFO] - Training Epoch: 1/2, step 5396/7134 completed (loss: 0.04342501237988472, acc: 0.9894737005233765)
[2025-02-13 03:17:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:48][root][INFO] - Training Epoch: 1/2, step 5397/7134 completed (loss: 0.02300741896033287, acc: 0.9915397763252258)
[2025-02-13 03:17:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:48][root][INFO] - Training Epoch: 1/2, step 5398/7134 completed (loss: 0.024116268381476402, acc: 0.991150438785553)
[2025-02-13 03:17:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:49][root][INFO] - Training Epoch: 1/2, step 5399/7134 completed (loss: 0.03548036515712738, acc: 0.9917920827865601)
[2025-02-13 03:17:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:49][root][INFO] - Training Epoch: 1/2, step 5400/7134 completed (loss: 0.11077880859375, acc: 0.9733542203903198)
[2025-02-13 03:17:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:50][root][INFO] - Training Epoch: 1/2, step 5401/7134 completed (loss: 0.02204803191125393, acc: 0.9919742941856384)
[2025-02-13 03:17:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:50][root][INFO] - Training Epoch: 1/2, step 5402/7134 completed (loss: 0.036326855421066284, acc: 0.9867841601371765)
[2025-02-13 03:17:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:50][root][INFO] - Training Epoch: 1/2, step 5403/7134 completed (loss: 0.07455377280712128, acc: 0.9820554852485657)
[2025-02-13 03:17:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:51][root][INFO] - Training Epoch: 1/2, step 5404/7134 completed (loss: 0.07316327095031738, acc: 0.982550323009491)
[2025-02-13 03:17:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:51][root][INFO] - Training Epoch: 1/2, step 5405/7134 completed (loss: 0.02630198560655117, acc: 0.9930192232131958)
[2025-02-13 03:17:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:52][root][INFO] - Training Epoch: 1/2, step 5406/7134 completed (loss: 0.028865894302725792, acc: 0.9922027587890625)
[2025-02-13 03:17:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:52][root][INFO] - Training Epoch: 1/2, step 5407/7134 completed (loss: 0.045061301440000534, acc: 0.987922728061676)
[2025-02-13 03:17:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:53][root][INFO] - Training Epoch: 1/2, step 5408/7134 completed (loss: 0.07077465206384659, acc: 0.9843993782997131)
[2025-02-13 03:17:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:53][root][INFO] - Training Epoch: 1/2, step 5409/7134 completed (loss: 0.11090964823961258, acc: 0.9803328514099121)
[2025-02-13 03:17:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:53][root][INFO] - Training Epoch: 1/2, step 5410/7134 completed (loss: 0.05769842118024826, acc: 0.9829396605491638)
[2025-02-13 03:17:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:54][root][INFO] - Training Epoch: 1/2, step 5411/7134 completed (loss: 0.05296884477138519, acc: 0.9854862093925476)
[2025-02-13 03:17:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:54][root][INFO] - Training Epoch: 1/2, step 5412/7134 completed (loss: 0.10460174828767776, acc: 0.9807692170143127)
[2025-02-13 03:17:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:55][root][INFO] - Training Epoch: 1/2, step 5413/7134 completed (loss: 0.11256587505340576, acc: 0.9700000286102295)
[2025-02-13 03:17:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:55][root][INFO] - Training Epoch: 1/2, step 5414/7134 completed (loss: 0.13604722917079926, acc: 0.9704369902610779)
[2025-02-13 03:17:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:56][root][INFO] - Training Epoch: 1/2, step 5415/7134 completed (loss: 0.09119454026222229, acc: 0.9756447076797485)
[2025-02-13 03:17:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:56][root][INFO] - Training Epoch: 1/2, step 5416/7134 completed (loss: 0.0634031593799591, acc: 0.9825783967971802)
[2025-02-13 03:17:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:57][root][INFO] - Training Epoch: 1/2, step 5417/7134 completed (loss: 0.038428544998168945, acc: 0.9900124669075012)
[2025-02-13 03:17:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:57][root][INFO] - Training Epoch: 1/2, step 5418/7134 completed (loss: 0.053135041147470474, acc: 0.985228955745697)
[2025-02-13 03:17:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:57][root][INFO] - Training Epoch: 1/2, step 5419/7134 completed (loss: 0.10538973659276962, acc: 0.9711864590644836)
[2025-02-13 03:17:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:58][root][INFO] - Training Epoch: 1/2, step 5420/7134 completed (loss: 0.10208415240049362, acc: 0.9685314893722534)
[2025-02-13 03:17:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:58][root][INFO] - Training Epoch: 1/2, step 5421/7134 completed (loss: 0.07556524872779846, acc: 0.9781591296195984)
[2025-02-13 03:17:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:59][root][INFO] - Training Epoch: 1/2, step 5422/7134 completed (loss: 0.0667906254529953, acc: 0.9792899489402771)
[2025-02-13 03:17:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:17:59][root][INFO] - Training Epoch: 1/2, step 5423/7134 completed (loss: 0.048289135098457336, acc: 0.9912023544311523)
[2025-02-13 03:17:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:00][root][INFO] - Training Epoch: 1/2, step 5424/7134 completed (loss: 0.046137068420648575, acc: 0.9876760840415955)
[2025-02-13 03:18:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:00][root][INFO] - Training Epoch: 1/2, step 5425/7134 completed (loss: 0.040591731667518616, acc: 0.9941747784614563)
[2025-02-13 03:18:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:00][root][INFO] - Training Epoch: 1/2, step 5426/7134 completed (loss: 0.07357991486787796, acc: 0.9839357137680054)
[2025-02-13 03:18:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:01][root][INFO] - Training Epoch: 1/2, step 5427/7134 completed (loss: 0.033524513244628906, acc: 0.9921011328697205)
[2025-02-13 03:18:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:01][root][INFO] - Training Epoch: 1/2, step 5428/7134 completed (loss: 0.054185785353183746, acc: 0.9812606573104858)
[2025-02-13 03:18:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:02][root][INFO] - Training Epoch: 1/2, step 5429/7134 completed (loss: 0.07231169193983078, acc: 0.9801443815231323)
[2025-02-13 03:18:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:02][root][INFO] - Training Epoch: 1/2, step 5430/7134 completed (loss: 0.02995864488184452, acc: 0.9905956387519836)
[2025-02-13 03:18:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:03][root][INFO] - Training Epoch: 1/2, step 5431/7134 completed (loss: 0.03405813127756119, acc: 0.9905481934547424)
[2025-02-13 03:18:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:03][root][INFO] - Training Epoch: 1/2, step 5432/7134 completed (loss: 0.021398084238171577, acc: 0.994350254535675)
[2025-02-13 03:18:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:03][root][INFO] - Training Epoch: 1/2, step 5433/7134 completed (loss: 0.04356659576296806, acc: 0.991919219493866)
[2025-02-13 03:18:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:04][root][INFO] - Training Epoch: 1/2, step 5434/7134 completed (loss: 0.02769922837615013, acc: 0.9879275560379028)
[2025-02-13 03:18:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:04][root][INFO] - Training Epoch: 1/2, step 5435/7134 completed (loss: 0.02835496887564659, acc: 0.991631805896759)
[2025-02-13 03:18:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:05][root][INFO] - Training Epoch: 1/2, step 5436/7134 completed (loss: 0.04592805355787277, acc: 0.9851632118225098)
[2025-02-13 03:18:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:05][root][INFO] - Training Epoch: 1/2, step 5437/7134 completed (loss: 0.01767868548631668, acc: 0.9965096116065979)
[2025-02-13 03:18:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:06][root][INFO] - Training Epoch: 1/2, step 5438/7134 completed (loss: 0.029169967398047447, acc: 0.9922480583190918)
[2025-02-13 03:18:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:06][root][INFO] - Training Epoch: 1/2, step 5439/7134 completed (loss: 0.04392344877123833, acc: 0.9840116500854492)
[2025-02-13 03:18:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:06][root][INFO] - Training Epoch: 1/2, step 5440/7134 completed (loss: 0.042079027742147446, acc: 0.9885057210922241)
[2025-02-13 03:18:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:07][root][INFO] - Training Epoch: 1/2, step 5441/7134 completed (loss: 0.03382888808846474, acc: 0.9914320707321167)
[2025-02-13 03:18:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:07][root][INFO] - Training Epoch: 1/2, step 5442/7134 completed (loss: 0.02326321043074131, acc: 0.9932340979576111)
[2025-02-13 03:18:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:08][root][INFO] - Training Epoch: 1/2, step 5443/7134 completed (loss: 0.031288329511880875, acc: 0.9900568127632141)
[2025-02-13 03:18:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:08][root][INFO] - Training Epoch: 1/2, step 5444/7134 completed (loss: 0.027194609865546227, acc: 0.9927007555961609)
[2025-02-13 03:18:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:09][root][INFO] - Training Epoch: 1/2, step 5445/7134 completed (loss: 0.057319238781929016, acc: 0.9853747487068176)
[2025-02-13 03:18:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:09][root][INFO] - Training Epoch: 1/2, step 5446/7134 completed (loss: 0.027732644230127335, acc: 0.9957982897758484)
[2025-02-13 03:18:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:09][root][INFO] - Training Epoch: 1/2, step 5447/7134 completed (loss: 0.03557131811976433, acc: 0.9855072498321533)
[2025-02-13 03:18:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:10][root][INFO] - Training Epoch: 1/2, step 5448/7134 completed (loss: 0.037758857011795044, acc: 0.9846368432044983)
[2025-02-13 03:18:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:10][root][INFO] - Training Epoch: 1/2, step 5449/7134 completed (loss: 0.027065914124250412, acc: 0.9929824471473694)
[2025-02-13 03:18:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:11][root][INFO] - Training Epoch: 1/2, step 5450/7134 completed (loss: 0.011803037486970425, acc: 0.9948520064353943)
[2025-02-13 03:18:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:11][root][INFO] - Training Epoch: 1/2, step 5451/7134 completed (loss: 0.02852557599544525, acc: 0.993446946144104)
[2025-02-13 03:18:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:12][root][INFO] - Training Epoch: 1/2, step 5452/7134 completed (loss: 0.05484398081898689, acc: 0.985788106918335)
[2025-02-13 03:18:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:12][root][INFO] - Training Epoch: 1/2, step 5453/7134 completed (loss: 0.04197867959737778, acc: 0.987730085849762)
[2025-02-13 03:18:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:13][root][INFO] - Training Epoch: 1/2, step 5454/7134 completed (loss: 0.03624531626701355, acc: 0.9908735156059265)
[2025-02-13 03:18:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:13][root][INFO] - Training Epoch: 1/2, step 5455/7134 completed (loss: 0.06640560179948807, acc: 0.9835858345031738)
[2025-02-13 03:18:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:13][root][INFO] - Training Epoch: 1/2, step 5456/7134 completed (loss: 0.035856056958436966, acc: 0.991183876991272)
[2025-02-13 03:18:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:14][root][INFO] - Training Epoch: 1/2, step 5457/7134 completed (loss: 0.06419597566127777, acc: 0.9865771532058716)
[2025-02-13 03:18:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:14][root][INFO] - Training Epoch: 1/2, step 5458/7134 completed (loss: 0.03519929572939873, acc: 0.9885350465774536)
[2025-02-13 03:18:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:15][root][INFO] - Training Epoch: 1/2, step 5459/7134 completed (loss: 0.008812672458589077, acc: 0.997019350528717)
[2025-02-13 03:18:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:15][root][INFO] - Training Epoch: 1/2, step 5460/7134 completed (loss: 0.03397602587938309, acc: 0.9930459260940552)
[2025-02-13 03:18:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:16][root][INFO] - Training Epoch: 1/2, step 5461/7134 completed (loss: 0.06161247193813324, acc: 0.9845474362373352)
[2025-02-13 03:18:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:16][root][INFO] - Training Epoch: 1/2, step 5462/7134 completed (loss: 0.008262352086603642, acc: 0.9985693693161011)
[2025-02-13 03:18:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:16][root][INFO] - Training Epoch: 1/2, step 5463/7134 completed (loss: 0.029725676402449608, acc: 0.9908397197723389)
[2025-02-13 03:18:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:17][root][INFO] - Training Epoch: 1/2, step 5464/7134 completed (loss: 0.051501013338565826, acc: 0.9854689836502075)
[2025-02-13 03:18:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:17][root][INFO] - Training Epoch: 1/2, step 5465/7134 completed (loss: 0.06093202903866768, acc: 0.9895366430282593)
[2025-02-13 03:18:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:18][root][INFO] - Training Epoch: 1/2, step 5466/7134 completed (loss: 0.020384298637509346, acc: 0.9930955171585083)
[2025-02-13 03:18:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:18][root][INFO] - Training Epoch: 1/2, step 5467/7134 completed (loss: 0.018908748403191566, acc: 0.9953051805496216)
[2025-02-13 03:18:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:19][root][INFO] - Training Epoch: 1/2, step 5468/7134 completed (loss: 0.042187321931123734, acc: 0.9918793439865112)
[2025-02-13 03:18:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:19][root][INFO] - Training Epoch: 1/2, step 5469/7134 completed (loss: 0.02353118546307087, acc: 0.9926380515098572)
[2025-02-13 03:18:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:20][root][INFO] - Training Epoch: 1/2, step 5470/7134 completed (loss: 0.009439641609787941, acc: 0.9987951517105103)
[2025-02-13 03:18:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:20][root][INFO] - Training Epoch: 1/2, step 5471/7134 completed (loss: 0.01733221486210823, acc: 0.9945945739746094)
[2025-02-13 03:18:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:21][root][INFO] - Training Epoch: 1/2, step 5472/7134 completed (loss: 0.0227635707706213, acc: 0.996259331703186)
[2025-02-13 03:18:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:21][root][INFO] - Training Epoch: 1/2, step 5473/7134 completed (loss: 0.018254905939102173, acc: 0.991411030292511)
[2025-02-13 03:18:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:21][root][INFO] - Training Epoch: 1/2, step 5474/7134 completed (loss: 0.023307539522647858, acc: 0.991428554058075)
[2025-02-13 03:18:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:22][root][INFO] - Training Epoch: 1/2, step 5475/7134 completed (loss: 0.026072723791003227, acc: 0.991465151309967)
[2025-02-13 03:18:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:22][root][INFO] - Training Epoch: 1/2, step 5476/7134 completed (loss: 0.009397326037287712, acc: 0.9956584572792053)
[2025-02-13 03:18:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:23][root][INFO] - Training Epoch: 1/2, step 5477/7134 completed (loss: 0.01663758046925068, acc: 0.9963054060935974)
[2025-02-13 03:18:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:23][root][INFO] - Training Epoch: 1/2, step 5478/7134 completed (loss: 0.05271872878074646, acc: 0.9811320900917053)
[2025-02-13 03:18:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:24][root][INFO] - Training Epoch: 1/2, step 5479/7134 completed (loss: 0.04249662533402443, acc: 0.98097825050354)
[2025-02-13 03:18:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:24][root][INFO] - Training Epoch: 1/2, step 5480/7134 completed (loss: 0.05336576700210571, acc: 0.9855263233184814)
[2025-02-13 03:18:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:25][root][INFO] - Training Epoch: 1/2, step 5481/7134 completed (loss: 0.02904914692044258, acc: 0.991919219493866)
[2025-02-13 03:18:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:25][root][INFO] - Training Epoch: 1/2, step 5482/7134 completed (loss: 0.046681296080350876, acc: 0.9866989254951477)
[2025-02-13 03:18:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:26][root][INFO] - Training Epoch: 1/2, step 5483/7134 completed (loss: 0.05015299841761589, acc: 0.9860724210739136)
[2025-02-13 03:18:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:26][root][INFO] - Training Epoch: 1/2, step 5484/7134 completed (loss: 0.06662771105766296, acc: 0.9791377186775208)
[2025-02-13 03:18:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:27][root][INFO] - Training Epoch: 1/2, step 5485/7134 completed (loss: 0.06318031996488571, acc: 0.9727767705917358)
[2025-02-13 03:18:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:27][root][INFO] - Training Epoch: 1/2, step 5486/7134 completed (loss: 0.06297870725393295, acc: 0.9775086641311646)
[2025-02-13 03:18:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:27][root][INFO] - Training Epoch: 1/2, step 5487/7134 completed (loss: 0.019750108942389488, acc: 0.9934959411621094)
[2025-02-13 03:18:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:28][root][INFO] - Training Epoch: 1/2, step 5488/7134 completed (loss: 0.0759149119257927, acc: 0.9710526466369629)
[2025-02-13 03:18:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:28][root][INFO] - Training Epoch: 1/2, step 5489/7134 completed (loss: 0.046041395515203476, acc: 0.9891641139984131)
[2025-02-13 03:18:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:29][root][INFO] - Training Epoch: 1/2, step 5490/7134 completed (loss: 0.020698489621281624, acc: 0.9931662678718567)
[2025-02-13 03:18:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:29][root][INFO] - Training Epoch: 1/2, step 5491/7134 completed (loss: 0.02666337788105011, acc: 0.9941349029541016)
[2025-02-13 03:18:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:30][root][INFO] - Training Epoch: 1/2, step 5492/7134 completed (loss: 0.03824244439601898, acc: 0.9915397763252258)
[2025-02-13 03:18:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:30][root][INFO] - Training Epoch: 1/2, step 5493/7134 completed (loss: 0.044364091008901596, acc: 0.979619562625885)
[2025-02-13 03:18:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:30][root][INFO] - Training Epoch: 1/2, step 5494/7134 completed (loss: 0.020510811358690262, acc: 0.9943100810050964)
[2025-02-13 03:18:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:31][root][INFO] - Training Epoch: 1/2, step 5495/7134 completed (loss: 0.02281513810157776, acc: 0.9928315281867981)
[2025-02-13 03:18:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:31][root][INFO] - Training Epoch: 1/2, step 5496/7134 completed (loss: 0.04284094274044037, acc: 0.9883268475532532)
[2025-02-13 03:18:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:32][root][INFO] - Training Epoch: 1/2, step 5497/7134 completed (loss: 0.07199164479970932, acc: 0.9780380725860596)
[2025-02-13 03:18:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:32][root][INFO] - Training Epoch: 1/2, step 5498/7134 completed (loss: 0.04301663488149643, acc: 0.9867374300956726)
[2025-02-13 03:18:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:33][root][INFO] - Training Epoch: 1/2, step 5499/7134 completed (loss: 0.05013474076986313, acc: 0.9773869514465332)
[2025-02-13 03:18:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:33][root][INFO] - Training Epoch: 1/2, step 5500/7134 completed (loss: 0.04163596034049988, acc: 0.9877216815948486)
[2025-02-13 03:18:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:33][root][INFO] - Training Epoch: 1/2, step 5501/7134 completed (loss: 0.026959529146552086, acc: 0.9876543283462524)
[2025-02-13 03:18:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:34][root][INFO] - Training Epoch: 1/2, step 5502/7134 completed (loss: 0.040748629719018936, acc: 0.9878542423248291)
[2025-02-13 03:18:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:34][root][INFO] - Training Epoch: 1/2, step 5503/7134 completed (loss: 0.05459479242563248, acc: 0.9920477271080017)
[2025-02-13 03:18:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:35][root][INFO] - Training Epoch: 1/2, step 5504/7134 completed (loss: 0.027350327000021935, acc: 0.9927361011505127)
[2025-02-13 03:18:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:35][root][INFO] - Training Epoch: 1/2, step 5505/7134 completed (loss: 0.03760958090424538, acc: 0.9872340559959412)
[2025-02-13 03:18:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:36][root][INFO] - Training Epoch: 1/2, step 5506/7134 completed (loss: 0.046366460621356964, acc: 0.979238748550415)
[2025-02-13 03:18:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:36][root][INFO] - Training Epoch: 1/2, step 5507/7134 completed (loss: 0.04554242640733719, acc: 0.9845938086509705)
[2025-02-13 03:18:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:36][root][INFO] - Training Epoch: 1/2, step 5508/7134 completed (loss: 0.022787561640143394, acc: 0.9941588640213013)
[2025-02-13 03:18:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:37][root][INFO] - Training Epoch: 1/2, step 5509/7134 completed (loss: 0.03594258427619934, acc: 0.9906432628631592)
[2025-02-13 03:18:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:37][root][INFO] - Training Epoch: 1/2, step 5510/7134 completed (loss: 0.04313234984874725, acc: 0.9884318709373474)
[2025-02-13 03:18:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:38][root][INFO] - Training Epoch: 1/2, step 5511/7134 completed (loss: 0.0626312643289566, acc: 0.9819004535675049)
[2025-02-13 03:18:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:38][root][INFO] - Training Epoch: 1/2, step 5512/7134 completed (loss: 0.01603907346725464, acc: 0.9973649382591248)
[2025-02-13 03:18:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:39][root][INFO] - Training Epoch: 1/2, step 5513/7134 completed (loss: 0.05197945237159729, acc: 0.9850543737411499)
[2025-02-13 03:18:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:39][root][INFO] - Training Epoch: 1/2, step 5514/7134 completed (loss: 0.1376878172159195, acc: 0.9626865386962891)
[2025-02-13 03:18:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:40][root][INFO] - Training Epoch: 1/2, step 5515/7134 completed (loss: 0.042782630771398544, acc: 0.9885877370834351)
[2025-02-13 03:18:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:40][root][INFO] - Training Epoch: 1/2, step 5516/7134 completed (loss: 0.04702106863260269, acc: 0.9915611743927002)
[2025-02-13 03:18:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:41][root][INFO] - Training Epoch: 1/2, step 5517/7134 completed (loss: 0.038488321006298065, acc: 0.9880319237709045)
[2025-02-13 03:18:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:41][root][INFO] - Training Epoch: 1/2, step 5518/7134 completed (loss: 0.048144593834877014, acc: 0.98777174949646)
[2025-02-13 03:18:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:41][root][INFO] - Training Epoch: 1/2, step 5519/7134 completed (loss: 0.019133372232317924, acc: 0.9944827556610107)
[2025-02-13 03:18:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:42][root][INFO] - Training Epoch: 1/2, step 5520/7134 completed (loss: 0.02216162346303463, acc: 0.9943374991416931)
[2025-02-13 03:18:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:42][root][INFO] - Training Epoch: 1/2, step 5521/7134 completed (loss: 0.05449507758021355, acc: 0.9886934757232666)
[2025-02-13 03:18:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:43][root][INFO] - Training Epoch: 1/2, step 5522/7134 completed (loss: 0.04886874929070473, acc: 0.9882352948188782)
[2025-02-13 03:18:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:43][root][INFO] - Training Epoch: 1/2, step 5523/7134 completed (loss: 0.0439043752849102, acc: 0.9901685118675232)
[2025-02-13 03:18:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:44][root][INFO] - Training Epoch: 1/2, step 5524/7134 completed (loss: 0.046004194766283035, acc: 0.9845201373100281)
[2025-02-13 03:18:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:44][root][INFO] - Training Epoch: 1/2, step 5525/7134 completed (loss: 0.03260640427470207, acc: 0.9882869720458984)
[2025-02-13 03:18:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:44][root][INFO] - Training Epoch: 1/2, step 5526/7134 completed (loss: 0.04923677071928978, acc: 0.9882179498672485)
[2025-02-13 03:18:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:45][root][INFO] - Training Epoch: 1/2, step 5527/7134 completed (loss: 0.04100674390792847, acc: 0.9882352948188782)
[2025-02-13 03:18:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:45][root][INFO] - Training Epoch: 1/2, step 5528/7134 completed (loss: 0.02983757108449936, acc: 0.9917159676551819)
[2025-02-13 03:18:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:46][root][INFO] - Training Epoch: 1/2, step 5529/7134 completed (loss: 0.04963658004999161, acc: 0.9834515452384949)
[2025-02-13 03:18:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:46][root][INFO] - Training Epoch: 1/2, step 5530/7134 completed (loss: 0.029973534867167473, acc: 0.9940387606620789)
[2025-02-13 03:18:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:47][root][INFO] - Training Epoch: 1/2, step 5531/7134 completed (loss: 0.05503743141889572, acc: 0.9813200235366821)
[2025-02-13 03:18:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:47][root][INFO] - Training Epoch: 1/2, step 5532/7134 completed (loss: 0.06090988591313362, acc: 0.9802225232124329)
[2025-02-13 03:18:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:48][root][INFO] - Training Epoch: 1/2, step 5533/7134 completed (loss: 0.014213467948138714, acc: 0.997732400894165)
[2025-02-13 03:18:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:48][root][INFO] - Training Epoch: 1/2, step 5534/7134 completed (loss: 0.03230317309498787, acc: 0.9906542301177979)
[2025-02-13 03:18:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:49][root][INFO] - Training Epoch: 1/2, step 5535/7134 completed (loss: 0.03763417899608612, acc: 0.9891186356544495)
[2025-02-13 03:18:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:49][root][INFO] - Training Epoch: 1/2, step 5536/7134 completed (loss: 0.02837696112692356, acc: 0.9918604493141174)
[2025-02-13 03:18:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:49][root][INFO] - Training Epoch: 1/2, step 5537/7134 completed (loss: 0.04962950944900513, acc: 0.9813302159309387)
[2025-02-13 03:18:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:50][root][INFO] - Training Epoch: 1/2, step 5538/7134 completed (loss: 0.027476904913783073, acc: 0.9931740760803223)
[2025-02-13 03:18:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:50][root][INFO] - Training Epoch: 1/2, step 5539/7134 completed (loss: 0.039830226451158524, acc: 0.9887387156486511)
[2025-02-13 03:18:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:51][root][INFO] - Training Epoch: 1/2, step 5540/7134 completed (loss: 0.04427638277411461, acc: 0.9881578683853149)
[2025-02-13 03:18:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:51][root][INFO] - Training Epoch: 1/2, step 5541/7134 completed (loss: 0.020929614081978798, acc: 0.9933510422706604)
[2025-02-13 03:18:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:52][root][INFO] - Training Epoch: 1/2, step 5542/7134 completed (loss: 0.019310247153043747, acc: 0.9942938685417175)
[2025-02-13 03:18:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:52][root][INFO] - Training Epoch: 1/2, step 5543/7134 completed (loss: 0.03289785981178284, acc: 0.9933481216430664)
[2025-02-13 03:18:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:53][root][INFO] - Training Epoch: 1/2, step 5544/7134 completed (loss: 0.023205500096082687, acc: 0.9881720542907715)
[2025-02-13 03:18:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:53][root][INFO] - Training Epoch: 1/2, step 5545/7134 completed (loss: 0.02940806746482849, acc: 0.9923497438430786)
[2025-02-13 03:18:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:54][root][INFO] - Training Epoch: 1/2, step 5546/7134 completed (loss: 0.03965737670660019, acc: 0.9911406636238098)
[2025-02-13 03:18:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:54][root][INFO] - Training Epoch: 1/2, step 5547/7134 completed (loss: 0.023214098066091537, acc: 0.9953917264938354)
[2025-02-13 03:18:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:54][root][INFO] - Training Epoch: 1/2, step 5548/7134 completed (loss: 0.0252621341496706, acc: 0.9953488111495972)
[2025-02-13 03:18:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:55][root][INFO] - Training Epoch: 1/2, step 5549/7134 completed (loss: 0.030674856156110764, acc: 0.9871428608894348)
[2025-02-13 03:18:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:55][root][INFO] - Training Epoch: 1/2, step 5550/7134 completed (loss: 0.013586238026618958, acc: 0.9929494857788086)
[2025-02-13 03:18:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:56][root][INFO] - Training Epoch: 1/2, step 5551/7134 completed (loss: 0.06542937457561493, acc: 0.9837618470191956)
[2025-02-13 03:18:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:56][root][INFO] - Training Epoch: 1/2, step 5552/7134 completed (loss: 0.02061665989458561, acc: 0.9952437281608582)
[2025-02-13 03:18:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:57][root][INFO] - Training Epoch: 1/2, step 5553/7134 completed (loss: 0.03687959536910057, acc: 0.9913473129272461)
[2025-02-13 03:18:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:57][root][INFO] - Training Epoch: 1/2, step 5554/7134 completed (loss: 0.021526996046304703, acc: 0.995110034942627)
[2025-02-13 03:18:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:57][root][INFO] - Training Epoch: 1/2, step 5555/7134 completed (loss: 0.024218905717134476, acc: 0.9893048405647278)
[2025-02-13 03:18:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:58][root][INFO] - Training Epoch: 1/2, step 5556/7134 completed (loss: 0.03606286272406578, acc: 0.9883268475532532)
[2025-02-13 03:18:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:58][root][INFO] - Training Epoch: 1/2, step 5557/7134 completed (loss: 0.022763922810554504, acc: 0.9957982897758484)
[2025-02-13 03:18:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:59][root][INFO] - Training Epoch: 1/2, step 5558/7134 completed (loss: 0.021924521774053574, acc: 0.992546558380127)
[2025-02-13 03:18:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:18:59][root][INFO] - Training Epoch: 1/2, step 5559/7134 completed (loss: 0.03186843544244766, acc: 0.992546558380127)
[2025-02-13 03:18:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:00][root][INFO] - Training Epoch: 1/2, step 5560/7134 completed (loss: 0.05794581398367882, acc: 0.9870466589927673)
[2025-02-13 03:19:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:00][root][INFO] - Training Epoch: 1/2, step 5561/7134 completed (loss: 0.044919922947883606, acc: 0.992438554763794)
[2025-02-13 03:19:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:00][root][INFO] - Training Epoch: 1/2, step 5562/7134 completed (loss: 0.03992997854948044, acc: 0.9948805570602417)
[2025-02-13 03:19:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:01][root][INFO] - Training Epoch: 1/2, step 5563/7134 completed (loss: 0.05023687705397606, acc: 0.9924924969673157)
[2025-02-13 03:19:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:01][root][INFO] - Training Epoch: 1/2, step 5564/7134 completed (loss: 0.025538837537169456, acc: 0.9972106218338013)
[2025-02-13 03:19:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:02][root][INFO] - Training Epoch: 1/2, step 5565/7134 completed (loss: 0.0069042411632835865, acc: 0.9984639286994934)
[2025-02-13 03:19:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:02][root][INFO] - Training Epoch: 1/2, step 5566/7134 completed (loss: 0.011735117994248867, acc: 0.9988518953323364)
[2025-02-13 03:19:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:03][root][INFO] - Training Epoch: 1/2, step 5567/7134 completed (loss: 0.014186615124344826, acc: 0.9954338073730469)
[2025-02-13 03:19:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:03][root][INFO] - Training Epoch: 1/2, step 5568/7134 completed (loss: 0.024636035785079002, acc: 0.9922680258750916)
[2025-02-13 03:19:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:03][root][INFO] - Training Epoch: 1/2, step 5569/7134 completed (loss: 0.01144691463559866, acc: 0.9977169036865234)
[2025-02-13 03:19:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:04][root][INFO] - Training Epoch: 1/2, step 5570/7134 completed (loss: 0.028423672541975975, acc: 0.9937888383865356)
[2025-02-13 03:19:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:04][root][INFO] - Training Epoch: 1/2, step 5571/7134 completed (loss: 0.04139862582087517, acc: 0.9866270422935486)
[2025-02-13 03:19:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:05][root][INFO] - Training Epoch: 1/2, step 5572/7134 completed (loss: 0.04208273068070412, acc: 0.9945054650306702)
[2025-02-13 03:19:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:05][root][INFO] - Training Epoch: 1/2, step 5573/7134 completed (loss: 0.05091317743062973, acc: 0.9883551597595215)
[2025-02-13 03:19:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:06][root][INFO] - Training Epoch: 1/2, step 5574/7134 completed (loss: 0.01002438087016344, acc: 0.9968253970146179)
[2025-02-13 03:19:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:06][root][INFO] - Training Epoch: 1/2, step 5575/7134 completed (loss: 0.026609772816300392, acc: 0.9904305934906006)
[2025-02-13 03:19:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:06][root][INFO] - Training Epoch: 1/2, step 5576/7134 completed (loss: 0.026140792295336723, acc: 0.984240710735321)
[2025-02-13 03:19:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:07][root][INFO] - Training Epoch: 1/2, step 5577/7134 completed (loss: 0.03143421560525894, acc: 0.9895287752151489)
[2025-02-13 03:19:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:07][root][INFO] - Training Epoch: 1/2, step 5578/7134 completed (loss: 0.01970670372247696, acc: 0.991391658782959)
[2025-02-13 03:19:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:08][root][INFO] - Training Epoch: 1/2, step 5579/7134 completed (loss: 0.033404167741537094, acc: 0.9930843710899353)
[2025-02-13 03:19:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:08][root][INFO] - Training Epoch: 1/2, step 5580/7134 completed (loss: 0.011795852333307266, acc: 0.9972714781761169)
[2025-02-13 03:19:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:09][root][INFO] - Training Epoch: 1/2, step 5581/7134 completed (loss: 0.016753217205405235, acc: 0.9946808218955994)
[2025-02-13 03:19:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:09][root][INFO] - Training Epoch: 1/2, step 5582/7134 completed (loss: 0.021110648289322853, acc: 0.9947848916053772)
[2025-02-13 03:19:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:10][root][INFO] - Training Epoch: 1/2, step 5583/7134 completed (loss: 0.019978294149041176, acc: 0.994962215423584)
[2025-02-13 03:19:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:10][root][INFO] - Training Epoch: 1/2, step 5584/7134 completed (loss: 0.021689053624868393, acc: 0.9940828680992126)
[2025-02-13 03:19:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:10][root][INFO] - Training Epoch: 1/2, step 5585/7134 completed (loss: 0.037281204015016556, acc: 0.9915151596069336)
[2025-02-13 03:19:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:11][root][INFO] - Training Epoch: 1/2, step 5586/7134 completed (loss: 0.01651526801288128, acc: 0.9946714043617249)
[2025-02-13 03:19:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:11][root][INFO] - Training Epoch: 1/2, step 5587/7134 completed (loss: 0.02100987732410431, acc: 0.994452178478241)
[2025-02-13 03:19:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:12][root][INFO] - Training Epoch: 1/2, step 5588/7134 completed (loss: 0.01180744357407093, acc: 0.9969742894172668)
[2025-02-13 03:19:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:12][root][INFO] - Training Epoch: 1/2, step 5589/7134 completed (loss: 0.022230984643101692, acc: 0.9952681660652161)
[2025-02-13 03:19:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:13][root][INFO] - Training Epoch: 1/2, step 5590/7134 completed (loss: 0.03718600794672966, acc: 0.9901408553123474)
[2025-02-13 03:19:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:13][root][INFO] - Training Epoch: 1/2, step 5591/7134 completed (loss: 0.022998427972197533, acc: 0.9895287752151489)
[2025-02-13 03:19:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:13][root][INFO] - Training Epoch: 1/2, step 5592/7134 completed (loss: 0.01621781848371029, acc: 0.9958847761154175)
[2025-02-13 03:19:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:14][root][INFO] - Training Epoch: 1/2, step 5593/7134 completed (loss: 0.029607802629470825, acc: 0.9922279715538025)
[2025-02-13 03:19:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:14][root][INFO] - Training Epoch: 1/2, step 5594/7134 completed (loss: 0.017083629965782166, acc: 0.9946091771125793)
[2025-02-13 03:19:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:15][root][INFO] - Training Epoch: 1/2, step 5595/7134 completed (loss: 0.028428735211491585, acc: 0.9919571280479431)
[2025-02-13 03:19:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:15][root][INFO] - Training Epoch: 1/2, step 5596/7134 completed (loss: 0.01438160054385662, acc: 0.9946236610412598)
[2025-02-13 03:19:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:16][root][INFO] - Training Epoch: 1/2, step 5597/7134 completed (loss: 0.04273393750190735, acc: 0.9876033067703247)
[2025-02-13 03:19:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:16][root][INFO] - Training Epoch: 1/2, step 5598/7134 completed (loss: 0.027820350602269173, acc: 0.9916201233863831)
[2025-02-13 03:19:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:16][root][INFO] - Training Epoch: 1/2, step 5599/7134 completed (loss: 0.07220553606748581, acc: 0.9836065769195557)
[2025-02-13 03:19:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:17][root][INFO] - Training Epoch: 1/2, step 5600/7134 completed (loss: 0.015542848967015743, acc: 0.9957020282745361)
[2025-02-13 03:19:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:17][root][INFO] - Training Epoch: 1/2, step 5601/7134 completed (loss: 0.040570203214883804, acc: 0.9860869646072388)
[2025-02-13 03:19:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:18][root][INFO] - Training Epoch: 1/2, step 5602/7134 completed (loss: 0.04546619579195976, acc: 0.9871175289154053)
[2025-02-13 03:19:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:18][root][INFO] - Training Epoch: 1/2, step 5603/7134 completed (loss: 0.0062667131423950195, acc: 1.0)
[2025-02-13 03:19:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:18][root][INFO] - Training Epoch: 1/2, step 5604/7134 completed (loss: 0.04500122368335724, acc: 0.9823151230812073)
[2025-02-13 03:19:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:19][root][INFO] - Training Epoch: 1/2, step 5605/7134 completed (loss: 0.03670386224985123, acc: 0.9864176511764526)
[2025-02-13 03:19:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:19][root][INFO] - Training Epoch: 1/2, step 5606/7134 completed (loss: 0.040362436324357986, acc: 0.9914529919624329)
[2025-02-13 03:19:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:20][root][INFO] - Training Epoch: 1/2, step 5607/7134 completed (loss: 0.02266673743724823, acc: 0.9939849376678467)
[2025-02-13 03:19:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:20][root][INFO] - Training Epoch: 1/2, step 5608/7134 completed (loss: 0.030249934643507004, acc: 0.9885057210922241)
[2025-02-13 03:19:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:20][root][INFO] - Training Epoch: 1/2, step 5609/7134 completed (loss: 0.04052150622010231, acc: 0.9895651936531067)
[2025-02-13 03:19:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:21][root][INFO] - Training Epoch: 1/2, step 5610/7134 completed (loss: 0.024366937577724457, acc: 0.9939637780189514)
[2025-02-13 03:19:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:21][root][INFO] - Training Epoch: 1/2, step 5611/7134 completed (loss: 0.04268816113471985, acc: 0.9857594966888428)
[2025-02-13 03:19:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:22][root][INFO] - Training Epoch: 1/2, step 5612/7134 completed (loss: 0.04238986223936081, acc: 0.9861963391304016)
[2025-02-13 03:19:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:22][root][INFO] - Training Epoch: 1/2, step 5613/7134 completed (loss: 0.03975342959165573, acc: 0.989266574382782)
[2025-02-13 03:19:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:23][root][INFO] - Training Epoch: 1/2, step 5614/7134 completed (loss: 0.048219118267297745, acc: 0.9842857122421265)
[2025-02-13 03:19:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:23][root][INFO] - Training Epoch: 1/2, step 5615/7134 completed (loss: 0.05193055421113968, acc: 0.984674334526062)
[2025-02-13 03:19:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:23][root][INFO] - Training Epoch: 1/2, step 5616/7134 completed (loss: 0.026348479092121124, acc: 0.9925705790519714)
[2025-02-13 03:19:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:24][root][INFO] - Training Epoch: 1/2, step 5617/7134 completed (loss: 0.007672108709812164, acc: 0.9983766078948975)
[2025-02-13 03:19:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:24][root][INFO] - Training Epoch: 1/2, step 5618/7134 completed (loss: 0.054260317236185074, acc: 0.9821138381958008)
[2025-02-13 03:19:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:25][root][INFO] - Training Epoch: 1/2, step 5619/7134 completed (loss: 0.05471550300717354, acc: 0.9837728142738342)
[2025-02-13 03:19:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:25][root][INFO] - Training Epoch: 1/2, step 5620/7134 completed (loss: 0.037970684468746185, acc: 0.9906542301177979)
[2025-02-13 03:19:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:25][root][INFO] - Training Epoch: 1/2, step 5621/7134 completed (loss: 0.022892287001013756, acc: 0.9911054372787476)
[2025-02-13 03:19:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:26][root][INFO] - Training Epoch: 1/2, step 5622/7134 completed (loss: 0.04464161768555641, acc: 0.9918256402015686)
[2025-02-13 03:19:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:26][root][INFO] - Training Epoch: 1/2, step 5623/7134 completed (loss: 0.0245331060141325, acc: 0.9912891983985901)
[2025-02-13 03:19:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:27][root][INFO] - Training Epoch: 1/2, step 5624/7134 completed (loss: 0.03657698258757591, acc: 0.9885433912277222)
[2025-02-13 03:19:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:27][root][INFO] - Training Epoch: 1/2, step 5625/7134 completed (loss: 0.009943053126335144, acc: 1.0)
[2025-02-13 03:19:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:27][root][INFO] - Training Epoch: 1/2, step 5626/7134 completed (loss: 0.025398699566721916, acc: 0.9949238300323486)
[2025-02-13 03:19:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:28][root][INFO] - Training Epoch: 1/2, step 5627/7134 completed (loss: 0.11340539157390594, acc: 0.9703153967857361)
[2025-02-13 03:19:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:28][root][INFO] - Training Epoch: 1/2, step 5628/7134 completed (loss: 0.04214323312044144, acc: 0.9872495532035828)
[2025-02-13 03:19:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:29][root][INFO] - Training Epoch: 1/2, step 5629/7134 completed (loss: 0.053159795701503754, acc: 0.9789103865623474)
[2025-02-13 03:19:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:29][root][INFO] - Training Epoch: 1/2, step 5630/7134 completed (loss: 0.06306132674217224, acc: 0.977412760257721)
[2025-02-13 03:19:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:30][root][INFO] - Training Epoch: 1/2, step 5631/7134 completed (loss: 0.07081376016139984, acc: 0.9817578792572021)
[2025-02-13 03:19:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:30][root][INFO] - Training Epoch: 1/2, step 5632/7134 completed (loss: 0.03074369765818119, acc: 0.9927404522895813)
[2025-02-13 03:19:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:30][root][INFO] - Training Epoch: 1/2, step 5633/7134 completed (loss: 0.036391790956258774, acc: 0.990227997303009)
[2025-02-13 03:19:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:31][root][INFO] - Training Epoch: 1/2, step 5634/7134 completed (loss: 0.04915468767285347, acc: 0.9895287752151489)
[2025-02-13 03:19:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:31][root][INFO] - Training Epoch: 1/2, step 5635/7134 completed (loss: 0.014286564663052559, acc: 0.9949324131011963)
[2025-02-13 03:19:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:32][root][INFO] - Training Epoch: 1/2, step 5636/7134 completed (loss: 0.03084402158856392, acc: 0.9920127987861633)
[2025-02-13 03:19:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:32][root][INFO] - Training Epoch: 1/2, step 5637/7134 completed (loss: 0.034661032259464264, acc: 0.9932546615600586)
[2025-02-13 03:19:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:32][root][INFO] - Training Epoch: 1/2, step 5638/7134 completed (loss: 0.04122317209839821, acc: 0.98828125)
[2025-02-13 03:19:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:33][root][INFO] - Training Epoch: 1/2, step 5639/7134 completed (loss: 0.026219716295599937, acc: 0.9914529919624329)
[2025-02-13 03:19:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:33][root][INFO] - Training Epoch: 1/2, step 5640/7134 completed (loss: 0.037385523319244385, acc: 0.9902439117431641)
[2025-02-13 03:19:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:34][root][INFO] - Training Epoch: 1/2, step 5641/7134 completed (loss: 0.0696188360452652, acc: 0.9873737096786499)
[2025-02-13 03:19:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:34][root][INFO] - Training Epoch: 1/2, step 5642/7134 completed (loss: 0.06558765470981598, acc: 0.9803030490875244)
[2025-02-13 03:19:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:35][root][INFO] - Training Epoch: 1/2, step 5643/7134 completed (loss: 0.07185383141040802, acc: 0.980088472366333)
[2025-02-13 03:19:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:35][root][INFO] - Training Epoch: 1/2, step 5644/7134 completed (loss: 0.038404468446969986, acc: 0.9878048896789551)
[2025-02-13 03:19:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:35][root][INFO] - Training Epoch: 1/2, step 5645/7134 completed (loss: 0.02916465885937214, acc: 0.9911634922027588)
[2025-02-13 03:19:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:36][root][INFO] - Training Epoch: 1/2, step 5646/7134 completed (loss: 0.042999017983675, acc: 0.9824841022491455)
[2025-02-13 03:19:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:36][root][INFO] - Training Epoch: 1/2, step 5647/7134 completed (loss: 0.058501292020082474, acc: 0.9829457402229309)
[2025-02-13 03:19:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:37][root][INFO] - Training Epoch: 1/2, step 5648/7134 completed (loss: 0.08192736655473709, acc: 0.9820895791053772)
[2025-02-13 03:19:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:37][root][INFO] - Training Epoch: 1/2, step 5649/7134 completed (loss: 0.05602779611945152, acc: 0.9831804037094116)
[2025-02-13 03:19:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:37][root][INFO] - Training Epoch: 1/2, step 5650/7134 completed (loss: 0.02130686491727829, acc: 0.9931389093399048)
[2025-02-13 03:19:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:38][root][INFO] - Training Epoch: 1/2, step 5651/7134 completed (loss: 0.01901385188102722, acc: 0.9970414042472839)
[2025-02-13 03:19:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:38][root][INFO] - Training Epoch: 1/2, step 5652/7134 completed (loss: 0.03706514835357666, acc: 0.9855769276618958)
[2025-02-13 03:19:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:39][root][INFO] - Training Epoch: 1/2, step 5653/7134 completed (loss: 0.08844276517629623, acc: 0.984375)
[2025-02-13 03:19:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:39][root][INFO] - Training Epoch: 1/2, step 5654/7134 completed (loss: 0.027277937158942223, acc: 0.9924924969673157)
[2025-02-13 03:19:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:40][root][INFO] - Training Epoch: 1/2, step 5655/7134 completed (loss: 0.04433853179216385, acc: 0.9914966225624084)
[2025-02-13 03:19:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:40][root][INFO] - Training Epoch: 1/2, step 5656/7134 completed (loss: 0.021108515560626984, acc: 0.9923809766769409)
[2025-02-13 03:19:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:40][root][INFO] - Training Epoch: 1/2, step 5657/7134 completed (loss: 0.04129215329885483, acc: 0.9941434860229492)
[2025-02-13 03:19:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:41][root][INFO] - Training Epoch: 1/2, step 5658/7134 completed (loss: 0.04093629866838455, acc: 0.9848066568374634)
[2025-02-13 03:19:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:41][root][INFO] - Training Epoch: 1/2, step 5659/7134 completed (loss: 0.05096879228949547, acc: 0.9900709390640259)
[2025-02-13 03:19:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:42][root][INFO] - Training Epoch: 1/2, step 5660/7134 completed (loss: 0.023451250046491623, acc: 0.9917355179786682)
[2025-02-13 03:19:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:42][root][INFO] - Training Epoch: 1/2, step 5661/7134 completed (loss: 0.012341521680355072, acc: 0.9981752038002014)
[2025-02-13 03:19:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:43][root][INFO] - Training Epoch: 1/2, step 5662/7134 completed (loss: 0.02780354954302311, acc: 0.995192289352417)
[2025-02-13 03:19:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:43][root][INFO] - Training Epoch: 1/2, step 5663/7134 completed (loss: 0.012937850318849087, acc: 0.9969183206558228)
[2025-02-13 03:19:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:43][root][INFO] - Training Epoch: 1/2, step 5664/7134 completed (loss: 0.06292199343442917, acc: 0.9866270422935486)
[2025-02-13 03:19:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:44][root][INFO] - Training Epoch: 1/2, step 5665/7134 completed (loss: 0.0253825131803751, acc: 0.9914407730102539)
[2025-02-13 03:19:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:44][root][INFO] - Training Epoch: 1/2, step 5666/7134 completed (loss: 0.01001072023063898, acc: 0.9986206889152527)
[2025-02-13 03:19:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:45][root][INFO] - Training Epoch: 1/2, step 5667/7134 completed (loss: 0.024928685277700424, acc: 0.9945454597473145)
[2025-02-13 03:19:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:45][root][INFO] - Training Epoch: 1/2, step 5668/7134 completed (loss: 0.04267708584666252, acc: 0.9900332093238831)
[2025-02-13 03:19:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:45][root][INFO] - Training Epoch: 1/2, step 5669/7134 completed (loss: 0.025620492175221443, acc: 0.9897360801696777)
[2025-02-13 03:19:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:46][root][INFO] - Training Epoch: 1/2, step 5670/7134 completed (loss: 0.02244129590690136, acc: 0.9950000047683716)
[2025-02-13 03:19:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:46][root][INFO] - Training Epoch: 1/2, step 5671/7134 completed (loss: 0.020359564572572708, acc: 0.9950799345970154)
[2025-02-13 03:19:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:47][root][INFO] - Training Epoch: 1/2, step 5672/7134 completed (loss: 0.01635044813156128, acc: 0.9936908483505249)
[2025-02-13 03:19:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:47][root][INFO] - Training Epoch: 1/2, step 5673/7134 completed (loss: 0.014533840119838715, acc: 0.9985228776931763)
[2025-02-13 03:19:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:48][root][INFO] - Training Epoch: 1/2, step 5674/7134 completed (loss: 0.01881980150938034, acc: 0.9923547506332397)
[2025-02-13 03:19:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:48][root][INFO] - Training Epoch: 1/2, step 5675/7134 completed (loss: 0.05057844892144203, acc: 0.9900497794151306)
[2025-02-13 03:19:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:48][root][INFO] - Training Epoch: 1/2, step 5676/7134 completed (loss: 0.09341918677091599, acc: 0.976331353187561)
[2025-02-13 03:19:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:49][root][INFO] - Training Epoch: 1/2, step 5677/7134 completed (loss: 0.046002041548490524, acc: 0.9872159361839294)
[2025-02-13 03:19:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:49][root][INFO] - Training Epoch: 1/2, step 5678/7134 completed (loss: 0.03678480163216591, acc: 0.9911764860153198)
[2025-02-13 03:19:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:50][root][INFO] - Training Epoch: 1/2, step 5679/7134 completed (loss: 0.09301002323627472, acc: 0.9780380725860596)
[2025-02-13 03:19:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:50][root][INFO] - Training Epoch: 1/2, step 5680/7134 completed (loss: 0.09107857197523117, acc: 0.9687034487724304)
[2025-02-13 03:19:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:51][root][INFO] - Training Epoch: 1/2, step 5681/7134 completed (loss: 0.06468973308801651, acc: 0.9834087491035461)
[2025-02-13 03:19:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:51][root][INFO] - Training Epoch: 1/2, step 5682/7134 completed (loss: 0.04097511246800423, acc: 0.9841269850730896)
[2025-02-13 03:19:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:52][root][INFO] - Training Epoch: 1/2, step 5683/7134 completed (loss: 0.07524890452623367, acc: 0.9707174301147461)
[2025-02-13 03:19:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:52][root][INFO] - Training Epoch: 1/2, step 5684/7134 completed (loss: 0.06511148810386658, acc: 0.9865092635154724)
[2025-02-13 03:19:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:52][root][INFO] - Training Epoch: 1/2, step 5685/7134 completed (loss: 0.05669529363512993, acc: 0.9816176295280457)
[2025-02-13 03:19:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:53][root][INFO] - Training Epoch: 1/2, step 5686/7134 completed (loss: 0.07088826596736908, acc: 0.9780033826828003)
[2025-02-13 03:19:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:53][root][INFO] - Training Epoch: 1/2, step 5687/7134 completed (loss: 0.06721525639295578, acc: 0.9780219793319702)
[2025-02-13 03:19:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:54][root][INFO] - Training Epoch: 1/2, step 5688/7134 completed (loss: 0.06954894214868546, acc: 0.9773030877113342)
[2025-02-13 03:19:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:54][root][INFO] - Training Epoch: 1/2, step 5689/7134 completed (loss: 0.06998647749423981, acc: 0.9847792983055115)
[2025-02-13 03:19:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:55][root][INFO] - Training Epoch: 1/2, step 5690/7134 completed (loss: 0.029626132920384407, acc: 0.9917452931404114)
[2025-02-13 03:19:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:55][root][INFO] - Training Epoch: 1/2, step 5691/7134 completed (loss: 0.05236735939979553, acc: 0.9863353967666626)
[2025-02-13 03:19:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:55][root][INFO] - Training Epoch: 1/2, step 5692/7134 completed (loss: 0.05721638724207878, acc: 0.9879356622695923)
[2025-02-13 03:19:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:56][root][INFO] - Training Epoch: 1/2, step 5693/7134 completed (loss: 0.01373619306832552, acc: 0.995502233505249)
[2025-02-13 03:19:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:56][root][INFO] - Training Epoch: 1/2, step 5694/7134 completed (loss: 0.04633696749806404, acc: 0.9913169145584106)
[2025-02-13 03:19:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:57][root][INFO] - Training Epoch: 1/2, step 5695/7134 completed (loss: 0.018849337473511696, acc: 0.9928366541862488)
[2025-02-13 03:19:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:57][root][INFO] - Training Epoch: 1/2, step 5696/7134 completed (loss: 0.025485623627901077, acc: 0.9941176176071167)
[2025-02-13 03:19:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:58][root][INFO] - Training Epoch: 1/2, step 5697/7134 completed (loss: 0.046982329338788986, acc: 0.9915373921394348)
[2025-02-13 03:19:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:58][root][INFO] - Training Epoch: 1/2, step 5698/7134 completed (loss: 0.03779709339141846, acc: 0.9904761910438538)
[2025-02-13 03:19:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:59][root][INFO] - Training Epoch: 1/2, step 5699/7134 completed (loss: 0.04802507162094116, acc: 0.9834395051002502)
[2025-02-13 03:19:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:59][root][INFO] - Training Epoch: 1/2, step 5700/7134 completed (loss: 0.026773490011692047, acc: 0.9900709390640259)
[2025-02-13 03:19:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:19:59][root][INFO] - Training Epoch: 1/2, step 5701/7134 completed (loss: 0.04035105183720589, acc: 0.9949937462806702)
[2025-02-13 03:20:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:00][root][INFO] - Training Epoch: 1/2, step 5702/7134 completed (loss: 0.04139764979481697, acc: 0.9918808937072754)
[2025-02-13 03:20:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:00][root][INFO] - Training Epoch: 1/2, step 5703/7134 completed (loss: 0.019332481548190117, acc: 0.9957805871963501)
[2025-02-13 03:20:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:01][root][INFO] - Training Epoch: 1/2, step 5704/7134 completed (loss: 0.050971534103155136, acc: 0.9857346415519714)
[2025-02-13 03:20:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:01][root][INFO] - Training Epoch: 1/2, step 5705/7134 completed (loss: 0.02008008025586605, acc: 0.9892857074737549)
[2025-02-13 03:20:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:02][root][INFO] - Training Epoch: 1/2, step 5706/7134 completed (loss: 0.016929568722844124, acc: 0.9977628588676453)
[2025-02-13 03:20:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:02][root][INFO] - Training Epoch: 1/2, step 5707/7134 completed (loss: 0.03435061126947403, acc: 0.9911971688270569)
[2025-02-13 03:20:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:03][root][INFO] - Training Epoch: 1/2, step 5708/7134 completed (loss: 0.025778979063034058, acc: 0.9940740466117859)
[2025-02-13 03:20:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:03][root][INFO] - Training Epoch: 1/2, step 5709/7134 completed (loss: 0.04421425610780716, acc: 0.9917920827865601)
[2025-02-13 03:20:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:03][root][INFO] - Training Epoch: 1/2, step 5710/7134 completed (loss: 0.040046535432338715, acc: 0.9860050678253174)
[2025-02-13 03:20:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:04][root][INFO] - Training Epoch: 1/2, step 5711/7134 completed (loss: 0.05167222023010254, acc: 0.985602080821991)
[2025-02-13 03:20:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:04][root][INFO] - Training Epoch: 1/2, step 5712/7134 completed (loss: 0.058186836540699005, acc: 0.9800000190734863)
[2025-02-13 03:20:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:05][root][INFO] - Training Epoch: 1/2, step 5713/7134 completed (loss: 0.012624560855329037, acc: 0.9974586963653564)
[2025-02-13 03:20:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:05][root][INFO] - Training Epoch: 1/2, step 5714/7134 completed (loss: 0.02543570287525654, acc: 0.9950186610221863)
[2025-02-13 03:20:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:06][root][INFO] - Training Epoch: 1/2, step 5715/7134 completed (loss: 0.06694908440113068, acc: 0.9869646430015564)
[2025-02-13 03:20:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:06][root][INFO] - Training Epoch: 1/2, step 5716/7134 completed (loss: 0.10711643099784851, acc: 0.980461835861206)
[2025-02-13 03:20:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:06][root][INFO] - Training Epoch: 1/2, step 5717/7134 completed (loss: 0.10553450882434845, acc: 0.9782270789146423)
[2025-02-13 03:20:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:07][root][INFO] - Training Epoch: 1/2, step 5718/7134 completed (loss: 0.05520736426115036, acc: 0.9873816967010498)
[2025-02-13 03:20:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:07][root][INFO] - Training Epoch: 1/2, step 5719/7134 completed (loss: 0.039844948798418045, acc: 0.9925037622451782)
[2025-02-13 03:20:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:08][root][INFO] - Training Epoch: 1/2, step 5720/7134 completed (loss: 0.03360038995742798, acc: 0.9896373152732849)
[2025-02-13 03:20:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:08][root][INFO] - Training Epoch: 1/2, step 5721/7134 completed (loss: 0.04310784116387367, acc: 0.9868420958518982)
[2025-02-13 03:20:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:08][root][INFO] - Training Epoch: 1/2, step 5722/7134 completed (loss: 0.057418011128902435, acc: 0.9911308288574219)
[2025-02-13 03:20:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:09][root][INFO] - Training Epoch: 1/2, step 5723/7134 completed (loss: 0.03986877575516701, acc: 0.9909228682518005)
[2025-02-13 03:20:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:09][root][INFO] - Training Epoch: 1/2, step 5724/7134 completed (loss: 0.0280505008995533, acc: 0.9919999837875366)
[2025-02-13 03:20:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:10][root][INFO] - Training Epoch: 1/2, step 5725/7134 completed (loss: 0.025902383029460907, acc: 0.9942857027053833)
[2025-02-13 03:20:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:10][root][INFO] - Training Epoch: 1/2, step 5726/7134 completed (loss: 0.060150690376758575, acc: 0.9877675771713257)
[2025-02-13 03:20:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:11][root][INFO] - Training Epoch: 1/2, step 5727/7134 completed (loss: 0.017707528546452522, acc: 0.9946523904800415)
[2025-02-13 03:20:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:11][root][INFO] - Training Epoch: 1/2, step 5728/7134 completed (loss: 0.014268984086811543, acc: 0.9982455968856812)
[2025-02-13 03:20:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:11][root][INFO] - Training Epoch: 1/2, step 5729/7134 completed (loss: 0.018115976825356483, acc: 0.9950000047683716)
[2025-02-13 03:20:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:12][root][INFO] - Training Epoch: 1/2, step 5730/7134 completed (loss: 0.07380437105894089, acc: 0.9818181991577148)
[2025-02-13 03:20:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:12][root][INFO] - Training Epoch: 1/2, step 5731/7134 completed (loss: 0.04315349459648132, acc: 0.9883720874786377)
[2025-02-13 03:20:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:13][root][INFO] - Training Epoch: 1/2, step 5732/7134 completed (loss: 0.1334073394536972, acc: 0.9611650705337524)
[2025-02-13 03:20:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:13][root][INFO] - Training Epoch: 1/2, step 5733/7134 completed (loss: 0.0490599200129509, acc: 0.9854280352592468)
[2025-02-13 03:20:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:13][root][INFO] - Training Epoch: 1/2, step 5734/7134 completed (loss: 0.010050108656287193, acc: 0.996666669845581)
[2025-02-13 03:20:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:14][root][INFO] - Training Epoch: 1/2, step 5735/7134 completed (loss: 0.051930543035268784, acc: 0.9856630563735962)
[2025-02-13 03:20:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:14][root][INFO] - Training Epoch: 1/2, step 5736/7134 completed (loss: 0.060985106974840164, acc: 0.9819548726081848)
[2025-02-13 03:20:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:15][root][INFO] - Training Epoch: 1/2, step 5737/7134 completed (loss: 0.058010976761579514, acc: 0.9803625345230103)
[2025-02-13 03:20:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:15][root][INFO] - Training Epoch: 1/2, step 5738/7134 completed (loss: 0.04413558542728424, acc: 0.9818621277809143)
[2025-02-13 03:20:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:16][root][INFO] - Training Epoch: 1/2, step 5739/7134 completed (loss: 0.05329437181353569, acc: 0.9873684048652649)
[2025-02-13 03:20:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:16][root][INFO] - Training Epoch: 1/2, step 5740/7134 completed (loss: 0.031015420332551003, acc: 0.9923273921012878)
[2025-02-13 03:20:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:17][root][INFO] - Training Epoch: 1/2, step 5741/7134 completed (loss: 0.040272828191518784, acc: 0.9910141229629517)
[2025-02-13 03:20:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:17][root][INFO] - Training Epoch: 1/2, step 5742/7134 completed (loss: 0.04109721630811691, acc: 0.9881734848022461)
[2025-02-13 03:20:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:17][root][INFO] - Training Epoch: 1/2, step 5743/7134 completed (loss: 0.029373396188020706, acc: 0.9901960492134094)
[2025-02-13 03:20:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:18][root][INFO] - Training Epoch: 1/2, step 5744/7134 completed (loss: 0.015606778673827648, acc: 0.9958158731460571)
[2025-02-13 03:20:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:18][root][INFO] - Training Epoch: 1/2, step 5745/7134 completed (loss: 0.03804965689778328, acc: 0.9870466589927673)
[2025-02-13 03:20:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:19][root][INFO] - Training Epoch: 1/2, step 5746/7134 completed (loss: 0.0662359893321991, acc: 0.9882352948188782)
[2025-02-13 03:20:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:19][root][INFO] - Training Epoch: 1/2, step 5747/7134 completed (loss: 0.03264991566538811, acc: 0.9889196753501892)
[2025-02-13 03:20:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:19][root][INFO] - Training Epoch: 1/2, step 5748/7134 completed (loss: 0.025327034294605255, acc: 0.9926874041557312)
[2025-02-13 03:20:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:20][root][INFO] - Training Epoch: 1/2, step 5749/7134 completed (loss: 0.030838297680020332, acc: 0.9921466112136841)
[2025-02-13 03:20:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:20][root][INFO] - Training Epoch: 1/2, step 5750/7134 completed (loss: 0.041333019733428955, acc: 0.9851484894752502)
[2025-02-13 03:20:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:21][root][INFO] - Training Epoch: 1/2, step 5751/7134 completed (loss: 0.04112422838807106, acc: 0.9910141229629517)
[2025-02-13 03:20:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:21][root][INFO] - Training Epoch: 1/2, step 5752/7134 completed (loss: 0.09225195646286011, acc: 0.9744779467582703)
[2025-02-13 03:20:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:22][root][INFO] - Training Epoch: 1/2, step 5753/7134 completed (loss: 0.07941220700740814, acc: 0.9765625)
[2025-02-13 03:20:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:22][root][INFO] - Training Epoch: 1/2, step 5754/7134 completed (loss: 0.06966093927621841, acc: 0.9821656346321106)
[2025-02-13 03:20:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:22][root][INFO] - Training Epoch: 1/2, step 5755/7134 completed (loss: 0.046752046793699265, acc: 0.980966329574585)
[2025-02-13 03:20:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:23][root][INFO] - Training Epoch: 1/2, step 5756/7134 completed (loss: 0.051521554589271545, acc: 0.9840425252914429)
[2025-02-13 03:20:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:23][root][INFO] - Training Epoch: 1/2, step 5757/7134 completed (loss: 0.0308542363345623, acc: 0.9910600185394287)
[2025-02-13 03:20:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:24][root][INFO] - Training Epoch: 1/2, step 5758/7134 completed (loss: 0.04043366014957428, acc: 0.9808823466300964)
[2025-02-13 03:20:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:24][root][INFO] - Training Epoch: 1/2, step 5759/7134 completed (loss: 0.046112172305583954, acc: 0.9811320900917053)
[2025-02-13 03:20:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:25][root][INFO] - Training Epoch: 1/2, step 5760/7134 completed (loss: 0.032010264694690704, acc: 0.989313006401062)
[2025-02-13 03:20:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:25][root][INFO] - Training Epoch: 1/2, step 5761/7134 completed (loss: 0.04367266222834587, acc: 0.9871794581413269)
[2025-02-13 03:20:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:25][root][INFO] - Training Epoch: 1/2, step 5762/7134 completed (loss: 0.07574228942394257, acc: 0.9781022071838379)
[2025-02-13 03:20:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:26][root][INFO] - Training Epoch: 1/2, step 5763/7134 completed (loss: 0.04269673302769661, acc: 0.9912170767784119)
[2025-02-13 03:20:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:26][root][INFO] - Training Epoch: 1/2, step 5764/7134 completed (loss: 0.02561775967478752, acc: 0.9897435903549194)
[2025-02-13 03:20:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:27][root][INFO] - Training Epoch: 1/2, step 5765/7134 completed (loss: 0.016637230291962624, acc: 0.994397759437561)
[2025-02-13 03:20:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:27][root][INFO] - Training Epoch: 1/2, step 5766/7134 completed (loss: 0.05247124657034874, acc: 0.9910314083099365)
[2025-02-13 03:20:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:27][root][INFO] - Training Epoch: 1/2, step 5767/7134 completed (loss: 0.011282216757535934, acc: 0.9944367408752441)
[2025-02-13 03:20:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:28][root][INFO] - Training Epoch: 1/2, step 5768/7134 completed (loss: 0.015912538394331932, acc: 0.9955089688301086)
[2025-02-13 03:20:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:28][root][INFO] - Training Epoch: 1/2, step 5769/7134 completed (loss: 0.005848555359989405, acc: 0.9983633160591125)
[2025-02-13 03:20:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:29][root][INFO] - Training Epoch: 1/2, step 5770/7134 completed (loss: 0.04797090217471123, acc: 0.990208089351654)
[2025-02-13 03:20:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:29][root][INFO] - Training Epoch: 1/2, step 5771/7134 completed (loss: 0.04314790293574333, acc: 0.991847813129425)
[2025-02-13 03:20:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:30][root][INFO] - Training Epoch: 1/2, step 5772/7134 completed (loss: 0.020783359184861183, acc: 0.9930434823036194)
[2025-02-13 03:20:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:30][root][INFO] - Training Epoch: 1/2, step 5773/7134 completed (loss: 0.015409034676849842, acc: 0.9966555237770081)
[2025-02-13 03:20:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:31][root][INFO] - Training Epoch: 1/2, step 5774/7134 completed (loss: 0.016124533489346504, acc: 0.9955489635467529)
[2025-02-13 03:20:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:31][root][INFO] - Training Epoch: 1/2, step 5775/7134 completed (loss: 0.018264101818203926, acc: 0.9934102296829224)
[2025-02-13 03:20:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:31][root][INFO] - Training Epoch: 1/2, step 5776/7134 completed (loss: 0.04636935517191887, acc: 0.9901685118675232)
[2025-02-13 03:20:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:32][root][INFO] - Training Epoch: 1/2, step 5777/7134 completed (loss: 0.052865706384181976, acc: 0.9876237511634827)
[2025-02-13 03:20:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:32][root][INFO] - Training Epoch: 1/2, step 5778/7134 completed (loss: 0.08496659249067307, acc: 0.9759615659713745)
[2025-02-13 03:20:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:33][root][INFO] - Training Epoch: 1/2, step 5779/7134 completed (loss: 0.04966680333018303, acc: 0.9876203536987305)
[2025-02-13 03:20:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:33][root][INFO] - Training Epoch: 1/2, step 5780/7134 completed (loss: 0.04306740313768387, acc: 0.9880668520927429)
[2025-02-13 03:20:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:34][root][INFO] - Training Epoch: 1/2, step 5781/7134 completed (loss: 0.03623367100954056, acc: 0.9875690340995789)
[2025-02-13 03:20:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:34][root][INFO] - Training Epoch: 1/2, step 5782/7134 completed (loss: 0.021043337881565094, acc: 0.9948186278343201)
[2025-02-13 03:20:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:34][root][INFO] - Training Epoch: 1/2, step 5783/7134 completed (loss: 0.03361380100250244, acc: 0.9920318722724915)
[2025-02-13 03:20:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:35][root][INFO] - Training Epoch: 1/2, step 5784/7134 completed (loss: 0.05371745675802231, acc: 0.9878869652748108)
[2025-02-13 03:20:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:35][root][INFO] - Training Epoch: 1/2, step 5785/7134 completed (loss: 0.047583628445863724, acc: 0.9824798107147217)
[2025-02-13 03:20:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:36][root][INFO] - Training Epoch: 1/2, step 5786/7134 completed (loss: 0.03523118421435356, acc: 0.9897540807723999)
[2025-02-13 03:20:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:36][root][INFO] - Training Epoch: 1/2, step 5787/7134 completed (loss: 0.026364943012595177, acc: 0.9900497794151306)
[2025-02-13 03:20:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:37][root][INFO] - Training Epoch: 1/2, step 5788/7134 completed (loss: 0.01776883564889431, acc: 0.9907692074775696)
[2025-02-13 03:20:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:37][root][INFO] - Training Epoch: 1/2, step 5789/7134 completed (loss: 0.04459746927022934, acc: 0.9838709831237793)
[2025-02-13 03:20:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:37][root][INFO] - Training Epoch: 1/2, step 5790/7134 completed (loss: 0.027659598737955093, acc: 0.9876373410224915)
[2025-02-13 03:20:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:38][root][INFO] - Training Epoch: 1/2, step 5791/7134 completed (loss: 0.06148158758878708, acc: 0.9776632189750671)
[2025-02-13 03:20:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:38][root][INFO] - Training Epoch: 1/2, step 5792/7134 completed (loss: 0.07120590656995773, acc: 0.9762309193611145)
[2025-02-13 03:20:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:39][root][INFO] - Training Epoch: 1/2, step 5793/7134 completed (loss: 0.07595411688089371, acc: 0.9732704162597656)
[2025-02-13 03:20:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:39][root][INFO] - Training Epoch: 1/2, step 5794/7134 completed (loss: 0.042722202837467194, acc: 0.9848484992980957)
[2025-02-13 03:20:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:40][root][INFO] - Training Epoch: 1/2, step 5795/7134 completed (loss: 0.10035612434148788, acc: 0.970588207244873)
[2025-02-13 03:20:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:40][root][INFO] - Training Epoch: 1/2, step 5796/7134 completed (loss: 0.0268258024007082, acc: 0.9932318329811096)
[2025-02-13 03:20:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:40][root][INFO] - Training Epoch: 1/2, step 5797/7134 completed (loss: 0.030992966145277023, acc: 0.9887999892234802)
[2025-02-13 03:20:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:41][root][INFO] - Training Epoch: 1/2, step 5798/7134 completed (loss: 0.1564607173204422, acc: 0.9652174115180969)
[2025-02-13 03:20:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:41][root][INFO] - Training Epoch: 1/2, step 5799/7134 completed (loss: 0.07314778864383698, acc: 0.9802761077880859)
[2025-02-13 03:20:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:42][root][INFO] - Training Epoch: 1/2, step 5800/7134 completed (loss: 0.06302428990602493, acc: 0.9779411554336548)
[2025-02-13 03:20:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:42][root][INFO] - Training Epoch: 1/2, step 5801/7134 completed (loss: 0.053570136427879333, acc: 0.9817073345184326)
[2025-02-13 03:20:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:42][root][INFO] - Training Epoch: 1/2, step 5802/7134 completed (loss: 0.045777056366205215, acc: 0.9870610237121582)
[2025-02-13 03:20:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:43][root][INFO] - Training Epoch: 1/2, step 5803/7134 completed (loss: 0.026981888338923454, acc: 0.9936224222183228)
[2025-02-13 03:20:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:43][root][INFO] - Training Epoch: 1/2, step 5804/7134 completed (loss: 0.0739343911409378, acc: 0.9729363918304443)
[2025-02-13 03:20:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:44][root][INFO] - Training Epoch: 1/2, step 5805/7134 completed (loss: 0.05055541545152664, acc: 0.9870503544807434)
[2025-02-13 03:20:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:44][root][INFO] - Training Epoch: 1/2, step 5806/7134 completed (loss: 0.0327531062066555, acc: 0.9918032884597778)
[2025-02-13 03:20:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:45][root][INFO] - Training Epoch: 1/2, step 5807/7134 completed (loss: 0.062128253281116486, acc: 0.9878970980644226)
[2025-02-13 03:20:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:45][root][INFO] - Training Epoch: 1/2, step 5808/7134 completed (loss: 0.030589357018470764, acc: 0.9915611743927002)
[2025-02-13 03:20:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:45][root][INFO] - Training Epoch: 1/2, step 5809/7134 completed (loss: 0.04008041322231293, acc: 0.9934210777282715)
[2025-02-13 03:20:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:46][root][INFO] - Training Epoch: 1/2, step 5810/7134 completed (loss: 0.03239598497748375, acc: 0.9878296256065369)
[2025-02-13 03:20:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:46][root][INFO] - Training Epoch: 1/2, step 5811/7134 completed (loss: 0.05245547369122505, acc: 0.9807229042053223)
[2025-02-13 03:20:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:47][root][INFO] - Training Epoch: 1/2, step 5812/7134 completed (loss: 0.013648252002894878, acc: 0.9966996908187866)
[2025-02-13 03:20:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:47][root][INFO] - Training Epoch: 1/2, step 5813/7134 completed (loss: 0.031243562698364258, acc: 0.9905660152435303)
[2025-02-13 03:20:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:47][root][INFO] - Training Epoch: 1/2, step 5814/7134 completed (loss: 0.023378917947411537, acc: 0.9915397763252258)
[2025-02-13 03:20:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:48][root][INFO] - Training Epoch: 1/2, step 5815/7134 completed (loss: 0.03138357773423195, acc: 0.9924670457839966)
[2025-02-13 03:20:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:48][root][INFO] - Training Epoch: 1/2, step 5816/7134 completed (loss: 0.005962277762591839, acc: 1.0)
[2025-02-13 03:20:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:49][root][INFO] - Training Epoch: 1/2, step 5817/7134 completed (loss: 0.05496823042631149, acc: 0.98740553855896)
[2025-02-13 03:20:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:49][root][INFO] - Training Epoch: 1/2, step 5818/7134 completed (loss: 0.03664945438504219, acc: 0.9866666793823242)
[2025-02-13 03:20:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:50][root][INFO] - Training Epoch: 1/2, step 5819/7134 completed (loss: 0.02981422282755375, acc: 0.9913793206214905)
[2025-02-13 03:20:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:50][root][INFO] - Training Epoch: 1/2, step 5820/7134 completed (loss: 0.040697697550058365, acc: 0.9855371713638306)
[2025-02-13 03:20:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:50][root][INFO] - Training Epoch: 1/2, step 5821/7134 completed (loss: 0.009882996790111065, acc: 0.9969183206558228)
[2025-02-13 03:20:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:51][root][INFO] - Training Epoch: 1/2, step 5822/7134 completed (loss: 0.0293453149497509, acc: 0.9935483932495117)
[2025-02-13 03:20:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:51][root][INFO] - Training Epoch: 1/2, step 5823/7134 completed (loss: 0.034785669296979904, acc: 0.9875583052635193)
[2025-02-13 03:20:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:52][root][INFO] - Training Epoch: 1/2, step 5824/7134 completed (loss: 0.024838261306285858, acc: 0.996889591217041)
[2025-02-13 03:20:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:52][root][INFO] - Training Epoch: 1/2, step 5825/7134 completed (loss: 0.03354952856898308, acc: 0.9896193742752075)
[2025-02-13 03:20:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:52][root][INFO] - Training Epoch: 1/2, step 5826/7134 completed (loss: 0.009829437360167503, acc: 0.9982269406318665)
[2025-02-13 03:20:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:53][root][INFO] - Training Epoch: 1/2, step 5827/7134 completed (loss: 0.021483279764652252, acc: 0.9942307472229004)
[2025-02-13 03:20:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:53][root][INFO] - Training Epoch: 1/2, step 5828/7134 completed (loss: 0.035375967621803284, acc: 0.9881129264831543)
[2025-02-13 03:20:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:54][root][INFO] - Training Epoch: 1/2, step 5829/7134 completed (loss: 0.033589042723178864, acc: 0.9934123754501343)
[2025-02-13 03:20:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:54][root][INFO] - Training Epoch: 1/2, step 5830/7134 completed (loss: 0.030959477648139, acc: 0.9886578321456909)
[2025-02-13 03:20:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:54][root][INFO] - Training Epoch: 1/2, step 5831/7134 completed (loss: 0.013773501850664616, acc: 0.9976744055747986)
[2025-02-13 03:20:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:55][root][INFO] - Training Epoch: 1/2, step 5832/7134 completed (loss: 0.04405553266406059, acc: 0.9898648858070374)
[2025-02-13 03:20:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:55][root][INFO] - Training Epoch: 1/2, step 5833/7134 completed (loss: 0.029759181663393974, acc: 0.9910394549369812)
[2025-02-13 03:20:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:56][root][INFO] - Training Epoch: 1/2, step 5834/7134 completed (loss: 0.029239900410175323, acc: 0.9944444298744202)
[2025-02-13 03:20:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:56][root][INFO] - Training Epoch: 1/2, step 5835/7134 completed (loss: 0.009737692773342133, acc: 0.9971098303794861)
[2025-02-13 03:20:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:56][root][INFO] - Training Epoch: 1/2, step 5836/7134 completed (loss: 0.023783518001437187, acc: 0.9927954077720642)
[2025-02-13 03:20:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:57][root][INFO] - Training Epoch: 1/2, step 5837/7134 completed (loss: 0.051058899611234665, acc: 0.9915540814399719)
[2025-02-13 03:20:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:57][root][INFO] - Training Epoch: 1/2, step 5838/7134 completed (loss: 0.034941572695970535, acc: 0.9950576424598694)
[2025-02-13 03:20:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:58][root][INFO] - Training Epoch: 1/2, step 5839/7134 completed (loss: 0.02586059831082821, acc: 0.9910714030265808)
[2025-02-13 03:20:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:58][root][INFO] - Training Epoch: 1/2, step 5840/7134 completed (loss: 0.016239725053310394, acc: 0.995230495929718)
[2025-02-13 03:20:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:59][root][INFO] - Training Epoch: 1/2, step 5841/7134 completed (loss: 0.026496825739741325, acc: 0.9952152967453003)
[2025-02-13 03:20:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:59][root][INFO] - Training Epoch: 1/2, step 5842/7134 completed (loss: 0.024577155709266663, acc: 0.9927536249160767)
[2025-02-13 03:20:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:20:59][root][INFO] - Training Epoch: 1/2, step 5843/7134 completed (loss: 0.038935478776693344, acc: 0.9919678568840027)
[2025-02-13 03:21:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:00][root][INFO] - Training Epoch: 1/2, step 5844/7134 completed (loss: 0.021220624446868896, acc: 0.9957325458526611)
[2025-02-13 03:21:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:00][root][INFO] - Training Epoch: 1/2, step 5845/7134 completed (loss: 0.035265665501356125, acc: 0.9886040091514587)
[2025-02-13 03:21:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:01][root][INFO] - Training Epoch: 1/2, step 5846/7134 completed (loss: 0.033640772104263306, acc: 0.9904240965843201)
[2025-02-13 03:21:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:01][root][INFO] - Training Epoch: 1/2, step 5847/7134 completed (loss: 0.01854315772652626, acc: 0.9931318759918213)
[2025-02-13 03:21:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:02][root][INFO] - Training Epoch: 1/2, step 5848/7134 completed (loss: 0.04126882553100586, acc: 0.9895833134651184)
[2025-02-13 03:21:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:02][root][INFO] - Training Epoch: 1/2, step 5849/7134 completed (loss: 0.05427124351263046, acc: 0.982300877571106)
[2025-02-13 03:21:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:02][root][INFO] - Training Epoch: 1/2, step 5850/7134 completed (loss: 0.014139601029455662, acc: 0.9938931465148926)
[2025-02-13 03:21:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:03][root][INFO] - Training Epoch: 1/2, step 5851/7134 completed (loss: 0.028307989239692688, acc: 0.9901315569877625)
[2025-02-13 03:21:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:03][root][INFO] - Training Epoch: 1/2, step 5852/7134 completed (loss: 0.019863178953528404, acc: 0.9897810220718384)
[2025-02-13 03:21:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:04][root][INFO] - Training Epoch: 1/2, step 5853/7134 completed (loss: 0.0511956587433815, acc: 0.9918808937072754)
[2025-02-13 03:21:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:04][root][INFO] - Training Epoch: 1/2, step 5854/7134 completed (loss: 0.015470581129193306, acc: 0.9946902394294739)
[2025-02-13 03:21:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:04][root][INFO] - Training Epoch: 1/2, step 5855/7134 completed (loss: 0.05184558406472206, acc: 0.9879102110862732)
[2025-02-13 03:21:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:05][root][INFO] - Training Epoch: 1/2, step 5856/7134 completed (loss: 0.049616821110248566, acc: 0.9909365773200989)
[2025-02-13 03:21:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:05][root][INFO] - Training Epoch: 1/2, step 5857/7134 completed (loss: 0.03807768225669861, acc: 0.9873617887496948)
[2025-02-13 03:21:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:06][root][INFO] - Training Epoch: 1/2, step 5858/7134 completed (loss: 0.018514832481741905, acc: 0.9981516003608704)
[2025-02-13 03:21:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:06][root][INFO] - Training Epoch: 1/2, step 5859/7134 completed (loss: 0.050992514938116074, acc: 0.98828125)
[2025-02-13 03:21:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:07][root][INFO] - Training Epoch: 1/2, step 5860/7134 completed (loss: 0.06316805630922318, acc: 0.9834710955619812)
[2025-02-13 03:21:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:07][root][INFO] - Training Epoch: 1/2, step 5861/7134 completed (loss: 0.046786386519670486, acc: 0.9878296256065369)
[2025-02-13 03:21:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:07][root][INFO] - Training Epoch: 1/2, step 5862/7134 completed (loss: 0.029308756813406944, acc: 0.991416335105896)
[2025-02-13 03:21:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:08][root][INFO] - Training Epoch: 1/2, step 5863/7134 completed (loss: 0.036087892949581146, acc: 0.9949495196342468)
[2025-02-13 03:21:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:08][root][INFO] - Training Epoch: 1/2, step 5864/7134 completed (loss: 0.050208330154418945, acc: 0.985029935836792)
[2025-02-13 03:21:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:09][root][INFO] - Training Epoch: 1/2, step 5865/7134 completed (loss: 0.060962118208408356, acc: 0.9857142567634583)
[2025-02-13 03:21:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:09][root][INFO] - Training Epoch: 1/2, step 5866/7134 completed (loss: 0.014958968386054039, acc: 0.9947090148925781)
[2025-02-13 03:21:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:09][root][INFO] - Training Epoch: 1/2, step 5867/7134 completed (loss: 0.011997606605291367, acc: 0.9966942071914673)
[2025-02-13 03:21:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:10][root][INFO] - Training Epoch: 1/2, step 5868/7134 completed (loss: 0.04439883679151535, acc: 0.9879310131072998)
[2025-02-13 03:21:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:10][root][INFO] - Training Epoch: 1/2, step 5869/7134 completed (loss: 0.03847581893205643, acc: 0.9880596995353699)
[2025-02-13 03:21:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:11][root][INFO] - Training Epoch: 1/2, step 5870/7134 completed (loss: 0.058291245251894, acc: 0.9874476790428162)
[2025-02-13 03:21:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:11][root][INFO] - Training Epoch: 1/2, step 5871/7134 completed (loss: 0.022536803036928177, acc: 0.9957537055015564)
[2025-02-13 03:21:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:12][root][INFO] - Training Epoch: 1/2, step 5872/7134 completed (loss: 0.027078120037913322, acc: 0.9923896789550781)
[2025-02-13 03:21:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:12][root][INFO] - Training Epoch: 1/2, step 5873/7134 completed (loss: 0.03309720754623413, acc: 0.9893333315849304)
[2025-02-13 03:21:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:12][root][INFO] - Training Epoch: 1/2, step 5874/7134 completed (loss: 0.017891855910420418, acc: 0.9939758777618408)
[2025-02-13 03:21:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:13][root][INFO] - Training Epoch: 1/2, step 5875/7134 completed (loss: 0.03633252531290054, acc: 0.991150438785553)
[2025-02-13 03:21:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:13][root][INFO] - Training Epoch: 1/2, step 5876/7134 completed (loss: 0.011212249286472797, acc: 0.9951456189155579)
[2025-02-13 03:21:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:14][root][INFO] - Training Epoch: 1/2, step 5877/7134 completed (loss: 0.051262762397527695, acc: 0.989847719669342)
[2025-02-13 03:21:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:14][root][INFO] - Training Epoch: 1/2, step 5878/7134 completed (loss: 0.017442641779780388, acc: 0.9956140518188477)
[2025-02-13 03:21:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:14][root][INFO] - Training Epoch: 1/2, step 5879/7134 completed (loss: 0.013366328552365303, acc: 1.0)
[2025-02-13 03:21:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:15][root][INFO] - Training Epoch: 1/2, step 5880/7134 completed (loss: 0.029284551739692688, acc: 0.9911110997200012)
[2025-02-13 03:21:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:15][root][INFO] - Training Epoch: 1/2, step 5881/7134 completed (loss: 0.01421570684760809, acc: 0.9923224449157715)
[2025-02-13 03:21:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:16][root][INFO] - Training Epoch: 1/2, step 5882/7134 completed (loss: 0.01954619027674198, acc: 0.9952940940856934)
[2025-02-13 03:21:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:16][root][INFO] - Training Epoch: 1/2, step 5883/7134 completed (loss: 0.06679195165634155, acc: 0.9868913888931274)
[2025-02-13 03:21:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:16][root][INFO] - Training Epoch: 1/2, step 5884/7134 completed (loss: 0.06140275299549103, acc: 0.9781022071838379)
[2025-02-13 03:21:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:17][root][INFO] - Training Epoch: 1/2, step 5885/7134 completed (loss: 0.09362747520208359, acc: 0.97826087474823)
[2025-02-13 03:21:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:17][root][INFO] - Training Epoch: 1/2, step 5886/7134 completed (loss: 0.02788429521024227, acc: 0.991428554058075)
[2025-02-13 03:21:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:18][root][INFO] - Training Epoch: 1/2, step 5887/7134 completed (loss: 0.03383156657218933, acc: 0.993630588054657)
[2025-02-13 03:21:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:18][root][INFO] - Training Epoch: 1/2, step 5888/7134 completed (loss: 0.03862487152218819, acc: 0.9898648858070374)
[2025-02-13 03:21:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:19][root][INFO] - Training Epoch: 1/2, step 5889/7134 completed (loss: 0.009697084315121174, acc: 1.0)
[2025-02-13 03:21:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:19][root][INFO] - Training Epoch: 1/2, step 5890/7134 completed (loss: 0.02077292650938034, acc: 0.9947552680969238)
[2025-02-13 03:21:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:19][root][INFO] - Training Epoch: 1/2, step 5891/7134 completed (loss: 0.03657166287302971, acc: 0.9871086478233337)
[2025-02-13 03:21:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:20][root][INFO] - Training Epoch: 1/2, step 5892/7134 completed (loss: 0.03851735219359398, acc: 0.991253674030304)
[2025-02-13 03:21:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:20][root][INFO] - Training Epoch: 1/2, step 5893/7134 completed (loss: 0.04028264805674553, acc: 0.98740154504776)
[2025-02-13 03:21:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:21][root][INFO] - Training Epoch: 1/2, step 5894/7134 completed (loss: 0.008126732893288136, acc: 1.0)
[2025-02-13 03:21:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:21][root][INFO] - Training Epoch: 1/2, step 5895/7134 completed (loss: 0.017573099583387375, acc: 0.9952380657196045)
[2025-02-13 03:21:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:22][root][INFO] - Training Epoch: 1/2, step 5896/7134 completed (loss: 0.04207306355237961, acc: 0.988095223903656)
[2025-02-13 03:21:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:22][root][INFO] - Training Epoch: 1/2, step 5897/7134 completed (loss: 0.019229386001825333, acc: 0.9931129217147827)
[2025-02-13 03:21:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:22][root][INFO] - Training Epoch: 1/2, step 5898/7134 completed (loss: 0.013088046573102474, acc: 0.9956896305084229)
[2025-02-13 03:21:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:23][root][INFO] - Training Epoch: 1/2, step 5899/7134 completed (loss: 0.010416042990982533, acc: 1.0)
[2025-02-13 03:21:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:23][root][INFO] - Training Epoch: 1/2, step 5900/7134 completed (loss: 0.020147280767560005, acc: 0.9940476417541504)
[2025-02-13 03:21:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:24][root][INFO] - Training Epoch: 1/2, step 5901/7134 completed (loss: 0.045567456632852554, acc: 0.987500011920929)
[2025-02-13 03:21:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:24][root][INFO] - Training Epoch: 1/2, step 5902/7134 completed (loss: 0.07146266847848892, acc: 0.9819587469100952)
[2025-02-13 03:21:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:25][root][INFO] - Training Epoch: 1/2, step 5903/7134 completed (loss: 0.045416589826345444, acc: 0.989051103591919)
[2025-02-13 03:21:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:25][root][INFO] - Training Epoch: 1/2, step 5904/7134 completed (loss: 0.03431563079357147, acc: 0.9890710115432739)
[2025-02-13 03:21:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:25][root][INFO] - Training Epoch: 1/2, step 5905/7134 completed (loss: 0.041571199893951416, acc: 0.9860917925834656)
[2025-02-13 03:21:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:26][root][INFO] - Training Epoch: 1/2, step 5906/7134 completed (loss: 0.05231555923819542, acc: 0.9840294718742371)
[2025-02-13 03:21:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:26][root][INFO] - Training Epoch: 1/2, step 5907/7134 completed (loss: 0.06713362783193588, acc: 0.9844760894775391)
[2025-02-13 03:21:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:27][root][INFO] - Training Epoch: 1/2, step 5908/7134 completed (loss: 0.030112145468592644, acc: 0.9910485744476318)
[2025-02-13 03:21:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:27][root][INFO] - Training Epoch: 1/2, step 5909/7134 completed (loss: 0.037543486803770065, acc: 0.9884726405143738)
[2025-02-13 03:21:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:28][root][INFO] - Training Epoch: 1/2, step 5910/7134 completed (loss: 0.054915957152843475, acc: 0.9833564758300781)
[2025-02-13 03:21:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:28][root][INFO] - Training Epoch: 1/2, step 5911/7134 completed (loss: 0.044596366584300995, acc: 0.9900826215744019)
[2025-02-13 03:21:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:29][root][INFO] - Training Epoch: 1/2, step 5912/7134 completed (loss: 0.026910878717899323, acc: 0.9943740963935852)
[2025-02-13 03:21:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:29][root][INFO] - Training Epoch: 1/2, step 5913/7134 completed (loss: 0.09745745360851288, acc: 0.9755434989929199)
[2025-02-13 03:21:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:30][root][INFO] - Training Epoch: 1/2, step 5914/7134 completed (loss: 0.021625764667987823, acc: 0.9935794472694397)
[2025-02-13 03:21:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:30][root][INFO] - Training Epoch: 1/2, step 5915/7134 completed (loss: 0.04551699012517929, acc: 0.987596869468689)
[2025-02-13 03:21:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:31][root][INFO] - Training Epoch: 1/2, step 5916/7134 completed (loss: 0.04302249103784561, acc: 0.9882044792175293)
[2025-02-13 03:21:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:31][root][INFO] - Training Epoch: 1/2, step 5917/7134 completed (loss: 0.03318747878074646, acc: 0.9930675625801086)
[2025-02-13 03:21:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:31][root][INFO] - Training Epoch: 1/2, step 5918/7134 completed (loss: 0.09037875384092331, acc: 0.9811320900917053)
[2025-02-13 03:21:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:32][root][INFO] - Training Epoch: 1/2, step 5919/7134 completed (loss: 0.03705998510122299, acc: 0.9908814430236816)
[2025-02-13 03:21:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:32][root][INFO] - Training Epoch: 1/2, step 5920/7134 completed (loss: 0.029687535017728806, acc: 0.9916805028915405)
[2025-02-13 03:21:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:33][root][INFO] - Training Epoch: 1/2, step 5921/7134 completed (loss: 0.05810166895389557, acc: 0.9770354628562927)
[2025-02-13 03:21:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:33][root][INFO] - Training Epoch: 1/2, step 5922/7134 completed (loss: 0.18478645384311676, acc: 0.9592198729515076)
[2025-02-13 03:21:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:34][root][INFO] - Training Epoch: 1/2, step 5923/7134 completed (loss: 0.087506964802742, acc: 0.9795022010803223)
[2025-02-13 03:21:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:34][root][INFO] - Training Epoch: 1/2, step 5924/7134 completed (loss: 0.03560885041952133, acc: 0.9832935333251953)
[2025-02-13 03:21:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:34][root][INFO] - Training Epoch: 1/2, step 5925/7134 completed (loss: 0.20166845619678497, acc: 0.9648093581199646)
[2025-02-13 03:21:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:35][root][INFO] - Training Epoch: 1/2, step 5926/7134 completed (loss: 0.08193828910589218, acc: 0.9816666841506958)
[2025-02-13 03:21:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:35][root][INFO] - Training Epoch: 1/2, step 5927/7134 completed (loss: 0.08105145394802094, acc: 0.9755011200904846)
[2025-02-13 03:21:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:36][root][INFO] - Training Epoch: 1/2, step 5928/7134 completed (loss: 0.20287497341632843, acc: 0.9452054500579834)
[2025-02-13 03:21:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:36][root][INFO] - Training Epoch: 1/2, step 5929/7134 completed (loss: 0.0651240199804306, acc: 0.9776119589805603)
[2025-02-13 03:21:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:36][root][INFO] - Training Epoch: 1/2, step 5930/7134 completed (loss: 0.08281683176755905, acc: 0.9777283072471619)
[2025-02-13 03:21:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:37][root][INFO] - Training Epoch: 1/2, step 5931/7134 completed (loss: 0.04601995646953583, acc: 0.9892008900642395)
[2025-02-13 03:21:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:37][root][INFO] - Training Epoch: 1/2, step 5932/7134 completed (loss: 0.07386486977338791, acc: 0.9740740656852722)
[2025-02-13 03:21:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:38][root][INFO] - Training Epoch: 1/2, step 5933/7134 completed (loss: 0.13069988787174225, acc: 0.9664570093154907)
[2025-02-13 03:21:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:38][root][INFO] - Training Epoch: 1/2, step 5934/7134 completed (loss: 0.0449962317943573, acc: 0.9928951859474182)
[2025-02-13 03:21:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:38][root][INFO] - Training Epoch: 1/2, step 5935/7134 completed (loss: 0.0184031892567873, acc: 0.9941747784614563)
[2025-02-13 03:21:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:39][root][INFO] - Training Epoch: 1/2, step 5936/7134 completed (loss: 0.06509727239608765, acc: 0.9831546545028687)
[2025-02-13 03:21:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:39][root][INFO] - Training Epoch: 1/2, step 5937/7134 completed (loss: 0.029168903827667236, acc: 0.9904305934906006)
[2025-02-13 03:21:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:40][root][INFO] - Training Epoch: 1/2, step 5938/7134 completed (loss: 0.0630832090973854, acc: 0.9866156578063965)
[2025-02-13 03:21:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:40][root][INFO] - Training Epoch: 1/2, step 5939/7134 completed (loss: 0.13541090488433838, acc: 0.959276020526886)
[2025-02-13 03:21:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:40][root][INFO] - Training Epoch: 1/2, step 5940/7134 completed (loss: 0.06624100357294083, acc: 0.9737609624862671)
[2025-02-13 03:21:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:41][root][INFO] - Training Epoch: 1/2, step 5941/7134 completed (loss: 0.07978235185146332, acc: 0.9781249761581421)
[2025-02-13 03:21:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:41][root][INFO] - Training Epoch: 1/2, step 5942/7134 completed (loss: 0.016231052577495575, acc: 0.9961758852005005)
[2025-02-13 03:21:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:42][root][INFO] - Training Epoch: 1/2, step 5943/7134 completed (loss: 0.012415568344295025, acc: 1.0)
[2025-02-13 03:21:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:42][root][INFO] - Training Epoch: 1/2, step 5944/7134 completed (loss: 0.053192224353551865, acc: 0.981675386428833)
[2025-02-13 03:21:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:43][root][INFO] - Training Epoch: 1/2, step 5945/7134 completed (loss: 0.02953537181019783, acc: 0.9894894957542419)
[2025-02-13 03:21:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:43][root][INFO] - Training Epoch: 1/2, step 5946/7134 completed (loss: 0.03752969577908516, acc: 0.9909420013427734)
[2025-02-13 03:21:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:43][root][INFO] - Training Epoch: 1/2, step 5947/7134 completed (loss: 0.04458906501531601, acc: 0.986369252204895)
[2025-02-13 03:21:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:44][root][INFO] - Training Epoch: 1/2, step 5948/7134 completed (loss: 0.0661284551024437, acc: 0.9779999852180481)
[2025-02-13 03:21:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:44][root][INFO] - Training Epoch: 1/2, step 5949/7134 completed (loss: 0.01396029070019722, acc: 0.9974905848503113)
[2025-02-13 03:21:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:45][root][INFO] - Training Epoch: 1/2, step 5950/7134 completed (loss: 0.11228539049625397, acc: 0.9714964628219604)
[2025-02-13 03:21:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:45][root][INFO] - Training Epoch: 1/2, step 5951/7134 completed (loss: 0.04886940121650696, acc: 0.9856528043746948)
[2025-02-13 03:21:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:46][root][INFO] - Training Epoch: 1/2, step 5952/7134 completed (loss: 0.15446560084819794, acc: 0.971875011920929)
[2025-02-13 03:21:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:46][root][INFO] - Training Epoch: 1/2, step 5953/7134 completed (loss: 0.053530432283878326, acc: 0.9862259030342102)
[2025-02-13 03:21:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:46][root][INFO] - Training Epoch: 1/2, step 5954/7134 completed (loss: 0.017228087410330772, acc: 0.9949238300323486)
[2025-02-13 03:21:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:47][root][INFO] - Training Epoch: 1/2, step 5955/7134 completed (loss: 0.04269285500049591, acc: 0.9952380657196045)
[2025-02-13 03:21:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:47][root][INFO] - Training Epoch: 1/2, step 5956/7134 completed (loss: 0.05834539607167244, acc: 0.9784313440322876)
[2025-02-13 03:21:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:48][root][INFO] - Training Epoch: 1/2, step 5957/7134 completed (loss: 0.05252525955438614, acc: 0.9850075244903564)
[2025-02-13 03:21:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:48][root][INFO] - Training Epoch: 1/2, step 5958/7134 completed (loss: 0.03167243301868439, acc: 0.990604043006897)
[2025-02-13 03:21:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:48][root][INFO] - Training Epoch: 1/2, step 5959/7134 completed (loss: 0.050958000123500824, acc: 0.9893190860748291)
[2025-02-13 03:21:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:49][root][INFO] - Training Epoch: 1/2, step 5960/7134 completed (loss: 0.015301890671253204, acc: 0.9957864880561829)
[2025-02-13 03:21:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:49][root][INFO] - Training Epoch: 1/2, step 5961/7134 completed (loss: 0.040263690054416656, acc: 0.9897540807723999)
[2025-02-13 03:21:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:50][root][INFO] - Training Epoch: 1/2, step 5962/7134 completed (loss: 0.011710798367857933, acc: 0.995121955871582)
[2025-02-13 03:21:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:50][root][INFO] - Training Epoch: 1/2, step 5963/7134 completed (loss: 0.023224353790283203, acc: 0.9888268113136292)
[2025-02-13 03:21:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:50][root][INFO] - Training Epoch: 1/2, step 5964/7134 completed (loss: 0.02148258499801159, acc: 0.9926062822341919)
[2025-02-13 03:21:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:51][root][INFO] - Training Epoch: 1/2, step 5965/7134 completed (loss: 0.050871968269348145, acc: 0.9880059957504272)
[2025-02-13 03:21:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:51][root][INFO] - Training Epoch: 1/2, step 5966/7134 completed (loss: 0.04077855497598648, acc: 0.9902912378311157)
[2025-02-13 03:21:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:52][root][INFO] - Training Epoch: 1/2, step 5967/7134 completed (loss: 0.025452041998505592, acc: 0.9940298795700073)
[2025-02-13 03:21:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:52][root][INFO] - Training Epoch: 1/2, step 5968/7134 completed (loss: 0.03291303664445877, acc: 0.9931507110595703)
[2025-02-13 03:21:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:53][root][INFO] - Training Epoch: 1/2, step 5969/7134 completed (loss: 0.04275716096162796, acc: 0.9891641139984131)
[2025-02-13 03:21:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:53][root][INFO] - Training Epoch: 1/2, step 5970/7134 completed (loss: 0.04892176389694214, acc: 0.9918032884597778)
[2025-02-13 03:21:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:53][root][INFO] - Training Epoch: 1/2, step 5971/7134 completed (loss: 0.027175050228834152, acc: 0.9912280440330505)
[2025-02-13 03:21:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:54][root][INFO] - Training Epoch: 1/2, step 5972/7134 completed (loss: 0.07262106984853745, acc: 0.9867841601371765)
[2025-02-13 03:21:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:54][root][INFO] - Training Epoch: 1/2, step 5973/7134 completed (loss: 0.06516259163618088, acc: 0.9784172773361206)
[2025-02-13 03:21:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:55][root][INFO] - Training Epoch: 1/2, step 5974/7134 completed (loss: 0.012543903663754463, acc: 0.9965338110923767)
[2025-02-13 03:21:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:55][root][INFO] - Training Epoch: 1/2, step 5975/7134 completed (loss: 0.011615944094955921, acc: 0.9955752491950989)
[2025-02-13 03:21:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:56][root][INFO] - Training Epoch: 1/2, step 5976/7134 completed (loss: 0.037634242326021194, acc: 0.9907975196838379)
[2025-02-13 03:21:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:56][root][INFO] - Training Epoch: 1/2, step 5977/7134 completed (loss: 0.027672991156578064, acc: 0.9938042163848877)
[2025-02-13 03:21:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:57][root][INFO] - Training Epoch: 1/2, step 5978/7134 completed (loss: 0.02832489088177681, acc: 0.9919871687889099)
[2025-02-13 03:21:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:57][root][INFO] - Training Epoch: 1/2, step 5979/7134 completed (loss: 0.04740898683667183, acc: 0.9876543283462524)
[2025-02-13 03:21:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:57][root][INFO] - Training Epoch: 1/2, step 5980/7134 completed (loss: 0.0331881120800972, acc: 0.9935979247093201)
[2025-02-13 03:21:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:58][root][INFO] - Training Epoch: 1/2, step 5981/7134 completed (loss: 0.024629173800349236, acc: 0.99190753698349)
[2025-02-13 03:21:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:58][root][INFO] - Training Epoch: 1/2, step 5982/7134 completed (loss: 0.030473893508315086, acc: 0.9940357804298401)
[2025-02-13 03:21:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:59][root][INFO] - Training Epoch: 1/2, step 5983/7134 completed (loss: 0.037298355251550674, acc: 0.987500011920929)
[2025-02-13 03:21:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:59][root][INFO] - Training Epoch: 1/2, step 5984/7134 completed (loss: 0.03241245821118355, acc: 0.9929078221321106)
[2025-02-13 03:21:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:21:59][root][INFO] - Training Epoch: 1/2, step 5985/7134 completed (loss: 0.021483203396201134, acc: 0.9974968433380127)
[2025-02-13 03:22:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:00][root][INFO] - Training Epoch: 1/2, step 5986/7134 completed (loss: 0.03480197489261627, acc: 0.9898089170455933)
[2025-02-13 03:22:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:00][root][INFO] - Training Epoch: 1/2, step 5987/7134 completed (loss: 0.04921405762434006, acc: 0.9860140085220337)
[2025-02-13 03:22:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:01][root][INFO] - Training Epoch: 1/2, step 5988/7134 completed (loss: 0.014592752791941166, acc: 0.9959999918937683)
[2025-02-13 03:22:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:01][root][INFO] - Training Epoch: 1/2, step 5989/7134 completed (loss: 0.06275004148483276, acc: 0.9870848655700684)
[2025-02-13 03:22:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:02][root][INFO] - Training Epoch: 1/2, step 5990/7134 completed (loss: 0.024939991533756256, acc: 0.9904109835624695)
[2025-02-13 03:22:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:02][root][INFO] - Training Epoch: 1/2, step 5991/7134 completed (loss: 0.0887804627418518, acc: 0.9804469347000122)
[2025-02-13 03:22:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:03][root][INFO] - Training Epoch: 1/2, step 5992/7134 completed (loss: 0.026774467900395393, acc: 0.99303138256073)
[2025-02-13 03:22:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:03][root][INFO] - Training Epoch: 1/2, step 5993/7134 completed (loss: 0.02821965701878071, acc: 0.9930651783943176)
[2025-02-13 03:22:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:03][root][INFO] - Training Epoch: 1/2, step 5994/7134 completed (loss: 0.01640366204082966, acc: 0.9961977005004883)
[2025-02-13 03:22:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:04][root][INFO] - Training Epoch: 1/2, step 5995/7134 completed (loss: 0.016185764223337173, acc: 0.9957746267318726)
[2025-02-13 03:22:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:04][root][INFO] - Training Epoch: 1/2, step 5996/7134 completed (loss: 0.053251393139362335, acc: 0.9812206625938416)
[2025-02-13 03:22:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:05][root][INFO] - Training Epoch: 1/2, step 5997/7134 completed (loss: 0.04188796877861023, acc: 0.9933244585990906)
[2025-02-13 03:22:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:05][root][INFO] - Training Epoch: 1/2, step 5998/7134 completed (loss: 0.038781944662332535, acc: 0.9900249242782593)
[2025-02-13 03:22:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:06][root][INFO] - Training Epoch: 1/2, step 5999/7134 completed (loss: 0.034855931997299194, acc: 0.9888476133346558)
[2025-02-13 03:22:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:06][root][INFO] - Training Epoch: 1/2, step 6000/7134 completed (loss: 0.03938370198011398, acc: 0.9850249290466309)
[2025-02-13 03:22:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:06][root][INFO] - Training Epoch: 1/2, step 6001/7134 completed (loss: 0.019253600388765335, acc: 0.9948849081993103)
[2025-02-13 03:22:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:07][root][INFO] - Training Epoch: 1/2, step 6002/7134 completed (loss: 0.0405302457511425, acc: 0.988304078578949)
[2025-02-13 03:22:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:07][root][INFO] - Training Epoch: 1/2, step 6003/7134 completed (loss: 0.028240898624062538, acc: 0.9897210001945496)
[2025-02-13 03:22:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:08][root][INFO] - Training Epoch: 1/2, step 6004/7134 completed (loss: 0.030657384544610977, acc: 0.9888734221458435)
[2025-02-13 03:22:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:08][root][INFO] - Training Epoch: 1/2, step 6005/7134 completed (loss: 0.029794277623295784, acc: 0.9893454909324646)
[2025-02-13 03:22:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:08][root][INFO] - Training Epoch: 1/2, step 6006/7134 completed (loss: 0.05320580303668976, acc: 0.9864603281021118)
[2025-02-13 03:22:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:09][root][INFO] - Training Epoch: 1/2, step 6007/7134 completed (loss: 0.01874884031713009, acc: 0.9966611266136169)
[2025-02-13 03:22:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:09][root][INFO] - Training Epoch: 1/2, step 6008/7134 completed (loss: 0.042740143835544586, acc: 0.9866468906402588)
[2025-02-13 03:22:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:10][root][INFO] - Training Epoch: 1/2, step 6009/7134 completed (loss: 0.0561910904943943, acc: 0.9860248565673828)
[2025-02-13 03:22:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:10][root][INFO] - Training Epoch: 1/2, step 6010/7134 completed (loss: 0.05735894665122032, acc: 0.9850948452949524)
[2025-02-13 03:22:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:11][root][INFO] - Training Epoch: 1/2, step 6011/7134 completed (loss: 0.048501141369342804, acc: 0.984455943107605)
[2025-02-13 03:22:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:11][root][INFO] - Training Epoch: 1/2, step 6012/7134 completed (loss: 0.03600505366921425, acc: 0.9865671396255493)
[2025-02-13 03:22:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:12][root][INFO] - Training Epoch: 1/2, step 6013/7134 completed (loss: 0.04516497999429703, acc: 0.9866310358047485)
[2025-02-13 03:22:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:12][root][INFO] - Training Epoch: 1/2, step 6014/7134 completed (loss: 0.020613262429833412, acc: 0.9945553541183472)
[2025-02-13 03:22:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:12][root][INFO] - Training Epoch: 1/2, step 6015/7134 completed (loss: 0.09265104681253433, acc: 0.9752066135406494)
[2025-02-13 03:22:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:13][root][INFO] - Training Epoch: 1/2, step 6016/7134 completed (loss: 0.023961056023836136, acc: 0.9964157938957214)
[2025-02-13 03:22:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:13][root][INFO] - Training Epoch: 1/2, step 6017/7134 completed (loss: 0.08421865105628967, acc: 0.9900990128517151)
[2025-02-13 03:22:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:13][root][INFO] - Training Epoch: 1/2, step 6018/7134 completed (loss: 0.01582116261124611, acc: 0.9947368502616882)
[2025-02-13 03:22:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:14][root][INFO] - Training Epoch: 1/2, step 6019/7134 completed (loss: 0.08272943645715714, acc: 0.9741379022598267)
[2025-02-13 03:22:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:14][root][INFO] - Training Epoch: 1/2, step 6020/7134 completed (loss: 0.007616310380399227, acc: 1.0)
[2025-02-13 03:22:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:14][root][INFO] - Training Epoch: 1/2, step 6021/7134 completed (loss: 0.024563878774642944, acc: 0.9869281053543091)
[2025-02-13 03:22:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:15][root][INFO] - Training Epoch: 1/2, step 6022/7134 completed (loss: 0.027042526751756668, acc: 0.9926199316978455)
[2025-02-13 03:22:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:15][root][INFO] - Training Epoch: 1/2, step 6023/7134 completed (loss: 0.04330063238739967, acc: 0.989159882068634)
[2025-02-13 03:22:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:15][root][INFO] - Training Epoch: 1/2, step 6024/7134 completed (loss: 0.006854923442006111, acc: 1.0)
[2025-02-13 03:22:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:16][root][INFO] - Training Epoch: 1/2, step 6025/7134 completed (loss: 0.011418753303587437, acc: 0.9979079365730286)
[2025-02-13 03:22:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:16][root][INFO] - Training Epoch: 1/2, step 6026/7134 completed (loss: 0.028345750644803047, acc: 0.9942857027053833)
[2025-02-13 03:22:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:17][root][INFO] - Training Epoch: 1/2, step 6027/7134 completed (loss: 0.01753593608736992, acc: 0.9974293112754822)
[2025-02-13 03:22:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:17][root][INFO] - Training Epoch: 1/2, step 6028/7134 completed (loss: 0.017135392874479294, acc: 0.995555579662323)
[2025-02-13 03:22:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:17][root][INFO] - Training Epoch: 1/2, step 6029/7134 completed (loss: 0.021484490483999252, acc: 0.9891696572303772)
[2025-02-13 03:22:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:18][root][INFO] - Training Epoch: 1/2, step 6030/7134 completed (loss: 0.034897394478321075, acc: 0.992337167263031)
[2025-02-13 03:22:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:18][root][INFO] - Training Epoch: 1/2, step 6031/7134 completed (loss: 0.02876048907637596, acc: 0.9941520690917969)
[2025-02-13 03:22:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:18][root][INFO] - Training Epoch: 1/2, step 6032/7134 completed (loss: 0.02484084665775299, acc: 0.9974683523178101)
[2025-02-13 03:22:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:19][root][INFO] - Training Epoch: 1/2, step 6033/7134 completed (loss: 0.08212361484766006, acc: 0.9754098653793335)
[2025-02-13 03:22:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:19][root][INFO] - Training Epoch: 1/2, step 6034/7134 completed (loss: 0.06199547275900841, acc: 0.9836065769195557)
[2025-02-13 03:22:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:20][root][INFO] - Training Epoch: 1/2, step 6035/7134 completed (loss: 0.07510119676589966, acc: 0.9742690324783325)
[2025-02-13 03:22:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:20][root][INFO] - Training Epoch: 1/2, step 6036/7134 completed (loss: 0.0600460022687912, acc: 0.9846153855323792)
[2025-02-13 03:22:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:20][root][INFO] - Training Epoch: 1/2, step 6037/7134 completed (loss: 0.025769595056772232, acc: 0.991525411605835)
[2025-02-13 03:22:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:21][root][INFO] - Training Epoch: 1/2, step 6038/7134 completed (loss: 0.03577835485339165, acc: 0.9876957535743713)
[2025-02-13 03:22:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:21][root][INFO] - Training Epoch: 1/2, step 6039/7134 completed (loss: 0.03086717054247856, acc: 0.9928825497627258)
[2025-02-13 03:22:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:22][root][INFO] - Training Epoch: 1/2, step 6040/7134 completed (loss: 0.057660575956106186, acc: 0.9854469895362854)
[2025-02-13 03:22:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:22][root][INFO] - Training Epoch: 1/2, step 6041/7134 completed (loss: 0.04496169835329056, acc: 0.9868938326835632)
[2025-02-13 03:22:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:23][root][INFO] - Training Epoch: 1/2, step 6042/7134 completed (loss: 0.021418457850813866, acc: 0.9929999709129333)
[2025-02-13 03:22:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:23][root][INFO] - Training Epoch: 1/2, step 6043/7134 completed (loss: 0.04099183529615402, acc: 0.9892086386680603)
[2025-02-13 03:22:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:24][root][INFO] - Training Epoch: 1/2, step 6044/7134 completed (loss: 0.05291742458939552, acc: 0.9864197373390198)
[2025-02-13 03:22:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:24][root][INFO] - Training Epoch: 1/2, step 6045/7134 completed (loss: 0.03749174252152443, acc: 0.9869822263717651)
[2025-02-13 03:22:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:24][root][INFO] - Training Epoch: 1/2, step 6046/7134 completed (loss: 0.04530095309019089, acc: 0.9878683090209961)
[2025-02-13 03:22:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:25][root][INFO] - Training Epoch: 1/2, step 6047/7134 completed (loss: 0.12554055452346802, acc: 0.9679595232009888)
[2025-02-13 03:22:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:25][root][INFO] - Training Epoch: 1/2, step 6048/7134 completed (loss: 0.06609701365232468, acc: 0.9799599051475525)
[2025-02-13 03:22:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:26][root][INFO] - Training Epoch: 1/2, step 6049/7134 completed (loss: 0.019294749945402145, acc: 0.9931740760803223)
[2025-02-13 03:22:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:26][root][INFO] - Training Epoch: 1/2, step 6050/7134 completed (loss: 0.06506282836198807, acc: 0.9767441749572754)
[2025-02-13 03:22:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:27][root][INFO] - Training Epoch: 1/2, step 6051/7134 completed (loss: 0.09285794943571091, acc: 0.9734120965003967)
[2025-02-13 03:22:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:27][root][INFO] - Training Epoch: 1/2, step 6052/7134 completed (loss: 0.08684306591749191, acc: 0.9795918464660645)
[2025-02-13 03:22:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:28][root][INFO] - Training Epoch: 1/2, step 6053/7134 completed (loss: 0.04525298252701759, acc: 0.9857456088066101)
[2025-02-13 03:22:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:28][root][INFO] - Training Epoch: 1/2, step 6054/7134 completed (loss: 0.050699200481176376, acc: 0.9842995405197144)
[2025-02-13 03:22:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:28][root][INFO] - Training Epoch: 1/2, step 6055/7134 completed (loss: 0.03489074110984802, acc: 0.9905213117599487)
[2025-02-13 03:22:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:29][root][INFO] - Training Epoch: 1/2, step 6056/7134 completed (loss: 0.06572224944829941, acc: 0.981566846370697)
[2025-02-13 03:22:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:29][root][INFO] - Training Epoch: 1/2, step 6057/7134 completed (loss: 0.053256794810295105, acc: 0.9868420958518982)
[2025-02-13 03:22:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:30][root][INFO] - Training Epoch: 1/2, step 6058/7134 completed (loss: 0.015909846872091293, acc: 0.9954954981803894)
[2025-02-13 03:22:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:30][root][INFO] - Training Epoch: 1/2, step 6059/7134 completed (loss: 0.04099104180932045, acc: 0.982332170009613)
[2025-02-13 03:22:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:30][root][INFO] - Training Epoch: 1/2, step 6060/7134 completed (loss: 0.044224534183740616, acc: 0.9834586381912231)
[2025-02-13 03:22:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:31][root][INFO] - Training Epoch: 1/2, step 6061/7134 completed (loss: 0.06637465953826904, acc: 0.9830508232116699)
[2025-02-13 03:22:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:31][root][INFO] - Training Epoch: 1/2, step 6062/7134 completed (loss: 0.07598326355218887, acc: 0.9819121360778809)
[2025-02-13 03:22:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:32][root][INFO] - Training Epoch: 1/2, step 6063/7134 completed (loss: 0.09816181659698486, acc: 0.9801084995269775)
[2025-02-13 03:22:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:32][root][INFO] - Training Epoch: 1/2, step 6064/7134 completed (loss: 0.0359610915184021, acc: 0.986975371837616)
[2025-02-13 03:22:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:33][root][INFO] - Training Epoch: 1/2, step 6065/7134 completed (loss: 0.049561988562345505, acc: 0.9878493547439575)
[2025-02-13 03:22:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:33][root][INFO] - Training Epoch: 1/2, step 6066/7134 completed (loss: 0.0633796751499176, acc: 0.9802306294441223)
[2025-02-13 03:22:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:33][root][INFO] - Training Epoch: 1/2, step 6067/7134 completed (loss: 0.06055707484483719, acc: 0.98124098777771)
[2025-02-13 03:22:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:34][root][INFO] - Training Epoch: 1/2, step 6068/7134 completed (loss: 0.10200745612382889, acc: 0.9749687314033508)
[2025-02-13 03:22:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:34][root][INFO] - Training Epoch: 1/2, step 6069/7134 completed (loss: 0.07211892306804657, acc: 0.9766423106193542)
[2025-02-13 03:22:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:35][root][INFO] - Training Epoch: 1/2, step 6070/7134 completed (loss: 0.0778261199593544, acc: 0.9826897382736206)
[2025-02-13 03:22:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:35][root][INFO] - Training Epoch: 1/2, step 6071/7134 completed (loss: 0.05217549949884415, acc: 0.9811574816703796)
[2025-02-13 03:22:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:36][root][INFO] - Training Epoch: 1/2, step 6072/7134 completed (loss: 0.12775276601314545, acc: 0.9621848464012146)
[2025-02-13 03:22:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:36][root][INFO] - Training Epoch: 1/2, step 6073/7134 completed (loss: 0.08135836571455002, acc: 0.9790209531784058)
[2025-02-13 03:22:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:36][root][INFO] - Training Epoch: 1/2, step 6074/7134 completed (loss: 0.0804004892706871, acc: 0.9823529124259949)
[2025-02-13 03:22:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:37][root][INFO] - Training Epoch: 1/2, step 6075/7134 completed (loss: 0.06076645478606224, acc: 0.9844124913215637)
[2025-02-13 03:22:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:37][root][INFO] - Training Epoch: 1/2, step 6076/7134 completed (loss: 0.0769379734992981, acc: 0.977011501789093)
[2025-02-13 03:22:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:38][root][INFO] - Training Epoch: 1/2, step 6077/7134 completed (loss: 0.051747702062129974, acc: 0.9808306694030762)
[2025-02-13 03:22:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:38][root][INFO] - Training Epoch: 1/2, step 6078/7134 completed (loss: 0.09074899554252625, acc: 0.9774647951126099)
[2025-02-13 03:22:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:39][root][INFO] - Training Epoch: 1/2, step 6079/7134 completed (loss: 0.03240877017378807, acc: 0.9945255517959595)
[2025-02-13 03:22:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:39][root][INFO] - Training Epoch: 1/2, step 6080/7134 completed (loss: 0.043889693915843964, acc: 0.987270176410675)
[2025-02-13 03:22:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:39][root][INFO] - Training Epoch: 1/2, step 6081/7134 completed (loss: 0.04564273729920387, acc: 0.9862595200538635)
[2025-02-13 03:22:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:40][root][INFO] - Training Epoch: 1/2, step 6082/7134 completed (loss: 0.02681429125368595, acc: 0.993630588054657)
[2025-02-13 03:22:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:40][root][INFO] - Training Epoch: 1/2, step 6083/7134 completed (loss: 0.02013861946761608, acc: 0.9950860142707825)
[2025-02-13 03:22:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:41][root][INFO] - Training Epoch: 1/2, step 6084/7134 completed (loss: 0.04761389270424843, acc: 0.9889937043190002)
[2025-02-13 03:22:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:41][root][INFO] - Training Epoch: 1/2, step 6085/7134 completed (loss: 0.034581590443849564, acc: 0.9856887459754944)
[2025-02-13 03:22:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:42][root][INFO] - Training Epoch: 1/2, step 6086/7134 completed (loss: 0.027005018666386604, acc: 0.9915730357170105)
[2025-02-13 03:22:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:42][root][INFO] - Training Epoch: 1/2, step 6087/7134 completed (loss: 0.059726037085056305, acc: 0.9773585200309753)
[2025-02-13 03:22:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:43][root][INFO] - Training Epoch: 1/2, step 6088/7134 completed (loss: 0.02730576880276203, acc: 0.989276111125946)
[2025-02-13 03:22:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:43][root][INFO] - Training Epoch: 1/2, step 6089/7134 completed (loss: 0.045155201107263565, acc: 0.9889655113220215)
[2025-02-13 03:22:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:43][root][INFO] - Training Epoch: 1/2, step 6090/7134 completed (loss: 0.026889970526099205, acc: 0.9931412935256958)
[2025-02-13 03:22:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:44][root][INFO] - Training Epoch: 1/2, step 6091/7134 completed (loss: 0.03784581646323204, acc: 0.9888535141944885)
[2025-02-13 03:22:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:44][root][INFO] - Training Epoch: 1/2, step 6092/7134 completed (loss: 0.03330152854323387, acc: 0.9936507940292358)
[2025-02-13 03:22:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:45][root][INFO] - Training Epoch: 1/2, step 6093/7134 completed (loss: 0.0415278784930706, acc: 0.9888682961463928)
[2025-02-13 03:22:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:45][root][INFO] - Training Epoch: 1/2, step 6094/7134 completed (loss: 0.03451230749487877, acc: 0.9915966391563416)
[2025-02-13 03:22:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:45][root][INFO] - Training Epoch: 1/2, step 6095/7134 completed (loss: 0.026536593213677406, acc: 0.9913793206214905)
[2025-02-13 03:22:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:46][root][INFO] - Training Epoch: 1/2, step 6096/7134 completed (loss: 0.04591735452413559, acc: 0.9856801629066467)
[2025-02-13 03:22:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:46][root][INFO] - Training Epoch: 1/2, step 6097/7134 completed (loss: 0.04789679870009422, acc: 0.9833794832229614)
[2025-02-13 03:22:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:47][root][INFO] - Training Epoch: 1/2, step 6098/7134 completed (loss: 0.04483594745397568, acc: 0.9858430027961731)
[2025-02-13 03:22:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:47][root][INFO] - Training Epoch: 1/2, step 6099/7134 completed (loss: 0.0792401134967804, acc: 0.9801223278045654)
[2025-02-13 03:22:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:48][root][INFO] - Training Epoch: 1/2, step 6100/7134 completed (loss: 0.062291402369737625, acc: 0.9786585569381714)
[2025-02-13 03:22:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:48][root][INFO] - Training Epoch: 1/2, step 6101/7134 completed (loss: 0.026423176750540733, acc: 0.9936102032661438)
[2025-02-13 03:22:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:48][root][INFO] - Training Epoch: 1/2, step 6102/7134 completed (loss: 0.043197136372327805, acc: 0.9911971688270569)
[2025-02-13 03:22:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:49][root][INFO] - Training Epoch: 1/2, step 6103/7134 completed (loss: 0.04214702174067497, acc: 0.9833333492279053)
[2025-02-13 03:22:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:49][root][INFO] - Training Epoch: 1/2, step 6104/7134 completed (loss: 0.08941002935171127, acc: 0.9777777791023254)
[2025-02-13 03:22:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:50][root][INFO] - Training Epoch: 1/2, step 6105/7134 completed (loss: 0.06555062532424927, acc: 0.9851190447807312)
[2025-02-13 03:22:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:50][root][INFO] - Training Epoch: 1/2, step 6106/7134 completed (loss: 0.05172455683350563, acc: 0.9856887459754944)
[2025-02-13 03:22:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:50][root][INFO] - Training Epoch: 1/2, step 6107/7134 completed (loss: 0.058037664741277695, acc: 0.9852941036224365)
[2025-02-13 03:22:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:51][root][INFO] - Training Epoch: 1/2, step 6108/7134 completed (loss: 0.051485657691955566, acc: 0.9817850589752197)
[2025-02-13 03:22:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:51][root][INFO] - Training Epoch: 1/2, step 6109/7134 completed (loss: 0.02005869336426258, acc: 0.991525411605835)
[2025-02-13 03:22:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:52][root][INFO] - Training Epoch: 1/2, step 6110/7134 completed (loss: 0.10835114866495132, acc: 0.9700854420661926)
[2025-02-13 03:22:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:52][root][INFO] - Training Epoch: 1/2, step 6111/7134 completed (loss: 0.09220139682292938, acc: 0.97050940990448)
[2025-02-13 03:22:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:53][root][INFO] - Training Epoch: 1/2, step 6112/7134 completed (loss: 0.043430715799331665, acc: 0.9816053509712219)
[2025-02-13 03:22:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:53][root][INFO] - Training Epoch: 1/2, step 6113/7134 completed (loss: 0.01990063674747944, acc: 0.9924127459526062)
[2025-02-13 03:22:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:53][root][INFO] - Training Epoch: 1/2, step 6114/7134 completed (loss: 0.07704080641269684, acc: 0.9768595099449158)
[2025-02-13 03:22:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:54][root][INFO] - Training Epoch: 1/2, step 6115/7134 completed (loss: 0.05462152510881424, acc: 0.99042147397995)
[2025-02-13 03:22:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:54][root][INFO] - Training Epoch: 1/2, step 6116/7134 completed (loss: 0.11042714864015579, acc: 0.9723320007324219)
[2025-02-13 03:22:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:55][root][INFO] - Training Epoch: 1/2, step 6117/7134 completed (loss: 0.09127473831176758, acc: 0.9836065769195557)
[2025-02-13 03:22:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:55][root][INFO] - Training Epoch: 1/2, step 6118/7134 completed (loss: 0.07609721273183823, acc: 0.9797794222831726)
[2025-02-13 03:22:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:55][root][INFO] - Training Epoch: 1/2, step 6119/7134 completed (loss: 0.04328491538763046, acc: 0.9855942130088806)
[2025-02-13 03:22:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:56][root][INFO] - Training Epoch: 1/2, step 6120/7134 completed (loss: 0.03854287415742874, acc: 0.991055428981781)
[2025-02-13 03:22:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:56][root][INFO] - Training Epoch: 1/2, step 6121/7134 completed (loss: 0.06019086763262749, acc: 0.980215847492218)
[2025-02-13 03:22:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:57][root][INFO] - Training Epoch: 1/2, step 6122/7134 completed (loss: 0.0362234003841877, acc: 0.9867924451828003)
[2025-02-13 03:22:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:57][root][INFO] - Training Epoch: 1/2, step 6123/7134 completed (loss: 0.10428337007761002, acc: 0.975051999092102)
[2025-02-13 03:22:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:58][root][INFO] - Training Epoch: 1/2, step 6124/7134 completed (loss: 0.027100035920739174, acc: 0.9865384697914124)
[2025-02-13 03:22:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:58][root][INFO] - Training Epoch: 1/2, step 6125/7134 completed (loss: 0.14561311900615692, acc: 0.9710843563079834)
[2025-02-13 03:22:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:58][root][INFO] - Training Epoch: 1/2, step 6126/7134 completed (loss: 0.06543993949890137, acc: 0.9858934283256531)
[2025-02-13 03:22:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:59][root][INFO] - Training Epoch: 1/2, step 6127/7134 completed (loss: 0.03839040547609329, acc: 0.9857142567634583)
[2025-02-13 03:22:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:22:59][root][INFO] - Training Epoch: 1/2, step 6128/7134 completed (loss: 0.037376079708337784, acc: 0.9838056564331055)
[2025-02-13 03:22:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:00][root][INFO] - Training Epoch: 1/2, step 6129/7134 completed (loss: 0.045913878828287125, acc: 0.9845288395881653)
[2025-02-13 03:23:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:00][root][INFO] - Training Epoch: 1/2, step 6130/7134 completed (loss: 0.032615918666124344, acc: 0.9910394549369812)
[2025-02-13 03:23:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:01][root][INFO] - Training Epoch: 1/2, step 6131/7134 completed (loss: 0.04673938453197479, acc: 0.9916167855262756)
[2025-02-13 03:23:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:01][root][INFO] - Training Epoch: 1/2, step 6132/7134 completed (loss: 0.06265072524547577, acc: 0.9828125238418579)
[2025-02-13 03:23:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:01][root][INFO] - Training Epoch: 1/2, step 6133/7134 completed (loss: 0.05123506858944893, acc: 0.9890776872634888)
[2025-02-13 03:23:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:02][root][INFO] - Training Epoch: 1/2, step 6134/7134 completed (loss: 0.08215807378292084, acc: 0.9785605072975159)
[2025-02-13 03:23:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:02][root][INFO] - Training Epoch: 1/2, step 6135/7134 completed (loss: 0.01697573997080326, acc: 0.996515691280365)
[2025-02-13 03:23:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:03][root][INFO] - Training Epoch: 1/2, step 6136/7134 completed (loss: 0.06524156779050827, acc: 0.9901960492134094)
[2025-02-13 03:23:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:03][root][INFO] - Training Epoch: 1/2, step 6137/7134 completed (loss: 0.08549264073371887, acc: 0.9828392863273621)
[2025-02-13 03:23:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:03][root][INFO] - Training Epoch: 1/2, step 6138/7134 completed (loss: 0.08016426861286163, acc: 0.9831029176712036)
[2025-02-13 03:23:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:04][root][INFO] - Training Epoch: 1/2, step 6139/7134 completed (loss: 0.06948861479759216, acc: 0.979629635810852)
[2025-02-13 03:23:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:04][root][INFO] - Training Epoch: 1/2, step 6140/7134 completed (loss: 0.035364434123039246, acc: 0.9929328560829163)
[2025-02-13 03:23:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:05][root][INFO] - Training Epoch: 1/2, step 6141/7134 completed (loss: 0.10545610636472702, acc: 0.9816513657569885)
[2025-02-13 03:23:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:05][root][INFO] - Training Epoch: 1/2, step 6142/7134 completed (loss: 0.04484083876013756, acc: 0.9884892106056213)
[2025-02-13 03:23:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:06][root][INFO] - Training Epoch: 1/2, step 6143/7134 completed (loss: 0.03547937050461769, acc: 0.9932432174682617)
[2025-02-13 03:23:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:06][root][INFO] - Training Epoch: 1/2, step 6144/7134 completed (loss: 0.02362794801592827, acc: 0.9932773113250732)
[2025-02-13 03:23:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:06][root][INFO] - Training Epoch: 1/2, step 6145/7134 completed (loss: 0.07330553233623505, acc: 0.9815059304237366)
[2025-02-13 03:23:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:07][root][INFO] - Training Epoch: 1/2, step 6146/7134 completed (loss: 0.06597089022397995, acc: 0.9834087491035461)
[2025-02-13 03:23:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:07][root][INFO] - Training Epoch: 1/2, step 6147/7134 completed (loss: 0.05847562476992607, acc: 0.9841269850730896)
[2025-02-13 03:23:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:08][root][INFO] - Training Epoch: 1/2, step 6148/7134 completed (loss: 0.0764055848121643, acc: 0.9765493869781494)
[2025-02-13 03:23:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:08][root][INFO] - Training Epoch: 1/2, step 6149/7134 completed (loss: 0.04479563981294632, acc: 0.9871382713317871)
[2025-02-13 03:23:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:09][root][INFO] - Training Epoch: 1/2, step 6150/7134 completed (loss: 0.03478698804974556, acc: 0.9877192974090576)
[2025-02-13 03:23:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:09][root][INFO] - Training Epoch: 1/2, step 6151/7134 completed (loss: 0.03841426968574524, acc: 0.9858712553977966)
[2025-02-13 03:23:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:09][root][INFO] - Training Epoch: 1/2, step 6152/7134 completed (loss: 0.04191619157791138, acc: 0.9864253401756287)
[2025-02-13 03:23:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:10][root][INFO] - Training Epoch: 1/2, step 6153/7134 completed (loss: 0.028407655656337738, acc: 0.9884225726127625)
[2025-02-13 03:23:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:10][root][INFO] - Training Epoch: 1/2, step 6154/7134 completed (loss: 0.04903247579932213, acc: 0.9902371168136597)
[2025-02-13 03:23:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:11][root][INFO] - Training Epoch: 1/2, step 6155/7134 completed (loss: 0.062593974173069, acc: 0.9792284965515137)
[2025-02-13 03:23:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:11][root][INFO] - Training Epoch: 1/2, step 6156/7134 completed (loss: 0.02785898558795452, acc: 0.9904912710189819)
[2025-02-13 03:23:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:12][root][INFO] - Training Epoch: 1/2, step 6157/7134 completed (loss: 0.03640587627887726, acc: 0.9858934283256531)
[2025-02-13 03:23:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:12][root][INFO] - Training Epoch: 1/2, step 6158/7134 completed (loss: 0.0783090591430664, acc: 0.9819168448448181)
[2025-02-13 03:23:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:12][root][INFO] - Training Epoch: 1/2, step 6159/7134 completed (loss: 0.05143965035676956, acc: 0.9843184351921082)
[2025-02-13 03:23:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:13][root][INFO] - Training Epoch: 1/2, step 6160/7134 completed (loss: 0.08265780657529831, acc: 0.9772727489471436)
[2025-02-13 03:23:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:13][root][INFO] - Training Epoch: 1/2, step 6161/7134 completed (loss: 0.07707031071186066, acc: 0.9718804955482483)
[2025-02-13 03:23:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:14][root][INFO] - Training Epoch: 1/2, step 6162/7134 completed (loss: 0.06422185897827148, acc: 0.9836333990097046)
[2025-02-13 03:23:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:14][root][INFO] - Training Epoch: 1/2, step 6163/7134 completed (loss: 0.09356740862131119, acc: 0.9821428656578064)
[2025-02-13 03:23:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:15][root][INFO] - Training Epoch: 1/2, step 6164/7134 completed (loss: 0.08350052684545517, acc: 0.977337121963501)
[2025-02-13 03:23:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:15][root][INFO] - Training Epoch: 1/2, step 6165/7134 completed (loss: 0.06701357662677765, acc: 0.9833948612213135)
[2025-02-13 03:23:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:16][root][INFO] - Training Epoch: 1/2, step 6166/7134 completed (loss: 0.03984765708446503, acc: 0.9912126660346985)
[2025-02-13 03:23:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:16][root][INFO] - Training Epoch: 1/2, step 6167/7134 completed (loss: 0.03044166788458824, acc: 0.9929278492927551)
[2025-02-13 03:23:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:16][root][INFO] - Training Epoch: 1/2, step 6168/7134 completed (loss: 0.03295091167092323, acc: 0.992277979850769)
[2025-02-13 03:23:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:17][root][INFO] - Training Epoch: 1/2, step 6169/7134 completed (loss: 0.04689088836312294, acc: 0.9897959232330322)
[2025-02-13 03:23:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:17][root][INFO] - Training Epoch: 1/2, step 6170/7134 completed (loss: 0.027778958901762962, acc: 0.992438554763794)
[2025-02-13 03:23:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:18][root][INFO] - Training Epoch: 1/2, step 6171/7134 completed (loss: 0.02724425680935383, acc: 0.9881734848022461)
[2025-02-13 03:23:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:18][root][INFO] - Training Epoch: 1/2, step 6172/7134 completed (loss: 0.022946685552597046, acc: 0.9899799823760986)
[2025-02-13 03:23:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:18][root][INFO] - Training Epoch: 1/2, step 6173/7134 completed (loss: 0.014146802946925163, acc: 0.9941089749336243)
[2025-02-13 03:23:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:19][root][INFO] - Training Epoch: 1/2, step 6174/7134 completed (loss: 0.011844730004668236, acc: 0.9970674514770508)
[2025-02-13 03:23:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:19][root][INFO] - Training Epoch: 1/2, step 6175/7134 completed (loss: 0.03519747778773308, acc: 0.9881656765937805)
[2025-02-13 03:23:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:20][root][INFO] - Training Epoch: 1/2, step 6176/7134 completed (loss: 0.04144596680998802, acc: 0.986975371837616)
[2025-02-13 03:23:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:20][root][INFO] - Training Epoch: 1/2, step 6177/7134 completed (loss: 0.0304041039198637, acc: 0.9872123003005981)
[2025-02-13 03:23:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:21][root][INFO] - Training Epoch: 1/2, step 6178/7134 completed (loss: 0.016331732273101807, acc: 0.9955752491950989)
[2025-02-13 03:23:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:21][root][INFO] - Training Epoch: 1/2, step 6179/7134 completed (loss: 0.049320414662361145, acc: 0.9875776171684265)
[2025-02-13 03:23:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:21][root][INFO] - Training Epoch: 1/2, step 6180/7134 completed (loss: 0.15249711275100708, acc: 0.9585987329483032)
[2025-02-13 03:23:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:22][root][INFO] - Training Epoch: 1/2, step 6181/7134 completed (loss: 0.25629711151123047, acc: 0.9214876294136047)
[2025-02-13 03:23:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:22][root][INFO] - Training Epoch: 1/2, step 6182/7134 completed (loss: 0.04820244014263153, acc: 0.9814471006393433)
[2025-02-13 03:23:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:23][root][INFO] - Training Epoch: 1/2, step 6183/7134 completed (loss: 0.0660252571105957, acc: 0.9754977226257324)
[2025-02-13 03:23:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:23][root][INFO] - Training Epoch: 1/2, step 6184/7134 completed (loss: 0.1334763616323471, acc: 0.9637096524238586)
[2025-02-13 03:23:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:23][root][INFO] - Training Epoch: 1/2, step 6185/7134 completed (loss: 0.0398624986410141, acc: 0.9889705777168274)
[2025-02-13 03:23:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:24][root][INFO] - Training Epoch: 1/2, step 6186/7134 completed (loss: 0.027678338810801506, acc: 0.9895104765892029)
[2025-02-13 03:23:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:24][root][INFO] - Training Epoch: 1/2, step 6187/7134 completed (loss: 0.04321955516934395, acc: 0.988095223903656)
[2025-02-13 03:23:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:25][root][INFO] - Training Epoch: 1/2, step 6188/7134 completed (loss: 0.06700234860181808, acc: 0.9816642999649048)
[2025-02-13 03:23:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:25][root][INFO] - Training Epoch: 1/2, step 6189/7134 completed (loss: 0.09258359670639038, acc: 0.9682539701461792)
[2025-02-13 03:23:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:26][root][INFO] - Training Epoch: 1/2, step 6190/7134 completed (loss: 0.08408913016319275, acc: 0.9717608094215393)
[2025-02-13 03:23:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:26][root][INFO] - Training Epoch: 1/2, step 6191/7134 completed (loss: 0.053754087537527084, acc: 0.9790209531784058)
[2025-02-13 03:23:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:27][root][INFO] - Training Epoch: 1/2, step 6192/7134 completed (loss: 0.03287230432033539, acc: 0.9924973249435425)
[2025-02-13 03:23:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:27][root][INFO] - Training Epoch: 1/2, step 6193/7134 completed (loss: 0.05262790247797966, acc: 0.9809750318527222)
[2025-02-13 03:23:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:27][root][INFO] - Training Epoch: 1/2, step 6194/7134 completed (loss: 0.06307420879602432, acc: 0.9727626442909241)
[2025-02-13 03:23:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:28][root][INFO] - Training Epoch: 1/2, step 6195/7134 completed (loss: 0.07582534104585648, acc: 0.9784688949584961)
[2025-02-13 03:23:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:28][root][INFO] - Training Epoch: 1/2, step 6196/7134 completed (loss: 0.04644094780087471, acc: 0.9848484992980957)
[2025-02-13 03:23:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:29][root][INFO] - Training Epoch: 1/2, step 6197/7134 completed (loss: 0.07612749934196472, acc: 0.977931022644043)
[2025-02-13 03:23:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:29][root][INFO] - Training Epoch: 1/2, step 6198/7134 completed (loss: 0.03955727070569992, acc: 0.9917012453079224)
[2025-02-13 03:23:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:30][root][INFO] - Training Epoch: 1/2, step 6199/7134 completed (loss: 0.05100693926215172, acc: 0.9849187731742859)
[2025-02-13 03:23:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:30][root][INFO] - Training Epoch: 1/2, step 6200/7134 completed (loss: 0.03043876215815544, acc: 0.9914634227752686)
[2025-02-13 03:23:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:31][root][INFO] - Training Epoch: 1/2, step 6201/7134 completed (loss: 0.02329743281006813, acc: 0.9933110475540161)
[2025-02-13 03:23:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:31][root][INFO] - Training Epoch: 1/2, step 6202/7134 completed (loss: 0.051150064915418625, acc: 0.9869847893714905)
[2025-02-13 03:23:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:31][root][INFO] - Training Epoch: 1/2, step 6203/7134 completed (loss: 0.022760197520256042, acc: 0.9923469424247742)
[2025-02-13 03:23:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:32][root][INFO] - Training Epoch: 1/2, step 6204/7134 completed (loss: 0.031174011528491974, acc: 0.994106113910675)
[2025-02-13 03:23:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:32][root][INFO] - Training Epoch: 1/2, step 6205/7134 completed (loss: 0.05068710818886757, acc: 0.9829351305961609)
[2025-02-13 03:23:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:33][root][INFO] - Training Epoch: 1/2, step 6206/7134 completed (loss: 0.04191276431083679, acc: 0.9898697733879089)
[2025-02-13 03:23:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:33][root][INFO] - Training Epoch: 1/2, step 6207/7134 completed (loss: 0.13278375566005707, acc: 0.9666221737861633)
[2025-02-13 03:23:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:34][root][INFO] - Training Epoch: 1/2, step 6208/7134 completed (loss: 0.04016483202576637, acc: 0.9849537014961243)
[2025-02-13 03:23:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:34][root][INFO] - Training Epoch: 1/2, step 6209/7134 completed (loss: 0.05566130951046944, acc: 0.9857697486877441)
[2025-02-13 03:23:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:34][root][INFO] - Training Epoch: 1/2, step 6210/7134 completed (loss: 0.05170082673430443, acc: 0.9824175834655762)
[2025-02-13 03:23:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:35][root][INFO] - Training Epoch: 1/2, step 6211/7134 completed (loss: 0.051017194986343384, acc: 0.9851149916648865)
[2025-02-13 03:23:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:35][root][INFO] - Training Epoch: 1/2, step 6212/7134 completed (loss: 0.05429378151893616, acc: 0.9877192974090576)
[2025-02-13 03:23:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:36][root][INFO] - Training Epoch: 1/2, step 6213/7134 completed (loss: 0.010445729829370975, acc: 1.0)
[2025-02-13 03:23:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:36][root][INFO] - Training Epoch: 1/2, step 6214/7134 completed (loss: 0.03584560006856918, acc: 0.9897304177284241)
[2025-02-13 03:23:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:37][root][INFO] - Training Epoch: 1/2, step 6215/7134 completed (loss: 0.05570443347096443, acc: 0.9827188849449158)
[2025-02-13 03:23:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:37][root][INFO] - Training Epoch: 1/2, step 6216/7134 completed (loss: 0.0414479523897171, acc: 0.9872340559959412)
[2025-02-13 03:23:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:37][root][INFO] - Training Epoch: 1/2, step 6217/7134 completed (loss: 0.03474455699324608, acc: 0.9892617464065552)
[2025-02-13 03:23:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:38][root][INFO] - Training Epoch: 1/2, step 6218/7134 completed (loss: 0.052253611385822296, acc: 0.9862805008888245)
[2025-02-13 03:23:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:38][root][INFO] - Training Epoch: 1/2, step 6219/7134 completed (loss: 0.018513543531298637, acc: 0.9929676651954651)
[2025-02-13 03:23:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:39][root][INFO] - Training Epoch: 1/2, step 6220/7134 completed (loss: 0.037107307463884354, acc: 0.9857142567634583)
[2025-02-13 03:23:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:39][root][INFO] - Training Epoch: 1/2, step 6221/7134 completed (loss: 0.011547707952558994, acc: 0.9964912533760071)
[2025-02-13 03:23:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:40][root][INFO] - Training Epoch: 1/2, step 6222/7134 completed (loss: 0.06021910905838013, acc: 0.9849726557731628)
[2025-02-13 03:23:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:40][root][INFO] - Training Epoch: 1/2, step 6223/7134 completed (loss: 0.04093233495950699, acc: 0.9890109896659851)
[2025-02-13 03:23:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:40][root][INFO] - Training Epoch: 1/2, step 6224/7134 completed (loss: 0.015203868970274925, acc: 0.9970972537994385)
[2025-02-13 03:23:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:41][root][INFO] - Training Epoch: 1/2, step 6225/7134 completed (loss: 0.03994334116578102, acc: 0.9897959232330322)
[2025-02-13 03:23:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:41][root][INFO] - Training Epoch: 1/2, step 6226/7134 completed (loss: 0.04192965105175972, acc: 0.9855832457542419)
[2025-02-13 03:23:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:42][root][INFO] - Training Epoch: 1/2, step 6227/7134 completed (loss: 0.01572810672223568, acc: 0.9946452379226685)
[2025-02-13 03:23:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:42][root][INFO] - Training Epoch: 1/2, step 6228/7134 completed (loss: 0.02253270149230957, acc: 0.9914004802703857)
[2025-02-13 03:23:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:43][root][INFO] - Training Epoch: 1/2, step 6229/7134 completed (loss: 0.07698262482881546, acc: 0.9756380319595337)
[2025-02-13 03:23:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:43][root][INFO] - Training Epoch: 1/2, step 6230/7134 completed (loss: 0.03510270267724991, acc: 0.9925187230110168)
[2025-02-13 03:23:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:43][root][INFO] - Training Epoch: 1/2, step 6231/7134 completed (loss: 0.020569659769535065, acc: 0.9935400485992432)
[2025-02-13 03:23:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:44][root][INFO] - Training Epoch: 1/2, step 6232/7134 completed (loss: 0.02413293719291687, acc: 0.9886363744735718)
[2025-02-13 03:23:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:44][root][INFO] - Training Epoch: 1/2, step 6233/7134 completed (loss: 0.04232959821820259, acc: 0.9865951538085938)
[2025-02-13 03:23:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:45][root][INFO] - Training Epoch: 1/2, step 6234/7134 completed (loss: 0.05721289664506912, acc: 0.9826202988624573)
[2025-02-13 03:23:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:45][root][INFO] - Training Epoch: 1/2, step 6235/7134 completed (loss: 0.020132912322878838, acc: 0.9947848916053772)
[2025-02-13 03:23:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:46][root][INFO] - Training Epoch: 1/2, step 6236/7134 completed (loss: 0.03668335825204849, acc: 0.9877216815948486)
[2025-02-13 03:23:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:46][root][INFO] - Training Epoch: 1/2, step 6237/7134 completed (loss: 0.039596736431121826, acc: 0.9892966151237488)
[2025-02-13 03:23:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:47][root][INFO] - Training Epoch: 1/2, step 6238/7134 completed (loss: 0.010234356857836246, acc: 0.9983844757080078)
[2025-02-13 03:23:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:47][root][INFO] - Training Epoch: 1/2, step 6239/7134 completed (loss: 0.034034986048936844, acc: 0.9940263032913208)
[2025-02-13 03:23:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:47][root][INFO] - Training Epoch: 1/2, step 6240/7134 completed (loss: 0.004510717466473579, acc: 1.0)
[2025-02-13 03:23:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:48][root][INFO] - Training Epoch: 1/2, step 6241/7134 completed (loss: 0.03216814994812012, acc: 0.9920886158943176)
[2025-02-13 03:23:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:48][root][INFO] - Training Epoch: 1/2, step 6242/7134 completed (loss: 0.015297561883926392, acc: 0.9948979616165161)
[2025-02-13 03:23:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:49][root][INFO] - Training Epoch: 1/2, step 6243/7134 completed (loss: 0.019963962957262993, acc: 0.9952229261398315)
[2025-02-13 03:23:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:49][root][INFO] - Training Epoch: 1/2, step 6244/7134 completed (loss: 0.01589234732091427, acc: 0.9945130348205566)
[2025-02-13 03:23:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:50][root][INFO] - Training Epoch: 1/2, step 6245/7134 completed (loss: 0.02339196391403675, acc: 0.99452805519104)
[2025-02-13 03:23:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:50][root][INFO] - Training Epoch: 1/2, step 6246/7134 completed (loss: 0.03423355892300606, acc: 0.9881423115730286)
[2025-02-13 03:23:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:50][root][INFO] - Training Epoch: 1/2, step 6247/7134 completed (loss: 0.05011184141039848, acc: 0.984337329864502)
[2025-02-13 03:23:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:51][root][INFO] - Training Epoch: 1/2, step 6248/7134 completed (loss: 0.05098598450422287, acc: 0.9844236969947815)
[2025-02-13 03:23:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:51][root][INFO] - Training Epoch: 1/2, step 6249/7134 completed (loss: 0.006522366777062416, acc: 0.9968553185462952)
[2025-02-13 03:23:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:52][root][INFO] - Training Epoch: 1/2, step 6250/7134 completed (loss: 0.002725450089201331, acc: 1.0)
[2025-02-13 03:23:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:52][root][INFO] - Training Epoch: 1/2, step 6251/7134 completed (loss: 0.027247078716754913, acc: 0.9938650131225586)
[2025-02-13 03:23:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:53][root][INFO] - Training Epoch: 1/2, step 6252/7134 completed (loss: 0.014025937765836716, acc: 0.9935275316238403)
[2025-02-13 03:23:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:53][root][INFO] - Training Epoch: 1/2, step 6253/7134 completed (loss: 0.02188282087445259, acc: 0.9931034445762634)
[2025-02-13 03:23:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:53][root][INFO] - Training Epoch: 1/2, step 6254/7134 completed (loss: 0.025520706549286842, acc: 0.9930796027183533)
[2025-02-13 03:23:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:54][root][INFO] - Training Epoch: 1/2, step 6255/7134 completed (loss: 0.01991729997098446, acc: 0.9918699264526367)
[2025-02-13 03:23:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:54][root][INFO] - Training Epoch: 1/2, step 6256/7134 completed (loss: 0.01911325380206108, acc: 0.9914893507957458)
[2025-02-13 03:23:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:55][root][INFO] - Training Epoch: 1/2, step 6257/7134 completed (loss: 0.017584720626473427, acc: 0.9913669228553772)
[2025-02-13 03:23:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:55][root][INFO] - Training Epoch: 1/2, step 6258/7134 completed (loss: 0.012506863102316856, acc: 0.995199978351593)
[2025-02-13 03:23:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:56][root][INFO] - Training Epoch: 1/2, step 6259/7134 completed (loss: 0.018785560503602028, acc: 0.9909909963607788)
[2025-02-13 03:23:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:56][root][INFO] - Training Epoch: 1/2, step 6260/7134 completed (loss: 0.01931883580982685, acc: 0.994966447353363)
[2025-02-13 03:23:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:56][root][INFO] - Training Epoch: 1/2, step 6261/7134 completed (loss: 0.020776374265551567, acc: 0.9939024448394775)
[2025-02-13 03:23:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:57][root][INFO] - Training Epoch: 1/2, step 6262/7134 completed (loss: 0.007052658125758171, acc: 0.9971305727958679)
[2025-02-13 03:23:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:57][root][INFO] - Training Epoch: 1/2, step 6263/7134 completed (loss: 0.00838734395802021, acc: 0.9956076145172119)
[2025-02-13 03:23:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:58][root][INFO] - Training Epoch: 1/2, step 6264/7134 completed (loss: 0.010987662710249424, acc: 0.9950739145278931)
[2025-02-13 03:23:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:58][root][INFO] - Training Epoch: 1/2, step 6265/7134 completed (loss: 0.01528653409332037, acc: 0.9955621361732483)
[2025-02-13 03:23:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:58][root][INFO] - Training Epoch: 1/2, step 6266/7134 completed (loss: 0.009902991354465485, acc: 0.996503472328186)
[2025-02-13 03:23:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:59][root][INFO] - Training Epoch: 1/2, step 6267/7134 completed (loss: 0.022073376923799515, acc: 0.9967426657676697)
[2025-02-13 03:23:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:23:59][root][INFO] - Training Epoch: 1/2, step 6268/7134 completed (loss: 0.03200862929224968, acc: 0.9955357313156128)
[2025-02-13 03:23:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:00][root][INFO] - Training Epoch: 1/2, step 6269/7134 completed (loss: 0.02114967815577984, acc: 0.9957355856895447)
[2025-02-13 03:24:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:00][root][INFO] - Training Epoch: 1/2, step 6270/7134 completed (loss: 0.014970751479268074, acc: 0.995398759841919)
[2025-02-13 03:24:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:01][root][INFO] - Training Epoch: 1/2, step 6271/7134 completed (loss: 0.032563839107751846, acc: 0.9946523904800415)
[2025-02-13 03:24:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:01][root][INFO] - Training Epoch: 1/2, step 6272/7134 completed (loss: 0.008089759387075901, acc: 0.9941262602806091)
[2025-02-13 03:24:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:02][root][INFO] - Training Epoch: 1/2, step 6273/7134 completed (loss: 0.00946283619850874, acc: 0.99863201379776)
[2025-02-13 03:24:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:02][root][INFO] - Training Epoch: 1/2, step 6274/7134 completed (loss: 0.019183579832315445, acc: 0.9942938685417175)
[2025-02-13 03:24:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:02][root][INFO] - Training Epoch: 1/2, step 6275/7134 completed (loss: 0.036237262189388275, acc: 0.9939117431640625)
[2025-02-13 03:24:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:03][root][INFO] - Training Epoch: 1/2, step 6276/7134 completed (loss: 0.023215070366859436, acc: 0.9894921183586121)
[2025-02-13 03:24:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:03][root][INFO] - Training Epoch: 1/2, step 6277/7134 completed (loss: 0.07911591231822968, acc: 0.9751130938529968)
[2025-02-13 03:24:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:04][root][INFO] - Training Epoch: 1/2, step 6278/7134 completed (loss: 0.009906473569571972, acc: 0.9967105388641357)
[2025-02-13 03:24:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:04][root][INFO] - Training Epoch: 1/2, step 6279/7134 completed (loss: 0.01650836132466793, acc: 0.9897260069847107)
[2025-02-13 03:24:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:04][root][INFO] - Training Epoch: 1/2, step 6280/7134 completed (loss: 0.02262541837990284, acc: 0.9930192232131958)
[2025-02-13 03:24:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:05][root][INFO] - Training Epoch: 1/2, step 6281/7134 completed (loss: 0.02960149198770523, acc: 0.9930675625801086)
[2025-02-13 03:24:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:05][root][INFO] - Training Epoch: 1/2, step 6282/7134 completed (loss: 0.01255572959780693, acc: 0.9968553185462952)
[2025-02-13 03:24:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:06][root][INFO] - Training Epoch: 1/2, step 6283/7134 completed (loss: 0.038629721850156784, acc: 0.9912663698196411)
[2025-02-13 03:24:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:06][root][INFO] - Training Epoch: 1/2, step 6284/7134 completed (loss: 0.0506705678999424, acc: 0.9803063273429871)
[2025-02-13 03:24:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:06][root][INFO] - Training Epoch: 1/2, step 6285/7134 completed (loss: 0.02845601737499237, acc: 0.9921259880065918)
[2025-02-13 03:24:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:07][root][INFO] - Training Epoch: 1/2, step 6286/7134 completed (loss: 0.01759517937898636, acc: 0.9950658082962036)
[2025-02-13 03:24:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:07][root][INFO] - Training Epoch: 1/2, step 6287/7134 completed (loss: 0.04443981125950813, acc: 0.9871323704719543)
[2025-02-13 03:24:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:08][root][INFO] - Training Epoch: 1/2, step 6288/7134 completed (loss: 0.0079498877748847, acc: 0.9967319965362549)
[2025-02-13 03:24:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:08][root][INFO] - Training Epoch: 1/2, step 6289/7134 completed (loss: 0.022079868242144585, acc: 0.9945945739746094)
[2025-02-13 03:24:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:08][root][INFO] - Training Epoch: 1/2, step 6290/7134 completed (loss: 0.02648019790649414, acc: 0.9904000163078308)
[2025-02-13 03:24:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:09][root][INFO] - Training Epoch: 1/2, step 6291/7134 completed (loss: 0.02476516179740429, acc: 0.9909090995788574)
[2025-02-13 03:24:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:09][root][INFO] - Training Epoch: 1/2, step 6292/7134 completed (loss: 0.04248299449682236, acc: 0.9863325953483582)
[2025-02-13 03:24:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:10][root][INFO] - Training Epoch: 1/2, step 6293/7134 completed (loss: 0.055311065167188644, acc: 0.9812286496162415)
[2025-02-13 03:24:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:10][root][INFO] - Training Epoch: 1/2, step 6294/7134 completed (loss: 0.027005165815353394, acc: 0.990275502204895)
[2025-02-13 03:24:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:11][root][INFO] - Training Epoch: 1/2, step 6295/7134 completed (loss: 0.022905990481376648, acc: 0.9909583926200867)
[2025-02-13 03:24:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:11][root][INFO] - Training Epoch: 1/2, step 6296/7134 completed (loss: 0.06238004192709923, acc: 0.9822834730148315)
[2025-02-13 03:24:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:11][root][INFO] - Training Epoch: 1/2, step 6297/7134 completed (loss: 0.02898205630481243, acc: 0.9901639223098755)
[2025-02-13 03:24:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:12][root][INFO] - Training Epoch: 1/2, step 6298/7134 completed (loss: 0.045958977192640305, acc: 0.9879724979400635)
[2025-02-13 03:24:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:12][root][INFO] - Training Epoch: 1/2, step 6299/7134 completed (loss: 0.05681585147976875, acc: 0.9840255379676819)
[2025-02-13 03:24:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:13][root][INFO] - Training Epoch: 1/2, step 6300/7134 completed (loss: 0.048539161682128906, acc: 0.9865996837615967)
[2025-02-13 03:24:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:13][root][INFO] - Training Epoch: 1/2, step 6301/7134 completed (loss: 0.013522383756935596, acc: 0.9965517520904541)
[2025-02-13 03:24:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:14][root][INFO] - Training Epoch: 1/2, step 6302/7134 completed (loss: 0.015948880463838577, acc: 0.9944751262664795)
[2025-02-13 03:24:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:14][root][INFO] - Training Epoch: 1/2, step 6303/7134 completed (loss: 0.011996082961559296, acc: 0.9968051314353943)
[2025-02-13 03:24:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:14][root][INFO] - Training Epoch: 1/2, step 6304/7134 completed (loss: 0.021329209208488464, acc: 0.995945930480957)
[2025-02-13 03:24:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:15][root][INFO] - Training Epoch: 1/2, step 6305/7134 completed (loss: 0.015087428502738476, acc: 0.9967266917228699)
[2025-02-13 03:24:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:15][root][INFO] - Training Epoch: 1/2, step 6306/7134 completed (loss: 0.04216233640909195, acc: 0.9879952073097229)
[2025-02-13 03:24:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:16][root][INFO] - Training Epoch: 1/2, step 6307/7134 completed (loss: 0.05455901101231575, acc: 0.9876543283462524)
[2025-02-13 03:24:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:16][root][INFO] - Training Epoch: 1/2, step 6308/7134 completed (loss: 0.025647088885307312, acc: 0.9938575029373169)
[2025-02-13 03:24:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:17][root][INFO] - Training Epoch: 1/2, step 6309/7134 completed (loss: 0.018090611323714256, acc: 0.9904000163078308)
[2025-02-13 03:24:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:17][root][INFO] - Training Epoch: 1/2, step 6310/7134 completed (loss: 0.0873463824391365, acc: 0.9824175834655762)
[2025-02-13 03:24:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:17][root][INFO] - Training Epoch: 1/2, step 6311/7134 completed (loss: 0.04526766017079353, acc: 0.9864698648452759)
[2025-02-13 03:24:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:18][root][INFO] - Training Epoch: 1/2, step 6312/7134 completed (loss: 0.040253836661577225, acc: 0.9889937043190002)
[2025-02-13 03:24:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:18][root][INFO] - Training Epoch: 1/2, step 6313/7134 completed (loss: 0.010372626595199108, acc: 0.9975460171699524)
[2025-02-13 03:24:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:19][root][INFO] - Training Epoch: 1/2, step 6314/7134 completed (loss: 0.015772970393300056, acc: 0.9987389445304871)
[2025-02-13 03:24:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:19][root][INFO] - Training Epoch: 1/2, step 6315/7134 completed (loss: 0.0403633639216423, acc: 0.9901639223098755)
[2025-02-13 03:24:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:20][root][INFO] - Training Epoch: 1/2, step 6316/7134 completed (loss: 0.03261525556445122, acc: 0.990275502204895)
[2025-02-13 03:24:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:20][root][INFO] - Training Epoch: 1/2, step 6317/7134 completed (loss: 0.015443401411175728, acc: 0.9967690110206604)
[2025-02-13 03:24:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:21][root][INFO] - Training Epoch: 1/2, step 6318/7134 completed (loss: 0.014602098613977432, acc: 0.995230495929718)
[2025-02-13 03:24:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:21][root][INFO] - Training Epoch: 1/2, step 6319/7134 completed (loss: 0.009635889902710915, acc: 0.9969135522842407)
[2025-02-13 03:24:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:21][root][INFO] - Training Epoch: 1/2, step 6320/7134 completed (loss: 0.014458568766713142, acc: 0.9952606558799744)
[2025-02-13 03:24:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:22][root][INFO] - Training Epoch: 1/2, step 6321/7134 completed (loss: 0.04223620519042015, acc: 0.9910714030265808)
[2025-02-13 03:24:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:22][root][INFO] - Training Epoch: 1/2, step 6322/7134 completed (loss: 0.030935246497392654, acc: 0.9941860437393188)
[2025-02-13 03:24:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:23][root][INFO] - Training Epoch: 1/2, step 6323/7134 completed (loss: 0.02961897663772106, acc: 0.9879879951477051)
[2025-02-13 03:24:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:23][root][INFO] - Training Epoch: 1/2, step 6324/7134 completed (loss: 0.023101337254047394, acc: 0.9920106530189514)
[2025-02-13 03:24:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:24][root][INFO] - Training Epoch: 1/2, step 6325/7134 completed (loss: 0.024515720084309578, acc: 0.99245285987854)
[2025-02-13 03:24:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:24][root][INFO] - Training Epoch: 1/2, step 6326/7134 completed (loss: 0.042775075882673264, acc: 0.9855538010597229)
[2025-02-13 03:24:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:25][root][INFO] - Training Epoch: 1/2, step 6327/7134 completed (loss: 0.010635999962687492, acc: 0.9973614811897278)
[2025-02-13 03:24:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:25][root][INFO] - Training Epoch: 1/2, step 6328/7134 completed (loss: 0.021679863333702087, acc: 0.9936203956604004)
[2025-02-13 03:24:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:25][root][INFO] - Training Epoch: 1/2, step 6329/7134 completed (loss: 0.024659784510731697, acc: 0.9939393997192383)
[2025-02-13 03:24:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:26][root][INFO] - Training Epoch: 1/2, step 6330/7134 completed (loss: 0.02885420434176922, acc: 0.9900744557380676)
[2025-02-13 03:24:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:26][root][INFO] - Training Epoch: 1/2, step 6331/7134 completed (loss: 0.016956869512796402, acc: 0.9964664578437805)
[2025-02-13 03:24:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:27][root][INFO] - Training Epoch: 1/2, step 6332/7134 completed (loss: 0.05983727052807808, acc: 0.9832713603973389)
[2025-02-13 03:24:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:27][root][INFO] - Training Epoch: 1/2, step 6333/7134 completed (loss: 0.036032043397426605, acc: 0.9841269850730896)
[2025-02-13 03:24:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:27][root][INFO] - Training Epoch: 1/2, step 6334/7134 completed (loss: 0.04179201275110245, acc: 0.9886363744735718)
[2025-02-13 03:24:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:28][root][INFO] - Training Epoch: 1/2, step 6335/7134 completed (loss: 0.025369372218847275, acc: 0.9932885766029358)
[2025-02-13 03:24:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:28][root][INFO] - Training Epoch: 1/2, step 6336/7134 completed (loss: 0.040840499103069305, acc: 0.9885321259498596)
[2025-02-13 03:24:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:29][root][INFO] - Training Epoch: 1/2, step 6337/7134 completed (loss: 0.01363922469317913, acc: 0.9927007555961609)
[2025-02-13 03:24:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:29][root][INFO] - Training Epoch: 1/2, step 6338/7134 completed (loss: 0.028399713337421417, acc: 0.9873617887496948)
[2025-02-13 03:24:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:30][root][INFO] - Training Epoch: 1/2, step 6339/7134 completed (loss: 0.04517094045877457, acc: 0.987889289855957)
[2025-02-13 03:24:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:30][root][INFO] - Training Epoch: 1/2, step 6340/7134 completed (loss: 0.016680795699357986, acc: 0.992668628692627)
[2025-02-13 03:24:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:30][root][INFO] - Training Epoch: 1/2, step 6341/7134 completed (loss: 0.01576261594891548, acc: 0.9981024861335754)
[2025-02-13 03:24:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:31][root][INFO] - Training Epoch: 1/2, step 6342/7134 completed (loss: 0.02319766767323017, acc: 0.9932705163955688)
[2025-02-13 03:24:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:31][root][INFO] - Training Epoch: 1/2, step 6343/7134 completed (loss: 0.026322385296225548, acc: 0.9913941621780396)
[2025-02-13 03:24:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:32][root][INFO] - Training Epoch: 1/2, step 6344/7134 completed (loss: 0.021052658557891846, acc: 0.99452805519104)
[2025-02-13 03:24:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:32][root][INFO] - Training Epoch: 1/2, step 6345/7134 completed (loss: 0.026250263676047325, acc: 0.9904371500015259)
[2025-02-13 03:24:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:33][root][INFO] - Training Epoch: 1/2, step 6346/7134 completed (loss: 0.060766562819480896, acc: 0.9798761606216431)
[2025-02-13 03:24:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:33][root][INFO] - Training Epoch: 1/2, step 6347/7134 completed (loss: 0.04126439616084099, acc: 0.9891774654388428)
[2025-02-13 03:24:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:33][root][INFO] - Training Epoch: 1/2, step 6348/7134 completed (loss: 0.056285884231328964, acc: 0.9809027910232544)
[2025-02-13 03:24:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:34][root][INFO] - Training Epoch: 1/2, step 6349/7134 completed (loss: 0.033350251615047455, acc: 0.9872495532035828)
[2025-02-13 03:24:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:34][root][INFO] - Training Epoch: 1/2, step 6350/7134 completed (loss: 0.038544025272130966, acc: 0.9891156554222107)
[2025-02-13 03:24:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:35][root][INFO] - Training Epoch: 1/2, step 6351/7134 completed (loss: 0.054155610501766205, acc: 0.983582079410553)
[2025-02-13 03:24:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:35][root][INFO] - Training Epoch: 1/2, step 6352/7134 completed (loss: 0.12626679241657257, acc: 0.96879643201828)
[2025-02-13 03:24:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:36][root][INFO] - Training Epoch: 1/2, step 6353/7134 completed (loss: 0.03741117939352989, acc: 0.9882869720458984)
[2025-02-13 03:24:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:36][root][INFO] - Training Epoch: 1/2, step 6354/7134 completed (loss: 0.024253826588392258, acc: 0.9930747747421265)
[2025-02-13 03:24:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:36][root][INFO] - Training Epoch: 1/2, step 6355/7134 completed (loss: 0.02269025519490242, acc: 0.9902912378311157)
[2025-02-13 03:24:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:37][root][INFO] - Training Epoch: 1/2, step 6356/7134 completed (loss: 0.03601514548063278, acc: 0.9895366430282593)
[2025-02-13 03:24:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:37][root][INFO] - Training Epoch: 1/2, step 6357/7134 completed (loss: 0.011455219238996506, acc: 0.9972222447395325)
[2025-02-13 03:24:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:38][root][INFO] - Training Epoch: 1/2, step 6358/7134 completed (loss: 0.023012246936559677, acc: 0.9943661689758301)
[2025-02-13 03:24:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:38][root][INFO] - Training Epoch: 1/2, step 6359/7134 completed (loss: 0.04665840044617653, acc: 0.9873949289321899)
[2025-02-13 03:24:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:38][root][INFO] - Training Epoch: 1/2, step 6360/7134 completed (loss: 0.026725267991423607, acc: 0.9920381903648376)
[2025-02-13 03:24:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:39][root][INFO] - Training Epoch: 1/2, step 6361/7134 completed (loss: 0.03619664907455444, acc: 0.989051103591919)
[2025-02-13 03:24:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:39][root][INFO] - Training Epoch: 1/2, step 6362/7134 completed (loss: 0.03039289079606533, acc: 0.9923780560493469)
[2025-02-13 03:24:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:40][root][INFO] - Training Epoch: 1/2, step 6363/7134 completed (loss: 0.04133991897106171, acc: 0.9882698059082031)
[2025-02-13 03:24:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:40][root][INFO] - Training Epoch: 1/2, step 6364/7134 completed (loss: 0.01517034973949194, acc: 0.995708167552948)
[2025-02-13 03:24:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:41][root][INFO] - Training Epoch: 1/2, step 6365/7134 completed (loss: 0.023911751806735992, acc: 0.9939024448394775)
[2025-02-13 03:24:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:41][root][INFO] - Training Epoch: 1/2, step 6366/7134 completed (loss: 0.030652929097414017, acc: 0.9907529950141907)
[2025-02-13 03:24:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:41][root][INFO] - Training Epoch: 1/2, step 6367/7134 completed (loss: 0.04888768121600151, acc: 0.9867674708366394)
[2025-02-13 03:24:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:42][root][INFO] - Training Epoch: 1/2, step 6368/7134 completed (loss: 0.06833919882774353, acc: 0.9858757257461548)
[2025-02-13 03:24:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:42][root][INFO] - Training Epoch: 1/2, step 6369/7134 completed (loss: 0.05272047966718674, acc: 0.9842312932014465)
[2025-02-13 03:24:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:43][root][INFO] - Training Epoch: 1/2, step 6370/7134 completed (loss: 0.12077256292104721, acc: 0.9696570038795471)
[2025-02-13 03:24:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:43][root][INFO] - Training Epoch: 1/2, step 6371/7134 completed (loss: 0.05171610787510872, acc: 0.9861496090888977)
[2025-02-13 03:24:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:44][root][INFO] - Training Epoch: 1/2, step 6372/7134 completed (loss: 0.1466878205537796, acc: 0.9577735066413879)
[2025-02-13 03:24:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:44][root][INFO] - Training Epoch: 1/2, step 6373/7134 completed (loss: 0.07836772501468658, acc: 0.9744991064071655)
[2025-02-13 03:24:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:44][root][INFO] - Training Epoch: 1/2, step 6374/7134 completed (loss: 0.023439666256308556, acc: 0.9937759041786194)
[2025-02-13 03:24:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:45][root][INFO] - Training Epoch: 1/2, step 6375/7134 completed (loss: 0.055857058614492416, acc: 0.9858793616294861)
[2025-02-13 03:24:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:45][root][INFO] - Training Epoch: 1/2, step 6376/7134 completed (loss: 0.017256014049053192, acc: 0.9944649338722229)
[2025-02-13 03:24:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:46][root][INFO] - Training Epoch: 1/2, step 6377/7134 completed (loss: 0.031132463365793228, acc: 0.9890109896659851)
[2025-02-13 03:24:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:46][root][INFO] - Training Epoch: 1/2, step 6378/7134 completed (loss: 0.050928641110658646, acc: 0.9817792177200317)
[2025-02-13 03:24:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:47][root][INFO] - Training Epoch: 1/2, step 6379/7134 completed (loss: 0.05276087298989296, acc: 0.9873096346855164)
[2025-02-13 03:24:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:47][root][INFO] - Training Epoch: 1/2, step 6380/7134 completed (loss: 0.08472923934459686, acc: 0.9835329055786133)
[2025-02-13 03:24:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:48][root][INFO] - Training Epoch: 1/2, step 6381/7134 completed (loss: 0.02141888067126274, acc: 0.9938931465148926)
[2025-02-13 03:24:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:48][root][INFO] - Training Epoch: 1/2, step 6382/7134 completed (loss: 0.024771086871623993, acc: 0.9973718523979187)
[2025-02-13 03:24:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:48][root][INFO] - Training Epoch: 1/2, step 6383/7134 completed (loss: 0.05905887857079506, acc: 0.9905020594596863)
[2025-02-13 03:24:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:49][root][INFO] - Training Epoch: 1/2, step 6384/7134 completed (loss: 0.024542009457945824, acc: 0.9955157041549683)
[2025-02-13 03:24:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:49][root][INFO] - Training Epoch: 1/2, step 6385/7134 completed (loss: 0.05865619704127312, acc: 0.9892215728759766)
[2025-02-13 03:24:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:50][root][INFO] - Training Epoch: 1/2, step 6386/7134 completed (loss: 0.02575097419321537, acc: 0.9938949942588806)
[2025-02-13 03:24:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:50][root][INFO] - Training Epoch: 1/2, step 6387/7134 completed (loss: 0.0481683686375618, acc: 0.9870967864990234)
[2025-02-13 03:24:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:51][root][INFO] - Training Epoch: 1/2, step 6388/7134 completed (loss: 0.04660523682832718, acc: 0.9895209670066833)
[2025-02-13 03:24:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:51][root][INFO] - Training Epoch: 1/2, step 6389/7134 completed (loss: 0.050318822264671326, acc: 0.9853479862213135)
[2025-02-13 03:24:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:52][root][INFO] - Training Epoch: 1/2, step 6390/7134 completed (loss: 0.060831476002931595, acc: 0.98886638879776)
[2025-02-13 03:24:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:52][root][INFO] - Training Epoch: 1/2, step 6391/7134 completed (loss: 0.07253199070692062, acc: 0.980793833732605)
[2025-02-13 03:24:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:52][root][INFO] - Training Epoch: 1/2, step 6392/7134 completed (loss: 0.06545748561620712, acc: 0.983589768409729)
[2025-02-13 03:24:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:53][root][INFO] - Training Epoch: 1/2, step 6393/7134 completed (loss: 0.033870551735162735, acc: 0.9897611141204834)
[2025-02-13 03:24:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:53][root][INFO] - Training Epoch: 1/2, step 6394/7134 completed (loss: 0.0750955194234848, acc: 0.9833101630210876)
[2025-02-13 03:24:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:54][root][INFO] - Training Epoch: 1/2, step 6395/7134 completed (loss: 0.042594410479068756, acc: 0.9889298677444458)
[2025-02-13 03:24:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:54][root][INFO] - Training Epoch: 1/2, step 6396/7134 completed (loss: 0.05222589895129204, acc: 0.9908257126808167)
[2025-02-13 03:24:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:55][root][INFO] - Training Epoch: 1/2, step 6397/7134 completed (loss: 0.021127842366695404, acc: 0.9937965273857117)
[2025-02-13 03:24:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:55][root][INFO] - Training Epoch: 1/2, step 6398/7134 completed (loss: 0.035881418734788895, acc: 0.9915611743927002)
[2025-02-13 03:24:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:56][root][INFO] - Training Epoch: 1/2, step 6399/7134 completed (loss: 0.05770384520292282, acc: 0.9887217879295349)
[2025-02-13 03:24:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:56][root][INFO] - Training Epoch: 1/2, step 6400/7134 completed (loss: 0.024849267676472664, acc: 0.994301974773407)
[2025-02-13 03:24:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:57][root][INFO] - Training Epoch: 1/2, step 6401/7134 completed (loss: 0.048628587275743484, acc: 0.9869706630706787)
[2025-02-13 03:24:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:57][root][INFO] - Training Epoch: 1/2, step 6402/7134 completed (loss: 0.04578172788023949, acc: 0.9920544624328613)
[2025-02-13 03:24:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:57][root][INFO] - Training Epoch: 1/2, step 6403/7134 completed (loss: 0.035859137773513794, acc: 0.9897959232330322)
[2025-02-13 03:24:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:58][root][INFO] - Training Epoch: 1/2, step 6404/7134 completed (loss: 0.019123904407024384, acc: 0.9916550517082214)
[2025-02-13 03:24:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:58][root][INFO] - Training Epoch: 1/2, step 6405/7134 completed (loss: 0.017471063882112503, acc: 0.9956989288330078)
[2025-02-13 03:24:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:59][root][INFO] - Training Epoch: 1/2, step 6406/7134 completed (loss: 0.04278627038002014, acc: 0.986316978931427)
[2025-02-13 03:24:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:24:59][root][INFO] - Training Epoch: 1/2, step 6407/7134 completed (loss: 0.0163332000374794, acc: 0.9960578083992004)
[2025-02-13 03:24:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:00][root][INFO] - Training Epoch: 1/2, step 6408/7134 completed (loss: 0.04374032840132713, acc: 0.9911373853683472)
[2025-02-13 03:25:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:00][root][INFO] - Training Epoch: 1/2, step 6409/7134 completed (loss: 0.03480910509824753, acc: 0.9947368502616882)
[2025-02-13 03:25:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:01][root][INFO] - Training Epoch: 1/2, step 6410/7134 completed (loss: 0.015844443812966347, acc: 0.9926470518112183)
[2025-02-13 03:25:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:01][root][INFO] - Training Epoch: 1/2, step 6411/7134 completed (loss: 0.0374826081097126, acc: 0.9916247725486755)
[2025-02-13 03:25:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:01][root][INFO] - Training Epoch: 1/2, step 6412/7134 completed (loss: 0.02192988619208336, acc: 0.9933333396911621)
[2025-02-13 03:25:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:02][root][INFO] - Training Epoch: 1/2, step 6413/7134 completed (loss: 0.04052587226033211, acc: 0.9880136847496033)
[2025-02-13 03:25:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:02][root][INFO] - Training Epoch: 1/2, step 6414/7134 completed (loss: 0.012912831269204617, acc: 0.995468258857727)
[2025-02-13 03:25:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:03][root][INFO] - Training Epoch: 1/2, step 6415/7134 completed (loss: 0.01893259398639202, acc: 0.9935815334320068)
[2025-02-13 03:25:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:03][root][INFO] - Training Epoch: 1/2, step 6416/7134 completed (loss: 0.052421871572732925, acc: 0.9810246825218201)
[2025-02-13 03:25:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:04][root][INFO] - Training Epoch: 1/2, step 6417/7134 completed (loss: 0.07293517887592316, acc: 0.9841827750205994)
[2025-02-13 03:25:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:04][root][INFO] - Training Epoch: 1/2, step 6418/7134 completed (loss: 0.09627337753772736, acc: 0.9747191071510315)
[2025-02-13 03:25:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:04][root][INFO] - Training Epoch: 1/2, step 6419/7134 completed (loss: 0.06232726201415062, acc: 0.9836868047714233)
[2025-02-13 03:25:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:05][root][INFO] - Training Epoch: 1/2, step 6420/7134 completed (loss: 0.03778247535228729, acc: 0.9884726405143738)
[2025-02-13 03:25:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:05][root][INFO] - Training Epoch: 1/2, step 6421/7134 completed (loss: 0.03950297832489014, acc: 0.9910846948623657)
[2025-02-13 03:25:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:06][root][INFO] - Training Epoch: 1/2, step 6422/7134 completed (loss: 0.04042806476354599, acc: 0.9917920827865601)
[2025-02-13 03:25:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:06][root][INFO] - Training Epoch: 1/2, step 6423/7134 completed (loss: 0.05721672251820564, acc: 0.9882698059082031)
[2025-02-13 03:25:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:07][root][INFO] - Training Epoch: 1/2, step 6424/7134 completed (loss: 0.046185459941625595, acc: 0.9861286282539368)
[2025-02-13 03:25:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:07][root][INFO] - Training Epoch: 1/2, step 6425/7134 completed (loss: 0.038833845406770706, acc: 0.9895697236061096)
[2025-02-13 03:25:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:07][root][INFO] - Training Epoch: 1/2, step 6426/7134 completed (loss: 0.03674108162522316, acc: 0.9881266355514526)
[2025-02-13 03:25:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:08][root][INFO] - Training Epoch: 1/2, step 6427/7134 completed (loss: 0.04333486780524254, acc: 0.9886363744735718)
[2025-02-13 03:25:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:08][root][INFO] - Training Epoch: 1/2, step 6428/7134 completed (loss: 0.020712794736027718, acc: 0.9916201233863831)
[2025-02-13 03:25:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:09][root][INFO] - Training Epoch: 1/2, step 6429/7134 completed (loss: 0.014368122443556786, acc: 0.9969696998596191)
[2025-02-13 03:25:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:09][root][INFO] - Training Epoch: 1/2, step 6430/7134 completed (loss: 0.054350197315216064, acc: 0.9856114983558655)
[2025-02-13 03:25:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:10][root][INFO] - Training Epoch: 1/2, step 6431/7134 completed (loss: 0.05554583668708801, acc: 0.9868420958518982)
[2025-02-13 03:25:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:10][root][INFO] - Training Epoch: 1/2, step 6432/7134 completed (loss: 0.19536009430885315, acc: 0.9545454382896423)
[2025-02-13 03:25:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:10][root][INFO] - Training Epoch: 1/2, step 6433/7134 completed (loss: 0.04642403870820999, acc: 0.9877049326896667)
[2025-02-13 03:25:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:11][root][INFO] - Training Epoch: 1/2, step 6434/7134 completed (loss: 0.011959586292505264, acc: 0.9960052967071533)
[2025-02-13 03:25:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:11][root][INFO] - Training Epoch: 1/2, step 6435/7134 completed (loss: 0.040683966130018234, acc: 0.9896013736724854)
[2025-02-13 03:25:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:12][root][INFO] - Training Epoch: 1/2, step 6436/7134 completed (loss: 0.013971401378512383, acc: 0.9969087839126587)
[2025-02-13 03:25:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:12][root][INFO] - Training Epoch: 1/2, step 6437/7134 completed (loss: 0.038637448102235794, acc: 0.9950000047683716)
[2025-02-13 03:25:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:13][root][INFO] - Training Epoch: 1/2, step 6438/7134 completed (loss: 0.03986337408423424, acc: 0.9876390695571899)
[2025-02-13 03:25:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:13][root][INFO] - Training Epoch: 1/2, step 6439/7134 completed (loss: 0.04741997271776199, acc: 0.9858267903327942)
[2025-02-13 03:25:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:14][root][INFO] - Training Epoch: 1/2, step 6440/7134 completed (loss: 0.03251883015036583, acc: 0.9892473220825195)
[2025-02-13 03:25:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:14][root][INFO] - Training Epoch: 1/2, step 6441/7134 completed (loss: 0.019388092681765556, acc: 0.9955686926841736)
[2025-02-13 03:25:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:14][root][INFO] - Training Epoch: 1/2, step 6442/7134 completed (loss: 0.043743330985307693, acc: 0.9920544624328613)
[2025-02-13 03:25:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:15][root][INFO] - Training Epoch: 1/2, step 6443/7134 completed (loss: 0.01922636106610298, acc: 0.9950494766235352)
[2025-02-13 03:25:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:15][root][INFO] - Training Epoch: 1/2, step 6444/7134 completed (loss: 0.045808084309101105, acc: 0.9846368432044983)
[2025-02-13 03:25:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:16][root][INFO] - Training Epoch: 1/2, step 6445/7134 completed (loss: 0.05952731892466545, acc: 0.9889025688171387)
[2025-02-13 03:25:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:16][root][INFO] - Training Epoch: 1/2, step 6446/7134 completed (loss: 0.02646123431622982, acc: 0.9928486347198486)
[2025-02-13 03:25:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:17][root][INFO] - Training Epoch: 1/2, step 6447/7134 completed (loss: 0.04058905318379402, acc: 0.9896432757377625)
[2025-02-13 03:25:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:17][root][INFO] - Training Epoch: 1/2, step 6448/7134 completed (loss: 0.08552655577659607, acc: 0.9735202789306641)
[2025-02-13 03:25:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:18][root][INFO] - Training Epoch: 1/2, step 6449/7134 completed (loss: 0.029664840549230576, acc: 0.9942029118537903)
[2025-02-13 03:25:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:18][root][INFO] - Training Epoch: 1/2, step 6450/7134 completed (loss: 0.02163289673626423, acc: 0.9955357313156128)
[2025-02-13 03:25:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:19][root][INFO] - Training Epoch: 1/2, step 6451/7134 completed (loss: 0.05722302570939064, acc: 0.984329104423523)
[2025-02-13 03:25:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:19][root][INFO] - Training Epoch: 1/2, step 6452/7134 completed (loss: 0.014812611043453217, acc: 0.994425892829895)
[2025-02-13 03:25:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:20][root][INFO] - Training Epoch: 1/2, step 6453/7134 completed (loss: 0.02537892945110798, acc: 0.9919354915618896)
[2025-02-13 03:25:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:20][root][INFO] - Training Epoch: 1/2, step 6454/7134 completed (loss: 0.028128156438469887, acc: 0.9905771613121033)
[2025-02-13 03:25:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:20][root][INFO] - Training Epoch: 1/2, step 6455/7134 completed (loss: 0.015064938925206661, acc: 0.9966044425964355)
[2025-02-13 03:25:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:21][root][INFO] - Training Epoch: 1/2, step 6456/7134 completed (loss: 0.02584245055913925, acc: 0.995398759841919)
[2025-02-13 03:25:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:21][root][INFO] - Training Epoch: 1/2, step 6457/7134 completed (loss: 0.019883790984749794, acc: 0.9977973699569702)
[2025-02-13 03:25:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:22][root][INFO] - Training Epoch: 1/2, step 6458/7134 completed (loss: 0.024484310299158096, acc: 0.9899159669876099)
[2025-02-13 03:25:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:22][root][INFO] - Training Epoch: 1/2, step 6459/7134 completed (loss: 0.0029689359944313765, acc: 1.0)
[2025-02-13 03:25:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:23][root][INFO] - Training Epoch: 1/2, step 6460/7134 completed (loss: 0.009828793816268444, acc: 0.996458113193512)
[2025-02-13 03:25:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:23][root][INFO] - Training Epoch: 1/2, step 6461/7134 completed (loss: 0.024461369961500168, acc: 0.9921156167984009)
[2025-02-13 03:25:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:23][root][INFO] - Training Epoch: 1/2, step 6462/7134 completed (loss: 0.017997514456510544, acc: 0.9926199316978455)
[2025-02-13 03:25:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:24][root][INFO] - Training Epoch: 1/2, step 6463/7134 completed (loss: 0.01716819778084755, acc: 0.9954441785812378)
[2025-02-13 03:25:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:24][root][INFO] - Training Epoch: 1/2, step 6464/7134 completed (loss: 0.023830149322748184, acc: 0.9921976327896118)
[2025-02-13 03:25:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:25][root][INFO] - Training Epoch: 1/2, step 6465/7134 completed (loss: 0.022514553740620613, acc: 0.9935732483863831)
[2025-02-13 03:25:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:25][root][INFO] - Training Epoch: 1/2, step 6466/7134 completed (loss: 0.026646047830581665, acc: 0.9947159886360168)
[2025-02-13 03:25:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:26][root][INFO] - Training Epoch: 1/2, step 6467/7134 completed (loss: 0.09031414240598679, acc: 0.981679379940033)
[2025-02-13 03:25:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:26][root][INFO] - Training Epoch: 1/2, step 6468/7134 completed (loss: 0.025316977873444557, acc: 0.9943714737892151)
[2025-02-13 03:25:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:27][root][INFO] - Training Epoch: 1/2, step 6469/7134 completed (loss: 0.041993990540504456, acc: 0.9853836894035339)
[2025-02-13 03:25:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:27][root][INFO] - Training Epoch: 1/2, step 6470/7134 completed (loss: 0.04678509011864662, acc: 0.9853300452232361)
[2025-02-13 03:25:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:27][root][INFO] - Training Epoch: 1/2, step 6471/7134 completed (loss: 0.01333700306713581, acc: 0.9967426657676697)
[2025-02-13 03:25:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:28][root][INFO] - Training Epoch: 1/2, step 6472/7134 completed (loss: 0.05438260734081268, acc: 0.9858155846595764)
[2025-02-13 03:25:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:28][root][INFO] - Training Epoch: 1/2, step 6473/7134 completed (loss: 0.014963698573410511, acc: 0.9946380853652954)
[2025-02-13 03:25:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:29][root][INFO] - Training Epoch: 1/2, step 6474/7134 completed (loss: 0.01248891744762659, acc: 0.9973081946372986)
[2025-02-13 03:25:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:29][root][INFO] - Training Epoch: 1/2, step 6475/7134 completed (loss: 0.0477694496512413, acc: 0.9883333444595337)
[2025-02-13 03:25:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:30][root][INFO] - Training Epoch: 1/2, step 6476/7134 completed (loss: 0.06072724238038063, acc: 0.9884488582611084)
[2025-02-13 03:25:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:30][root][INFO] - Training Epoch: 1/2, step 6477/7134 completed (loss: 0.019511932507157326, acc: 0.9957627058029175)
[2025-02-13 03:25:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:30][root][INFO] - Training Epoch: 1/2, step 6478/7134 completed (loss: 0.023103702813386917, acc: 0.9926199316978455)
[2025-02-13 03:25:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:31][root][INFO] - Training Epoch: 1/2, step 6479/7134 completed (loss: 0.02329651638865471, acc: 0.9916247725486755)
[2025-02-13 03:25:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:31][root][INFO] - Training Epoch: 1/2, step 6480/7134 completed (loss: 0.08307743072509766, acc: 0.9793814420700073)
[2025-02-13 03:25:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:32][root][INFO] - Training Epoch: 1/2, step 6481/7134 completed (loss: 0.00903661921620369, acc: 0.9948979616165161)
[2025-02-13 03:25:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:32][root][INFO] - Training Epoch: 1/2, step 6482/7134 completed (loss: 0.007281738333404064, acc: 1.0)
[2025-02-13 03:25:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:32][root][INFO] - Training Epoch: 1/2, step 6483/7134 completed (loss: 0.06260526925325394, acc: 0.9891696572303772)
[2025-02-13 03:25:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:33][root][INFO] - Training Epoch: 1/2, step 6484/7134 completed (loss: 0.03426472842693329, acc: 0.9912023544311523)
[2025-02-13 03:25:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:33][root][INFO] - Training Epoch: 1/2, step 6485/7134 completed (loss: 0.011266062036156654, acc: 0.9960474371910095)
[2025-02-13 03:25:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:34][root][INFO] - Training Epoch: 1/2, step 6486/7134 completed (loss: 0.12907159328460693, acc: 0.9745454788208008)
[2025-02-13 03:25:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:34][root][INFO] - Training Epoch: 1/2, step 6487/7134 completed (loss: 0.20399634540081024, acc: 0.9473684430122375)
[2025-02-13 03:25:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:34][root][INFO] - Training Epoch: 1/2, step 6488/7134 completed (loss: 0.08694039285182953, acc: 0.9657947421073914)
[2025-02-13 03:25:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:35][root][INFO] - Training Epoch: 1/2, step 6489/7134 completed (loss: 0.1162576898932457, acc: 0.9776119589805603)
[2025-02-13 03:25:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:35][root][INFO] - Training Epoch: 1/2, step 6490/7134 completed (loss: 0.06772493571043015, acc: 0.9845626354217529)
[2025-02-13 03:25:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:36][root][INFO] - Training Epoch: 1/2, step 6491/7134 completed (loss: 0.09617342799901962, acc: 0.9770580530166626)
[2025-02-13 03:25:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:36][root][INFO] - Training Epoch: 1/2, step 6492/7134 completed (loss: 0.03571220859885216, acc: 0.9896507263183594)
[2025-02-13 03:25:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:37][root][INFO] - Training Epoch: 1/2, step 6493/7134 completed (loss: 0.08576419204473495, acc: 0.9783715009689331)
[2025-02-13 03:25:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:37][root][INFO] - Training Epoch: 1/2, step 6494/7134 completed (loss: 0.045359961688518524, acc: 0.990774929523468)
[2025-02-13 03:25:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:37][root][INFO] - Training Epoch: 1/2, step 6495/7134 completed (loss: 0.054691851139068604, acc: 0.9817671775817871)
[2025-02-13 03:25:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:38][root][INFO] - Training Epoch: 1/2, step 6496/7134 completed (loss: 0.04279563948512077, acc: 0.988664984703064)
[2025-02-13 03:25:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:38][root][INFO] - Training Epoch: 1/2, step 6497/7134 completed (loss: 0.10442402958869934, acc: 0.9782270789146423)
[2025-02-13 03:25:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:39][root][INFO] - Training Epoch: 1/2, step 6498/7134 completed (loss: 0.029149681329727173, acc: 0.992682933807373)
[2025-02-13 03:25:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:39][root][INFO] - Training Epoch: 1/2, step 6499/7134 completed (loss: 0.04526888579130173, acc: 0.9900373816490173)
[2025-02-13 03:25:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:40][root][INFO] - Training Epoch: 1/2, step 6500/7134 completed (loss: 0.08634336292743683, acc: 0.9733893275260925)
[2025-02-13 03:25:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:40][root][INFO] - Training Epoch: 1/2, step 6501/7134 completed (loss: 0.14175094664096832, acc: 0.9680306911468506)
[2025-02-13 03:25:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:41][root][INFO] - Training Epoch: 1/2, step 6502/7134 completed (loss: 0.03553685545921326, acc: 0.9869358539581299)
[2025-02-13 03:25:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:41][root][INFO] - Training Epoch: 1/2, step 6503/7134 completed (loss: 0.04520126059651375, acc: 0.9897360801696777)
[2025-02-13 03:25:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:41][root][INFO] - Training Epoch: 1/2, step 6504/7134 completed (loss: 0.06321386992931366, acc: 0.9842022061347961)
[2025-02-13 03:25:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:42][root][INFO] - Training Epoch: 1/2, step 6505/7134 completed (loss: 0.09868727624416351, acc: 0.9724637866020203)
[2025-02-13 03:25:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:42][root][INFO] - Training Epoch: 1/2, step 6506/7134 completed (loss: 0.05755999684333801, acc: 0.9793702363967896)
[2025-02-13 03:25:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:43][root][INFO] - Training Epoch: 1/2, step 6507/7134 completed (loss: 0.0877944603562355, acc: 0.9788199663162231)
[2025-02-13 03:25:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:43][root][INFO] - Training Epoch: 1/2, step 6508/7134 completed (loss: 0.058196622878313065, acc: 0.9882628917694092)
[2025-02-13 03:25:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:44][root][INFO] - Training Epoch: 1/2, step 6509/7134 completed (loss: 0.039739347994327545, acc: 0.9878183603286743)
[2025-02-13 03:25:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:44][root][INFO] - Training Epoch: 1/2, step 6510/7134 completed (loss: 0.05511420965194702, acc: 0.9830867052078247)
[2025-02-13 03:25:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:45][root][INFO] - Training Epoch: 1/2, step 6511/7134 completed (loss: 0.06660173088312149, acc: 0.9849624037742615)
[2025-02-13 03:25:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:45][root][INFO] - Training Epoch: 1/2, step 6512/7134 completed (loss: 0.022272227331995964, acc: 0.9885931611061096)
[2025-02-13 03:25:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:45][root][INFO] - Training Epoch: 1/2, step 6513/7134 completed (loss: 0.04380110278725624, acc: 0.9830247163772583)
[2025-02-13 03:25:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:46][root][INFO] - Training Epoch: 1/2, step 6514/7134 completed (loss: 0.05225352197885513, acc: 0.9857954382896423)
[2025-02-13 03:25:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:46][root][INFO] - Training Epoch: 1/2, step 6515/7134 completed (loss: 0.061342500150203705, acc: 0.9835025668144226)
[2025-02-13 03:25:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:47][root][INFO] - Training Epoch: 1/2, step 6516/7134 completed (loss: 0.09727500379085541, acc: 0.970822274684906)
[2025-02-13 03:25:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:47][root][INFO] - Training Epoch: 1/2, step 6517/7134 completed (loss: 0.05004309490323067, acc: 0.9842105507850647)
[2025-02-13 03:25:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:48][root][INFO] - Training Epoch: 1/2, step 6518/7134 completed (loss: 0.024005301296710968, acc: 0.9958620667457581)
[2025-02-13 03:25:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:48][root][INFO] - Training Epoch: 1/2, step 6519/7134 completed (loss: 0.05632874369621277, acc: 0.9893617033958435)
[2025-02-13 03:25:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:49][root][INFO] - Training Epoch: 1/2, step 6520/7134 completed (loss: 0.020395508036017418, acc: 0.993779182434082)
[2025-02-13 03:25:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:49][root][INFO] - Training Epoch: 1/2, step 6521/7134 completed (loss: 0.015151179395616055, acc: 0.9947916865348816)
[2025-02-13 03:25:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:49][root][INFO] - Training Epoch: 1/2, step 6522/7134 completed (loss: 0.04174872860312462, acc: 0.9862778782844543)
[2025-02-13 03:25:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:50][root][INFO] - Training Epoch: 1/2, step 6523/7134 completed (loss: 0.07450155913829803, acc: 0.9691470265388489)
[2025-02-13 03:25:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:50][root][INFO] - Training Epoch: 1/2, step 6524/7134 completed (loss: 0.07086019217967987, acc: 0.9858611822128296)
[2025-02-13 03:25:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:51][root][INFO] - Training Epoch: 1/2, step 6525/7134 completed (loss: 0.018018122762441635, acc: 0.9959839582443237)
[2025-02-13 03:25:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:51][root][INFO] - Training Epoch: 1/2, step 6526/7134 completed (loss: 0.05121923238039017, acc: 0.9884763360023499)
[2025-02-13 03:25:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:52][root][INFO] - Training Epoch: 1/2, step 6527/7134 completed (loss: 0.03752068430185318, acc: 0.9925816059112549)
[2025-02-13 03:25:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:52][root][INFO] - Training Epoch: 1/2, step 6528/7134 completed (loss: 0.06607965379953384, acc: 0.987364649772644)
[2025-02-13 03:25:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:52][root][INFO] - Training Epoch: 1/2, step 6529/7134 completed (loss: 0.07089102268218994, acc: 0.9830827116966248)
[2025-02-13 03:25:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:53][root][INFO] - Training Epoch: 1/2, step 6530/7134 completed (loss: 0.030646605417132378, acc: 0.9900142550468445)
[2025-02-13 03:25:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:53][root][INFO] - Training Epoch: 1/2, step 6531/7134 completed (loss: 0.017104506492614746, acc: 0.9956647157669067)
[2025-02-13 03:25:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:54][root][INFO] - Training Epoch: 1/2, step 6532/7134 completed (loss: 0.04379390552639961, acc: 0.987484335899353)
[2025-02-13 03:25:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:54][root][INFO] - Training Epoch: 1/2, step 6533/7134 completed (loss: 0.019428182393312454, acc: 0.9895522594451904)
[2025-02-13 03:25:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:54][root][INFO] - Training Epoch: 1/2, step 6534/7134 completed (loss: 0.05421486124396324, acc: 0.9906166195869446)
[2025-02-13 03:25:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:55][root][INFO] - Training Epoch: 1/2, step 6535/7134 completed (loss: 0.040739960968494415, acc: 0.9882903695106506)
[2025-02-13 03:25:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:55][root][INFO] - Training Epoch: 1/2, step 6536/7134 completed (loss: 0.036793846637010574, acc: 0.9841269850730896)
[2025-02-13 03:25:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:56][root][INFO] - Training Epoch: 1/2, step 6537/7134 completed (loss: 0.07105434685945511, acc: 0.9668141603469849)
[2025-02-13 03:25:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:56][root][INFO] - Training Epoch: 1/2, step 6538/7134 completed (loss: 0.08385562896728516, acc: 0.9786856174468994)
[2025-02-13 03:25:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:56][root][INFO] - Training Epoch: 1/2, step 6539/7134 completed (loss: 0.013861033134162426, acc: 0.9969879388809204)
[2025-02-13 03:25:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:57][root][INFO] - Training Epoch: 1/2, step 6540/7134 completed (loss: 0.2784000039100647, acc: 0.9480874538421631)
[2025-02-13 03:25:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:57][root][INFO] - Training Epoch: 1/2, step 6541/7134 completed (loss: 0.07237124443054199, acc: 0.9791666865348816)
[2025-02-13 03:25:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:58][root][INFO] - Training Epoch: 1/2, step 6542/7134 completed (loss: 0.055955782532691956, acc: 0.9796609878540039)
[2025-02-13 03:25:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:58][root][INFO] - Training Epoch: 1/2, step 6543/7134 completed (loss: 0.032471224665641785, acc: 0.9895178079605103)
[2025-02-13 03:25:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:59][root][INFO] - Training Epoch: 1/2, step 6544/7134 completed (loss: 0.049699001014232635, acc: 0.9900596141815186)
[2025-02-13 03:25:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:59][root][INFO] - Training Epoch: 1/2, step 6545/7134 completed (loss: 0.02625843696296215, acc: 0.9962825179100037)
[2025-02-13 03:25:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:25:59][root][INFO] - Training Epoch: 1/2, step 6546/7134 completed (loss: 0.04001971706748009, acc: 0.990227997303009)
[2025-02-13 03:26:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:00][root][INFO] - Training Epoch: 1/2, step 6547/7134 completed (loss: 0.1140507236123085, acc: 0.9812606573104858)
[2025-02-13 03:26:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:00][root][INFO] - Training Epoch: 1/2, step 6548/7134 completed (loss: 0.018355103209614754, acc: 0.9940476417541504)
[2025-02-13 03:26:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:01][root][INFO] - Training Epoch: 1/2, step 6549/7134 completed (loss: 0.09199823439121246, acc: 0.9786407947540283)
[2025-02-13 03:26:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:01][root][INFO] - Training Epoch: 1/2, step 6550/7134 completed (loss: 0.08709310740232468, acc: 0.9815384745597839)
[2025-02-13 03:26:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:02][root][INFO] - Training Epoch: 1/2, step 6551/7134 completed (loss: 0.020726505666971207, acc: 0.9954441785812378)
[2025-02-13 03:26:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:02][root][INFO] - Training Epoch: 1/2, step 6552/7134 completed (loss: 0.028378041461110115, acc: 0.9950199127197266)
[2025-02-13 03:26:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:02][root][INFO] - Training Epoch: 1/2, step 6553/7134 completed (loss: 0.040129538625478745, acc: 0.9847328066825867)
[2025-02-13 03:26:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:03][root][INFO] - Training Epoch: 1/2, step 6554/7134 completed (loss: 0.03306237980723381, acc: 0.9945945739746094)
[2025-02-13 03:26:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:03][root][INFO] - Training Epoch: 1/2, step 6555/7134 completed (loss: 0.06100989133119583, acc: 0.9808853268623352)
[2025-02-13 03:26:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:04][root][INFO] - Training Epoch: 1/2, step 6556/7134 completed (loss: 0.040807489305734634, acc: 0.9884210228919983)
[2025-02-13 03:26:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:04][root][INFO] - Training Epoch: 1/2, step 6557/7134 completed (loss: 0.034444574266672134, acc: 0.9899193644523621)
[2025-02-13 03:26:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:05][root][INFO] - Training Epoch: 1/2, step 6558/7134 completed (loss: 0.049939874559640884, acc: 0.9869565367698669)
[2025-02-13 03:26:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:05][root][INFO] - Training Epoch: 1/2, step 6559/7134 completed (loss: 0.0326010063290596, acc: 0.9902912378311157)
[2025-02-13 03:26:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:06][root][INFO] - Training Epoch: 1/2, step 6560/7134 completed (loss: 0.030834652483463287, acc: 0.9904875159263611)
[2025-02-13 03:26:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:06][root][INFO] - Training Epoch: 1/2, step 6561/7134 completed (loss: 0.02198093943297863, acc: 0.9929824471473694)
[2025-02-13 03:26:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:07][root][INFO] - Training Epoch: 1/2, step 6562/7134 completed (loss: 0.053174588829278946, acc: 0.980567991733551)
[2025-02-13 03:26:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:07][root][INFO] - Training Epoch: 1/2, step 6563/7134 completed (loss: 0.05773315578699112, acc: 0.9825378060340881)
[2025-02-13 03:26:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:08][root][INFO] - Training Epoch: 1/2, step 6564/7134 completed (loss: 0.04244016483426094, acc: 0.9919028282165527)
[2025-02-13 03:26:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:08][root][INFO] - Training Epoch: 1/2, step 6565/7134 completed (loss: 0.05825074017047882, acc: 0.9830867052078247)
[2025-02-13 03:26:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:09][root][INFO] - Training Epoch: 1/2, step 6566/7134 completed (loss: 0.024533472955226898, acc: 0.9914529919624329)
[2025-02-13 03:26:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:09][root][INFO] - Training Epoch: 1/2, step 6567/7134 completed (loss: 0.023040683940052986, acc: 0.9908814430236816)
[2025-02-13 03:26:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:10][root][INFO] - Training Epoch: 1/2, step 6568/7134 completed (loss: 0.01645488478243351, acc: 0.996216893196106)
[2025-02-13 03:26:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:10][root][INFO] - Training Epoch: 1/2, step 6569/7134 completed (loss: 0.00949174351990223, acc: 0.995604395866394)
[2025-02-13 03:26:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:10][root][INFO] - Training Epoch: 1/2, step 6570/7134 completed (loss: 0.016851359978318214, acc: 0.9953542351722717)
[2025-02-13 03:26:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:11][root][INFO] - Training Epoch: 1/2, step 6571/7134 completed (loss: 0.02309390716254711, acc: 0.9934554696083069)
[2025-02-13 03:26:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:11][root][INFO] - Training Epoch: 1/2, step 6572/7134 completed (loss: 0.042998503893613815, acc: 0.9878048896789551)
[2025-02-13 03:26:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:12][root][INFO] - Training Epoch: 1/2, step 6573/7134 completed (loss: 0.021657655015587807, acc: 0.9920993447303772)
[2025-02-13 03:26:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:12][root][INFO] - Training Epoch: 1/2, step 6574/7134 completed (loss: 0.020119832828640938, acc: 0.9928425550460815)
[2025-02-13 03:26:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:13][root][INFO] - Training Epoch: 1/2, step 6575/7134 completed (loss: 0.03994770720601082, acc: 0.9867403507232666)
[2025-02-13 03:26:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:13][root][INFO] - Training Epoch: 1/2, step 6576/7134 completed (loss: 0.014761894010007381, acc: 0.9968487620353699)
[2025-02-13 03:26:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:14][root][INFO] - Training Epoch: 1/2, step 6577/7134 completed (loss: 0.036067258566617966, acc: 0.9859943985939026)
[2025-02-13 03:26:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:14][root][INFO] - Training Epoch: 1/2, step 6578/7134 completed (loss: 0.01944221556186676, acc: 0.9937722682952881)
[2025-02-13 03:26:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:15][root][INFO] - Training Epoch: 1/2, step 6579/7134 completed (loss: 0.042423367500305176, acc: 0.9872958064079285)
[2025-02-13 03:26:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:15][root][INFO] - Training Epoch: 1/2, step 6580/7134 completed (loss: 0.0161362886428833, acc: 0.9976470470428467)
[2025-02-13 03:26:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:16][root][INFO] - Training Epoch: 1/2, step 6581/7134 completed (loss: 0.026700347661972046, acc: 0.9913899302482605)
[2025-02-13 03:26:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:16][root][INFO] - Training Epoch: 1/2, step 6582/7134 completed (loss: 0.06488188356161118, acc: 0.9798561334609985)
[2025-02-13 03:26:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:16][root][INFO] - Training Epoch: 1/2, step 6583/7134 completed (loss: 0.021968867629766464, acc: 0.991990864276886)
[2025-02-13 03:26:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:17][root][INFO] - Training Epoch: 1/2, step 6584/7134 completed (loss: 0.03956823796033859, acc: 0.9889841079711914)
[2025-02-13 03:26:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:17][root][INFO] - Training Epoch: 1/2, step 6585/7134 completed (loss: 0.026497922837734222, acc: 0.9901315569877625)
[2025-02-13 03:26:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:18][root][INFO] - Training Epoch: 1/2, step 6586/7134 completed (loss: 0.029420755803585052, acc: 0.9922279715538025)
[2025-02-13 03:26:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:18][root][INFO] - Training Epoch: 1/2, step 6587/7134 completed (loss: 0.06906155496835709, acc: 0.9811320900917053)
[2025-02-13 03:26:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:19][root][INFO] - Training Epoch: 1/2, step 6588/7134 completed (loss: 0.044009141623973846, acc: 0.9911949634552002)
[2025-02-13 03:26:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:19][root][INFO] - Training Epoch: 1/2, step 6589/7134 completed (loss: 0.06970606744289398, acc: 0.9834024906158447)
[2025-02-13 03:26:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:19][root][INFO] - Training Epoch: 1/2, step 6590/7134 completed (loss: 0.03694994002580643, acc: 0.9871944189071655)
[2025-02-13 03:26:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:20][root][INFO] - Training Epoch: 1/2, step 6591/7134 completed (loss: 0.03752502426505089, acc: 0.9888476133346558)
[2025-02-13 03:26:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:20][root][INFO] - Training Epoch: 1/2, step 6592/7134 completed (loss: 0.0436132550239563, acc: 0.986379086971283)
[2025-02-13 03:26:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:21][root][INFO] - Training Epoch: 1/2, step 6593/7134 completed (loss: 0.07976635545492172, acc: 0.9847792983055115)
[2025-02-13 03:26:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:21][root][INFO] - Training Epoch: 1/2, step 6594/7134 completed (loss: 0.036360036581754684, acc: 0.9894366264343262)
[2025-02-13 03:26:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:22][root][INFO] - Training Epoch: 1/2, step 6595/7134 completed (loss: 0.05173085629940033, acc: 0.9881305694580078)
[2025-02-13 03:26:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:22][root][INFO] - Training Epoch: 1/2, step 6596/7134 completed (loss: 0.0343971848487854, acc: 0.9893778562545776)
[2025-02-13 03:26:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:23][root][INFO] - Training Epoch: 1/2, step 6597/7134 completed (loss: 0.038268111646175385, acc: 0.9905660152435303)
[2025-02-13 03:26:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:23][root][INFO] - Training Epoch: 1/2, step 6598/7134 completed (loss: 0.032146669924259186, acc: 0.9892617464065552)
[2025-02-13 03:26:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:23][root][INFO] - Training Epoch: 1/2, step 6599/7134 completed (loss: 0.054531414061784744, acc: 0.9867149591445923)
[2025-02-13 03:26:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:24][root][INFO] - Training Epoch: 1/2, step 6600/7134 completed (loss: 0.046703651547431946, acc: 0.9825673699378967)
[2025-02-13 03:26:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:24][root][INFO] - Training Epoch: 1/2, step 6601/7134 completed (loss: 0.05862204357981682, acc: 0.9847418069839478)
[2025-02-13 03:26:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:25][root][INFO] - Training Epoch: 1/2, step 6602/7134 completed (loss: 0.03318728134036064, acc: 0.9915764331817627)
[2025-02-13 03:26:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:25][root][INFO] - Training Epoch: 1/2, step 6603/7134 completed (loss: 0.02451653592288494, acc: 0.9929873943328857)
[2025-02-13 03:26:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:26][root][INFO] - Training Epoch: 1/2, step 6604/7134 completed (loss: 0.06212693080306053, acc: 0.982758641242981)
[2025-02-13 03:26:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:26][root][INFO] - Training Epoch: 1/2, step 6605/7134 completed (loss: 0.08337169140577316, acc: 0.97648686170578)
[2025-02-13 03:26:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:27][root][INFO] - Training Epoch: 1/2, step 6606/7134 completed (loss: 0.0274912491440773, acc: 0.993686854839325)
[2025-02-13 03:26:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:27][root][INFO] - Training Epoch: 1/2, step 6607/7134 completed (loss: 0.06510946154594421, acc: 0.9880319237709045)
[2025-02-13 03:26:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:27][root][INFO] - Training Epoch: 1/2, step 6608/7134 completed (loss: 0.025054888799786568, acc: 0.9933599233627319)
[2025-02-13 03:26:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:28][root][INFO] - Training Epoch: 1/2, step 6609/7134 completed (loss: 0.03714095428586006, acc: 0.9914425611495972)
[2025-02-13 03:26:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:28][root][INFO] - Training Epoch: 1/2, step 6610/7134 completed (loss: 0.08630002290010452, acc: 0.9784366488456726)
[2025-02-13 03:26:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:29][root][INFO] - Training Epoch: 1/2, step 6611/7134 completed (loss: 0.13308127224445343, acc: 0.9630769491195679)
[2025-02-13 03:26:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:29][root][INFO] - Training Epoch: 1/2, step 6612/7134 completed (loss: 0.06414777785539627, acc: 0.9824970960617065)
[2025-02-13 03:26:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:30][root][INFO] - Training Epoch: 1/2, step 6613/7134 completed (loss: 0.042266733944416046, acc: 0.9866071343421936)
[2025-02-13 03:26:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:30][root][INFO] - Training Epoch: 1/2, step 6614/7134 completed (loss: 0.06272947788238525, acc: 0.9878987669944763)
[2025-02-13 03:26:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:31][root][INFO] - Training Epoch: 1/2, step 6615/7134 completed (loss: 0.04004623368382454, acc: 0.9900990128517151)
[2025-02-13 03:26:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:31][root][INFO] - Training Epoch: 1/2, step 6616/7134 completed (loss: 0.03281993791460991, acc: 0.987089216709137)
[2025-02-13 03:26:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:31][root][INFO] - Training Epoch: 1/2, step 6617/7134 completed (loss: 0.039412934333086014, acc: 0.9913544654846191)
[2025-02-13 03:26:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:32][root][INFO] - Training Epoch: 1/2, step 6618/7134 completed (loss: 0.04214858636260033, acc: 0.9881129264831543)
[2025-02-13 03:26:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:32][root][INFO] - Training Epoch: 1/2, step 6619/7134 completed (loss: 0.03370600938796997, acc: 0.9910314083099365)
[2025-02-13 03:26:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:33][root][INFO] - Training Epoch: 1/2, step 6620/7134 completed (loss: 0.03629819303750992, acc: 0.993122398853302)
[2025-02-13 03:26:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:33][root][INFO] - Training Epoch: 1/2, step 6621/7134 completed (loss: 0.030348386615514755, acc: 0.9930635690689087)
[2025-02-13 03:26:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:34][root][INFO] - Training Epoch: 1/2, step 6622/7134 completed (loss: 0.03812916576862335, acc: 0.9890710115432739)
[2025-02-13 03:26:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:34][root][INFO] - Training Epoch: 1/2, step 6623/7134 completed (loss: 0.020077913999557495, acc: 0.9937106966972351)
[2025-02-13 03:26:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:35][root][INFO] - Training Epoch: 1/2, step 6624/7134 completed (loss: 0.02535100281238556, acc: 0.9947916865348816)
[2025-02-13 03:26:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:35][root][INFO] - Training Epoch: 1/2, step 6625/7134 completed (loss: 0.02947935089468956, acc: 0.9917469024658203)
[2025-02-13 03:26:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:35][root][INFO] - Training Epoch: 1/2, step 6626/7134 completed (loss: 0.017563028261065483, acc: 0.9939393997192383)
[2025-02-13 03:26:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:36][root][INFO] - Training Epoch: 1/2, step 6627/7134 completed (loss: 0.024371884763240814, acc: 0.9935135245323181)
[2025-02-13 03:26:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:36][root][INFO] - Training Epoch: 1/2, step 6628/7134 completed (loss: 0.04755432903766632, acc: 0.9881656765937805)
[2025-02-13 03:26:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:37][root][INFO] - Training Epoch: 1/2, step 6629/7134 completed (loss: 0.060082122683525085, acc: 0.9871630072593689)
[2025-02-13 03:26:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:37][root][INFO] - Training Epoch: 1/2, step 6630/7134 completed (loss: 0.046788763254880905, acc: 0.9910813570022583)
[2025-02-13 03:26:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:38][root][INFO] - Training Epoch: 1/2, step 6631/7134 completed (loss: 0.039637837558984756, acc: 0.9882044792175293)
[2025-02-13 03:26:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:38][root][INFO] - Training Epoch: 1/2, step 6632/7134 completed (loss: 0.032515086233615875, acc: 0.9910011291503906)
[2025-02-13 03:26:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:39][root][INFO] - Training Epoch: 1/2, step 6633/7134 completed (loss: 0.04175053909420967, acc: 0.9902439117431641)
[2025-02-13 03:26:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:39][root][INFO] - Training Epoch: 1/2, step 6634/7134 completed (loss: 0.04223526269197464, acc: 0.9874776601791382)
[2025-02-13 03:26:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:39][root][INFO] - Training Epoch: 1/2, step 6635/7134 completed (loss: 0.09456223994493484, acc: 0.9833759665489197)
[2025-02-13 03:26:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:40][root][INFO] - Training Epoch: 1/2, step 6636/7134 completed (loss: 0.04011128842830658, acc: 0.9897260069847107)
[2025-02-13 03:26:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:40][root][INFO] - Training Epoch: 1/2, step 6637/7134 completed (loss: 0.03849922493100166, acc: 0.9927219748497009)
[2025-02-13 03:26:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:41][root][INFO] - Training Epoch: 1/2, step 6638/7134 completed (loss: 0.020252663642168045, acc: 0.9949173927307129)
[2025-02-13 03:26:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:41][root][INFO] - Training Epoch: 1/2, step 6639/7134 completed (loss: 0.02599031664431095, acc: 0.9940387606620789)
[2025-02-13 03:26:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:42][root][INFO] - Training Epoch: 1/2, step 6640/7134 completed (loss: 0.01728968136012554, acc: 0.9954057931900024)
[2025-02-13 03:26:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:42][root][INFO] - Training Epoch: 1/2, step 6641/7134 completed (loss: 0.06906072795391083, acc: 0.9800703525543213)
[2025-02-13 03:26:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:43][root][INFO] - Training Epoch: 1/2, step 6642/7134 completed (loss: 0.03675934672355652, acc: 0.9894578456878662)
[2025-02-13 03:26:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:43][root][INFO] - Training Epoch: 1/2, step 6643/7134 completed (loss: 0.026296278461813927, acc: 0.9920739531517029)
[2025-02-13 03:26:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:44][root][INFO] - Training Epoch: 1/2, step 6644/7134 completed (loss: 0.060639698058366776, acc: 0.9847406148910522)
[2025-02-13 03:26:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:44][root][INFO] - Training Epoch: 1/2, step 6645/7134 completed (loss: 0.030626732856035233, acc: 0.9914004802703857)
[2025-02-13 03:26:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:45][root][INFO] - Training Epoch: 1/2, step 6646/7134 completed (loss: 0.041413869708776474, acc: 0.9889867901802063)
[2025-02-13 03:26:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:45][root][INFO] - Training Epoch: 1/2, step 6647/7134 completed (loss: 0.018419738858938217, acc: 0.9932065010070801)
[2025-02-13 03:26:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:45][root][INFO] - Training Epoch: 1/2, step 6648/7134 completed (loss: 0.10212786495685577, acc: 0.9775999784469604)
[2025-02-13 03:26:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:46][root][INFO] - Training Epoch: 1/2, step 6649/7134 completed (loss: 0.04946265369653702, acc: 0.9881936311721802)
[2025-02-13 03:26:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:46][root][INFO] - Training Epoch: 1/2, step 6650/7134 completed (loss: 0.02258622646331787, acc: 0.9933035969734192)
[2025-02-13 03:26:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:47][root][INFO] - Training Epoch: 1/2, step 6651/7134 completed (loss: 0.027502143755555153, acc: 0.9904502034187317)
[2025-02-13 03:26:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:47][root][INFO] - Training Epoch: 1/2, step 6652/7134 completed (loss: 0.037401583045721054, acc: 0.9867149591445923)
[2025-02-13 03:26:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:48][root][INFO] - Training Epoch: 1/2, step 6653/7134 completed (loss: 0.03960534930229187, acc: 0.9857612252235413)
[2025-02-13 03:26:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:48][root][INFO] - Training Epoch: 1/2, step 6654/7134 completed (loss: 0.07677719742059708, acc: 0.9829457402229309)
[2025-02-13 03:26:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:49][root][INFO] - Training Epoch: 1/2, step 6655/7134 completed (loss: 0.045715682208538055, acc: 0.985111653804779)
[2025-02-13 03:26:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:49][root][INFO] - Training Epoch: 1/2, step 6656/7134 completed (loss: 0.045927222818136215, acc: 0.9847328066825867)
[2025-02-13 03:26:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:50][root][INFO] - Training Epoch: 1/2, step 6657/7134 completed (loss: 0.04351898655295372, acc: 0.9837398529052734)
[2025-02-13 03:26:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:50][root][INFO] - Training Epoch: 1/2, step 6658/7134 completed (loss: 0.03017677739262581, acc: 0.9935483932495117)
[2025-02-13 03:26:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:50][root][INFO] - Training Epoch: 1/2, step 6659/7134 completed (loss: 0.04711894318461418, acc: 0.9869791865348816)
[2025-02-13 03:26:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:51][root][INFO] - Training Epoch: 1/2, step 6660/7134 completed (loss: 0.0286650899797678, acc: 0.9940476417541504)
[2025-02-13 03:26:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:51][root][INFO] - Training Epoch: 1/2, step 6661/7134 completed (loss: 0.03735285997390747, acc: 0.9876237511634827)
[2025-02-13 03:26:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:52][root][INFO] - Training Epoch: 1/2, step 6662/7134 completed (loss: 0.07473238557577133, acc: 0.9742489457130432)
[2025-02-13 03:26:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:52][root][INFO] - Training Epoch: 1/2, step 6663/7134 completed (loss: 0.03272063657641411, acc: 0.9934425950050354)
[2025-02-13 03:26:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:53][root][INFO] - Training Epoch: 1/2, step 6664/7134 completed (loss: 0.08985389769077301, acc: 0.9760147333145142)
[2025-02-13 03:26:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:53][root][INFO] - Training Epoch: 1/2, step 6665/7134 completed (loss: 0.08126473426818848, acc: 0.9659090638160706)
[2025-02-13 03:26:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:54][root][INFO] - Training Epoch: 1/2, step 6666/7134 completed (loss: 0.03356232866644859, acc: 0.9911167621612549)
[2025-02-13 03:26:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:54][root][INFO] - Training Epoch: 1/2, step 6667/7134 completed (loss: 0.04865500330924988, acc: 0.982332170009613)
[2025-02-13 03:26:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:54][root][INFO] - Training Epoch: 1/2, step 6668/7134 completed (loss: 0.01821223273873329, acc: 0.9913344979286194)
[2025-02-13 03:26:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:55][root][INFO] - Training Epoch: 1/2, step 6669/7134 completed (loss: 0.02865501306951046, acc: 0.9882550239562988)
[2025-02-13 03:26:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:55][root][INFO] - Training Epoch: 1/2, step 6670/7134 completed (loss: 0.027689959853887558, acc: 0.9916387796401978)
[2025-02-13 03:26:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:56][root][INFO] - Training Epoch: 1/2, step 6671/7134 completed (loss: 0.021010514348745346, acc: 0.9941349029541016)
[2025-02-13 03:26:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:56][root][INFO] - Training Epoch: 1/2, step 6672/7134 completed (loss: 0.02151012234389782, acc: 0.9924952983856201)
[2025-02-13 03:26:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:56][root][INFO] - Training Epoch: 1/2, step 6673/7134 completed (loss: 0.0253146942704916, acc: 0.9926578402519226)
[2025-02-13 03:26:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:57][root][INFO] - Training Epoch: 1/2, step 6674/7134 completed (loss: 0.016462864354252815, acc: 0.9961315393447876)
[2025-02-13 03:26:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:57][root][INFO] - Training Epoch: 1/2, step 6675/7134 completed (loss: 0.016308657824993134, acc: 0.9925925731658936)
[2025-02-13 03:26:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:58][root][INFO] - Training Epoch: 1/2, step 6676/7134 completed (loss: 0.017616454511880875, acc: 0.9904534816741943)
[2025-02-13 03:26:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:58][root][INFO] - Training Epoch: 1/2, step 6677/7134 completed (loss: 0.066269151866436, acc: 0.9885495901107788)
[2025-02-13 03:26:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:59][root][INFO] - Training Epoch: 1/2, step 6678/7134 completed (loss: 0.013519546948373318, acc: 0.9964285492897034)
[2025-02-13 03:26:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:59][root][INFO] - Training Epoch: 1/2, step 6679/7134 completed (loss: 0.026712501421570778, acc: 0.9890829920768738)
[2025-02-13 03:26:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:26:59][root][INFO] - Training Epoch: 1/2, step 6680/7134 completed (loss: 0.014383050613105297, acc: 0.9945945739746094)
[2025-02-13 03:26:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:00][root][INFO] - Training Epoch: 1/2, step 6681/7134 completed (loss: 0.007274607662111521, acc: 0.9958419799804688)
[2025-02-13 03:27:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:00][root][INFO] - Training Epoch: 1/2, step 6682/7134 completed (loss: 0.05473959445953369, acc: 0.9906542301177979)
[2025-02-13 03:27:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:01][root][INFO] - Training Epoch: 1/2, step 6683/7134 completed (loss: 0.037754468619823456, acc: 0.9890109896659851)
[2025-02-13 03:27:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:01][root][INFO] - Training Epoch: 1/2, step 6684/7134 completed (loss: 0.04081602767109871, acc: 0.9928571581840515)
[2025-02-13 03:27:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:01][root][INFO] - Training Epoch: 1/2, step 6685/7134 completed (loss: 0.029742049053311348, acc: 0.9904580116271973)
[2025-02-13 03:27:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:02][root][INFO] - Training Epoch: 1/2, step 6686/7134 completed (loss: 0.08800482004880905, acc: 0.9834710955619812)
[2025-02-13 03:27:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:02][root][INFO] - Training Epoch: 1/2, step 6687/7134 completed (loss: 0.1073603481054306, acc: 0.9691358208656311)
[2025-02-13 03:27:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:03][root][INFO] - Training Epoch: 1/2, step 6688/7134 completed (loss: 0.08236457407474518, acc: 0.9810996651649475)
[2025-02-13 03:27:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:03][root][INFO] - Training Epoch: 1/2, step 6689/7134 completed (loss: 0.06665419042110443, acc: 0.9782178401947021)
[2025-02-13 03:27:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:03][root][INFO] - Training Epoch: 1/2, step 6690/7134 completed (loss: 0.12342076748609543, acc: 0.9636803865432739)
[2025-02-13 03:27:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:04][root][INFO] - Training Epoch: 1/2, step 6691/7134 completed (loss: 0.0337265282869339, acc: 0.9918166995048523)
[2025-02-13 03:27:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:04][root][INFO] - Training Epoch: 1/2, step 6692/7134 completed (loss: 0.03279677405953407, acc: 0.991525411605835)
[2025-02-13 03:27:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:05][root][INFO] - Training Epoch: 1/2, step 6693/7134 completed (loss: 0.02832793816924095, acc: 0.9884488582611084)
[2025-02-13 03:27:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:05][root][INFO] - Training Epoch: 1/2, step 6694/7134 completed (loss: 0.05538848415017128, acc: 0.9847095012664795)
[2025-02-13 03:27:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:05][root][INFO] - Training Epoch: 1/2, step 6695/7134 completed (loss: 0.04620867967605591, acc: 0.9863201379776001)
[2025-02-13 03:27:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:06][root][INFO] - Training Epoch: 1/2, step 6696/7134 completed (loss: 0.07153750211000443, acc: 0.9829721450805664)
[2025-02-13 03:27:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:06][root][INFO] - Training Epoch: 1/2, step 6697/7134 completed (loss: 0.08643214404582977, acc: 0.9786885380744934)
[2025-02-13 03:27:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:07][root][INFO] - Training Epoch: 1/2, step 6698/7134 completed (loss: 0.02777804620563984, acc: 0.9896142482757568)
[2025-02-13 03:27:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:07][root][INFO] - Training Epoch: 1/2, step 6699/7134 completed (loss: 0.04016328603029251, acc: 0.9863325953483582)
[2025-02-13 03:27:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:08][root][INFO] - Training Epoch: 1/2, step 6700/7134 completed (loss: 0.08615332841873169, acc: 0.9742646813392639)
[2025-02-13 03:27:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:08][root][INFO] - Training Epoch: 1/2, step 6701/7134 completed (loss: 0.10236737877130508, acc: 0.9722222089767456)
[2025-02-13 03:27:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:08][root][INFO] - Training Epoch: 1/2, step 6702/7134 completed (loss: 0.07589991390705109, acc: 0.9764243364334106)
[2025-02-13 03:27:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:09][root][INFO] - Training Epoch: 1/2, step 6703/7134 completed (loss: 0.0783752053976059, acc: 0.9753289222717285)
[2025-02-13 03:27:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:09][root][INFO] - Training Epoch: 1/2, step 6704/7134 completed (loss: 0.09709759801626205, acc: 0.9754204154014587)
[2025-02-13 03:27:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:10][root][INFO] - Training Epoch: 1/2, step 6705/7134 completed (loss: 0.04986845701932907, acc: 0.9869961142539978)
[2025-02-13 03:27:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:10][root][INFO] - Training Epoch: 1/2, step 6706/7134 completed (loss: 0.030637960880994797, acc: 0.9896694421768188)
[2025-02-13 03:27:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:10][root][INFO] - Training Epoch: 1/2, step 6707/7134 completed (loss: 0.055647291243076324, acc: 0.9842209219932556)
[2025-02-13 03:27:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:11][root][INFO] - Training Epoch: 1/2, step 6708/7134 completed (loss: 0.053679294884204865, acc: 0.9884488582611084)
[2025-02-13 03:27:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:11][root][INFO] - Training Epoch: 1/2, step 6709/7134 completed (loss: 0.04439682140946388, acc: 0.9895209670066833)
[2025-02-13 03:27:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:12][root][INFO] - Training Epoch: 1/2, step 6710/7134 completed (loss: 0.0694330707192421, acc: 0.9835466146469116)
[2025-02-13 03:27:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:12][root][INFO] - Training Epoch: 1/2, step 6711/7134 completed (loss: 0.017727894708514214, acc: 0.9984126687049866)
[2025-02-13 03:27:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:13][root][INFO] - Training Epoch: 1/2, step 6712/7134 completed (loss: 0.05385722219944, acc: 0.9848713874816895)
[2025-02-13 03:27:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:13][root][INFO] - Training Epoch: 1/2, step 6713/7134 completed (loss: 0.04903964325785637, acc: 0.9855282306671143)
[2025-02-13 03:27:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:14][root][INFO] - Training Epoch: 1/2, step 6714/7134 completed (loss: 0.03813951462507248, acc: 0.9866270422935486)
[2025-02-13 03:27:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:14][root][INFO] - Training Epoch: 1/2, step 6715/7134 completed (loss: 0.07938753068447113, acc: 0.9701727032661438)
[2025-02-13 03:27:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:14][root][INFO] - Training Epoch: 1/2, step 6716/7134 completed (loss: 0.02395927719771862, acc: 0.9940119981765747)
[2025-02-13 03:27:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:15][root][INFO] - Training Epoch: 1/2, step 6717/7134 completed (loss: 0.029052304103970528, acc: 0.9919484853744507)
[2025-02-13 03:27:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:15][root][INFO] - Training Epoch: 1/2, step 6718/7134 completed (loss: 0.04957013577222824, acc: 0.9833333492279053)
[2025-02-13 03:27:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:16][root][INFO] - Training Epoch: 1/2, step 6719/7134 completed (loss: 0.026741953566670418, acc: 0.991830050945282)
[2025-02-13 03:27:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:16][root][INFO] - Training Epoch: 1/2, step 6720/7134 completed (loss: 0.06789133697748184, acc: 0.9902234673500061)
[2025-02-13 03:27:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:16][root][INFO] - Training Epoch: 1/2, step 6721/7134 completed (loss: 0.07602714747190475, acc: 0.9859594106674194)
[2025-02-13 03:27:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:17][root][INFO] - Training Epoch: 1/2, step 6722/7134 completed (loss: 0.08815895020961761, acc: 0.9859402179718018)
[2025-02-13 03:27:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:17][root][INFO] - Training Epoch: 1/2, step 6723/7134 completed (loss: 0.024025218561291695, acc: 0.9958333373069763)
[2025-02-13 03:27:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:18][root][INFO] - Training Epoch: 1/2, step 6724/7134 completed (loss: 0.04186396673321724, acc: 0.9912152290344238)
[2025-02-13 03:27:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:18][root][INFO] - Training Epoch: 1/2, step 6725/7134 completed (loss: 0.038913119584321976, acc: 0.9897611141204834)
[2025-02-13 03:27:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:18][root][INFO] - Training Epoch: 1/2, step 6726/7134 completed (loss: 0.08468737453222275, acc: 0.9859437942504883)
[2025-02-13 03:27:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:19][root][INFO] - Training Epoch: 1/2, step 6727/7134 completed (loss: 0.04662131518125534, acc: 0.9841549396514893)
[2025-02-13 03:27:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:19][root][INFO] - Training Epoch: 1/2, step 6728/7134 completed (loss: 0.06302177160978317, acc: 0.9861751198768616)
[2025-02-13 03:27:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:20][root][INFO] - Training Epoch: 1/2, step 6729/7134 completed (loss: 0.020878678187727928, acc: 0.9972826242446899)
[2025-02-13 03:27:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:20][root][INFO] - Training Epoch: 1/2, step 6730/7134 completed (loss: 0.04253176227211952, acc: 0.9905808568000793)
[2025-02-13 03:27:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:21][root][INFO] - Training Epoch: 1/2, step 6731/7134 completed (loss: 0.02278704009950161, acc: 0.9924585223197937)
[2025-02-13 03:27:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:21][root][INFO] - Training Epoch: 1/2, step 6732/7134 completed (loss: 0.04102795571088791, acc: 0.9842767119407654)
[2025-02-13 03:27:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:21][root][INFO] - Training Epoch: 1/2, step 6733/7134 completed (loss: 0.04393162205815315, acc: 0.9867172837257385)
[2025-02-13 03:27:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:22][root][INFO] - Training Epoch: 1/2, step 6734/7134 completed (loss: 0.057817257940769196, acc: 0.9869158864021301)
[2025-02-13 03:27:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:22][root][INFO] - Training Epoch: 1/2, step 6735/7134 completed (loss: 0.05616185814142227, acc: 0.9846389889717102)
[2025-02-13 03:27:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:23][root][INFO] - Training Epoch: 1/2, step 6736/7134 completed (loss: 0.053384553641080856, acc: 0.9888579249382019)
[2025-02-13 03:27:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:23][root][INFO] - Training Epoch: 1/2, step 6737/7134 completed (loss: 0.05324782058596611, acc: 0.9837925434112549)
[2025-02-13 03:27:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:24][root][INFO] - Training Epoch: 1/2, step 6738/7134 completed (loss: 0.04607537016272545, acc: 0.9888059496879578)
[2025-02-13 03:27:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:24][root][INFO] - Training Epoch: 1/2, step 6739/7134 completed (loss: 0.03764433041214943, acc: 0.9852941036224365)
[2025-02-13 03:27:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:24][root][INFO] - Training Epoch: 1/2, step 6740/7134 completed (loss: 0.05140725523233414, acc: 0.9915110468864441)
[2025-02-13 03:27:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:25][root][INFO] - Training Epoch: 1/2, step 6741/7134 completed (loss: 0.04641684889793396, acc: 0.9883889555931091)
[2025-02-13 03:27:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:25][root][INFO] - Training Epoch: 1/2, step 6742/7134 completed (loss: 0.02532714605331421, acc: 0.9884560108184814)
[2025-02-13 03:27:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:26][root][INFO] - Training Epoch: 1/2, step 6743/7134 completed (loss: 0.016695378348231316, acc: 0.9972752332687378)
[2025-02-13 03:27:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:26][root][INFO] - Training Epoch: 1/2, step 6744/7134 completed (loss: 0.018854165449738503, acc: 0.9935897588729858)
[2025-02-13 03:27:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:26][root][INFO] - Training Epoch: 1/2, step 6745/7134 completed (loss: 0.00816301815211773, acc: 0.9986245036125183)
[2025-02-13 03:27:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:27][root][INFO] - Training Epoch: 1/2, step 6746/7134 completed (loss: 0.019867587834596634, acc: 0.9947090148925781)
[2025-02-13 03:27:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:27][root][INFO] - Training Epoch: 1/2, step 6747/7134 completed (loss: 0.02037566341459751, acc: 0.9962732791900635)
[2025-02-13 03:27:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:28][root][INFO] - Training Epoch: 1/2, step 6748/7134 completed (loss: 0.020911959931254387, acc: 0.9966177940368652)
[2025-02-13 03:27:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:28][root][INFO] - Training Epoch: 1/2, step 6749/7134 completed (loss: 0.03029603883624077, acc: 0.9912853837013245)
[2025-02-13 03:27:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:29][root][INFO] - Training Epoch: 1/2, step 6750/7134 completed (loss: 0.015979262068867683, acc: 0.9988465905189514)
[2025-02-13 03:27:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:29][root][INFO] - Training Epoch: 1/2, step 6751/7134 completed (loss: 0.011789110489189625, acc: 0.9988584518432617)
[2025-02-13 03:27:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:30][root][INFO] - Training Epoch: 1/2, step 6752/7134 completed (loss: 0.01626523770391941, acc: 0.994194507598877)
[2025-02-13 03:27:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:30][root][INFO] - Training Epoch: 1/2, step 6753/7134 completed (loss: 0.014365325681865215, acc: 0.997706413269043)
[2025-02-13 03:27:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:30][root][INFO] - Training Epoch: 1/2, step 6754/7134 completed (loss: 0.008013345301151276, acc: 0.9987951517105103)
[2025-02-13 03:27:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:31][root][INFO] - Training Epoch: 1/2, step 6755/7134 completed (loss: 0.011923838406801224, acc: 0.9975369572639465)
[2025-02-13 03:27:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:31][root][INFO] - Training Epoch: 1/2, step 6756/7134 completed (loss: 0.0049050720408558846, acc: 0.9986594915390015)
[2025-02-13 03:27:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:32][root][INFO] - Training Epoch: 1/2, step 6757/7134 completed (loss: 0.018810071051120758, acc: 0.9939024448394775)
[2025-02-13 03:27:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:32][root][INFO] - Training Epoch: 1/2, step 6758/7134 completed (loss: 0.012045660056173801, acc: 0.9961190223693848)
[2025-02-13 03:27:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:33][root][INFO] - Training Epoch: 1/2, step 6759/7134 completed (loss: 0.030628520995378494, acc: 0.9932885766029358)
[2025-02-13 03:27:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:33][root][INFO] - Training Epoch: 1/2, step 6760/7134 completed (loss: 0.018628451973199844, acc: 0.9934383034706116)
[2025-02-13 03:27:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:34][root][INFO] - Training Epoch: 1/2, step 6761/7134 completed (loss: 0.0329267717897892, acc: 0.9891566038131714)
[2025-02-13 03:27:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:34][root][INFO] - Training Epoch: 1/2, step 6762/7134 completed (loss: 0.03688408434391022, acc: 0.9877451062202454)
[2025-02-13 03:27:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:34][root][INFO] - Training Epoch: 1/2, step 6763/7134 completed (loss: 0.011004133149981499, acc: 0.9962359070777893)
[2025-02-13 03:27:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:35][root][INFO] - Training Epoch: 1/2, step 6764/7134 completed (loss: 0.03170161321759224, acc: 0.9913793206214905)
[2025-02-13 03:27:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:35][root][INFO] - Training Epoch: 1/2, step 6765/7134 completed (loss: 0.015098183415830135, acc: 0.9960052967071533)
[2025-02-13 03:27:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:36][root][INFO] - Training Epoch: 1/2, step 6766/7134 completed (loss: 0.014587878249585629, acc: 0.9958620667457581)
[2025-02-13 03:27:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:36][root][INFO] - Training Epoch: 1/2, step 6767/7134 completed (loss: 0.07063626497983932, acc: 0.9864864945411682)
[2025-02-13 03:27:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:37][root][INFO] - Training Epoch: 1/2, step 6768/7134 completed (loss: 0.009852753952145576, acc: 0.9961977005004883)
[2025-02-13 03:27:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:37][root][INFO] - Training Epoch: 1/2, step 6769/7134 completed (loss: 0.020660698413848877, acc: 0.9876543283462524)
[2025-02-13 03:27:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:37][root][INFO] - Training Epoch: 1/2, step 6770/7134 completed (loss: 0.0426325760781765, acc: 0.9865771532058716)
[2025-02-13 03:27:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:38][root][INFO] - Training Epoch: 1/2, step 6771/7134 completed (loss: 0.025378461927175522, acc: 0.9936908483505249)
[2025-02-13 03:27:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:38][root][INFO] - Training Epoch: 1/2, step 6772/7134 completed (loss: 0.0755084753036499, acc: 0.9893048405647278)
[2025-02-13 03:27:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:39][root][INFO] - Training Epoch: 1/2, step 6773/7134 completed (loss: 0.03821703791618347, acc: 0.9873417615890503)
[2025-02-13 03:27:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:39][root][INFO] - Training Epoch: 1/2, step 6774/7134 completed (loss: 0.04538102447986603, acc: 0.9840764403343201)
[2025-02-13 03:27:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:40][root][INFO] - Training Epoch: 1/2, step 6775/7134 completed (loss: 0.019601790234446526, acc: 0.9943289160728455)
[2025-02-13 03:27:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:40][root][INFO] - Training Epoch: 1/2, step 6776/7134 completed (loss: 0.03504015505313873, acc: 0.9872340559959412)
[2025-02-13 03:27:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:40][root][INFO] - Training Epoch: 1/2, step 6777/7134 completed (loss: 0.022978484630584717, acc: 0.9946308732032776)
[2025-02-13 03:27:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:41][root][INFO] - Training Epoch: 1/2, step 6778/7134 completed (loss: 0.045478612184524536, acc: 0.98959881067276)
[2025-02-13 03:27:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:41][root][INFO] - Training Epoch: 1/2, step 6779/7134 completed (loss: 0.030431823804974556, acc: 0.9880596995353699)
[2025-02-13 03:27:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:42][root][INFO] - Training Epoch: 1/2, step 6780/7134 completed (loss: 0.019426992163062096, acc: 0.9955157041549683)
[2025-02-13 03:27:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:42][root][INFO] - Training Epoch: 1/2, step 6781/7134 completed (loss: 0.023125333711504936, acc: 0.9895209670066833)
[2025-02-13 03:27:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:43][root][INFO] - Training Epoch: 1/2, step 6782/7134 completed (loss: 0.029387738555669785, acc: 0.9947916865348816)
[2025-02-13 03:27:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:43][root][INFO] - Training Epoch: 1/2, step 6783/7134 completed (loss: 0.04419589042663574, acc: 0.9884836673736572)
[2025-02-13 03:27:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:43][root][INFO] - Training Epoch: 1/2, step 6784/7134 completed (loss: 0.015907367691397667, acc: 0.9950082898139954)
[2025-02-13 03:27:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:44][root][INFO] - Training Epoch: 1/2, step 6785/7134 completed (loss: 0.012773132883012295, acc: 0.99842768907547)
[2025-02-13 03:27:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:44][root][INFO] - Training Epoch: 1/2, step 6786/7134 completed (loss: 0.007934380322694778, acc: 0.9971222877502441)
[2025-02-13 03:27:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:45][root][INFO] - Training Epoch: 1/2, step 6787/7134 completed (loss: 0.04025598615407944, acc: 0.9907894730567932)
[2025-02-13 03:27:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:45][root][INFO] - Training Epoch: 1/2, step 6788/7134 completed (loss: 0.01451604999601841, acc: 0.9982014298439026)
[2025-02-13 03:27:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:45][root][INFO] - Training Epoch: 1/2, step 6789/7134 completed (loss: 0.01982155628502369, acc: 0.994163453578949)
[2025-02-13 03:27:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:46][root][INFO] - Training Epoch: 1/2, step 6790/7134 completed (loss: 0.054988835006952286, acc: 0.9856321811676025)
[2025-02-13 03:27:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:46][root][INFO] - Training Epoch: 1/2, step 6791/7134 completed (loss: 0.02151825837790966, acc: 0.991482138633728)
[2025-02-13 03:27:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:47][root][INFO] - Training Epoch: 1/2, step 6792/7134 completed (loss: 0.012476957403123379, acc: 0.9963302612304688)
[2025-02-13 03:27:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:47][root][INFO] - Training Epoch: 1/2, step 6793/7134 completed (loss: 0.010401423089206219, acc: 0.997890293598175)
[2025-02-13 03:27:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:48][root][INFO] - Training Epoch: 1/2, step 6794/7134 completed (loss: 0.03427838161587715, acc: 0.9934318661689758)
[2025-02-13 03:27:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:48][root][INFO] - Training Epoch: 1/2, step 6795/7134 completed (loss: 0.012794427573680878, acc: 0.9948052167892456)
[2025-02-13 03:27:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:48][root][INFO] - Training Epoch: 1/2, step 6796/7134 completed (loss: 0.014560647308826447, acc: 0.9956011772155762)
[2025-02-13 03:27:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:49][root][INFO] - Training Epoch: 1/2, step 6797/7134 completed (loss: 0.057181235402822495, acc: 0.9854439496994019)
[2025-02-13 03:27:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:49][root][INFO] - Training Epoch: 1/2, step 6798/7134 completed (loss: 0.026577776297926903, acc: 0.9903581142425537)
[2025-02-13 03:27:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:50][root][INFO] - Training Epoch: 1/2, step 6799/7134 completed (loss: 0.024361100047826767, acc: 0.9931600689888)
[2025-02-13 03:27:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:50][root][INFO] - Training Epoch: 1/2, step 6800/7134 completed (loss: 0.029524657875299454, acc: 0.9875690340995789)
[2025-02-13 03:27:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:51][root][INFO] - Training Epoch: 1/2, step 6801/7134 completed (loss: 0.03572619706392288, acc: 0.9921259880065918)
[2025-02-13 03:27:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:51][root][INFO] - Training Epoch: 1/2, step 6802/7134 completed (loss: 0.019019776955246925, acc: 0.9941434860229492)
[2025-02-13 03:27:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:51][root][INFO] - Training Epoch: 1/2, step 6803/7134 completed (loss: 0.012763257138431072, acc: 0.9937106966972351)
[2025-02-13 03:27:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:52][root][INFO] - Training Epoch: 1/2, step 6804/7134 completed (loss: 0.03694558143615723, acc: 0.9921875)
[2025-02-13 03:27:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:52][root][INFO] - Training Epoch: 1/2, step 6805/7134 completed (loss: 0.034707847982645035, acc: 0.9935897588729858)
[2025-02-13 03:27:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:53][root][INFO] - Training Epoch: 1/2, step 6806/7134 completed (loss: 0.04726912081241608, acc: 0.9898734092712402)
[2025-02-13 03:27:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:53][root][INFO] - Training Epoch: 1/2, step 6807/7134 completed (loss: 0.016790466383099556, acc: 0.9971264600753784)
[2025-02-13 03:27:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:54][root][INFO] - Training Epoch: 1/2, step 6808/7134 completed (loss: 0.05824294313788414, acc: 0.9872408509254456)
[2025-02-13 03:27:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:54][root][INFO] - Training Epoch: 1/2, step 6809/7134 completed (loss: 0.035900622606277466, acc: 0.9905149340629578)
[2025-02-13 03:27:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:54][root][INFO] - Training Epoch: 1/2, step 6810/7134 completed (loss: 0.010812618769705296, acc: 0.9947368502616882)
[2025-02-13 03:27:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:55][root][INFO] - Training Epoch: 1/2, step 6811/7134 completed (loss: 0.05699462071061134, acc: 0.9842725992202759)
[2025-02-13 03:27:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:55][root][INFO] - Training Epoch: 1/2, step 6812/7134 completed (loss: 0.023915227502584457, acc: 0.9886547923088074)
[2025-02-13 03:27:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:56][root][INFO] - Training Epoch: 1/2, step 6813/7134 completed (loss: 0.015482828952372074, acc: 0.997183084487915)
[2025-02-13 03:27:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:56][root][INFO] - Training Epoch: 1/2, step 6814/7134 completed (loss: 0.012405135668814182, acc: 0.9973118305206299)
[2025-02-13 03:27:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:57][root][INFO] - Training Epoch: 1/2, step 6815/7134 completed (loss: 0.04128797724843025, acc: 0.9894875288009644)
[2025-02-13 03:27:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:57][root][INFO] - Training Epoch: 1/2, step 6816/7134 completed (loss: 0.01981271244585514, acc: 0.9949832558631897)
[2025-02-13 03:27:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:58][root][INFO] - Training Epoch: 1/2, step 6817/7134 completed (loss: 0.059545207768678665, acc: 0.9846153855323792)
[2025-02-13 03:27:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:58][root][INFO] - Training Epoch: 1/2, step 6818/7134 completed (loss: 0.01944224163889885, acc: 0.9925037622451782)
[2025-02-13 03:27:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:58][root][INFO] - Training Epoch: 1/2, step 6819/7134 completed (loss: 0.05650763958692551, acc: 0.9856114983558655)
[2025-02-13 03:27:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:59][root][INFO] - Training Epoch: 1/2, step 6820/7134 completed (loss: 0.020672721788287163, acc: 0.9945945739746094)
[2025-02-13 03:27:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:27:59][root][INFO] - Training Epoch: 1/2, step 6821/7134 completed (loss: 0.02908388338983059, acc: 0.9898989796638489)
[2025-02-13 03:27:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:00][root][INFO] - Training Epoch: 1/2, step 6822/7134 completed (loss: 0.058769114315509796, acc: 0.9842932224273682)
[2025-02-13 03:28:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:00][root][INFO] - Training Epoch: 1/2, step 6823/7134 completed (loss: 0.03883873298764229, acc: 0.9856938719749451)
[2025-02-13 03:28:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:00][root][INFO] - Training Epoch: 1/2, step 6824/7134 completed (loss: 0.07897882163524628, acc: 0.9737903475761414)
[2025-02-13 03:28:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:01][root][INFO] - Training Epoch: 1/2, step 6825/7134 completed (loss: 0.023914504796266556, acc: 0.992682933807373)
[2025-02-13 03:28:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:01][root][INFO] - Training Epoch: 1/2, step 6826/7134 completed (loss: 0.04228721186518669, acc: 0.9915825128555298)
[2025-02-13 03:28:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:02][root][INFO] - Training Epoch: 1/2, step 6827/7134 completed (loss: 0.11774083971977234, acc: 0.9716494679450989)
[2025-02-13 03:28:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:02][root][INFO] - Training Epoch: 1/2, step 6828/7134 completed (loss: 0.061143577098846436, acc: 0.9817517995834351)
[2025-02-13 03:28:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:02][root][INFO] - Training Epoch: 1/2, step 6829/7134 completed (loss: 0.061114925891160965, acc: 0.9821958541870117)
[2025-02-13 03:28:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:03][root][INFO] - Training Epoch: 1/2, step 6830/7134 completed (loss: 0.036492690443992615, acc: 0.9901153445243835)
[2025-02-13 03:28:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:03][root][INFO] - Training Epoch: 1/2, step 6831/7134 completed (loss: 0.03514116257429123, acc: 0.9892473220825195)
[2025-02-13 03:28:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:04][root][INFO] - Training Epoch: 1/2, step 6832/7134 completed (loss: 0.04743016138672829, acc: 0.9870610237121582)
[2025-02-13 03:28:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:04][root][INFO] - Training Epoch: 1/2, step 6833/7134 completed (loss: 0.046350687742233276, acc: 0.9823151230812073)
[2025-02-13 03:28:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:04][root][INFO] - Training Epoch: 1/2, step 6834/7134 completed (loss: 0.032021522521972656, acc: 0.991482138633728)
[2025-02-13 03:28:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:05][root][INFO] - Training Epoch: 1/2, step 6835/7134 completed (loss: 0.025134287774562836, acc: 0.9918830990791321)
[2025-02-13 03:28:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:05][root][INFO] - Training Epoch: 1/2, step 6836/7134 completed (loss: 0.05001755431294441, acc: 0.9861809015274048)
[2025-02-13 03:28:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:06][root][INFO] - Training Epoch: 1/2, step 6837/7134 completed (loss: 0.03600132837891579, acc: 0.987730085849762)
[2025-02-13 03:28:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:06][root][INFO] - Training Epoch: 1/2, step 6838/7134 completed (loss: 0.06761935353279114, acc: 0.9814814925193787)
[2025-02-13 03:28:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:06][root][INFO] - Training Epoch: 1/2, step 6839/7134 completed (loss: 0.09004150331020355, acc: 0.9768977165222168)
[2025-02-13 03:28:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:07][root][INFO] - Training Epoch: 1/2, step 6840/7134 completed (loss: 0.0661933422088623, acc: 0.9799426794052124)
[2025-02-13 03:28:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:07][root][INFO] - Training Epoch: 1/2, step 6841/7134 completed (loss: 0.03435712680220604, acc: 0.9884560108184814)
[2025-02-13 03:28:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:08][root][INFO] - Training Epoch: 1/2, step 6842/7134 completed (loss: 0.037683069705963135, acc: 0.9887217879295349)
[2025-02-13 03:28:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:08][root][INFO] - Training Epoch: 1/2, step 6843/7134 completed (loss: 0.0540718138217926, acc: 0.9790382385253906)
[2025-02-13 03:28:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:09][root][INFO] - Training Epoch: 1/2, step 6844/7134 completed (loss: 0.028473611921072006, acc: 0.9913644194602966)
[2025-02-13 03:28:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:09][root][INFO] - Training Epoch: 1/2, step 6845/7134 completed (loss: 0.024328023195266724, acc: 0.9931972622871399)
[2025-02-13 03:28:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:09][root][INFO] - Training Epoch: 1/2, step 6846/7134 completed (loss: 0.029373828321695328, acc: 0.9928366541862488)
[2025-02-13 03:28:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:10][root][INFO] - Training Epoch: 1/2, step 6847/7134 completed (loss: 0.04787375405430794, acc: 0.9848484992980957)
[2025-02-13 03:28:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:10][root][INFO] - Training Epoch: 1/2, step 6848/7134 completed (loss: 0.01096518337726593, acc: 1.0)
[2025-02-13 03:28:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:11][root][INFO] - Training Epoch: 1/2, step 6849/7134 completed (loss: 0.041358303278684616, acc: 0.9914089441299438)
[2025-02-13 03:28:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:11][root][INFO] - Training Epoch: 1/2, step 6850/7134 completed (loss: 0.020161030814051628, acc: 0.9908466935157776)
[2025-02-13 03:28:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:12][root][INFO] - Training Epoch: 1/2, step 6851/7134 completed (loss: 0.056286122649908066, acc: 0.9830747246742249)
[2025-02-13 03:28:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:12][root][INFO] - Training Epoch: 1/2, step 6852/7134 completed (loss: 0.019867217168211937, acc: 0.996503472328186)
[2025-02-13 03:28:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:12][root][INFO] - Training Epoch: 1/2, step 6853/7134 completed (loss: 0.03474495932459831, acc: 0.9917012453079224)
[2025-02-13 03:28:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:13][root][INFO] - Training Epoch: 1/2, step 6854/7134 completed (loss: 0.04986580088734627, acc: 0.9913232326507568)
[2025-02-13 03:28:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:13][root][INFO] - Training Epoch: 1/2, step 6855/7134 completed (loss: 0.00952132698148489, acc: 1.0)
[2025-02-13 03:28:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:14][root][INFO] - Training Epoch: 1/2, step 6856/7134 completed (loss: 0.009896283969283104, acc: 0.9942775368690491)
[2025-02-13 03:28:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:14][root][INFO] - Training Epoch: 1/2, step 6857/7134 completed (loss: 0.02841641753911972, acc: 0.992668628692627)
[2025-02-13 03:28:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:15][root][INFO] - Training Epoch: 1/2, step 6858/7134 completed (loss: 0.0037108862306922674, acc: 1.0)
[2025-02-13 03:28:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:15][root][INFO] - Training Epoch: 1/2, step 6859/7134 completed (loss: 0.011297602206468582, acc: 0.99615877866745)
[2025-02-13 03:28:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:15][root][INFO] - Training Epoch: 1/2, step 6860/7134 completed (loss: 0.012899454683065414, acc: 0.9982269406318665)
[2025-02-13 03:28:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:16][root][INFO] - Training Epoch: 1/2, step 6861/7134 completed (loss: 0.009511548094451427, acc: 0.9965397715568542)
[2025-02-13 03:28:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:16][root][INFO] - Training Epoch: 1/2, step 6862/7134 completed (loss: 0.03264385089278221, acc: 0.9923809766769409)
[2025-02-13 03:28:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:17][root][INFO] - Training Epoch: 1/2, step 6863/7134 completed (loss: 0.01807749643921852, acc: 0.994350254535675)
[2025-02-13 03:28:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:17][root][INFO] - Training Epoch: 1/2, step 6864/7134 completed (loss: 0.01159962359815836, acc: 0.9983360767364502)
[2025-02-13 03:28:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:18][root][INFO] - Training Epoch: 1/2, step 6865/7134 completed (loss: 0.04516174644231796, acc: 0.985567033290863)
[2025-02-13 03:28:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:18][root][INFO] - Training Epoch: 1/2, step 6866/7134 completed (loss: 0.007156867068260908, acc: 0.9984802603721619)
[2025-02-13 03:28:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:18][root][INFO] - Training Epoch: 1/2, step 6867/7134 completed (loss: 0.003046842059120536, acc: 1.0)
[2025-02-13 03:28:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:19][root][INFO] - Training Epoch: 1/2, step 6868/7134 completed (loss: 0.0023960552643984556, acc: 1.0)
[2025-02-13 03:28:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:19][root][INFO] - Training Epoch: 1/2, step 6869/7134 completed (loss: 0.0043269312009215355, acc: 1.0)
[2025-02-13 03:28:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:20][root][INFO] - Training Epoch: 1/2, step 6870/7134 completed (loss: 0.032747212797403336, acc: 0.9934853315353394)
[2025-02-13 03:28:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:20][root][INFO] - Training Epoch: 1/2, step 6871/7134 completed (loss: 0.0023095905780792236, acc: 1.0)
[2025-02-13 03:28:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:20][root][INFO] - Training Epoch: 1/2, step 6872/7134 completed (loss: 0.008502262644469738, acc: 0.9958449006080627)
[2025-02-13 03:28:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:21][root][INFO] - Training Epoch: 1/2, step 6873/7134 completed (loss: 0.010292330756783485, acc: 0.9971988797187805)
[2025-02-13 03:28:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:21][root][INFO] - Training Epoch: 1/2, step 6874/7134 completed (loss: 0.020654937252402306, acc: 0.991134762763977)
[2025-02-13 03:28:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:22][root][INFO] - Training Epoch: 1/2, step 6875/7134 completed (loss: 0.017353616654872894, acc: 0.9922720193862915)
[2025-02-13 03:28:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:22][root][INFO] - Training Epoch: 1/2, step 6876/7134 completed (loss: 0.02304559014737606, acc: 0.9915397763252258)
[2025-02-13 03:28:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:22][root][INFO] - Training Epoch: 1/2, step 6877/7134 completed (loss: 0.031004946678876877, acc: 0.992977499961853)
[2025-02-13 03:28:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:23][root][INFO] - Training Epoch: 1/2, step 6878/7134 completed (loss: 0.015493486076593399, acc: 0.9970015287399292)
[2025-02-13 03:28:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:23][root][INFO] - Training Epoch: 1/2, step 6879/7134 completed (loss: 0.011979976668953896, acc: 0.9958158731460571)
[2025-02-13 03:28:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:24][root][INFO] - Training Epoch: 1/2, step 6880/7134 completed (loss: 0.007414766121655703, acc: 0.9976717233657837)
[2025-02-13 03:28:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:24][root][INFO] - Training Epoch: 1/2, step 6881/7134 completed (loss: 0.05414183437824249, acc: 0.9914945363998413)
[2025-02-13 03:28:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:25][root][INFO] - Training Epoch: 1/2, step 6882/7134 completed (loss: 0.03361554816365242, acc: 0.9906014800071716)
[2025-02-13 03:28:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:25][root][INFO] - Training Epoch: 1/2, step 6883/7134 completed (loss: 0.010416174307465553, acc: 0.9966996908187866)
[2025-02-13 03:28:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:25][root][INFO] - Training Epoch: 1/2, step 6884/7134 completed (loss: 0.055878646671772, acc: 0.989924430847168)
[2025-02-13 03:28:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:26][root][INFO] - Training Epoch: 1/2, step 6885/7134 completed (loss: 0.06992613524198532, acc: 0.9888392686843872)
[2025-02-13 03:28:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:26][root][INFO] - Training Epoch: 1/2, step 6886/7134 completed (loss: 0.026017075404524803, acc: 0.9889065027236938)
[2025-02-13 03:28:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:27][root][INFO] - Training Epoch: 1/2, step 6887/7134 completed (loss: 0.04323156923055649, acc: 0.9897330403327942)
[2025-02-13 03:28:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:27][root][INFO] - Training Epoch: 1/2, step 6888/7134 completed (loss: 0.01661888137459755, acc: 0.9950166344642639)
[2025-02-13 03:28:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:27][root][INFO] - Training Epoch: 1/2, step 6889/7134 completed (loss: 0.005777647253125906, acc: 1.0)
[2025-02-13 03:28:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:28][root][INFO] - Training Epoch: 1/2, step 6890/7134 completed (loss: 0.03043956123292446, acc: 0.9881109595298767)
[2025-02-13 03:28:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:28][root][INFO] - Training Epoch: 1/2, step 6891/7134 completed (loss: 0.023552844300866127, acc: 0.9911110997200012)
[2025-02-13 03:28:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:29][root][INFO] - Training Epoch: 1/2, step 6892/7134 completed (loss: 0.01738119311630726, acc: 0.9947916865348816)
[2025-02-13 03:28:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:29][root][INFO] - Training Epoch: 1/2, step 6893/7134 completed (loss: 0.045091476291418076, acc: 0.9789719581604004)
[2025-02-13 03:28:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:30][root][INFO] - Training Epoch: 1/2, step 6894/7134 completed (loss: 0.05529380589723587, acc: 0.9828947186470032)
[2025-02-13 03:28:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:30][root][INFO] - Training Epoch: 1/2, step 6895/7134 completed (loss: 0.04204768314957619, acc: 0.9844124913215637)
[2025-02-13 03:28:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:31][root][INFO] - Training Epoch: 1/2, step 6896/7134 completed (loss: 0.021180273965001106, acc: 0.9942029118537903)
[2025-02-13 03:28:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:31][root][INFO] - Training Epoch: 1/2, step 6897/7134 completed (loss: 0.023406285792589188, acc: 0.9938650131225586)
[2025-02-13 03:28:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:31][root][INFO] - Training Epoch: 1/2, step 6898/7134 completed (loss: 0.017907649278640747, acc: 0.9945054650306702)
[2025-02-13 03:28:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:32][root][INFO] - Training Epoch: 1/2, step 6899/7134 completed (loss: 0.03656555339694023, acc: 0.9937185645103455)
[2025-02-13 03:28:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:32][root][INFO] - Training Epoch: 1/2, step 6900/7134 completed (loss: 0.041409268975257874, acc: 0.987270176410675)
[2025-02-13 03:28:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:33][root][INFO] - Training Epoch: 1/2, step 6901/7134 completed (loss: 0.038669098168611526, acc: 0.9907529950141907)
[2025-02-13 03:28:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:33][root][INFO] - Training Epoch: 1/2, step 6902/7134 completed (loss: 0.024881478399038315, acc: 0.9930651783943176)
[2025-02-13 03:28:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:34][root][INFO] - Training Epoch: 1/2, step 6903/7134 completed (loss: 0.022127510979771614, acc: 0.9923954606056213)
[2025-02-13 03:28:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:34][root][INFO] - Training Epoch: 1/2, step 6904/7134 completed (loss: 0.01971699483692646, acc: 0.9925373196601868)
[2025-02-13 03:28:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:35][root][INFO] - Training Epoch: 1/2, step 6905/7134 completed (loss: 0.02706092782318592, acc: 0.9923273921012878)
[2025-02-13 03:28:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:35][root][INFO] - Training Epoch: 1/2, step 6906/7134 completed (loss: 0.02970890887081623, acc: 0.9952095746994019)
[2025-02-13 03:28:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:35][root][INFO] - Training Epoch: 1/2, step 6907/7134 completed (loss: 0.0289387758821249, acc: 0.9937965273857117)
[2025-02-13 03:28:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:36][root][INFO] - Training Epoch: 1/2, step 6908/7134 completed (loss: 0.029562436044216156, acc: 0.9935232996940613)
[2025-02-13 03:28:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:36][root][INFO] - Training Epoch: 1/2, step 6909/7134 completed (loss: 0.02734246291220188, acc: 0.9934210777282715)
[2025-02-13 03:28:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:37][root][INFO] - Training Epoch: 1/2, step 6910/7134 completed (loss: 0.01909220963716507, acc: 0.9949044585227966)
[2025-02-13 03:28:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:37][root][INFO] - Training Epoch: 1/2, step 6911/7134 completed (loss: 0.03447147458791733, acc: 0.9912790656089783)
[2025-02-13 03:28:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:38][root][INFO] - Training Epoch: 1/2, step 6912/7134 completed (loss: 0.031091028824448586, acc: 0.995006263256073)
[2025-02-13 03:28:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:38][root][INFO] - Training Epoch: 1/2, step 6913/7134 completed (loss: 0.03188442438840866, acc: 0.9890377521514893)
[2025-02-13 03:28:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:38][root][INFO] - Training Epoch: 1/2, step 6914/7134 completed (loss: 0.07720198482275009, acc: 0.9865319728851318)
[2025-02-13 03:28:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:39][root][INFO] - Training Epoch: 1/2, step 6915/7134 completed (loss: 0.023305613547563553, acc: 0.9896373152732849)
[2025-02-13 03:28:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:39][root][INFO] - Training Epoch: 1/2, step 6916/7134 completed (loss: 0.032356150448322296, acc: 0.9938650131225586)
[2025-02-13 03:28:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:40][root][INFO] - Training Epoch: 1/2, step 6917/7134 completed (loss: 0.045614879578351974, acc: 0.9898989796638489)
[2025-02-13 03:28:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:40][root][INFO] - Training Epoch: 1/2, step 6918/7134 completed (loss: 0.02201310731470585, acc: 0.9964871406555176)
[2025-02-13 03:28:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:41][root][INFO] - Training Epoch: 1/2, step 6919/7134 completed (loss: 0.040262505412101746, acc: 0.9907727837562561)
[2025-02-13 03:28:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:41][root][INFO] - Training Epoch: 1/2, step 6920/7134 completed (loss: 0.01642906293272972, acc: 0.994301974773407)
[2025-02-13 03:28:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:42][root][INFO] - Training Epoch: 1/2, step 6921/7134 completed (loss: 0.06140011548995972, acc: 0.982679009437561)
[2025-02-13 03:28:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:42][root][INFO] - Training Epoch: 1/2, step 6922/7134 completed (loss: 0.06996103376150131, acc: 0.9797191619873047)
[2025-02-13 03:28:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:42][root][INFO] - Training Epoch: 1/2, step 6923/7134 completed (loss: 0.0723935142159462, acc: 0.9863813519477844)
[2025-02-13 03:28:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:43][root][INFO] - Training Epoch: 1/2, step 6924/7134 completed (loss: 0.02320736274123192, acc: 0.9918256402015686)
[2025-02-13 03:28:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:43][root][INFO] - Training Epoch: 1/2, step 6925/7134 completed (loss: 0.026162507012486458, acc: 0.9921875)
[2025-02-13 03:28:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:44][root][INFO] - Training Epoch: 1/2, step 6926/7134 completed (loss: 0.04434939846396446, acc: 0.9891501069068909)
[2025-02-13 03:28:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:44][root][INFO] - Training Epoch: 1/2, step 6927/7134 completed (loss: 0.044261250644922256, acc: 0.986810564994812)
[2025-02-13 03:28:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:45][root][INFO] - Training Epoch: 1/2, step 6928/7134 completed (loss: 0.05788211151957512, acc: 0.9882583022117615)
[2025-02-13 03:28:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:45][root][INFO] - Training Epoch: 1/2, step 6929/7134 completed (loss: 0.044494301080703735, acc: 0.991525411605835)
[2025-02-13 03:28:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:46][root][INFO] - Training Epoch: 1/2, step 6930/7134 completed (loss: 0.0393824465572834, acc: 0.9925558567047119)
[2025-02-13 03:28:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:46][root][INFO] - Training Epoch: 1/2, step 6931/7134 completed (loss: 0.03950805589556694, acc: 0.9865853786468506)
[2025-02-13 03:28:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:46][root][INFO] - Training Epoch: 1/2, step 6932/7134 completed (loss: 0.041473668068647385, acc: 0.9882352948188782)
[2025-02-13 03:28:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:47][root][INFO] - Training Epoch: 1/2, step 6933/7134 completed (loss: 0.0725436732172966, acc: 0.9870634078979492)
[2025-02-13 03:28:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:47][root][INFO] - Training Epoch: 1/2, step 6934/7134 completed (loss: 0.0674603283405304, acc: 0.9839486479759216)
[2025-02-13 03:28:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:48][root][INFO] - Training Epoch: 1/2, step 6935/7134 completed (loss: 0.03992460295557976, acc: 0.9921466112136841)
[2025-02-13 03:28:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:48][root][INFO] - Training Epoch: 1/2, step 6936/7134 completed (loss: 0.027885107323527336, acc: 0.9879999756813049)
[2025-02-13 03:28:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:49][root][INFO] - Training Epoch: 1/2, step 6937/7134 completed (loss: 0.10067504644393921, acc: 0.9754816293716431)
[2025-02-13 03:28:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:49][root][INFO] - Training Epoch: 1/2, step 6938/7134 completed (loss: 0.10785897076129913, acc: 0.9750000238418579)
[2025-02-13 03:28:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:50][root][INFO] - Training Epoch: 1/2, step 6939/7134 completed (loss: 0.07125921547412872, acc: 0.9860917925834656)
[2025-02-13 03:28:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:50][root][INFO] - Training Epoch: 1/2, step 6940/7134 completed (loss: 0.0768241435289383, acc: 0.9794520735740662)
[2025-02-13 03:28:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:50][root][INFO] - Training Epoch: 1/2, step 6941/7134 completed (loss: 0.03828324377536774, acc: 0.9895833134651184)
[2025-02-13 03:28:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:51][root][INFO] - Training Epoch: 1/2, step 6942/7134 completed (loss: 0.055679623037576675, acc: 0.9849315285682678)
[2025-02-13 03:28:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:51][root][INFO] - Training Epoch: 1/2, step 6943/7134 completed (loss: 0.03920968621969223, acc: 0.9874476790428162)
[2025-02-13 03:28:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:52][root][INFO] - Training Epoch: 1/2, step 6944/7134 completed (loss: 0.03301037475466728, acc: 0.993630588054657)
[2025-02-13 03:28:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:52][root][INFO] - Training Epoch: 1/2, step 6945/7134 completed (loss: 0.0669061467051506, acc: 0.9860383868217468)
[2025-02-13 03:28:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:52][root][INFO] - Training Epoch: 1/2, step 6946/7134 completed (loss: 0.03638610616326332, acc: 0.9885386824607849)
[2025-02-13 03:28:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:53][root][INFO] - Training Epoch: 1/2, step 6947/7134 completed (loss: 0.039149291813373566, acc: 0.991304337978363)
[2025-02-13 03:28:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:53][root][INFO] - Training Epoch: 1/2, step 6948/7134 completed (loss: 0.04057037830352783, acc: 0.9882746934890747)
[2025-02-13 03:28:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:54][root][INFO] - Training Epoch: 1/2, step 6949/7134 completed (loss: 0.03417886421084404, acc: 0.9935897588729858)
[2025-02-13 03:28:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:54][root][INFO] - Training Epoch: 1/2, step 6950/7134 completed (loss: 0.03654155507683754, acc: 0.9948979616165161)
[2025-02-13 03:28:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:54][root][INFO] - Training Epoch: 1/2, step 6951/7134 completed (loss: 0.026709608733654022, acc: 0.991525411605835)
[2025-02-13 03:28:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:55][root][INFO] - Training Epoch: 1/2, step 6952/7134 completed (loss: 0.019497880712151527, acc: 0.9929906725883484)
[2025-02-13 03:28:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:55][root][INFO] - Training Epoch: 1/2, step 6953/7134 completed (loss: 0.019830619916319847, acc: 0.9934425950050354)
[2025-02-13 03:28:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:56][root][INFO] - Training Epoch: 1/2, step 6954/7134 completed (loss: 0.0734541043639183, acc: 0.9850746393203735)
[2025-02-13 03:28:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:56][root][INFO] - Training Epoch: 1/2, step 6955/7134 completed (loss: 0.045332662761211395, acc: 0.9847561120986938)
[2025-02-13 03:28:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:56][root][INFO] - Training Epoch: 1/2, step 6956/7134 completed (loss: 0.036101654171943665, acc: 0.9946714043617249)
[2025-02-13 03:28:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:57][root][INFO] - Training Epoch: 1/2, step 6957/7134 completed (loss: 0.01991373486816883, acc: 0.9941291809082031)
[2025-02-13 03:28:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:57][root][INFO] - Training Epoch: 1/2, step 6958/7134 completed (loss: 0.06451421976089478, acc: 0.9785575270652771)
[2025-02-13 03:28:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:58][root][INFO] - Training Epoch: 1/2, step 6959/7134 completed (loss: 0.03749849647283554, acc: 0.9844789505004883)
[2025-02-13 03:28:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:58][root][INFO] - Training Epoch: 1/2, step 6960/7134 completed (loss: 0.03190497308969498, acc: 0.9886178970336914)
[2025-02-13 03:28:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:58][root][INFO] - Training Epoch: 1/2, step 6961/7134 completed (loss: 0.03648506477475166, acc: 0.9939393997192383)
[2025-02-13 03:28:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:59][root][INFO] - Training Epoch: 1/2, step 6962/7134 completed (loss: 0.03248750790953636, acc: 0.9930434823036194)
[2025-02-13 03:28:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:28:59][root][INFO] - Training Epoch: 1/2, step 6963/7134 completed (loss: 0.01484807301312685, acc: 0.9983165264129639)
[2025-02-13 03:28:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:00][root][INFO] - Training Epoch: 1/2, step 6964/7134 completed (loss: 0.015653636306524277, acc: 0.994584858417511)
[2025-02-13 03:29:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:00][root][INFO] - Training Epoch: 1/2, step 6965/7134 completed (loss: 0.016649512574076653, acc: 0.9926739931106567)
[2025-02-13 03:29:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:00][root][INFO] - Training Epoch: 1/2, step 6966/7134 completed (loss: 0.037455007433891296, acc: 0.9875311851501465)
[2025-02-13 03:29:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:01][root][INFO] - Training Epoch: 1/2, step 6967/7134 completed (loss: 0.018201855942606926, acc: 0.993966817855835)
[2025-02-13 03:29:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:01][root][INFO] - Training Epoch: 1/2, step 6968/7134 completed (loss: 0.027487706393003464, acc: 0.9896551966667175)
[2025-02-13 03:29:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:02][root][INFO] - Training Epoch: 1/2, step 6969/7134 completed (loss: 0.08071812987327576, acc: 0.9771309494972229)
[2025-02-13 03:29:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:02][root][INFO] - Training Epoch: 1/2, step 6970/7134 completed (loss: 0.09843459725379944, acc: 0.9768421053886414)
[2025-02-13 03:29:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:02][root][INFO] - Training Epoch: 1/2, step 6971/7134 completed (loss: 0.05172460526227951, acc: 0.984455943107605)
[2025-02-13 03:29:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:03][root][INFO] - Training Epoch: 1/2, step 6972/7134 completed (loss: 0.01409419346600771, acc: 0.9946523904800415)
[2025-02-13 03:29:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:03][root][INFO] - Training Epoch: 1/2, step 6973/7134 completed (loss: 0.040429167449474335, acc: 0.9911894202232361)
[2025-02-13 03:29:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:04][root][INFO] - Training Epoch: 1/2, step 6974/7134 completed (loss: 0.036032937467098236, acc: 0.9879310131072998)
[2025-02-13 03:29:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:04][root][INFO] - Training Epoch: 1/2, step 6975/7134 completed (loss: 0.035514313727617264, acc: 0.9934924244880676)
[2025-02-13 03:29:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:04][root][INFO] - Training Epoch: 1/2, step 6976/7134 completed (loss: 0.051406729966402054, acc: 0.984402060508728)
[2025-02-13 03:29:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:05][root][INFO] - Training Epoch: 1/2, step 6977/7134 completed (loss: 0.035628147423267365, acc: 0.9887217879295349)
[2025-02-13 03:29:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:05][root][INFO] - Training Epoch: 1/2, step 6978/7134 completed (loss: 0.026648370549082756, acc: 0.995121955871582)
[2025-02-13 03:29:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:06][root][INFO] - Training Epoch: 1/2, step 6979/7134 completed (loss: 0.055559441447257996, acc: 0.9847561120986938)
[2025-02-13 03:29:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:06][root][INFO] - Training Epoch: 1/2, step 6980/7134 completed (loss: 0.10181363672018051, acc: 0.9660377502441406)
[2025-02-13 03:29:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:07][root][INFO] - Training Epoch: 1/2, step 6981/7134 completed (loss: 0.08243981003761292, acc: 0.977011501789093)
[2025-02-13 03:29:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:07][root][INFO] - Training Epoch: 1/2, step 6982/7134 completed (loss: 0.09356358647346497, acc: 0.9829476475715637)
[2025-02-13 03:29:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:07][root][INFO] - Training Epoch: 1/2, step 6983/7134 completed (loss: 0.12442600727081299, acc: 0.9688427448272705)
[2025-02-13 03:29:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:08][root][INFO] - Training Epoch: 1/2, step 6984/7134 completed (loss: 0.042348463088274, acc: 0.9924973249435425)
[2025-02-13 03:29:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:08][root][INFO] - Training Epoch: 1/2, step 6985/7134 completed (loss: 0.10426107794046402, acc: 0.9753845930099487)
[2025-02-13 03:29:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:09][root][INFO] - Training Epoch: 1/2, step 6986/7134 completed (loss: 0.03686074540019035, acc: 0.9881129264831543)
[2025-02-13 03:29:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:09][root][INFO] - Training Epoch: 1/2, step 6987/7134 completed (loss: 0.0537138469517231, acc: 0.9818181991577148)
[2025-02-13 03:29:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:10][root][INFO] - Training Epoch: 1/2, step 6988/7134 completed (loss: 0.04373713210225105, acc: 0.9880715608596802)
[2025-02-13 03:29:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:10][root][INFO] - Training Epoch: 1/2, step 6989/7134 completed (loss: 0.07735548913478851, acc: 0.9818181991577148)
[2025-02-13 03:29:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:10][root][INFO] - Training Epoch: 1/2, step 6990/7134 completed (loss: 0.01984904520213604, acc: 0.9933035969734192)
[2025-02-13 03:29:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:11][root][INFO] - Training Epoch: 1/2, step 6991/7134 completed (loss: 0.06886126846075058, acc: 0.9824561476707458)
[2025-02-13 03:29:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:11][root][INFO] - Training Epoch: 1/2, step 6992/7134 completed (loss: 0.07585674524307251, acc: 0.9789473414421082)
[2025-02-13 03:29:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:12][root][INFO] - Training Epoch: 1/2, step 6993/7134 completed (loss: 0.09313514083623886, acc: 0.9773755669593811)
[2025-02-13 03:29:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:12][root][INFO] - Training Epoch: 1/2, step 6994/7134 completed (loss: 0.1240374967455864, acc: 0.9727126955986023)
[2025-02-13 03:29:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:12][root][INFO] - Training Epoch: 1/2, step 6995/7134 completed (loss: 0.043745845556259155, acc: 0.9882199168205261)
[2025-02-13 03:29:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:13][root][INFO] - Training Epoch: 1/2, step 6996/7134 completed (loss: 0.03517051041126251, acc: 0.9886363744735718)
[2025-02-13 03:29:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:13][root][INFO] - Training Epoch: 1/2, step 6997/7134 completed (loss: 0.044089749455451965, acc: 0.9890965819358826)
[2025-02-13 03:29:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:14][root][INFO] - Training Epoch: 1/2, step 6998/7134 completed (loss: 0.1066901907324791, acc: 0.9730392098426819)
[2025-02-13 03:29:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:14][root][INFO] - Training Epoch: 1/2, step 6999/7134 completed (loss: 0.0242092777043581, acc: 0.9921568632125854)
[2025-02-13 03:29:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:14][root][INFO] - Training Epoch: 1/2, step 7000/7134 completed (loss: 0.021233603358268738, acc: 0.9957627058029175)
[2025-02-13 03:29:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:15][root][INFO] - Training Epoch: 1/2, step 7001/7134 completed (loss: 0.07719255983829498, acc: 0.9819276928901672)
[2025-02-13 03:29:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:15][root][INFO] - Training Epoch: 1/2, step 7002/7134 completed (loss: 0.011431567370891571, acc: 1.0)
[2025-02-13 03:29:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:16][root][INFO] - Training Epoch: 1/2, step 7003/7134 completed (loss: 0.021911965683102608, acc: 0.9934282302856445)
[2025-02-13 03:29:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:16][root][INFO] - Training Epoch: 1/2, step 7004/7134 completed (loss: 0.08462062478065491, acc: 0.9816272854804993)
[2025-02-13 03:29:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:17][root][INFO] - Training Epoch: 1/2, step 7005/7134 completed (loss: 0.046703584492206573, acc: 0.9878542423248291)
[2025-02-13 03:29:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:17][root][INFO] - Training Epoch: 1/2, step 7006/7134 completed (loss: 0.03541414067149162, acc: 0.9890561103820801)
[2025-02-13 03:29:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:18][root][INFO] - Training Epoch: 1/2, step 7007/7134 completed (loss: 0.04601413756608963, acc: 0.9895209670066833)
[2025-02-13 03:29:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:18][root][INFO] - Training Epoch: 1/2, step 7008/7134 completed (loss: 0.031555987894535065, acc: 0.9938347935676575)
[2025-02-13 03:29:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:18][root][INFO] - Training Epoch: 1/2, step 7009/7134 completed (loss: 0.04721243306994438, acc: 0.9814814925193787)
[2025-02-13 03:29:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:19][root][INFO] - Training Epoch: 1/2, step 7010/7134 completed (loss: 0.013537093997001648, acc: 0.9984591603279114)
[2025-02-13 03:29:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:19][root][INFO] - Training Epoch: 1/2, step 7011/7134 completed (loss: 0.027124818414449692, acc: 0.9915865659713745)
[2025-02-13 03:29:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:20][root][INFO] - Training Epoch: 1/2, step 7012/7134 completed (loss: 0.07241714745759964, acc: 0.9766764044761658)
[2025-02-13 03:29:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:20][root][INFO] - Training Epoch: 1/2, step 7013/7134 completed (loss: 0.011547930538654327, acc: 0.9942362904548645)
[2025-02-13 03:29:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:21][root][INFO] - Training Epoch: 1/2, step 7014/7134 completed (loss: 0.025231193751096725, acc: 0.9935587644577026)
[2025-02-13 03:29:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:21][root][INFO] - Training Epoch: 1/2, step 7015/7134 completed (loss: 0.044204890727996826, acc: 0.980322003364563)
[2025-02-13 03:29:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:22][root][INFO] - Training Epoch: 1/2, step 7016/7134 completed (loss: 0.050634417682886124, acc: 0.9797688126564026)
[2025-02-13 03:29:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:22][root][INFO] - Training Epoch: 1/2, step 7017/7134 completed (loss: 0.0624006949365139, acc: 0.9771783947944641)
[2025-02-13 03:29:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:22][root][INFO] - Training Epoch: 1/2, step 7018/7134 completed (loss: 0.05606608837842941, acc: 0.9844961166381836)
[2025-02-13 03:29:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:23][root][INFO] - Training Epoch: 1/2, step 7019/7134 completed (loss: 0.11176243424415588, acc: 0.9707602262496948)
[2025-02-13 03:29:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:23][root][INFO] - Training Epoch: 1/2, step 7020/7134 completed (loss: 0.02679825946688652, acc: 0.9929478168487549)
[2025-02-13 03:29:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:24][root][INFO] - Training Epoch: 1/2, step 7021/7134 completed (loss: 0.031140312552452087, acc: 0.991482138633728)
[2025-02-13 03:29:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:24][root][INFO] - Training Epoch: 1/2, step 7022/7134 completed (loss: 0.08097860217094421, acc: 0.9814814925193787)
[2025-02-13 03:29:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:24][root][INFO] - Training Epoch: 1/2, step 7023/7134 completed (loss: 0.09264454245567322, acc: 0.9737470149993896)
[2025-02-13 03:29:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:25][root][INFO] - Training Epoch: 1/2, step 7024/7134 completed (loss: 0.03268410265445709, acc: 0.9904458522796631)
[2025-02-13 03:29:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:25][root][INFO] - Training Epoch: 1/2, step 7025/7134 completed (loss: 0.14225894212722778, acc: 0.9609195590019226)
[2025-02-13 03:29:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:26][root][INFO] - Training Epoch: 1/2, step 7026/7134 completed (loss: 0.09514760226011276, acc: 0.969111979007721)
[2025-02-13 03:29:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:26][root][INFO] - Training Epoch: 1/2, step 7027/7134 completed (loss: 0.042564019560813904, acc: 0.9817073345184326)
[2025-02-13 03:29:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:26][root][INFO] - Training Epoch: 1/2, step 7028/7134 completed (loss: 0.022762950509786606, acc: 0.9951573610305786)
[2025-02-13 03:29:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:27][root][INFO] - Training Epoch: 1/2, step 7029/7134 completed (loss: 0.029582567512989044, acc: 0.9907192587852478)
[2025-02-13 03:29:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:27][root][INFO] - Training Epoch: 1/2, step 7030/7134 completed (loss: 0.10103928297758102, acc: 0.971563994884491)
[2025-02-13 03:29:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:27][root][INFO] - Training Epoch: 1/2, step 7031/7134 completed (loss: 0.08550266921520233, acc: 0.9670050740242004)
[2025-02-13 03:29:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:28][root][INFO] - Training Epoch: 1/2, step 7032/7134 completed (loss: 0.06978101283311844, acc: 0.9864498376846313)
[2025-02-13 03:29:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:28][root][INFO] - Training Epoch: 1/2, step 7033/7134 completed (loss: 0.060077935457229614, acc: 0.9765396118164062)
[2025-02-13 03:29:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:29][root][INFO] - Training Epoch: 1/2, step 7034/7134 completed (loss: 0.09127341955900192, acc: 0.9787985682487488)
[2025-02-13 03:29:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:29][root][INFO] - Training Epoch: 1/2, step 7035/7134 completed (loss: 0.13501304388046265, acc: 0.9801587462425232)
[2025-02-13 03:29:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:29][root][INFO] - Training Epoch: 1/2, step 7036/7134 completed (loss: 0.04395689815282822, acc: 0.9835391044616699)
[2025-02-13 03:29:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:30][root][INFO] - Training Epoch: 1/2, step 7037/7134 completed (loss: 0.03830893337726593, acc: 0.98046875)
[2025-02-13 03:29:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:30][root][INFO] - Training Epoch: 1/2, step 7038/7134 completed (loss: 0.09644865989685059, acc: 0.9762532711029053)
[2025-02-13 03:29:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:30][root][INFO] - Training Epoch: 1/2, step 7039/7134 completed (loss: 0.05185694247484207, acc: 0.9806763529777527)
[2025-02-13 03:29:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:31][root][INFO] - Training Epoch: 1/2, step 7040/7134 completed (loss: 0.13681505620479584, acc: 0.954023003578186)
[2025-02-13 03:29:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:31][root][INFO] - Training Epoch: 1/2, step 7041/7134 completed (loss: 0.08821780979633331, acc: 0.9748427867889404)
[2025-02-13 03:29:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:32][root][INFO] - Training Epoch: 1/2, step 7042/7134 completed (loss: 0.04630659520626068, acc: 0.9846153855323792)
[2025-02-13 03:29:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:32][root][INFO] - Training Epoch: 1/2, step 7043/7134 completed (loss: 0.07445874810218811, acc: 0.9786666631698608)
[2025-02-13 03:29:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:32][root][INFO] - Training Epoch: 1/2, step 7044/7134 completed (loss: 0.11466015130281448, acc: 0.9694189429283142)
[2025-02-13 03:29:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:33][root][INFO] - Training Epoch: 1/2, step 7045/7134 completed (loss: 0.043158095329999924, acc: 0.9846938848495483)
[2025-02-13 03:29:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:33][root][INFO] - Training Epoch: 1/2, step 7046/7134 completed (loss: 0.08198623359203339, acc: 0.9767759442329407)
[2025-02-13 03:29:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:34][root][INFO] - Training Epoch: 1/2, step 7047/7134 completed (loss: 0.07528351247310638, acc: 0.9773809313774109)
[2025-02-13 03:29:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:34][root][INFO] - Training Epoch: 1/2, step 7048/7134 completed (loss: 0.0070670899003744125, acc: 0.9976498484611511)
[2025-02-13 03:29:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:34][root][INFO] - Training Epoch: 1/2, step 7049/7134 completed (loss: 0.028375819325447083, acc: 0.9919785857200623)
[2025-02-13 03:29:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:35][root][INFO] - Training Epoch: 1/2, step 7050/7134 completed (loss: 0.03240623325109482, acc: 0.9896373152732849)
[2025-02-13 03:29:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:35][root][INFO] - Training Epoch: 1/2, step 7051/7134 completed (loss: 0.041929133236408234, acc: 0.9873577952384949)
[2025-02-13 03:29:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:36][root][INFO] - Training Epoch: 1/2, step 7052/7134 completed (loss: 0.049006134271621704, acc: 0.989347517490387)
[2025-02-13 03:29:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:36][root][INFO] - Training Epoch: 1/2, step 7053/7134 completed (loss: 0.01721058040857315, acc: 0.9939098954200745)
[2025-02-13 03:29:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:37][root][INFO] - Training Epoch: 1/2, step 7054/7134 completed (loss: 0.05798381567001343, acc: 0.9841772317886353)
[2025-02-13 03:29:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:37][root][INFO] - Training Epoch: 1/2, step 7055/7134 completed (loss: 0.0377303808927536, acc: 0.9872449040412903)
[2025-02-13 03:29:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:37][root][INFO] - Training Epoch: 1/2, step 7056/7134 completed (loss: 0.05030566826462746, acc: 0.9903961420059204)
[2025-02-13 03:29:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:38][root][INFO] - Training Epoch: 1/2, step 7057/7134 completed (loss: 0.10333448648452759, acc: 0.9767123460769653)
[2025-02-13 03:29:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:38][root][INFO] - Training Epoch: 1/2, step 7058/7134 completed (loss: 0.0495346263051033, acc: 0.9842519760131836)
[2025-02-13 03:29:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:39][root][INFO] - Training Epoch: 1/2, step 7059/7134 completed (loss: 0.014334805309772491, acc: 0.9958100318908691)
[2025-02-13 03:29:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:39][root][INFO] - Training Epoch: 1/2, step 7060/7134 completed (loss: 0.0733468234539032, acc: 0.9822580814361572)
[2025-02-13 03:29:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:40][root][INFO] - Training Epoch: 1/2, step 7061/7134 completed (loss: 0.04661160707473755, acc: 0.9908854365348816)
[2025-02-13 03:29:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:40][root][INFO] - Training Epoch: 1/2, step 7062/7134 completed (loss: 0.10349668562412262, acc: 0.9711111187934875)
[2025-02-13 03:29:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:40][root][INFO] - Training Epoch: 1/2, step 7063/7134 completed (loss: 0.07759534567594528, acc: 0.9882352948188782)
[2025-02-13 03:29:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:41][root][INFO] - Training Epoch: 1/2, step 7064/7134 completed (loss: 0.02402760274708271, acc: 0.9955489635467529)
[2025-02-13 03:29:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:41][root][INFO] - Training Epoch: 1/2, step 7065/7134 completed (loss: 0.02346683107316494, acc: 0.9953650236129761)
[2025-02-13 03:29:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:42][root][INFO] - Training Epoch: 1/2, step 7066/7134 completed (loss: 0.035174813121557236, acc: 0.9928656220436096)
[2025-02-13 03:29:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:42][root][INFO] - Training Epoch: 1/2, step 7067/7134 completed (loss: 0.025852762162685394, acc: 0.9898089170455933)
[2025-02-13 03:29:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:43][root][INFO] - Training Epoch: 1/2, step 7068/7134 completed (loss: 0.018012043088674545, acc: 0.9951748847961426)
[2025-02-13 03:29:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:43][root][INFO] - Training Epoch: 1/2, step 7069/7134 completed (loss: 0.026902208104729652, acc: 0.9940828680992126)
[2025-02-13 03:29:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:44][root][INFO] - Training Epoch: 1/2, step 7070/7134 completed (loss: 0.028619928285479546, acc: 0.991769552230835)
[2025-02-13 03:29:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:44][root][INFO] - Training Epoch: 1/2, step 7071/7134 completed (loss: 0.06282167881727219, acc: 0.9887217879295349)
[2025-02-13 03:29:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:45][root][INFO] - Training Epoch: 1/2, step 7072/7134 completed (loss: 0.0238922331482172, acc: 0.9903030395507812)
[2025-02-13 03:29:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:45][root][INFO] - Training Epoch: 1/2, step 7073/7134 completed (loss: 0.03276561573147774, acc: 0.9931740760803223)
[2025-02-13 03:29:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:45][root][INFO] - Training Epoch: 1/2, step 7074/7134 completed (loss: 0.038213975727558136, acc: 0.9923518300056458)
[2025-02-13 03:29:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:46][root][INFO] - Training Epoch: 1/2, step 7075/7134 completed (loss: 0.027807272970676422, acc: 0.9925261735916138)
[2025-02-13 03:29:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:46][root][INFO] - Training Epoch: 1/2, step 7076/7134 completed (loss: 0.04103199765086174, acc: 0.9886040091514587)
[2025-02-13 03:29:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:47][root][INFO] - Training Epoch: 1/2, step 7077/7134 completed (loss: 0.04612036049365997, acc: 0.9836065769195557)
[2025-02-13 03:29:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:47][root][INFO] - Training Epoch: 1/2, step 7078/7134 completed (loss: 0.024286756291985512, acc: 0.9919871687889099)
[2025-02-13 03:29:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:47][root][INFO] - Training Epoch: 1/2, step 7079/7134 completed (loss: 0.036502767354249954, acc: 0.9917898178100586)
[2025-02-13 03:29:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:48][root][INFO] - Training Epoch: 1/2, step 7080/7134 completed (loss: 0.025249654427170753, acc: 0.9916943311691284)
[2025-02-13 03:29:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:48][root][INFO] - Training Epoch: 1/2, step 7081/7134 completed (loss: 0.03922959044575691, acc: 0.9873772859573364)
[2025-02-13 03:29:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:49][root][INFO] - Training Epoch: 1/2, step 7082/7134 completed (loss: 0.04918818175792694, acc: 0.987075924873352)
[2025-02-13 03:29:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:49][root][INFO] - Training Epoch: 1/2, step 7083/7134 completed (loss: 0.02036036178469658, acc: 0.9949832558631897)
[2025-02-13 03:29:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:50][root][INFO] - Training Epoch: 1/2, step 7084/7134 completed (loss: 0.04170705005526543, acc: 0.9854862093925476)
[2025-02-13 03:29:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:50][root][INFO] - Training Epoch: 1/2, step 7085/7134 completed (loss: 0.04353950917720795, acc: 0.9861111044883728)
[2025-02-13 03:29:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:50][root][INFO] - Training Epoch: 1/2, step 7086/7134 completed (loss: 0.02482699230313301, acc: 0.9904240965843201)
[2025-02-13 03:29:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:51][root][INFO] - Training Epoch: 1/2, step 7087/7134 completed (loss: 0.039814259856939316, acc: 0.9926062822341919)
[2025-02-13 03:29:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:51][root][INFO] - Training Epoch: 1/2, step 7088/7134 completed (loss: 0.057903580367565155, acc: 0.9884105920791626)
[2025-02-13 03:29:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:52][root][INFO] - Training Epoch: 1/2, step 7089/7134 completed (loss: 0.11680587381124496, acc: 0.9792993664741516)
[2025-02-13 03:29:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:52][root][INFO] - Training Epoch: 1/2, step 7090/7134 completed (loss: 0.027216270565986633, acc: 0.9945799708366394)
[2025-02-13 03:29:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:52][root][INFO] - Training Epoch: 1/2, step 7091/7134 completed (loss: 0.009826057590544224, acc: 0.9972144961357117)
[2025-02-13 03:29:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:53][root][INFO] - Training Epoch: 1/2, step 7092/7134 completed (loss: 0.019988030195236206, acc: 0.989708423614502)
[2025-02-13 03:29:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:53][root][INFO] - Training Epoch: 1/2, step 7093/7134 completed (loss: 0.03624703735113144, acc: 0.9861111044883728)
[2025-02-13 03:29:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:54][root][INFO] - Training Epoch: 1/2, step 7094/7134 completed (loss: 0.017812440171837807, acc: 0.9884792566299438)
[2025-02-13 03:29:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:54][root][INFO] - Training Epoch: 1/2, step 7095/7134 completed (loss: 0.0384526252746582, acc: 0.9917355179786682)
[2025-02-13 03:29:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:55][root][INFO] - Training Epoch: 1/2, step 7096/7134 completed (loss: 0.058104127645492554, acc: 0.9815126061439514)
[2025-02-13 03:29:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:55][root][INFO] - Training Epoch: 1/2, step 7097/7134 completed (loss: 0.056129567325115204, acc: 0.9855907559394836)
[2025-02-13 03:29:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:55][root][INFO] - Training Epoch: 1/2, step 7098/7134 completed (loss: 0.02599469944834709, acc: 0.9919742941856384)
[2025-02-13 03:29:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:56][root][INFO] - Training Epoch: 1/2, step 7099/7134 completed (loss: 0.03747314214706421, acc: 0.9863247871398926)
[2025-02-13 03:29:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:56][root][INFO] - Training Epoch: 1/2, step 7100/7134 completed (loss: 0.06284761428833008, acc: 0.9751937985420227)
[2025-02-13 03:29:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:57][root][INFO] - Training Epoch: 1/2, step 7101/7134 completed (loss: 0.06431428343057632, acc: 0.9838998317718506)
[2025-02-13 03:29:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:57][root][INFO] - Training Epoch: 1/2, step 7102/7134 completed (loss: 0.020886236801743507, acc: 0.9941860437393188)
[2025-02-13 03:29:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:57][root][INFO] - Training Epoch: 1/2, step 7103/7134 completed (loss: 0.040204379707574844, acc: 0.9959677457809448)
[2025-02-13 03:29:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:58][root][INFO] - Training Epoch: 1/2, step 7104/7134 completed (loss: 0.027773451060056686, acc: 0.9938271641731262)
[2025-02-13 03:29:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:58][root][INFO] - Training Epoch: 1/2, step 7105/7134 completed (loss: 0.0315755270421505, acc: 0.9916666746139526)
[2025-02-13 03:29:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:59][root][INFO] - Training Epoch: 1/2, step 7106/7134 completed (loss: 0.01621856726706028, acc: 0.994991660118103)
[2025-02-13 03:29:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:29:59][root][INFO] - Training Epoch: 1/2, step 7107/7134 completed (loss: 0.01981903426349163, acc: 0.9953415989875793)
[2025-02-13 03:29:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:00][root][INFO] - Training Epoch: 1/2, step 7108/7134 completed (loss: 0.035543303936719894, acc: 0.9933333396911621)
[2025-02-13 03:30:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:00][root][INFO] - Training Epoch: 1/2, step 7109/7134 completed (loss: 0.06600697338581085, acc: 0.9756097793579102)
[2025-02-13 03:30:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:01][root][INFO] - Training Epoch: 1/2, step 7110/7134 completed (loss: 0.040877602994441986, acc: 0.9864406585693359)
[2025-02-13 03:30:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:01][root][INFO] - Training Epoch: 1/2, step 7111/7134 completed (loss: 0.06081058457493782, acc: 0.9778270721435547)
[2025-02-13 03:30:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:01][root][INFO] - Training Epoch: 1/2, step 7112/7134 completed (loss: 0.03801616653800011, acc: 0.9897119402885437)
[2025-02-13 03:30:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:02][root][INFO] - Training Epoch: 1/2, step 7113/7134 completed (loss: 0.07403307408094406, acc: 0.9716088175773621)
[2025-02-13 03:30:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:02][root][INFO] - Training Epoch: 1/2, step 7114/7134 completed (loss: 0.02898481674492359, acc: 0.9942638874053955)
[2025-02-13 03:30:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:03][root][INFO] - Training Epoch: 1/2, step 7115/7134 completed (loss: 0.02637728862464428, acc: 0.9858906269073486)
[2025-02-13 03:30:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:03][root][INFO] - Training Epoch: 1/2, step 7116/7134 completed (loss: 0.022933131083846092, acc: 0.9894551634788513)
[2025-02-13 03:30:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:03][root][INFO] - Training Epoch: 1/2, step 7117/7134 completed (loss: 0.021385325118899345, acc: 0.9950000047683716)
[2025-02-13 03:30:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:04][root][INFO] - Training Epoch: 1/2, step 7118/7134 completed (loss: 0.056476060301065445, acc: 0.9869186282157898)
[2025-02-13 03:30:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:04][root][INFO] - Training Epoch: 1/2, step 7119/7134 completed (loss: 0.026337547227740288, acc: 0.9934533834457397)
[2025-02-13 03:30:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:05][root][INFO] - Training Epoch: 1/2, step 7120/7134 completed (loss: 0.03203709051012993, acc: 0.9883177280426025)
[2025-02-13 03:30:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:05][root][INFO] - Training Epoch: 1/2, step 7121/7134 completed (loss: 0.023507433012127876, acc: 0.9894366264343262)
[2025-02-13 03:30:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:05][root][INFO] - Training Epoch: 1/2, step 7122/7134 completed (loss: 0.030494360253214836, acc: 0.9866310358047485)
[2025-02-13 03:30:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:06][root][INFO] - Training Epoch: 1/2, step 7123/7134 completed (loss: 0.043710414320230484, acc: 0.9829268455505371)
[2025-02-13 03:30:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:06][root][INFO] - Training Epoch: 1/2, step 7124/7134 completed (loss: 0.03663889691233635, acc: 0.9906191229820251)
[2025-02-13 03:30:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:07][root][INFO] - Training Epoch: 1/2, step 7125/7134 completed (loss: 0.02267215959727764, acc: 0.9933664798736572)
[2025-02-13 03:30:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:07][root][INFO] - Training Epoch: 1/2, step 7126/7134 completed (loss: 0.06975147873163223, acc: 0.9730185270309448)
[2025-02-13 03:30:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:07][root][INFO] - Training Epoch: 1/2, step 7127/7134 completed (loss: 0.01732691004872322, acc: 0.993630588054657)
[2025-02-13 03:30:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:08][root][INFO] - Training Epoch: 1/2, step 7128/7134 completed (loss: 0.04035935923457146, acc: 0.9837398529052734)
[2025-02-13 03:30:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:08][root][INFO] - Training Epoch: 1/2, step 7129/7134 completed (loss: 0.07775190472602844, acc: 0.9709172248840332)
[2025-02-13 03:30:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:09][root][INFO] - Training Epoch: 1/2, step 7130/7134 completed (loss: 0.10185570269823074, acc: 0.9642857313156128)
[2025-02-13 03:30:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:09][root][INFO] - Training Epoch: 1/2, step 7131/7134 completed (loss: 0.015859942883253098, acc: 0.9976798295974731)
[2025-02-13 03:30:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:30:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:31:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:32:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:33:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:14][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.0486, device='cuda:0') eval_epoch_loss=tensor(0.0475, device='cuda:0') eval_epoch_acc=tensor(0.9869, device='cuda:0')
[2025-02-13 03:34:14][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-13 03:34:14][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-13 03:34:14][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme/asr_epoch_1_step_7132_loss_0.047481898218393326/model.pt
[2025-02-13 03:34:14][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme directory
[2025-02-13 03:34:14][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.047481898218393326
[2025-02-13 03:34:14][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.9869428277015686
[2025-02-13 03:34:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:15][root][INFO] - Training Epoch: 1/2, step 7132/7134 completed (loss: 0.022141119465231895, acc: 0.9898989796638489)
[2025-02-13 03:34:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:15][root][INFO] - Training Epoch: 1/2, step 7133/7134 completed (loss: 0.007854434661567211, acc: 0.9981516003608704)
[2025-02-13 03:34:16][slam_llm.utils.train_utils][INFO] - Epoch 1: train_perplexity=1.0752, train_epoch_loss=0.0725, epoch time 4125.134121477604s
[2025-02-13 03:34:16][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 14 GB
[2025-02-13 03:34:16][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 30 GB
[2025-02-13 03:34:16][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 14 GB
[2025-02-13 03:34:16][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 6
[2025-02-13 03:34:16][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 7 GB
[2025-02-13 03:34:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:17][root][INFO] - Training Epoch: 2/2, step 0/7134 completed (loss: 0.03321065753698349, acc: 0.9881129264831543)
[2025-02-13 03:34:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:17][root][INFO] - Training Epoch: 2/2, step 1/7134 completed (loss: 0.0329800620675087, acc: 0.991631805896759)
[2025-02-13 03:34:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:18][root][INFO] - Training Epoch: 2/2, step 2/7134 completed (loss: 0.03320595994591713, acc: 0.993306577205658)
[2025-02-13 03:34:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:18][root][INFO] - Training Epoch: 2/2, step 3/7134 completed (loss: 0.006145957857370377, acc: 0.9987195730209351)
[2025-02-13 03:34:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:19][root][INFO] - Training Epoch: 2/2, step 4/7134 completed (loss: 0.03176300227642059, acc: 0.9904502034187317)
[2025-02-13 03:34:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:19][root][INFO] - Training Epoch: 2/2, step 5/7134 completed (loss: 0.01811317726969719, acc: 0.9949238300323486)
[2025-02-13 03:34:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:20][root][INFO] - Training Epoch: 2/2, step 6/7134 completed (loss: 0.009223880246281624, acc: 0.9967897534370422)
[2025-02-13 03:34:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:20][root][INFO] - Training Epoch: 2/2, step 7/7134 completed (loss: 0.037385620176792145, acc: 0.9912827014923096)
[2025-02-13 03:34:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:21][root][INFO] - Training Epoch: 2/2, step 8/7134 completed (loss: 0.06731145083904266, acc: 0.9821183085441589)
[2025-02-13 03:34:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:21][root][INFO] - Training Epoch: 2/2, step 9/7134 completed (loss: 0.01421560812741518, acc: 0.9953488111495972)
[2025-02-13 03:34:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:22][root][INFO] - Training Epoch: 2/2, step 10/7134 completed (loss: 0.037625204771757126, acc: 0.9871976971626282)
[2025-02-13 03:34:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:22][root][INFO] - Training Epoch: 2/2, step 11/7134 completed (loss: 0.009938440285623074, acc: 0.998123824596405)
[2025-02-13 03:34:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:23][root][INFO] - Training Epoch: 2/2, step 12/7134 completed (loss: 0.007883424870669842, acc: 0.9973081946372986)
[2025-02-13 03:34:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:23][root][INFO] - Training Epoch: 2/2, step 13/7134 completed (loss: 0.023433921858668327, acc: 0.993122398853302)
[2025-02-13 03:34:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:23][root][INFO] - Training Epoch: 2/2, step 14/7134 completed (loss: 0.015813706442713737, acc: 0.9966386556625366)
[2025-02-13 03:34:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:24][root][INFO] - Training Epoch: 2/2, step 15/7134 completed (loss: 0.03232685849070549, acc: 0.9948365092277527)
[2025-02-13 03:34:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:24][root][INFO] - Training Epoch: 2/2, step 16/7134 completed (loss: 0.002875683829188347, acc: 1.0)
[2025-02-13 03:34:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:25][root][INFO] - Training Epoch: 2/2, step 17/7134 completed (loss: 0.07491124421358109, acc: 0.9756757020950317)
[2025-02-13 03:34:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:25][root][INFO] - Training Epoch: 2/2, step 18/7134 completed (loss: 0.023517662659287453, acc: 0.9914772510528564)
[2025-02-13 03:34:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:26][root][INFO] - Training Epoch: 2/2, step 19/7134 completed (loss: 0.03324231505393982, acc: 0.9862068891525269)
[2025-02-13 03:34:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:26][root][INFO] - Training Epoch: 2/2, step 20/7134 completed (loss: 0.009356272406876087, acc: 0.996052622795105)
[2025-02-13 03:34:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:27][root][INFO] - Training Epoch: 2/2, step 21/7134 completed (loss: 0.008049155585467815, acc: 0.998711347579956)
[2025-02-13 03:34:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:27][root][INFO] - Training Epoch: 2/2, step 22/7134 completed (loss: 0.0016690798802301288, acc: 1.0)
[2025-02-13 03:34:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:27][root][INFO] - Training Epoch: 2/2, step 23/7134 completed (loss: 0.005200530402362347, acc: 1.0)
[2025-02-13 03:34:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:28][root][INFO] - Training Epoch: 2/2, step 24/7134 completed (loss: 0.013548909686505795, acc: 0.9958217144012451)
[2025-02-13 03:34:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:28][root][INFO] - Training Epoch: 2/2, step 25/7134 completed (loss: 0.016767874360084534, acc: 0.9971590638160706)
[2025-02-13 03:34:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:29][root][INFO] - Training Epoch: 2/2, step 26/7134 completed (loss: 0.032168466597795486, acc: 0.9882006049156189)
[2025-02-13 03:34:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:29][root][INFO] - Training Epoch: 2/2, step 27/7134 completed (loss: 0.011569460853934288, acc: 0.9985228776931763)
[2025-02-13 03:34:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:30][root][INFO] - Training Epoch: 2/2, step 28/7134 completed (loss: 0.06308958679437637, acc: 0.983132541179657)
[2025-02-13 03:34:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:30][root][INFO] - Training Epoch: 2/2, step 29/7134 completed (loss: 0.037551067769527435, acc: 0.9900850057601929)
[2025-02-13 03:34:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:31][root][INFO] - Training Epoch: 2/2, step 30/7134 completed (loss: 0.025182297453284264, acc: 0.989830493927002)
[2025-02-13 03:34:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:31][root][INFO] - Training Epoch: 2/2, step 31/7134 completed (loss: 0.02664554864168167, acc: 0.992584764957428)
[2025-02-13 03:34:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:32][root][INFO] - Training Epoch: 2/2, step 32/7134 completed (loss: 0.05614640191197395, acc: 0.9843953251838684)
[2025-02-13 03:34:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:32][root][INFO] - Training Epoch: 2/2, step 33/7134 completed (loss: 0.08668701350688934, acc: 0.980079710483551)
[2025-02-13 03:34:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:32][root][INFO] - Training Epoch: 2/2, step 34/7134 completed (loss: 0.044498782604932785, acc: 0.9882766604423523)
[2025-02-13 03:34:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:33][root][INFO] - Training Epoch: 2/2, step 35/7134 completed (loss: 0.04375726729631424, acc: 0.9868049025535583)
[2025-02-13 03:34:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:34][root][INFO] - Training Epoch: 2/2, step 36/7134 completed (loss: 0.047453418374061584, acc: 0.9913138151168823)
[2025-02-13 03:34:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:34][root][INFO] - Training Epoch: 2/2, step 37/7134 completed (loss: 0.03828943520784378, acc: 0.9887359142303467)
[2025-02-13 03:34:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:34][root][INFO] - Training Epoch: 2/2, step 38/7134 completed (loss: 0.024939944967627525, acc: 0.9910714030265808)
[2025-02-13 03:34:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:35][root][INFO] - Training Epoch: 2/2, step 39/7134 completed (loss: 0.004097843077033758, acc: 1.0)
[2025-02-13 03:34:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:35][root][INFO] - Training Epoch: 2/2, step 40/7134 completed (loss: 0.014439238235354424, acc: 0.9955157041549683)
[2025-02-13 03:34:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:36][root][INFO] - Training Epoch: 2/2, step 41/7134 completed (loss: 0.016603631898760796, acc: 0.994452178478241)
[2025-02-13 03:34:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:36][root][INFO] - Training Epoch: 2/2, step 42/7134 completed (loss: 0.0562320202589035, acc: 0.9901823401451111)
[2025-02-13 03:34:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:37][root][INFO] - Training Epoch: 2/2, step 43/7134 completed (loss: 0.012065967544913292, acc: 0.996129035949707)
[2025-02-13 03:34:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:37][root][INFO] - Training Epoch: 2/2, step 44/7134 completed (loss: 0.041746754199266434, acc: 0.9923567175865173)
[2025-02-13 03:34:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:38][root][INFO] - Training Epoch: 2/2, step 45/7134 completed (loss: 0.03682409226894379, acc: 0.9945828914642334)
[2025-02-13 03:34:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:38][root][INFO] - Training Epoch: 2/2, step 46/7134 completed (loss: 0.04364639148116112, acc: 0.9906666874885559)
[2025-02-13 03:34:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:38][root][INFO] - Training Epoch: 2/2, step 47/7134 completed (loss: 0.03786525875329971, acc: 0.9898989796638489)
[2025-02-13 03:34:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:39][root][INFO] - Training Epoch: 2/2, step 48/7134 completed (loss: 0.016369611024856567, acc: 0.9966292381286621)
[2025-02-13 03:34:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:39][root][INFO] - Training Epoch: 2/2, step 49/7134 completed (loss: 0.015253004617989063, acc: 0.9972565174102783)
[2025-02-13 03:34:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:40][root][INFO] - Training Epoch: 2/2, step 50/7134 completed (loss: 0.01063498668372631, acc: 0.9976931810379028)
[2025-02-13 03:34:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:40][root][INFO] - Training Epoch: 2/2, step 51/7134 completed (loss: 0.013663684949278831, acc: 0.9974619150161743)
[2025-02-13 03:34:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:41][root][INFO] - Training Epoch: 2/2, step 52/7134 completed (loss: 0.0047492398880422115, acc: 0.9983739852905273)
[2025-02-13 03:34:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:41][root][INFO] - Training Epoch: 2/2, step 53/7134 completed (loss: 0.01182324718683958, acc: 0.9947506785392761)
[2025-02-13 03:34:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:42][root][INFO] - Training Epoch: 2/2, step 54/7134 completed (loss: 0.004800151567906141, acc: 0.9976984858512878)
[2025-02-13 03:34:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:42][root][INFO] - Training Epoch: 2/2, step 55/7134 completed (loss: 0.02231498621404171, acc: 0.9936548471450806)
[2025-02-13 03:34:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:43][root][INFO] - Training Epoch: 2/2, step 56/7134 completed (loss: 0.011017831042408943, acc: 0.9964285492897034)
[2025-02-13 03:34:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:43][root][INFO] - Training Epoch: 2/2, step 57/7134 completed (loss: 0.046801116317510605, acc: 0.9850931763648987)
[2025-02-13 03:34:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:44][root][INFO] - Training Epoch: 2/2, step 58/7134 completed (loss: 0.031863391399383545, acc: 0.9923175573348999)
[2025-02-13 03:34:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:44][root][INFO] - Training Epoch: 2/2, step 59/7134 completed (loss: 0.016331937164068222, acc: 0.9952718615531921)
[2025-02-13 03:34:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:45][root][INFO] - Training Epoch: 2/2, step 60/7134 completed (loss: 0.028994308784604073, acc: 0.9914772510528564)
[2025-02-13 03:34:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:45][root][INFO] - Training Epoch: 2/2, step 61/7134 completed (loss: 0.03355865925550461, acc: 0.993880033493042)
[2025-02-13 03:34:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:45][root][INFO] - Training Epoch: 2/2, step 62/7134 completed (loss: 0.021429074928164482, acc: 0.9898348450660706)
[2025-02-13 03:34:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:46][root][INFO] - Training Epoch: 2/2, step 63/7134 completed (loss: 0.04999946430325508, acc: 0.9887920022010803)
[2025-02-13 03:34:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:46][root][INFO] - Training Epoch: 2/2, step 64/7134 completed (loss: 0.015850050374865532, acc: 0.9946236610412598)
[2025-02-13 03:34:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:47][root][INFO] - Training Epoch: 2/2, step 65/7134 completed (loss: 0.049142733216285706, acc: 0.9815725088119507)
[2025-02-13 03:34:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:47][root][INFO] - Training Epoch: 2/2, step 66/7134 completed (loss: 0.04889672249555588, acc: 0.9856630563735962)
[2025-02-13 03:34:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:48][root][INFO] - Training Epoch: 2/2, step 67/7134 completed (loss: 0.03707598149776459, acc: 0.9882352948188782)
[2025-02-13 03:34:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:48][root][INFO] - Training Epoch: 2/2, step 68/7134 completed (loss: 0.044641267508268356, acc: 0.9858064651489258)
[2025-02-13 03:34:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:49][root][INFO] - Training Epoch: 2/2, step 69/7134 completed (loss: 0.04864375293254852, acc: 0.9837905168533325)
[2025-02-13 03:34:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:49][root][INFO] - Training Epoch: 2/2, step 70/7134 completed (loss: 0.04094883054494858, acc: 0.9912280440330505)
[2025-02-13 03:34:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:50][root][INFO] - Training Epoch: 2/2, step 71/7134 completed (loss: 0.01731746830046177, acc: 0.9957982897758484)
[2025-02-13 03:34:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:50][root][INFO] - Training Epoch: 2/2, step 72/7134 completed (loss: 0.028913168236613274, acc: 0.9910485744476318)
[2025-02-13 03:34:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:51][root][INFO] - Training Epoch: 2/2, step 73/7134 completed (loss: 0.01948394812643528, acc: 0.991847813129425)
[2025-02-13 03:34:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:51][root][INFO] - Training Epoch: 2/2, step 74/7134 completed (loss: 0.027265263721346855, acc: 0.9916434288024902)
[2025-02-13 03:34:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:52][root][INFO] - Training Epoch: 2/2, step 75/7134 completed (loss: 0.05660715326666832, acc: 0.9865471124649048)
[2025-02-13 03:34:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:52][root][INFO] - Training Epoch: 2/2, step 76/7134 completed (loss: 0.011657673865556717, acc: 0.996221661567688)
[2025-02-13 03:34:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:52][root][INFO] - Training Epoch: 2/2, step 77/7134 completed (loss: 0.02337789349257946, acc: 0.9962406158447266)
[2025-02-13 03:34:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:53][root][INFO] - Training Epoch: 2/2, step 78/7134 completed (loss: 0.048726167529821396, acc: 0.9795597195625305)
[2025-02-13 03:34:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:53][root][INFO] - Training Epoch: 2/2, step 79/7134 completed (loss: 0.05282801762223244, acc: 0.9783950448036194)
[2025-02-13 03:34:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:54][root][INFO] - Training Epoch: 2/2, step 80/7134 completed (loss: 0.030139511451125145, acc: 0.9947916865348816)
[2025-02-13 03:34:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:54][root][INFO] - Training Epoch: 2/2, step 81/7134 completed (loss: 0.028253799304366112, acc: 0.9921466112136841)
[2025-02-13 03:34:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:55][root][INFO] - Training Epoch: 2/2, step 82/7134 completed (loss: 0.04527983069419861, acc: 0.9844412803649902)
[2025-02-13 03:34:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:55][root][INFO] - Training Epoch: 2/2, step 83/7134 completed (loss: 0.03223299980163574, acc: 0.9884318709373474)
[2025-02-13 03:34:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:56][root][INFO] - Training Epoch: 2/2, step 84/7134 completed (loss: 0.01715908572077751, acc: 0.9948186278343201)
[2025-02-13 03:34:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:56][root][INFO] - Training Epoch: 2/2, step 85/7134 completed (loss: 0.034421149641275406, acc: 0.9904534816741943)
[2025-02-13 03:34:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:57][root][INFO] - Training Epoch: 2/2, step 86/7134 completed (loss: 0.02724580653011799, acc: 0.9908257126808167)
[2025-02-13 03:34:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:57][root][INFO] - Training Epoch: 2/2, step 87/7134 completed (loss: 0.09557989239692688, acc: 0.9765051603317261)
[2025-02-13 03:34:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:58][root][INFO] - Training Epoch: 2/2, step 88/7134 completed (loss: 0.08896220475435257, acc: 0.97648686170578)
[2025-02-13 03:34:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:58][root][INFO] - Training Epoch: 2/2, step 89/7134 completed (loss: 0.03570037707686424, acc: 0.9922978281974792)
[2025-02-13 03:34:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:59][root][INFO] - Training Epoch: 2/2, step 90/7134 completed (loss: 0.03409244120121002, acc: 0.9895104765892029)
[2025-02-13 03:34:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:59][root][INFO] - Training Epoch: 2/2, step 91/7134 completed (loss: 0.03426827862858772, acc: 0.9897959232330322)
[2025-02-13 03:34:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:34:59][root][INFO] - Training Epoch: 2/2, step 92/7134 completed (loss: 0.04118948057293892, acc: 0.9859747290611267)
[2025-02-13 03:35:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:00][root][INFO] - Training Epoch: 2/2, step 93/7134 completed (loss: 0.01457070279866457, acc: 0.994490385055542)
[2025-02-13 03:35:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:00][root][INFO] - Training Epoch: 2/2, step 94/7134 completed (loss: 0.06886941939592361, acc: 0.9837586879730225)
[2025-02-13 03:35:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:01][root][INFO] - Training Epoch: 2/2, step 95/7134 completed (loss: 0.03288515284657478, acc: 0.9899497628211975)
[2025-02-13 03:35:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:01][root][INFO] - Training Epoch: 2/2, step 96/7134 completed (loss: 0.02336558885872364, acc: 0.9925512075424194)
[2025-02-13 03:35:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:02][root][INFO] - Training Epoch: 2/2, step 97/7134 completed (loss: 0.18893228471279144, acc: 0.9698795080184937)
[2025-02-13 03:35:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:02][root][INFO] - Training Epoch: 2/2, step 98/7134 completed (loss: 0.07175381481647491, acc: 0.9805825352668762)
[2025-02-13 03:35:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:03][root][INFO] - Training Epoch: 2/2, step 99/7134 completed (loss: 0.06982272863388062, acc: 0.9743119478225708)
[2025-02-13 03:35:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:03][root][INFO] - Training Epoch: 2/2, step 100/7134 completed (loss: 0.03632419928908348, acc: 0.9897435903549194)
[2025-02-13 03:35:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:03][root][INFO] - Training Epoch: 2/2, step 101/7134 completed (loss: 0.04727950692176819, acc: 0.9819079041481018)
[2025-02-13 03:35:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:04][root][INFO] - Training Epoch: 2/2, step 102/7134 completed (loss: 0.03442306071519852, acc: 0.990231990814209)
[2025-02-13 03:35:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:04][root][INFO] - Training Epoch: 2/2, step 103/7134 completed (loss: 0.04649738222360611, acc: 0.9816666841506958)
[2025-02-13 03:35:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:05][root][INFO] - Training Epoch: 2/2, step 104/7134 completed (loss: 0.01715170219540596, acc: 0.9939576983451843)
[2025-02-13 03:35:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:05][root][INFO] - Training Epoch: 2/2, step 105/7134 completed (loss: 0.021786637604236603, acc: 0.995106041431427)
[2025-02-13 03:35:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:06][root][INFO] - Training Epoch: 2/2, step 106/7134 completed (loss: 0.05681242048740387, acc: 0.9786700010299683)
[2025-02-13 03:35:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:06][root][INFO] - Training Epoch: 2/2, step 107/7134 completed (loss: 0.10899554938077927, acc: 0.9627507328987122)
[2025-02-13 03:35:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:07][root][INFO] - Training Epoch: 2/2, step 108/7134 completed (loss: 0.05111435428261757, acc: 0.986810564994812)
[2025-02-13 03:35:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:07][root][INFO] - Training Epoch: 2/2, step 109/7134 completed (loss: 0.028191376477479935, acc: 0.9871323704719543)
[2025-02-13 03:35:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:08][root][INFO] - Training Epoch: 2/2, step 110/7134 completed (loss: 0.06018589809536934, acc: 0.9835487604141235)
[2025-02-13 03:35:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:08][root][INFO] - Training Epoch: 2/2, step 111/7134 completed (loss: 0.04400338605046272, acc: 0.9895470142364502)
[2025-02-13 03:35:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:08][root][INFO] - Training Epoch: 2/2, step 112/7134 completed (loss: 0.021872282028198242, acc: 0.9922958612442017)
[2025-02-13 03:35:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:09][root][INFO] - Training Epoch: 2/2, step 113/7134 completed (loss: 0.016778359189629555, acc: 0.9974683523178101)
[2025-02-13 03:35:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:09][root][INFO] - Training Epoch: 2/2, step 114/7134 completed (loss: 0.038217250257730484, acc: 0.9854111671447754)
[2025-02-13 03:35:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:10][root][INFO] - Training Epoch: 2/2, step 115/7134 completed (loss: 0.09109421819448471, acc: 0.9685039520263672)
[2025-02-13 03:35:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:10][root][INFO] - Training Epoch: 2/2, step 116/7134 completed (loss: 0.0424468107521534, acc: 0.9858657121658325)
[2025-02-13 03:35:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:11][root][INFO] - Training Epoch: 2/2, step 117/7134 completed (loss: 0.02461782656610012, acc: 0.9889196753501892)
[2025-02-13 03:35:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:11][root][INFO] - Training Epoch: 2/2, step 118/7134 completed (loss: 0.025942256674170494, acc: 0.9885203838348389)
[2025-02-13 03:35:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:12][root][INFO] - Training Epoch: 2/2, step 119/7134 completed (loss: 0.039745017886161804, acc: 0.9854721426963806)
[2025-02-13 03:35:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:12][root][INFO] - Training Epoch: 2/2, step 120/7134 completed (loss: 0.018243122845888138, acc: 0.9947575330734253)
[2025-02-13 03:35:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:12][root][INFO] - Training Epoch: 2/2, step 121/7134 completed (loss: 0.02881481498479843, acc: 0.9947159886360168)
[2025-02-13 03:35:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:13][root][INFO] - Training Epoch: 2/2, step 122/7134 completed (loss: 0.022108346223831177, acc: 0.9894039630889893)
[2025-02-13 03:35:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:13][root][INFO] - Training Epoch: 2/2, step 123/7134 completed (loss: 0.010830226354300976, acc: 0.9954954981803894)
[2025-02-13 03:35:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:14][root][INFO] - Training Epoch: 2/2, step 124/7134 completed (loss: 0.020046452060341835, acc: 0.9913793206214905)
[2025-02-13 03:35:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:14][root][INFO] - Training Epoch: 2/2, step 125/7134 completed (loss: 0.02407354675233364, acc: 0.9955752491950989)
[2025-02-13 03:35:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:15][root][INFO] - Training Epoch: 2/2, step 126/7134 completed (loss: 0.014153605327010155, acc: 0.9946666955947876)
[2025-02-13 03:35:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:15][root][INFO] - Training Epoch: 2/2, step 127/7134 completed (loss: 0.014975770376622677, acc: 0.9938271641731262)
[2025-02-13 03:35:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:16][root][INFO] - Training Epoch: 2/2, step 128/7134 completed (loss: 0.021161705255508423, acc: 0.9965397715568542)
[2025-02-13 03:35:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:16][root][INFO] - Training Epoch: 2/2, step 129/7134 completed (loss: 0.024126389995217323, acc: 0.9938725233078003)
[2025-02-13 03:35:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:17][root][INFO] - Training Epoch: 2/2, step 130/7134 completed (loss: 0.020657112821936607, acc: 0.99210524559021)
[2025-02-13 03:35:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:17][root][INFO] - Training Epoch: 2/2, step 131/7134 completed (loss: 0.01729719340801239, acc: 0.99210524559021)
[2025-02-13 03:35:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:17][root][INFO] - Training Epoch: 2/2, step 132/7134 completed (loss: 0.024789486080408096, acc: 0.9912917017936707)
[2025-02-13 03:35:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:18][root][INFO] - Training Epoch: 2/2, step 133/7134 completed (loss: 0.004927605856209993, acc: 1.0)
[2025-02-13 03:35:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:18][root][INFO] - Training Epoch: 2/2, step 134/7134 completed (loss: 0.028886903077363968, acc: 0.9929203391075134)
[2025-02-13 03:35:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:19][root][INFO] - Training Epoch: 2/2, step 135/7134 completed (loss: 0.014349608682096004, acc: 0.995555579662323)
[2025-02-13 03:35:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:19][root][INFO] - Training Epoch: 2/2, step 136/7134 completed (loss: 0.018527699634432793, acc: 0.9952210187911987)
[2025-02-13 03:35:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:20][root][INFO] - Training Epoch: 2/2, step 137/7134 completed (loss: 0.039651695638895035, acc: 0.9913899302482605)
[2025-02-13 03:35:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:20][root][INFO] - Training Epoch: 2/2, step 138/7134 completed (loss: 0.020144088193774223, acc: 0.992414653301239)
[2025-02-13 03:35:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:21][root][INFO] - Training Epoch: 2/2, step 139/7134 completed (loss: 0.038678985089063644, acc: 0.9910485744476318)
[2025-02-13 03:35:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:21][root][INFO] - Training Epoch: 2/2, step 140/7134 completed (loss: 0.015907326713204384, acc: 0.9940298795700073)
[2025-02-13 03:35:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:22][root][INFO] - Training Epoch: 2/2, step 141/7134 completed (loss: 0.00558716244995594, acc: 0.9976415038108826)
[2025-02-13 03:35:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:22][root][INFO] - Training Epoch: 2/2, step 142/7134 completed (loss: 0.053779251873493195, acc: 0.9888734221458435)
[2025-02-13 03:35:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:23][root][INFO] - Training Epoch: 2/2, step 143/7134 completed (loss: 0.010808605700731277, acc: 0.9947848916053772)
[2025-02-13 03:35:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:23][root][INFO] - Training Epoch: 2/2, step 144/7134 completed (loss: 0.08302699029445648, acc: 0.9799666404724121)
[2025-02-13 03:35:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:24][root][INFO] - Training Epoch: 2/2, step 145/7134 completed (loss: 0.10251836478710175, acc: 0.9661971926689148)
[2025-02-13 03:35:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:24][root][INFO] - Training Epoch: 2/2, step 146/7134 completed (loss: 0.06662855297327042, acc: 0.9786324501037598)
[2025-02-13 03:35:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:24][root][INFO] - Training Epoch: 2/2, step 147/7134 completed (loss: 0.061465125530958176, acc: 0.9861351847648621)
[2025-02-13 03:35:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:25][root][INFO] - Training Epoch: 2/2, step 148/7134 completed (loss: 0.1162761002779007, acc: 0.9605568647384644)
[2025-02-13 03:35:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:25][root][INFO] - Training Epoch: 2/2, step 149/7134 completed (loss: 0.11867538839578629, acc: 0.9767742156982422)
[2025-02-13 03:35:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:26][root][INFO] - Training Epoch: 2/2, step 150/7134 completed (loss: 0.07721998542547226, acc: 0.9759358167648315)
[2025-02-13 03:35:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:26][root][INFO] - Training Epoch: 2/2, step 151/7134 completed (loss: 0.05088400840759277, acc: 0.9868735074996948)
[2025-02-13 03:35:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:27][root][INFO] - Training Epoch: 2/2, step 152/7134 completed (loss: 0.04595494642853737, acc: 0.9874715209007263)
[2025-02-13 03:35:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:27][root][INFO] - Training Epoch: 2/2, step 153/7134 completed (loss: 0.09098633378744125, acc: 0.9818435907363892)
[2025-02-13 03:35:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:28][root][INFO] - Training Epoch: 2/2, step 154/7134 completed (loss: 0.04873628169298172, acc: 0.9878419637680054)
[2025-02-13 03:35:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:28][root][INFO] - Training Epoch: 2/2, step 155/7134 completed (loss: 0.045741867274045944, acc: 0.9857142567634583)
[2025-02-13 03:35:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:29][root][INFO] - Training Epoch: 2/2, step 156/7134 completed (loss: 0.033780287951231, acc: 0.9883313775062561)
[2025-02-13 03:35:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:29][root][INFO] - Training Epoch: 2/2, step 157/7134 completed (loss: 0.07673423737287521, acc: 0.9855421781539917)
[2025-02-13 03:35:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:30][root][INFO] - Training Epoch: 2/2, step 158/7134 completed (loss: 0.020584098994731903, acc: 0.9960370063781738)
[2025-02-13 03:35:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:30][root][INFO] - Training Epoch: 2/2, step 159/7134 completed (loss: 0.05439385026693344, acc: 0.9870129823684692)
[2025-02-13 03:35:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:31][root][INFO] - Training Epoch: 2/2, step 160/7134 completed (loss: 0.05368056148290634, acc: 0.9797570705413818)
[2025-02-13 03:35:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:31][root][INFO] - Training Epoch: 2/2, step 161/7134 completed (loss: 0.018115870654582977, acc: 0.9912790656089783)
[2025-02-13 03:35:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:31][root][INFO] - Training Epoch: 2/2, step 162/7134 completed (loss: 0.035138439387083054, acc: 0.9820972084999084)
[2025-02-13 03:35:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:32][root][INFO] - Training Epoch: 2/2, step 163/7134 completed (loss: 0.05508122593164444, acc: 0.9881831407546997)
[2025-02-13 03:35:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:32][root][INFO] - Training Epoch: 2/2, step 164/7134 completed (loss: 0.04740563780069351, acc: 0.9893617033958435)
[2025-02-13 03:35:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:33][root][INFO] - Training Epoch: 2/2, step 165/7134 completed (loss: 0.06332389265298843, acc: 0.9832535982131958)
[2025-02-13 03:35:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:33][root][INFO] - Training Epoch: 2/2, step 166/7134 completed (loss: 0.039035335183143616, acc: 0.9940298795700073)
[2025-02-13 03:35:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:33][root][INFO] - Training Epoch: 2/2, step 167/7134 completed (loss: 0.48184263706207275, acc: 0.9001782536506653)
[2025-02-13 03:35:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:34][root][INFO] - Training Epoch: 2/2, step 168/7134 completed (loss: 1.3929702043533325, acc: 0.7176470756530762)
[2025-02-13 03:35:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:34][root][INFO] - Training Epoch: 2/2, step 169/7134 completed (loss: 0.948585033416748, acc: 0.7899159789085388)
[2025-02-13 03:35:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:35][root][INFO] - Training Epoch: 2/2, step 170/7134 completed (loss: 0.36124688386917114, acc: 0.8994307518005371)
[2025-02-13 03:35:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:35][root][INFO] - Training Epoch: 2/2, step 171/7134 completed (loss: 0.45418494939804077, acc: 0.9058441519737244)
[2025-02-13 03:35:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:36][root][INFO] - Training Epoch: 2/2, step 172/7134 completed (loss: 0.1887490451335907, acc: 0.9486552476882935)
[2025-02-13 03:35:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:36][root][INFO] - Training Epoch: 2/2, step 173/7134 completed (loss: 0.14369451999664307, acc: 0.9579645991325378)
[2025-02-13 03:35:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:36][root][INFO] - Training Epoch: 2/2, step 174/7134 completed (loss: 0.1370217502117157, acc: 0.9683098793029785)
[2025-02-13 03:35:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:37][root][INFO] - Training Epoch: 2/2, step 175/7134 completed (loss: 0.15682917833328247, acc: 0.9636363387107849)
[2025-02-13 03:35:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:37][root][INFO] - Training Epoch: 2/2, step 176/7134 completed (loss: 0.15651749074459076, acc: 0.9521738886833191)
[2025-02-13 03:35:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:38][root][INFO] - Training Epoch: 2/2, step 177/7134 completed (loss: 0.13609737157821655, acc: 0.9648148417472839)
[2025-02-13 03:35:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:38][root][INFO] - Training Epoch: 2/2, step 178/7134 completed (loss: 0.12278789281845093, acc: 0.9496123790740967)
[2025-02-13 03:35:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:39][root][INFO] - Training Epoch: 2/2, step 179/7134 completed (loss: 0.07410921156406403, acc: 0.9748549461364746)
[2025-02-13 03:35:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:39][root][INFO] - Training Epoch: 2/2, step 180/7134 completed (loss: 0.057438917458057404, acc: 0.980719804763794)
[2025-02-13 03:35:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:40][root][INFO] - Training Epoch: 2/2, step 181/7134 completed (loss: 0.041114985942840576, acc: 0.9841897487640381)
[2025-02-13 03:35:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:40][root][INFO] - Training Epoch: 2/2, step 182/7134 completed (loss: 0.02700495906174183, acc: 0.9963369965553284)
[2025-02-13 03:35:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:40][root][INFO] - Training Epoch: 2/2, step 183/7134 completed (loss: 0.03839989751577377, acc: 0.9904631972312927)
[2025-02-13 03:35:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:41][root][INFO] - Training Epoch: 2/2, step 184/7134 completed (loss: 0.037395115941762924, acc: 0.9877111911773682)
[2025-02-13 03:35:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:41][root][INFO] - Training Epoch: 2/2, step 185/7134 completed (loss: 0.014434436336159706, acc: 0.9971387982368469)
[2025-02-13 03:35:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:42][root][INFO] - Training Epoch: 2/2, step 186/7134 completed (loss: 0.045793566852808, acc: 0.9871794581413269)
[2025-02-13 03:35:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:42][root][INFO] - Training Epoch: 2/2, step 187/7134 completed (loss: 0.049939773976802826, acc: 0.9860334992408752)
[2025-02-13 03:35:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:43][root][INFO] - Training Epoch: 2/2, step 188/7134 completed (loss: 0.10804668813943863, acc: 0.9666666388511658)
[2025-02-13 03:35:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:43][root][INFO] - Training Epoch: 2/2, step 189/7134 completed (loss: 0.05261114612221718, acc: 0.9865384697914124)
[2025-02-13 03:35:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:44][root][INFO] - Training Epoch: 2/2, step 190/7134 completed (loss: 0.1180192083120346, acc: 0.9734789133071899)
[2025-02-13 03:35:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:44][root][INFO] - Training Epoch: 2/2, step 191/7134 completed (loss: 0.057612475007772446, acc: 0.9935897588729858)
[2025-02-13 03:35:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:44][root][INFO] - Training Epoch: 2/2, step 192/7134 completed (loss: 0.008357813581824303, acc: 0.9977090358734131)
[2025-02-13 03:35:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:45][root][INFO] - Training Epoch: 2/2, step 193/7134 completed (loss: 0.04404871538281441, acc: 0.9865771532058716)
[2025-02-13 03:35:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:45][root][INFO] - Training Epoch: 2/2, step 194/7134 completed (loss: 0.04374261200428009, acc: 0.9856528043746948)
[2025-02-13 03:35:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:46][root][INFO] - Training Epoch: 2/2, step 195/7134 completed (loss: 0.031999148428440094, acc: 0.9865546226501465)
[2025-02-13 03:35:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:46][root][INFO] - Training Epoch: 2/2, step 196/7134 completed (loss: 0.07319148629903793, acc: 0.9866310358047485)
[2025-02-13 03:35:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:47][root][INFO] - Training Epoch: 2/2, step 197/7134 completed (loss: 0.03365206718444824, acc: 0.9914529919624329)
[2025-02-13 03:35:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:47][root][INFO] - Training Epoch: 2/2, step 198/7134 completed (loss: 0.025416580960154533, acc: 0.9935979247093201)
[2025-02-13 03:35:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:48][root][INFO] - Training Epoch: 2/2, step 199/7134 completed (loss: 0.0531696155667305, acc: 0.9835796356201172)
[2025-02-13 03:35:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:48][root][INFO] - Training Epoch: 2/2, step 200/7134 completed (loss: 0.026826942339539528, acc: 0.9946308732032776)
[2025-02-13 03:35:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:48][root][INFO] - Training Epoch: 2/2, step 201/7134 completed (loss: 0.051552820950746536, acc: 0.983146071434021)
[2025-02-13 03:35:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:49][root][INFO] - Training Epoch: 2/2, step 202/7134 completed (loss: 0.07182849198579788, acc: 0.9806896448135376)
[2025-02-13 03:35:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:49][root][INFO] - Training Epoch: 2/2, step 203/7134 completed (loss: 0.07755811512470245, acc: 0.9752066135406494)
[2025-02-13 03:35:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:50][root][INFO] - Training Epoch: 2/2, step 204/7134 completed (loss: 0.07074763625860214, acc: 0.9772036671638489)
[2025-02-13 03:35:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:50][root][INFO] - Training Epoch: 2/2, step 205/7134 completed (loss: 0.06397389620542526, acc: 0.9754204154014587)
[2025-02-13 03:35:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:51][root][INFO] - Training Epoch: 2/2, step 206/7134 completed (loss: 0.06290654093027115, acc: 0.9855538010597229)
[2025-02-13 03:35:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:51][root][INFO] - Training Epoch: 2/2, step 207/7134 completed (loss: 0.06486303359270096, acc: 0.9829351305961609)
[2025-02-13 03:35:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:52][root][INFO] - Training Epoch: 2/2, step 208/7134 completed (loss: 0.10379859805107117, acc: 0.9745097756385803)
[2025-02-13 03:35:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:52][root][INFO] - Training Epoch: 2/2, step 209/7134 completed (loss: 0.10290272533893585, acc: 0.9740932583808899)
[2025-02-13 03:35:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:53][root][INFO] - Training Epoch: 2/2, step 210/7134 completed (loss: 0.0798196941614151, acc: 0.9783913493156433)
[2025-02-13 03:35:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:53][root][INFO] - Training Epoch: 2/2, step 211/7134 completed (loss: 0.10974302887916565, acc: 0.9754902124404907)
[2025-02-13 03:35:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:53][root][INFO] - Training Epoch: 2/2, step 212/7134 completed (loss: 0.16149337589740753, acc: 0.9731404781341553)
[2025-02-13 03:35:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:54][root][INFO] - Training Epoch: 2/2, step 213/7134 completed (loss: 0.06590887904167175, acc: 0.9773463010787964)
[2025-02-13 03:35:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:54][root][INFO] - Training Epoch: 2/2, step 214/7134 completed (loss: 0.07010670751333237, acc: 0.9829059839248657)
[2025-02-13 03:35:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:55][root][INFO] - Training Epoch: 2/2, step 215/7134 completed (loss: 0.049880485981702805, acc: 0.9861111044883728)
[2025-02-13 03:35:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:55][root][INFO] - Training Epoch: 2/2, step 216/7134 completed (loss: 0.028409814462065697, acc: 0.9930555820465088)
[2025-02-13 03:35:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:56][root][INFO] - Training Epoch: 2/2, step 217/7134 completed (loss: 0.04275435954332352, acc: 0.9903030395507812)
[2025-02-13 03:35:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:56][root][INFO] - Training Epoch: 2/2, step 218/7134 completed (loss: 0.028771502897143364, acc: 0.9961832165718079)
[2025-02-13 03:35:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:56][root][INFO] - Training Epoch: 2/2, step 219/7134 completed (loss: 0.050563860684633255, acc: 0.9902533888816833)
[2025-02-13 03:35:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:57][root][INFO] - Training Epoch: 2/2, step 220/7134 completed (loss: 0.04220478609204292, acc: 0.9895397424697876)
[2025-02-13 03:35:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:57][root][INFO] - Training Epoch: 2/2, step 221/7134 completed (loss: 0.032247379422187805, acc: 0.9865671396255493)
[2025-02-13 03:35:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:58][root][INFO] - Training Epoch: 2/2, step 222/7134 completed (loss: 0.051468733698129654, acc: 0.9882199168205261)
[2025-02-13 03:35:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:58][root][INFO] - Training Epoch: 2/2, step 223/7134 completed (loss: 0.06538905948400497, acc: 0.9820282459259033)
[2025-02-13 03:35:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:59][root][INFO] - Training Epoch: 2/2, step 224/7134 completed (loss: 0.08688424527645111, acc: 0.9766536951065063)
[2025-02-13 03:35:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:35:59][root][INFO] - Training Epoch: 2/2, step 225/7134 completed (loss: 0.08635640889406204, acc: 0.973042368888855)
[2025-02-13 03:35:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:00][root][INFO] - Training Epoch: 2/2, step 226/7134 completed (loss: 0.04827725142240524, acc: 0.9889975786209106)
[2025-02-13 03:36:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:00][root][INFO] - Training Epoch: 2/2, step 227/7134 completed (loss: 0.10169906169176102, acc: 0.977667510509491)
[2025-02-13 03:36:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:01][root][INFO] - Training Epoch: 2/2, step 228/7134 completed (loss: 0.057538729161024094, acc: 0.986975371837616)
[2025-02-13 03:36:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:01][root][INFO] - Training Epoch: 2/2, step 229/7134 completed (loss: 0.08167227357625961, acc: 0.9810126423835754)
[2025-02-13 03:36:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:01][root][INFO] - Training Epoch: 2/2, step 230/7134 completed (loss: 0.04341864958405495, acc: 0.9901719689369202)
[2025-02-13 03:36:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:02][root][INFO] - Training Epoch: 2/2, step 231/7134 completed (loss: 0.06349929422140121, acc: 0.9849340915679932)
[2025-02-13 03:36:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:02][root][INFO] - Training Epoch: 2/2, step 232/7134 completed (loss: 0.047861989587545395, acc: 0.9785832166671753)
[2025-02-13 03:36:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:03][root][INFO] - Training Epoch: 2/2, step 233/7134 completed (loss: 0.059400901198387146, acc: 0.9825673699378967)
[2025-02-13 03:36:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:03][root][INFO] - Training Epoch: 2/2, step 234/7134 completed (loss: 0.05124061927199364, acc: 0.9831081032752991)
[2025-02-13 03:36:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:04][root][INFO] - Training Epoch: 2/2, step 235/7134 completed (loss: 0.08982770144939423, acc: 0.9814814925193787)
[2025-02-13 03:36:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:04][root][INFO] - Training Epoch: 2/2, step 236/7134 completed (loss: 0.02384082041680813, acc: 0.995312511920929)
[2025-02-13 03:36:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:05][root][INFO] - Training Epoch: 2/2, step 237/7134 completed (loss: 0.052649229764938354, acc: 0.9865871667861938)
[2025-02-13 03:36:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:05][root][INFO] - Training Epoch: 2/2, step 238/7134 completed (loss: 0.07946949452161789, acc: 0.986940324306488)
[2025-02-13 03:36:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:05][root][INFO] - Training Epoch: 2/2, step 239/7134 completed (loss: 0.019428787752985954, acc: 0.993697464466095)
[2025-02-13 03:36:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:06][root][INFO] - Training Epoch: 2/2, step 240/7134 completed (loss: 0.056088197976350784, acc: 0.9851694703102112)
[2025-02-13 03:36:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:06][root][INFO] - Training Epoch: 2/2, step 241/7134 completed (loss: 0.09382939338684082, acc: 0.9822866320610046)
[2025-02-13 03:36:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:07][root][INFO] - Training Epoch: 2/2, step 242/7134 completed (loss: 0.102030910551548, acc: 0.9636048674583435)
[2025-02-13 03:36:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:07][root][INFO] - Training Epoch: 2/2, step 243/7134 completed (loss: 0.10279030352830887, acc: 0.9710884094238281)
[2025-02-13 03:36:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:08][root][INFO] - Training Epoch: 2/2, step 244/7134 completed (loss: 0.015804603695869446, acc: 0.9924585223197937)
[2025-02-13 03:36:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:08][root][INFO] - Training Epoch: 2/2, step 245/7134 completed (loss: 0.0938580185174942, acc: 0.9824868440628052)
[2025-02-13 03:36:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:08][root][INFO] - Training Epoch: 2/2, step 246/7134 completed (loss: 0.046507470309734344, acc: 0.988959014415741)
[2025-02-13 03:36:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:09][root][INFO] - Training Epoch: 2/2, step 247/7134 completed (loss: 0.06915682554244995, acc: 0.9836660623550415)
[2025-02-13 03:36:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:09][root][INFO] - Training Epoch: 2/2, step 248/7134 completed (loss: 0.04094764217734337, acc: 0.9883720874786377)
[2025-02-13 03:36:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:10][root][INFO] - Training Epoch: 2/2, step 249/7134 completed (loss: 0.05126560479402542, acc: 0.9812332391738892)
[2025-02-13 03:36:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:10][root][INFO] - Training Epoch: 2/2, step 250/7134 completed (loss: 0.060974810272455215, acc: 0.9810126423835754)
[2025-02-13 03:36:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:10][root][INFO] - Training Epoch: 2/2, step 251/7134 completed (loss: 0.034697581082582474, acc: 0.9846153855323792)
[2025-02-13 03:36:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:11][root][INFO] - Training Epoch: 2/2, step 252/7134 completed (loss: 0.022465545684099197, acc: 0.9934425950050354)
[2025-02-13 03:36:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:11][root][INFO] - Training Epoch: 2/2, step 253/7134 completed (loss: 0.024413524195551872, acc: 0.9946523904800415)
[2025-02-13 03:36:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:12][root][INFO] - Training Epoch: 2/2, step 254/7134 completed (loss: 0.026634706184267998, acc: 0.9928571581840515)
[2025-02-13 03:36:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:12][root][INFO] - Training Epoch: 2/2, step 255/7134 completed (loss: 0.04177098348736763, acc: 0.9853801131248474)
[2025-02-13 03:36:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:13][root][INFO] - Training Epoch: 2/2, step 256/7134 completed (loss: 0.03940311819314957, acc: 0.9852941036224365)
[2025-02-13 03:36:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:13][root][INFO] - Training Epoch: 2/2, step 257/7134 completed (loss: 0.045967962592840195, acc: 0.9846625924110413)
[2025-02-13 03:36:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:13][root][INFO] - Training Epoch: 2/2, step 258/7134 completed (loss: 0.04371918737888336, acc: 0.9867768883705139)
[2025-02-13 03:36:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:14][root][INFO] - Training Epoch: 2/2, step 259/7134 completed (loss: 0.039917316287755966, acc: 0.9854133129119873)
[2025-02-13 03:36:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:14][root][INFO] - Training Epoch: 2/2, step 260/7134 completed (loss: 0.03108581341803074, acc: 0.9881556630134583)
[2025-02-13 03:36:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:15][root][INFO] - Training Epoch: 2/2, step 261/7134 completed (loss: 0.037835415452718735, acc: 0.9938650131225586)
[2025-02-13 03:36:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:15][root][INFO] - Training Epoch: 2/2, step 262/7134 completed (loss: 0.01804043911397457, acc: 0.9950082898139954)
[2025-02-13 03:36:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:16][root][INFO] - Training Epoch: 2/2, step 263/7134 completed (loss: 0.05035035312175751, acc: 0.9872286319732666)
[2025-02-13 03:36:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:16][root][INFO] - Training Epoch: 2/2, step 264/7134 completed (loss: 0.03281698003411293, acc: 0.9894859790802002)
[2025-02-13 03:36:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:17][root][INFO] - Training Epoch: 2/2, step 265/7134 completed (loss: 0.055702704936265945, acc: 0.9858247637748718)
[2025-02-13 03:36:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:17][root][INFO] - Training Epoch: 2/2, step 266/7134 completed (loss: 0.03684241697192192, acc: 0.9885321259498596)
[2025-02-13 03:36:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:18][root][INFO] - Training Epoch: 2/2, step 267/7134 completed (loss: 0.025325031951069832, acc: 0.9908046126365662)
[2025-02-13 03:36:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:18][root][INFO] - Training Epoch: 2/2, step 268/7134 completed (loss: 0.050790030509233475, acc: 0.9870298504829407)
[2025-02-13 03:36:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:19][root][INFO] - Training Epoch: 2/2, step 269/7134 completed (loss: 0.051524702459573746, acc: 0.9833101630210876)
[2025-02-13 03:36:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:19][root][INFO] - Training Epoch: 2/2, step 270/7134 completed (loss: 0.03452341631054878, acc: 0.9910447597503662)
[2025-02-13 03:36:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:20][root][INFO] - Training Epoch: 2/2, step 271/7134 completed (loss: 0.04261335730552673, acc: 0.9873896837234497)
[2025-02-13 03:36:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:20][root][INFO] - Training Epoch: 2/2, step 272/7134 completed (loss: 0.031347062438726425, acc: 0.9872773289680481)
[2025-02-13 03:36:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:20][root][INFO] - Training Epoch: 2/2, step 273/7134 completed (loss: 0.01690957508981228, acc: 0.9940119981765747)
[2025-02-13 03:36:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:21][root][INFO] - Training Epoch: 2/2, step 274/7134 completed (loss: 0.018936488777399063, acc: 0.993122398853302)
[2025-02-13 03:36:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:21][root][INFO] - Training Epoch: 2/2, step 275/7134 completed (loss: 0.022779518738389015, acc: 0.9930394291877747)
[2025-02-13 03:36:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:22][root][INFO] - Training Epoch: 2/2, step 276/7134 completed (loss: 0.03543863445520401, acc: 0.9878197312355042)
[2025-02-13 03:36:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:22][root][INFO] - Training Epoch: 2/2, step 277/7134 completed (loss: 0.03757579252123833, acc: 0.9897040128707886)
[2025-02-13 03:36:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:23][root][INFO] - Training Epoch: 2/2, step 278/7134 completed (loss: 0.021846195682883263, acc: 0.9897828698158264)
[2025-02-13 03:36:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:23][root][INFO] - Training Epoch: 2/2, step 279/7134 completed (loss: 0.026408830657601357, acc: 0.9921259880065918)
[2025-02-13 03:36:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:24][root][INFO] - Training Epoch: 2/2, step 280/7134 completed (loss: 0.04126504436135292, acc: 0.9838709831237793)
[2025-02-13 03:36:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:24][root][INFO] - Training Epoch: 2/2, step 281/7134 completed (loss: 0.05282535403966904, acc: 0.9819999933242798)
[2025-02-13 03:36:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:25][root][INFO] - Training Epoch: 2/2, step 282/7134 completed (loss: 0.06708580255508423, acc: 0.9786259531974792)
[2025-02-13 03:36:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:25][root][INFO] - Training Epoch: 2/2, step 283/7134 completed (loss: 0.03638259321451187, acc: 0.9864176511764526)
[2025-02-13 03:36:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:26][root][INFO] - Training Epoch: 2/2, step 284/7134 completed (loss: 0.020421748980879784, acc: 0.9899328947067261)
[2025-02-13 03:36:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:26][root][INFO] - Training Epoch: 2/2, step 285/7134 completed (loss: 0.03481689468026161, acc: 0.9876543283462524)
[2025-02-13 03:36:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:27][root][INFO] - Training Epoch: 2/2, step 286/7134 completed (loss: 0.052023712545633316, acc: 0.9858406782150269)
[2025-02-13 03:36:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:27][root][INFO] - Training Epoch: 2/2, step 287/7134 completed (loss: 0.028236573562026024, acc: 0.9935794472694397)
[2025-02-13 03:36:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:27][root][INFO] - Training Epoch: 2/2, step 288/7134 completed (loss: 0.044261012226343155, acc: 0.9871086478233337)
[2025-02-13 03:36:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:28][root][INFO] - Training Epoch: 2/2, step 289/7134 completed (loss: 0.027423368766903877, acc: 0.9855334758758545)
[2025-02-13 03:36:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:28][root][INFO] - Training Epoch: 2/2, step 290/7134 completed (loss: 0.022854331880807877, acc: 0.9924012422561646)
[2025-02-13 03:36:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:29][root][INFO] - Training Epoch: 2/2, step 291/7134 completed (loss: 0.06452800333499908, acc: 0.9851973652839661)
[2025-02-13 03:36:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:29][root][INFO] - Training Epoch: 2/2, step 292/7134 completed (loss: 0.059978216886520386, acc: 0.9815950989723206)
[2025-02-13 03:36:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:30][root][INFO] - Training Epoch: 2/2, step 293/7134 completed (loss: 0.04351317882537842, acc: 0.9839357137680054)
[2025-02-13 03:36:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:30][root][INFO] - Training Epoch: 2/2, step 294/7134 completed (loss: 0.03858112916350365, acc: 0.9844054579734802)
[2025-02-13 03:36:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:31][root][INFO] - Training Epoch: 2/2, step 295/7134 completed (loss: 0.06249302625656128, acc: 0.9829351305961609)
[2025-02-13 03:36:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:31][root][INFO] - Training Epoch: 2/2, step 296/7134 completed (loss: 0.04021546244621277, acc: 0.9937984347343445)
[2025-02-13 03:36:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:32][root][INFO] - Training Epoch: 2/2, step 297/7134 completed (loss: 0.03937000781297684, acc: 0.9928951859474182)
[2025-02-13 03:36:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:32][root][INFO] - Training Epoch: 2/2, step 298/7134 completed (loss: 0.025083787739276886, acc: 0.9904000163078308)
[2025-02-13 03:36:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:33][root][INFO] - Training Epoch: 2/2, step 299/7134 completed (loss: 0.03345309942960739, acc: 0.9849624037742615)
[2025-02-13 03:36:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:33][root][INFO] - Training Epoch: 2/2, step 300/7134 completed (loss: 0.021231673657894135, acc: 0.991909384727478)
[2025-02-13 03:36:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:33][root][INFO] - Training Epoch: 2/2, step 301/7134 completed (loss: 0.026295369490981102, acc: 0.9929278492927551)
[2025-02-13 03:36:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:34][root][INFO] - Training Epoch: 2/2, step 302/7134 completed (loss: 0.04863765835762024, acc: 0.9863547682762146)
[2025-02-13 03:36:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:34][root][INFO] - Training Epoch: 2/2, step 303/7134 completed (loss: 0.034453488886356354, acc: 0.9902912378311157)
[2025-02-13 03:36:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:35][root][INFO] - Training Epoch: 2/2, step 304/7134 completed (loss: 0.014819478616118431, acc: 0.9981481432914734)
[2025-02-13 03:36:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:35][root][INFO] - Training Epoch: 2/2, step 305/7134 completed (loss: 0.026318952441215515, acc: 0.995230495929718)
[2025-02-13 03:36:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:36][root][INFO] - Training Epoch: 2/2, step 306/7134 completed (loss: 0.0053400942124426365, acc: 1.0)
[2025-02-13 03:36:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:36][root][INFO] - Training Epoch: 2/2, step 307/7134 completed (loss: 0.04952403903007507, acc: 0.982206404209137)
[2025-02-13 03:36:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:37][root][INFO] - Training Epoch: 2/2, step 308/7134 completed (loss: 0.014841794967651367, acc: 0.9950658082962036)
[2025-02-13 03:36:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:37][root][INFO] - Training Epoch: 2/2, step 309/7134 completed (loss: 0.028998779132962227, acc: 0.9862068891525269)
[2025-02-13 03:36:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:38][root][INFO] - Training Epoch: 2/2, step 310/7134 completed (loss: 0.016079315915703773, acc: 0.9985380172729492)
[2025-02-13 03:36:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:38][root][INFO] - Training Epoch: 2/2, step 311/7134 completed (loss: 0.024037033319473267, acc: 0.993779182434082)
[2025-02-13 03:36:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:38][root][INFO] - Training Epoch: 2/2, step 312/7134 completed (loss: 0.01639951951801777, acc: 0.9965338110923767)
[2025-02-13 03:36:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:39][root][INFO] - Training Epoch: 2/2, step 313/7134 completed (loss: 0.021546579897403717, acc: 0.9947826266288757)
[2025-02-13 03:36:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:39][root][INFO] - Training Epoch: 2/2, step 314/7134 completed (loss: 0.026838436722755432, acc: 0.9921875)
[2025-02-13 03:36:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:40][root][INFO] - Training Epoch: 2/2, step 315/7134 completed (loss: 0.03001134842634201, acc: 0.9901639223098755)
[2025-02-13 03:36:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:40][root][INFO] - Training Epoch: 2/2, step 316/7134 completed (loss: 0.034920308738946915, acc: 0.9891696572303772)
[2025-02-13 03:36:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:41][root][INFO] - Training Epoch: 2/2, step 317/7134 completed (loss: 0.003920842427760363, acc: 1.0)
[2025-02-13 03:36:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:41][root][INFO] - Training Epoch: 2/2, step 318/7134 completed (loss: 0.061763960868120193, acc: 0.9816849827766418)
[2025-02-13 03:36:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:42][root][INFO] - Training Epoch: 2/2, step 319/7134 completed (loss: 0.05734401196241379, acc: 0.9873772859573364)
[2025-02-13 03:36:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:42][root][INFO] - Training Epoch: 2/2, step 320/7134 completed (loss: 0.02975638024508953, acc: 0.9914407730102539)
[2025-02-13 03:36:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:43][root][INFO] - Training Epoch: 2/2, step 321/7134 completed (loss: 0.06907355040311813, acc: 0.9782886505126953)
[2025-02-13 03:36:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:43][root][INFO] - Training Epoch: 2/2, step 322/7134 completed (loss: 0.0357537604868412, acc: 0.9864864945411682)
[2025-02-13 03:36:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:44][root][INFO] - Training Epoch: 2/2, step 323/7134 completed (loss: 0.04408559575676918, acc: 0.9884467124938965)
[2025-02-13 03:36:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:44][root][INFO] - Training Epoch: 2/2, step 324/7134 completed (loss: 0.05172823369503021, acc: 0.9861910343170166)
[2025-02-13 03:36:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:45][root][INFO] - Training Epoch: 2/2, step 325/7134 completed (loss: 0.023626474663615227, acc: 0.9922879338264465)
[2025-02-13 03:36:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:45][root][INFO] - Training Epoch: 2/2, step 326/7134 completed (loss: 0.021291017532348633, acc: 0.9939098954200745)
[2025-02-13 03:36:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:45][root][INFO] - Training Epoch: 2/2, step 327/7134 completed (loss: 0.046275608241558075, acc: 0.9887387156486511)
[2025-02-13 03:36:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:46][root][INFO] - Training Epoch: 2/2, step 328/7134 completed (loss: 0.024206779897212982, acc: 0.9911110997200012)
[2025-02-13 03:36:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:46][root][INFO] - Training Epoch: 2/2, step 329/7134 completed (loss: 0.010846133343875408, acc: 0.9976498484611511)
[2025-02-13 03:36:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:47][root][INFO] - Training Epoch: 2/2, step 330/7134 completed (loss: 0.0295107364654541, acc: 0.9879153966903687)
[2025-02-13 03:36:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:47][root][INFO] - Training Epoch: 2/2, step 331/7134 completed (loss: 0.03614765405654907, acc: 0.9920182228088379)
[2025-02-13 03:36:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:48][root][INFO] - Training Epoch: 2/2, step 332/7134 completed (loss: 0.011602768674492836, acc: 0.9976387023925781)
[2025-02-13 03:36:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:48][root][INFO] - Training Epoch: 2/2, step 333/7134 completed (loss: 0.03144877403974533, acc: 0.989051103591919)
[2025-02-13 03:36:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:49][root][INFO] - Training Epoch: 2/2, step 334/7134 completed (loss: 0.017080793157219887, acc: 0.9949302673339844)
[2025-02-13 03:36:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:49][root][INFO] - Training Epoch: 2/2, step 335/7134 completed (loss: 0.012420538812875748, acc: 0.9970887899398804)
[2025-02-13 03:36:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:50][root][INFO] - Training Epoch: 2/2, step 336/7134 completed (loss: 0.03782009705901146, acc: 0.9880239367485046)
[2025-02-13 03:36:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:50][root][INFO] - Training Epoch: 2/2, step 337/7134 completed (loss: 0.013549364171922207, acc: 0.9969325065612793)
[2025-02-13 03:36:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:51][root][INFO] - Training Epoch: 2/2, step 338/7134 completed (loss: 0.055106502026319504, acc: 0.988664984703064)
[2025-02-13 03:36:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:51][root][INFO] - Training Epoch: 2/2, step 339/7134 completed (loss: 0.025832712650299072, acc: 0.9925000071525574)
[2025-02-13 03:36:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:52][root][INFO] - Training Epoch: 2/2, step 340/7134 completed (loss: 0.035685960203409195, acc: 0.9923664331436157)
[2025-02-13 03:36:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:52][root][INFO] - Training Epoch: 2/2, step 341/7134 completed (loss: 0.014119260013103485, acc: 0.996216893196106)
[2025-02-13 03:36:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:53][root][INFO] - Training Epoch: 2/2, step 342/7134 completed (loss: 0.02142290212213993, acc: 0.9924242496490479)
[2025-02-13 03:36:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:53][root][INFO] - Training Epoch: 2/2, step 343/7134 completed (loss: 0.032913368195295334, acc: 0.992438554763794)
[2025-02-13 03:36:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:53][root][INFO] - Training Epoch: 2/2, step 344/7134 completed (loss: 0.012882161885499954, acc: 0.9963503479957581)
[2025-02-13 03:36:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:54][root][INFO] - Training Epoch: 2/2, step 345/7134 completed (loss: 0.037642888724803925, acc: 0.9946428537368774)
[2025-02-13 03:36:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:54][root][INFO] - Training Epoch: 2/2, step 346/7134 completed (loss: 0.024258306249976158, acc: 0.995230495929718)
[2025-02-13 03:36:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:55][root][INFO] - Training Epoch: 2/2, step 347/7134 completed (loss: 0.02253095619380474, acc: 0.9929873943328857)
[2025-02-13 03:36:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:55][root][INFO] - Training Epoch: 2/2, step 348/7134 completed (loss: 0.009181744419038296, acc: 0.998643159866333)
[2025-02-13 03:36:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:56][root][INFO] - Training Epoch: 2/2, step 349/7134 completed (loss: 0.021313993260264397, acc: 0.990604043006897)
[2025-02-13 03:36:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:56][root][INFO] - Training Epoch: 2/2, step 350/7134 completed (loss: 0.046064481139183044, acc: 0.9867629408836365)
[2025-02-13 03:36:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:57][root][INFO] - Training Epoch: 2/2, step 351/7134 completed (loss: 0.024597445502877235, acc: 0.9923664331436157)
[2025-02-13 03:36:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:57][root][INFO] - Training Epoch: 2/2, step 352/7134 completed (loss: 0.0160802211612463, acc: 0.9959239363670349)
[2025-02-13 03:36:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:58][root][INFO] - Training Epoch: 2/2, step 353/7134 completed (loss: 0.020440230146050453, acc: 0.9920212626457214)
[2025-02-13 03:36:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:58][root][INFO] - Training Epoch: 2/2, step 354/7134 completed (loss: 0.012325240299105644, acc: 0.9965075850486755)
[2025-02-13 03:36:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:59][root][INFO] - Training Epoch: 2/2, step 355/7134 completed (loss: 0.04807284101843834, acc: 0.9888268113136292)
[2025-02-13 03:36:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:36:59][root][INFO] - Training Epoch: 2/2, step 356/7134 completed (loss: 0.01887998916208744, acc: 0.9920634627342224)
[2025-02-13 03:36:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:00][root][INFO] - Training Epoch: 2/2, step 357/7134 completed (loss: 0.02761674113571644, acc: 0.9923664331436157)
[2025-02-13 03:37:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:00][root][INFO] - Training Epoch: 2/2, step 358/7134 completed (loss: 0.02547212317585945, acc: 0.9931787252426147)
[2025-02-13 03:37:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:01][root][INFO] - Training Epoch: 2/2, step 359/7134 completed (loss: 0.015806250274181366, acc: 0.9932432174682617)
[2025-02-13 03:37:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:01][root][INFO] - Training Epoch: 2/2, step 360/7134 completed (loss: 0.01738305389881134, acc: 0.9939758777618408)
[2025-02-13 03:37:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:01][root][INFO] - Training Epoch: 2/2, step 361/7134 completed (loss: 0.034046996384859085, acc: 0.9926578402519226)
[2025-02-13 03:37:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:02][root][INFO] - Training Epoch: 2/2, step 362/7134 completed (loss: 0.03178158774971962, acc: 0.9940119981765747)
[2025-02-13 03:37:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:02][root][INFO] - Training Epoch: 2/2, step 363/7134 completed (loss: 0.040384143590927124, acc: 0.98828125)
[2025-02-13 03:37:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:03][root][INFO] - Training Epoch: 2/2, step 364/7134 completed (loss: 0.02552460879087448, acc: 0.9906542301177979)
[2025-02-13 03:37:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:03][root][INFO] - Training Epoch: 2/2, step 365/7134 completed (loss: 0.01911955513060093, acc: 0.994490385055542)
[2025-02-13 03:37:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:04][root][INFO] - Training Epoch: 2/2, step 366/7134 completed (loss: 0.0278621818870306, acc: 0.9874607920646667)
[2025-02-13 03:37:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:04][root][INFO] - Training Epoch: 2/2, step 367/7134 completed (loss: 0.019376961514353752, acc: 0.9925000071525574)
[2025-02-13 03:37:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:05][root][INFO] - Training Epoch: 2/2, step 368/7134 completed (loss: 0.011198921129107475, acc: 0.9940564632415771)
[2025-02-13 03:37:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:05][root][INFO] - Training Epoch: 2/2, step 369/7134 completed (loss: 0.02002045512199402, acc: 0.9938744306564331)
[2025-02-13 03:37:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:05][root][INFO] - Training Epoch: 2/2, step 370/7134 completed (loss: 0.03714223951101303, acc: 0.9898107647895813)
[2025-02-13 03:37:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:06][root][INFO] - Training Epoch: 2/2, step 371/7134 completed (loss: 0.029223445802927017, acc: 0.9924812316894531)
[2025-02-13 03:37:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:06][root][INFO] - Training Epoch: 2/2, step 372/7134 completed (loss: 0.027387145906686783, acc: 0.98959881067276)
[2025-02-13 03:37:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:07][root][INFO] - Training Epoch: 2/2, step 373/7134 completed (loss: 0.011341431178152561, acc: 0.9970104694366455)
[2025-02-13 03:37:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:07][root][INFO] - Training Epoch: 2/2, step 374/7134 completed (loss: 0.020214015617966652, acc: 0.9919614195823669)
[2025-02-13 03:37:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:08][root][INFO] - Training Epoch: 2/2, step 375/7134 completed (loss: 0.03340655565261841, acc: 0.9906291961669922)
[2025-02-13 03:37:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:08][root][INFO] - Training Epoch: 2/2, step 376/7134 completed (loss: 0.03449507802724838, acc: 0.9862068891525269)
[2025-02-13 03:37:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:09][root][INFO] - Training Epoch: 2/2, step 377/7134 completed (loss: 0.028200440108776093, acc: 0.9903714060783386)
[2025-02-13 03:37:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:09][root][INFO] - Training Epoch: 2/2, step 378/7134 completed (loss: 0.017960943281650543, acc: 0.9931880235671997)
[2025-02-13 03:37:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:10][root][INFO] - Training Epoch: 2/2, step 379/7134 completed (loss: 0.01414496824145317, acc: 0.9937185645103455)
[2025-02-13 03:37:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:10][root][INFO] - Training Epoch: 2/2, step 380/7134 completed (loss: 0.05651050806045532, acc: 0.9763469099998474)
[2025-02-13 03:37:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:10][root][INFO] - Training Epoch: 2/2, step 381/7134 completed (loss: 0.03636718541383743, acc: 0.9948320388793945)
[2025-02-13 03:37:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:11][root][INFO] - Training Epoch: 2/2, step 382/7134 completed (loss: 0.0255476962774992, acc: 0.9910141229629517)
[2025-02-13 03:37:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:11][root][INFO] - Training Epoch: 2/2, step 383/7134 completed (loss: 0.023686444386839867, acc: 0.992438554763794)
[2025-02-13 03:37:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:12][root][INFO] - Training Epoch: 2/2, step 384/7134 completed (loss: 0.023831909522414207, acc: 0.9946595430374146)
[2025-02-13 03:37:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:12][root][INFO] - Training Epoch: 2/2, step 385/7134 completed (loss: 0.01703057996928692, acc: 0.9968602657318115)
[2025-02-13 03:37:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:13][root][INFO] - Training Epoch: 2/2, step 386/7134 completed (loss: 0.012313026003539562, acc: 0.9959100484848022)
[2025-02-13 03:37:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:13][root][INFO] - Training Epoch: 2/2, step 387/7134 completed (loss: 0.030859449878335, acc: 0.9927113652229309)
[2025-02-13 03:37:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:14][root][INFO] - Training Epoch: 2/2, step 388/7134 completed (loss: 0.010586737655103207, acc: 0.998826265335083)
[2025-02-13 03:37:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:14][root][INFO] - Training Epoch: 2/2, step 389/7134 completed (loss: 0.04208097979426384, acc: 0.9876033067703247)
[2025-02-13 03:37:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:14][root][INFO] - Training Epoch: 2/2, step 390/7134 completed (loss: 0.027362314984202385, acc: 0.9967637658119202)
[2025-02-13 03:37:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:15][root][INFO] - Training Epoch: 2/2, step 391/7134 completed (loss: 0.030136367306113243, acc: 0.9897959232330322)
[2025-02-13 03:37:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:15][root][INFO] - Training Epoch: 2/2, step 392/7134 completed (loss: 0.03090668097138405, acc: 0.9911110997200012)
[2025-02-13 03:37:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:16][root][INFO] - Training Epoch: 2/2, step 393/7134 completed (loss: 0.02623959630727768, acc: 0.987500011920929)
[2025-02-13 03:37:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:16][root][INFO] - Training Epoch: 2/2, step 394/7134 completed (loss: 0.03231849521398544, acc: 0.9883268475532532)
[2025-02-13 03:37:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:17][root][INFO] - Training Epoch: 2/2, step 395/7134 completed (loss: 0.058903079479932785, acc: 0.9876543283462524)
[2025-02-13 03:37:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:17][root][INFO] - Training Epoch: 2/2, step 396/7134 completed (loss: 0.020309152081608772, acc: 0.9913978576660156)
[2025-02-13 03:37:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:18][root][INFO] - Training Epoch: 2/2, step 397/7134 completed (loss: 0.05011555552482605, acc: 0.9867724776268005)
[2025-02-13 03:37:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:18][root][INFO] - Training Epoch: 2/2, step 398/7134 completed (loss: 0.019880492240190506, acc: 0.9961340427398682)
[2025-02-13 03:37:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:19][root][INFO] - Training Epoch: 2/2, step 399/7134 completed (loss: 0.030067749321460724, acc: 0.9920760989189148)
[2025-02-13 03:37:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:19][root][INFO] - Training Epoch: 2/2, step 400/7134 completed (loss: 0.08520697802305222, acc: 0.9791044592857361)
[2025-02-13 03:37:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:19][root][INFO] - Training Epoch: 2/2, step 401/7134 completed (loss: 0.02563883736729622, acc: 0.9938725233078003)
[2025-02-13 03:37:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:20][root][INFO] - Training Epoch: 2/2, step 402/7134 completed (loss: 0.02541462704539299, acc: 0.9911280274391174)
[2025-02-13 03:37:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:20][root][INFO] - Training Epoch: 2/2, step 403/7134 completed (loss: 0.024677367880940437, acc: 0.9913606643676758)
[2025-02-13 03:37:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:21][root][INFO] - Training Epoch: 2/2, step 404/7134 completed (loss: 0.026399338617920876, acc: 0.9923076629638672)
[2025-02-13 03:37:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:21][root][INFO] - Training Epoch: 2/2, step 405/7134 completed (loss: 0.08174872398376465, acc: 0.9828660488128662)
[2025-02-13 03:37:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:22][root][INFO] - Training Epoch: 2/2, step 406/7134 completed (loss: 0.0660044476389885, acc: 0.9841269850730896)
[2025-02-13 03:37:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:22][root][INFO] - Training Epoch: 2/2, step 407/7134 completed (loss: 0.11073379218578339, acc: 0.9813559055328369)
[2025-02-13 03:37:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:23][root][INFO] - Training Epoch: 2/2, step 408/7134 completed (loss: 0.06401271373033524, acc: 0.9807956218719482)
[2025-02-13 03:37:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:23][root][INFO] - Training Epoch: 2/2, step 409/7134 completed (loss: 0.03247629478573799, acc: 0.9899371266365051)
[2025-02-13 03:37:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:24][root][INFO] - Training Epoch: 2/2, step 410/7134 completed (loss: 0.05087493732571602, acc: 0.9851632118225098)
[2025-02-13 03:37:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:24][root][INFO] - Training Epoch: 2/2, step 411/7134 completed (loss: 0.03599431738257408, acc: 0.9907940030097961)
[2025-02-13 03:37:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:25][root][INFO] - Training Epoch: 2/2, step 412/7134 completed (loss: 0.018478823825716972, acc: 0.9931350350379944)
[2025-02-13 03:37:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:25][root][INFO] - Training Epoch: 2/2, step 413/7134 completed (loss: 0.02249104529619217, acc: 0.9933244585990906)
[2025-02-13 03:37:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:26][root][INFO] - Training Epoch: 2/2, step 414/7134 completed (loss: 0.030116571113467216, acc: 0.9925816059112549)
[2025-02-13 03:37:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:26][root][INFO] - Training Epoch: 2/2, step 415/7134 completed (loss: 0.032757408916950226, acc: 0.9953703880310059)
[2025-02-13 03:37:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:27][root][INFO] - Training Epoch: 2/2, step 416/7134 completed (loss: 0.023858459666371346, acc: 0.9917355179786682)
[2025-02-13 03:37:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:27][root][INFO] - Training Epoch: 2/2, step 417/7134 completed (loss: 0.0588313490152359, acc: 0.9864130616188049)
[2025-02-13 03:37:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:27][root][INFO] - Training Epoch: 2/2, step 418/7134 completed (loss: 0.025450915098190308, acc: 0.991963267326355)
[2025-02-13 03:37:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:28][root][INFO] - Training Epoch: 2/2, step 419/7134 completed (loss: 0.027681834995746613, acc: 0.9913151264190674)
[2025-02-13 03:37:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:28][root][INFO] - Training Epoch: 2/2, step 420/7134 completed (loss: 0.021568503230810165, acc: 0.9932318329811096)
[2025-02-13 03:37:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:29][root][INFO] - Training Epoch: 2/2, step 421/7134 completed (loss: 0.017475789412856102, acc: 0.9920381903648376)
[2025-02-13 03:37:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:29][root][INFO] - Training Epoch: 2/2, step 422/7134 completed (loss: 0.031334757804870605, acc: 0.9897058606147766)
[2025-02-13 03:37:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:30][root][INFO] - Training Epoch: 2/2, step 423/7134 completed (loss: 0.016155626624822617, acc: 0.9942857027053833)
[2025-02-13 03:37:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:30][root][INFO] - Training Epoch: 2/2, step 424/7134 completed (loss: 0.02554149180650711, acc: 0.9889240264892578)
[2025-02-13 03:37:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:31][root][INFO] - Training Epoch: 2/2, step 425/7134 completed (loss: 0.01339756604284048, acc: 0.9956395626068115)
[2025-02-13 03:37:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:31][root][INFO] - Training Epoch: 2/2, step 426/7134 completed (loss: 0.007777869701385498, acc: 0.9972144961357117)
[2025-02-13 03:37:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:32][root][INFO] - Training Epoch: 2/2, step 427/7134 completed (loss: 0.014009136706590652, acc: 0.9969651103019714)
[2025-02-13 03:37:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:32][root][INFO] - Training Epoch: 2/2, step 428/7134 completed (loss: 0.023411257192492485, acc: 0.990963876247406)
[2025-02-13 03:37:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:32][root][INFO] - Training Epoch: 2/2, step 429/7134 completed (loss: 0.028726940974593163, acc: 0.9888712167739868)
[2025-02-13 03:37:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:33][root][INFO] - Training Epoch: 2/2, step 430/7134 completed (loss: 0.01618274487555027, acc: 0.9956958293914795)
[2025-02-13 03:37:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:33][root][INFO] - Training Epoch: 2/2, step 431/7134 completed (loss: 0.008995942771434784, acc: 0.9985590577125549)
[2025-02-13 03:37:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:34][root][INFO] - Training Epoch: 2/2, step 432/7134 completed (loss: 0.014003324322402477, acc: 0.9945155382156372)
[2025-02-13 03:37:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:34][root][INFO] - Training Epoch: 2/2, step 433/7134 completed (loss: 0.013531648553907871, acc: 0.9959239363670349)
[2025-02-13 03:37:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:35][root][INFO] - Training Epoch: 2/2, step 434/7134 completed (loss: 0.02092902734875679, acc: 0.9912152290344238)
[2025-02-13 03:37:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:35][root][INFO] - Training Epoch: 2/2, step 435/7134 completed (loss: 0.026349637657403946, acc: 0.9914893507957458)
[2025-02-13 03:37:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:36][root][INFO] - Training Epoch: 2/2, step 436/7134 completed (loss: 0.02765357308089733, acc: 0.9901574850082397)
[2025-02-13 03:37:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:36][root][INFO] - Training Epoch: 2/2, step 437/7134 completed (loss: 0.04138880968093872, acc: 0.9929328560829163)
[2025-02-13 03:37:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:36][root][INFO] - Training Epoch: 2/2, step 438/7134 completed (loss: 0.016302933916449547, acc: 0.9942307472229004)
[2025-02-13 03:37:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:37][root][INFO] - Training Epoch: 2/2, step 439/7134 completed (loss: 0.026516716927289963, acc: 0.9921466112136841)
[2025-02-13 03:37:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:37][root][INFO] - Training Epoch: 2/2, step 440/7134 completed (loss: 0.022228211164474487, acc: 0.9930675625801086)
[2025-02-13 03:37:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:38][root][INFO] - Training Epoch: 2/2, step 441/7134 completed (loss: 0.006874625105410814, acc: 0.9970015287399292)
[2025-02-13 03:37:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:38][root][INFO] - Training Epoch: 2/2, step 442/7134 completed (loss: 0.006113443057984114, acc: 1.0)
[2025-02-13 03:37:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:39][root][INFO] - Training Epoch: 2/2, step 443/7134 completed (loss: 0.0020076667424291372, acc: 1.0)
[2025-02-13 03:37:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:39][root][INFO] - Training Epoch: 2/2, step 444/7134 completed (loss: 0.013076873496174812, acc: 0.9950000047683716)
[2025-02-13 03:37:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:39][root][INFO] - Training Epoch: 2/2, step 445/7134 completed (loss: 0.010741193778812885, acc: 0.9970845580101013)
[2025-02-13 03:37:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:40][root][INFO] - Training Epoch: 2/2, step 446/7134 completed (loss: 0.0055892737582325935, acc: 0.9986072182655334)
[2025-02-13 03:37:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:40][root][INFO] - Training Epoch: 2/2, step 447/7134 completed (loss: 0.01105488184839487, acc: 0.9958391189575195)
[2025-02-13 03:37:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:41][root][INFO] - Training Epoch: 2/2, step 448/7134 completed (loss: 0.04305721074342728, acc: 0.9897698163986206)
[2025-02-13 03:37:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:41][root][INFO] - Training Epoch: 2/2, step 449/7134 completed (loss: 0.08009816706180573, acc: 0.9803664684295654)
[2025-02-13 03:37:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:42][root][INFO] - Training Epoch: 2/2, step 450/7134 completed (loss: 0.04038049280643463, acc: 0.9925690293312073)
[2025-02-13 03:37:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:42][root][INFO] - Training Epoch: 2/2, step 451/7134 completed (loss: 0.06189389526844025, acc: 0.9821640849113464)
[2025-02-13 03:37:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:43][root][INFO] - Training Epoch: 2/2, step 452/7134 completed (loss: 0.04346446692943573, acc: 0.9849315285682678)
[2025-02-13 03:37:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:43][root][INFO] - Training Epoch: 2/2, step 453/7134 completed (loss: 0.042793553322553635, acc: 0.988990843296051)
[2025-02-13 03:37:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:44][root][INFO] - Training Epoch: 2/2, step 454/7134 completed (loss: 0.06264766305685043, acc: 0.9817007780075073)
[2025-02-13 03:37:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:44][root][INFO] - Training Epoch: 2/2, step 455/7134 completed (loss: 0.07319176197052002, acc: 0.9843137264251709)
[2025-02-13 03:37:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:45][root][INFO] - Training Epoch: 2/2, step 456/7134 completed (loss: 0.016298439353704453, acc: 0.9929178357124329)
[2025-02-13 03:37:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:45][root][INFO] - Training Epoch: 2/2, step 457/7134 completed (loss: 0.028995245695114136, acc: 0.9916107654571533)
[2025-02-13 03:37:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:45][root][INFO] - Training Epoch: 2/2, step 458/7134 completed (loss: 0.023674769327044487, acc: 0.9920477271080017)
[2025-02-13 03:37:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:46][root][INFO] - Training Epoch: 2/2, step 459/7134 completed (loss: 0.07290729880332947, acc: 0.9807407259941101)
[2025-02-13 03:37:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:46][root][INFO] - Training Epoch: 2/2, step 460/7134 completed (loss: 0.05043189600110054, acc: 0.9858267903327942)
[2025-02-13 03:37:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:47][root][INFO] - Training Epoch: 2/2, step 461/7134 completed (loss: 0.02909296192228794, acc: 0.9887797832489014)
[2025-02-13 03:37:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:47][root][INFO] - Training Epoch: 2/2, step 462/7134 completed (loss: 0.06313511729240417, acc: 0.9824766516685486)
[2025-02-13 03:37:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:48][root][INFO] - Training Epoch: 2/2, step 463/7134 completed (loss: 0.039362359791994095, acc: 0.9874529242515564)
[2025-02-13 03:37:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:48][root][INFO] - Training Epoch: 2/2, step 464/7134 completed (loss: 0.03374200314283371, acc: 0.9900850057601929)
[2025-02-13 03:37:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:49][root][INFO] - Training Epoch: 2/2, step 465/7134 completed (loss: 0.06700890511274338, acc: 0.9843342304229736)
[2025-02-13 03:37:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:49][root][INFO] - Training Epoch: 2/2, step 466/7134 completed (loss: 0.0828385129570961, acc: 0.9729729890823364)
[2025-02-13 03:37:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:50][root][INFO] - Training Epoch: 2/2, step 467/7134 completed (loss: 0.09566134959459305, acc: 0.9739524126052856)
[2025-02-13 03:37:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:50][root][INFO] - Training Epoch: 2/2, step 468/7134 completed (loss: 0.156873881816864, acc: 0.9683631658554077)
[2025-02-13 03:37:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:50][root][INFO] - Training Epoch: 2/2, step 469/7134 completed (loss: 0.07804195582866669, acc: 0.9769452214241028)
[2025-02-13 03:37:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:51][root][INFO] - Training Epoch: 2/2, step 470/7134 completed (loss: 0.06204334273934364, acc: 0.9850339889526367)
[2025-02-13 03:37:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:51][root][INFO] - Training Epoch: 2/2, step 471/7134 completed (loss: 0.06055254489183426, acc: 0.9822485446929932)
[2025-02-13 03:37:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:52][root][INFO] - Training Epoch: 2/2, step 472/7134 completed (loss: 0.08290927112102509, acc: 0.9780058860778809)
[2025-02-13 03:37:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:52][root][INFO] - Training Epoch: 2/2, step 473/7134 completed (loss: 0.034189049154520035, acc: 0.9830247163772583)
[2025-02-13 03:37:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:53][root][INFO] - Training Epoch: 2/2, step 474/7134 completed (loss: 0.06597918272018433, acc: 0.9841897487640381)
[2025-02-13 03:37:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:53][root][INFO] - Training Epoch: 2/2, step 475/7134 completed (loss: 0.09433378279209137, acc: 0.9779411554336548)
[2025-02-13 03:37:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:53][root][INFO] - Training Epoch: 2/2, step 476/7134 completed (loss: 0.07128439098596573, acc: 0.9813753366470337)
[2025-02-13 03:37:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:54][root][INFO] - Training Epoch: 2/2, step 477/7134 completed (loss: 0.05394182354211807, acc: 0.9816625714302063)
[2025-02-13 03:37:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:54][root][INFO] - Training Epoch: 2/2, step 478/7134 completed (loss: 0.1155463382601738, acc: 0.9704017043113708)
[2025-02-13 03:37:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:55][root][INFO] - Training Epoch: 2/2, step 479/7134 completed (loss: 0.026657648384571075, acc: 0.9928264021873474)
[2025-02-13 03:37:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:55][root][INFO] - Training Epoch: 2/2, step 480/7134 completed (loss: 0.02111300639808178, acc: 0.9956204295158386)
[2025-02-13 03:37:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:56][root][INFO] - Training Epoch: 2/2, step 481/7134 completed (loss: 0.03535567596554756, acc: 0.9908854365348816)
[2025-02-13 03:37:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:56][root][INFO] - Training Epoch: 2/2, step 482/7134 completed (loss: 0.014562292024493217, acc: 0.9971428513526917)
[2025-02-13 03:37:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:57][root][INFO] - Training Epoch: 2/2, step 483/7134 completed (loss: 0.011780467815697193, acc: 0.9975429773330688)
[2025-02-13 03:37:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:57][root][INFO] - Training Epoch: 2/2, step 484/7134 completed (loss: 0.04477499797940254, acc: 0.9866310358047485)
[2025-02-13 03:37:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:57][root][INFO] - Training Epoch: 2/2, step 485/7134 completed (loss: 0.07592372596263885, acc: 0.9818456768989563)
[2025-02-13 03:37:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:58][root][INFO] - Training Epoch: 2/2, step 486/7134 completed (loss: 0.040926337242126465, acc: 0.9884868264198303)
[2025-02-13 03:37:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:58][root][INFO] - Training Epoch: 2/2, step 487/7134 completed (loss: 0.035154957324266434, acc: 0.9897210001945496)
[2025-02-13 03:37:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:59][root][INFO] - Training Epoch: 2/2, step 488/7134 completed (loss: 0.08478040993213654, acc: 0.9800724387168884)
[2025-02-13 03:37:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:37:59][root][INFO] - Training Epoch: 2/2, step 489/7134 completed (loss: 0.053507424890995026, acc: 0.9848739504814148)
[2025-02-13 03:37:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:00][root][INFO] - Training Epoch: 2/2, step 490/7134 completed (loss: 0.04796343296766281, acc: 0.9904761910438538)
[2025-02-13 03:38:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:00][root][INFO] - Training Epoch: 2/2, step 491/7134 completed (loss: 0.03092808462679386, acc: 0.9917762875556946)
[2025-02-13 03:38:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:00][root][INFO] - Training Epoch: 2/2, step 492/7134 completed (loss: 0.01025699358433485, acc: 0.9969742894172668)
[2025-02-13 03:38:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:01][root][INFO] - Training Epoch: 2/2, step 493/7134 completed (loss: 0.040639594197273254, acc: 0.9856733679771423)
[2025-02-13 03:38:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:01][root][INFO] - Training Epoch: 2/2, step 494/7134 completed (loss: 0.01987430453300476, acc: 0.9921875)
[2025-02-13 03:38:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:02][root][INFO] - Training Epoch: 2/2, step 495/7134 completed (loss: 0.031382039189338684, acc: 0.9941176176071167)
[2025-02-13 03:38:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:02][root][INFO] - Training Epoch: 2/2, step 496/7134 completed (loss: 0.036349959671497345, acc: 0.9918699264526367)
[2025-02-13 03:38:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:03][root][INFO] - Training Epoch: 2/2, step 497/7134 completed (loss: 0.06899721920490265, acc: 0.9806201457977295)
[2025-02-13 03:38:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:03][root][INFO] - Training Epoch: 2/2, step 498/7134 completed (loss: 0.02675478532910347, acc: 0.9935897588729858)
[2025-02-13 03:38:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:03][root][INFO] - Training Epoch: 2/2, step 499/7134 completed (loss: 0.01778409816324711, acc: 0.9930459260940552)
[2025-02-13 03:38:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:04][root][INFO] - Training Epoch: 2/2, step 500/7134 completed (loss: 0.02885732799768448, acc: 0.9906666874885559)
[2025-02-13 03:38:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:04][root][INFO] - Training Epoch: 2/2, step 501/7134 completed (loss: 0.012665013782680035, acc: 0.9944598078727722)
[2025-02-13 03:38:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:05][root][INFO] - Training Epoch: 2/2, step 502/7134 completed (loss: 0.007336697541177273, acc: 0.9984709620475769)
[2025-02-13 03:38:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:05][root][INFO] - Training Epoch: 2/2, step 503/7134 completed (loss: 0.02653893269598484, acc: 0.991946280002594)
[2025-02-13 03:38:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:06][root][INFO] - Training Epoch: 2/2, step 504/7134 completed (loss: 0.010364599525928497, acc: 0.9958100318908691)
[2025-02-13 03:38:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:06][root][INFO] - Training Epoch: 2/2, step 505/7134 completed (loss: 0.007681115530431271, acc: 0.9973822236061096)
[2025-02-13 03:38:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:07][root][INFO] - Training Epoch: 2/2, step 506/7134 completed (loss: 0.010100152343511581, acc: 0.996268630027771)
[2025-02-13 03:38:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:07][root][INFO] - Training Epoch: 2/2, step 507/7134 completed (loss: 0.042663052678108215, acc: 0.99210524559021)
[2025-02-13 03:38:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:08][root][INFO] - Training Epoch: 2/2, step 508/7134 completed (loss: 0.028834495693445206, acc: 0.9888268113136292)
[2025-02-13 03:38:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:08][root][INFO] - Training Epoch: 2/2, step 509/7134 completed (loss: 0.03356323018670082, acc: 0.9799072742462158)
[2025-02-13 03:38:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:08][root][INFO] - Training Epoch: 2/2, step 510/7134 completed (loss: 0.08096571266651154, acc: 0.9792099595069885)
[2025-02-13 03:38:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:09][root][INFO] - Training Epoch: 2/2, step 511/7134 completed (loss: 0.03648702800273895, acc: 0.995110034942627)
[2025-02-13 03:38:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:09][root][INFO] - Training Epoch: 2/2, step 512/7134 completed (loss: 0.08010335266590118, acc: 0.9839679598808289)
[2025-02-13 03:38:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:10][root][INFO] - Training Epoch: 2/2, step 513/7134 completed (loss: 0.05824904143810272, acc: 0.9862306118011475)
[2025-02-13 03:38:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:10][root][INFO] - Training Epoch: 2/2, step 514/7134 completed (loss: 0.06953582912683487, acc: 0.979899525642395)
[2025-02-13 03:38:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:11][root][INFO] - Training Epoch: 2/2, step 515/7134 completed (loss: 0.0233883298933506, acc: 0.9943422675132751)
[2025-02-13 03:38:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:11][root][INFO] - Training Epoch: 2/2, step 516/7134 completed (loss: 0.018322523683309555, acc: 0.9977116584777832)
[2025-02-13 03:38:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:11][root][INFO] - Training Epoch: 2/2, step 517/7134 completed (loss: 0.07148133218288422, acc: 0.9874551892280579)
[2025-02-13 03:38:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:12][root][INFO] - Training Epoch: 2/2, step 518/7134 completed (loss: 0.024065306410193443, acc: 0.9940029978752136)
[2025-02-13 03:38:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:12][root][INFO] - Training Epoch: 2/2, step 519/7134 completed (loss: 0.02500096522271633, acc: 0.9896907210350037)
[2025-02-13 03:38:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:13][root][INFO] - Training Epoch: 2/2, step 520/7134 completed (loss: 0.017520442605018616, acc: 0.9936708807945251)
[2025-02-13 03:38:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:13][root][INFO] - Training Epoch: 2/2, step 521/7134 completed (loss: 0.034066397696733475, acc: 0.9933244585990906)
[2025-02-13 03:38:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:14][root][INFO] - Training Epoch: 2/2, step 522/7134 completed (loss: 0.03886549174785614, acc: 0.9906166195869446)
[2025-02-13 03:38:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:14][root][INFO] - Training Epoch: 2/2, step 523/7134 completed (loss: 0.01775028556585312, acc: 0.9956140518188477)
[2025-02-13 03:38:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:14][root][INFO] - Training Epoch: 2/2, step 524/7134 completed (loss: 0.03372052311897278, acc: 0.9892473220825195)
[2025-02-13 03:38:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:15][root][INFO] - Training Epoch: 2/2, step 525/7134 completed (loss: 0.06584887951612473, acc: 0.9860896468162537)
[2025-02-13 03:38:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:15][root][INFO] - Training Epoch: 2/2, step 526/7134 completed (loss: 0.02793434076011181, acc: 0.9931507110595703)
[2025-02-13 03:38:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:16][root][INFO] - Training Epoch: 2/2, step 527/7134 completed (loss: 0.03735199570655823, acc: 0.9944030046463013)
[2025-02-13 03:38:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:16][root][INFO] - Training Epoch: 2/2, step 528/7134 completed (loss: 0.005142551846802235, acc: 1.0)
[2025-02-13 03:38:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:17][root][INFO] - Training Epoch: 2/2, step 529/7134 completed (loss: 0.036882612854242325, acc: 0.9920634627342224)
[2025-02-13 03:38:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:17][root][INFO] - Training Epoch: 2/2, step 530/7134 completed (loss: 0.025208480656147003, acc: 0.994854211807251)
[2025-02-13 03:38:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:18][root][INFO] - Training Epoch: 2/2, step 531/7134 completed (loss: 0.08233663439750671, acc: 0.9863247871398926)
[2025-02-13 03:38:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:18][root][INFO] - Training Epoch: 2/2, step 532/7134 completed (loss: 0.011864365078508854, acc: 0.9979209899902344)
[2025-02-13 03:38:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:18][root][INFO] - Training Epoch: 2/2, step 533/7134 completed (loss: 0.070144422352314, acc: 0.9862542748451233)
[2025-02-13 03:38:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:19][root][INFO] - Training Epoch: 2/2, step 534/7134 completed (loss: 0.011743126437067986, acc: 0.9972527623176575)
[2025-02-13 03:38:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:19][root][INFO] - Training Epoch: 2/2, step 535/7134 completed (loss: 0.030970662832260132, acc: 0.9912126660346985)
[2025-02-13 03:38:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:20][root][INFO] - Training Epoch: 2/2, step 536/7134 completed (loss: 0.028270017355680466, acc: 0.9897810220718384)
[2025-02-13 03:38:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:20][root][INFO] - Training Epoch: 2/2, step 537/7134 completed (loss: 0.03892379254102707, acc: 0.9852724671363831)
[2025-02-13 03:38:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:21][root][INFO] - Training Epoch: 2/2, step 538/7134 completed (loss: 0.0385158509016037, acc: 0.9896142482757568)
[2025-02-13 03:38:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:21][root][INFO] - Training Epoch: 2/2, step 539/7134 completed (loss: 0.04227312281727791, acc: 0.9861591458320618)
[2025-02-13 03:38:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:22][root][INFO] - Training Epoch: 2/2, step 540/7134 completed (loss: 0.028128385543823242, acc: 0.9898989796638489)
[2025-02-13 03:38:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:22][root][INFO] - Training Epoch: 2/2, step 541/7134 completed (loss: 0.04811526834964752, acc: 0.9867021441459656)
[2025-02-13 03:38:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:22][root][INFO] - Training Epoch: 2/2, step 542/7134 completed (loss: 0.05933140590786934, acc: 0.98591548204422)
[2025-02-13 03:38:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:23][root][INFO] - Training Epoch: 2/2, step 543/7134 completed (loss: 0.07932080328464508, acc: 0.9728752374649048)
[2025-02-13 03:38:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:23][root][INFO] - Training Epoch: 2/2, step 544/7134 completed (loss: 0.051515717059373856, acc: 0.9842271208763123)
[2025-02-13 03:38:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:24][root][INFO] - Training Epoch: 2/2, step 545/7134 completed (loss: 0.04816100001335144, acc: 0.9855072498321533)
[2025-02-13 03:38:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:24][root][INFO] - Training Epoch: 2/2, step 546/7134 completed (loss: 0.10835917294025421, acc: 0.9645389914512634)
[2025-02-13 03:38:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:25][root][INFO] - Training Epoch: 2/2, step 547/7134 completed (loss: 0.04825076460838318, acc: 0.9869375824928284)
[2025-02-13 03:38:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:25][root][INFO] - Training Epoch: 2/2, step 548/7134 completed (loss: 0.02943871170282364, acc: 0.9921773076057434)
[2025-02-13 03:38:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:25][root][INFO] - Training Epoch: 2/2, step 549/7134 completed (loss: 0.06292684376239777, acc: 0.9829192757606506)
[2025-02-13 03:38:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:26][root][INFO] - Training Epoch: 2/2, step 550/7134 completed (loss: 0.053478747606277466, acc: 0.9897611141204834)
[2025-02-13 03:38:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:26][root][INFO] - Training Epoch: 2/2, step 551/7134 completed (loss: 0.04551398381590843, acc: 0.9779005646705627)
[2025-02-13 03:38:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:27][root][INFO] - Training Epoch: 2/2, step 552/7134 completed (loss: 0.0316045768558979, acc: 0.9886040091514587)
[2025-02-13 03:38:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:27][root][INFO] - Training Epoch: 2/2, step 553/7134 completed (loss: 0.02977060154080391, acc: 0.9890350699424744)
[2025-02-13 03:38:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:27][root][INFO] - Training Epoch: 2/2, step 554/7134 completed (loss: 0.042519304901361465, acc: 0.9871612191200256)
[2025-02-13 03:38:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:28][root][INFO] - Training Epoch: 2/2, step 555/7134 completed (loss: 0.04032890871167183, acc: 0.9873617887496948)
[2025-02-13 03:38:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:28][root][INFO] - Training Epoch: 2/2, step 556/7134 completed (loss: 0.04088537022471428, acc: 0.9886845946311951)
[2025-02-13 03:38:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:29][root][INFO] - Training Epoch: 2/2, step 557/7134 completed (loss: 0.054883502423763275, acc: 0.9763113260269165)
[2025-02-13 03:38:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:29][root][INFO] - Training Epoch: 2/2, step 558/7134 completed (loss: 0.027158329263329506, acc: 0.9900709390640259)
[2025-02-13 03:38:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:30][root][INFO] - Training Epoch: 2/2, step 559/7134 completed (loss: 0.07659677416086197, acc: 0.9729344844818115)
[2025-02-13 03:38:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:30][root][INFO] - Training Epoch: 2/2, step 560/7134 completed (loss: 0.03659708425402641, acc: 0.9912126660346985)
[2025-02-13 03:38:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:31][root][INFO] - Training Epoch: 2/2, step 561/7134 completed (loss: 0.05278006196022034, acc: 0.9859514832496643)
[2025-02-13 03:38:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:31][root][INFO] - Training Epoch: 2/2, step 562/7134 completed (loss: 0.03249099850654602, acc: 0.9867549538612366)
[2025-02-13 03:38:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:31][root][INFO] - Training Epoch: 2/2, step 563/7134 completed (loss: 0.08279597014188766, acc: 0.9746835231781006)
[2025-02-13 03:38:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:32][root][INFO] - Training Epoch: 2/2, step 564/7134 completed (loss: 0.03998824581503868, acc: 0.988875150680542)
[2025-02-13 03:38:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:32][root][INFO] - Training Epoch: 2/2, step 565/7134 completed (loss: 0.043736815452575684, acc: 0.9832496047019958)
[2025-02-13 03:38:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:33][root][INFO] - Training Epoch: 2/2, step 566/7134 completed (loss: 0.067323699593544, acc: 0.9791183471679688)
[2025-02-13 03:38:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:33][root][INFO] - Training Epoch: 2/2, step 567/7134 completed (loss: 0.053141772747039795, acc: 0.9887797832489014)
[2025-02-13 03:38:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:34][root][INFO] - Training Epoch: 2/2, step 568/7134 completed (loss: 0.037870075553655624, acc: 0.9921568632125854)
[2025-02-13 03:38:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:34][root][INFO] - Training Epoch: 2/2, step 569/7134 completed (loss: 0.020405564457178116, acc: 0.9962962865829468)
[2025-02-13 03:38:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:34][root][INFO] - Training Epoch: 2/2, step 570/7134 completed (loss: 0.02157348208129406, acc: 0.9905992746353149)
[2025-02-13 03:38:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:35][root][INFO] - Training Epoch: 2/2, step 571/7134 completed (loss: 0.019008856266736984, acc: 0.992443323135376)
[2025-02-13 03:38:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:35][root][INFO] - Training Epoch: 2/2, step 572/7134 completed (loss: 0.020038040354847908, acc: 0.9961389899253845)
[2025-02-13 03:38:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:36][root][INFO] - Training Epoch: 2/2, step 573/7134 completed (loss: 0.015228094533085823, acc: 0.9961636662483215)
[2025-02-13 03:38:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:36][root][INFO] - Training Epoch: 2/2, step 574/7134 completed (loss: 0.017315100878477097, acc: 0.9947299361228943)
[2025-02-13 03:38:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:37][root][INFO] - Training Epoch: 2/2, step 575/7134 completed (loss: 0.016441598534584045, acc: 0.9948586225509644)
[2025-02-13 03:38:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:37][root][INFO] - Training Epoch: 2/2, step 576/7134 completed (loss: 0.018466772511601448, acc: 0.9938119053840637)
[2025-02-13 03:38:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:38][root][INFO] - Training Epoch: 2/2, step 577/7134 completed (loss: 0.01900409162044525, acc: 0.992094874382019)
[2025-02-13 03:38:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:38][root][INFO] - Training Epoch: 2/2, step 578/7134 completed (loss: 0.02625771425664425, acc: 0.988399088382721)
[2025-02-13 03:38:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:39][root][INFO] - Training Epoch: 2/2, step 579/7134 completed (loss: 0.037581488490104675, acc: 0.9884467124938965)
[2025-02-13 03:38:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:39][root][INFO] - Training Epoch: 2/2, step 580/7134 completed (loss: 0.0571957603096962, acc: 0.981794536113739)
[2025-02-13 03:38:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:39][root][INFO] - Training Epoch: 2/2, step 581/7134 completed (loss: 0.023425575345754623, acc: 0.9930264949798584)
[2025-02-13 03:38:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:40][root][INFO] - Training Epoch: 2/2, step 582/7134 completed (loss: 0.02958923764526844, acc: 0.9904191493988037)
[2025-02-13 03:38:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:40][root][INFO] - Training Epoch: 2/2, step 583/7134 completed (loss: 0.025162406265735626, acc: 0.9922077655792236)
[2025-02-13 03:38:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:41][root][INFO] - Training Epoch: 2/2, step 584/7134 completed (loss: 0.014771794900298119, acc: 0.9940191507339478)
[2025-02-13 03:38:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:41][root][INFO] - Training Epoch: 2/2, step 585/7134 completed (loss: 0.019271133467555046, acc: 0.9963768124580383)
[2025-02-13 03:38:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:42][root][INFO] - Training Epoch: 2/2, step 586/7134 completed (loss: 0.01571575365960598, acc: 0.9960886836051941)
[2025-02-13 03:38:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:42][root][INFO] - Training Epoch: 2/2, step 587/7134 completed (loss: 0.043972693383693695, acc: 0.9911054372787476)
[2025-02-13 03:38:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:43][root][INFO] - Training Epoch: 2/2, step 588/7134 completed (loss: 0.026447394862771034, acc: 0.9937205910682678)
[2025-02-13 03:38:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:43][root][INFO] - Training Epoch: 2/2, step 589/7134 completed (loss: 0.02910936437547207, acc: 0.9867841601371765)
[2025-02-13 03:38:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:43][root][INFO] - Training Epoch: 2/2, step 590/7134 completed (loss: 0.027072198688983917, acc: 0.9939849376678467)
[2025-02-13 03:38:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:44][root][INFO] - Training Epoch: 2/2, step 591/7134 completed (loss: 0.024978384375572205, acc: 0.9951573610305786)
[2025-02-13 03:38:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:44][root][INFO] - Training Epoch: 2/2, step 592/7134 completed (loss: 0.02017848938703537, acc: 0.9949238300323486)
[2025-02-13 03:38:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:45][root][INFO] - Training Epoch: 2/2, step 593/7134 completed (loss: 0.0345018170773983, acc: 0.9868766665458679)
[2025-02-13 03:38:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:45][root][INFO] - Training Epoch: 2/2, step 594/7134 completed (loss: 0.04924621060490608, acc: 0.9886792302131653)
[2025-02-13 03:38:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:46][root][INFO] - Training Epoch: 2/2, step 595/7134 completed (loss: 0.028384095057845116, acc: 0.9898862242698669)
[2025-02-13 03:38:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:46][root][INFO] - Training Epoch: 2/2, step 596/7134 completed (loss: 0.023000087589025497, acc: 0.9909793734550476)
[2025-02-13 03:38:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:47][root][INFO] - Training Epoch: 2/2, step 597/7134 completed (loss: 0.10942506045103073, acc: 0.9762845635414124)
[2025-02-13 03:38:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:47][root][INFO] - Training Epoch: 2/2, step 598/7134 completed (loss: 0.0407966673374176, acc: 0.9864681959152222)
[2025-02-13 03:38:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:48][root][INFO] - Training Epoch: 2/2, step 599/7134 completed (loss: 0.02249056100845337, acc: 0.9912280440330505)
[2025-02-13 03:38:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:48][root][INFO] - Training Epoch: 2/2, step 600/7134 completed (loss: 0.03859621286392212, acc: 0.9891696572303772)
[2025-02-13 03:38:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:49][root][INFO] - Training Epoch: 2/2, step 601/7134 completed (loss: 0.027242520824074745, acc: 0.9875518679618835)
[2025-02-13 03:38:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:49][root][INFO] - Training Epoch: 2/2, step 602/7134 completed (loss: 0.033690597862005234, acc: 0.9941995143890381)
[2025-02-13 03:38:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:49][root][INFO] - Training Epoch: 2/2, step 603/7134 completed (loss: 0.019086483865976334, acc: 0.9948586225509644)
[2025-02-13 03:38:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:50][root][INFO] - Training Epoch: 2/2, step 604/7134 completed (loss: 0.019810277968645096, acc: 0.9941434860229492)
[2025-02-13 03:38:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:50][root][INFO] - Training Epoch: 2/2, step 605/7134 completed (loss: 0.021525217220187187, acc: 0.9909326434135437)
[2025-02-13 03:38:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:51][root][INFO] - Training Epoch: 2/2, step 606/7134 completed (loss: 0.08684760332107544, acc: 0.978986382484436)
[2025-02-13 03:38:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:51][root][INFO] - Training Epoch: 2/2, step 607/7134 completed (loss: 0.026755481958389282, acc: 0.9933244585990906)
[2025-02-13 03:38:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:52][root][INFO] - Training Epoch: 2/2, step 608/7134 completed (loss: 0.023650260642170906, acc: 0.9935232996940613)
[2025-02-13 03:38:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:52][root][INFO] - Training Epoch: 2/2, step 609/7134 completed (loss: 0.027620425447821617, acc: 0.9906322956085205)
[2025-02-13 03:38:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:53][root][INFO] - Training Epoch: 2/2, step 610/7134 completed (loss: 0.05575665086507797, acc: 0.984829306602478)
[2025-02-13 03:38:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:53][root][INFO] - Training Epoch: 2/2, step 611/7134 completed (loss: 0.0222672950476408, acc: 0.9914634227752686)
[2025-02-13 03:38:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:53][root][INFO] - Training Epoch: 2/2, step 612/7134 completed (loss: 0.03036840260028839, acc: 0.9881266355514526)
[2025-02-13 03:38:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:54][root][INFO] - Training Epoch: 2/2, step 613/7134 completed (loss: 0.02632574923336506, acc: 0.9929659962654114)
[2025-02-13 03:38:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:54][root][INFO] - Training Epoch: 2/2, step 614/7134 completed (loss: 0.021803386509418488, acc: 0.9898074865341187)
[2025-02-13 03:38:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:55][root][INFO] - Training Epoch: 2/2, step 615/7134 completed (loss: 0.010384569875895977, acc: 0.9987729787826538)
[2025-02-13 03:38:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:55][root][INFO] - Training Epoch: 2/2, step 616/7134 completed (loss: 0.022297179326415062, acc: 0.9943181872367859)
[2025-02-13 03:38:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:56][root][INFO] - Training Epoch: 2/2, step 617/7134 completed (loss: 0.03743726387619972, acc: 0.9908883571624756)
[2025-02-13 03:38:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:56][root][INFO] - Training Epoch: 2/2, step 618/7134 completed (loss: 0.029517095535993576, acc: 0.9879679083824158)
[2025-02-13 03:38:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:57][root][INFO] - Training Epoch: 2/2, step 619/7134 completed (loss: 0.03318854421377182, acc: 0.9939271211624146)
[2025-02-13 03:38:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:57][root][INFO] - Training Epoch: 2/2, step 620/7134 completed (loss: 0.05084655433893204, acc: 0.987261176109314)
[2025-02-13 03:38:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:58][root][INFO] - Training Epoch: 2/2, step 621/7134 completed (loss: 0.03253563120961189, acc: 0.990867555141449)
[2025-02-13 03:38:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:58][root][INFO] - Training Epoch: 2/2, step 622/7134 completed (loss: 0.03264570236206055, acc: 0.9885222315788269)
[2025-02-13 03:38:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:58][root][INFO] - Training Epoch: 2/2, step 623/7134 completed (loss: 0.07859084010124207, acc: 0.9854227304458618)
[2025-02-13 03:38:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:59][root][INFO] - Training Epoch: 2/2, step 624/7134 completed (loss: 0.022936100140213966, acc: 0.995945930480957)
[2025-02-13 03:38:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:38:59][root][INFO] - Training Epoch: 2/2, step 625/7134 completed (loss: 0.04889633506536484, acc: 0.9881266355514526)
[2025-02-13 03:38:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:00][root][INFO] - Training Epoch: 2/2, step 626/7134 completed (loss: 0.08268366754055023, acc: 0.9810996651649475)
[2025-02-13 03:39:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:00][root][INFO] - Training Epoch: 2/2, step 627/7134 completed (loss: 0.026806676760315895, acc: 0.99262535572052)
[2025-02-13 03:39:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:01][root][INFO] - Training Epoch: 2/2, step 628/7134 completed (loss: 0.0258659515529871, acc: 0.994452178478241)
[2025-02-13 03:39:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:01][root][INFO] - Training Epoch: 2/2, step 629/7134 completed (loss: 0.04155382513999939, acc: 0.9871244430541992)
[2025-02-13 03:39:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:01][root][INFO] - Training Epoch: 2/2, step 630/7134 completed (loss: 0.02604088746011257, acc: 0.9935232996940613)
[2025-02-13 03:39:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:02][root][INFO] - Training Epoch: 2/2, step 631/7134 completed (loss: 0.02098490111529827, acc: 0.99210524559021)
[2025-02-13 03:39:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:02][root][INFO] - Training Epoch: 2/2, step 632/7134 completed (loss: 0.021940071135759354, acc: 0.991909384727478)
[2025-02-13 03:39:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:03][root][INFO] - Training Epoch: 2/2, step 633/7134 completed (loss: 0.038300227373838425, acc: 0.9898403286933899)
[2025-02-13 03:39:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:03][root][INFO] - Training Epoch: 2/2, step 634/7134 completed (loss: 0.014832994900643826, acc: 0.9952531456947327)
[2025-02-13 03:39:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:04][root][INFO] - Training Epoch: 2/2, step 635/7134 completed (loss: 0.03160242736339569, acc: 0.9938931465148926)
[2025-02-13 03:39:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:04][root][INFO] - Training Epoch: 2/2, step 636/7134 completed (loss: 0.06757654994726181, acc: 0.9875862002372742)
[2025-02-13 03:39:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:05][root][INFO] - Training Epoch: 2/2, step 637/7134 completed (loss: 0.03955827280879021, acc: 0.9888268113136292)
[2025-02-13 03:39:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:05][root][INFO] - Training Epoch: 2/2, step 638/7134 completed (loss: 0.04169810190796852, acc: 0.9875195026397705)
[2025-02-13 03:39:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:05][root][INFO] - Training Epoch: 2/2, step 639/7134 completed (loss: 0.016346562653779984, acc: 0.9950000047683716)
[2025-02-13 03:39:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:06][root][INFO] - Training Epoch: 2/2, step 640/7134 completed (loss: 0.0339207723736763, acc: 0.9930434823036194)
[2025-02-13 03:39:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:06][root][INFO] - Training Epoch: 2/2, step 641/7134 completed (loss: 0.042868006974458694, acc: 0.9901800155639648)
[2025-02-13 03:39:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:07][root][INFO] - Training Epoch: 2/2, step 642/7134 completed (loss: 0.014638375490903854, acc: 0.9948979616165161)
[2025-02-13 03:39:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:07][root][INFO] - Training Epoch: 2/2, step 643/7134 completed (loss: 0.01214041281491518, acc: 0.9987162947654724)
[2025-02-13 03:39:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:08][root][INFO] - Training Epoch: 2/2, step 644/7134 completed (loss: 0.011683067306876183, acc: 0.9980695247650146)
[2025-02-13 03:39:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:08][root][INFO] - Training Epoch: 2/2, step 645/7134 completed (loss: 0.03201127052307129, acc: 0.9932659864425659)
[2025-02-13 03:39:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:08][root][INFO] - Training Epoch: 2/2, step 646/7134 completed (loss: 0.030919194221496582, acc: 0.9938837885856628)
[2025-02-13 03:39:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:09][root][INFO] - Training Epoch: 2/2, step 647/7134 completed (loss: 0.007089731749147177, acc: 0.9983079433441162)
[2025-02-13 03:39:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:09][root][INFO] - Training Epoch: 2/2, step 648/7134 completed (loss: 0.06400955468416214, acc: 0.9808027744293213)
[2025-02-13 03:39:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:10][root][INFO] - Training Epoch: 2/2, step 649/7134 completed (loss: 0.029692620038986206, acc: 0.9923076629638672)
[2025-02-13 03:39:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:10][root][INFO] - Training Epoch: 2/2, step 650/7134 completed (loss: 0.04758892208337784, acc: 0.9843013882637024)
[2025-02-13 03:39:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:11][root][INFO] - Training Epoch: 2/2, step 651/7134 completed (loss: 0.062297068536281586, acc: 0.9831804037094116)
[2025-02-13 03:39:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:11][root][INFO] - Training Epoch: 2/2, step 652/7134 completed (loss: 0.035975951701402664, acc: 0.9862542748451233)
[2025-02-13 03:39:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:12][root][INFO] - Training Epoch: 2/2, step 653/7134 completed (loss: 0.020896166563034058, acc: 0.9900709390640259)
[2025-02-13 03:39:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:12][root][INFO] - Training Epoch: 2/2, step 654/7134 completed (loss: 0.09364382177591324, acc: 0.9697624444961548)
[2025-02-13 03:39:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:12][root][INFO] - Training Epoch: 2/2, step 655/7134 completed (loss: 0.04945150390267372, acc: 0.9858871102333069)
[2025-02-13 03:39:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:13][root][INFO] - Training Epoch: 2/2, step 656/7134 completed (loss: 0.060640063136816025, acc: 0.9841688871383667)
[2025-02-13 03:39:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:13][root][INFO] - Training Epoch: 2/2, step 657/7134 completed (loss: 0.031495608389377594, acc: 0.9903692007064819)
[2025-02-13 03:39:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:14][root][INFO] - Training Epoch: 2/2, step 658/7134 completed (loss: 0.0392814539372921, acc: 0.9881423115730286)
[2025-02-13 03:39:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:14][root][INFO] - Training Epoch: 2/2, step 659/7134 completed (loss: 0.022118743509054184, acc: 0.9912023544311523)
[2025-02-13 03:39:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:15][root][INFO] - Training Epoch: 2/2, step 660/7134 completed (loss: 0.057327888906002045, acc: 0.9853801131248474)
[2025-02-13 03:39:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:15][root][INFO] - Training Epoch: 2/2, step 661/7134 completed (loss: 0.07668652385473251, acc: 0.9853556752204895)
[2025-02-13 03:39:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:16][root][INFO] - Training Epoch: 2/2, step 662/7134 completed (loss: 0.02597733959555626, acc: 0.9932975769042969)
[2025-02-13 03:39:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:16][root][INFO] - Training Epoch: 2/2, step 663/7134 completed (loss: 0.03613101691007614, acc: 0.9876543283462524)
[2025-02-13 03:39:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:17][root][INFO] - Training Epoch: 2/2, step 664/7134 completed (loss: 0.04874285310506821, acc: 0.9916666746139526)
[2025-02-13 03:39:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:17][root][INFO] - Training Epoch: 2/2, step 665/7134 completed (loss: 0.037502288818359375, acc: 0.9919246435165405)
[2025-02-13 03:39:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:18][root][INFO] - Training Epoch: 2/2, step 666/7134 completed (loss: 0.06439066678285599, acc: 0.9821428656578064)
[2025-02-13 03:39:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:18][root][INFO] - Training Epoch: 2/2, step 667/7134 completed (loss: 0.04147534817457199, acc: 0.9852724671363831)
[2025-02-13 03:39:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:19][root][INFO] - Training Epoch: 2/2, step 668/7134 completed (loss: 0.03070276416838169, acc: 0.9923954606056213)
[2025-02-13 03:39:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:19][root][INFO] - Training Epoch: 2/2, step 669/7134 completed (loss: 0.030403584241867065, acc: 0.9972714781761169)
[2025-02-13 03:39:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:20][root][INFO] - Training Epoch: 2/2, step 670/7134 completed (loss: 0.0429442897439003, acc: 0.987679660320282)
[2025-02-13 03:39:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:20][root][INFO] - Training Epoch: 2/2, step 671/7134 completed (loss: 0.03712642192840576, acc: 0.9871382713317871)
[2025-02-13 03:39:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:21][root][INFO] - Training Epoch: 2/2, step 672/7134 completed (loss: 0.06116579473018646, acc: 0.9910581111907959)
[2025-02-13 03:39:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:21][root][INFO] - Training Epoch: 2/2, step 673/7134 completed (loss: 0.041891440749168396, acc: 0.9877150058746338)
[2025-02-13 03:39:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:22][root][INFO] - Training Epoch: 2/2, step 674/7134 completed (loss: 0.045271359384059906, acc: 0.9879336357116699)
[2025-02-13 03:39:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:22][root][INFO] - Training Epoch: 2/2, step 675/7134 completed (loss: 0.06736414134502411, acc: 0.9856584072113037)
[2025-02-13 03:39:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:22][root][INFO] - Training Epoch: 2/2, step 676/7134 completed (loss: 0.036813706159591675, acc: 0.9917159676551819)
[2025-02-13 03:39:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:23][root][INFO] - Training Epoch: 2/2, step 677/7134 completed (loss: 0.011521907523274422, acc: 0.9963008761405945)
[2025-02-13 03:39:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:23][root][INFO] - Training Epoch: 2/2, step 678/7134 completed (loss: 0.028931938111782074, acc: 0.9946164488792419)
[2025-02-13 03:39:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:24][root][INFO] - Training Epoch: 2/2, step 679/7134 completed (loss: 0.01314014196395874, acc: 0.996219277381897)
[2025-02-13 03:39:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:24][root][INFO] - Training Epoch: 2/2, step 680/7134 completed (loss: 0.050858017057180405, acc: 0.986522912979126)
[2025-02-13 03:39:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:25][root][INFO] - Training Epoch: 2/2, step 681/7134 completed (loss: 0.04045039415359497, acc: 0.9889570474624634)
[2025-02-13 03:39:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:25][root][INFO] - Training Epoch: 2/2, step 682/7134 completed (loss: 0.026341749355196953, acc: 0.9913544654846191)
[2025-02-13 03:39:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:26][root][INFO] - Training Epoch: 2/2, step 683/7134 completed (loss: 0.04098396748304367, acc: 0.9876543283462524)
[2025-02-13 03:39:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:26][root][INFO] - Training Epoch: 2/2, step 684/7134 completed (loss: 0.04146404564380646, acc: 0.9889196753501892)
[2025-02-13 03:39:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:27][root][INFO] - Training Epoch: 2/2, step 685/7134 completed (loss: 0.02455458603799343, acc: 0.9925187230110168)
[2025-02-13 03:39:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:27][root][INFO] - Training Epoch: 2/2, step 686/7134 completed (loss: 0.008147694170475006, acc: 0.997357964515686)
[2025-02-13 03:39:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:28][root][INFO] - Training Epoch: 2/2, step 687/7134 completed (loss: 0.013350652530789375, acc: 0.9961315393447876)
[2025-02-13 03:39:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:28][root][INFO] - Training Epoch: 2/2, step 688/7134 completed (loss: 0.006159034091979265, acc: 0.9985207319259644)
[2025-02-13 03:39:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:29][root][INFO] - Training Epoch: 2/2, step 689/7134 completed (loss: 0.016130590811371803, acc: 0.9948717951774597)
[2025-02-13 03:39:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:29][root][INFO] - Training Epoch: 2/2, step 690/7134 completed (loss: 0.015124579891562462, acc: 0.995230495929718)
[2025-02-13 03:39:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:29][root][INFO] - Training Epoch: 2/2, step 691/7134 completed (loss: 0.0448201522231102, acc: 0.9856321811676025)
[2025-02-13 03:39:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:30][root][INFO] - Training Epoch: 2/2, step 692/7134 completed (loss: 0.04429446905851364, acc: 0.9879999756813049)
[2025-02-13 03:39:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:30][root][INFO] - Training Epoch: 2/2, step 693/7134 completed (loss: 0.04561874270439148, acc: 0.9838449358940125)
[2025-02-13 03:39:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:31][root][INFO] - Training Epoch: 2/2, step 694/7134 completed (loss: 0.05257945880293846, acc: 0.9868612885475159)
[2025-02-13 03:39:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:31][root][INFO] - Training Epoch: 2/2, step 695/7134 completed (loss: 0.07074063271284103, acc: 0.9827883243560791)
[2025-02-13 03:39:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:32][root][INFO] - Training Epoch: 2/2, step 696/7134 completed (loss: 0.046560052782297134, acc: 0.9870550036430359)
[2025-02-13 03:39:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:32][root][INFO] - Training Epoch: 2/2, step 697/7134 completed (loss: 0.029736941680312157, acc: 0.996277928352356)
[2025-02-13 03:39:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:33][root][INFO] - Training Epoch: 2/2, step 698/7134 completed (loss: 0.013773376122117043, acc: 0.9967426657676697)
[2025-02-13 03:39:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:33][root][INFO] - Training Epoch: 2/2, step 699/7134 completed (loss: 0.015706254169344902, acc: 0.9984423518180847)
[2025-02-13 03:39:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:34][root][INFO] - Training Epoch: 2/2, step 700/7134 completed (loss: 0.044910725206136703, acc: 0.986940324306488)
[2025-02-13 03:39:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:34][root][INFO] - Training Epoch: 2/2, step 701/7134 completed (loss: 0.03929644823074341, acc: 0.9858155846595764)
[2025-02-13 03:39:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:35][root][INFO] - Training Epoch: 2/2, step 702/7134 completed (loss: 0.03371802717447281, acc: 0.9853895902633667)
[2025-02-13 03:39:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:35][root][INFO] - Training Epoch: 2/2, step 703/7134 completed (loss: 0.029749194160103798, acc: 0.9903069734573364)
[2025-02-13 03:39:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:35][root][INFO] - Training Epoch: 2/2, step 704/7134 completed (loss: 0.03363073989748955, acc: 0.9893617033958435)
[2025-02-13 03:39:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:36][root][INFO] - Training Epoch: 2/2, step 705/7134 completed (loss: 0.054405082017183304, acc: 0.9821228981018066)
[2025-02-13 03:39:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:36][root][INFO] - Training Epoch: 2/2, step 706/7134 completed (loss: 0.0254611074924469, acc: 0.9934354424476624)
[2025-02-13 03:39:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:37][root][INFO] - Training Epoch: 2/2, step 707/7134 completed (loss: 0.025671513751149178, acc: 0.9892086386680603)
[2025-02-13 03:39:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:37][root][INFO] - Training Epoch: 2/2, step 708/7134 completed (loss: 0.020504551008343697, acc: 0.9938042163848877)
[2025-02-13 03:39:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:38][root][INFO] - Training Epoch: 2/2, step 709/7134 completed (loss: 0.034958671778440475, acc: 0.9916467666625977)
[2025-02-13 03:39:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:38][root][INFO] - Training Epoch: 2/2, step 710/7134 completed (loss: 0.01821294240653515, acc: 0.9963680505752563)
[2025-02-13 03:39:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:39][root][INFO] - Training Epoch: 2/2, step 711/7134 completed (loss: 0.016684288159012794, acc: 0.996458113193512)
[2025-02-13 03:39:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:39][root][INFO] - Training Epoch: 2/2, step 712/7134 completed (loss: 0.025582529604434967, acc: 0.9926739931106567)
[2025-02-13 03:39:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:40][root][INFO] - Training Epoch: 2/2, step 713/7134 completed (loss: 0.015285402536392212, acc: 0.9953863620758057)
[2025-02-13 03:39:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:40][root][INFO] - Training Epoch: 2/2, step 714/7134 completed (loss: 0.02872907929122448, acc: 0.9927536249160767)
[2025-02-13 03:39:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:41][root][INFO] - Training Epoch: 2/2, step 715/7134 completed (loss: 0.008776233531534672, acc: 0.9971590638160706)
[2025-02-13 03:39:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:41][root][INFO] - Training Epoch: 2/2, step 716/7134 completed (loss: 0.01564086601138115, acc: 0.9943116903305054)
[2025-02-13 03:39:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:42][root][INFO] - Training Epoch: 2/2, step 717/7134 completed (loss: 0.03531457111239433, acc: 0.9949430823326111)
[2025-02-13 03:39:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:42][root][INFO] - Training Epoch: 2/2, step 718/7134 completed (loss: 0.015666523948311806, acc: 0.9949874877929688)
[2025-02-13 03:39:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:43][root][INFO] - Training Epoch: 2/2, step 719/7134 completed (loss: 0.014258801005780697, acc: 0.9965517520904541)
[2025-02-13 03:39:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:43][root][INFO] - Training Epoch: 2/2, step 720/7134 completed (loss: 0.019954295828938484, acc: 0.9949874877929688)
[2025-02-13 03:39:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:44][root][INFO] - Training Epoch: 2/2, step 721/7134 completed (loss: 0.018915027379989624, acc: 0.9935979247093201)
[2025-02-13 03:39:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:44][root][INFO] - Training Epoch: 2/2, step 722/7134 completed (loss: 0.011374348774552345, acc: 0.9964994192123413)
[2025-02-13 03:39:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:45][root][INFO] - Training Epoch: 2/2, step 723/7134 completed (loss: 0.01065483782440424, acc: 0.9951573610305786)
[2025-02-13 03:39:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:45][root][INFO] - Training Epoch: 2/2, step 724/7134 completed (loss: 0.01703924685716629, acc: 0.995708167552948)
[2025-02-13 03:39:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:45][root][INFO] - Training Epoch: 2/2, step 725/7134 completed (loss: 0.010547148995101452, acc: 0.9956584572792053)
[2025-02-13 03:39:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:46][root][INFO] - Training Epoch: 2/2, step 726/7134 completed (loss: 0.006470812484622002, acc: 0.9986053109169006)
[2025-02-13 03:39:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:46][root][INFO] - Training Epoch: 2/2, step 727/7134 completed (loss: 0.0066945478320121765, acc: 0.9985569715499878)
[2025-02-13 03:39:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:47][root][INFO] - Training Epoch: 2/2, step 728/7134 completed (loss: 0.0048753791488707066, acc: 1.0)
[2025-02-13 03:39:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:47][root][INFO] - Training Epoch: 2/2, step 729/7134 completed (loss: 0.014560692943632603, acc: 0.9944367408752441)
[2025-02-13 03:39:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:48][root][INFO] - Training Epoch: 2/2, step 730/7134 completed (loss: 0.01178043708205223, acc: 0.9961089491844177)
[2025-02-13 03:39:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:48][root][INFO] - Training Epoch: 2/2, step 731/7134 completed (loss: 0.01725194603204727, acc: 0.9959999918937683)
[2025-02-13 03:39:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:49][root][INFO] - Training Epoch: 2/2, step 732/7134 completed (loss: 0.038814835250377655, acc: 0.9830188751220703)
[2025-02-13 03:39:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:49][root][INFO] - Training Epoch: 2/2, step 733/7134 completed (loss: 0.03379438444972038, acc: 0.9875665903091431)
[2025-02-13 03:39:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:50][root][INFO] - Training Epoch: 2/2, step 734/7134 completed (loss: 0.008966173976659775, acc: 0.9984639286994934)
[2025-02-13 03:39:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:50][root][INFO] - Training Epoch: 2/2, step 735/7134 completed (loss: 0.021703962236642838, acc: 0.9887459874153137)
[2025-02-13 03:39:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:51][root][INFO] - Training Epoch: 2/2, step 736/7134 completed (loss: 0.04353592172265053, acc: 0.9824175834655762)
[2025-02-13 03:39:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:51][root][INFO] - Training Epoch: 2/2, step 737/7134 completed (loss: 0.10433752834796906, acc: 0.9817444086074829)
[2025-02-13 03:39:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:51][root][INFO] - Training Epoch: 2/2, step 738/7134 completed (loss: 0.008975723758339882, acc: 0.9961538314819336)
[2025-02-13 03:39:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:52][root][INFO] - Training Epoch: 2/2, step 739/7134 completed (loss: 0.013057342730462551, acc: 0.9944953918457031)
[2025-02-13 03:39:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:52][root][INFO] - Training Epoch: 2/2, step 740/7134 completed (loss: 0.002179414499551058, acc: 1.0)
[2025-02-13 03:39:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:53][root][INFO] - Training Epoch: 2/2, step 741/7134 completed (loss: 0.015570087358355522, acc: 0.9921135902404785)
[2025-02-13 03:39:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:53][root][INFO] - Training Epoch: 2/2, step 742/7134 completed (loss: 0.020356394350528717, acc: 0.9916142821311951)
[2025-02-13 03:39:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:54][root][INFO] - Training Epoch: 2/2, step 743/7134 completed (loss: 0.03921911120414734, acc: 0.9894578456878662)
[2025-02-13 03:39:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:54][root][INFO] - Training Epoch: 2/2, step 744/7134 completed (loss: 0.020525380969047546, acc: 0.9920634627342224)
[2025-02-13 03:39:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:55][root][INFO] - Training Epoch: 2/2, step 745/7134 completed (loss: 0.023566853255033493, acc: 0.9939576983451843)
[2025-02-13 03:39:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:55][root][INFO] - Training Epoch: 2/2, step 746/7134 completed (loss: 0.035457611083984375, acc: 0.9917898178100586)
[2025-02-13 03:39:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:56][root][INFO] - Training Epoch: 2/2, step 747/7134 completed (loss: 0.08896145224571228, acc: 0.9800664186477661)
[2025-02-13 03:39:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:56][root][INFO] - Training Epoch: 2/2, step 748/7134 completed (loss: 0.016591336578130722, acc: 0.995488703250885)
[2025-02-13 03:39:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:56][root][INFO] - Training Epoch: 2/2, step 749/7134 completed (loss: 0.01269332505762577, acc: 0.9971181750297546)
[2025-02-13 03:39:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:57][root][INFO] - Training Epoch: 2/2, step 750/7134 completed (loss: 0.018919970840215683, acc: 0.9945945739746094)
[2025-02-13 03:39:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:57][root][INFO] - Training Epoch: 2/2, step 751/7134 completed (loss: 0.06557902693748474, acc: 0.9863713979721069)
[2025-02-13 03:39:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:58][root][INFO] - Training Epoch: 2/2, step 752/7134 completed (loss: 0.06633415818214417, acc: 0.9849849939346313)
[2025-02-13 03:39:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:58][root][INFO] - Training Epoch: 2/2, step 753/7134 completed (loss: 0.03214145824313164, acc: 0.9898648858070374)
[2025-02-13 03:39:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:59][root][INFO] - Training Epoch: 2/2, step 754/7134 completed (loss: 0.02516661398112774, acc: 0.9909502267837524)
[2025-02-13 03:39:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:39:59][root][INFO] - Training Epoch: 2/2, step 755/7134 completed (loss: 0.06366632133722305, acc: 0.9851484894752502)
[2025-02-13 03:39:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:00][root][INFO] - Training Epoch: 2/2, step 756/7134 completed (loss: 0.02130219340324402, acc: 0.9925233721733093)
[2025-02-13 03:40:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:00][root][INFO] - Training Epoch: 2/2, step 757/7134 completed (loss: 0.02135971002280712, acc: 0.992337167263031)
[2025-02-13 03:40:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:00][root][INFO] - Training Epoch: 2/2, step 758/7134 completed (loss: 0.020725157111883163, acc: 0.9948365092277527)
[2025-02-13 03:40:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:01][root][INFO] - Training Epoch: 2/2, step 759/7134 completed (loss: 0.02882714569568634, acc: 0.9883720874786377)
[2025-02-13 03:40:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:01][root][INFO] - Training Epoch: 2/2, step 760/7134 completed (loss: 0.008706452324986458, acc: 0.9965338110923767)
[2025-02-13 03:40:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:02][root][INFO] - Training Epoch: 2/2, step 761/7134 completed (loss: 0.014936456456780434, acc: 0.9953051805496216)
[2025-02-13 03:40:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:02][root][INFO] - Training Epoch: 2/2, step 762/7134 completed (loss: 0.016318973153829575, acc: 0.9946236610412598)
[2025-02-13 03:40:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:03][root][INFO] - Training Epoch: 2/2, step 763/7134 completed (loss: 0.05105183273553848, acc: 0.9869706630706787)
[2025-02-13 03:40:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:03][root][INFO] - Training Epoch: 2/2, step 764/7134 completed (loss: 0.021315982565283775, acc: 0.9942611455917358)
[2025-02-13 03:40:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:04][root][INFO] - Training Epoch: 2/2, step 765/7134 completed (loss: 0.009019758552312851, acc: 0.9970717430114746)
[2025-02-13 03:40:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:04][root][INFO] - Training Epoch: 2/2, step 766/7134 completed (loss: 0.012230606749653816, acc: 0.998516321182251)
[2025-02-13 03:40:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:04][root][INFO] - Training Epoch: 2/2, step 767/7134 completed (loss: 0.023206273093819618, acc: 0.9941434860229492)
[2025-02-13 03:40:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:05][root][INFO] - Training Epoch: 2/2, step 768/7134 completed (loss: 0.035498302429914474, acc: 0.9900850057601929)
[2025-02-13 03:40:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:05][root][INFO] - Training Epoch: 2/2, step 769/7134 completed (loss: 0.014713886193931103, acc: 0.9942693114280701)
[2025-02-13 03:40:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:06][root][INFO] - Training Epoch: 2/2, step 770/7134 completed (loss: 0.022470250725746155, acc: 0.9938744306564331)
[2025-02-13 03:40:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:06][root][INFO] - Training Epoch: 2/2, step 771/7134 completed (loss: 0.0261062141507864, acc: 0.9900990128517151)
[2025-02-13 03:40:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:07][root][INFO] - Training Epoch: 2/2, step 772/7134 completed (loss: 0.027609197422862053, acc: 0.9916247725486755)
[2025-02-13 03:40:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:07][root][INFO] - Training Epoch: 2/2, step 773/7134 completed (loss: 0.01955605484545231, acc: 0.9935732483863831)
[2025-02-13 03:40:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:08][root][INFO] - Training Epoch: 2/2, step 774/7134 completed (loss: 0.013751156628131866, acc: 0.9950166344642639)
[2025-02-13 03:40:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:08][root][INFO] - Training Epoch: 2/2, step 775/7134 completed (loss: 0.01769580878317356, acc: 0.9973368644714355)
[2025-02-13 03:40:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:09][root][INFO] - Training Epoch: 2/2, step 776/7134 completed (loss: 0.008654884062707424, acc: 0.9970414042472839)
[2025-02-13 03:40:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:09][root][INFO] - Training Epoch: 2/2, step 777/7134 completed (loss: 0.027495164424180984, acc: 0.995184600353241)
[2025-02-13 03:40:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:09][root][INFO] - Training Epoch: 2/2, step 778/7134 completed (loss: 0.038811519742012024, acc: 0.9903846383094788)
[2025-02-13 03:40:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:10][root][INFO] - Training Epoch: 2/2, step 779/7134 completed (loss: 0.029760461300611496, acc: 0.9927007555961609)
[2025-02-13 03:40:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:10][root][INFO] - Training Epoch: 2/2, step 780/7134 completed (loss: 0.023131828755140305, acc: 0.9936628937721252)
[2025-02-13 03:40:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:11][root][INFO] - Training Epoch: 2/2, step 781/7134 completed (loss: 0.03835539147257805, acc: 0.9865689873695374)
[2025-02-13 03:40:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:11][root][INFO] - Training Epoch: 2/2, step 782/7134 completed (loss: 0.022418001666665077, acc: 0.9920886158943176)
[2025-02-13 03:40:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:12][root][INFO] - Training Epoch: 2/2, step 783/7134 completed (loss: 0.024342995136976242, acc: 0.993630588054657)
[2025-02-13 03:40:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:12][root][INFO] - Training Epoch: 2/2, step 784/7134 completed (loss: 0.04437519609928131, acc: 0.9890377521514893)
[2025-02-13 03:40:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:13][root][INFO] - Training Epoch: 2/2, step 785/7134 completed (loss: 0.01659722812473774, acc: 0.9966555237770081)
[2025-02-13 03:40:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:13][root][INFO] - Training Epoch: 2/2, step 786/7134 completed (loss: 0.039861053228378296, acc: 0.9925650358200073)
[2025-02-13 03:40:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:14][root][INFO] - Training Epoch: 2/2, step 787/7134 completed (loss: 0.006052633281797171, acc: 1.0)
[2025-02-13 03:40:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:14][root][INFO] - Training Epoch: 2/2, step 788/7134 completed (loss: 0.037400584667921066, acc: 0.9897210001945496)
[2025-02-13 03:40:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:15][root][INFO] - Training Epoch: 2/2, step 789/7134 completed (loss: 0.02221255749464035, acc: 0.9969465732574463)
[2025-02-13 03:40:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:15][root][INFO] - Training Epoch: 2/2, step 790/7134 completed (loss: 0.054220449179410934, acc: 0.9822485446929932)
[2025-02-13 03:40:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:15][root][INFO] - Training Epoch: 2/2, step 791/7134 completed (loss: 0.05207955464720726, acc: 0.9881423115730286)
[2025-02-13 03:40:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:16][root][INFO] - Training Epoch: 2/2, step 792/7134 completed (loss: 0.04213246703147888, acc: 0.9876352548599243)
[2025-02-13 03:40:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:16][root][INFO] - Training Epoch: 2/2, step 793/7134 completed (loss: 0.017977122217416763, acc: 0.9940828680992126)
[2025-02-13 03:40:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:17][root][INFO] - Training Epoch: 2/2, step 794/7134 completed (loss: 0.023712093010544777, acc: 0.9906759858131409)
[2025-02-13 03:40:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:17][root][INFO] - Training Epoch: 2/2, step 795/7134 completed (loss: 0.04311962053179741, acc: 0.9875389337539673)
[2025-02-13 03:40:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:18][root][INFO] - Training Epoch: 2/2, step 796/7134 completed (loss: 0.05393615737557411, acc: 0.9797979593276978)
[2025-02-13 03:40:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:18][root][INFO] - Training Epoch: 2/2, step 797/7134 completed (loss: 0.04952774941921234, acc: 0.9839357137680054)
[2025-02-13 03:40:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:18][root][INFO] - Training Epoch: 2/2, step 798/7134 completed (loss: 0.0189651008695364, acc: 0.9894179701805115)
[2025-02-13 03:40:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:19][root][INFO] - Training Epoch: 2/2, step 799/7134 completed (loss: 0.025506792590022087, acc: 0.9892473220825195)
[2025-02-13 03:40:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:19][root][INFO] - Training Epoch: 2/2, step 800/7134 completed (loss: 0.04705408960580826, acc: 0.9888268113136292)
[2025-02-13 03:40:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:20][root][INFO] - Training Epoch: 2/2, step 801/7134 completed (loss: 0.041734401136636734, acc: 0.984000027179718)
[2025-02-13 03:40:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:20][root][INFO] - Training Epoch: 2/2, step 802/7134 completed (loss: 0.03915025293827057, acc: 0.988727867603302)
[2025-02-13 03:40:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:21][root][INFO] - Training Epoch: 2/2, step 803/7134 completed (loss: 0.01665552891790867, acc: 0.9928571581840515)
[2025-02-13 03:40:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:21][root][INFO] - Training Epoch: 2/2, step 804/7134 completed (loss: 0.020132778212428093, acc: 0.9959266781806946)
[2025-02-13 03:40:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:22][root][INFO] - Training Epoch: 2/2, step 805/7134 completed (loss: 0.050678081810474396, acc: 0.9868913888931274)
[2025-02-13 03:40:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:22][root][INFO] - Training Epoch: 2/2, step 806/7134 completed (loss: 0.013260087929666042, acc: 0.9972066879272461)
[2025-02-13 03:40:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:23][root][INFO] - Training Epoch: 2/2, step 807/7134 completed (loss: 0.032898202538490295, acc: 0.9902676343917847)
[2025-02-13 03:40:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:23][root][INFO] - Training Epoch: 2/2, step 808/7134 completed (loss: 0.01336584147065878, acc: 0.9964072108268738)
[2025-02-13 03:40:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:23][root][INFO] - Training Epoch: 2/2, step 809/7134 completed (loss: 0.007733683101832867, acc: 1.0)
[2025-02-13 03:40:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:24][root][INFO] - Training Epoch: 2/2, step 810/7134 completed (loss: 0.032957274466753006, acc: 0.9919028282165527)
[2025-02-13 03:40:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:24][root][INFO] - Training Epoch: 2/2, step 811/7134 completed (loss: 0.06858255714178085, acc: 0.982425332069397)
[2025-02-13 03:40:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:25][root][INFO] - Training Epoch: 2/2, step 812/7134 completed (loss: 0.025850830599665642, acc: 0.9899117350578308)
[2025-02-13 03:40:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:25][root][INFO] - Training Epoch: 2/2, step 813/7134 completed (loss: 0.02815777063369751, acc: 0.9898403286933899)
[2025-02-13 03:40:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:26][root][INFO] - Training Epoch: 2/2, step 814/7134 completed (loss: 0.03878138214349747, acc: 0.9896774291992188)
[2025-02-13 03:40:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:26][root][INFO] - Training Epoch: 2/2, step 815/7134 completed (loss: 0.033773768693208694, acc: 0.9876352548599243)
[2025-02-13 03:40:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:27][root][INFO] - Training Epoch: 2/2, step 816/7134 completed (loss: 0.04836231470108032, acc: 0.991525411605835)
[2025-02-13 03:40:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:27][root][INFO] - Training Epoch: 2/2, step 817/7134 completed (loss: 0.061239566653966904, acc: 0.9908592104911804)
[2025-02-13 03:40:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:28][root][INFO] - Training Epoch: 2/2, step 818/7134 completed (loss: 0.04701472073793411, acc: 0.9828178882598877)
[2025-02-13 03:40:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:28][root][INFO] - Training Epoch: 2/2, step 819/7134 completed (loss: 0.03823789209127426, acc: 0.9919999837875366)
[2025-02-13 03:40:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:29][root][INFO] - Training Epoch: 2/2, step 820/7134 completed (loss: 0.03476628661155701, acc: 0.9938176274299622)
[2025-02-13 03:40:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:29][root][INFO] - Training Epoch: 2/2, step 821/7134 completed (loss: 0.023483864963054657, acc: 0.9946380853652954)
[2025-02-13 03:40:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:29][root][INFO] - Training Epoch: 2/2, step 822/7134 completed (loss: 0.015564408153295517, acc: 0.9937205910682678)
[2025-02-13 03:40:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:30][root][INFO] - Training Epoch: 2/2, step 823/7134 completed (loss: 0.006785308942198753, acc: 0.9962476491928101)
[2025-02-13 03:40:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:30][root][INFO] - Training Epoch: 2/2, step 824/7134 completed (loss: 0.03663546219468117, acc: 0.9858657121658325)
[2025-02-13 03:40:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:31][root][INFO] - Training Epoch: 2/2, step 825/7134 completed (loss: 0.029131701216101646, acc: 0.992094874382019)
[2025-02-13 03:40:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:31][root][INFO] - Training Epoch: 2/2, step 826/7134 completed (loss: 0.017749233171343803, acc: 0.9945504069328308)
[2025-02-13 03:40:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:32][root][INFO] - Training Epoch: 2/2, step 827/7134 completed (loss: 0.02869487926363945, acc: 0.990813672542572)
[2025-02-13 03:40:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:32][root][INFO] - Training Epoch: 2/2, step 828/7134 completed (loss: 0.02707595005631447, acc: 0.9922239780426025)
[2025-02-13 03:40:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:33][root][INFO] - Training Epoch: 2/2, step 829/7134 completed (loss: 0.031731247901916504, acc: 0.9902912378311157)
[2025-02-13 03:40:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:33][root][INFO] - Training Epoch: 2/2, step 830/7134 completed (loss: 0.02466413751244545, acc: 0.9883720874786377)
[2025-02-13 03:40:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:33][root][INFO] - Training Epoch: 2/2, step 831/7134 completed (loss: 0.012065660208463669, acc: 0.998630166053772)
[2025-02-13 03:40:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:34][root][INFO] - Training Epoch: 2/2, step 832/7134 completed (loss: 0.08162489533424377, acc: 0.979522168636322)
[2025-02-13 03:40:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:34][root][INFO] - Training Epoch: 2/2, step 833/7134 completed (loss: 0.015228917822241783, acc: 0.994397759437561)
[2025-02-13 03:40:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:35][root][INFO] - Training Epoch: 2/2, step 834/7134 completed (loss: 0.01706310734152794, acc: 0.9944055676460266)
[2025-02-13 03:40:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:35][root][INFO] - Training Epoch: 2/2, step 835/7134 completed (loss: 0.02891973778605461, acc: 0.9916083812713623)
[2025-02-13 03:40:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:36][root][INFO] - Training Epoch: 2/2, step 836/7134 completed (loss: 0.025531860068440437, acc: 0.9941995143890381)
[2025-02-13 03:40:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:36][root][INFO] - Training Epoch: 2/2, step 837/7134 completed (loss: 0.03295272961258888, acc: 0.9918256402015686)
[2025-02-13 03:40:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:37][root][INFO] - Training Epoch: 2/2, step 838/7134 completed (loss: 0.01566437818109989, acc: 0.996820330619812)
[2025-02-13 03:40:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:37][root][INFO] - Training Epoch: 2/2, step 839/7134 completed (loss: 0.015287552028894424, acc: 0.9952152967453003)
[2025-02-13 03:40:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:38][root][INFO] - Training Epoch: 2/2, step 840/7134 completed (loss: 0.01649402268230915, acc: 0.9972106218338013)
[2025-02-13 03:40:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:38][root][INFO] - Training Epoch: 2/2, step 841/7134 completed (loss: 0.031165875494480133, acc: 0.9889349937438965)
[2025-02-13 03:40:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:39][root][INFO] - Training Epoch: 2/2, step 842/7134 completed (loss: 0.015231274999678135, acc: 0.9946091771125793)
[2025-02-13 03:40:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:39][root][INFO] - Training Epoch: 2/2, step 843/7134 completed (loss: 0.019786695018410683, acc: 0.9944367408752441)
[2025-02-13 03:40:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:39][root][INFO] - Training Epoch: 2/2, step 844/7134 completed (loss: 0.03145604580640793, acc: 0.9889415502548218)
[2025-02-13 03:40:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:40][root][INFO] - Training Epoch: 2/2, step 845/7134 completed (loss: 0.023984471336007118, acc: 0.9929203391075134)
[2025-02-13 03:40:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:40][root][INFO] - Training Epoch: 2/2, step 846/7134 completed (loss: 0.011249965056777, acc: 0.998516321182251)
[2025-02-13 03:40:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:41][root][INFO] - Training Epoch: 2/2, step 847/7134 completed (loss: 0.008472735993564129, acc: 0.9974392056465149)
[2025-02-13 03:40:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:41][root][INFO] - Training Epoch: 2/2, step 848/7134 completed (loss: 0.03580488637089729, acc: 0.9864029884338379)
[2025-02-13 03:40:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:42][root][INFO] - Training Epoch: 2/2, step 849/7134 completed (loss: 0.01955476403236389, acc: 0.9975816011428833)
[2025-02-13 03:40:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:42][root][INFO] - Training Epoch: 2/2, step 850/7134 completed (loss: 0.008624383248388767, acc: 0.9973154067993164)
[2025-02-13 03:40:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:43][root][INFO] - Training Epoch: 2/2, step 851/7134 completed (loss: 0.02816435694694519, acc: 0.9908015727996826)
[2025-02-13 03:40:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:43][root][INFO] - Training Epoch: 2/2, step 852/7134 completed (loss: 0.009397417306900024, acc: 0.9987789988517761)
[2025-02-13 03:40:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:44][root][INFO] - Training Epoch: 2/2, step 853/7134 completed (loss: 0.01946434937417507, acc: 0.9944933652877808)
[2025-02-13 03:40:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:44][root][INFO] - Training Epoch: 2/2, step 854/7134 completed (loss: 0.024205811321735382, acc: 0.9937810897827148)
[2025-02-13 03:40:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:45][root][INFO] - Training Epoch: 2/2, step 855/7134 completed (loss: 0.033762168139219284, acc: 0.9918032884597778)
[2025-02-13 03:40:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:45][root][INFO] - Training Epoch: 2/2, step 856/7134 completed (loss: 0.011086610145866871, acc: 0.998789370059967)
[2025-02-13 03:40:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:46][root][INFO] - Training Epoch: 2/2, step 857/7134 completed (loss: 0.03423662111163139, acc: 0.9941262602806091)
[2025-02-13 03:40:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:46][root][INFO] - Training Epoch: 2/2, step 858/7134 completed (loss: 0.036792878061532974, acc: 0.9888059496879578)
[2025-02-13 03:40:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:47][root][INFO] - Training Epoch: 2/2, step 859/7134 completed (loss: 0.03149779513478279, acc: 0.9869668483734131)
[2025-02-13 03:40:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:47][root][INFO] - Training Epoch: 2/2, step 860/7134 completed (loss: 0.031604766845703125, acc: 0.992414653301239)
[2025-02-13 03:40:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:47][root][INFO] - Training Epoch: 2/2, step 861/7134 completed (loss: 0.044628456234931946, acc: 0.98591548204422)
[2025-02-13 03:40:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:48][root][INFO] - Training Epoch: 2/2, step 862/7134 completed (loss: 0.02175789885222912, acc: 0.9915764331817627)
[2025-02-13 03:40:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:48][root][INFO] - Training Epoch: 2/2, step 863/7134 completed (loss: 0.02507472224533558, acc: 0.9941792488098145)
[2025-02-13 03:40:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:49][root][INFO] - Training Epoch: 2/2, step 864/7134 completed (loss: 0.06282471865415573, acc: 0.9787610769271851)
[2025-02-13 03:40:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:49][root][INFO] - Training Epoch: 2/2, step 865/7134 completed (loss: 0.03995304927229881, acc: 0.9866179823875427)
[2025-02-13 03:40:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:50][root][INFO] - Training Epoch: 2/2, step 866/7134 completed (loss: 0.04441653937101364, acc: 0.9851852059364319)
[2025-02-13 03:40:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:50][root][INFO] - Training Epoch: 2/2, step 867/7134 completed (loss: 0.041143134236335754, acc: 0.987484335899353)
[2025-02-13 03:40:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:51][root][INFO] - Training Epoch: 2/2, step 868/7134 completed (loss: 0.07178046554327011, acc: 0.9842932224273682)
[2025-02-13 03:40:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:51][root][INFO] - Training Epoch: 2/2, step 869/7134 completed (loss: 0.03666551783680916, acc: 0.9840764403343201)
[2025-02-13 03:40:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:52][root][INFO] - Training Epoch: 2/2, step 870/7134 completed (loss: 0.024649417027831078, acc: 0.9919137358665466)
[2025-02-13 03:40:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:52][root][INFO] - Training Epoch: 2/2, step 871/7134 completed (loss: 0.0616048201918602, acc: 0.9885993599891663)
[2025-02-13 03:40:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:52][root][INFO] - Training Epoch: 2/2, step 872/7134 completed (loss: 0.03773042932152748, acc: 0.9890109896659851)
[2025-02-13 03:40:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:53][root][INFO] - Training Epoch: 2/2, step 873/7134 completed (loss: 0.048108261078596115, acc: 0.98591548204422)
[2025-02-13 03:40:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:53][root][INFO] - Training Epoch: 2/2, step 874/7134 completed (loss: 0.01913250796496868, acc: 0.9938367009162903)
[2025-02-13 03:40:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:54][root][INFO] - Training Epoch: 2/2, step 875/7134 completed (loss: 0.016926299780607224, acc: 0.9970717430114746)
[2025-02-13 03:40:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:54][root][INFO] - Training Epoch: 2/2, step 876/7134 completed (loss: 0.025486059486865997, acc: 0.9897660613059998)
[2025-02-13 03:40:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:55][root][INFO] - Training Epoch: 2/2, step 877/7134 completed (loss: 0.06270862370729446, acc: 0.9827814698219299)
[2025-02-13 03:40:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:55][root][INFO] - Training Epoch: 2/2, step 878/7134 completed (loss: 0.030552633106708527, acc: 0.9907161593437195)
[2025-02-13 03:40:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:56][root][INFO] - Training Epoch: 2/2, step 879/7134 completed (loss: 0.05070873722434044, acc: 0.9883551597595215)
[2025-02-13 03:40:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:56][root][INFO] - Training Epoch: 2/2, step 880/7134 completed (loss: 0.010258288122713566, acc: 0.9970887899398804)
[2025-02-13 03:40:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:57][root][INFO] - Training Epoch: 2/2, step 881/7134 completed (loss: 0.010278182104229927, acc: 0.9979959726333618)
[2025-02-13 03:40:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:57][root][INFO] - Training Epoch: 2/2, step 882/7134 completed (loss: 0.02915862947702408, acc: 0.9928057789802551)
[2025-02-13 03:40:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:57][root][INFO] - Training Epoch: 2/2, step 883/7134 completed (loss: 0.05741475895047188, acc: 0.9831804037094116)
[2025-02-13 03:40:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:58][root][INFO] - Training Epoch: 2/2, step 884/7134 completed (loss: 0.025584662333130836, acc: 0.989276111125946)
[2025-02-13 03:40:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:58][root][INFO] - Training Epoch: 2/2, step 885/7134 completed (loss: 0.006637324579060078, acc: 0.9971910119056702)
[2025-02-13 03:40:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:59][root][INFO] - Training Epoch: 2/2, step 886/7134 completed (loss: 0.045558467507362366, acc: 0.9902533888816833)
[2025-02-13 03:40:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:40:59][root][INFO] - Training Epoch: 2/2, step 887/7134 completed (loss: 0.028514908626675606, acc: 0.9964601993560791)
[2025-02-13 03:40:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:00][root][INFO] - Training Epoch: 2/2, step 888/7134 completed (loss: 0.04988357052206993, acc: 0.9863429665565491)
[2025-02-13 03:41:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:00][root][INFO] - Training Epoch: 2/2, step 889/7134 completed (loss: 0.009049763903021812, acc: 0.9977777600288391)
[2025-02-13 03:41:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:00][root][INFO] - Training Epoch: 2/2, step 890/7134 completed (loss: 0.04588857293128967, acc: 0.9898132681846619)
[2025-02-13 03:41:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:01][root][INFO] - Training Epoch: 2/2, step 891/7134 completed (loss: 0.047170355916023254, acc: 0.9874826073646545)
[2025-02-13 03:41:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:01][root][INFO] - Training Epoch: 2/2, step 892/7134 completed (loss: 0.045021045953035355, acc: 0.9883720874786377)
[2025-02-13 03:41:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:02][root][INFO] - Training Epoch: 2/2, step 893/7134 completed (loss: 0.03221478685736656, acc: 0.9922600388526917)
[2025-02-13 03:41:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:02][root][INFO] - Training Epoch: 2/2, step 894/7134 completed (loss: 0.024769950658082962, acc: 0.9916201233863831)
[2025-02-13 03:41:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:03][root][INFO] - Training Epoch: 2/2, step 895/7134 completed (loss: 0.009011926129460335, acc: 0.9965811967849731)
[2025-02-13 03:41:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:03][root][INFO] - Training Epoch: 2/2, step 896/7134 completed (loss: 0.02179328165948391, acc: 0.9959999918937683)
[2025-02-13 03:41:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:04][root][INFO] - Training Epoch: 2/2, step 897/7134 completed (loss: 0.014871385879814625, acc: 0.9944289922714233)
[2025-02-13 03:41:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:04][root][INFO] - Training Epoch: 2/2, step 898/7134 completed (loss: 0.007397002540528774, acc: 0.9987546801567078)
[2025-02-13 03:41:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:05][root][INFO] - Training Epoch: 2/2, step 899/7134 completed (loss: 0.01598602905869484, acc: 0.9950248599052429)
[2025-02-13 03:41:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:05][root][INFO] - Training Epoch: 2/2, step 900/7134 completed (loss: 0.0160959605127573, acc: 0.9943740963935852)
[2025-02-13 03:41:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:06][root][INFO] - Training Epoch: 2/2, step 901/7134 completed (loss: 0.012958812527358532, acc: 0.9959999918937683)
[2025-02-13 03:41:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:06][root][INFO] - Training Epoch: 2/2, step 902/7134 completed (loss: 0.018657615408301353, acc: 0.9942611455917358)
[2025-02-13 03:41:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:06][root][INFO] - Training Epoch: 2/2, step 903/7134 completed (loss: 0.00383821246214211, acc: 1.0)
[2025-02-13 03:41:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:07][root][INFO] - Training Epoch: 2/2, step 904/7134 completed (loss: 0.010609428398311138, acc: 0.9973958134651184)
[2025-02-13 03:41:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:07][root][INFO] - Training Epoch: 2/2, step 905/7134 completed (loss: 0.012081717140972614, acc: 0.9959568977355957)
[2025-02-13 03:41:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:08][root][INFO] - Training Epoch: 2/2, step 906/7134 completed (loss: 0.008175789378583431, acc: 0.9986594915390015)
[2025-02-13 03:41:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:08][root][INFO] - Training Epoch: 2/2, step 907/7134 completed (loss: 0.008148379623889923, acc: 0.9986842274665833)
[2025-02-13 03:41:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:09][root][INFO] - Training Epoch: 2/2, step 908/7134 completed (loss: 0.012190017849206924, acc: 0.9973992109298706)
[2025-02-13 03:41:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:09][root][INFO] - Training Epoch: 2/2, step 909/7134 completed (loss: 0.032182835042476654, acc: 0.9924924969673157)
[2025-02-13 03:41:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:10][root][INFO] - Training Epoch: 2/2, step 910/7134 completed (loss: 0.03102700039744377, acc: 0.9906542301177979)
[2025-02-13 03:41:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:10][root][INFO] - Training Epoch: 2/2, step 911/7134 completed (loss: 0.014554928988218307, acc: 0.9944055676460266)
[2025-02-13 03:41:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:11][root][INFO] - Training Epoch: 2/2, step 912/7134 completed (loss: 0.039383526891469955, acc: 0.9886506795883179)
[2025-02-13 03:41:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:11][root][INFO] - Training Epoch: 2/2, step 913/7134 completed (loss: 0.013571064919233322, acc: 0.9949748516082764)
[2025-02-13 03:41:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:12][root][INFO] - Training Epoch: 2/2, step 914/7134 completed (loss: 0.02564910054206848, acc: 0.9897959232330322)
[2025-02-13 03:41:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:12][root][INFO] - Training Epoch: 2/2, step 915/7134 completed (loss: 0.05016464740037918, acc: 0.9886731505393982)
[2025-02-13 03:41:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:13][root][INFO] - Training Epoch: 2/2, step 916/7134 completed (loss: 0.012066368013620377, acc: 0.9930192232131958)
[2025-02-13 03:41:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:13][root][INFO] - Training Epoch: 2/2, step 917/7134 completed (loss: 0.04030589759349823, acc: 0.9894419312477112)
[2025-02-13 03:41:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:13][root][INFO] - Training Epoch: 2/2, step 918/7134 completed (loss: 0.009555132128298283, acc: 0.9969372153282166)
[2025-02-13 03:41:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:14][root][INFO] - Training Epoch: 2/2, step 919/7134 completed (loss: 0.009801789186894894, acc: 0.9971140027046204)
[2025-02-13 03:41:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:14][root][INFO] - Training Epoch: 2/2, step 920/7134 completed (loss: 0.0201441440731287, acc: 0.9929971694946289)
[2025-02-13 03:41:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:15][root][INFO] - Training Epoch: 2/2, step 921/7134 completed (loss: 0.008930006064474583, acc: 0.9972899556159973)
[2025-02-13 03:41:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:15][root][INFO] - Training Epoch: 2/2, step 922/7134 completed (loss: 0.017460571601986885, acc: 0.9934425950050354)
[2025-02-13 03:41:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:16][root][INFO] - Training Epoch: 2/2, step 923/7134 completed (loss: 0.04013101011514664, acc: 0.9920634627342224)
[2025-02-13 03:41:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:16][root][INFO] - Training Epoch: 2/2, step 924/7134 completed (loss: 0.004022734705358744, acc: 0.998792290687561)
[2025-02-13 03:41:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:17][root][INFO] - Training Epoch: 2/2, step 925/7134 completed (loss: 0.022769249975681305, acc: 0.9941262602806091)
[2025-02-13 03:41:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:17][root][INFO] - Training Epoch: 2/2, step 926/7134 completed (loss: 0.03145778551697731, acc: 0.995006263256073)
[2025-02-13 03:41:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:18][root][INFO] - Training Epoch: 2/2, step 927/7134 completed (loss: 0.05474676936864853, acc: 0.991830050945282)
[2025-02-13 03:41:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:18][root][INFO] - Training Epoch: 2/2, step 928/7134 completed (loss: 0.011743170209228992, acc: 0.9940828680992126)
[2025-02-13 03:41:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:19][root][INFO] - Training Epoch: 2/2, step 929/7134 completed (loss: 0.018238799646496773, acc: 0.9936808943748474)
[2025-02-13 03:41:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:19][root][INFO] - Training Epoch: 2/2, step 930/7134 completed (loss: 0.032554082572460175, acc: 0.9899193644523621)
[2025-02-13 03:41:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:19][root][INFO] - Training Epoch: 2/2, step 931/7134 completed (loss: 0.05247393622994423, acc: 0.9841269850730896)
[2025-02-13 03:41:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:20][root][INFO] - Training Epoch: 2/2, step 932/7134 completed (loss: 0.02822086773812771, acc: 0.9914039969444275)
[2025-02-13 03:41:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:20][root][INFO] - Training Epoch: 2/2, step 933/7134 completed (loss: 0.024019526317715645, acc: 0.9971791505813599)
[2025-02-13 03:41:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:21][root][INFO] - Training Epoch: 2/2, step 934/7134 completed (loss: 0.008800960145890713, acc: 0.9973822236061096)
[2025-02-13 03:41:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:21][root][INFO] - Training Epoch: 2/2, step 935/7134 completed (loss: 0.014278357848525047, acc: 0.994991660118103)
[2025-02-13 03:41:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:22][root][INFO] - Training Epoch: 2/2, step 936/7134 completed (loss: 0.028220349922776222, acc: 0.9929701089859009)
[2025-02-13 03:41:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:22][root][INFO] - Training Epoch: 2/2, step 937/7134 completed (loss: 0.021179841831326485, acc: 0.9982394576072693)
[2025-02-13 03:41:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:22][root][INFO] - Training Epoch: 2/2, step 938/7134 completed (loss: 0.010807228274643421, acc: 0.9957020282745361)
[2025-02-13 03:41:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:23][root][INFO] - Training Epoch: 2/2, step 939/7134 completed (loss: 0.029973063617944717, acc: 0.9875389337539673)
[2025-02-13 03:41:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:23][root][INFO] - Training Epoch: 2/2, step 940/7134 completed (loss: 0.029052957892417908, acc: 0.9885550737380981)
[2025-02-13 03:41:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:24][root][INFO] - Training Epoch: 2/2, step 941/7134 completed (loss: 0.018115535378456116, acc: 0.9931034445762634)
[2025-02-13 03:41:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:24][root][INFO] - Training Epoch: 2/2, step 942/7134 completed (loss: 0.03255458548665047, acc: 0.9927431344985962)
[2025-02-13 03:41:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:25][root][INFO] - Training Epoch: 2/2, step 943/7134 completed (loss: 0.011532100848853588, acc: 0.9969558715820312)
[2025-02-13 03:41:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:25][root][INFO] - Training Epoch: 2/2, step 944/7134 completed (loss: 0.024639323353767395, acc: 0.9925037622451782)
[2025-02-13 03:41:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:26][root][INFO] - Training Epoch: 2/2, step 945/7134 completed (loss: 0.025710755959153175, acc: 0.9958041906356812)
[2025-02-13 03:41:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:26][root][INFO] - Training Epoch: 2/2, step 946/7134 completed (loss: 0.004321589600294828, acc: 1.0)
[2025-02-13 03:41:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:27][root][INFO] - Training Epoch: 2/2, step 947/7134 completed (loss: 0.02499883621931076, acc: 0.9965576529502869)
[2025-02-13 03:41:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:27][root][INFO] - Training Epoch: 2/2, step 948/7134 completed (loss: 0.02838711254298687, acc: 0.9902642369270325)
[2025-02-13 03:41:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:28][root][INFO] - Training Epoch: 2/2, step 949/7134 completed (loss: 0.021303316578269005, acc: 0.9929873943328857)
[2025-02-13 03:41:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:28][root][INFO] - Training Epoch: 2/2, step 950/7134 completed (loss: 0.016126912087202072, acc: 0.9938271641731262)
[2025-02-13 03:41:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:28][root][INFO] - Training Epoch: 2/2, step 951/7134 completed (loss: 0.014640793204307556, acc: 0.9947183132171631)
[2025-02-13 03:41:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:29][root][INFO] - Training Epoch: 2/2, step 952/7134 completed (loss: 0.03636260703206062, acc: 0.9883527159690857)
[2025-02-13 03:41:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:29][root][INFO] - Training Epoch: 2/2, step 953/7134 completed (loss: 0.05520658940076828, acc: 0.9852761030197144)
[2025-02-13 03:41:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:30][root][INFO] - Training Epoch: 2/2, step 954/7134 completed (loss: 0.03538968414068222, acc: 0.9911894202232361)
[2025-02-13 03:41:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:30][root][INFO] - Training Epoch: 2/2, step 955/7134 completed (loss: 0.017384350299835205, acc: 0.9946452379226685)
[2025-02-13 03:41:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:31][root][INFO] - Training Epoch: 2/2, step 956/7134 completed (loss: 0.01844627968966961, acc: 0.992682933807373)
[2025-02-13 03:41:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:31][root][INFO] - Training Epoch: 2/2, step 957/7134 completed (loss: 0.01699451170861721, acc: 0.9943116903305054)
[2025-02-13 03:41:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:32][root][INFO] - Training Epoch: 2/2, step 958/7134 completed (loss: 0.02663002721965313, acc: 0.9926470518112183)
[2025-02-13 03:41:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:32][root][INFO] - Training Epoch: 2/2, step 959/7134 completed (loss: 0.058398593217134476, acc: 0.9826202988624573)
[2025-02-13 03:41:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:33][root][INFO] - Training Epoch: 2/2, step 960/7134 completed (loss: 0.02930884249508381, acc: 0.9886578321456909)
[2025-02-13 03:41:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:33][root][INFO] - Training Epoch: 2/2, step 961/7134 completed (loss: 0.03729911893606186, acc: 0.9878706336021423)
[2025-02-13 03:41:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:33][root][INFO] - Training Epoch: 2/2, step 962/7134 completed (loss: 0.0774453803896904, acc: 0.9834482669830322)
[2025-02-13 03:41:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:34][root][INFO] - Training Epoch: 2/2, step 963/7134 completed (loss: 0.0522293820977211, acc: 0.980182945728302)
[2025-02-13 03:41:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:34][root][INFO] - Training Epoch: 2/2, step 964/7134 completed (loss: 0.11265598237514496, acc: 0.9730135202407837)
[2025-02-13 03:41:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:35][root][INFO] - Training Epoch: 2/2, step 965/7134 completed (loss: 0.06374455988407135, acc: 0.9799138903617859)
[2025-02-13 03:41:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:35][root][INFO] - Training Epoch: 2/2, step 966/7134 completed (loss: 0.03328163921833038, acc: 0.9934425950050354)
[2025-02-13 03:41:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:36][root][INFO] - Training Epoch: 2/2, step 967/7134 completed (loss: 0.030035056173801422, acc: 0.9863574504852295)
[2025-02-13 03:41:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:36][root][INFO] - Training Epoch: 2/2, step 968/7134 completed (loss: 0.06389809399843216, acc: 0.9721670150756836)
[2025-02-13 03:41:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:37][root][INFO] - Training Epoch: 2/2, step 969/7134 completed (loss: 0.02412279322743416, acc: 0.9909502267837524)
[2025-02-13 03:41:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:37][root][INFO] - Training Epoch: 2/2, step 970/7134 completed (loss: 0.04706175997853279, acc: 0.9866412281990051)
[2025-02-13 03:41:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:38][root][INFO] - Training Epoch: 2/2, step 971/7134 completed (loss: 0.02699497900903225, acc: 0.9901840686798096)
[2025-02-13 03:41:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:38][root][INFO] - Training Epoch: 2/2, step 972/7134 completed (loss: 0.03408922627568245, acc: 0.9887920022010803)
[2025-02-13 03:41:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:38][root][INFO] - Training Epoch: 2/2, step 973/7134 completed (loss: 0.08632548153400421, acc: 0.9769230484962463)
[2025-02-13 03:41:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:39][root][INFO] - Training Epoch: 2/2, step 974/7134 completed (loss: 0.04067760333418846, acc: 0.9908779859542847)
[2025-02-13 03:41:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:39][root][INFO] - Training Epoch: 2/2, step 975/7134 completed (loss: 0.06387209892272949, acc: 0.9844497442245483)
[2025-02-13 03:41:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:40][root][INFO] - Training Epoch: 2/2, step 976/7134 completed (loss: 0.04609541967511177, acc: 0.9875389337539673)
[2025-02-13 03:41:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:40][root][INFO] - Training Epoch: 2/2, step 977/7134 completed (loss: 0.0437450148165226, acc: 0.9886363744735718)
[2025-02-13 03:41:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:41][root][INFO] - Training Epoch: 2/2, step 978/7134 completed (loss: 0.019151002168655396, acc: 0.9939831495285034)
[2025-02-13 03:41:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:41][root][INFO] - Training Epoch: 2/2, step 979/7134 completed (loss: 0.02430633082985878, acc: 0.9893048405647278)
[2025-02-13 03:41:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:42][root][INFO] - Training Epoch: 2/2, step 980/7134 completed (loss: 0.03992808610200882, acc: 0.9887499809265137)
[2025-02-13 03:41:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:42][root][INFO] - Training Epoch: 2/2, step 981/7134 completed (loss: 0.08931326121091843, acc: 0.9839228391647339)
[2025-02-13 03:41:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:43][root][INFO] - Training Epoch: 2/2, step 982/7134 completed (loss: 0.014673649333417416, acc: 0.9957864880561829)
[2025-02-13 03:41:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:43][root][INFO] - Training Epoch: 2/2, step 983/7134 completed (loss: 0.03495066240429878, acc: 0.9914383292198181)
[2025-02-13 03:41:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:44][root][INFO] - Training Epoch: 2/2, step 984/7134 completed (loss: 0.03464405983686447, acc: 0.9944853186607361)
[2025-02-13 03:41:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:44][root][INFO] - Training Epoch: 2/2, step 985/7134 completed (loss: 0.02800537832081318, acc: 0.9923469424247742)
[2025-02-13 03:41:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:45][root][INFO] - Training Epoch: 2/2, step 986/7134 completed (loss: 0.01374521292746067, acc: 0.9957507252693176)
[2025-02-13 03:41:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:45][root][INFO] - Training Epoch: 2/2, step 987/7134 completed (loss: 0.0235415231436491, acc: 0.9951768517494202)
[2025-02-13 03:41:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:45][root][INFO] - Training Epoch: 2/2, step 988/7134 completed (loss: 0.0810050293803215, acc: 0.9805825352668762)
[2025-02-13 03:41:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:46][root][INFO] - Training Epoch: 2/2, step 989/7134 completed (loss: 0.048350073397159576, acc: 0.9888734221458435)
[2025-02-13 03:41:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:46][root][INFO] - Training Epoch: 2/2, step 990/7134 completed (loss: 0.026281485334038734, acc: 0.991150438785553)
[2025-02-13 03:41:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:47][root][INFO] - Training Epoch: 2/2, step 991/7134 completed (loss: 0.05552232265472412, acc: 0.9874804615974426)
[2025-02-13 03:41:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:47][root][INFO] - Training Epoch: 2/2, step 992/7134 completed (loss: 0.035232800990343094, acc: 0.9907894730567932)
[2025-02-13 03:41:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:48][root][INFO] - Training Epoch: 2/2, step 993/7134 completed (loss: 0.030464062467217445, acc: 0.9869067072868347)
[2025-02-13 03:41:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:48][root][INFO] - Training Epoch: 2/2, step 994/7134 completed (loss: 0.03513338044285774, acc: 0.9932432174682617)
[2025-02-13 03:41:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:49][root][INFO] - Training Epoch: 2/2, step 995/7134 completed (loss: 0.025299448519945145, acc: 0.9891975522041321)
[2025-02-13 03:41:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:49][root][INFO] - Training Epoch: 2/2, step 996/7134 completed (loss: 0.03163519501686096, acc: 0.9846368432044983)
[2025-02-13 03:41:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:49][root][INFO] - Training Epoch: 2/2, step 997/7134 completed (loss: 0.02039039134979248, acc: 0.9928366541862488)
[2025-02-13 03:41:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:50][root][INFO] - Training Epoch: 2/2, step 998/7134 completed (loss: 0.027484294027090073, acc: 0.9901408553123474)
[2025-02-13 03:41:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:50][root][INFO] - Training Epoch: 2/2, step 999/7134 completed (loss: 0.01185398455709219, acc: 0.9928315281867981)
[2025-02-13 03:41:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:51][root][INFO] - Training Epoch: 2/2, step 1000/7134 completed (loss: 0.025621751323342323, acc: 0.9948630332946777)
[2025-02-13 03:41:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:51][root][INFO] - Training Epoch: 2/2, step 1001/7134 completed (loss: 0.03703150525689125, acc: 0.9939393997192383)
[2025-02-13 03:41:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:52][root][INFO] - Training Epoch: 2/2, step 1002/7134 completed (loss: 0.029088832437992096, acc: 0.9942938685417175)
[2025-02-13 03:41:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:52][root][INFO] - Training Epoch: 2/2, step 1003/7134 completed (loss: 0.008957520127296448, acc: 0.9985141158103943)
[2025-02-13 03:41:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:53][root][INFO] - Training Epoch: 2/2, step 1004/7134 completed (loss: 0.038286518305540085, acc: 0.9938144087791443)
[2025-02-13 03:41:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:53][root][INFO] - Training Epoch: 2/2, step 1005/7134 completed (loss: 0.06962071359157562, acc: 0.9884105920791626)
[2025-02-13 03:41:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:54][root][INFO] - Training Epoch: 2/2, step 1006/7134 completed (loss: 0.038677290081977844, acc: 0.9930362105369568)
[2025-02-13 03:41:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:54][root][INFO] - Training Epoch: 2/2, step 1007/7134 completed (loss: 0.08103350549936295, acc: 0.9795918464660645)
[2025-02-13 03:41:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:54][root][INFO] - Training Epoch: 2/2, step 1008/7134 completed (loss: 0.03799862042069435, acc: 0.9872340559959412)
[2025-02-13 03:41:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:55][root][INFO] - Training Epoch: 2/2, step 1009/7134 completed (loss: 0.030868086963891983, acc: 0.9946714043617249)
[2025-02-13 03:41:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:55][root][INFO] - Training Epoch: 2/2, step 1010/7134 completed (loss: 0.029328089207410812, acc: 0.992094874382019)
[2025-02-13 03:41:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:56][root][INFO] - Training Epoch: 2/2, step 1011/7134 completed (loss: 0.019977424293756485, acc: 0.9930843710899353)
[2025-02-13 03:41:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:56][root][INFO] - Training Epoch: 2/2, step 1012/7134 completed (loss: 0.0362115316092968, acc: 0.9915013909339905)
[2025-02-13 03:41:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:57][root][INFO] - Training Epoch: 2/2, step 1013/7134 completed (loss: 0.04605871066451073, acc: 0.9813753366470337)
[2025-02-13 03:41:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:57][root][INFO] - Training Epoch: 2/2, step 1014/7134 completed (loss: 0.023613369092345238, acc: 0.9950739145278931)
[2025-02-13 03:41:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:58][root][INFO] - Training Epoch: 2/2, step 1015/7134 completed (loss: 0.03705611452460289, acc: 0.9894578456878662)
[2025-02-13 03:41:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:58][root][INFO] - Training Epoch: 2/2, step 1016/7134 completed (loss: 0.039850931614637375, acc: 0.9923518300056458)
[2025-02-13 03:41:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:59][root][INFO] - Training Epoch: 2/2, step 1017/7134 completed (loss: 0.002563724759966135, acc: 1.0)
[2025-02-13 03:41:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:41:59][root][INFO] - Training Epoch: 2/2, step 1018/7134 completed (loss: 0.03737342730164528, acc: 0.9956076145172119)
[2025-02-13 03:41:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:00][root][INFO] - Training Epoch: 2/2, step 1019/7134 completed (loss: 0.012163829058408737, acc: 0.9967948794364929)
[2025-02-13 03:42:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:00][root][INFO] - Training Epoch: 2/2, step 1020/7134 completed (loss: 0.004646247252821922, acc: 1.0)
[2025-02-13 03:42:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:00][root][INFO] - Training Epoch: 2/2, step 1021/7134 completed (loss: 0.007569441571831703, acc: 0.9968152642250061)
[2025-02-13 03:42:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:01][root][INFO] - Training Epoch: 2/2, step 1022/7134 completed (loss: 0.017090564593672752, acc: 0.9934123754501343)
[2025-02-13 03:42:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:01][root][INFO] - Training Epoch: 2/2, step 1023/7134 completed (loss: 0.03404432162642479, acc: 0.993565022945404)
[2025-02-13 03:42:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:02][root][INFO] - Training Epoch: 2/2, step 1024/7134 completed (loss: 0.031636081635951996, acc: 0.9928186535835266)
[2025-02-13 03:42:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:02][root][INFO] - Training Epoch: 2/2, step 1025/7134 completed (loss: 0.010025909170508385, acc: 0.9982206225395203)
[2025-02-13 03:42:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:03][root][INFO] - Training Epoch: 2/2, step 1026/7134 completed (loss: 0.035827551037073135, acc: 0.9915789365768433)
[2025-02-13 03:42:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:03][root][INFO] - Training Epoch: 2/2, step 1027/7134 completed (loss: 0.016528910025954247, acc: 0.9959404468536377)
[2025-02-13 03:42:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:04][root][INFO] - Training Epoch: 2/2, step 1028/7134 completed (loss: 0.016119053587317467, acc: 0.994020938873291)
[2025-02-13 03:42:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:04][root][INFO] - Training Epoch: 2/2, step 1029/7134 completed (loss: 0.012811277061700821, acc: 0.9985141158103943)
[2025-02-13 03:42:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:05][root][INFO] - Training Epoch: 2/2, step 1030/7134 completed (loss: 0.010426551103591919, acc: 0.9959568977355957)
[2025-02-13 03:42:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:05][root][INFO] - Training Epoch: 2/2, step 1031/7134 completed (loss: 0.03030778095126152, acc: 0.9908536672592163)
[2025-02-13 03:42:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:06][root][INFO] - Training Epoch: 2/2, step 1032/7134 completed (loss: 0.020711053162813187, acc: 0.9930875301361084)
[2025-02-13 03:42:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:06][root][INFO] - Training Epoch: 2/2, step 1033/7134 completed (loss: 0.01628616265952587, acc: 0.9971305727958679)
[2025-02-13 03:42:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:06][root][INFO] - Training Epoch: 2/2, step 1034/7134 completed (loss: 0.03699206933379173, acc: 0.987034022808075)
[2025-02-13 03:42:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:07][root][INFO] - Training Epoch: 2/2, step 1035/7134 completed (loss: 0.13043616712093353, acc: 0.9596491456031799)
[2025-02-13 03:42:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:07][root][INFO] - Training Epoch: 2/2, step 1036/7134 completed (loss: 0.1250002086162567, acc: 0.9711111187934875)
[2025-02-13 03:42:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:08][root][INFO] - Training Epoch: 2/2, step 1037/7134 completed (loss: 0.009823733009397984, acc: 0.9988095164299011)
[2025-02-13 03:42:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:08][root][INFO] - Training Epoch: 2/2, step 1038/7134 completed (loss: 0.009527988731861115, acc: 1.0)
[2025-02-13 03:42:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:09][root][INFO] - Training Epoch: 2/2, step 1039/7134 completed (loss: 0.013752802275121212, acc: 0.9979296326637268)
[2025-02-13 03:42:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:09][root][INFO] - Training Epoch: 2/2, step 1040/7134 completed (loss: 0.004721500910818577, acc: 1.0)
[2025-02-13 03:42:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:10][root][INFO] - Training Epoch: 2/2, step 1041/7134 completed (loss: 0.018395928665995598, acc: 0.9968847632408142)
[2025-02-13 03:42:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:10][root][INFO] - Training Epoch: 2/2, step 1042/7134 completed (loss: 0.025363141670823097, acc: 0.9884615540504456)
[2025-02-13 03:42:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:11][root][INFO] - Training Epoch: 2/2, step 1043/7134 completed (loss: 0.02036229707300663, acc: 0.9927797913551331)
[2025-02-13 03:42:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:11][root][INFO] - Training Epoch: 2/2, step 1044/7134 completed (loss: 0.03306787833571434, acc: 0.9917808175086975)
[2025-02-13 03:42:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:11][root][INFO] - Training Epoch: 2/2, step 1045/7134 completed (loss: 0.08262921869754791, acc: 0.9806138873100281)
[2025-02-13 03:42:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:12][root][INFO] - Training Epoch: 2/2, step 1046/7134 completed (loss: 0.029811665415763855, acc: 0.9903475046157837)
[2025-02-13 03:42:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:12][root][INFO] - Training Epoch: 2/2, step 1047/7134 completed (loss: 0.0553717203438282, acc: 0.9792592525482178)
[2025-02-13 03:42:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:13][root][INFO] - Training Epoch: 2/2, step 1048/7134 completed (loss: 0.011248267255723476, acc: 0.9956011772155762)
[2025-02-13 03:42:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:13][root][INFO] - Training Epoch: 2/2, step 1049/7134 completed (loss: 0.02845614217221737, acc: 0.9868228435516357)
[2025-02-13 03:42:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:14][root][INFO] - Training Epoch: 2/2, step 1050/7134 completed (loss: 0.03256353363394737, acc: 0.9877049326896667)
[2025-02-13 03:42:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:14][root][INFO] - Training Epoch: 2/2, step 1051/7134 completed (loss: 0.024626076221466064, acc: 0.9890410900115967)
[2025-02-13 03:42:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:15][root][INFO] - Training Epoch: 2/2, step 1052/7134 completed (loss: 0.04679842293262482, acc: 0.9838449358940125)
[2025-02-13 03:42:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:15][root][INFO] - Training Epoch: 2/2, step 1053/7134 completed (loss: 0.01482636108994484, acc: 0.9943661689758301)
[2025-02-13 03:42:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:16][root][INFO] - Training Epoch: 2/2, step 1054/7134 completed (loss: 0.022370392456650734, acc: 0.9933775067329407)
[2025-02-13 03:42:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:16][root][INFO] - Training Epoch: 2/2, step 1055/7134 completed (loss: 0.05350612476468086, acc: 0.9778270721435547)
[2025-02-13 03:42:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:17][root][INFO] - Training Epoch: 2/2, step 1056/7134 completed (loss: 0.05343112349510193, acc: 0.9832496047019958)
[2025-02-13 03:42:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:17][root][INFO] - Training Epoch: 2/2, step 1057/7134 completed (loss: 0.029372576624155045, acc: 0.9927745461463928)
[2025-02-13 03:42:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:18][root][INFO] - Training Epoch: 2/2, step 1058/7134 completed (loss: 0.019666438922286034, acc: 0.9956521987915039)
[2025-02-13 03:42:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:18][root][INFO] - Training Epoch: 2/2, step 1059/7134 completed (loss: 0.02737455815076828, acc: 0.9909677505493164)
[2025-02-13 03:42:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:18][root][INFO] - Training Epoch: 2/2, step 1060/7134 completed (loss: 0.025693947449326515, acc: 0.9936908483505249)
[2025-02-13 03:42:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:19][root][INFO] - Training Epoch: 2/2, step 1061/7134 completed (loss: 0.01944779045879841, acc: 0.9912790656089783)
[2025-02-13 03:42:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:19][root][INFO] - Training Epoch: 2/2, step 1062/7134 completed (loss: 0.02354271523654461, acc: 0.989830493927002)
[2025-02-13 03:42:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:20][root][INFO] - Training Epoch: 2/2, step 1063/7134 completed (loss: 0.009082860313355923, acc: 1.0)
[2025-02-13 03:42:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:20][root][INFO] - Training Epoch: 2/2, step 1064/7134 completed (loss: 0.0330827422440052, acc: 0.9876543283462524)
[2025-02-13 03:42:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:21][root][INFO] - Training Epoch: 2/2, step 1065/7134 completed (loss: 0.017233623191714287, acc: 0.9945799708366394)
[2025-02-13 03:42:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:21][root][INFO] - Training Epoch: 2/2, step 1066/7134 completed (loss: 0.05648214742541313, acc: 0.9798657894134521)
[2025-02-13 03:42:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:22][root][INFO] - Training Epoch: 2/2, step 1067/7134 completed (loss: 0.013744411990046501, acc: 0.9969183206558228)
[2025-02-13 03:42:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:22][root][INFO] - Training Epoch: 2/2, step 1068/7134 completed (loss: 0.02635122276842594, acc: 0.9887429475784302)
[2025-02-13 03:42:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:22][root][INFO] - Training Epoch: 2/2, step 1069/7134 completed (loss: 0.02240345999598503, acc: 0.9933221936225891)
[2025-02-13 03:42:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:23][root][INFO] - Training Epoch: 2/2, step 1070/7134 completed (loss: 0.01796935684978962, acc: 0.9917898178100586)
[2025-02-13 03:42:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:23][root][INFO] - Training Epoch: 2/2, step 1071/7134 completed (loss: 0.01267005130648613, acc: 0.9973226189613342)
[2025-02-13 03:42:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:24][root][INFO] - Training Epoch: 2/2, step 1072/7134 completed (loss: 0.0019791158847510815, acc: 1.0)
[2025-02-13 03:42:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:24][root][INFO] - Training Epoch: 2/2, step 1073/7134 completed (loss: 0.007074815221130848, acc: 0.9971181750297546)
[2025-02-13 03:42:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:25][root][INFO] - Training Epoch: 2/2, step 1074/7134 completed (loss: 0.04714105650782585, acc: 0.9827288389205933)
[2025-02-13 03:42:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:25][root][INFO] - Training Epoch: 2/2, step 1075/7134 completed (loss: 0.06890378892421722, acc: 0.979742169380188)
[2025-02-13 03:42:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:26][root][INFO] - Training Epoch: 2/2, step 1076/7134 completed (loss: 0.04497373476624489, acc: 0.9853723645210266)
[2025-02-13 03:42:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:26][root][INFO] - Training Epoch: 2/2, step 1077/7134 completed (loss: 0.044970422983169556, acc: 0.9819819927215576)
[2025-02-13 03:42:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:27][root][INFO] - Training Epoch: 2/2, step 1078/7134 completed (loss: 0.05805980786681175, acc: 0.987075924873352)
[2025-02-13 03:42:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:27][root][INFO] - Training Epoch: 2/2, step 1079/7134 completed (loss: 0.019641879945993423, acc: 0.9919261932373047)
[2025-02-13 03:42:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:27][root][INFO] - Training Epoch: 2/2, step 1080/7134 completed (loss: 0.037368759512901306, acc: 0.9943609237670898)
[2025-02-13 03:42:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:28][root][INFO] - Training Epoch: 2/2, step 1081/7134 completed (loss: 0.014794119633734226, acc: 0.9957924485206604)
[2025-02-13 03:42:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:28][root][INFO] - Training Epoch: 2/2, step 1082/7134 completed (loss: 0.00607292028144002, acc: 0.9979444742202759)
[2025-02-13 03:42:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:29][root][INFO] - Training Epoch: 2/2, step 1083/7134 completed (loss: 0.009577527642250061, acc: 0.9977400898933411)
[2025-02-13 03:42:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:29][root][INFO] - Training Epoch: 2/2, step 1084/7134 completed (loss: 0.012099923565983772, acc: 0.9962825179100037)
[2025-02-13 03:42:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:30][root][INFO] - Training Epoch: 2/2, step 1085/7134 completed (loss: 0.012075900100171566, acc: 0.9959839582443237)
[2025-02-13 03:42:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:30][root][INFO] - Training Epoch: 2/2, step 1086/7134 completed (loss: 0.04755844175815582, acc: 0.9848771095275879)
[2025-02-13 03:42:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:31][root][INFO] - Training Epoch: 2/2, step 1087/7134 completed (loss: 0.017110615968704224, acc: 0.9939024448394775)
[2025-02-13 03:42:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:31][root][INFO] - Training Epoch: 2/2, step 1088/7134 completed (loss: 0.019788186997175217, acc: 0.9972066879272461)
[2025-02-13 03:42:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:32][root][INFO] - Training Epoch: 2/2, step 1089/7134 completed (loss: 0.03757213428616524, acc: 0.992548406124115)
[2025-02-13 03:42:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:32][root][INFO] - Training Epoch: 2/2, step 1090/7134 completed (loss: 0.027526527643203735, acc: 0.9906396269798279)
[2025-02-13 03:42:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:33][root][INFO] - Training Epoch: 2/2, step 1091/7134 completed (loss: 0.015706514939665794, acc: 0.9984779357910156)
[2025-02-13 03:42:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:33][root][INFO] - Training Epoch: 2/2, step 1092/7134 completed (loss: 0.007547501940280199, acc: 0.9985548853874207)
[2025-02-13 03:42:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:34][root][INFO] - Training Epoch: 2/2, step 1093/7134 completed (loss: 0.06571400910615921, acc: 0.9843260049819946)
[2025-02-13 03:42:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:34][root][INFO] - Training Epoch: 2/2, step 1094/7134 completed (loss: 0.03575972095131874, acc: 0.9869888424873352)
[2025-02-13 03:42:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:35][root][INFO] - Training Epoch: 2/2, step 1095/7134 completed (loss: 0.029963692650198936, acc: 0.9914737939834595)
[2025-02-13 03:42:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:35][root][INFO] - Training Epoch: 2/2, step 1096/7134 completed (loss: 0.02635975182056427, acc: 0.9949367046356201)
[2025-02-13 03:42:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:35][root][INFO] - Training Epoch: 2/2, step 1097/7134 completed (loss: 0.04309710115194321, acc: 0.9873595237731934)
[2025-02-13 03:42:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:36][root][INFO] - Training Epoch: 2/2, step 1098/7134 completed (loss: 0.010041380301117897, acc: 0.9962916970252991)
[2025-02-13 03:42:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:36][root][INFO] - Training Epoch: 2/2, step 1099/7134 completed (loss: 0.01985178142786026, acc: 0.9974358677864075)
[2025-02-13 03:42:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:37][root][INFO] - Training Epoch: 2/2, step 1100/7134 completed (loss: 0.021843405440449715, acc: 0.9932065010070801)
[2025-02-13 03:42:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:37][root][INFO] - Training Epoch: 2/2, step 1101/7134 completed (loss: 0.019664216786623, acc: 0.9948052167892456)
[2025-02-13 03:42:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:38][root][INFO] - Training Epoch: 2/2, step 1102/7134 completed (loss: 0.041286785155534744, acc: 0.9921721816062927)
[2025-02-13 03:42:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:38][root][INFO] - Training Epoch: 2/2, step 1103/7134 completed (loss: 0.060108963400125504, acc: 0.9879999756813049)
[2025-02-13 03:42:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:39][root][INFO] - Training Epoch: 2/2, step 1104/7134 completed (loss: 0.02238178625702858, acc: 0.9913544654846191)
[2025-02-13 03:42:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:39][root][INFO] - Training Epoch: 2/2, step 1105/7134 completed (loss: 0.022523000836372375, acc: 0.9905660152435303)
[2025-02-13 03:42:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:40][root][INFO] - Training Epoch: 2/2, step 1106/7134 completed (loss: 0.03375397250056267, acc: 0.9876033067703247)
[2025-02-13 03:42:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:40][root][INFO] - Training Epoch: 2/2, step 1107/7134 completed (loss: 0.025928203016519547, acc: 0.9941383600234985)
[2025-02-13 03:42:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:41][root][INFO] - Training Epoch: 2/2, step 1108/7134 completed (loss: 0.01126791164278984, acc: 0.9974226951599121)
[2025-02-13 03:42:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:41][root][INFO] - Training Epoch: 2/2, step 1109/7134 completed (loss: 0.02302313968539238, acc: 0.99301677942276)
[2025-02-13 03:42:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:42][root][INFO] - Training Epoch: 2/2, step 1110/7134 completed (loss: 0.05214289575815201, acc: 0.9895150661468506)
[2025-02-13 03:42:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:42][root][INFO] - Training Epoch: 2/2, step 1111/7134 completed (loss: 0.03194665536284447, acc: 0.9944751262664795)
[2025-02-13 03:42:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:42][root][INFO] - Training Epoch: 2/2, step 1112/7134 completed (loss: 0.017893420532345772, acc: 0.9941176176071167)
[2025-02-13 03:42:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:43][root][INFO] - Training Epoch: 2/2, step 1113/7134 completed (loss: 0.010981576517224312, acc: 0.9961389899253845)
[2025-02-13 03:42:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:43][root][INFO] - Training Epoch: 2/2, step 1114/7134 completed (loss: 0.03651135414838791, acc: 0.9902794361114502)
[2025-02-13 03:42:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:44][root][INFO] - Training Epoch: 2/2, step 1115/7134 completed (loss: 0.027780059725046158, acc: 0.9905660152435303)
[2025-02-13 03:42:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:44][root][INFO] - Training Epoch: 2/2, step 1116/7134 completed (loss: 0.010727894492447376, acc: 0.996268630027771)
[2025-02-13 03:42:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:45][root][INFO] - Training Epoch: 2/2, step 1117/7134 completed (loss: 0.008943273685872555, acc: 0.9984662532806396)
[2025-02-13 03:42:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:45][root][INFO] - Training Epoch: 2/2, step 1118/7134 completed (loss: 0.013076629489660263, acc: 0.9965517520904541)
[2025-02-13 03:42:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:46][root][INFO] - Training Epoch: 2/2, step 1119/7134 completed (loss: 0.02224407158792019, acc: 0.9889415502548218)
[2025-02-13 03:42:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:46][root][INFO] - Training Epoch: 2/2, step 1120/7134 completed (loss: 0.027704596519470215, acc: 0.9921383857727051)
[2025-02-13 03:42:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:46][root][INFO] - Training Epoch: 2/2, step 1121/7134 completed (loss: 0.017086664214730263, acc: 0.9940915703773499)
[2025-02-13 03:42:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:47][root][INFO] - Training Epoch: 2/2, step 1122/7134 completed (loss: 0.018471596762537956, acc: 0.9922330379486084)
[2025-02-13 03:42:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:47][root][INFO] - Training Epoch: 2/2, step 1123/7134 completed (loss: 0.017568834125995636, acc: 0.9935794472694397)
[2025-02-13 03:42:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:48][root][INFO] - Training Epoch: 2/2, step 1124/7134 completed (loss: 0.021095074713230133, acc: 0.99071204662323)
[2025-02-13 03:42:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:48][root][INFO] - Training Epoch: 2/2, step 1125/7134 completed (loss: 0.008998027071356773, acc: 0.9972936511039734)
[2025-02-13 03:42:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:49][root][INFO] - Training Epoch: 2/2, step 1126/7134 completed (loss: 0.02720971591770649, acc: 0.988950252532959)
[2025-02-13 03:42:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:49][root][INFO] - Training Epoch: 2/2, step 1127/7134 completed (loss: 0.024610048159956932, acc: 0.9936000108718872)
[2025-02-13 03:42:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:49][root][INFO] - Training Epoch: 2/2, step 1128/7134 completed (loss: 0.019706182181835175, acc: 0.9895833134651184)
[2025-02-13 03:42:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:50][root][INFO] - Training Epoch: 2/2, step 1129/7134 completed (loss: 0.032139189541339874, acc: 0.9910314083099365)
[2025-02-13 03:42:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:50][root][INFO] - Training Epoch: 2/2, step 1130/7134 completed (loss: 0.006749429740011692, acc: 0.9951534867286682)
[2025-02-13 03:42:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:51][root][INFO] - Training Epoch: 2/2, step 1131/7134 completed (loss: 0.030760126188397408, acc: 0.9952830076217651)
[2025-02-13 03:42:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:51][root][INFO] - Training Epoch: 2/2, step 1132/7134 completed (loss: 0.09703026711940765, acc: 0.9800918698310852)
[2025-02-13 03:42:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:52][root][INFO] - Training Epoch: 2/2, step 1133/7134 completed (loss: 0.07459473609924316, acc: 0.9814550876617432)
[2025-02-13 03:42:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:52][root][INFO] - Training Epoch: 2/2, step 1134/7134 completed (loss: 0.049361467361450195, acc: 0.9850373864173889)
[2025-02-13 03:42:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:52][root][INFO] - Training Epoch: 2/2, step 1135/7134 completed (loss: 0.04002843797206879, acc: 0.9872000217437744)
[2025-02-13 03:42:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:53][root][INFO] - Training Epoch: 2/2, step 1136/7134 completed (loss: 0.0512283630669117, acc: 0.9836734533309937)
[2025-02-13 03:42:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:53][root][INFO] - Training Epoch: 2/2, step 1137/7134 completed (loss: 0.035718221217393875, acc: 0.9886363744735718)
[2025-02-13 03:42:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:54][root][INFO] - Training Epoch: 2/2, step 1138/7134 completed (loss: 0.02880912274122238, acc: 0.9918032884597778)
[2025-02-13 03:42:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:54][root][INFO] - Training Epoch: 2/2, step 1139/7134 completed (loss: 0.0842336118221283, acc: 0.979763925075531)
[2025-02-13 03:42:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:55][root][INFO] - Training Epoch: 2/2, step 1140/7134 completed (loss: 0.02450060285627842, acc: 0.9899749159812927)
[2025-02-13 03:42:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:55][root][INFO] - Training Epoch: 2/2, step 1141/7134 completed (loss: 0.013408970087766647, acc: 0.9976580739021301)
[2025-02-13 03:42:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:56][root][INFO] - Training Epoch: 2/2, step 1142/7134 completed (loss: 0.04081504046916962, acc: 0.9934895634651184)
[2025-02-13 03:42:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:56][root][INFO] - Training Epoch: 2/2, step 1143/7134 completed (loss: 0.0578521192073822, acc: 0.9805285334587097)
[2025-02-13 03:42:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:56][root][INFO] - Training Epoch: 2/2, step 1144/7134 completed (loss: 0.05185125395655632, acc: 0.9900000095367432)
[2025-02-13 03:42:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:57][root][INFO] - Training Epoch: 2/2, step 1145/7134 completed (loss: 0.03044951520860195, acc: 0.9952038526535034)
[2025-02-13 03:42:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:57][root][INFO] - Training Epoch: 2/2, step 1146/7134 completed (loss: 0.015008753165602684, acc: 0.9963768124580383)
[2025-02-13 03:42:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:58][root][INFO] - Training Epoch: 2/2, step 1147/7134 completed (loss: 0.013124342076480389, acc: 0.9975062608718872)
[2025-02-13 03:42:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:58][root][INFO] - Training Epoch: 2/2, step 1148/7134 completed (loss: 0.04225832223892212, acc: 0.9863481521606445)
[2025-02-13 03:42:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:59][root][INFO] - Training Epoch: 2/2, step 1149/7134 completed (loss: 0.05323193967342377, acc: 0.9824175834655762)
[2025-02-13 03:42:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:42:59][root][INFO] - Training Epoch: 2/2, step 1150/7134 completed (loss: 0.022992728278040886, acc: 0.9953488111495972)
[2025-02-13 03:42:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:00][root][INFO] - Training Epoch: 2/2, step 1151/7134 completed (loss: 0.006306514609605074, acc: 1.0)
[2025-02-13 03:43:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:00][root][INFO] - Training Epoch: 2/2, step 1152/7134 completed (loss: 0.026322713121771812, acc: 0.9914529919624329)
[2025-02-13 03:43:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:01][root][INFO] - Training Epoch: 2/2, step 1153/7134 completed (loss: 0.04642308130860329, acc: 0.9891008138656616)
[2025-02-13 03:43:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:01][root][INFO] - Training Epoch: 2/2, step 1154/7134 completed (loss: 0.03965846076607704, acc: 0.9893805384635925)
[2025-02-13 03:43:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:02][root][INFO] - Training Epoch: 2/2, step 1155/7134 completed (loss: 0.05338500812649727, acc: 0.9736841917037964)
[2025-02-13 03:43:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:02][root][INFO] - Training Epoch: 2/2, step 1156/7134 completed (loss: 0.012089761905372143, acc: 0.996927797794342)
[2025-02-13 03:43:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:03][root][INFO] - Training Epoch: 2/2, step 1157/7134 completed (loss: 0.04943786934018135, acc: 0.9881154298782349)
[2025-02-13 03:43:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:03][root][INFO] - Training Epoch: 2/2, step 1158/7134 completed (loss: 0.043074190616607666, acc: 0.9842209219932556)
[2025-02-13 03:43:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:03][root][INFO] - Training Epoch: 2/2, step 1159/7134 completed (loss: 0.042040351778268814, acc: 0.9872340559959412)
[2025-02-13 03:43:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:04][root][INFO] - Training Epoch: 2/2, step 1160/7134 completed (loss: 0.018459632992744446, acc: 0.9940476417541504)
[2025-02-13 03:43:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:04][root][INFO] - Training Epoch: 2/2, step 1161/7134 completed (loss: 0.06640757620334625, acc: 0.985318124294281)
[2025-02-13 03:43:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:05][root][INFO] - Training Epoch: 2/2, step 1162/7134 completed (loss: 0.023187221959233284, acc: 0.9917355179786682)
[2025-02-13 03:43:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:05][root][INFO] - Training Epoch: 2/2, step 1163/7134 completed (loss: 0.03243756666779518, acc: 0.9914675951004028)
[2025-02-13 03:43:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:06][root][INFO] - Training Epoch: 2/2, step 1164/7134 completed (loss: 0.04846138879656792, acc: 0.9902642369270325)
[2025-02-13 03:43:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:06][root][INFO] - Training Epoch: 2/2, step 1165/7134 completed (loss: 0.018147718161344528, acc: 0.9959294199943542)
[2025-02-13 03:43:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:07][root][INFO] - Training Epoch: 2/2, step 1166/7134 completed (loss: 0.014671500772237778, acc: 0.9972565174102783)
[2025-02-13 03:43:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:07][root][INFO] - Training Epoch: 2/2, step 1167/7134 completed (loss: 0.009925633668899536, acc: 0.9980353713035583)
[2025-02-13 03:43:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:08][root][INFO] - Training Epoch: 2/2, step 1168/7134 completed (loss: 0.007675209082663059, acc: 0.9973683953285217)
[2025-02-13 03:43:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:08][root][INFO] - Training Epoch: 2/2, step 1169/7134 completed (loss: 0.028795747086405754, acc: 0.9864864945411682)
[2025-02-13 03:43:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:08][root][INFO] - Training Epoch: 2/2, step 1170/7134 completed (loss: 0.025949489325284958, acc: 0.9943820238113403)
[2025-02-13 03:43:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:09][root][INFO] - Training Epoch: 2/2, step 1171/7134 completed (loss: 0.020493773743510246, acc: 0.9933221936225891)
[2025-02-13 03:43:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:09][root][INFO] - Training Epoch: 2/2, step 1172/7134 completed (loss: 0.02753034606575966, acc: 0.9894099831581116)
[2025-02-13 03:43:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:10][root][INFO] - Training Epoch: 2/2, step 1173/7134 completed (loss: 0.026249555870890617, acc: 0.991304337978363)
[2025-02-13 03:43:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:10][root][INFO] - Training Epoch: 2/2, step 1174/7134 completed (loss: 0.01502988301217556, acc: 0.9954954981803894)
[2025-02-13 03:43:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:11][root][INFO] - Training Epoch: 2/2, step 1175/7134 completed (loss: 0.01707245036959648, acc: 0.9956521987915039)
[2025-02-13 03:43:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:11][root][INFO] - Training Epoch: 2/2, step 1176/7134 completed (loss: 0.0030946400947868824, acc: 1.0)
[2025-02-13 03:43:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:11][root][INFO] - Training Epoch: 2/2, step 1177/7134 completed (loss: 0.008525416254997253, acc: 0.998039186000824)
[2025-02-13 03:43:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:12][root][INFO] - Training Epoch: 2/2, step 1178/7134 completed (loss: 0.00877757091075182, acc: 0.9953271150588989)
[2025-02-13 03:43:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:12][root][INFO] - Training Epoch: 2/2, step 1179/7134 completed (loss: 0.00896848738193512, acc: 0.9980879426002502)
[2025-02-13 03:43:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:13][root][INFO] - Training Epoch: 2/2, step 1180/7134 completed (loss: 0.00535535765811801, acc: 1.0)
[2025-02-13 03:43:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:13][root][INFO] - Training Epoch: 2/2, step 1181/7134 completed (loss: 0.02259128913283348, acc: 0.9899280667304993)
[2025-02-13 03:43:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:14][root][INFO] - Training Epoch: 2/2, step 1182/7134 completed (loss: 0.003406999632716179, acc: 1.0)
[2025-02-13 03:43:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:14][root][INFO] - Training Epoch: 2/2, step 1183/7134 completed (loss: 0.004804346244782209, acc: 1.0)
[2025-02-13 03:43:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:15][root][INFO] - Training Epoch: 2/2, step 1184/7134 completed (loss: 0.03563915193080902, acc: 0.9928469061851501)
[2025-02-13 03:43:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:15][root][INFO] - Training Epoch: 2/2, step 1185/7134 completed (loss: 0.006081117317080498, acc: 0.9984962344169617)
[2025-02-13 03:43:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:16][root][INFO] - Training Epoch: 2/2, step 1186/7134 completed (loss: 0.019899457693099976, acc: 0.9974059462547302)
[2025-02-13 03:43:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:16][root][INFO] - Training Epoch: 2/2, step 1187/7134 completed (loss: 0.02180740423500538, acc: 0.9958449006080627)
[2025-02-13 03:43:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:16][root][INFO] - Training Epoch: 2/2, step 1188/7134 completed (loss: 0.007974588312208652, acc: 0.9986807107925415)
[2025-02-13 03:43:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:17][root][INFO] - Training Epoch: 2/2, step 1189/7134 completed (loss: 0.008886468596756458, acc: 0.9985548853874207)
[2025-02-13 03:43:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:17][root][INFO] - Training Epoch: 2/2, step 1190/7134 completed (loss: 0.005547746084630489, acc: 0.9974554777145386)
[2025-02-13 03:43:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:18][root][INFO] - Training Epoch: 2/2, step 1191/7134 completed (loss: 0.026119956746697426, acc: 0.9955686926841736)
[2025-02-13 03:43:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:18][root][INFO] - Training Epoch: 2/2, step 1192/7134 completed (loss: 0.006755344569683075, acc: 1.0)
[2025-02-13 03:43:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:19][root][INFO] - Training Epoch: 2/2, step 1193/7134 completed (loss: 0.027195977047085762, acc: 0.9936203956604004)
[2025-02-13 03:43:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:19][root][INFO] - Training Epoch: 2/2, step 1194/7134 completed (loss: 0.04866698384284973, acc: 0.9917582273483276)
[2025-02-13 03:43:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:20][root][INFO] - Training Epoch: 2/2, step 1195/7134 completed (loss: 0.023270245641469955, acc: 0.9885621070861816)
[2025-02-13 03:43:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:20][root][INFO] - Training Epoch: 2/2, step 1196/7134 completed (loss: 0.039744943380355835, acc: 0.9901639223098755)
[2025-02-13 03:43:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:20][root][INFO] - Training Epoch: 2/2, step 1197/7134 completed (loss: 0.02205946110188961, acc: 0.9897360801696777)
[2025-02-13 03:43:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:21][root][INFO] - Training Epoch: 2/2, step 1198/7134 completed (loss: 0.019037827849388123, acc: 0.9942445755004883)
[2025-02-13 03:43:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:21][root][INFO] - Training Epoch: 2/2, step 1199/7134 completed (loss: 0.010153397917747498, acc: 0.9985272288322449)
[2025-02-13 03:43:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:22][root][INFO] - Training Epoch: 2/2, step 1200/7134 completed (loss: 0.01358806248754263, acc: 0.9958932399749756)
[2025-02-13 03:43:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:22][root][INFO] - Training Epoch: 2/2, step 1201/7134 completed (loss: 0.02750208042562008, acc: 0.9911634922027588)
[2025-02-13 03:43:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:23][root][INFO] - Training Epoch: 2/2, step 1202/7134 completed (loss: 0.0316099114716053, acc: 0.9865410327911377)
[2025-02-13 03:43:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:23][root][INFO] - Training Epoch: 2/2, step 1203/7134 completed (loss: 0.00364700797945261, acc: 1.0)
[2025-02-13 03:43:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:24][root][INFO] - Training Epoch: 2/2, step 1204/7134 completed (loss: 0.02691350318491459, acc: 0.9906716346740723)
[2025-02-13 03:43:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:24][root][INFO] - Training Epoch: 2/2, step 1205/7134 completed (loss: 0.07864496111869812, acc: 0.9802817106246948)
[2025-02-13 03:43:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:25][root][INFO] - Training Epoch: 2/2, step 1206/7134 completed (loss: 0.029857130721211433, acc: 0.9875930547714233)
[2025-02-13 03:43:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:25][root][INFO] - Training Epoch: 2/2, step 1207/7134 completed (loss: 0.022149836644530296, acc: 0.9937185645103455)
[2025-02-13 03:43:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:26][root][INFO] - Training Epoch: 2/2, step 1208/7134 completed (loss: 0.029669152572751045, acc: 0.9850746393203735)
[2025-02-13 03:43:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:26][root][INFO] - Training Epoch: 2/2, step 1209/7134 completed (loss: 0.07181865721940994, acc: 0.9835466146469116)
[2025-02-13 03:43:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:26][root][INFO] - Training Epoch: 2/2, step 1210/7134 completed (loss: 0.04901085048913956, acc: 0.980861246585846)
[2025-02-13 03:43:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:27][root][INFO] - Training Epoch: 2/2, step 1211/7134 completed (loss: 0.08739633113145828, acc: 0.9691714644432068)
[2025-02-13 03:43:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:27][root][INFO] - Training Epoch: 2/2, step 1212/7134 completed (loss: 0.09974411875009537, acc: 0.9740853905677795)
[2025-02-13 03:43:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:28][root][INFO] - Training Epoch: 2/2, step 1213/7134 completed (loss: 0.0196100901812315, acc: 0.9929278492927551)
[2025-02-13 03:43:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:28][root][INFO] - Training Epoch: 2/2, step 1214/7134 completed (loss: 0.04952619597315788, acc: 0.989230751991272)
[2025-02-13 03:43:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:29][root][INFO] - Training Epoch: 2/2, step 1215/7134 completed (loss: 0.047145381569862366, acc: 0.9862068891525269)
[2025-02-13 03:43:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:29][root][INFO] - Training Epoch: 2/2, step 1216/7134 completed (loss: 0.023996517062187195, acc: 0.9941605925559998)
[2025-02-13 03:43:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:30][root][INFO] - Training Epoch: 2/2, step 1217/7134 completed (loss: 0.04041283205151558, acc: 0.9886363744735718)
[2025-02-13 03:43:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:30][root][INFO] - Training Epoch: 2/2, step 1218/7134 completed (loss: 0.06366168707609177, acc: 0.9820895791053772)
[2025-02-13 03:43:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:31][root][INFO] - Training Epoch: 2/2, step 1219/7134 completed (loss: 0.06202699616551399, acc: 0.9841827750205994)
[2025-02-13 03:43:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:31][root][INFO] - Training Epoch: 2/2, step 1220/7134 completed (loss: 0.07134158164262772, acc: 0.9835025668144226)
[2025-02-13 03:43:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:32][root][INFO] - Training Epoch: 2/2, step 1221/7134 completed (loss: 0.06300649046897888, acc: 0.9835526347160339)
[2025-02-13 03:43:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:32][root][INFO] - Training Epoch: 2/2, step 1222/7134 completed (loss: 0.05250369384884834, acc: 0.9845053553581238)
[2025-02-13 03:43:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:33][root][INFO] - Training Epoch: 2/2, step 1223/7134 completed (loss: 0.12458471953868866, acc: 0.9725651741027832)
[2025-02-13 03:43:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:33][root][INFO] - Training Epoch: 2/2, step 1224/7134 completed (loss: 0.03165952116250992, acc: 0.9933775067329407)
[2025-02-13 03:43:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:34][root][INFO] - Training Epoch: 2/2, step 1225/7134 completed (loss: 0.05479288101196289, acc: 0.9875141978263855)
[2025-02-13 03:43:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:34][root][INFO] - Training Epoch: 2/2, step 1226/7134 completed (loss: 0.09674505144357681, acc: 0.9758551120758057)
[2025-02-13 03:43:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:35][root][INFO] - Training Epoch: 2/2, step 1227/7134 completed (loss: 0.08091331273317337, acc: 0.9758713245391846)
[2025-02-13 03:43:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:35][root][INFO] - Training Epoch: 2/2, step 1228/7134 completed (loss: 0.07673893123865128, acc: 0.9828326106071472)
[2025-02-13 03:43:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:35][root][INFO] - Training Epoch: 2/2, step 1229/7134 completed (loss: 0.028433555737137794, acc: 0.9951980710029602)
[2025-02-13 03:43:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:36][root][INFO] - Training Epoch: 2/2, step 1230/7134 completed (loss: 0.06147363781929016, acc: 0.9819208979606628)
[2025-02-13 03:43:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:36][root][INFO] - Training Epoch: 2/2, step 1231/7134 completed (loss: 0.049934886395931244, acc: 0.9855222105979919)
[2025-02-13 03:43:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:37][root][INFO] - Training Epoch: 2/2, step 1232/7134 completed (loss: 0.04181836172938347, acc: 0.9872204661369324)
[2025-02-13 03:43:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:37][root][INFO] - Training Epoch: 2/2, step 1233/7134 completed (loss: 0.07793965935707092, acc: 0.9780346751213074)
[2025-02-13 03:43:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:38][root][INFO] - Training Epoch: 2/2, step 1234/7134 completed (loss: 0.024572288617491722, acc: 0.9922360181808472)
[2025-02-13 03:43:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:38][root][INFO] - Training Epoch: 2/2, step 1235/7134 completed (loss: 0.05629521608352661, acc: 0.9808153510093689)
[2025-02-13 03:43:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:39][root][INFO] - Training Epoch: 2/2, step 1236/7134 completed (loss: 0.06465136259794235, acc: 0.9795022010803223)
[2025-02-13 03:43:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:39][root][INFO] - Training Epoch: 2/2, step 1237/7134 completed (loss: 0.05944884568452835, acc: 0.9813664555549622)
[2025-02-13 03:43:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:40][root][INFO] - Training Epoch: 2/2, step 1238/7134 completed (loss: 0.09385930746793747, acc: 0.9708878993988037)
[2025-02-13 03:43:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:40][root][INFO] - Training Epoch: 2/2, step 1239/7134 completed (loss: 0.06435247510671616, acc: 0.9814019799232483)
[2025-02-13 03:43:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:41][root][INFO] - Training Epoch: 2/2, step 1240/7134 completed (loss: 0.061901431530714035, acc: 0.9828042387962341)
[2025-02-13 03:43:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:41][root][INFO] - Training Epoch: 2/2, step 1241/7134 completed (loss: 0.023894643411040306, acc: 0.9919678568840027)
[2025-02-13 03:43:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:42][root][INFO] - Training Epoch: 2/2, step 1242/7134 completed (loss: 0.04694196954369545, acc: 0.9867788553237915)
[2025-02-13 03:43:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:42][root][INFO] - Training Epoch: 2/2, step 1243/7134 completed (loss: 0.047666534781455994, acc: 0.9895397424697876)
[2025-02-13 03:43:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:43][root][INFO] - Training Epoch: 2/2, step 1244/7134 completed (loss: 0.05875338613986969, acc: 0.9900662302970886)
[2025-02-13 03:43:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:43][root][INFO] - Training Epoch: 2/2, step 1245/7134 completed (loss: 0.05645214021205902, acc: 0.984886646270752)
[2025-02-13 03:43:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:44][root][INFO] - Training Epoch: 2/2, step 1246/7134 completed (loss: 0.081342414021492, acc: 0.9807445406913757)
[2025-02-13 03:43:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:44][root][INFO] - Training Epoch: 2/2, step 1247/7134 completed (loss: 0.027546491473913193, acc: 0.9942113161087036)
[2025-02-13 03:43:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:44][root][INFO] - Training Epoch: 2/2, step 1248/7134 completed (loss: 0.04927448928356171, acc: 0.9908466935157776)
[2025-02-13 03:43:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:45][root][INFO] - Training Epoch: 2/2, step 1249/7134 completed (loss: 0.020130213350057602, acc: 0.9928825497627258)
[2025-02-13 03:43:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:45][root][INFO] - Training Epoch: 2/2, step 1250/7134 completed (loss: 0.023117078468203545, acc: 0.9915013909339905)
[2025-02-13 03:43:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:46][root][INFO] - Training Epoch: 2/2, step 1251/7134 completed (loss: 0.019873695448040962, acc: 0.9931412935256958)
[2025-02-13 03:43:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:46][root][INFO] - Training Epoch: 2/2, step 1252/7134 completed (loss: 0.022939438000321388, acc: 0.9907407164573669)
[2025-02-13 03:43:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:47][root][INFO] - Training Epoch: 2/2, step 1253/7134 completed (loss: 0.04533257707953453, acc: 0.9903475046157837)
[2025-02-13 03:43:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:47][root][INFO] - Training Epoch: 2/2, step 1254/7134 completed (loss: 0.04228048771619797, acc: 0.9848484992980957)
[2025-02-13 03:43:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:48][root][INFO] - Training Epoch: 2/2, step 1255/7134 completed (loss: 0.034706953912973404, acc: 0.9897959232330322)
[2025-02-13 03:43:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:48][root][INFO] - Training Epoch: 2/2, step 1256/7134 completed (loss: 0.03538950905203819, acc: 0.9871382713317871)
[2025-02-13 03:43:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:48][root][INFO] - Training Epoch: 2/2, step 1257/7134 completed (loss: 0.01776128262281418, acc: 0.9951534867286682)
[2025-02-13 03:43:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:49][root][INFO] - Training Epoch: 2/2, step 1258/7134 completed (loss: 0.044690266251564026, acc: 0.985358715057373)
[2025-02-13 03:43:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:49][root][INFO] - Training Epoch: 2/2, step 1259/7134 completed (loss: 0.06106024608016014, acc: 0.991416335105896)
[2025-02-13 03:43:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:50][root][INFO] - Training Epoch: 2/2, step 1260/7134 completed (loss: 0.008707908913493156, acc: 0.9985548853874207)
[2025-02-13 03:43:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:50][root][INFO] - Training Epoch: 2/2, step 1261/7134 completed (loss: 0.02837528847157955, acc: 0.9910979270935059)
[2025-02-13 03:43:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:51][root][INFO] - Training Epoch: 2/2, step 1262/7134 completed (loss: 0.005002773832529783, acc: 0.9981684684753418)
[2025-02-13 03:43:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:51][root][INFO] - Training Epoch: 2/2, step 1263/7134 completed (loss: 0.009426338598132133, acc: 0.9971428513526917)
[2025-02-13 03:43:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:51][root][INFO] - Training Epoch: 2/2, step 1264/7134 completed (loss: 0.014053504914045334, acc: 0.9937694668769836)
[2025-02-13 03:43:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:52][root][INFO] - Training Epoch: 2/2, step 1265/7134 completed (loss: 0.016867736354470253, acc: 0.9949874877929688)
[2025-02-13 03:43:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:52][root][INFO] - Training Epoch: 2/2, step 1266/7134 completed (loss: 0.017303114756941795, acc: 0.9965576529502869)
[2025-02-13 03:43:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:53][root][INFO] - Training Epoch: 2/2, step 1267/7134 completed (loss: 0.016889940947294235, acc: 0.9958620667457581)
[2025-02-13 03:43:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:53][root][INFO] - Training Epoch: 2/2, step 1268/7134 completed (loss: 0.02596663497388363, acc: 0.9926144480705261)
[2025-02-13 03:43:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:54][root][INFO] - Training Epoch: 2/2, step 1269/7134 completed (loss: 0.06897598505020142, acc: 0.9775862097740173)
[2025-02-13 03:43:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:54][root][INFO] - Training Epoch: 2/2, step 1270/7134 completed (loss: 0.013470353558659554, acc: 0.9944211840629578)
[2025-02-13 03:43:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:55][root][INFO] - Training Epoch: 2/2, step 1271/7134 completed (loss: 0.055045902729034424, acc: 0.9870370626449585)
[2025-02-13 03:43:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:55][root][INFO] - Training Epoch: 2/2, step 1272/7134 completed (loss: 0.016358880326151848, acc: 0.9933949708938599)
[2025-02-13 03:43:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:55][root][INFO] - Training Epoch: 2/2, step 1273/7134 completed (loss: 0.015321583487093449, acc: 0.9967741966247559)
[2025-02-13 03:43:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:56][root][INFO] - Training Epoch: 2/2, step 1274/7134 completed (loss: 0.021472018212080002, acc: 0.9932432174682617)
[2025-02-13 03:43:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:56][root][INFO] - Training Epoch: 2/2, step 1275/7134 completed (loss: 0.03735048696398735, acc: 0.9928186535835266)
[2025-02-13 03:43:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:57][root][INFO] - Training Epoch: 2/2, step 1276/7134 completed (loss: 0.0184010099619627, acc: 0.9951612949371338)
[2025-02-13 03:43:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:57][root][INFO] - Training Epoch: 2/2, step 1277/7134 completed (loss: 0.0223939660936594, acc: 0.9919224381446838)
[2025-02-13 03:43:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:58][root][INFO] - Training Epoch: 2/2, step 1278/7134 completed (loss: 0.03620591759681702, acc: 0.993779182434082)
[2025-02-13 03:43:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:58][root][INFO] - Training Epoch: 2/2, step 1279/7134 completed (loss: 0.040065374225378036, acc: 0.9888337254524231)
[2025-02-13 03:43:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:59][root][INFO] - Training Epoch: 2/2, step 1280/7134 completed (loss: 0.047718413174152374, acc: 0.9894875288009644)
[2025-02-13 03:43:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:59][root][INFO] - Training Epoch: 2/2, step 1281/7134 completed (loss: 0.027087397873401642, acc: 0.9931153059005737)
[2025-02-13 03:43:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:43:59][root][INFO] - Training Epoch: 2/2, step 1282/7134 completed (loss: 0.09116516262292862, acc: 0.9799465537071228)
[2025-02-13 03:44:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:00][root][INFO] - Training Epoch: 2/2, step 1283/7134 completed (loss: 0.05241897702217102, acc: 0.9911971688270569)
[2025-02-13 03:44:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:00][root][INFO] - Training Epoch: 2/2, step 1284/7134 completed (loss: 0.05141961947083473, acc: 0.9891008138656616)
[2025-02-13 03:44:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:01][root][INFO] - Training Epoch: 2/2, step 1285/7134 completed (loss: 0.0634889006614685, acc: 0.9884560108184814)
[2025-02-13 03:44:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:01][root][INFO] - Training Epoch: 2/2, step 1286/7134 completed (loss: 0.025017352774739265, acc: 0.995708167552948)
[2025-02-13 03:44:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:02][root][INFO] - Training Epoch: 2/2, step 1287/7134 completed (loss: 0.07385879009962082, acc: 0.9817276000976562)
[2025-02-13 03:44:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:02][root][INFO] - Training Epoch: 2/2, step 1288/7134 completed (loss: 0.03432440757751465, acc: 0.9911167621612549)
[2025-02-13 03:44:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:03][root][INFO] - Training Epoch: 2/2, step 1289/7134 completed (loss: 0.040414176881313324, acc: 0.9899497628211975)
[2025-02-13 03:44:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:03][root][INFO] - Training Epoch: 2/2, step 1290/7134 completed (loss: 0.035872943699359894, acc: 0.9929577708244324)
[2025-02-13 03:44:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:04][root][INFO] - Training Epoch: 2/2, step 1291/7134 completed (loss: 0.02309258095920086, acc: 0.9958158731460571)
[2025-02-13 03:44:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:04][root][INFO] - Training Epoch: 2/2, step 1292/7134 completed (loss: 0.03140970692038536, acc: 0.9961038827896118)
[2025-02-13 03:44:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:04][root][INFO] - Training Epoch: 2/2, step 1293/7134 completed (loss: 0.01695483922958374, acc: 0.9965075850486755)
[2025-02-13 03:44:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:05][root][INFO] - Training Epoch: 2/2, step 1294/7134 completed (loss: 0.039281461387872696, acc: 0.9882965087890625)
[2025-02-13 03:44:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:05][root][INFO] - Training Epoch: 2/2, step 1295/7134 completed (loss: 0.023382501676678658, acc: 0.9932088255882263)
[2025-02-13 03:44:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:06][root][INFO] - Training Epoch: 2/2, step 1296/7134 completed (loss: 0.009994013234972954, acc: 0.9955752491950989)
[2025-02-13 03:44:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:06][root][INFO] - Training Epoch: 2/2, step 1297/7134 completed (loss: 0.024798674508929253, acc: 0.9916840195655823)
[2025-02-13 03:44:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:07][root][INFO] - Training Epoch: 2/2, step 1298/7134 completed (loss: 0.051331039518117905, acc: 0.9874100685119629)
[2025-02-13 03:44:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:07][root][INFO] - Training Epoch: 2/2, step 1299/7134 completed (loss: 0.007971609942615032, acc: 1.0)
[2025-02-13 03:44:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:08][root][INFO] - Training Epoch: 2/2, step 1300/7134 completed (loss: 0.01978827267885208, acc: 0.9939098954200745)
[2025-02-13 03:44:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:08][root][INFO] - Training Epoch: 2/2, step 1301/7134 completed (loss: 0.009123705327510834, acc: 0.998516321182251)
[2025-02-13 03:44:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:09][root][INFO] - Training Epoch: 2/2, step 1302/7134 completed (loss: 0.03798671439290047, acc: 0.9877232313156128)
[2025-02-13 03:44:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:09][root][INFO] - Training Epoch: 2/2, step 1303/7134 completed (loss: 0.03288408741354942, acc: 0.9879807829856873)
[2025-02-13 03:44:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:09][root][INFO] - Training Epoch: 2/2, step 1304/7134 completed (loss: 0.0596330501139164, acc: 0.9855072498321533)
[2025-02-13 03:44:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:10][root][INFO] - Training Epoch: 2/2, step 1305/7134 completed (loss: 0.03069179132580757, acc: 0.9911167621612549)
[2025-02-13 03:44:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:10][root][INFO] - Training Epoch: 2/2, step 1306/7134 completed (loss: 0.045135747641325, acc: 0.9908952713012695)
[2025-02-13 03:44:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:11][root][INFO] - Training Epoch: 2/2, step 1307/7134 completed (loss: 0.013478715904057026, acc: 0.998420238494873)
[2025-02-13 03:44:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:11][root][INFO] - Training Epoch: 2/2, step 1308/7134 completed (loss: 0.019289445132017136, acc: 0.9924812316894531)
[2025-02-13 03:44:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:12][root][INFO] - Training Epoch: 2/2, step 1309/7134 completed (loss: 0.03266439959406853, acc: 0.989234447479248)
[2025-02-13 03:44:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:12][root][INFO] - Training Epoch: 2/2, step 1310/7134 completed (loss: 0.028119299560785294, acc: 0.9904761910438538)
[2025-02-13 03:44:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:13][root][INFO] - Training Epoch: 2/2, step 1311/7134 completed (loss: 0.04512878507375717, acc: 0.9864253401756287)
[2025-02-13 03:44:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:13][root][INFO] - Training Epoch: 2/2, step 1312/7134 completed (loss: 0.06910506635904312, acc: 0.986522912979126)
[2025-02-13 03:44:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:14][root][INFO] - Training Epoch: 2/2, step 1313/7134 completed (loss: 0.046409472823143005, acc: 0.9858356714248657)
[2025-02-13 03:44:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:14][root][INFO] - Training Epoch: 2/2, step 1314/7134 completed (loss: 0.03279806300997734, acc: 0.9894737005233765)
[2025-02-13 03:44:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:14][root][INFO] - Training Epoch: 2/2, step 1315/7134 completed (loss: 0.12301786243915558, acc: 0.9706666469573975)
[2025-02-13 03:44:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:15][root][INFO] - Training Epoch: 2/2, step 1316/7134 completed (loss: 0.015257330611348152, acc: 0.9981167316436768)
[2025-02-13 03:44:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:15][root][INFO] - Training Epoch: 2/2, step 1317/7134 completed (loss: 0.049410056322813034, acc: 0.9796748161315918)
[2025-02-13 03:44:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:16][root][INFO] - Training Epoch: 2/2, step 1318/7134 completed (loss: 0.04048053175210953, acc: 0.9836512207984924)
[2025-02-13 03:44:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:16][root][INFO] - Training Epoch: 2/2, step 1319/7134 completed (loss: 0.062201693654060364, acc: 0.9794050455093384)
[2025-02-13 03:44:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:17][root][INFO] - Training Epoch: 2/2, step 1320/7134 completed (loss: 0.0842866599559784, acc: 0.975570023059845)
[2025-02-13 03:44:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:17][root][INFO] - Training Epoch: 2/2, step 1321/7134 completed (loss: 0.016802847385406494, acc: 0.9950980544090271)
[2025-02-13 03:44:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:18][root][INFO] - Training Epoch: 2/2, step 1322/7134 completed (loss: 0.03776344284415245, acc: 0.9869067072868347)
[2025-02-13 03:44:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:18][root][INFO] - Training Epoch: 2/2, step 1323/7134 completed (loss: 0.02886420674622059, acc: 0.9946428537368774)
[2025-02-13 03:44:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:18][root][INFO] - Training Epoch: 2/2, step 1324/7134 completed (loss: 0.025664694607257843, acc: 0.9917762875556946)
[2025-02-13 03:44:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:19][root][INFO] - Training Epoch: 2/2, step 1325/7134 completed (loss: 0.030679551884531975, acc: 0.9909297227859497)
[2025-02-13 03:44:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:19][root][INFO] - Training Epoch: 2/2, step 1326/7134 completed (loss: 0.035422611981630325, acc: 0.9901207685470581)
[2025-02-13 03:44:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:20][root][INFO] - Training Epoch: 2/2, step 1327/7134 completed (loss: 0.04131186008453369, acc: 0.9865067601203918)
[2025-02-13 03:44:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:20][root][INFO] - Training Epoch: 2/2, step 1328/7134 completed (loss: 0.0640728771686554, acc: 0.9837209582328796)
[2025-02-13 03:44:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:21][root][INFO] - Training Epoch: 2/2, step 1329/7134 completed (loss: 0.04889211058616638, acc: 0.9834254384040833)
[2025-02-13 03:44:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:21][root][INFO] - Training Epoch: 2/2, step 1330/7134 completed (loss: 0.013536201789975166, acc: 0.9982847571372986)
[2025-02-13 03:44:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:22][root][INFO] - Training Epoch: 2/2, step 1331/7134 completed (loss: 0.005664673168212175, acc: 0.9983713626861572)
[2025-02-13 03:44:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:22][root][INFO] - Training Epoch: 2/2, step 1332/7134 completed (loss: 0.012794388458132744, acc: 0.9956331849098206)
[2025-02-13 03:44:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:22][root][INFO] - Training Epoch: 2/2, step 1333/7134 completed (loss: 0.04439006745815277, acc: 0.9897959232330322)
[2025-02-13 03:44:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:23][root][INFO] - Training Epoch: 2/2, step 1334/7134 completed (loss: 0.08811229467391968, acc: 0.9767080545425415)
[2025-02-13 03:44:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:23][root][INFO] - Training Epoch: 2/2, step 1335/7134 completed (loss: 0.09653319418430328, acc: 0.9826897382736206)
[2025-02-13 03:44:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:24][root][INFO] - Training Epoch: 2/2, step 1336/7134 completed (loss: 0.02320346049964428, acc: 0.9918808937072754)
[2025-02-13 03:44:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:24][root][INFO] - Training Epoch: 2/2, step 1337/7134 completed (loss: 0.013614676892757416, acc: 0.995398759841919)
[2025-02-13 03:44:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:25][root][INFO] - Training Epoch: 2/2, step 1338/7134 completed (loss: 0.03869866579771042, acc: 0.9919678568840027)
[2025-02-13 03:44:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:25][root][INFO] - Training Epoch: 2/2, step 1339/7134 completed (loss: 0.037004295736551285, acc: 0.9859550595283508)
[2025-02-13 03:44:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:25][root][INFO] - Training Epoch: 2/2, step 1340/7134 completed (loss: 0.056687042117118835, acc: 0.9836065769195557)
[2025-02-13 03:44:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:26][root][INFO] - Training Epoch: 2/2, step 1341/7134 completed (loss: 0.04120759665966034, acc: 0.9887005686759949)
[2025-02-13 03:44:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:26][root][INFO] - Training Epoch: 2/2, step 1342/7134 completed (loss: 0.00554509786888957, acc: 1.0)
[2025-02-13 03:44:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:27][root][INFO] - Training Epoch: 2/2, step 1343/7134 completed (loss: 0.06643487513065338, acc: 0.9838998317718506)
[2025-02-13 03:44:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:27][root][INFO] - Training Epoch: 2/2, step 1344/7134 completed (loss: 0.02139967307448387, acc: 0.9909502267837524)
[2025-02-13 03:44:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:27][root][INFO] - Training Epoch: 2/2, step 1345/7134 completed (loss: 0.02385755628347397, acc: 0.9920424222946167)
[2025-02-13 03:44:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:28][root][INFO] - Training Epoch: 2/2, step 1346/7134 completed (loss: 0.05420850217342377, acc: 0.9866488575935364)
[2025-02-13 03:44:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:28][root][INFO] - Training Epoch: 2/2, step 1347/7134 completed (loss: 0.05293415114283562, acc: 0.9895470142364502)
[2025-02-13 03:44:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:29][root][INFO] - Training Epoch: 2/2, step 1348/7134 completed (loss: 0.021060481667518616, acc: 0.9960681796073914)
[2025-02-13 03:44:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:29][root][INFO] - Training Epoch: 2/2, step 1349/7134 completed (loss: 0.022611036896705627, acc: 0.9948717951774597)
[2025-02-13 03:44:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:30][root][INFO] - Training Epoch: 2/2, step 1350/7134 completed (loss: 0.030102068558335304, acc: 0.9927641153335571)
[2025-02-13 03:44:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:30][root][INFO] - Training Epoch: 2/2, step 1351/7134 completed (loss: 0.01581696979701519, acc: 0.994106113910675)
[2025-02-13 03:44:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:31][root][INFO] - Training Epoch: 2/2, step 1352/7134 completed (loss: 0.02542273700237274, acc: 0.9940476417541504)
[2025-02-13 03:44:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:31][root][INFO] - Training Epoch: 2/2, step 1353/7134 completed (loss: 0.045306045562028885, acc: 0.992337167263031)
[2025-02-13 03:44:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:31][root][INFO] - Training Epoch: 2/2, step 1354/7134 completed (loss: 0.008547302335500717, acc: 0.9967585206031799)
[2025-02-13 03:44:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:32][root][INFO] - Training Epoch: 2/2, step 1355/7134 completed (loss: 0.00702241063117981, acc: 0.9984423518180847)
[2025-02-13 03:44:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:32][root][INFO] - Training Epoch: 2/2, step 1356/7134 completed (loss: 0.0060780784115195274, acc: 0.9986842274665833)
[2025-02-13 03:44:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:33][root][INFO] - Training Epoch: 2/2, step 1357/7134 completed (loss: 0.009520568884909153, acc: 0.9974779486656189)
[2025-02-13 03:44:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:33][root][INFO] - Training Epoch: 2/2, step 1358/7134 completed (loss: 0.022663017734885216, acc: 0.9948052167892456)
[2025-02-13 03:44:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:34][root][INFO] - Training Epoch: 2/2, step 1359/7134 completed (loss: 0.011011611670255661, acc: 0.9959514141082764)
[2025-02-13 03:44:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:34][root][INFO] - Training Epoch: 2/2, step 1360/7134 completed (loss: 0.004552062135189772, acc: 0.9986559152603149)
[2025-02-13 03:44:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:35][root][INFO] - Training Epoch: 2/2, step 1361/7134 completed (loss: 0.011977488175034523, acc: 0.9956204295158386)
[2025-02-13 03:44:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:35][root][INFO] - Training Epoch: 2/2, step 1362/7134 completed (loss: 0.007540085818618536, acc: 0.998633861541748)
[2025-02-13 03:44:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:35][root][INFO] - Training Epoch: 2/2, step 1363/7134 completed (loss: 0.007223489228636026, acc: 0.9986594915390015)
[2025-02-13 03:44:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:36][root][INFO] - Training Epoch: 2/2, step 1364/7134 completed (loss: 0.0060248905792832375, acc: 0.9971428513526917)
[2025-02-13 03:44:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:36][root][INFO] - Training Epoch: 2/2, step 1365/7134 completed (loss: 0.013177194632589817, acc: 0.9945054650306702)
[2025-02-13 03:44:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:37][root][INFO] - Training Epoch: 2/2, step 1366/7134 completed (loss: 0.01876208372414112, acc: 0.9982143044471741)
[2025-02-13 03:44:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:37][root][INFO] - Training Epoch: 2/2, step 1367/7134 completed (loss: 0.0011558362748473883, acc: 1.0)
[2025-02-13 03:44:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:38][root][INFO] - Training Epoch: 2/2, step 1368/7134 completed (loss: 0.017843596637248993, acc: 0.9939024448394775)
[2025-02-13 03:44:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:38][root][INFO] - Training Epoch: 2/2, step 1369/7134 completed (loss: 0.025716328993439674, acc: 0.9912434220314026)
[2025-02-13 03:44:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:38][root][INFO] - Training Epoch: 2/2, step 1370/7134 completed (loss: 0.04227998107671738, acc: 0.992414653301239)
[2025-02-13 03:44:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:39][root][INFO] - Training Epoch: 2/2, step 1371/7134 completed (loss: 0.0281437486410141, acc: 0.9956331849098206)
[2025-02-13 03:44:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:39][root][INFO] - Training Epoch: 2/2, step 1372/7134 completed (loss: 0.03811156749725342, acc: 0.9925093650817871)
[2025-02-13 03:44:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:40][root][INFO] - Training Epoch: 2/2, step 1373/7134 completed (loss: 0.021898841485381126, acc: 0.9921259880065918)
[2025-02-13 03:44:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:40][root][INFO] - Training Epoch: 2/2, step 1374/7134 completed (loss: 0.01614478789269924, acc: 0.9920791983604431)
[2025-02-13 03:44:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:41][root][INFO] - Training Epoch: 2/2, step 1375/7134 completed (loss: 0.032461706548929214, acc: 0.9879807829856873)
[2025-02-13 03:44:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:41][root][INFO] - Training Epoch: 2/2, step 1376/7134 completed (loss: 0.04215066507458687, acc: 0.9874739050865173)
[2025-02-13 03:44:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:42][root][INFO] - Training Epoch: 2/2, step 1377/7134 completed (loss: 0.020158786326646805, acc: 0.9952940940856934)
[2025-02-13 03:44:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:42][root][INFO] - Training Epoch: 2/2, step 1378/7134 completed (loss: 0.05374060571193695, acc: 0.9844054579734802)
[2025-02-13 03:44:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:42][root][INFO] - Training Epoch: 2/2, step 1379/7134 completed (loss: 0.03166106715798378, acc: 0.9849462509155273)
[2025-02-13 03:44:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:43][root][INFO] - Training Epoch: 2/2, step 1380/7134 completed (loss: 0.024183811619877815, acc: 0.9962616562843323)
[2025-02-13 03:44:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:43][root][INFO] - Training Epoch: 2/2, step 1381/7134 completed (loss: 0.027261706069111824, acc: 0.9867330193519592)
[2025-02-13 03:44:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:44][root][INFO] - Training Epoch: 2/2, step 1382/7134 completed (loss: 0.049085017293691635, acc: 0.9873015880584717)
[2025-02-13 03:44:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:44][root][INFO] - Training Epoch: 2/2, step 1383/7134 completed (loss: 0.022580912336707115, acc: 0.9948365092277527)
[2025-02-13 03:44:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:45][root][INFO] - Training Epoch: 2/2, step 1384/7134 completed (loss: 0.05067942664027214, acc: 0.981697142124176)
[2025-02-13 03:44:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:45][root][INFO] - Training Epoch: 2/2, step 1385/7134 completed (loss: 0.043375346809625626, acc: 0.988990843296051)
[2025-02-13 03:44:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:45][root][INFO] - Training Epoch: 2/2, step 1386/7134 completed (loss: 0.030358456075191498, acc: 0.991150438785553)
[2025-02-13 03:44:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:46][root][INFO] - Training Epoch: 2/2, step 1387/7134 completed (loss: 0.03020430915057659, acc: 0.9928366541862488)
[2025-02-13 03:44:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:46][root][INFO] - Training Epoch: 2/2, step 1388/7134 completed (loss: 0.04095212742686272, acc: 0.9880749583244324)
[2025-02-13 03:44:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:47][root][INFO] - Training Epoch: 2/2, step 1389/7134 completed (loss: 0.03433607891201973, acc: 0.990275502204895)
[2025-02-13 03:44:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:47][root][INFO] - Training Epoch: 2/2, step 1390/7134 completed (loss: 0.023443294689059258, acc: 0.9937597513198853)
[2025-02-13 03:44:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:48][root][INFO] - Training Epoch: 2/2, step 1391/7134 completed (loss: 0.0551382340490818, acc: 0.9859402179718018)
[2025-02-13 03:44:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:48][root][INFO] - Training Epoch: 2/2, step 1392/7134 completed (loss: 0.028962960466742516, acc: 0.991349458694458)
[2025-02-13 03:44:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:48][root][INFO] - Training Epoch: 2/2, step 1393/7134 completed (loss: 0.031846530735492706, acc: 0.9861751198768616)
[2025-02-13 03:44:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:49][root][INFO] - Training Epoch: 2/2, step 1394/7134 completed (loss: 0.017849665135145187, acc: 0.9940476417541504)
[2025-02-13 03:44:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:49][root][INFO] - Training Epoch: 2/2, step 1395/7134 completed (loss: 0.04810231924057007, acc: 0.9881656765937805)
[2025-02-13 03:44:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:50][root][INFO] - Training Epoch: 2/2, step 1396/7134 completed (loss: 0.03712570294737816, acc: 0.9831081032752991)
[2025-02-13 03:44:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:50][root][INFO] - Training Epoch: 2/2, step 1397/7134 completed (loss: 0.06231318786740303, acc: 0.9831365942955017)
[2025-02-13 03:44:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:51][root][INFO] - Training Epoch: 2/2, step 1398/7134 completed (loss: 0.017862029373645782, acc: 0.993914783000946)
[2025-02-13 03:44:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:51][root][INFO] - Training Epoch: 2/2, step 1399/7134 completed (loss: 0.051531530916690826, acc: 0.9873188138008118)
[2025-02-13 03:44:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:52][root][INFO] - Training Epoch: 2/2, step 1400/7134 completed (loss: 0.025806214660406113, acc: 0.9930192232131958)
[2025-02-13 03:44:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:52][root][INFO] - Training Epoch: 2/2, step 1401/7134 completed (loss: 0.01105406228452921, acc: 0.9983525276184082)
[2025-02-13 03:44:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:52][root][INFO] - Training Epoch: 2/2, step 1402/7134 completed (loss: 0.0166473351418972, acc: 0.9948186278343201)
[2025-02-13 03:44:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:53][root][INFO] - Training Epoch: 2/2, step 1403/7134 completed (loss: 0.024438122287392616, acc: 0.9879518151283264)
[2025-02-13 03:44:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:53][root][INFO] - Training Epoch: 2/2, step 1404/7134 completed (loss: 0.014210235327482224, acc: 0.9973474740982056)
[2025-02-13 03:44:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:54][root][INFO] - Training Epoch: 2/2, step 1405/7134 completed (loss: 0.015244963578879833, acc: 0.994301974773407)
[2025-02-13 03:44:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:54][root][INFO] - Training Epoch: 2/2, step 1406/7134 completed (loss: 0.009329667314887047, acc: 0.9971264600753784)
[2025-02-13 03:44:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:55][root][INFO] - Training Epoch: 2/2, step 1407/7134 completed (loss: 0.01982472650706768, acc: 0.9921773076057434)
[2025-02-13 03:44:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:55][root][INFO] - Training Epoch: 2/2, step 1408/7134 completed (loss: 0.019841475412249565, acc: 0.9935897588729858)
[2025-02-13 03:44:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:56][root][INFO] - Training Epoch: 2/2, step 1409/7134 completed (loss: 0.009439119137823582, acc: 0.995529055595398)
[2025-02-13 03:44:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:56][root][INFO] - Training Epoch: 2/2, step 1410/7134 completed (loss: 0.022259296849370003, acc: 0.99609375)
[2025-02-13 03:44:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:57][root][INFO] - Training Epoch: 2/2, step 1411/7134 completed (loss: 0.007563029415905476, acc: 0.998670220375061)
[2025-02-13 03:44:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:57][root][INFO] - Training Epoch: 2/2, step 1412/7134 completed (loss: 0.005324471741914749, acc: 0.9988123774528503)
[2025-02-13 03:44:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:58][root][INFO] - Training Epoch: 2/2, step 1413/7134 completed (loss: 0.0323924645781517, acc: 0.9928057789802551)
[2025-02-13 03:44:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:58][root][INFO] - Training Epoch: 2/2, step 1414/7134 completed (loss: 0.010440660640597343, acc: 0.9968847632408142)
[2025-02-13 03:44:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:58][root][INFO] - Training Epoch: 2/2, step 1415/7134 completed (loss: 0.015392953529953957, acc: 0.9933775067329407)
[2025-02-13 03:44:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:59][root][INFO] - Training Epoch: 2/2, step 1416/7134 completed (loss: 0.0071186283603310585, acc: 0.9985954761505127)
[2025-02-13 03:44:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:44:59][root][INFO] - Training Epoch: 2/2, step 1417/7134 completed (loss: 0.011097105219960213, acc: 0.9948979616165161)
[2025-02-13 03:45:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:00][root][INFO] - Training Epoch: 2/2, step 1418/7134 completed (loss: 0.011797167360782623, acc: 0.9961832165718079)
[2025-02-13 03:45:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:00][root][INFO] - Training Epoch: 2/2, step 1419/7134 completed (loss: 0.01482276152819395, acc: 0.997300922870636)
[2025-02-13 03:45:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:01][root][INFO] - Training Epoch: 2/2, step 1420/7134 completed (loss: 0.01903439499437809, acc: 0.993261456489563)
[2025-02-13 03:45:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:01][root][INFO] - Training Epoch: 2/2, step 1421/7134 completed (loss: 0.009944828227162361, acc: 0.9983974099159241)
[2025-02-13 03:45:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:02][root][INFO] - Training Epoch: 2/2, step 1422/7134 completed (loss: 0.03942634537816048, acc: 0.9914529919624329)
[2025-02-13 03:45:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:02][root][INFO] - Training Epoch: 2/2, step 1423/7134 completed (loss: 0.04193799942731857, acc: 0.9877551198005676)
[2025-02-13 03:45:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:02][root][INFO] - Training Epoch: 2/2, step 1424/7134 completed (loss: 0.07879314571619034, acc: 0.9771863222122192)
[2025-02-13 03:45:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:03][root][INFO] - Training Epoch: 2/2, step 1425/7134 completed (loss: 0.02950187399983406, acc: 0.9880775213241577)
[2025-02-13 03:45:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:03][root][INFO] - Training Epoch: 2/2, step 1426/7134 completed (loss: 0.02419620007276535, acc: 0.9897172451019287)
[2025-02-13 03:45:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:04][root][INFO] - Training Epoch: 2/2, step 1427/7134 completed (loss: 0.039326027035713196, acc: 0.9833971858024597)
[2025-02-13 03:45:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:04][root][INFO] - Training Epoch: 2/2, step 1428/7134 completed (loss: 0.1826532781124115, acc: 0.9456193447113037)
[2025-02-13 03:45:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:05][root][INFO] - Training Epoch: 2/2, step 1429/7134 completed (loss: 0.024774815887212753, acc: 0.9958419799804688)
[2025-02-13 03:45:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:05][root][INFO] - Training Epoch: 2/2, step 1430/7134 completed (loss: 0.0439244769513607, acc: 0.9815497994422913)
[2025-02-13 03:45:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:06][root][INFO] - Training Epoch: 2/2, step 1431/7134 completed (loss: 0.013067184947431087, acc: 0.9959294199943542)
[2025-02-13 03:45:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:06][root][INFO] - Training Epoch: 2/2, step 1432/7134 completed (loss: 0.011784455738961697, acc: 0.9961389899253845)
[2025-02-13 03:45:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:07][root][INFO] - Training Epoch: 2/2, step 1433/7134 completed (loss: 0.01081722229719162, acc: 1.0)
[2025-02-13 03:45:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:07][root][INFO] - Training Epoch: 2/2, step 1434/7134 completed (loss: 0.04738375544548035, acc: 0.9852744340896606)
[2025-02-13 03:45:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:08][root][INFO] - Training Epoch: 2/2, step 1435/7134 completed (loss: 0.03050471842288971, acc: 0.9897360801696777)
[2025-02-13 03:45:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:08][root][INFO] - Training Epoch: 2/2, step 1436/7134 completed (loss: 0.04940926656126976, acc: 0.9758883118629456)
[2025-02-13 03:45:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:08][root][INFO] - Training Epoch: 2/2, step 1437/7134 completed (loss: 0.032617248594760895, acc: 0.9888579249382019)
[2025-02-13 03:45:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:09][root][INFO] - Training Epoch: 2/2, step 1438/7134 completed (loss: 0.028579965233802795, acc: 0.9941605925559998)
[2025-02-13 03:45:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:09][root][INFO] - Training Epoch: 2/2, step 1439/7134 completed (loss: 0.023602930828928947, acc: 0.9907235503196716)
[2025-02-13 03:45:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:10][root][INFO] - Training Epoch: 2/2, step 1440/7134 completed (loss: 0.04550051689147949, acc: 0.9759036302566528)
[2025-02-13 03:45:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:10][root][INFO] - Training Epoch: 2/2, step 1441/7134 completed (loss: 0.017133375629782677, acc: 0.9941349029541016)
[2025-02-13 03:45:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:11][root][INFO] - Training Epoch: 2/2, step 1442/7134 completed (loss: 0.030549852177500725, acc: 0.9904305934906006)
[2025-02-13 03:45:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:11][root][INFO] - Training Epoch: 2/2, step 1443/7134 completed (loss: 0.05987253040075302, acc: 0.9894551634788513)
[2025-02-13 03:45:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:12][root][INFO] - Training Epoch: 2/2, step 1444/7134 completed (loss: 0.02516418695449829, acc: 0.9961089491844177)
[2025-02-13 03:45:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:12][root][INFO] - Training Epoch: 2/2, step 1445/7134 completed (loss: 0.04244589805603027, acc: 0.9850249290466309)
[2025-02-13 03:45:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:13][root][INFO] - Training Epoch: 2/2, step 1446/7134 completed (loss: 0.015241947025060654, acc: 0.9928876161575317)
[2025-02-13 03:45:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:13][root][INFO] - Training Epoch: 2/2, step 1447/7134 completed (loss: 0.035906918346881866, acc: 0.9858155846595764)
[2025-02-13 03:45:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:13][root][INFO] - Training Epoch: 2/2, step 1448/7134 completed (loss: 0.043774932622909546, acc: 0.9872881174087524)
[2025-02-13 03:45:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:14][root][INFO] - Training Epoch: 2/2, step 1449/7134 completed (loss: 0.06894822418689728, acc: 0.9887640476226807)
[2025-02-13 03:45:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:14][root][INFO] - Training Epoch: 2/2, step 1450/7134 completed (loss: 0.022536111995577812, acc: 0.9912917017936707)
[2025-02-13 03:45:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:15][root][INFO] - Training Epoch: 2/2, step 1451/7134 completed (loss: 0.01361147128045559, acc: 0.9970717430114746)
[2025-02-13 03:45:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:15][root][INFO] - Training Epoch: 2/2, step 1452/7134 completed (loss: 0.05402020364999771, acc: 0.9878472089767456)
[2025-02-13 03:45:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:16][root][INFO] - Training Epoch: 2/2, step 1453/7134 completed (loss: 0.05525677278637886, acc: 0.9825673699378967)
[2025-02-13 03:45:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:16][root][INFO] - Training Epoch: 2/2, step 1454/7134 completed (loss: 0.04172813519835472, acc: 0.9892328381538391)
[2025-02-13 03:45:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:17][root][INFO] - Training Epoch: 2/2, step 1455/7134 completed (loss: 0.047926224768161774, acc: 0.9871559739112854)
[2025-02-13 03:45:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:17][root][INFO] - Training Epoch: 2/2, step 1456/7134 completed (loss: 0.009474867023527622, acc: 1.0)
[2025-02-13 03:45:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:18][root][INFO] - Training Epoch: 2/2, step 1457/7134 completed (loss: 0.02880154922604561, acc: 0.9953488111495972)
[2025-02-13 03:45:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:18][root][INFO] - Training Epoch: 2/2, step 1458/7134 completed (loss: 0.00913279876112938, acc: 0.9970545172691345)
[2025-02-13 03:45:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:18][root][INFO] - Training Epoch: 2/2, step 1459/7134 completed (loss: 0.028216052800416946, acc: 0.9928951859474182)
[2025-02-13 03:45:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:19][root][INFO] - Training Epoch: 2/2, step 1460/7134 completed (loss: 0.05127754062414169, acc: 0.9815789461135864)
[2025-02-13 03:45:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:19][root][INFO] - Training Epoch: 2/2, step 1461/7134 completed (loss: 0.022114424034953117, acc: 0.9888357520103455)
[2025-02-13 03:45:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:20][root][INFO] - Training Epoch: 2/2, step 1462/7134 completed (loss: 0.0728553906083107, acc: 0.9821428656578064)
[2025-02-13 03:45:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:20][root][INFO] - Training Epoch: 2/2, step 1463/7134 completed (loss: 0.04884800687432289, acc: 0.9846583008766174)
[2025-02-13 03:45:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:21][root][INFO] - Training Epoch: 2/2, step 1464/7134 completed (loss: 0.06188763305544853, acc: 0.9879879951477051)
[2025-02-13 03:45:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:21][root][INFO] - Training Epoch: 2/2, step 1465/7134 completed (loss: 0.031975481659173965, acc: 0.9921630024909973)
[2025-02-13 03:45:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:22][root][INFO] - Training Epoch: 2/2, step 1466/7134 completed (loss: 0.009453637525439262, acc: 0.9971671104431152)
[2025-02-13 03:45:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:22][root][INFO] - Training Epoch: 2/2, step 1467/7134 completed (loss: 0.010075114667415619, acc: 0.9971181750297546)
[2025-02-13 03:45:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:22][root][INFO] - Training Epoch: 2/2, step 1468/7134 completed (loss: 0.013849765993654728, acc: 0.9920739531517029)
[2025-02-13 03:45:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:23][root][INFO] - Training Epoch: 2/2, step 1469/7134 completed (loss: 0.013740587048232555, acc: 0.9946164488792419)
[2025-02-13 03:45:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:23][root][INFO] - Training Epoch: 2/2, step 1470/7134 completed (loss: 0.02459132671356201, acc: 0.9898843765258789)
[2025-02-13 03:45:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:24][root][INFO] - Training Epoch: 2/2, step 1471/7134 completed (loss: 0.01592913828790188, acc: 0.9911190271377563)
[2025-02-13 03:45:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:24][root][INFO] - Training Epoch: 2/2, step 1472/7134 completed (loss: 0.02005847916007042, acc: 0.9937106966972351)
[2025-02-13 03:45:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:25][root][INFO] - Training Epoch: 2/2, step 1473/7134 completed (loss: 0.03362021595239639, acc: 0.9925816059112549)
[2025-02-13 03:45:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:25][root][INFO] - Training Epoch: 2/2, step 1474/7134 completed (loss: 0.029725216329097748, acc: 0.9882550239562988)
[2025-02-13 03:45:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:26][root][INFO] - Training Epoch: 2/2, step 1475/7134 completed (loss: 0.0436997227370739, acc: 0.9920844435691833)
[2025-02-13 03:45:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:26][root][INFO] - Training Epoch: 2/2, step 1476/7134 completed (loss: 0.02461625635623932, acc: 0.9875776171684265)
[2025-02-13 03:45:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:26][root][INFO] - Training Epoch: 2/2, step 1477/7134 completed (loss: 0.02836722694337368, acc: 0.9912739992141724)
[2025-02-13 03:45:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:27][root][INFO] - Training Epoch: 2/2, step 1478/7134 completed (loss: 0.024347761645913124, acc: 0.9941605925559998)
[2025-02-13 03:45:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:27][root][INFO] - Training Epoch: 2/2, step 1479/7134 completed (loss: 0.01659371517598629, acc: 0.9942528605461121)
[2025-02-13 03:45:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:28][root][INFO] - Training Epoch: 2/2, step 1480/7134 completed (loss: 0.00470324931666255, acc: 1.0)
[2025-02-13 03:45:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:28][root][INFO] - Training Epoch: 2/2, step 1481/7134 completed (loss: 0.03371773660182953, acc: 0.9934210777282715)
[2025-02-13 03:45:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:29][root][INFO] - Training Epoch: 2/2, step 1482/7134 completed (loss: 0.029948873445391655, acc: 0.9928977489471436)
[2025-02-13 03:45:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:29][root][INFO] - Training Epoch: 2/2, step 1483/7134 completed (loss: 0.013066108338534832, acc: 0.9968404173851013)
[2025-02-13 03:45:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:30][root][INFO] - Training Epoch: 2/2, step 1484/7134 completed (loss: 0.0424419566988945, acc: 0.9884393215179443)
[2025-02-13 03:45:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:30][root][INFO] - Training Epoch: 2/2, step 1485/7134 completed (loss: 0.013425474986433983, acc: 0.993678867816925)
[2025-02-13 03:45:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:30][root][INFO] - Training Epoch: 2/2, step 1486/7134 completed (loss: 0.02571992017328739, acc: 0.9945873022079468)
[2025-02-13 03:45:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:31][root][INFO] - Training Epoch: 2/2, step 1487/7134 completed (loss: 0.027142580598592758, acc: 0.9960784316062927)
[2025-02-13 03:45:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:31][root][INFO] - Training Epoch: 2/2, step 1488/7134 completed (loss: 0.01931167021393776, acc: 0.9935829043388367)
[2025-02-13 03:45:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:32][root][INFO] - Training Epoch: 2/2, step 1489/7134 completed (loss: 0.013178069144487381, acc: 0.9953271150588989)
[2025-02-13 03:45:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:32][root][INFO] - Training Epoch: 2/2, step 1490/7134 completed (loss: 0.011597826145589352, acc: 0.9968253970146179)
[2025-02-13 03:45:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:33][root][INFO] - Training Epoch: 2/2, step 1491/7134 completed (loss: 0.014802053570747375, acc: 0.9965517520904541)
[2025-02-13 03:45:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:33][root][INFO] - Training Epoch: 2/2, step 1492/7134 completed (loss: 0.044721752405166626, acc: 0.988399088382721)
[2025-02-13 03:45:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:34][root][INFO] - Training Epoch: 2/2, step 1493/7134 completed (loss: 0.02559823729097843, acc: 0.9942396283149719)
[2025-02-13 03:45:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:34][root][INFO] - Training Epoch: 2/2, step 1494/7134 completed (loss: 0.010180647484958172, acc: 0.9951338171958923)
[2025-02-13 03:45:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:35][root][INFO] - Training Epoch: 2/2, step 1495/7134 completed (loss: 0.009410621598362923, acc: 0.9964072108268738)
[2025-02-13 03:45:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:35][root][INFO] - Training Epoch: 2/2, step 1496/7134 completed (loss: 0.03294166177511215, acc: 0.9900285005569458)
[2025-02-13 03:45:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:36][root][INFO] - Training Epoch: 2/2, step 1497/7134 completed (loss: 0.018226679414510727, acc: 0.9943438768386841)
[2025-02-13 03:45:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:36][root][INFO] - Training Epoch: 2/2, step 1498/7134 completed (loss: 0.0077730948105454445, acc: 0.9960474371910095)
[2025-02-13 03:45:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:37][root][INFO] - Training Epoch: 2/2, step 1499/7134 completed (loss: 0.006701907142996788, acc: 0.9976274967193604)
[2025-02-13 03:45:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:37][root][INFO] - Training Epoch: 2/2, step 1500/7134 completed (loss: 0.008191120810806751, acc: 0.9950330853462219)
[2025-02-13 03:45:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:37][root][INFO] - Training Epoch: 2/2, step 1501/7134 completed (loss: 0.025755608454346657, acc: 0.99262535572052)
[2025-02-13 03:45:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:38][root][INFO] - Training Epoch: 2/2, step 1502/7134 completed (loss: 0.0198587104678154, acc: 0.994363009929657)
[2025-02-13 03:45:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:38][root][INFO] - Training Epoch: 2/2, step 1503/7134 completed (loss: 0.050894033163785934, acc: 0.984375)
[2025-02-13 03:45:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:39][root][INFO] - Training Epoch: 2/2, step 1504/7134 completed (loss: 0.020280539989471436, acc: 0.99622642993927)
[2025-02-13 03:45:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:39][root][INFO] - Training Epoch: 2/2, step 1505/7134 completed (loss: 0.024262551218271255, acc: 0.9922118186950684)
[2025-02-13 03:45:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:40][root][INFO] - Training Epoch: 2/2, step 1506/7134 completed (loss: 0.03312009572982788, acc: 0.9893617033958435)
[2025-02-13 03:45:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:40][root][INFO] - Training Epoch: 2/2, step 1507/7134 completed (loss: 0.019925789907574654, acc: 0.9947643876075745)
[2025-02-13 03:45:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:41][root][INFO] - Training Epoch: 2/2, step 1508/7134 completed (loss: 0.01534288004040718, acc: 0.9967177510261536)
[2025-02-13 03:45:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:41][root][INFO] - Training Epoch: 2/2, step 1509/7134 completed (loss: 0.012970023788511753, acc: 0.9957982897758484)
[2025-02-13 03:45:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:42][root][INFO] - Training Epoch: 2/2, step 1510/7134 completed (loss: 0.02836204133927822, acc: 0.9916666746139526)
[2025-02-13 03:45:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:42][root][INFO] - Training Epoch: 2/2, step 1511/7134 completed (loss: 0.050412215292453766, acc: 0.9872340559959412)
[2025-02-13 03:45:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:43][root][INFO] - Training Epoch: 2/2, step 1512/7134 completed (loss: 0.013524395413696766, acc: 0.9984779357910156)
[2025-02-13 03:45:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:43][root][INFO] - Training Epoch: 2/2, step 1513/7134 completed (loss: 0.010612182319164276, acc: 0.9973545074462891)
[2025-02-13 03:45:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:43][root][INFO] - Training Epoch: 2/2, step 1514/7134 completed (loss: 0.03724784031510353, acc: 0.9937888383865356)
[2025-02-13 03:45:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:44][root][INFO] - Training Epoch: 2/2, step 1515/7134 completed (loss: 0.01710408926010132, acc: 0.9929906725883484)
[2025-02-13 03:45:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:44][root][INFO] - Training Epoch: 2/2, step 1516/7134 completed (loss: 0.03486920893192291, acc: 0.9886040091514587)
[2025-02-13 03:45:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:45][root][INFO] - Training Epoch: 2/2, step 1517/7134 completed (loss: 0.048716142773628235, acc: 0.9873949289321899)
[2025-02-13 03:45:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:45][root][INFO] - Training Epoch: 2/2, step 1518/7134 completed (loss: 0.025105023756623268, acc: 0.9903614521026611)
[2025-02-13 03:45:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:46][root][INFO] - Training Epoch: 2/2, step 1519/7134 completed (loss: 0.006586973089724779, acc: 0.9976958632469177)
[2025-02-13 03:45:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:46][root][INFO] - Training Epoch: 2/2, step 1520/7134 completed (loss: 0.01724618673324585, acc: 0.9937597513198853)
[2025-02-13 03:45:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:47][root][INFO] - Training Epoch: 2/2, step 1521/7134 completed (loss: 0.06857657432556152, acc: 0.9795158505439758)
[2025-02-13 03:45:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:47][root][INFO] - Training Epoch: 2/2, step 1522/7134 completed (loss: 0.058855026960372925, acc: 0.9807322025299072)
[2025-02-13 03:45:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:48][root][INFO] - Training Epoch: 2/2, step 1523/7134 completed (loss: 0.07577136904001236, acc: 0.9760119915008545)
[2025-02-13 03:45:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:48][root][INFO] - Training Epoch: 2/2, step 1524/7134 completed (loss: 0.018447687849402428, acc: 0.9966443181037903)
[2025-02-13 03:45:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:48][root][INFO] - Training Epoch: 2/2, step 1525/7134 completed (loss: 0.019296882674098015, acc: 0.9932126402854919)
[2025-02-13 03:45:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:49][root][INFO] - Training Epoch: 2/2, step 1526/7134 completed (loss: 0.06040553003549576, acc: 0.9825834631919861)
[2025-02-13 03:45:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:49][root][INFO] - Training Epoch: 2/2, step 1527/7134 completed (loss: 0.061242327094078064, acc: 0.989983320236206)
[2025-02-13 03:45:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:50][root][INFO] - Training Epoch: 2/2, step 1528/7134 completed (loss: 0.05923839658498764, acc: 0.9838969111442566)
[2025-02-13 03:45:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:50][root][INFO] - Training Epoch: 2/2, step 1529/7134 completed (loss: 0.01494034007191658, acc: 0.9964328408241272)
[2025-02-13 03:45:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:51][root][INFO] - Training Epoch: 2/2, step 1530/7134 completed (loss: 0.05112408474087715, acc: 0.990439772605896)
[2025-02-13 03:45:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:51][root][INFO] - Training Epoch: 2/2, step 1531/7134 completed (loss: 0.11323484778404236, acc: 0.9680851101875305)
[2025-02-13 03:45:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:52][root][INFO] - Training Epoch: 2/2, step 1532/7134 completed (loss: 0.03789083659648895, acc: 0.9907514452934265)
[2025-02-13 03:45:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:52][root][INFO] - Training Epoch: 2/2, step 1533/7134 completed (loss: 0.02713100239634514, acc: 0.9875518679618835)
[2025-02-13 03:45:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:53][root][INFO] - Training Epoch: 2/2, step 1534/7134 completed (loss: 0.05472677946090698, acc: 0.9875346422195435)
[2025-02-13 03:45:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:53][root][INFO] - Training Epoch: 2/2, step 1535/7134 completed (loss: 0.030389603227376938, acc: 0.9892215728759766)
[2025-02-13 03:45:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:54][root][INFO] - Training Epoch: 2/2, step 1536/7134 completed (loss: 0.03225015848875046, acc: 0.9886220097541809)
[2025-02-13 03:45:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:54][root][INFO] - Training Epoch: 2/2, step 1537/7134 completed (loss: 0.03633957356214523, acc: 0.9895287752151489)
[2025-02-13 03:45:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:55][root][INFO] - Training Epoch: 2/2, step 1538/7134 completed (loss: 0.054974861443042755, acc: 0.98591548204422)
[2025-02-13 03:45:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:55][root][INFO] - Training Epoch: 2/2, step 1539/7134 completed (loss: 0.029568811878561974, acc: 0.990777313709259)
[2025-02-13 03:45:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:56][root][INFO] - Training Epoch: 2/2, step 1540/7134 completed (loss: 0.02249867096543312, acc: 0.9932975769042969)
[2025-02-13 03:45:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:56][root][INFO] - Training Epoch: 2/2, step 1541/7134 completed (loss: 0.030182870104908943, acc: 0.99245285987854)
[2025-02-13 03:45:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:57][root][INFO] - Training Epoch: 2/2, step 1542/7134 completed (loss: 0.020793253555893898, acc: 0.993261456489563)
[2025-02-13 03:45:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:57][root][INFO] - Training Epoch: 2/2, step 1543/7134 completed (loss: 0.02061278000473976, acc: 0.9934425950050354)
[2025-02-13 03:45:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:58][root][INFO] - Training Epoch: 2/2, step 1544/7134 completed (loss: 0.016839373856782913, acc: 0.9927184581756592)
[2025-02-13 03:45:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:58][root][INFO] - Training Epoch: 2/2, step 1545/7134 completed (loss: 0.022991998121142387, acc: 0.991391658782959)
[2025-02-13 03:45:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:59][root][INFO] - Training Epoch: 2/2, step 1546/7134 completed (loss: 0.06080067157745361, acc: 0.9859747290611267)
[2025-02-13 03:45:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:59][root][INFO] - Training Epoch: 2/2, step 1547/7134 completed (loss: 0.025817623361945152, acc: 0.9910447597503662)
[2025-02-13 03:45:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:45:59][root][INFO] - Training Epoch: 2/2, step 1548/7134 completed (loss: 0.022649602964520454, acc: 0.9954493641853333)
[2025-02-13 03:46:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:00][root][INFO] - Training Epoch: 2/2, step 1549/7134 completed (loss: 0.020406153053045273, acc: 0.9900332093238831)
[2025-02-13 03:46:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:00][root][INFO] - Training Epoch: 2/2, step 1550/7134 completed (loss: 0.016236064955592155, acc: 0.9949302673339844)
[2025-02-13 03:46:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:01][root][INFO] - Training Epoch: 2/2, step 1551/7134 completed (loss: 0.017334016039967537, acc: 0.9935064911842346)
[2025-02-13 03:46:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:01][root][INFO] - Training Epoch: 2/2, step 1552/7134 completed (loss: 0.0256806593388319, acc: 0.993534505367279)
[2025-02-13 03:46:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:02][root][INFO] - Training Epoch: 2/2, step 1553/7134 completed (loss: 0.031215809285640717, acc: 0.9934980273246765)
[2025-02-13 03:46:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:02][root][INFO] - Training Epoch: 2/2, step 1554/7134 completed (loss: 0.024202656000852585, acc: 0.9932356476783752)
[2025-02-13 03:46:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:03][root][INFO] - Training Epoch: 2/2, step 1555/7134 completed (loss: 0.033551912754774094, acc: 0.9910414218902588)
[2025-02-13 03:46:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:03][root][INFO] - Training Epoch: 2/2, step 1556/7134 completed (loss: 0.020095616579055786, acc: 0.9941245317459106)
[2025-02-13 03:46:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:04][root][INFO] - Training Epoch: 2/2, step 1557/7134 completed (loss: 0.013361109420657158, acc: 0.9955257177352905)
[2025-02-13 03:46:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:04][root][INFO] - Training Epoch: 2/2, step 1558/7134 completed (loss: 0.029878806322813034, acc: 0.9887482523918152)
[2025-02-13 03:46:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:05][root][INFO] - Training Epoch: 2/2, step 1559/7134 completed (loss: 0.005054168403148651, acc: 1.0)
[2025-02-13 03:46:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:05][root][INFO] - Training Epoch: 2/2, step 1560/7134 completed (loss: 0.02274908497929573, acc: 0.9936467409133911)
[2025-02-13 03:46:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:05][root][INFO] - Training Epoch: 2/2, step 1561/7134 completed (loss: 0.06900551915168762, acc: 0.9848320484161377)
[2025-02-13 03:46:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:06][root][INFO] - Training Epoch: 2/2, step 1562/7134 completed (loss: 0.04092446714639664, acc: 0.9863842725753784)
[2025-02-13 03:46:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:06][root][INFO] - Training Epoch: 2/2, step 1563/7134 completed (loss: 0.024977367371320724, acc: 0.99314284324646)
[2025-02-13 03:46:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:07][root][INFO] - Training Epoch: 2/2, step 1564/7134 completed (loss: 0.052938979119062424, acc: 0.9839109182357788)
[2025-02-13 03:46:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:07][root][INFO] - Training Epoch: 2/2, step 1565/7134 completed (loss: 0.05087724328041077, acc: 0.9831932783126831)
[2025-02-13 03:46:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:08][root][INFO] - Training Epoch: 2/2, step 1566/7134 completed (loss: 0.029303835704922676, acc: 0.9904240965843201)
[2025-02-13 03:46:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:08][root][INFO] - Training Epoch: 2/2, step 1567/7134 completed (loss: 0.03373406454920769, acc: 0.9925187230110168)
[2025-02-13 03:46:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:09][root][INFO] - Training Epoch: 2/2, step 1568/7134 completed (loss: 0.04416893422603607, acc: 0.9881656765937805)
[2025-02-13 03:46:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:09][root][INFO] - Training Epoch: 2/2, step 1569/7134 completed (loss: 0.03210761770606041, acc: 0.9926900863647461)
[2025-02-13 03:46:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:10][root][INFO] - Training Epoch: 2/2, step 1570/7134 completed (loss: 0.08000678569078445, acc: 0.9834619760513306)
[2025-02-13 03:46:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:10][root][INFO] - Training Epoch: 2/2, step 1571/7134 completed (loss: 0.04957671836018562, acc: 0.9881376028060913)
[2025-02-13 03:46:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:10][root][INFO] - Training Epoch: 2/2, step 1572/7134 completed (loss: 0.027358200401067734, acc: 0.9905771613121033)
[2025-02-13 03:46:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:11][root][INFO] - Training Epoch: 2/2, step 1573/7134 completed (loss: 0.013599228113889694, acc: 0.9970972537994385)
[2025-02-13 03:46:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:11][root][INFO] - Training Epoch: 2/2, step 1574/7134 completed (loss: 0.04034984111785889, acc: 0.9879649877548218)
[2025-02-13 03:46:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:12][root][INFO] - Training Epoch: 2/2, step 1575/7134 completed (loss: 0.025217406451702118, acc: 0.9955654144287109)
[2025-02-13 03:46:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:12][root][INFO] - Training Epoch: 2/2, step 1576/7134 completed (loss: 0.028329944238066673, acc: 0.9941588640213013)
[2025-02-13 03:46:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:13][root][INFO] - Training Epoch: 2/2, step 1577/7134 completed (loss: 0.029329106211662292, acc: 0.9929328560829163)
[2025-02-13 03:46:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:13][root][INFO] - Training Epoch: 2/2, step 1578/7134 completed (loss: 0.02735748142004013, acc: 0.9918808937072754)
[2025-02-13 03:46:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:14][root][INFO] - Training Epoch: 2/2, step 1579/7134 completed (loss: 0.019542984664440155, acc: 0.9953917264938354)
[2025-02-13 03:46:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:14][root][INFO] - Training Epoch: 2/2, step 1580/7134 completed (loss: 0.047778528183698654, acc: 0.9867841601371765)
[2025-02-13 03:46:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:15][root][INFO] - Training Epoch: 2/2, step 1581/7134 completed (loss: 0.06594721227884293, acc: 0.9861111044883728)
[2025-02-13 03:46:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:15][root][INFO] - Training Epoch: 2/2, step 1582/7134 completed (loss: 0.03143521398305893, acc: 0.9937402009963989)
[2025-02-13 03:46:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:15][root][INFO] - Training Epoch: 2/2, step 1583/7134 completed (loss: 0.012310911901295185, acc: 0.9971346855163574)
[2025-02-13 03:46:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:16][root][INFO] - Training Epoch: 2/2, step 1584/7134 completed (loss: 0.023127593100070953, acc: 0.9925053715705872)
[2025-02-13 03:46:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:16][root][INFO] - Training Epoch: 2/2, step 1585/7134 completed (loss: 0.014746787026524544, acc: 0.997644305229187)
[2025-02-13 03:46:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:17][root][INFO] - Training Epoch: 2/2, step 1586/7134 completed (loss: 0.03240841627120972, acc: 0.9928740859031677)
[2025-02-13 03:46:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:17][root][INFO] - Training Epoch: 2/2, step 1587/7134 completed (loss: 0.01988855004310608, acc: 0.9937205910682678)
[2025-02-13 03:46:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:18][root][INFO] - Training Epoch: 2/2, step 1588/7134 completed (loss: 0.044539690017700195, acc: 0.9880159497261047)
[2025-02-13 03:46:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:18][root][INFO] - Training Epoch: 2/2, step 1589/7134 completed (loss: 0.026265868917107582, acc: 0.992443323135376)
[2025-02-13 03:46:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:19][root][INFO] - Training Epoch: 2/2, step 1590/7134 completed (loss: 0.03351520001888275, acc: 0.9894259572029114)
[2025-02-13 03:46:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:19][root][INFO] - Training Epoch: 2/2, step 1591/7134 completed (loss: 0.02417631447315216, acc: 0.9915397763252258)
[2025-02-13 03:46:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:20][root][INFO] - Training Epoch: 2/2, step 1592/7134 completed (loss: 0.02581038326025009, acc: 0.9900142550468445)
[2025-02-13 03:46:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:20][root][INFO] - Training Epoch: 2/2, step 1593/7134 completed (loss: 0.03955236077308655, acc: 0.9919540286064148)
[2025-02-13 03:46:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:21][root][INFO] - Training Epoch: 2/2, step 1594/7134 completed (loss: 0.016935106366872787, acc: 0.9966942071914673)
[2025-02-13 03:46:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:21][root][INFO] - Training Epoch: 2/2, step 1595/7134 completed (loss: 0.023884352296590805, acc: 0.9946236610412598)
[2025-02-13 03:46:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:22][root][INFO] - Training Epoch: 2/2, step 1596/7134 completed (loss: 0.04624117910861969, acc: 0.9905511736869812)
[2025-02-13 03:46:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:22][root][INFO] - Training Epoch: 2/2, step 1597/7134 completed (loss: 0.011627021245658398, acc: 0.9971346855163574)
[2025-02-13 03:46:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:22][root][INFO] - Training Epoch: 2/2, step 1598/7134 completed (loss: 0.07311784476041794, acc: 0.9780439138412476)
[2025-02-13 03:46:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:23][root][INFO] - Training Epoch: 2/2, step 1599/7134 completed (loss: 0.07817152887582779, acc: 0.9768595099449158)
[2025-02-13 03:46:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:23][root][INFO] - Training Epoch: 2/2, step 1600/7134 completed (loss: 0.0389363169670105, acc: 0.994226336479187)
[2025-02-13 03:46:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:24][root][INFO] - Training Epoch: 2/2, step 1601/7134 completed (loss: 0.016965622082352638, acc: 0.9944979548454285)
[2025-02-13 03:46:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:24][root][INFO] - Training Epoch: 2/2, step 1602/7134 completed (loss: 0.05099448189139366, acc: 0.9924585223197937)
[2025-02-13 03:46:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:25][root][INFO] - Training Epoch: 2/2, step 1603/7134 completed (loss: 0.05715766176581383, acc: 0.9897698163986206)
[2025-02-13 03:46:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:25][root][INFO] - Training Epoch: 2/2, step 1604/7134 completed (loss: 0.025990769267082214, acc: 0.9933686852455139)
[2025-02-13 03:46:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:26][root][INFO] - Training Epoch: 2/2, step 1605/7134 completed (loss: 0.02082499861717224, acc: 0.9934810996055603)
[2025-02-13 03:46:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:26][root][INFO] - Training Epoch: 2/2, step 1606/7134 completed (loss: 0.016655515879392624, acc: 0.9969372153282166)
[2025-02-13 03:46:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:26][root][INFO] - Training Epoch: 2/2, step 1607/7134 completed (loss: 0.011319201439619064, acc: 0.9918533563613892)
[2025-02-13 03:46:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:27][root][INFO] - Training Epoch: 2/2, step 1608/7134 completed (loss: 0.010835797525942326, acc: 0.9981412887573242)
[2025-02-13 03:46:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:27][root][INFO] - Training Epoch: 2/2, step 1609/7134 completed (loss: 0.03085549734532833, acc: 0.9937694668769836)
[2025-02-13 03:46:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:28][root][INFO] - Training Epoch: 2/2, step 1610/7134 completed (loss: 0.036235593259334564, acc: 0.9933221936225891)
[2025-02-13 03:46:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:28][root][INFO] - Training Epoch: 2/2, step 1611/7134 completed (loss: 0.0453387126326561, acc: 0.988095223903656)
[2025-02-13 03:46:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:29][root][INFO] - Training Epoch: 2/2, step 1612/7134 completed (loss: 0.05690498277544975, acc: 0.9860050678253174)
[2025-02-13 03:46:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:29][root][INFO] - Training Epoch: 2/2, step 1613/7134 completed (loss: 0.04749898612499237, acc: 0.9868247509002686)
[2025-02-13 03:46:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:29][root][INFO] - Training Epoch: 2/2, step 1614/7134 completed (loss: 0.031970053911209106, acc: 0.993446946144104)
[2025-02-13 03:46:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:30][root][INFO] - Training Epoch: 2/2, step 1615/7134 completed (loss: 0.04533596709370613, acc: 0.9865771532058716)
[2025-02-13 03:46:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:30][root][INFO] - Training Epoch: 2/2, step 1616/7134 completed (loss: 0.019929135218262672, acc: 0.9956709742546082)
[2025-02-13 03:46:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:31][root][INFO] - Training Epoch: 2/2, step 1617/7134 completed (loss: 0.04871068522334099, acc: 0.9921996593475342)
[2025-02-13 03:46:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:31][root][INFO] - Training Epoch: 2/2, step 1618/7134 completed (loss: 0.048084013164043427, acc: 0.9894737005233765)
[2025-02-13 03:46:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:32][root][INFO] - Training Epoch: 2/2, step 1619/7134 completed (loss: 0.03452490270137787, acc: 0.9891008138656616)
[2025-02-13 03:46:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:32][root][INFO] - Training Epoch: 2/2, step 1620/7134 completed (loss: 0.042863879352808, acc: 0.9848713874816895)
[2025-02-13 03:46:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:33][root][INFO] - Training Epoch: 2/2, step 1621/7134 completed (loss: 0.06700921803712845, acc: 0.9783783555030823)
[2025-02-13 03:46:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:33][root][INFO] - Training Epoch: 2/2, step 1622/7134 completed (loss: 0.042871441692113876, acc: 0.9930362105369568)
[2025-02-13 03:46:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:34][root][INFO] - Training Epoch: 2/2, step 1623/7134 completed (loss: 0.03438659757375717, acc: 0.9915730357170105)
[2025-02-13 03:46:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:34][root][INFO] - Training Epoch: 2/2, step 1624/7134 completed (loss: 0.050676532089710236, acc: 0.98531574010849)
[2025-02-13 03:46:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:34][root][INFO] - Training Epoch: 2/2, step 1625/7134 completed (loss: 0.025879792869091034, acc: 0.9914529919624329)
[2025-02-13 03:46:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:35][root][INFO] - Training Epoch: 2/2, step 1626/7134 completed (loss: 0.0576188750565052, acc: 0.9814814925193787)
[2025-02-13 03:46:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:35][root][INFO] - Training Epoch: 2/2, step 1627/7134 completed (loss: 0.02015427127480507, acc: 0.9908758997917175)
[2025-02-13 03:46:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:36][root][INFO] - Training Epoch: 2/2, step 1628/7134 completed (loss: 0.008817874826490879, acc: 0.9970845580101013)
[2025-02-13 03:46:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:36][root][INFO] - Training Epoch: 2/2, step 1629/7134 completed (loss: 0.020999645814299583, acc: 0.9890909194946289)
[2025-02-13 03:46:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:37][root][INFO] - Training Epoch: 2/2, step 1630/7134 completed (loss: 0.02122686058282852, acc: 0.995555579662323)
[2025-02-13 03:46:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:37][root][INFO] - Training Epoch: 2/2, step 1631/7134 completed (loss: 0.056794095784425735, acc: 0.9855305552482605)
[2025-02-13 03:46:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:37][root][INFO] - Training Epoch: 2/2, step 1632/7134 completed (loss: 0.01314803771674633, acc: 0.9966832399368286)
[2025-02-13 03:46:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:38][root][INFO] - Training Epoch: 2/2, step 1633/7134 completed (loss: 0.019604165107011795, acc: 0.9921507239341736)
[2025-02-13 03:46:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:38][root][INFO] - Training Epoch: 2/2, step 1634/7134 completed (loss: 0.011661821976304054, acc: 0.9984685778617859)
[2025-02-13 03:46:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:39][root][INFO] - Training Epoch: 2/2, step 1635/7134 completed (loss: 0.011217353865504265, acc: 0.9938900470733643)
[2025-02-13 03:46:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:39][root][INFO] - Training Epoch: 2/2, step 1636/7134 completed (loss: 0.022483963519334793, acc: 0.9923809766769409)
[2025-02-13 03:46:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:40][root][INFO] - Training Epoch: 2/2, step 1637/7134 completed (loss: 0.01231553964316845, acc: 0.9985875487327576)
[2025-02-13 03:46:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:40][root][INFO] - Training Epoch: 2/2, step 1638/7134 completed (loss: 0.047984518110752106, acc: 0.9779286980628967)
[2025-02-13 03:46:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:41][root][INFO] - Training Epoch: 2/2, step 1639/7134 completed (loss: 0.09172014147043228, acc: 0.9763663411140442)
[2025-02-13 03:46:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:41][root][INFO] - Training Epoch: 2/2, step 1640/7134 completed (loss: 0.02777392603456974, acc: 0.9923780560493469)
[2025-02-13 03:46:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:41][root][INFO] - Training Epoch: 2/2, step 1641/7134 completed (loss: 0.010517013259232044, acc: 0.9956896305084229)
[2025-02-13 03:46:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:42][root][INFO] - Training Epoch: 2/2, step 1642/7134 completed (loss: 0.06755776703357697, acc: 0.9842105507850647)
[2025-02-13 03:46:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:42][root][INFO] - Training Epoch: 2/2, step 1643/7134 completed (loss: 0.09276691824197769, acc: 0.9774696826934814)
[2025-02-13 03:46:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:43][root][INFO] - Training Epoch: 2/2, step 1644/7134 completed (loss: 0.04790622740983963, acc: 0.9856584072113037)
[2025-02-13 03:46:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:43][root][INFO] - Training Epoch: 2/2, step 1645/7134 completed (loss: 0.04909925535321236, acc: 0.9856321811676025)
[2025-02-13 03:46:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:44][root][INFO] - Training Epoch: 2/2, step 1646/7134 completed (loss: 0.06147889047861099, acc: 0.9808306694030762)
[2025-02-13 03:46:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:44][root][INFO] - Training Epoch: 2/2, step 1647/7134 completed (loss: 0.027592061087489128, acc: 0.9899216294288635)
[2025-02-13 03:46:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:45][root][INFO] - Training Epoch: 2/2, step 1648/7134 completed (loss: 0.0522504560649395, acc: 0.9840425252914429)
[2025-02-13 03:46:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:45][root][INFO] - Training Epoch: 2/2, step 1649/7134 completed (loss: 0.026691192761063576, acc: 0.9922308325767517)
[2025-02-13 03:46:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:46][root][INFO] - Training Epoch: 2/2, step 1650/7134 completed (loss: 0.02362985536456108, acc: 0.9922680258750916)
[2025-02-13 03:46:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:46][root][INFO] - Training Epoch: 2/2, step 1651/7134 completed (loss: 0.04564950615167618, acc: 0.98893803358078)
[2025-02-13 03:46:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:47][root][INFO] - Training Epoch: 2/2, step 1652/7134 completed (loss: 0.017822682857513428, acc: 0.994547426700592)
[2025-02-13 03:46:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:47][root][INFO] - Training Epoch: 2/2, step 1653/7134 completed (loss: 0.0267721526324749, acc: 0.993773341178894)
[2025-02-13 03:46:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:48][root][INFO] - Training Epoch: 2/2, step 1654/7134 completed (loss: 0.02428349480032921, acc: 0.9928644299507141)
[2025-02-13 03:46:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:48][root][INFO] - Training Epoch: 2/2, step 1655/7134 completed (loss: 0.032417163252830505, acc: 0.9876084327697754)
[2025-02-13 03:46:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:49][root][INFO] - Training Epoch: 2/2, step 1656/7134 completed (loss: 0.02823992818593979, acc: 0.9932432174682617)
[2025-02-13 03:46:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:49][root][INFO] - Training Epoch: 2/2, step 1657/7134 completed (loss: 0.024380479007959366, acc: 0.9945205450057983)
[2025-02-13 03:46:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:50][root][INFO] - Training Epoch: 2/2, step 1658/7134 completed (loss: 0.02561672404408455, acc: 0.9928571581840515)
[2025-02-13 03:46:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:50][root][INFO] - Training Epoch: 2/2, step 1659/7134 completed (loss: 0.034816283732652664, acc: 0.9905213117599487)
[2025-02-13 03:46:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:51][root][INFO] - Training Epoch: 2/2, step 1660/7134 completed (loss: 0.027649633586406708, acc: 0.9939613342285156)
[2025-02-13 03:46:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:51][root][INFO] - Training Epoch: 2/2, step 1661/7134 completed (loss: 0.0313376784324646, acc: 0.9901531934738159)
[2025-02-13 03:46:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:52][root][INFO] - Training Epoch: 2/2, step 1662/7134 completed (loss: 0.039254870265722275, acc: 0.9893955588340759)
[2025-02-13 03:46:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:52][root][INFO] - Training Epoch: 2/2, step 1663/7134 completed (loss: 0.027852710336446762, acc: 0.9948612451553345)
[2025-02-13 03:46:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:52][root][INFO] - Training Epoch: 2/2, step 1664/7134 completed (loss: 0.01103039737790823, acc: 0.9952324032783508)
[2025-02-13 03:46:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:53][root][INFO] - Training Epoch: 2/2, step 1665/7134 completed (loss: 0.019139474257826805, acc: 0.9928057789802551)
[2025-02-13 03:46:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:54][root][INFO] - Training Epoch: 2/2, step 1666/7134 completed (loss: 0.01323554664850235, acc: 0.9940119981765747)
[2025-02-13 03:46:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:54][root][INFO] - Training Epoch: 2/2, step 1667/7134 completed (loss: 0.02116098813712597, acc: 0.9952493906021118)
[2025-02-13 03:46:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:55][root][INFO] - Training Epoch: 2/2, step 1668/7134 completed (loss: 0.015261063352227211, acc: 0.993697464466095)
[2025-02-13 03:46:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:55][root][INFO] - Training Epoch: 2/2, step 1669/7134 completed (loss: 0.013520220294594765, acc: 0.9952918887138367)
[2025-02-13 03:46:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:56][root][INFO] - Training Epoch: 2/2, step 1670/7134 completed (loss: 0.03883242607116699, acc: 0.9914122223854065)
[2025-02-13 03:46:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:56][root][INFO] - Training Epoch: 2/2, step 1671/7134 completed (loss: 0.05492134392261505, acc: 0.9880525469779968)
[2025-02-13 03:46:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:57][root][INFO] - Training Epoch: 2/2, step 1672/7134 completed (loss: 0.030264845117926598, acc: 0.9880478382110596)
[2025-02-13 03:46:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:57][root][INFO] - Training Epoch: 2/2, step 1673/7134 completed (loss: 0.04008258879184723, acc: 0.9932795763015747)
[2025-02-13 03:46:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:58][root][INFO] - Training Epoch: 2/2, step 1674/7134 completed (loss: 0.02992875687777996, acc: 0.99301677942276)
[2025-02-13 03:46:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:58][root][INFO] - Training Epoch: 2/2, step 1675/7134 completed (loss: 0.06271311640739441, acc: 0.9877551198005676)
[2025-02-13 03:46:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:58][root][INFO] - Training Epoch: 2/2, step 1676/7134 completed (loss: 0.028614874929189682, acc: 0.9917126893997192)
[2025-02-13 03:46:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:59][root][INFO] - Training Epoch: 2/2, step 1677/7134 completed (loss: 0.0630476102232933, acc: 0.9819967150688171)
[2025-02-13 03:46:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:46:59][root][INFO] - Training Epoch: 2/2, step 1678/7134 completed (loss: 0.01330394484102726, acc: 0.9958904385566711)
[2025-02-13 03:47:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:00][root][INFO] - Training Epoch: 2/2, step 1679/7134 completed (loss: 0.034561630338430405, acc: 0.9882869720458984)
[2025-02-13 03:47:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:00][root][INFO] - Training Epoch: 2/2, step 1680/7134 completed (loss: 0.026072248816490173, acc: 0.9932975769042969)
[2025-02-13 03:47:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:01][root][INFO] - Training Epoch: 2/2, step 1681/7134 completed (loss: 0.026679158210754395, acc: 0.9943181872367859)
[2025-02-13 03:47:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:01][root][INFO] - Training Epoch: 2/2, step 1682/7134 completed (loss: 0.030129408463835716, acc: 0.9884726405143738)
[2025-02-13 03:47:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:02][root][INFO] - Training Epoch: 2/2, step 1683/7134 completed (loss: 0.025674132630228996, acc: 0.9935275316238403)
[2025-02-13 03:47:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:02][root][INFO] - Training Epoch: 2/2, step 1684/7134 completed (loss: 0.02697962149977684, acc: 0.9931507110595703)
[2025-02-13 03:47:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:03][root][INFO] - Training Epoch: 2/2, step 1685/7134 completed (loss: 0.03397355228662491, acc: 0.9847009778022766)
[2025-02-13 03:47:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:03][root][INFO] - Training Epoch: 2/2, step 1686/7134 completed (loss: 0.023312892764806747, acc: 0.9932432174682617)
[2025-02-13 03:47:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:03][root][INFO] - Training Epoch: 2/2, step 1687/7134 completed (loss: 0.018331831321120262, acc: 0.9923954606056213)
[2025-02-13 03:47:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:04][root][INFO] - Training Epoch: 2/2, step 1688/7134 completed (loss: 0.031005986034870148, acc: 0.9938176274299622)
[2025-02-13 03:47:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:04][root][INFO] - Training Epoch: 2/2, step 1689/7134 completed (loss: 0.02264869213104248, acc: 0.9956709742546082)
[2025-02-13 03:47:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:05][root][INFO] - Training Epoch: 2/2, step 1690/7134 completed (loss: 0.018861357122659683, acc: 0.9959349632263184)
[2025-02-13 03:47:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:05][root][INFO] - Training Epoch: 2/2, step 1691/7134 completed (loss: 0.0034476681612432003, acc: 0.998344361782074)
[2025-02-13 03:47:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:06][root][INFO] - Training Epoch: 2/2, step 1692/7134 completed (loss: 0.041587334126234055, acc: 0.9831365942955017)
[2025-02-13 03:47:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:06][root][INFO] - Training Epoch: 2/2, step 1693/7134 completed (loss: 0.036986637860536575, acc: 0.992514967918396)
[2025-02-13 03:47:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:06][root][INFO] - Training Epoch: 2/2, step 1694/7134 completed (loss: 0.027235237881541252, acc: 0.9906103014945984)
[2025-02-13 03:47:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:07][root][INFO] - Training Epoch: 2/2, step 1695/7134 completed (loss: 0.006151024717837572, acc: 1.0)
[2025-02-13 03:47:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:07][root][INFO] - Training Epoch: 2/2, step 1696/7134 completed (loss: 0.04796461760997772, acc: 0.9865996837615967)
[2025-02-13 03:47:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:08][root][INFO] - Training Epoch: 2/2, step 1697/7134 completed (loss: 0.023921048268675804, acc: 0.9906103014945984)
[2025-02-13 03:47:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:08][root][INFO] - Training Epoch: 2/2, step 1698/7134 completed (loss: 0.014549892395734787, acc: 0.9952977895736694)
[2025-02-13 03:47:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:09][root][INFO] - Training Epoch: 2/2, step 1699/7134 completed (loss: 0.017309628427028656, acc: 0.9930796027183533)
[2025-02-13 03:47:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:09][root][INFO] - Training Epoch: 2/2, step 1700/7134 completed (loss: 0.025573112070560455, acc: 0.991055428981781)
[2025-02-13 03:47:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:09][root][INFO] - Training Epoch: 2/2, step 1701/7134 completed (loss: 0.027716679498553276, acc: 0.9888734221458435)
[2025-02-13 03:47:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:10][root][INFO] - Training Epoch: 2/2, step 1702/7134 completed (loss: 0.03308920934796333, acc: 0.9888198971748352)
[2025-02-13 03:47:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:10][root][INFO] - Training Epoch: 2/2, step 1703/7134 completed (loss: 0.03596481308341026, acc: 0.9875690340995789)
[2025-02-13 03:47:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:11][root][INFO] - Training Epoch: 2/2, step 1704/7134 completed (loss: 0.06812268495559692, acc: 0.98097825050354)
[2025-02-13 03:47:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:11][root][INFO] - Training Epoch: 2/2, step 1705/7134 completed (loss: 0.040979597717523575, acc: 0.990138053894043)
[2025-02-13 03:47:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:12][root][INFO] - Training Epoch: 2/2, step 1706/7134 completed (loss: 0.019871274009346962, acc: 0.9924242496490479)
[2025-02-13 03:47:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:12][root][INFO] - Training Epoch: 2/2, step 1707/7134 completed (loss: 0.03445890545845032, acc: 0.9957716464996338)
[2025-02-13 03:47:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:13][root][INFO] - Training Epoch: 2/2, step 1708/7134 completed (loss: 0.03208497539162636, acc: 0.9902439117431641)
[2025-02-13 03:47:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:13][root][INFO] - Training Epoch: 2/2, step 1709/7134 completed (loss: 0.03517652302980423, acc: 0.9879310131072998)
[2025-02-13 03:47:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:13][root][INFO] - Training Epoch: 2/2, step 1710/7134 completed (loss: 0.04201526567339897, acc: 0.9841954112052917)
[2025-02-13 03:47:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:14][root][INFO] - Training Epoch: 2/2, step 1711/7134 completed (loss: 0.031788866966962814, acc: 0.9930915236473083)
[2025-02-13 03:47:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:14][root][INFO] - Training Epoch: 2/2, step 1712/7134 completed (loss: 0.05479024723172188, acc: 0.9859648942947388)
[2025-02-13 03:47:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:15][root][INFO] - Training Epoch: 2/2, step 1713/7134 completed (loss: 0.029836954548954964, acc: 0.9940119981765747)
[2025-02-13 03:47:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:15][root][INFO] - Training Epoch: 2/2, step 1714/7134 completed (loss: 0.057178009301424026, acc: 0.9835526347160339)
[2025-02-13 03:47:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:16][root][INFO] - Training Epoch: 2/2, step 1715/7134 completed (loss: 0.017713453620672226, acc: 0.993914783000946)
[2025-02-13 03:47:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:16][root][INFO] - Training Epoch: 2/2, step 1716/7134 completed (loss: 0.04231224209070206, acc: 0.9833333492279053)
[2025-02-13 03:47:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:17][root][INFO] - Training Epoch: 2/2, step 1717/7134 completed (loss: 0.02349090576171875, acc: 0.9922178983688354)
[2025-02-13 03:47:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:17][root][INFO] - Training Epoch: 2/2, step 1718/7134 completed (loss: 0.04367728531360626, acc: 0.9866369962692261)
[2025-02-13 03:47:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:17][root][INFO] - Training Epoch: 2/2, step 1719/7134 completed (loss: 0.07060170918703079, acc: 0.9890410900115967)
[2025-02-13 03:47:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:18][root][INFO] - Training Epoch: 2/2, step 1720/7134 completed (loss: 0.057953961193561554, acc: 0.9751552939414978)
[2025-02-13 03:47:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:18][root][INFO] - Training Epoch: 2/2, step 1721/7134 completed (loss: 0.030294563621282578, acc: 0.9865092635154724)
[2025-02-13 03:47:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:19][root][INFO] - Training Epoch: 2/2, step 1722/7134 completed (loss: 0.01476613711565733, acc: 0.9947368502616882)
[2025-02-13 03:47:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:19][root][INFO] - Training Epoch: 2/2, step 1723/7134 completed (loss: 0.06536781042814255, acc: 0.9865871667861938)
[2025-02-13 03:47:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:19][root][INFO] - Training Epoch: 2/2, step 1724/7134 completed (loss: 0.056617312133312225, acc: 0.9858712553977966)
[2025-02-13 03:47:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:20][root][INFO] - Training Epoch: 2/2, step 1725/7134 completed (loss: 0.012969584204256535, acc: 1.0)
[2025-02-13 03:47:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:20][root][INFO] - Training Epoch: 2/2, step 1726/7134 completed (loss: 0.026247460395097733, acc: 0.9955157041549683)
[2025-02-13 03:47:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:21][root][INFO] - Training Epoch: 2/2, step 1727/7134 completed (loss: 0.02244904451072216, acc: 0.9963503479957581)
[2025-02-13 03:47:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:21][root][INFO] - Training Epoch: 2/2, step 1728/7134 completed (loss: 0.01664094626903534, acc: 0.9950330853462219)
[2025-02-13 03:47:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:22][root][INFO] - Training Epoch: 2/2, step 1729/7134 completed (loss: 0.04886862635612488, acc: 0.9898167252540588)
[2025-02-13 03:47:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:22][root][INFO] - Training Epoch: 2/2, step 1730/7134 completed (loss: 0.014545091427862644, acc: 0.9935759902000427)
[2025-02-13 03:47:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:22][root][INFO] - Training Epoch: 2/2, step 1731/7134 completed (loss: 0.07812726497650146, acc: 0.9884393215179443)
[2025-02-13 03:47:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:23][root][INFO] - Training Epoch: 2/2, step 1732/7134 completed (loss: 0.023334192112088203, acc: 0.9938398599624634)
[2025-02-13 03:47:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:23][root][INFO] - Training Epoch: 2/2, step 1733/7134 completed (loss: 0.01882495917379856, acc: 0.9938271641731262)
[2025-02-13 03:47:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:24][root][INFO] - Training Epoch: 2/2, step 1734/7134 completed (loss: 0.05011734366416931, acc: 0.9852579832077026)
[2025-02-13 03:47:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:24][root][INFO] - Training Epoch: 2/2, step 1735/7134 completed (loss: 0.01634112559258938, acc: 0.9943898916244507)
[2025-02-13 03:47:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:25][root][INFO] - Training Epoch: 2/2, step 1736/7134 completed (loss: 0.024196693673729897, acc: 0.9903030395507812)
[2025-02-13 03:47:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:25][root][INFO] - Training Epoch: 2/2, step 1737/7134 completed (loss: 0.03494546189904213, acc: 0.9901685118675232)
[2025-02-13 03:47:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:26][root][INFO] - Training Epoch: 2/2, step 1738/7134 completed (loss: 0.014390378259122372, acc: 0.9975874423980713)
[2025-02-13 03:47:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:26][root][INFO] - Training Epoch: 2/2, step 1739/7134 completed (loss: 0.03598720207810402, acc: 0.9865269660949707)
[2025-02-13 03:47:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:26][root][INFO] - Training Epoch: 2/2, step 1740/7134 completed (loss: 0.014974717050790787, acc: 0.9912280440330505)
[2025-02-13 03:47:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:27][root][INFO] - Training Epoch: 2/2, step 1741/7134 completed (loss: 0.034838300198316574, acc: 0.9920634627342224)
[2025-02-13 03:47:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:27][root][INFO] - Training Epoch: 2/2, step 1742/7134 completed (loss: 0.020477209240198135, acc: 0.9943262338638306)
[2025-02-13 03:47:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:28][root][INFO] - Training Epoch: 2/2, step 1743/7134 completed (loss: 0.025731530040502548, acc: 0.994397759437561)
[2025-02-13 03:47:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:28][root][INFO] - Training Epoch: 2/2, step 1744/7134 completed (loss: 0.034363262355327606, acc: 0.9951140284538269)
[2025-02-13 03:47:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:29][root][INFO] - Training Epoch: 2/2, step 1745/7134 completed (loss: 0.01972849853336811, acc: 0.9924812316894531)
[2025-02-13 03:47:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:29][root][INFO] - Training Epoch: 2/2, step 1746/7134 completed (loss: 0.011699088849127293, acc: 0.9970930218696594)
[2025-02-13 03:47:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:30][root][INFO] - Training Epoch: 2/2, step 1747/7134 completed (loss: 0.026692910119891167, acc: 0.9899874925613403)
[2025-02-13 03:47:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:30][root][INFO] - Training Epoch: 2/2, step 1748/7134 completed (loss: 0.02071111463010311, acc: 0.9925558567047119)
[2025-02-13 03:47:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:31][root][INFO] - Training Epoch: 2/2, step 1749/7134 completed (loss: 0.02398006245493889, acc: 0.9943246245384216)
[2025-02-13 03:47:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:31][root][INFO] - Training Epoch: 2/2, step 1750/7134 completed (loss: 0.023697491735219955, acc: 0.9940047860145569)
[2025-02-13 03:47:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:32][root][INFO] - Training Epoch: 2/2, step 1751/7134 completed (loss: 0.0392056368291378, acc: 0.9842424392700195)
[2025-02-13 03:47:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:32][root][INFO] - Training Epoch: 2/2, step 1752/7134 completed (loss: 0.04906594753265381, acc: 0.9845361113548279)
[2025-02-13 03:47:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:33][root][INFO] - Training Epoch: 2/2, step 1753/7134 completed (loss: 0.04006257280707359, acc: 0.9908972978591919)
[2025-02-13 03:47:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:33][root][INFO] - Training Epoch: 2/2, step 1754/7134 completed (loss: 0.0250443946570158, acc: 0.9927954077720642)
[2025-02-13 03:47:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:33][root][INFO] - Training Epoch: 2/2, step 1755/7134 completed (loss: 0.01215596217662096, acc: 0.9960317611694336)
[2025-02-13 03:47:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:34][root][INFO] - Training Epoch: 2/2, step 1756/7134 completed (loss: 0.03512554243206978, acc: 0.9921976327896118)
[2025-02-13 03:47:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:34][root][INFO] - Training Epoch: 2/2, step 1757/7134 completed (loss: 0.03829268738627434, acc: 0.9900166392326355)
[2025-02-13 03:47:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:35][root][INFO] - Training Epoch: 2/2, step 1758/7134 completed (loss: 0.02116045542061329, acc: 0.9949495196342468)
[2025-02-13 03:47:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:35][root][INFO] - Training Epoch: 2/2, step 1759/7134 completed (loss: 0.020129956305027008, acc: 0.9930459260940552)
[2025-02-13 03:47:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:36][root][INFO] - Training Epoch: 2/2, step 1760/7134 completed (loss: 0.014206482097506523, acc: 0.9915966391563416)
[2025-02-13 03:47:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:36][root][INFO] - Training Epoch: 2/2, step 1761/7134 completed (loss: 0.020811425521969795, acc: 0.9943181872367859)
[2025-02-13 03:47:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:36][root][INFO] - Training Epoch: 2/2, step 1762/7134 completed (loss: 0.011787730269134045, acc: 0.9938144087791443)
[2025-02-13 03:47:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:37][root][INFO] - Training Epoch: 2/2, step 1763/7134 completed (loss: 0.017241260036826134, acc: 0.9935897588729858)
[2025-02-13 03:47:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:37][root][INFO] - Training Epoch: 2/2, step 1764/7134 completed (loss: 0.010791927576065063, acc: 0.9962825179100037)
[2025-02-13 03:47:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:38][root][INFO] - Training Epoch: 2/2, step 1765/7134 completed (loss: 0.012034139595925808, acc: 0.9969135522842407)
[2025-02-13 03:47:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:38][root][INFO] - Training Epoch: 2/2, step 1766/7134 completed (loss: 0.014154152013361454, acc: 0.9941349029541016)
[2025-02-13 03:47:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:39][root][INFO] - Training Epoch: 2/2, step 1767/7134 completed (loss: 0.04896671697497368, acc: 0.9915134310722351)
[2025-02-13 03:47:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:39][root][INFO] - Training Epoch: 2/2, step 1768/7134 completed (loss: 0.03040928579866886, acc: 0.9942196607589722)
[2025-02-13 03:47:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:40][root][INFO] - Training Epoch: 2/2, step 1769/7134 completed (loss: 0.019096501171588898, acc: 0.9926470518112183)
[2025-02-13 03:47:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:40][root][INFO] - Training Epoch: 2/2, step 1770/7134 completed (loss: 0.020738203078508377, acc: 0.9939302206039429)
[2025-02-13 03:47:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:40][root][INFO] - Training Epoch: 2/2, step 1771/7134 completed (loss: 0.008451402187347412, acc: 0.998603343963623)
[2025-02-13 03:47:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:41][root][INFO] - Training Epoch: 2/2, step 1772/7134 completed (loss: 0.010850589722394943, acc: 0.9966499209403992)
[2025-02-13 03:47:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:41][root][INFO] - Training Epoch: 2/2, step 1773/7134 completed (loss: 0.011127784848213196, acc: 0.9970930218696594)
[2025-02-13 03:47:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:42][root][INFO] - Training Epoch: 2/2, step 1774/7134 completed (loss: 0.015775365754961967, acc: 0.9950166344642639)
[2025-02-13 03:47:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:42][root][INFO] - Training Epoch: 2/2, step 1775/7134 completed (loss: 0.0030786797869950533, acc: 1.0)
[2025-02-13 03:47:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:43][root][INFO] - Training Epoch: 2/2, step 1776/7134 completed (loss: 0.005985517520457506, acc: 0.9970059990882874)
[2025-02-13 03:47:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:43][root][INFO] - Training Epoch: 2/2, step 1777/7134 completed (loss: 0.009535877965390682, acc: 0.9970414042472839)
[2025-02-13 03:47:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:43][root][INFO] - Training Epoch: 2/2, step 1778/7134 completed (loss: 0.0338473878800869, acc: 0.9955621361732483)
[2025-02-13 03:47:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:44][root][INFO] - Training Epoch: 2/2, step 1779/7134 completed (loss: 0.029897937551140785, acc: 0.991919219493866)
[2025-02-13 03:47:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:44][root][INFO] - Training Epoch: 2/2, step 1780/7134 completed (loss: 0.02284766174852848, acc: 0.99717116355896)
[2025-02-13 03:47:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:47:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:48:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:49:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:50:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:43][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.0436, device='cuda:0') eval_epoch_loss=tensor(0.0427, device='cuda:0') eval_epoch_acc=tensor(0.9886, device='cuda:0')
[2025-02-13 03:51:43][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-13 03:51:43][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-13 03:51:43][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme/asr_epoch_2_step_1781_loss_0.04271939769387245/model.pt
[2025-02-13 03:51:43][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme directory
[2025-02-13 03:51:43][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 2 is 0.04271939769387245
[2025-02-13 03:51:43][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.9886301159858704
[2025-02-13 03:51:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:43][root][INFO] - Training Epoch: 2/2, step 1781/7134 completed (loss: 0.011074824258685112, acc: 0.9969465732574463)
[2025-02-13 03:51:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:44][root][INFO] - Training Epoch: 2/2, step 1782/7134 completed (loss: 0.005199704319238663, acc: 1.0)
[2025-02-13 03:51:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:44][root][INFO] - Training Epoch: 2/2, step 1783/7134 completed (loss: 0.02224656380712986, acc: 0.9945945739746094)
[2025-02-13 03:51:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:45][root][INFO] - Training Epoch: 2/2, step 1784/7134 completed (loss: 0.030987359583377838, acc: 0.9956011772155762)
[2025-02-13 03:51:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:45][root][INFO] - Training Epoch: 2/2, step 1785/7134 completed (loss: 0.01857363060116768, acc: 0.993122398853302)
[2025-02-13 03:51:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:46][root][INFO] - Training Epoch: 2/2, step 1786/7134 completed (loss: 0.015147929079830647, acc: 0.9968798756599426)
[2025-02-13 03:51:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:46][root][INFO] - Training Epoch: 2/2, step 1787/7134 completed (loss: 0.01238784659653902, acc: 0.9946428537368774)
[2025-02-13 03:51:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:47][root][INFO] - Training Epoch: 2/2, step 1788/7134 completed (loss: 0.02113872952759266, acc: 0.9934533834457397)
[2025-02-13 03:51:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:47][root][INFO] - Training Epoch: 2/2, step 1789/7134 completed (loss: 0.010987537913024426, acc: 0.996927797794342)
[2025-02-13 03:51:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:48][root][INFO] - Training Epoch: 2/2, step 1790/7134 completed (loss: 0.09655482321977615, acc: 0.988727867603302)
[2025-02-13 03:51:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:48][root][INFO] - Training Epoch: 2/2, step 1791/7134 completed (loss: 0.08552762120962143, acc: 0.9885877370834351)
[2025-02-13 03:51:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:48][root][INFO] - Training Epoch: 2/2, step 1792/7134 completed (loss: 0.12517501413822174, acc: 0.9796748161315918)
[2025-02-13 03:51:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:49][root][INFO] - Training Epoch: 2/2, step 1793/7134 completed (loss: 0.09663807600736618, acc: 0.9810810685157776)
[2025-02-13 03:51:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:49][root][INFO] - Training Epoch: 2/2, step 1794/7134 completed (loss: 0.06878306716680527, acc: 0.9827044010162354)
[2025-02-13 03:51:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:50][root][INFO] - Training Epoch: 2/2, step 1795/7134 completed (loss: 0.12603046000003815, acc: 0.9698581695556641)
[2025-02-13 03:51:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:50][root][INFO] - Training Epoch: 2/2, step 1796/7134 completed (loss: 0.0679202750325203, acc: 0.9780564308166504)
[2025-02-13 03:51:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:51][root][INFO] - Training Epoch: 2/2, step 1797/7134 completed (loss: 0.04563915729522705, acc: 0.9845938086509705)
[2025-02-13 03:51:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:51][root][INFO] - Training Epoch: 2/2, step 1798/7134 completed (loss: 0.05489584803581238, acc: 0.9768339991569519)
[2025-02-13 03:51:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:52][root][INFO] - Training Epoch: 2/2, step 1799/7134 completed (loss: 0.07759523391723633, acc: 0.9786432385444641)
[2025-02-13 03:51:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:52][root][INFO] - Training Epoch: 2/2, step 1800/7134 completed (loss: 0.08772764354944229, acc: 0.9672897458076477)
[2025-02-13 03:51:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:52][root][INFO] - Training Epoch: 2/2, step 1801/7134 completed (loss: 0.06029193848371506, acc: 0.9851694703102112)
[2025-02-13 03:51:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:53][root][INFO] - Training Epoch: 2/2, step 1802/7134 completed (loss: 0.05735453590750694, acc: 0.9837296605110168)
[2025-02-13 03:51:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:53][root][INFO] - Training Epoch: 2/2, step 1803/7134 completed (loss: 0.03696296736598015, acc: 0.990755021572113)
[2025-02-13 03:51:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:54][root][INFO] - Training Epoch: 2/2, step 1804/7134 completed (loss: 0.03004288300871849, acc: 0.9914004802703857)
[2025-02-13 03:51:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:54][root][INFO] - Training Epoch: 2/2, step 1805/7134 completed (loss: 0.04692693427205086, acc: 0.9855700135231018)
[2025-02-13 03:51:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:55][root][INFO] - Training Epoch: 2/2, step 1806/7134 completed (loss: 0.03642188757658005, acc: 0.9907894730567932)
[2025-02-13 03:51:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:55][root][INFO] - Training Epoch: 2/2, step 1807/7134 completed (loss: 0.044606924057006836, acc: 0.9844412803649902)
[2025-02-13 03:51:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:56][root][INFO] - Training Epoch: 2/2, step 1808/7134 completed (loss: 0.021019503474235535, acc: 0.9960988163948059)
[2025-02-13 03:51:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:56][root][INFO] - Training Epoch: 2/2, step 1809/7134 completed (loss: 0.009292139671742916, acc: 0.9986486434936523)
[2025-02-13 03:51:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:56][root][INFO] - Training Epoch: 2/2, step 1810/7134 completed (loss: 0.013066953048110008, acc: 0.998711347579956)
[2025-02-13 03:51:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:57][root][INFO] - Training Epoch: 2/2, step 1811/7134 completed (loss: 0.04032878950238228, acc: 0.9877883195877075)
[2025-02-13 03:51:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:57][root][INFO] - Training Epoch: 2/2, step 1812/7134 completed (loss: 0.012057905085384846, acc: 0.9961734414100647)
[2025-02-13 03:51:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:58][root][INFO] - Training Epoch: 2/2, step 1813/7134 completed (loss: 0.041327252984046936, acc: 0.9880239367485046)
[2025-02-13 03:51:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:58][root][INFO] - Training Epoch: 2/2, step 1814/7134 completed (loss: 0.01535169780254364, acc: 0.997668981552124)
[2025-02-13 03:51:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:59][root][INFO] - Training Epoch: 2/2, step 1815/7134 completed (loss: 0.021977318450808525, acc: 0.9942528605461121)
[2025-02-13 03:51:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:51:59][root][INFO] - Training Epoch: 2/2, step 1816/7134 completed (loss: 0.03410270810127258, acc: 0.9890453815460205)
[2025-02-13 03:51:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:00][root][INFO] - Training Epoch: 2/2, step 1817/7134 completed (loss: 0.0266870204359293, acc: 0.9904534816741943)
[2025-02-13 03:52:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:00][root][INFO] - Training Epoch: 2/2, step 1818/7134 completed (loss: 0.041316207498311996, acc: 0.9925650358200073)
[2025-02-13 03:52:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:01][root][INFO] - Training Epoch: 2/2, step 1819/7134 completed (loss: 0.011843510903418064, acc: 0.9950124621391296)
[2025-02-13 03:52:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:01][root][INFO] - Training Epoch: 2/2, step 1820/7134 completed (loss: 0.01888378895819187, acc: 0.9934498071670532)
[2025-02-13 03:52:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:02][root][INFO] - Training Epoch: 2/2, step 1821/7134 completed (loss: 0.021228095516562462, acc: 0.9956236481666565)
[2025-02-13 03:52:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:02][root][INFO] - Training Epoch: 2/2, step 1822/7134 completed (loss: 0.00764478137716651, acc: 0.9985380172729492)
[2025-02-13 03:52:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:02][root][INFO] - Training Epoch: 2/2, step 1823/7134 completed (loss: 0.017696628347039223, acc: 0.9931787252426147)
[2025-02-13 03:52:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:03][root][INFO] - Training Epoch: 2/2, step 1824/7134 completed (loss: 0.006693391595035791, acc: 0.9975278377532959)
[2025-02-13 03:52:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:03][root][INFO] - Training Epoch: 2/2, step 1825/7134 completed (loss: 0.01711677759885788, acc: 0.9921524524688721)
[2025-02-13 03:52:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:04][root][INFO] - Training Epoch: 2/2, step 1826/7134 completed (loss: 0.010178280994296074, acc: 0.9973081946372986)
[2025-02-13 03:52:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:04][root][INFO] - Training Epoch: 2/2, step 1827/7134 completed (loss: 0.029899710789322853, acc: 0.9922822713851929)
[2025-02-13 03:52:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:05][root][INFO] - Training Epoch: 2/2, step 1828/7134 completed (loss: 0.00433262949809432, acc: 0.9987130165100098)
[2025-02-13 03:52:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:05][root][INFO] - Training Epoch: 2/2, step 1829/7134 completed (loss: 0.01946902833878994, acc: 0.9950310587882996)
[2025-02-13 03:52:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:06][root][INFO] - Training Epoch: 2/2, step 1830/7134 completed (loss: 0.02906455472111702, acc: 0.9939467310905457)
[2025-02-13 03:52:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:06][root][INFO] - Training Epoch: 2/2, step 1831/7134 completed (loss: 0.008601503446698189, acc: 0.996874988079071)
[2025-02-13 03:52:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:07][root][INFO] - Training Epoch: 2/2, step 1832/7134 completed (loss: 0.054544005542993546, acc: 0.9911660552024841)
[2025-02-13 03:52:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:07][root][INFO] - Training Epoch: 2/2, step 1833/7134 completed (loss: 0.15945978462696075, acc: 0.9661733508110046)
[2025-02-13 03:52:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:07][root][INFO] - Training Epoch: 2/2, step 1834/7134 completed (loss: 0.03119863010942936, acc: 0.9887323975563049)
[2025-02-13 03:52:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:08][root][INFO] - Training Epoch: 2/2, step 1835/7134 completed (loss: 0.06115627661347389, acc: 0.979651153087616)
[2025-02-13 03:52:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:08][root][INFO] - Training Epoch: 2/2, step 1836/7134 completed (loss: 0.027341822162270546, acc: 0.991725742816925)
[2025-02-13 03:52:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:09][root][INFO] - Training Epoch: 2/2, step 1837/7134 completed (loss: 0.016888907179236412, acc: 0.9950860142707825)
[2025-02-13 03:52:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:09][root][INFO] - Training Epoch: 2/2, step 1838/7134 completed (loss: 0.00579444132745266, acc: 0.9979591965675354)
[2025-02-13 03:52:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:10][root][INFO] - Training Epoch: 2/2, step 1839/7134 completed (loss: 0.020691916346549988, acc: 0.9935232996940613)
[2025-02-13 03:52:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:10][root][INFO] - Training Epoch: 2/2, step 1840/7134 completed (loss: 0.018960876390337944, acc: 0.991631805896759)
[2025-02-13 03:52:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:11][root][INFO] - Training Epoch: 2/2, step 1841/7134 completed (loss: 0.025233233347535133, acc: 0.9893617033958435)
[2025-02-13 03:52:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:11][root][INFO] - Training Epoch: 2/2, step 1842/7134 completed (loss: 0.03887415677309036, acc: 0.9904761910438538)
[2025-02-13 03:52:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:12][root][INFO] - Training Epoch: 2/2, step 1843/7134 completed (loss: 0.035641323775053024, acc: 0.987500011920929)
[2025-02-13 03:52:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:12][root][INFO] - Training Epoch: 2/2, step 1844/7134 completed (loss: 0.030597621574997902, acc: 0.9918699264526367)
[2025-02-13 03:52:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:13][root][INFO] - Training Epoch: 2/2, step 1845/7134 completed (loss: 0.029747366905212402, acc: 0.9874739050865173)
[2025-02-13 03:52:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:13][root][INFO] - Training Epoch: 2/2, step 1846/7134 completed (loss: 0.020843463018536568, acc: 0.9928774833679199)
[2025-02-13 03:52:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:13][root][INFO] - Training Epoch: 2/2, step 1847/7134 completed (loss: 0.013065717183053493, acc: 0.996927797794342)
[2025-02-13 03:52:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:14][root][INFO] - Training Epoch: 2/2, step 1848/7134 completed (loss: 0.03485901281237602, acc: 0.9929378628730774)
[2025-02-13 03:52:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:14][root][INFO] - Training Epoch: 2/2, step 1849/7134 completed (loss: 0.05617452785372734, acc: 0.9859402179718018)
[2025-02-13 03:52:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:15][root][INFO] - Training Epoch: 2/2, step 1850/7134 completed (loss: 0.021823227405548096, acc: 0.9926650524139404)
[2025-02-13 03:52:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:15][root][INFO] - Training Epoch: 2/2, step 1851/7134 completed (loss: 0.03633444011211395, acc: 0.990231990814209)
[2025-02-13 03:52:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:16][root][INFO] - Training Epoch: 2/2, step 1852/7134 completed (loss: 0.024982677772641182, acc: 0.9922360181808472)
[2025-02-13 03:52:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:16][root][INFO] - Training Epoch: 2/2, step 1853/7134 completed (loss: 0.04169836640357971, acc: 0.9928229451179504)
[2025-02-13 03:52:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:17][root][INFO] - Training Epoch: 2/2, step 1854/7134 completed (loss: 0.031745996326208115, acc: 0.9881094098091125)
[2025-02-13 03:52:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:17][root][INFO] - Training Epoch: 2/2, step 1855/7134 completed (loss: 0.04569561406970024, acc: 0.989595353603363)
[2025-02-13 03:52:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:18][root][INFO] - Training Epoch: 2/2, step 1856/7134 completed (loss: 0.033122844994068146, acc: 0.9889298677444458)
[2025-02-13 03:52:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:18][root][INFO] - Training Epoch: 2/2, step 1857/7134 completed (loss: 0.059202585369348526, acc: 0.9864531755447388)
[2025-02-13 03:52:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:19][root][INFO] - Training Epoch: 2/2, step 1858/7134 completed (loss: 0.01918862946331501, acc: 0.9916267991065979)
[2025-02-13 03:52:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:19][root][INFO] - Training Epoch: 2/2, step 1859/7134 completed (loss: 0.0198124460875988, acc: 0.9954493641853333)
[2025-02-13 03:52:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:20][root][INFO] - Training Epoch: 2/2, step 1860/7134 completed (loss: 0.015160132199525833, acc: 0.9936708807945251)
[2025-02-13 03:52:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:20][root][INFO] - Training Epoch: 2/2, step 1861/7134 completed (loss: 0.04169595614075661, acc: 0.9922480583190918)
[2025-02-13 03:52:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:21][root][INFO] - Training Epoch: 2/2, step 1862/7134 completed (loss: 0.01811949536204338, acc: 0.9945725798606873)
[2025-02-13 03:52:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:21][root][INFO] - Training Epoch: 2/2, step 1863/7134 completed (loss: 0.04332328960299492, acc: 0.9916067123413086)
[2025-02-13 03:52:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:21][root][INFO] - Training Epoch: 2/2, step 1864/7134 completed (loss: 0.1339217573404312, acc: 0.9741496443748474)
[2025-02-13 03:52:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:22][root][INFO] - Training Epoch: 2/2, step 1865/7134 completed (loss: 0.02130150981247425, acc: 0.9915397763252258)
[2025-02-13 03:52:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:22][root][INFO] - Training Epoch: 2/2, step 1866/7134 completed (loss: 0.005991615820676088, acc: 0.9971988797187805)
[2025-02-13 03:52:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:23][root][INFO] - Training Epoch: 2/2, step 1867/7134 completed (loss: 0.015260618180036545, acc: 0.9936948418617249)
[2025-02-13 03:52:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:23][root][INFO] - Training Epoch: 2/2, step 1868/7134 completed (loss: 0.05304459482431412, acc: 0.9867987036705017)
[2025-02-13 03:52:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:24][root][INFO] - Training Epoch: 2/2, step 1869/7134 completed (loss: 0.01771334558725357, acc: 0.9934747219085693)
[2025-02-13 03:52:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:24][root][INFO] - Training Epoch: 2/2, step 1870/7134 completed (loss: 0.012160557322204113, acc: 0.99609375)
[2025-02-13 03:52:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:25][root][INFO] - Training Epoch: 2/2, step 1871/7134 completed (loss: 0.07288088649511337, acc: 0.9885807633399963)
[2025-02-13 03:52:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:25][root][INFO] - Training Epoch: 2/2, step 1872/7134 completed (loss: 0.40687763690948486, acc: 0.9171270728111267)
[2025-02-13 03:52:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:26][root][INFO] - Training Epoch: 2/2, step 1873/7134 completed (loss: 0.11420877277851105, acc: 0.9691211581230164)
[2025-02-13 03:52:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:26][root][INFO] - Training Epoch: 2/2, step 1874/7134 completed (loss: 0.024459050968289375, acc: 0.9973787665367126)
[2025-02-13 03:52:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:26][root][INFO] - Training Epoch: 2/2, step 1875/7134 completed (loss: 0.025835979729890823, acc: 0.9934498071670532)
[2025-02-13 03:52:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:27][root][INFO] - Training Epoch: 2/2, step 1876/7134 completed (loss: 0.10374380648136139, acc: 0.9660441279411316)
[2025-02-13 03:52:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:27][root][INFO] - Training Epoch: 2/2, step 1877/7134 completed (loss: 0.03271288052201271, acc: 0.99370276927948)
[2025-02-13 03:52:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:28][root][INFO] - Training Epoch: 2/2, step 1878/7134 completed (loss: 0.03964395448565483, acc: 0.9876733422279358)
[2025-02-13 03:52:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:28][root][INFO] - Training Epoch: 2/2, step 1879/7134 completed (loss: 0.0697079673409462, acc: 0.9738988876342773)
[2025-02-13 03:52:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:29][root][INFO] - Training Epoch: 2/2, step 1880/7134 completed (loss: 0.04304710030555725, acc: 0.9939637780189514)
[2025-02-13 03:52:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:29][root][INFO] - Training Epoch: 2/2, step 1881/7134 completed (loss: 0.05793925002217293, acc: 0.983660101890564)
[2025-02-13 03:52:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:29][root][INFO] - Training Epoch: 2/2, step 1882/7134 completed (loss: 0.04191092774271965, acc: 0.982758641242981)
[2025-02-13 03:52:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:30][root][INFO] - Training Epoch: 2/2, step 1883/7134 completed (loss: 0.04470375180244446, acc: 0.9901574850082397)
[2025-02-13 03:52:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:30][root][INFO] - Training Epoch: 2/2, step 1884/7134 completed (loss: 0.07133179157972336, acc: 0.9778831005096436)
[2025-02-13 03:52:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:31][root][INFO] - Training Epoch: 2/2, step 1885/7134 completed (loss: 0.04243588075041771, acc: 0.9868667721748352)
[2025-02-13 03:52:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:31][root][INFO] - Training Epoch: 2/2, step 1886/7134 completed (loss: 0.05926762893795967, acc: 0.9749478101730347)
[2025-02-13 03:52:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:32][root][INFO] - Training Epoch: 2/2, step 1887/7134 completed (loss: 0.04534561187028885, acc: 0.9848197102546692)
[2025-02-13 03:52:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:32][root][INFO] - Training Epoch: 2/2, step 1888/7134 completed (loss: 0.05635644868016243, acc: 0.9827255010604858)
[2025-02-13 03:52:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:32][root][INFO] - Training Epoch: 2/2, step 1889/7134 completed (loss: 0.02469499409198761, acc: 0.986994206905365)
[2025-02-13 03:52:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:33][root][INFO] - Training Epoch: 2/2, step 1890/7134 completed (loss: 0.019515743479132652, acc: 0.9922600388526917)
[2025-02-13 03:52:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:33][root][INFO] - Training Epoch: 2/2, step 1891/7134 completed (loss: 0.05812031403183937, acc: 0.9805194735527039)
[2025-02-13 03:52:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:34][root][INFO] - Training Epoch: 2/2, step 1892/7134 completed (loss: 0.0326111763715744, acc: 0.9907264113426208)
[2025-02-13 03:52:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:34][root][INFO] - Training Epoch: 2/2, step 1893/7134 completed (loss: 0.032125066965818405, acc: 0.9875195026397705)
[2025-02-13 03:52:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:35][root][INFO] - Training Epoch: 2/2, step 1894/7134 completed (loss: 0.07896437495946884, acc: 0.9826388955116272)
[2025-02-13 03:52:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:35][root][INFO] - Training Epoch: 2/2, step 1895/7134 completed (loss: 0.19569118320941925, acc: 0.9542483687400818)
[2025-02-13 03:52:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:36][root][INFO] - Training Epoch: 2/2, step 1896/7134 completed (loss: 0.03312516212463379, acc: 0.9882352948188782)
[2025-02-13 03:52:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:36][root][INFO] - Training Epoch: 2/2, step 1897/7134 completed (loss: 0.011454125866293907, acc: 0.9981481432914734)
[2025-02-13 03:52:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:36][root][INFO] - Training Epoch: 2/2, step 1898/7134 completed (loss: 0.02205350622534752, acc: 0.9917080998420715)
[2025-02-13 03:52:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:37][root][INFO] - Training Epoch: 2/2, step 1899/7134 completed (loss: 0.03742236644029617, acc: 0.9909583926200867)
[2025-02-13 03:52:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:37][root][INFO] - Training Epoch: 2/2, step 1900/7134 completed (loss: 0.021587802097201347, acc: 0.9967637658119202)
[2025-02-13 03:52:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:38][root][INFO] - Training Epoch: 2/2, step 1901/7134 completed (loss: 0.055426549166440964, acc: 0.9908592104911804)
[2025-02-13 03:52:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:38][root][INFO] - Training Epoch: 2/2, step 1902/7134 completed (loss: 0.03080904670059681, acc: 0.9929701089859009)
[2025-02-13 03:52:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:39][root][INFO] - Training Epoch: 2/2, step 1903/7134 completed (loss: 0.030003249645233154, acc: 0.9913194179534912)
[2025-02-13 03:52:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:39][root][INFO] - Training Epoch: 2/2, step 1904/7134 completed (loss: 0.028718583285808563, acc: 0.9872881174087524)
[2025-02-13 03:52:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:39][root][INFO] - Training Epoch: 2/2, step 1905/7134 completed (loss: 0.04094794765114784, acc: 0.9897750616073608)
[2025-02-13 03:52:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:40][root][INFO] - Training Epoch: 2/2, step 1906/7134 completed (loss: 0.0459965355694294, acc: 0.9863547682762146)
[2025-02-13 03:52:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:40][root][INFO] - Training Epoch: 2/2, step 1907/7134 completed (loss: 0.030342163518071175, acc: 0.9882352948188782)
[2025-02-13 03:52:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:41][root][INFO] - Training Epoch: 2/2, step 1908/7134 completed (loss: 0.032976847141981125, acc: 0.9935553073883057)
[2025-02-13 03:52:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:41][root][INFO] - Training Epoch: 2/2, step 1909/7134 completed (loss: 0.035280704498291016, acc: 0.9867549538612366)
[2025-02-13 03:52:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:42][root][INFO] - Training Epoch: 2/2, step 1910/7134 completed (loss: 0.028907833620905876, acc: 0.9934210777282715)
[2025-02-13 03:52:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:42][root][INFO] - Training Epoch: 2/2, step 1911/7134 completed (loss: 0.02412952110171318, acc: 0.9927184581756592)
[2025-02-13 03:52:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:42][root][INFO] - Training Epoch: 2/2, step 1912/7134 completed (loss: 0.005835019517689943, acc: 1.0)
[2025-02-13 03:52:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:43][root][INFO] - Training Epoch: 2/2, step 1913/7134 completed (loss: 0.04061363637447357, acc: 0.9833024144172668)
[2025-02-13 03:52:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:43][root][INFO] - Training Epoch: 2/2, step 1914/7134 completed (loss: 0.056065578013658524, acc: 0.9851149916648865)
[2025-02-13 03:52:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:44][root][INFO] - Training Epoch: 2/2, step 1915/7134 completed (loss: 0.02042122185230255, acc: 0.9967373609542847)
[2025-02-13 03:52:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:44][root][INFO] - Training Epoch: 2/2, step 1916/7134 completed (loss: 0.030394626781344414, acc: 0.9902557730674744)
[2025-02-13 03:52:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:45][root][INFO] - Training Epoch: 2/2, step 1917/7134 completed (loss: 0.020749419927597046, acc: 0.994397759437561)
[2025-02-13 03:52:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:45][root][INFO] - Training Epoch: 2/2, step 1918/7134 completed (loss: 0.05768328160047531, acc: 0.9905956387519836)
[2025-02-13 03:52:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:45][root][INFO] - Training Epoch: 2/2, step 1919/7134 completed (loss: 0.027457477524876595, acc: 0.9928264021873474)
[2025-02-13 03:52:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:46][root][INFO] - Training Epoch: 2/2, step 1920/7134 completed (loss: 0.019373290240764618, acc: 0.9969135522842407)
[2025-02-13 03:52:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:46][root][INFO] - Training Epoch: 2/2, step 1921/7134 completed (loss: 0.01606898568570614, acc: 0.9958449006080627)
[2025-02-13 03:52:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:47][root][INFO] - Training Epoch: 2/2, step 1922/7134 completed (loss: 0.02539387159049511, acc: 0.9920477271080017)
[2025-02-13 03:52:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:47][root][INFO] - Training Epoch: 2/2, step 1923/7134 completed (loss: 0.022763220593333244, acc: 0.9932998418807983)
[2025-02-13 03:52:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:48][root][INFO] - Training Epoch: 2/2, step 1924/7134 completed (loss: 0.04125452786684036, acc: 0.9871244430541992)
[2025-02-13 03:52:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:48][root][INFO] - Training Epoch: 2/2, step 1925/7134 completed (loss: 0.052926432341337204, acc: 0.9862433671951294)
[2025-02-13 03:52:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:49][root][INFO] - Training Epoch: 2/2, step 1926/7134 completed (loss: 0.011898467317223549, acc: 0.9959142208099365)
[2025-02-13 03:52:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:49][root][INFO] - Training Epoch: 2/2, step 1927/7134 completed (loss: 0.04708154872059822, acc: 0.9911602139472961)
[2025-02-13 03:52:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:50][root][INFO] - Training Epoch: 2/2, step 1928/7134 completed (loss: 0.02476799115538597, acc: 0.9924012422561646)
[2025-02-13 03:52:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:50][root][INFO] - Training Epoch: 2/2, step 1929/7134 completed (loss: 0.010708414949476719, acc: 0.9957627058029175)
[2025-02-13 03:52:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:51][root][INFO] - Training Epoch: 2/2, step 1930/7134 completed (loss: 0.055395543575286865, acc: 0.9842519760131836)
[2025-02-13 03:52:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:51][root][INFO] - Training Epoch: 2/2, step 1931/7134 completed (loss: 0.07671245187520981, acc: 0.9810366630554199)
[2025-02-13 03:52:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:51][root][INFO] - Training Epoch: 2/2, step 1932/7134 completed (loss: 0.06973996758460999, acc: 0.9805970191955566)
[2025-02-13 03:52:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:52][root][INFO] - Training Epoch: 2/2, step 1933/7134 completed (loss: 0.09273996949195862, acc: 0.9775910377502441)
[2025-02-13 03:52:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:52][root][INFO] - Training Epoch: 2/2, step 1934/7134 completed (loss: 0.036141760647296906, acc: 0.9971751570701599)
[2025-02-13 03:52:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:53][root][INFO] - Training Epoch: 2/2, step 1935/7134 completed (loss: 0.08458685129880905, acc: 0.9752547144889832)
[2025-02-13 03:52:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:53][root][INFO] - Training Epoch: 2/2, step 1936/7134 completed (loss: 0.018246352672576904, acc: 0.9952606558799744)
[2025-02-13 03:52:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:54][root][INFO] - Training Epoch: 2/2, step 1937/7134 completed (loss: 0.0373975969851017, acc: 0.9886363744735718)
[2025-02-13 03:52:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:54][root][INFO] - Training Epoch: 2/2, step 1938/7134 completed (loss: 0.05185112729668617, acc: 0.9843304753303528)
[2025-02-13 03:52:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:55][root][INFO] - Training Epoch: 2/2, step 1939/7134 completed (loss: 0.059445593506097794, acc: 0.9872881174087524)
[2025-02-13 03:52:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:55][root][INFO] - Training Epoch: 2/2, step 1940/7134 completed (loss: 0.053806692361831665, acc: 0.9863945841789246)
[2025-02-13 03:52:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:55][root][INFO] - Training Epoch: 2/2, step 1941/7134 completed (loss: 0.03310254216194153, acc: 0.9879679083824158)
[2025-02-13 03:52:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:56][root][INFO] - Training Epoch: 2/2, step 1942/7134 completed (loss: 0.04803018644452095, acc: 0.9878048896789551)
[2025-02-13 03:52:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:56][root][INFO] - Training Epoch: 2/2, step 1943/7134 completed (loss: 0.03826895356178284, acc: 0.9902439117431641)
[2025-02-13 03:52:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:57][root][INFO] - Training Epoch: 2/2, step 1944/7134 completed (loss: 0.027926495298743248, acc: 0.9963436722755432)
[2025-02-13 03:52:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:57][root][INFO] - Training Epoch: 2/2, step 1945/7134 completed (loss: 0.03460128605365753, acc: 0.9881889820098877)
[2025-02-13 03:52:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:58][root][INFO] - Training Epoch: 2/2, step 1946/7134 completed (loss: 0.027347074821591377, acc: 0.9881154298782349)
[2025-02-13 03:52:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:58][root][INFO] - Training Epoch: 2/2, step 1947/7134 completed (loss: 0.04157033562660217, acc: 0.9855263233184814)
[2025-02-13 03:52:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:59][root][INFO] - Training Epoch: 2/2, step 1948/7134 completed (loss: 0.07541266083717346, acc: 0.9772295951843262)
[2025-02-13 03:52:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:59][root][INFO] - Training Epoch: 2/2, step 1949/7134 completed (loss: 0.035299912095069885, acc: 0.9866220951080322)
[2025-02-13 03:52:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:52:59][root][INFO] - Training Epoch: 2/2, step 1950/7134 completed (loss: 0.05757274851202965, acc: 0.9872262477874756)
[2025-02-13 03:53:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:00][root][INFO] - Training Epoch: 2/2, step 1951/7134 completed (loss: 0.04369235038757324, acc: 0.9933110475540161)
[2025-02-13 03:53:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:00][root][INFO] - Training Epoch: 2/2, step 1952/7134 completed (loss: 0.07001881301403046, acc: 0.981697142124176)
[2025-02-13 03:53:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:01][root][INFO] - Training Epoch: 2/2, step 1953/7134 completed (loss: 0.021351588889956474, acc: 0.9891641139984131)
[2025-02-13 03:53:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:01][root][INFO] - Training Epoch: 2/2, step 1954/7134 completed (loss: 0.01740587316453457, acc: 0.995121955871582)
[2025-02-13 03:53:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:01][root][INFO] - Training Epoch: 2/2, step 1955/7134 completed (loss: 0.03274078294634819, acc: 0.9894551634788513)
[2025-02-13 03:53:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:02][root][INFO] - Training Epoch: 2/2, step 1956/7134 completed (loss: 0.01312892697751522, acc: 0.9973118305206299)
[2025-02-13 03:53:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:02][root][INFO] - Training Epoch: 2/2, step 1957/7134 completed (loss: 0.03428366035223007, acc: 0.9925925731658936)
[2025-02-13 03:53:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:03][root][INFO] - Training Epoch: 2/2, step 1958/7134 completed (loss: 0.018027227371931076, acc: 0.9971346855163574)
[2025-02-13 03:53:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:03][root][INFO] - Training Epoch: 2/2, step 1959/7134 completed (loss: 0.02441593073308468, acc: 0.9978678226470947)
[2025-02-13 03:53:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:03][root][INFO] - Training Epoch: 2/2, step 1960/7134 completed (loss: 0.014577873051166534, acc: 0.9962406158447266)
[2025-02-13 03:53:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:04][root][INFO] - Training Epoch: 2/2, step 1961/7134 completed (loss: 0.01831912435591221, acc: 0.9966555237770081)
[2025-02-13 03:53:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:04][root][INFO] - Training Epoch: 2/2, step 1962/7134 completed (loss: 0.03478154540061951, acc: 0.9860334992408752)
[2025-02-13 03:53:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:05][root][INFO] - Training Epoch: 2/2, step 1963/7134 completed (loss: 0.024270426481962204, acc: 0.9930915236473083)
[2025-02-13 03:53:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:05][root][INFO] - Training Epoch: 2/2, step 1964/7134 completed (loss: 0.028371399268507957, acc: 0.9890109896659851)
[2025-02-13 03:53:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:06][root][INFO] - Training Epoch: 2/2, step 1965/7134 completed (loss: 0.01951163448393345, acc: 0.9944211840629578)
[2025-02-13 03:53:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:06][root][INFO] - Training Epoch: 2/2, step 1966/7134 completed (loss: 0.02338186837732792, acc: 0.9908257126808167)
[2025-02-13 03:53:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:06][root][INFO] - Training Epoch: 2/2, step 1967/7134 completed (loss: 0.04273048788309097, acc: 0.9859872460365295)
[2025-02-13 03:53:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:07][root][INFO] - Training Epoch: 2/2, step 1968/7134 completed (loss: 0.013112973421812057, acc: 0.9975308775901794)
[2025-02-13 03:53:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:07][root][INFO] - Training Epoch: 2/2, step 1969/7134 completed (loss: 0.03887847810983658, acc: 0.9906666874885559)
[2025-02-13 03:53:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:08][root][INFO] - Training Epoch: 2/2, step 1970/7134 completed (loss: 0.021771924570202827, acc: 0.995398759841919)
[2025-02-13 03:53:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:08][root][INFO] - Training Epoch: 2/2, step 1971/7134 completed (loss: 0.037870489060878754, acc: 0.9893898963928223)
[2025-02-13 03:53:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:09][root][INFO] - Training Epoch: 2/2, step 1972/7134 completed (loss: 0.12706977128982544, acc: 0.9631449580192566)
[2025-02-13 03:53:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:09][root][INFO] - Training Epoch: 2/2, step 1973/7134 completed (loss: 0.06759371608495712, acc: 0.9809523820877075)
[2025-02-13 03:53:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:09][root][INFO] - Training Epoch: 2/2, step 1974/7134 completed (loss: 0.0324188768863678, acc: 0.9861809015274048)
[2025-02-13 03:53:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:10][root][INFO] - Training Epoch: 2/2, step 1975/7134 completed (loss: 0.01910591684281826, acc: 0.9984848499298096)
[2025-02-13 03:53:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:10][root][INFO] - Training Epoch: 2/2, step 1976/7134 completed (loss: 0.028290394693613052, acc: 0.9898074865341187)
[2025-02-13 03:53:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:11][root][INFO] - Training Epoch: 2/2, step 1977/7134 completed (loss: 0.02998901903629303, acc: 0.9869565367698669)
[2025-02-13 03:53:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:11][root][INFO] - Training Epoch: 2/2, step 1978/7134 completed (loss: 0.015814349055290222, acc: 0.9946308732032776)
[2025-02-13 03:53:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:12][root][INFO] - Training Epoch: 2/2, step 1979/7134 completed (loss: 0.023531218990683556, acc: 0.9934895634651184)
[2025-02-13 03:53:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:12][root][INFO] - Training Epoch: 2/2, step 1980/7134 completed (loss: 0.0537271648645401, acc: 0.9908735156059265)
[2025-02-13 03:53:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:13][root][INFO] - Training Epoch: 2/2, step 1981/7134 completed (loss: 0.023230597376823425, acc: 0.9944055676460266)
[2025-02-13 03:53:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:13][root][INFO] - Training Epoch: 2/2, step 1982/7134 completed (loss: 0.013540372252464294, acc: 0.9961389899253845)
[2025-02-13 03:53:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:13][root][INFO] - Training Epoch: 2/2, step 1983/7134 completed (loss: 0.014345047995448112, acc: 0.9974554777145386)
[2025-02-13 03:53:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:14][root][INFO] - Training Epoch: 2/2, step 1984/7134 completed (loss: 0.011110913008451462, acc: 0.9959623217582703)
[2025-02-13 03:53:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:14][root][INFO] - Training Epoch: 2/2, step 1985/7134 completed (loss: 0.03232764080166817, acc: 0.9902507066726685)
[2025-02-13 03:53:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:15][root][INFO] - Training Epoch: 2/2, step 1986/7134 completed (loss: 0.02817477285861969, acc: 0.9897040128707886)
[2025-02-13 03:53:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:15][root][INFO] - Training Epoch: 2/2, step 1987/7134 completed (loss: 0.02844044752418995, acc: 0.9915013909339905)
[2025-02-13 03:53:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:16][root][INFO] - Training Epoch: 2/2, step 1988/7134 completed (loss: 0.033164847642183304, acc: 0.9885877370834351)
[2025-02-13 03:53:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:16][root][INFO] - Training Epoch: 2/2, step 1989/7134 completed (loss: 0.04398036748170853, acc: 0.9888357520103455)
[2025-02-13 03:53:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:17][root][INFO] - Training Epoch: 2/2, step 1990/7134 completed (loss: 0.029743511229753494, acc: 0.9939320683479309)
[2025-02-13 03:53:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:17][root][INFO] - Training Epoch: 2/2, step 1991/7134 completed (loss: 0.015094858594238758, acc: 0.9950166344642639)
[2025-02-13 03:53:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:18][root][INFO] - Training Epoch: 2/2, step 1992/7134 completed (loss: 0.028389494866132736, acc: 0.9906976819038391)
[2025-02-13 03:53:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:18][root][INFO] - Training Epoch: 2/2, step 1993/7134 completed (loss: 0.007495544850826263, acc: 1.0)
[2025-02-13 03:53:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:18][root][INFO] - Training Epoch: 2/2, step 1994/7134 completed (loss: 0.012429052963852882, acc: 0.9965517520904541)
[2025-02-13 03:53:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:19][root][INFO] - Training Epoch: 2/2, step 1995/7134 completed (loss: 0.02368091233074665, acc: 0.9953632354736328)
[2025-02-13 03:53:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:19][root][INFO] - Training Epoch: 2/2, step 1996/7134 completed (loss: 0.03071535937488079, acc: 0.9900826215744019)
[2025-02-13 03:53:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:20][root][INFO] - Training Epoch: 2/2, step 1997/7134 completed (loss: 0.021490978077054024, acc: 0.9970887899398804)
[2025-02-13 03:53:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:20][root][INFO] - Training Epoch: 2/2, step 1998/7134 completed (loss: 0.01651822030544281, acc: 0.9952903985977173)
[2025-02-13 03:53:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:20][root][INFO] - Training Epoch: 2/2, step 1999/7134 completed (loss: 0.031173385679721832, acc: 0.9864176511764526)
[2025-02-13 03:53:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:21][root][INFO] - Training Epoch: 2/2, step 2000/7134 completed (loss: 0.024601195007562637, acc: 0.9952681660652161)
[2025-02-13 03:53:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:21][root][INFO] - Training Epoch: 2/2, step 2001/7134 completed (loss: 0.012981820851564407, acc: 0.9954545497894287)
[2025-02-13 03:53:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:22][root][INFO] - Training Epoch: 2/2, step 2002/7134 completed (loss: 0.01730293408036232, acc: 0.9930434823036194)
[2025-02-13 03:53:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:22][root][INFO] - Training Epoch: 2/2, step 2003/7134 completed (loss: 0.013240360654890537, acc: 0.9958333373069763)
[2025-02-13 03:53:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:23][root][INFO] - Training Epoch: 2/2, step 2004/7134 completed (loss: 0.14998307824134827, acc: 0.9662446975708008)
[2025-02-13 03:53:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:23][root][INFO] - Training Epoch: 2/2, step 2005/7134 completed (loss: 0.08822924643754959, acc: 0.9808219075202942)
[2025-02-13 03:53:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:23][root][INFO] - Training Epoch: 2/2, step 2006/7134 completed (loss: 0.014761517755687237, acc: 0.9942362904548645)
[2025-02-13 03:53:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:24][root][INFO] - Training Epoch: 2/2, step 2007/7134 completed (loss: 0.03783988952636719, acc: 0.9899598360061646)
[2025-02-13 03:53:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:24][root][INFO] - Training Epoch: 2/2, step 2008/7134 completed (loss: 0.03638920560479164, acc: 0.9875444769859314)
[2025-02-13 03:53:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:25][root][INFO] - Training Epoch: 2/2, step 2009/7134 completed (loss: 0.03643970564007759, acc: 0.9909747242927551)
[2025-02-13 03:53:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:25][root][INFO] - Training Epoch: 2/2, step 2010/7134 completed (loss: 0.018708473071455956, acc: 0.994854211807251)
[2025-02-13 03:53:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:25][root][INFO] - Training Epoch: 2/2, step 2011/7134 completed (loss: 0.015786901116371155, acc: 0.9902152419090271)
[2025-02-13 03:53:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:26][root][INFO] - Training Epoch: 2/2, step 2012/7134 completed (loss: 0.022780632600188255, acc: 0.9894291758537292)
[2025-02-13 03:53:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:26][root][INFO] - Training Epoch: 2/2, step 2013/7134 completed (loss: 0.01517882477492094, acc: 0.9959999918937683)
[2025-02-13 03:53:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:27][root][INFO] - Training Epoch: 2/2, step 2014/7134 completed (loss: 0.01836317591369152, acc: 0.995275616645813)
[2025-02-13 03:53:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:27][root][INFO] - Training Epoch: 2/2, step 2015/7134 completed (loss: 0.025057636201381683, acc: 0.9967105388641357)
[2025-02-13 03:53:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:28][root][INFO] - Training Epoch: 2/2, step 2016/7134 completed (loss: 0.05126182734966278, acc: 0.9846153855323792)
[2025-02-13 03:53:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:28][root][INFO] - Training Epoch: 2/2, step 2017/7134 completed (loss: 0.05714669078588486, acc: 0.9905405640602112)
[2025-02-13 03:53:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:29][root][INFO] - Training Epoch: 2/2, step 2018/7134 completed (loss: 0.02225852943956852, acc: 0.9948717951774597)
[2025-02-13 03:53:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:29][root][INFO] - Training Epoch: 2/2, step 2019/7134 completed (loss: 0.028673550114035606, acc: 0.9903448224067688)
[2025-02-13 03:53:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:29][root][INFO] - Training Epoch: 2/2, step 2020/7134 completed (loss: 0.008138912729918957, acc: 0.9983079433441162)
[2025-02-13 03:53:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:30][root][INFO] - Training Epoch: 2/2, step 2021/7134 completed (loss: 0.04560267552733421, acc: 0.9909090995788574)
[2025-02-13 03:53:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:30][root][INFO] - Training Epoch: 2/2, step 2022/7134 completed (loss: 0.03130501136183739, acc: 0.9896013736724854)
[2025-02-13 03:53:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:31][root][INFO] - Training Epoch: 2/2, step 2023/7134 completed (loss: 0.02383158542215824, acc: 0.9956204295158386)
[2025-02-13 03:53:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:31][root][INFO] - Training Epoch: 2/2, step 2024/7134 completed (loss: 0.027625301852822304, acc: 0.9888888597488403)
[2025-02-13 03:53:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:32][root][INFO] - Training Epoch: 2/2, step 2025/7134 completed (loss: 0.029887080192565918, acc: 0.9946666955947876)
[2025-02-13 03:53:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:32][root][INFO] - Training Epoch: 2/2, step 2026/7134 completed (loss: 0.04064963385462761, acc: 0.9894578456878662)
[2025-02-13 03:53:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:32][root][INFO] - Training Epoch: 2/2, step 2027/7134 completed (loss: 0.031020918861031532, acc: 0.9900332093238831)
[2025-02-13 03:53:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:33][root][INFO] - Training Epoch: 2/2, step 2028/7134 completed (loss: 0.030111880972981453, acc: 0.9941520690917969)
[2025-02-13 03:53:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:33][root][INFO] - Training Epoch: 2/2, step 2029/7134 completed (loss: 0.02427692338824272, acc: 0.9886363744735718)
[2025-02-13 03:53:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:34][root][INFO] - Training Epoch: 2/2, step 2030/7134 completed (loss: 0.03620409220457077, acc: 0.987730085849762)
[2025-02-13 03:53:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:34][root][INFO] - Training Epoch: 2/2, step 2031/7134 completed (loss: 0.026355057954788208, acc: 0.9926380515098572)
[2025-02-13 03:53:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:35][root][INFO] - Training Epoch: 2/2, step 2032/7134 completed (loss: 0.0368502140045166, acc: 0.9892473220825195)
[2025-02-13 03:53:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:35][root][INFO] - Training Epoch: 2/2, step 2033/7134 completed (loss: 0.01606987603008747, acc: 0.9965694546699524)
[2025-02-13 03:53:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:35][root][INFO] - Training Epoch: 2/2, step 2034/7134 completed (loss: 0.03997601941227913, acc: 0.9892802238464355)
[2025-02-13 03:53:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:36][root][INFO] - Training Epoch: 2/2, step 2035/7134 completed (loss: 0.01108867023140192, acc: 0.9975093603134155)
[2025-02-13 03:53:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:36][root][INFO] - Training Epoch: 2/2, step 2036/7134 completed (loss: 0.013960137031972408, acc: 0.9965397715568542)
[2025-02-13 03:53:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:37][root][INFO] - Training Epoch: 2/2, step 2037/7134 completed (loss: 0.011882402934134007, acc: 0.9940652847290039)
[2025-02-13 03:53:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:37][root][INFO] - Training Epoch: 2/2, step 2038/7134 completed (loss: 0.015254749916493893, acc: 0.9940898418426514)
[2025-02-13 03:53:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:38][root][INFO] - Training Epoch: 2/2, step 2039/7134 completed (loss: 0.008205334655940533, acc: 0.9963964223861694)
[2025-02-13 03:53:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:38][root][INFO] - Training Epoch: 2/2, step 2040/7134 completed (loss: 0.027425818145275116, acc: 0.9938271641731262)
[2025-02-13 03:53:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:39][root][INFO] - Training Epoch: 2/2, step 2041/7134 completed (loss: 0.006833970546722412, acc: 0.9986263513565063)
[2025-02-13 03:53:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:39][root][INFO] - Training Epoch: 2/2, step 2042/7134 completed (loss: 0.008773809298872948, acc: 0.9979838728904724)
[2025-02-13 03:53:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:39][root][INFO] - Training Epoch: 2/2, step 2043/7134 completed (loss: 0.03291161730885506, acc: 0.9906250238418579)
[2025-02-13 03:53:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:40][root][INFO] - Training Epoch: 2/2, step 2044/7134 completed (loss: 0.023599248379468918, acc: 0.9946332573890686)
[2025-02-13 03:53:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:40][root][INFO] - Training Epoch: 2/2, step 2045/7134 completed (loss: 0.013819737359881401, acc: 0.994575023651123)
[2025-02-13 03:53:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:41][root][INFO] - Training Epoch: 2/2, step 2046/7134 completed (loss: 0.018372632563114166, acc: 0.9939024448394775)
[2025-02-13 03:53:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:41][root][INFO] - Training Epoch: 2/2, step 2047/7134 completed (loss: 0.02650338225066662, acc: 0.9916527271270752)
[2025-02-13 03:53:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:41][root][INFO] - Training Epoch: 2/2, step 2048/7134 completed (loss: 0.03191893547773361, acc: 0.9895366430282593)
[2025-02-13 03:53:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:42][root][INFO] - Training Epoch: 2/2, step 2049/7134 completed (loss: 0.06501933932304382, acc: 0.9788235425949097)
[2025-02-13 03:53:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:42][root][INFO] - Training Epoch: 2/2, step 2050/7134 completed (loss: 0.08269728720188141, acc: 0.9811965823173523)
[2025-02-13 03:53:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:43][root][INFO] - Training Epoch: 2/2, step 2051/7134 completed (loss: 0.046370670199394226, acc: 0.98828125)
[2025-02-13 03:53:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:43][root][INFO] - Training Epoch: 2/2, step 2052/7134 completed (loss: 0.07948122173547745, acc: 0.9778156876564026)
[2025-02-13 03:53:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:43][root][INFO] - Training Epoch: 2/2, step 2053/7134 completed (loss: 0.03710896149277687, acc: 0.9900000095367432)
[2025-02-13 03:53:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:44][root][INFO] - Training Epoch: 2/2, step 2054/7134 completed (loss: 0.08391212671995163, acc: 0.9745222926139832)
[2025-02-13 03:53:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:44][root][INFO] - Training Epoch: 2/2, step 2055/7134 completed (loss: 0.06296893209218979, acc: 0.9842519760131836)
[2025-02-13 03:53:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:45][root][INFO] - Training Epoch: 2/2, step 2056/7134 completed (loss: 0.035781729966402054, acc: 0.9922178983688354)
[2025-02-13 03:53:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:45][root][INFO] - Training Epoch: 2/2, step 2057/7134 completed (loss: 0.04987534508109093, acc: 0.9823608994483948)
[2025-02-13 03:53:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:46][root][INFO] - Training Epoch: 2/2, step 2058/7134 completed (loss: 0.07813701033592224, acc: 0.9799138903617859)
[2025-02-13 03:53:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:46][root][INFO] - Training Epoch: 2/2, step 2059/7134 completed (loss: 0.1160411462187767, acc: 0.9740034937858582)
[2025-02-13 03:53:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:47][root][INFO] - Training Epoch: 2/2, step 2060/7134 completed (loss: 0.03127160668373108, acc: 0.9942775368690491)
[2025-02-13 03:53:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:47][root][INFO] - Training Epoch: 2/2, step 2061/7134 completed (loss: 0.05172321945428848, acc: 0.9858356714248657)
[2025-02-13 03:53:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:47][root][INFO] - Training Epoch: 2/2, step 2062/7134 completed (loss: 0.06349542737007141, acc: 0.9829221963882446)
[2025-02-13 03:53:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:48][root][INFO] - Training Epoch: 2/2, step 2063/7134 completed (loss: 0.07148820906877518, acc: 0.983208954334259)
[2025-02-13 03:53:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:48][root][INFO] - Training Epoch: 2/2, step 2064/7134 completed (loss: 0.014634288847446442, acc: 0.995312511920929)
[2025-02-13 03:53:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:49][root][INFO] - Training Epoch: 2/2, step 2065/7134 completed (loss: 0.052713606506586075, acc: 0.9842519760131836)
[2025-02-13 03:53:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:49][root][INFO] - Training Epoch: 2/2, step 2066/7134 completed (loss: 0.03978516161441803, acc: 0.9879759550094604)
[2025-02-13 03:53:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:50][root][INFO] - Training Epoch: 2/2, step 2067/7134 completed (loss: 0.021095436066389084, acc: 0.9962049126625061)
[2025-02-13 03:53:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:50][root][INFO] - Training Epoch: 2/2, step 2068/7134 completed (loss: 0.04749883711338043, acc: 0.9859485030174255)
[2025-02-13 03:53:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:50][root][INFO] - Training Epoch: 2/2, step 2069/7134 completed (loss: 0.02480963245034218, acc: 0.9907692074775696)
[2025-02-13 03:53:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:51][root][INFO] - Training Epoch: 2/2, step 2070/7134 completed (loss: 0.03590724989771843, acc: 0.9923954606056213)
[2025-02-13 03:53:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:51][root][INFO] - Training Epoch: 2/2, step 2071/7134 completed (loss: 0.0071740467101335526, acc: 1.0)
[2025-02-13 03:53:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:52][root][INFO] - Training Epoch: 2/2, step 2072/7134 completed (loss: 0.01677102781832218, acc: 0.9950494766235352)
[2025-02-13 03:53:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:52][root][INFO] - Training Epoch: 2/2, step 2073/7134 completed (loss: 0.02855900675058365, acc: 0.9903846383094788)
[2025-02-13 03:53:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:53][root][INFO] - Training Epoch: 2/2, step 2074/7134 completed (loss: 0.03294118121266365, acc: 0.9929577708244324)
[2025-02-13 03:53:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:53][root][INFO] - Training Epoch: 2/2, step 2075/7134 completed (loss: 0.026804177090525627, acc: 0.9883138537406921)
[2025-02-13 03:53:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:54][root][INFO] - Training Epoch: 2/2, step 2076/7134 completed (loss: 0.028716327622532845, acc: 0.9907692074775696)
[2025-02-13 03:53:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:54][root][INFO] - Training Epoch: 2/2, step 2077/7134 completed (loss: 0.01795114204287529, acc: 0.9946879148483276)
[2025-02-13 03:53:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:54][root][INFO] - Training Epoch: 2/2, step 2078/7134 completed (loss: 0.019718555733561516, acc: 0.9941860437393188)
[2025-02-13 03:53:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:55][root][INFO] - Training Epoch: 2/2, step 2079/7134 completed (loss: 0.01176429633051157, acc: 0.9973545074462891)
[2025-02-13 03:53:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:55][root][INFO] - Training Epoch: 2/2, step 2080/7134 completed (loss: 0.02986893430352211, acc: 0.9926470518112183)
[2025-02-13 03:53:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:56][root][INFO] - Training Epoch: 2/2, step 2081/7134 completed (loss: 0.008868110366165638, acc: 0.9985486268997192)
[2025-02-13 03:53:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:56][root][INFO] - Training Epoch: 2/2, step 2082/7134 completed (loss: 0.08482252806425095, acc: 0.9790356159210205)
[2025-02-13 03:53:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:57][root][INFO] - Training Epoch: 2/2, step 2083/7134 completed (loss: 0.038589153438806534, acc: 0.9962962865829468)
[2025-02-13 03:53:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:57][root][INFO] - Training Epoch: 2/2, step 2084/7134 completed (loss: 0.021000389009714127, acc: 0.9968944191932678)
[2025-02-13 03:53:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:58][root][INFO] - Training Epoch: 2/2, step 2085/7134 completed (loss: 0.008550602942705154, acc: 0.9981516003608704)
[2025-02-13 03:53:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:58][root][INFO] - Training Epoch: 2/2, step 2086/7134 completed (loss: 0.013858084566891193, acc: 0.9954338073730469)
[2025-02-13 03:53:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:58][root][INFO] - Training Epoch: 2/2, step 2087/7134 completed (loss: 0.004238206427544355, acc: 1.0)
[2025-02-13 03:53:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:59][root][INFO] - Training Epoch: 2/2, step 2088/7134 completed (loss: 0.028834529221057892, acc: 0.9913544654846191)
[2025-02-13 03:53:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:53:59][root][INFO] - Training Epoch: 2/2, step 2089/7134 completed (loss: 0.023120278492569923, acc: 0.9921135902404785)
[2025-02-13 03:53:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:00][root][INFO] - Training Epoch: 2/2, step 2090/7134 completed (loss: 0.007743850350379944, acc: 0.998420238494873)
[2025-02-13 03:54:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:00][root][INFO] - Training Epoch: 2/2, step 2091/7134 completed (loss: 0.015104113146662712, acc: 0.9966216087341309)
[2025-02-13 03:54:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:01][root][INFO] - Training Epoch: 2/2, step 2092/7134 completed (loss: 0.01050164271146059, acc: 0.9984227418899536)
[2025-02-13 03:54:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:01][root][INFO] - Training Epoch: 2/2, step 2093/7134 completed (loss: 0.021059077233076096, acc: 0.9945725798606873)
[2025-02-13 03:54:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:02][root][INFO] - Training Epoch: 2/2, step 2094/7134 completed (loss: 0.031157756224274635, acc: 0.9918864369392395)
[2025-02-13 03:54:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:02][root][INFO] - Training Epoch: 2/2, step 2095/7134 completed (loss: 0.027861248701810837, acc: 0.9907651543617249)
[2025-02-13 03:54:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:02][root][INFO] - Training Epoch: 2/2, step 2096/7134 completed (loss: 0.01676355116069317, acc: 0.9948275685310364)
[2025-02-13 03:54:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:03][root][INFO] - Training Epoch: 2/2, step 2097/7134 completed (loss: 0.008030311204493046, acc: 0.9971949458122253)
[2025-02-13 03:54:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:03][root][INFO] - Training Epoch: 2/2, step 2098/7134 completed (loss: 0.016478421166539192, acc: 0.9967845678329468)
[2025-02-13 03:54:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:04][root][INFO] - Training Epoch: 2/2, step 2099/7134 completed (loss: 0.021505849435925484, acc: 0.9944953918457031)
[2025-02-13 03:54:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:04][root][INFO] - Training Epoch: 2/2, step 2100/7134 completed (loss: 0.01762564480304718, acc: 0.9933599233627319)
[2025-02-13 03:54:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:04][root][INFO] - Training Epoch: 2/2, step 2101/7134 completed (loss: 0.011471202597022057, acc: 0.992443323135376)
[2025-02-13 03:54:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:05][root][INFO] - Training Epoch: 2/2, step 2102/7134 completed (loss: 0.02104521356523037, acc: 0.9923469424247742)
[2025-02-13 03:54:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:05][root][INFO] - Training Epoch: 2/2, step 2103/7134 completed (loss: 0.01654043048620224, acc: 0.995398759841919)
[2025-02-13 03:54:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:06][root][INFO] - Training Epoch: 2/2, step 2104/7134 completed (loss: 0.013527413830161095, acc: 0.9974226951599121)
[2025-02-13 03:54:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:06][root][INFO] - Training Epoch: 2/2, step 2105/7134 completed (loss: 0.03930584341287613, acc: 0.9874804615974426)
[2025-02-13 03:54:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:07][root][INFO] - Training Epoch: 2/2, step 2106/7134 completed (loss: 0.03408634662628174, acc: 0.9913793206214905)
[2025-02-13 03:54:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:07][root][INFO] - Training Epoch: 2/2, step 2107/7134 completed (loss: 0.018946276977658272, acc: 0.9932523369789124)
[2025-02-13 03:54:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:08][root][INFO] - Training Epoch: 2/2, step 2108/7134 completed (loss: 0.01761229708790779, acc: 0.9925261735916138)
[2025-02-13 03:54:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:08][root][INFO] - Training Epoch: 2/2, step 2109/7134 completed (loss: 0.053198158740997314, acc: 0.9858906269073486)
[2025-02-13 03:54:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:08][root][INFO] - Training Epoch: 2/2, step 2110/7134 completed (loss: 0.04920906946063042, acc: 0.9825581312179565)
[2025-02-13 03:54:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:09][root][INFO] - Training Epoch: 2/2, step 2111/7134 completed (loss: 0.03862228989601135, acc: 0.9922178983688354)
[2025-02-13 03:54:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:09][root][INFO] - Training Epoch: 2/2, step 2112/7134 completed (loss: 0.05087970197200775, acc: 0.9840764403343201)
[2025-02-13 03:54:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:10][root][INFO] - Training Epoch: 2/2, step 2113/7134 completed (loss: 0.015480492264032364, acc: 0.995529055595398)
[2025-02-13 03:54:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:10][root][INFO] - Training Epoch: 2/2, step 2114/7134 completed (loss: 0.02260345220565796, acc: 0.9921568632125854)
[2025-02-13 03:54:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:11][root][INFO] - Training Epoch: 2/2, step 2115/7134 completed (loss: 0.029372382909059525, acc: 0.991239070892334)
[2025-02-13 03:54:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:11][root][INFO] - Training Epoch: 2/2, step 2116/7134 completed (loss: 0.025849521160125732, acc: 0.9938650131225586)
[2025-02-13 03:54:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:11][root][INFO] - Training Epoch: 2/2, step 2117/7134 completed (loss: 0.014356483705341816, acc: 0.9925373196601868)
[2025-02-13 03:54:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:12][root][INFO] - Training Epoch: 2/2, step 2118/7134 completed (loss: 0.02526089921593666, acc: 0.9945429563522339)
[2025-02-13 03:54:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:12][root][INFO] - Training Epoch: 2/2, step 2119/7134 completed (loss: 0.029557012021541595, acc: 0.9895012974739075)
[2025-02-13 03:54:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:13][root][INFO] - Training Epoch: 2/2, step 2120/7134 completed (loss: 0.0254904106259346, acc: 0.9939117431640625)
[2025-02-13 03:54:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:13][root][INFO] - Training Epoch: 2/2, step 2121/7134 completed (loss: 0.01948213018476963, acc: 0.9936507940292358)
[2025-02-13 03:54:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:14][root][INFO] - Training Epoch: 2/2, step 2122/7134 completed (loss: 0.022542545571923256, acc: 0.9908854365348816)
[2025-02-13 03:54:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:14][root][INFO] - Training Epoch: 2/2, step 2123/7134 completed (loss: 0.01899719424545765, acc: 0.9952830076217651)
[2025-02-13 03:54:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:14][root][INFO] - Training Epoch: 2/2, step 2124/7134 completed (loss: 0.017449375241994858, acc: 0.9922118186950684)
[2025-02-13 03:54:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:15][root][INFO] - Training Epoch: 2/2, step 2125/7134 completed (loss: 0.00641897227615118, acc: 0.9984848499298096)
[2025-02-13 03:54:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:15][root][INFO] - Training Epoch: 2/2, step 2126/7134 completed (loss: 0.022777849808335304, acc: 0.9932773113250732)
[2025-02-13 03:54:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:16][root][INFO] - Training Epoch: 2/2, step 2127/7134 completed (loss: 0.015966588631272316, acc: 0.9955752491950989)
[2025-02-13 03:54:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:16][root][INFO] - Training Epoch: 2/2, step 2128/7134 completed (loss: 0.013482647947967052, acc: 0.9973753094673157)
[2025-02-13 03:54:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:17][root][INFO] - Training Epoch: 2/2, step 2129/7134 completed (loss: 0.02511577494442463, acc: 0.9924585223197937)
[2025-02-13 03:54:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:17][root][INFO] - Training Epoch: 2/2, step 2130/7134 completed (loss: 0.01960849203169346, acc: 0.9950494766235352)
[2025-02-13 03:54:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:17][root][INFO] - Training Epoch: 2/2, step 2131/7134 completed (loss: 0.01086045615375042, acc: 0.9969040155410767)
[2025-02-13 03:54:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:18][root][INFO] - Training Epoch: 2/2, step 2132/7134 completed (loss: 0.010451780632138252, acc: 0.9973045587539673)
[2025-02-13 03:54:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:18][root][INFO] - Training Epoch: 2/2, step 2133/7134 completed (loss: 0.016124408692121506, acc: 0.997357964515686)
[2025-02-13 03:54:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:19][root][INFO] - Training Epoch: 2/2, step 2134/7134 completed (loss: 0.011322961188852787, acc: 0.9973226189613342)
[2025-02-13 03:54:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:19][root][INFO] - Training Epoch: 2/2, step 2135/7134 completed (loss: 0.028036026284098625, acc: 0.9894578456878662)
[2025-02-13 03:54:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:20][root][INFO] - Training Epoch: 2/2, step 2136/7134 completed (loss: 0.014040825888514519, acc: 0.9946595430374146)
[2025-02-13 03:54:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:20][root][INFO] - Training Epoch: 2/2, step 2137/7134 completed (loss: 0.011762982234358788, acc: 0.996129035949707)
[2025-02-13 03:54:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:20][root][INFO] - Training Epoch: 2/2, step 2138/7134 completed (loss: 0.009051182307302952, acc: 0.9969512224197388)
[2025-02-13 03:54:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:21][root][INFO] - Training Epoch: 2/2, step 2139/7134 completed (loss: 0.01334797590970993, acc: 0.9958391189575195)
[2025-02-13 03:54:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:21][root][INFO] - Training Epoch: 2/2, step 2140/7134 completed (loss: 0.03990718349814415, acc: 0.9876033067703247)
[2025-02-13 03:54:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:22][root][INFO] - Training Epoch: 2/2, step 2141/7134 completed (loss: 0.013544414192438126, acc: 0.9941520690917969)
[2025-02-13 03:54:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:22][root][INFO] - Training Epoch: 2/2, step 2142/7134 completed (loss: 0.021076323464512825, acc: 0.9929178357124329)
[2025-02-13 03:54:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:23][root][INFO] - Training Epoch: 2/2, step 2143/7134 completed (loss: 0.015202442184090614, acc: 0.9947368502616882)
[2025-02-13 03:54:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:23][root][INFO] - Training Epoch: 2/2, step 2144/7134 completed (loss: 0.002687039552256465, acc: 1.0)
[2025-02-13 03:54:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:24][root][INFO] - Training Epoch: 2/2, step 2145/7134 completed (loss: 0.03265588358044624, acc: 0.9928469061851501)
[2025-02-13 03:54:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:24][root][INFO] - Training Epoch: 2/2, step 2146/7134 completed (loss: 0.013051246292889118, acc: 0.9982394576072693)
[2025-02-13 03:54:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:25][root][INFO] - Training Epoch: 2/2, step 2147/7134 completed (loss: 0.02558601088821888, acc: 0.9925187230110168)
[2025-02-13 03:54:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:25][root][INFO] - Training Epoch: 2/2, step 2148/7134 completed (loss: 0.03772246837615967, acc: 0.9912663698196411)
[2025-02-13 03:54:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:25][root][INFO] - Training Epoch: 2/2, step 2149/7134 completed (loss: 0.010727187618613243, acc: 0.9943661689758301)
[2025-02-13 03:54:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:26][root][INFO] - Training Epoch: 2/2, step 2150/7134 completed (loss: 0.017827002331614494, acc: 0.9933333396911621)
[2025-02-13 03:54:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:26][root][INFO] - Training Epoch: 2/2, step 2151/7134 completed (loss: 0.013205972500145435, acc: 0.9971428513526917)
[2025-02-13 03:54:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:27][root][INFO] - Training Epoch: 2/2, step 2152/7134 completed (loss: 0.053535256534814835, acc: 0.9797794222831726)
[2025-02-13 03:54:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:27][root][INFO] - Training Epoch: 2/2, step 2153/7134 completed (loss: 0.03587540239095688, acc: 0.9911209940910339)
[2025-02-13 03:54:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:28][root][INFO] - Training Epoch: 2/2, step 2154/7134 completed (loss: 0.050438959151506424, acc: 0.9886220097541809)
[2025-02-13 03:54:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:28][root][INFO] - Training Epoch: 2/2, step 2155/7134 completed (loss: 0.04348239675164223, acc: 0.9972527623176575)
[2025-02-13 03:54:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:29][root][INFO] - Training Epoch: 2/2, step 2156/7134 completed (loss: 0.007208284456282854, acc: 0.9982638955116272)
[2025-02-13 03:54:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:29][root][INFO] - Training Epoch: 2/2, step 2157/7134 completed (loss: 0.019483191892504692, acc: 0.9932960867881775)
[2025-02-13 03:54:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:30][root][INFO] - Training Epoch: 2/2, step 2158/7134 completed (loss: 0.04352010041475296, acc: 0.9920634627342224)
[2025-02-13 03:54:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:30][root][INFO] - Training Epoch: 2/2, step 2159/7134 completed (loss: 0.021057406440377235, acc: 0.9961685538291931)
[2025-02-13 03:54:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:30][root][INFO] - Training Epoch: 2/2, step 2160/7134 completed (loss: 0.023908326402306557, acc: 0.9903714060783386)
[2025-02-13 03:54:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:31][root][INFO] - Training Epoch: 2/2, step 2161/7134 completed (loss: 0.03829561173915863, acc: 0.991037130355835)
[2025-02-13 03:54:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:31][root][INFO] - Training Epoch: 2/2, step 2162/7134 completed (loss: 0.027855176478624344, acc: 0.9939485788345337)
[2025-02-13 03:54:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:32][root][INFO] - Training Epoch: 2/2, step 2163/7134 completed (loss: 0.02881951443850994, acc: 0.9905511736869812)
[2025-02-13 03:54:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:32][root][INFO] - Training Epoch: 2/2, step 2164/7134 completed (loss: 0.015720220282673836, acc: 0.994854211807251)
[2025-02-13 03:54:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:33][root][INFO] - Training Epoch: 2/2, step 2165/7134 completed (loss: 0.020844344049692154, acc: 0.9942611455917358)
[2025-02-13 03:54:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:33][root][INFO] - Training Epoch: 2/2, step 2166/7134 completed (loss: 0.015072688460350037, acc: 0.9921875)
[2025-02-13 03:54:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:34][root][INFO] - Training Epoch: 2/2, step 2167/7134 completed (loss: 0.027339505031704903, acc: 0.9977728128433228)
[2025-02-13 03:54:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:34][root][INFO] - Training Epoch: 2/2, step 2168/7134 completed (loss: 0.020172012969851494, acc: 0.9952380657196045)
[2025-02-13 03:54:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:35][root][INFO] - Training Epoch: 2/2, step 2169/7134 completed (loss: 0.03439059108495712, acc: 0.9930394291877747)
[2025-02-13 03:54:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:35][root][INFO] - Training Epoch: 2/2, step 2170/7134 completed (loss: 0.02684829942882061, acc: 0.9883551597595215)
[2025-02-13 03:54:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:35][root][INFO] - Training Epoch: 2/2, step 2171/7134 completed (loss: 0.02359987609088421, acc: 0.9932050108909607)
[2025-02-13 03:54:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:36][root][INFO] - Training Epoch: 2/2, step 2172/7134 completed (loss: 0.02175397239625454, acc: 0.9927685856819153)
[2025-02-13 03:54:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:36][root][INFO] - Training Epoch: 2/2, step 2173/7134 completed (loss: 0.032655730843544006, acc: 0.995468258857727)
[2025-02-13 03:54:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:37][root][INFO] - Training Epoch: 2/2, step 2174/7134 completed (loss: 0.011410483159124851, acc: 0.9957447052001953)
[2025-02-13 03:54:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:37][root][INFO] - Training Epoch: 2/2, step 2175/7134 completed (loss: 0.05935234576463699, acc: 0.9916897416114807)
[2025-02-13 03:54:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:38][root][INFO] - Training Epoch: 2/2, step 2176/7134 completed (loss: 0.019139086827635765, acc: 0.9952606558799744)
[2025-02-13 03:54:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:38][root][INFO] - Training Epoch: 2/2, step 2177/7134 completed (loss: 0.015919726341962814, acc: 0.9935064911842346)
[2025-02-13 03:54:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:39][root][INFO] - Training Epoch: 2/2, step 2178/7134 completed (loss: 0.013572015799582005, acc: 0.9955849647521973)
[2025-02-13 03:54:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:39][root][INFO] - Training Epoch: 2/2, step 2179/7134 completed (loss: 0.026865150779485703, acc: 0.9949367046356201)
[2025-02-13 03:54:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:40][root][INFO] - Training Epoch: 2/2, step 2180/7134 completed (loss: 0.020201729610562325, acc: 0.9940898418426514)
[2025-02-13 03:54:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:40][root][INFO] - Training Epoch: 2/2, step 2181/7134 completed (loss: 0.017755700275301933, acc: 0.9971387982368469)
[2025-02-13 03:54:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:41][root][INFO] - Training Epoch: 2/2, step 2182/7134 completed (loss: 0.037867020815610886, acc: 0.9873125553131104)
[2025-02-13 03:54:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:41][root][INFO] - Training Epoch: 2/2, step 2183/7134 completed (loss: 0.022133678197860718, acc: 0.9953434467315674)
[2025-02-13 03:54:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:42][root][INFO] - Training Epoch: 2/2, step 2184/7134 completed (loss: 0.028620129451155663, acc: 0.9936102032661438)
[2025-02-13 03:54:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:42][root][INFO] - Training Epoch: 2/2, step 2185/7134 completed (loss: 0.04008930176496506, acc: 0.9858356714248657)
[2025-02-13 03:54:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:42][root][INFO] - Training Epoch: 2/2, step 2186/7134 completed (loss: 0.04236398637294769, acc: 0.9854133129119873)
[2025-02-13 03:54:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:43][root][INFO] - Training Epoch: 2/2, step 2187/7134 completed (loss: 0.08348774164915085, acc: 0.9775999784469604)
[2025-02-13 03:54:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:43][root][INFO] - Training Epoch: 2/2, step 2188/7134 completed (loss: 0.01828763447701931, acc: 0.99452805519104)
[2025-02-13 03:54:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:44][root][INFO] - Training Epoch: 2/2, step 2189/7134 completed (loss: 0.031653400510549545, acc: 0.9938837885856628)
[2025-02-13 03:54:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:44][root][INFO] - Training Epoch: 2/2, step 2190/7134 completed (loss: 0.028800101950764656, acc: 0.9907651543617249)
[2025-02-13 03:54:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:45][root][INFO] - Training Epoch: 2/2, step 2191/7134 completed (loss: 0.019247760996222496, acc: 0.9918699264526367)
[2025-02-13 03:54:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:45][root][INFO] - Training Epoch: 2/2, step 2192/7134 completed (loss: 0.004494430031627417, acc: 1.0)
[2025-02-13 03:54:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:46][root][INFO] - Training Epoch: 2/2, step 2193/7134 completed (loss: 0.016242431476712227, acc: 0.9946808218955994)
[2025-02-13 03:54:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:46][root][INFO] - Training Epoch: 2/2, step 2194/7134 completed (loss: 0.025217216461896896, acc: 0.9899497628211975)
[2025-02-13 03:54:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:46][root][INFO] - Training Epoch: 2/2, step 2195/7134 completed (loss: 0.011925671249628067, acc: 0.995199978351593)
[2025-02-13 03:54:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:47][root][INFO] - Training Epoch: 2/2, step 2196/7134 completed (loss: 0.020871542394161224, acc: 0.9935232996940613)
[2025-02-13 03:54:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:47][root][INFO] - Training Epoch: 2/2, step 2197/7134 completed (loss: 0.03160158544778824, acc: 0.9908925294876099)
[2025-02-13 03:54:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:48][root][INFO] - Training Epoch: 2/2, step 2198/7134 completed (loss: 0.026252787560224533, acc: 0.9879336357116699)
[2025-02-13 03:54:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:48][root][INFO] - Training Epoch: 2/2, step 2199/7134 completed (loss: 0.02152470499277115, acc: 0.995468258857727)
[2025-02-13 03:54:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:49][root][INFO] - Training Epoch: 2/2, step 2200/7134 completed (loss: 0.015779128298163414, acc: 0.9963833689689636)
[2025-02-13 03:54:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:49][root][INFO] - Training Epoch: 2/2, step 2201/7134 completed (loss: 0.008710135705769062, acc: 0.9969696998596191)
[2025-02-13 03:54:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:49][root][INFO] - Training Epoch: 2/2, step 2202/7134 completed (loss: 0.015123591758310795, acc: 0.9933221936225891)
[2025-02-13 03:54:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:50][root][INFO] - Training Epoch: 2/2, step 2203/7134 completed (loss: 0.00597524456679821, acc: 0.9972602725028992)
[2025-02-13 03:54:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:50][root][INFO] - Training Epoch: 2/2, step 2204/7134 completed (loss: 0.010139552876353264, acc: 0.9970760345458984)
[2025-02-13 03:54:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:51][root][INFO] - Training Epoch: 2/2, step 2205/7134 completed (loss: 0.012822871096432209, acc: 0.9964912533760071)
[2025-02-13 03:54:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:51][root][INFO] - Training Epoch: 2/2, step 2206/7134 completed (loss: 0.04037756845355034, acc: 0.9964850544929504)
[2025-02-13 03:54:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:52][root][INFO] - Training Epoch: 2/2, step 2207/7134 completed (loss: 0.018415899947285652, acc: 0.9967690110206604)
[2025-02-13 03:54:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:52][root][INFO] - Training Epoch: 2/2, step 2208/7134 completed (loss: 0.03804948180913925, acc: 0.9900826215744019)
[2025-02-13 03:54:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:52][root][INFO] - Training Epoch: 2/2, step 2209/7134 completed (loss: 0.019781868904829025, acc: 0.9929278492927551)
[2025-02-13 03:54:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:53][root][INFO] - Training Epoch: 2/2, step 2210/7134 completed (loss: 0.03143838793039322, acc: 0.9924127459526062)
[2025-02-13 03:54:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:53][root][INFO] - Training Epoch: 2/2, step 2211/7134 completed (loss: 0.023516371846199036, acc: 0.9944598078727722)
[2025-02-13 03:54:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:54][root][INFO] - Training Epoch: 2/2, step 2212/7134 completed (loss: 0.010027790442109108, acc: 0.9958333373069763)
[2025-02-13 03:54:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:54][root][INFO] - Training Epoch: 2/2, step 2213/7134 completed (loss: 0.024786539375782013, acc: 0.9929378628730774)
[2025-02-13 03:54:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:55][root][INFO] - Training Epoch: 2/2, step 2214/7134 completed (loss: 0.019118832424283028, acc: 0.9925373196601868)
[2025-02-13 03:54:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:55][root][INFO] - Training Epoch: 2/2, step 2215/7134 completed (loss: 0.010438023135066032, acc: 0.9943820238113403)
[2025-02-13 03:54:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:55][root][INFO] - Training Epoch: 2/2, step 2216/7134 completed (loss: 0.01220529992133379, acc: 0.9959946870803833)
[2025-02-13 03:54:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:56][root][INFO] - Training Epoch: 2/2, step 2217/7134 completed (loss: 0.0197458453476429, acc: 0.9925280213356018)
[2025-02-13 03:54:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:56][root][INFO] - Training Epoch: 2/2, step 2218/7134 completed (loss: 0.02764813043177128, acc: 0.9928160905838013)
[2025-02-13 03:54:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:57][root][INFO] - Training Epoch: 2/2, step 2219/7134 completed (loss: 0.008518144488334656, acc: 0.9984543919563293)
[2025-02-13 03:54:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:57][root][INFO] - Training Epoch: 2/2, step 2220/7134 completed (loss: 0.009081878699362278, acc: 0.99842768907547)
[2025-02-13 03:54:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:58][root][INFO] - Training Epoch: 2/2, step 2221/7134 completed (loss: 0.014361381530761719, acc: 0.9974026083946228)
[2025-02-13 03:54:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:58][root][INFO] - Training Epoch: 2/2, step 2222/7134 completed (loss: 0.020904498174786568, acc: 0.9927745461463928)
[2025-02-13 03:54:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:58][root][INFO] - Training Epoch: 2/2, step 2223/7134 completed (loss: 0.017791423946619034, acc: 0.9972106218338013)
[2025-02-13 03:54:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:59][root][INFO] - Training Epoch: 2/2, step 2224/7134 completed (loss: 0.023639705032110214, acc: 0.9931412935256958)
[2025-02-13 03:54:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:54:59][root][INFO] - Training Epoch: 2/2, step 2225/7134 completed (loss: 0.01449243538081646, acc: 0.9942113161087036)
[2025-02-13 03:54:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:00][root][INFO] - Training Epoch: 2/2, step 2226/7134 completed (loss: 0.027242597192525864, acc: 0.990867555141449)
[2025-02-13 03:55:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:00][root][INFO] - Training Epoch: 2/2, step 2227/7134 completed (loss: 0.026621654629707336, acc: 0.9956076145172119)
[2025-02-13 03:55:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:01][root][INFO] - Training Epoch: 2/2, step 2228/7134 completed (loss: 0.005926322657614946, acc: 0.9983739852905273)
[2025-02-13 03:55:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:01][root][INFO] - Training Epoch: 2/2, step 2229/7134 completed (loss: 0.01746603287756443, acc: 0.9940000176429749)
[2025-02-13 03:55:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:01][root][INFO] - Training Epoch: 2/2, step 2230/7134 completed (loss: 0.018881667405366898, acc: 0.9951456189155579)
[2025-02-13 03:55:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:02][root][INFO] - Training Epoch: 2/2, step 2231/7134 completed (loss: 0.03246818855404854, acc: 0.9880478382110596)
[2025-02-13 03:55:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:02][root][INFO] - Training Epoch: 2/2, step 2232/7134 completed (loss: 0.012607278302311897, acc: 0.9959893226623535)
[2025-02-13 03:55:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:03][root][INFO] - Training Epoch: 2/2, step 2233/7134 completed (loss: 0.01841983199119568, acc: 0.991465151309967)
[2025-02-13 03:55:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:03][root][INFO] - Training Epoch: 2/2, step 2234/7134 completed (loss: 0.0028528764378279448, acc: 1.0)
[2025-02-13 03:55:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:04][root][INFO] - Training Epoch: 2/2, step 2235/7134 completed (loss: 0.03395526483654976, acc: 0.9937304258346558)
[2025-02-13 03:55:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:04][root][INFO] - Training Epoch: 2/2, step 2236/7134 completed (loss: 0.01632559485733509, acc: 0.9940029978752136)
[2025-02-13 03:55:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:05][root][INFO] - Training Epoch: 2/2, step 2237/7134 completed (loss: 0.02936382405459881, acc: 0.9915134310722351)
[2025-02-13 03:55:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:05][root][INFO] - Training Epoch: 2/2, step 2238/7134 completed (loss: 0.04646221920847893, acc: 0.9855282306671143)
[2025-02-13 03:55:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:05][root][INFO] - Training Epoch: 2/2, step 2239/7134 completed (loss: 0.06883998960256577, acc: 0.9792208075523376)
[2025-02-13 03:55:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:06][root][INFO] - Training Epoch: 2/2, step 2240/7134 completed (loss: 0.03337659314274788, acc: 0.9922077655792236)
[2025-02-13 03:55:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:06][root][INFO] - Training Epoch: 2/2, step 2241/7134 completed (loss: 0.0599830336868763, acc: 0.9834515452384949)
[2025-02-13 03:55:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:07][root][INFO] - Training Epoch: 2/2, step 2242/7134 completed (loss: 0.08069787174463272, acc: 0.978622317314148)
[2025-02-13 03:55:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:07][root][INFO] - Training Epoch: 2/2, step 2243/7134 completed (loss: 0.07313469797372818, acc: 0.9801526665687561)
[2025-02-13 03:55:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:08][root][INFO] - Training Epoch: 2/2, step 2244/7134 completed (loss: 0.018706072121858597, acc: 0.9937106966972351)
[2025-02-13 03:55:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:08][root][INFO] - Training Epoch: 2/2, step 2245/7134 completed (loss: 0.05941147729754448, acc: 0.9773463010787964)
[2025-02-13 03:55:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:08][root][INFO] - Training Epoch: 2/2, step 2246/7134 completed (loss: 0.0776868686079979, acc: 0.9743243455886841)
[2025-02-13 03:55:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:09][root][INFO] - Training Epoch: 2/2, step 2247/7134 completed (loss: 0.05880986526608467, acc: 0.9885057210922241)
[2025-02-13 03:55:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:09][root][INFO] - Training Epoch: 2/2, step 2248/7134 completed (loss: 0.04626798257231712, acc: 0.9903181195259094)
[2025-02-13 03:55:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:10][root][INFO] - Training Epoch: 2/2, step 2249/7134 completed (loss: 0.03510307893157005, acc: 0.9888268113136292)
[2025-02-13 03:55:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:10][root][INFO] - Training Epoch: 2/2, step 2250/7134 completed (loss: 0.03453104570508003, acc: 0.9919785857200623)
[2025-02-13 03:55:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:11][root][INFO] - Training Epoch: 2/2, step 2251/7134 completed (loss: 0.03857745975255966, acc: 0.9931972622871399)
[2025-02-13 03:55:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:11][root][INFO] - Training Epoch: 2/2, step 2252/7134 completed (loss: 0.021642645820975304, acc: 0.994397759437561)
[2025-02-13 03:55:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:11][root][INFO] - Training Epoch: 2/2, step 2253/7134 completed (loss: 0.021662233397364616, acc: 0.9933110475540161)
[2025-02-13 03:55:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:12][root][INFO] - Training Epoch: 2/2, step 2254/7134 completed (loss: 0.08180779218673706, acc: 0.9780876636505127)
[2025-02-13 03:55:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:12][root][INFO] - Training Epoch: 2/2, step 2255/7134 completed (loss: 0.031569741666316986, acc: 0.9893778562545776)
[2025-02-13 03:55:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:13][root][INFO] - Training Epoch: 2/2, step 2256/7134 completed (loss: 0.0460987463593483, acc: 0.98893803358078)
[2025-02-13 03:55:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:13][root][INFO] - Training Epoch: 2/2, step 2257/7134 completed (loss: 0.019600505009293556, acc: 0.995468258857727)
[2025-02-13 03:55:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:14][root][INFO] - Training Epoch: 2/2, step 2258/7134 completed (loss: 0.02683485485613346, acc: 0.9923076629638672)
[2025-02-13 03:55:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:14][root][INFO] - Training Epoch: 2/2, step 2259/7134 completed (loss: 0.016167161986231804, acc: 0.9933333396911621)
[2025-02-13 03:55:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:14][root][INFO] - Training Epoch: 2/2, step 2260/7134 completed (loss: 0.050531212240457535, acc: 0.9863636493682861)
[2025-02-13 03:55:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:15][root][INFO] - Training Epoch: 2/2, step 2261/7134 completed (loss: 0.08366677910089493, acc: 0.974452555179596)
[2025-02-13 03:55:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:15][root][INFO] - Training Epoch: 2/2, step 2262/7134 completed (loss: 0.013775994069874287, acc: 0.9981583952903748)
[2025-02-13 03:55:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:16][root][INFO] - Training Epoch: 2/2, step 2263/7134 completed (loss: 0.031943414360284805, acc: 0.9945945739746094)
[2025-02-13 03:55:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:16][root][INFO] - Training Epoch: 2/2, step 2264/7134 completed (loss: 0.06996838003396988, acc: 0.9789029359817505)
[2025-02-13 03:55:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:16][root][INFO] - Training Epoch: 2/2, step 2265/7134 completed (loss: 0.06919433176517487, acc: 0.975944995880127)
[2025-02-13 03:55:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:17][root][INFO] - Training Epoch: 2/2, step 2266/7134 completed (loss: 0.03385522961616516, acc: 0.9932975769042969)
[2025-02-13 03:55:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:17][root][INFO] - Training Epoch: 2/2, step 2267/7134 completed (loss: 0.02227633260190487, acc: 0.9958847761154175)
[2025-02-13 03:55:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:18][root][INFO] - Training Epoch: 2/2, step 2268/7134 completed (loss: 0.031358055770397186, acc: 0.991391658782959)
[2025-02-13 03:55:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:18][root][INFO] - Training Epoch: 2/2, step 2269/7134 completed (loss: 0.010991022922098637, acc: 0.99622642993927)
[2025-02-13 03:55:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:19][root][INFO] - Training Epoch: 2/2, step 2270/7134 completed (loss: 0.05945814773440361, acc: 0.9850746393203735)
[2025-02-13 03:55:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:19][root][INFO] - Training Epoch: 2/2, step 2271/7134 completed (loss: 0.02623511292040348, acc: 0.9919447898864746)
[2025-02-13 03:55:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:19][root][INFO] - Training Epoch: 2/2, step 2272/7134 completed (loss: 0.019879460334777832, acc: 0.9963325262069702)
[2025-02-13 03:55:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:20][root][INFO] - Training Epoch: 2/2, step 2273/7134 completed (loss: 0.014325723052024841, acc: 0.9952095746994019)
[2025-02-13 03:55:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:20][root][INFO] - Training Epoch: 2/2, step 2274/7134 completed (loss: 0.09062641859054565, acc: 0.9834395051002502)
[2025-02-13 03:55:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:21][root][INFO] - Training Epoch: 2/2, step 2275/7134 completed (loss: 0.024629008024930954, acc: 0.9917840361595154)
[2025-02-13 03:55:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:21][root][INFO] - Training Epoch: 2/2, step 2276/7134 completed (loss: 0.03706854581832886, acc: 0.9928910136222839)
[2025-02-13 03:55:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:22][root][INFO] - Training Epoch: 2/2, step 2277/7134 completed (loss: 0.022260015830397606, acc: 0.9953810572624207)
[2025-02-13 03:55:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:22][root][INFO] - Training Epoch: 2/2, step 2278/7134 completed (loss: 0.07886593043804169, acc: 0.9840255379676819)
[2025-02-13 03:55:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:23][root][INFO] - Training Epoch: 2/2, step 2279/7134 completed (loss: 0.01815909706056118, acc: 0.9940968155860901)
[2025-02-13 03:55:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:23][root][INFO] - Training Epoch: 2/2, step 2280/7134 completed (loss: 0.04146904870867729, acc: 0.9893742799758911)
[2025-02-13 03:55:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:24][root][INFO] - Training Epoch: 2/2, step 2281/7134 completed (loss: 0.06301774829626083, acc: 0.9906666874885559)
[2025-02-13 03:55:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:24][root][INFO] - Training Epoch: 2/2, step 2282/7134 completed (loss: 0.009727771393954754, acc: 0.998763918876648)
[2025-02-13 03:55:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:24][root][INFO] - Training Epoch: 2/2, step 2283/7134 completed (loss: 0.020662734284996986, acc: 0.9926650524139404)
[2025-02-13 03:55:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:25][root][INFO] - Training Epoch: 2/2, step 2284/7134 completed (loss: 0.024668492376804352, acc: 0.9911373853683472)
[2025-02-13 03:55:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:25][root][INFO] - Training Epoch: 2/2, step 2285/7134 completed (loss: 0.010808680206537247, acc: 0.997770369052887)
[2025-02-13 03:55:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:26][root][INFO] - Training Epoch: 2/2, step 2286/7134 completed (loss: 0.026879893615841866, acc: 0.9896238446235657)
[2025-02-13 03:55:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:26][root][INFO] - Training Epoch: 2/2, step 2287/7134 completed (loss: 0.021098004654049873, acc: 0.989276111125946)
[2025-02-13 03:55:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:27][root][INFO] - Training Epoch: 2/2, step 2288/7134 completed (loss: 0.021120678633451462, acc: 0.9929328560829163)
[2025-02-13 03:55:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:27][root][INFO] - Training Epoch: 2/2, step 2289/7134 completed (loss: 0.01468619517982006, acc: 0.9974325895309448)
[2025-02-13 03:55:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:28][root][INFO] - Training Epoch: 2/2, step 2290/7134 completed (loss: 0.013910728506743908, acc: 0.9961783289909363)
[2025-02-13 03:55:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:28][root][INFO] - Training Epoch: 2/2, step 2291/7134 completed (loss: 0.020047705620527267, acc: 0.9946019053459167)
[2025-02-13 03:55:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:28][root][INFO] - Training Epoch: 2/2, step 2292/7134 completed (loss: 0.05280086025595665, acc: 0.9896774291992188)
[2025-02-13 03:55:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:29][root][INFO] - Training Epoch: 2/2, step 2293/7134 completed (loss: 0.028462961316108704, acc: 0.9922049045562744)
[2025-02-13 03:55:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:29][root][INFO] - Training Epoch: 2/2, step 2294/7134 completed (loss: 0.03770948946475983, acc: 0.9898762702941895)
[2025-02-13 03:55:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:30][root][INFO] - Training Epoch: 2/2, step 2295/7134 completed (loss: 0.02939162217080593, acc: 0.9914529919624329)
[2025-02-13 03:55:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:30][root][INFO] - Training Epoch: 2/2, step 2296/7134 completed (loss: 0.020147355273365974, acc: 0.9946808218955994)
[2025-02-13 03:55:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:31][root][INFO] - Training Epoch: 2/2, step 2297/7134 completed (loss: 0.019652288407087326, acc: 0.994918704032898)
[2025-02-13 03:55:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:31][root][INFO] - Training Epoch: 2/2, step 2298/7134 completed (loss: 0.041329145431518555, acc: 0.9884332418441772)
[2025-02-13 03:55:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:32][root][INFO] - Training Epoch: 2/2, step 2299/7134 completed (loss: 0.016102880239486694, acc: 0.997724711894989)
[2025-02-13 03:55:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:32][root][INFO] - Training Epoch: 2/2, step 2300/7134 completed (loss: 0.006665792316198349, acc: 1.0)
[2025-02-13 03:55:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:33][root][INFO] - Training Epoch: 2/2, step 2301/7134 completed (loss: 0.02131771109998226, acc: 0.9960265159606934)
[2025-02-13 03:55:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:33][root][INFO] - Training Epoch: 2/2, step 2302/7134 completed (loss: 0.031987279653549194, acc: 0.9865196347236633)
[2025-02-13 03:55:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:34][root][INFO] - Training Epoch: 2/2, step 2303/7134 completed (loss: 0.02162613719701767, acc: 0.9943052530288696)
[2025-02-13 03:55:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:34][root][INFO] - Training Epoch: 2/2, step 2304/7134 completed (loss: 0.03725481778383255, acc: 0.9876670241355896)
[2025-02-13 03:55:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:34][root][INFO] - Training Epoch: 2/2, step 2305/7134 completed (loss: 0.011933611705899239, acc: 0.9968051314353943)
[2025-02-13 03:55:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:35][root][INFO] - Training Epoch: 2/2, step 2306/7134 completed (loss: 0.012275227345526218, acc: 0.9955654144287109)
[2025-02-13 03:55:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:35][root][INFO] - Training Epoch: 2/2, step 2307/7134 completed (loss: 0.024491583928465843, acc: 0.9948347210884094)
[2025-02-13 03:55:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:36][root][INFO] - Training Epoch: 2/2, step 2308/7134 completed (loss: 0.028519947081804276, acc: 0.9918283820152283)
[2025-02-13 03:55:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:36][root][INFO] - Training Epoch: 2/2, step 2309/7134 completed (loss: 0.017672095447778702, acc: 0.9940047860145569)
[2025-02-13 03:55:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:37][root][INFO] - Training Epoch: 2/2, step 2310/7134 completed (loss: 0.014993914403021336, acc: 0.996610164642334)
[2025-02-13 03:55:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:37][root][INFO] - Training Epoch: 2/2, step 2311/7134 completed (loss: 0.02195567823946476, acc: 0.9931192398071289)
[2025-02-13 03:55:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:38][root][INFO] - Training Epoch: 2/2, step 2312/7134 completed (loss: 0.024477878585457802, acc: 0.9946004152297974)
[2025-02-13 03:55:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:38][root][INFO] - Training Epoch: 2/2, step 2313/7134 completed (loss: 0.02748752571642399, acc: 0.9920091032981873)
[2025-02-13 03:55:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:39][root][INFO] - Training Epoch: 2/2, step 2314/7134 completed (loss: 0.014847864396870136, acc: 0.9944506287574768)
[2025-02-13 03:55:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:39][root][INFO] - Training Epoch: 2/2, step 2315/7134 completed (loss: 0.01704590581357479, acc: 0.9952940940856934)
[2025-02-13 03:55:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:40][root][INFO] - Training Epoch: 2/2, step 2316/7134 completed (loss: 0.012748749926686287, acc: 0.9965477585792542)
[2025-02-13 03:55:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:40][root][INFO] - Training Epoch: 2/2, step 2317/7134 completed (loss: 0.0252229031175375, acc: 0.9947090148925781)
[2025-02-13 03:55:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:40][root][INFO] - Training Epoch: 2/2, step 2318/7134 completed (loss: 0.012391355820000172, acc: 0.9947698712348938)
[2025-02-13 03:55:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:41][root][INFO] - Training Epoch: 2/2, step 2319/7134 completed (loss: 0.012005609460175037, acc: 0.9967177510261536)
[2025-02-13 03:55:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:41][root][INFO] - Training Epoch: 2/2, step 2320/7134 completed (loss: 0.017261633649468422, acc: 0.9943181872367859)
[2025-02-13 03:55:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:42][root][INFO] - Training Epoch: 2/2, step 2321/7134 completed (loss: 0.016062073409557343, acc: 0.9930555820465088)
[2025-02-13 03:55:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:42][root][INFO] - Training Epoch: 2/2, step 2322/7134 completed (loss: 0.016849854961037636, acc: 0.9911110997200012)
[2025-02-13 03:55:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:43][root][INFO] - Training Epoch: 2/2, step 2323/7134 completed (loss: 0.030823228880763054, acc: 0.9858044385910034)
[2025-02-13 03:55:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:43][root][INFO] - Training Epoch: 2/2, step 2324/7134 completed (loss: 0.03944265842437744, acc: 0.9900285005569458)
[2025-02-13 03:55:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:44][root][INFO] - Training Epoch: 2/2, step 2325/7134 completed (loss: 0.03303789347410202, acc: 0.988034188747406)
[2025-02-13 03:55:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:44][root][INFO] - Training Epoch: 2/2, step 2326/7134 completed (loss: 0.009750421158969402, acc: 0.998161792755127)
[2025-02-13 03:55:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:44][root][INFO] - Training Epoch: 2/2, step 2327/7134 completed (loss: 0.009248292073607445, acc: 0.9967213273048401)
[2025-02-13 03:55:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:45][root][INFO] - Training Epoch: 2/2, step 2328/7134 completed (loss: 0.05140114203095436, acc: 0.9874551892280579)
[2025-02-13 03:55:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:45][root][INFO] - Training Epoch: 2/2, step 2329/7134 completed (loss: 0.025226959958672523, acc: 0.9984496235847473)
[2025-02-13 03:55:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:46][root][INFO] - Training Epoch: 2/2, step 2330/7134 completed (loss: 0.008643360808491707, acc: 0.9984732866287231)
[2025-02-13 03:55:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:46][root][INFO] - Training Epoch: 2/2, step 2331/7134 completed (loss: 0.00862846989184618, acc: 0.9984662532806396)
[2025-02-13 03:55:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:47][root][INFO] - Training Epoch: 2/2, step 2332/7134 completed (loss: 0.031894832849502563, acc: 0.9940564632415771)
[2025-02-13 03:55:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:47][root][INFO] - Training Epoch: 2/2, step 2333/7134 completed (loss: 0.014433003962039948, acc: 0.9955357313156128)
[2025-02-13 03:55:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:47][root][INFO] - Training Epoch: 2/2, step 2334/7134 completed (loss: 0.014747826382517815, acc: 0.9955223798751831)
[2025-02-13 03:55:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:48][root][INFO] - Training Epoch: 2/2, step 2335/7134 completed (loss: 0.019481416791677475, acc: 0.9903225898742676)
[2025-02-13 03:55:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:48][root][INFO] - Training Epoch: 2/2, step 2336/7134 completed (loss: 0.011108354665338993, acc: 0.9984543919563293)
[2025-02-13 03:55:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:49][root][INFO] - Training Epoch: 2/2, step 2337/7134 completed (loss: 0.002557432046160102, acc: 1.0)
[2025-02-13 03:55:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:49][root][INFO] - Training Epoch: 2/2, step 2338/7134 completed (loss: 0.011113815940916538, acc: 0.9953632354736328)
[2025-02-13 03:55:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:50][root][INFO] - Training Epoch: 2/2, step 2339/7134 completed (loss: 0.032511789351701736, acc: 0.990212082862854)
[2025-02-13 03:55:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:50][root][INFO] - Training Epoch: 2/2, step 2340/7134 completed (loss: 0.010811218060553074, acc: 0.9969834089279175)
[2025-02-13 03:55:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:50][root][INFO] - Training Epoch: 2/2, step 2341/7134 completed (loss: 0.004688458517193794, acc: 0.9981203079223633)
[2025-02-13 03:55:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:51][root][INFO] - Training Epoch: 2/2, step 2342/7134 completed (loss: 0.010925604030489922, acc: 0.9970458149909973)
[2025-02-13 03:55:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:51][root][INFO] - Training Epoch: 2/2, step 2343/7134 completed (loss: 0.010179794393479824, acc: 0.9968000054359436)
[2025-02-13 03:55:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:52][root][INFO] - Training Epoch: 2/2, step 2344/7134 completed (loss: 0.009964246302843094, acc: 0.9968701004981995)
[2025-02-13 03:55:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:52][root][INFO] - Training Epoch: 2/2, step 2345/7134 completed (loss: 0.00978147890418768, acc: 0.9958041906356812)
[2025-02-13 03:55:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:52][root][INFO] - Training Epoch: 2/2, step 2346/7134 completed (loss: 0.0116953756660223, acc: 0.9971671104431152)
[2025-02-13 03:55:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:53][root][INFO] - Training Epoch: 2/2, step 2347/7134 completed (loss: 0.025216007605195045, acc: 0.9934210777282715)
[2025-02-13 03:55:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:53][root][INFO] - Training Epoch: 2/2, step 2348/7134 completed (loss: 0.05725710093975067, acc: 0.9861286282539368)
[2025-02-13 03:55:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:54][root][INFO] - Training Epoch: 2/2, step 2349/7134 completed (loss: 0.027899863198399544, acc: 0.9916666746139526)
[2025-02-13 03:55:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:54][root][INFO] - Training Epoch: 2/2, step 2350/7134 completed (loss: 0.058843180537223816, acc: 0.9862805008888245)
[2025-02-13 03:55:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:55][root][INFO] - Training Epoch: 2/2, step 2351/7134 completed (loss: 0.046481210738420486, acc: 0.9863387942314148)
[2025-02-13 03:55:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:55][root][INFO] - Training Epoch: 2/2, step 2352/7134 completed (loss: 0.020370086655020714, acc: 0.9935897588729858)
[2025-02-13 03:55:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:56][root][INFO] - Training Epoch: 2/2, step 2353/7134 completed (loss: 0.04388232156634331, acc: 0.9886524677276611)
[2025-02-13 03:55:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:56][root][INFO] - Training Epoch: 2/2, step 2354/7134 completed (loss: 0.01937967725098133, acc: 0.9924050569534302)
[2025-02-13 03:55:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:57][root][INFO] - Training Epoch: 2/2, step 2355/7134 completed (loss: 0.0366186685860157, acc: 0.9888888597488403)
[2025-02-13 03:55:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:57][root][INFO] - Training Epoch: 2/2, step 2356/7134 completed (loss: 0.036152929067611694, acc: 0.9893993139266968)
[2025-02-13 03:55:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:57][root][INFO] - Training Epoch: 2/2, step 2357/7134 completed (loss: 0.021627912297844887, acc: 0.9960317611694336)
[2025-02-13 03:55:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:58][root][INFO] - Training Epoch: 2/2, step 2358/7134 completed (loss: 0.03415573015809059, acc: 0.9879518151283264)
[2025-02-13 03:55:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:58][root][INFO] - Training Epoch: 2/2, step 2359/7134 completed (loss: 0.028511174023151398, acc: 0.9911894202232361)
[2025-02-13 03:55:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:59][root][INFO] - Training Epoch: 2/2, step 2360/7134 completed (loss: 0.016628090292215347, acc: 0.9941434860229492)
[2025-02-13 03:55:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:55:59][root][INFO] - Training Epoch: 2/2, step 2361/7134 completed (loss: 0.029035454615950584, acc: 0.9940828680992126)
[2025-02-13 03:55:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:00][root][INFO] - Training Epoch: 2/2, step 2362/7134 completed (loss: 0.0169440396130085, acc: 0.9932735562324524)
[2025-02-13 03:56:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:00][root][INFO] - Training Epoch: 2/2, step 2363/7134 completed (loss: 0.03735837712883949, acc: 0.9918887615203857)
[2025-02-13 03:56:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:00][root][INFO] - Training Epoch: 2/2, step 2364/7134 completed (loss: 0.012278826907277107, acc: 0.9969558715820312)
[2025-02-13 03:56:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:01][root][INFO] - Training Epoch: 2/2, step 2365/7134 completed (loss: 0.01294027641415596, acc: 0.997245192527771)
[2025-02-13 03:56:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:01][root][INFO] - Training Epoch: 2/2, step 2366/7134 completed (loss: 0.008775257505476475, acc: 0.9976851940155029)
[2025-02-13 03:56:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:02][root][INFO] - Training Epoch: 2/2, step 2367/7134 completed (loss: 0.04356842115521431, acc: 0.9886934757232666)
[2025-02-13 03:56:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:02][root][INFO] - Training Epoch: 2/2, step 2368/7134 completed (loss: 0.030601920560002327, acc: 0.9878787994384766)
[2025-02-13 03:56:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:03][root][INFO] - Training Epoch: 2/2, step 2369/7134 completed (loss: 0.012522555887699127, acc: 0.9976470470428467)
[2025-02-13 03:56:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:03][root][INFO] - Training Epoch: 2/2, step 2370/7134 completed (loss: 0.018528403714299202, acc: 0.9969135522842407)
[2025-02-13 03:56:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:04][root][INFO] - Training Epoch: 2/2, step 2371/7134 completed (loss: 0.010631690733134747, acc: 0.9971181750297546)
[2025-02-13 03:56:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:04][root][INFO] - Training Epoch: 2/2, step 2372/7134 completed (loss: 0.02158184163272381, acc: 0.9920634627342224)
[2025-02-13 03:56:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:05][root][INFO] - Training Epoch: 2/2, step 2373/7134 completed (loss: 0.007267056964337826, acc: 0.9975185990333557)
[2025-02-13 03:56:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:05][root][INFO] - Training Epoch: 2/2, step 2374/7134 completed (loss: 0.006718724966049194, acc: 0.9973226189613342)
[2025-02-13 03:56:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:05][root][INFO] - Training Epoch: 2/2, step 2375/7134 completed (loss: 0.027558226138353348, acc: 0.9954545497894287)
[2025-02-13 03:56:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:06][root][INFO] - Training Epoch: 2/2, step 2376/7134 completed (loss: 0.06370382010936737, acc: 0.9829642176628113)
[2025-02-13 03:56:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:06][root][INFO] - Training Epoch: 2/2, step 2377/7134 completed (loss: 0.04567217454314232, acc: 0.9892473220825195)
[2025-02-13 03:56:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:07][root][INFO] - Training Epoch: 2/2, step 2378/7134 completed (loss: 0.0396755114197731, acc: 0.9917840361595154)
[2025-02-13 03:56:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:07][root][INFO] - Training Epoch: 2/2, step 2379/7134 completed (loss: 0.07020889967679977, acc: 0.9806896448135376)
[2025-02-13 03:56:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:08][root][INFO] - Training Epoch: 2/2, step 2380/7134 completed (loss: 0.03348856791853905, acc: 0.990326464176178)
[2025-02-13 03:56:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:08][root][INFO] - Training Epoch: 2/2, step 2381/7134 completed (loss: 0.09743933379650116, acc: 0.9763113260269165)
[2025-02-13 03:56:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:09][root][INFO] - Training Epoch: 2/2, step 2382/7134 completed (loss: 0.027460483834147453, acc: 0.9938837885856628)
[2025-02-13 03:56:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:09][root][INFO] - Training Epoch: 2/2, step 2383/7134 completed (loss: 0.07028190046548843, acc: 0.9819004535675049)
[2025-02-13 03:56:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:09][root][INFO] - Training Epoch: 2/2, step 2384/7134 completed (loss: 0.052023518830537796, acc: 0.9882352948188782)
[2025-02-13 03:56:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:10][root][INFO] - Training Epoch: 2/2, step 2385/7134 completed (loss: 0.023967985063791275, acc: 0.9930939078330994)
[2025-02-13 03:56:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:10][root][INFO] - Training Epoch: 2/2, step 2386/7134 completed (loss: 0.02464982308447361, acc: 0.9921466112136841)
[2025-02-13 03:56:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:11][root][INFO] - Training Epoch: 2/2, step 2387/7134 completed (loss: 0.02212883159518242, acc: 0.9941995143890381)
[2025-02-13 03:56:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:11][root][INFO] - Training Epoch: 2/2, step 2388/7134 completed (loss: 0.06689395755529404, acc: 0.9785459041595459)
[2025-02-13 03:56:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:12][root][INFO] - Training Epoch: 2/2, step 2389/7134 completed (loss: 0.042931683361530304, acc: 0.9850746393203735)
[2025-02-13 03:56:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:12][root][INFO] - Training Epoch: 2/2, step 2390/7134 completed (loss: 0.056281495839357376, acc: 0.9808841347694397)
[2025-02-13 03:56:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:13][root][INFO] - Training Epoch: 2/2, step 2391/7134 completed (loss: 0.050403300672769547, acc: 0.9834815859794617)
[2025-02-13 03:56:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:13][root][INFO] - Training Epoch: 2/2, step 2392/7134 completed (loss: 0.0393146350979805, acc: 0.9893190860748291)
[2025-02-13 03:56:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:14][root][INFO] - Training Epoch: 2/2, step 2393/7134 completed (loss: 0.045687176287174225, acc: 0.9896238446235657)
[2025-02-13 03:56:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:14][root][INFO] - Training Epoch: 2/2, step 2394/7134 completed (loss: 0.020820369943976402, acc: 0.9939393997192383)
[2025-02-13 03:56:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:15][root][INFO] - Training Epoch: 2/2, step 2395/7134 completed (loss: 0.104477658867836, acc: 0.9723126888275146)
[2025-02-13 03:56:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:15][root][INFO] - Training Epoch: 2/2, step 2396/7134 completed (loss: 0.056484404951334, acc: 0.9779614210128784)
[2025-02-13 03:56:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:16][root][INFO] - Training Epoch: 2/2, step 2397/7134 completed (loss: 0.07071055471897125, acc: 0.9781659245491028)
[2025-02-13 03:56:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:16][root][INFO] - Training Epoch: 2/2, step 2398/7134 completed (loss: 0.018689166754484177, acc: 0.9944979548454285)
[2025-02-13 03:56:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:16][root][INFO] - Training Epoch: 2/2, step 2399/7134 completed (loss: 0.04301200062036514, acc: 0.9842632412910461)
[2025-02-13 03:56:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:17][root][INFO] - Training Epoch: 2/2, step 2400/7134 completed (loss: 0.021294860169291496, acc: 0.9944827556610107)
[2025-02-13 03:56:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:17][root][INFO] - Training Epoch: 2/2, step 2401/7134 completed (loss: 0.08404184877872467, acc: 0.9841772317886353)
[2025-02-13 03:56:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:18][root][INFO] - Training Epoch: 2/2, step 2402/7134 completed (loss: 0.05318998917937279, acc: 0.9814077019691467)
[2025-02-13 03:56:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:18][root][INFO] - Training Epoch: 2/2, step 2403/7134 completed (loss: 0.030749047175049782, acc: 0.9886524677276611)
[2025-02-13 03:56:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:19][root][INFO] - Training Epoch: 2/2, step 2404/7134 completed (loss: 0.01602793298661709, acc: 0.994535505771637)
[2025-02-13 03:56:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:19][root][INFO] - Training Epoch: 2/2, step 2405/7134 completed (loss: 0.04026661068201065, acc: 0.9866220951080322)
[2025-02-13 03:56:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:20][root][INFO] - Training Epoch: 2/2, step 2406/7134 completed (loss: 0.0114893838763237, acc: 0.9960106611251831)
[2025-02-13 03:56:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:20][root][INFO] - Training Epoch: 2/2, step 2407/7134 completed (loss: 0.03172747418284416, acc: 0.9908814430236816)
[2025-02-13 03:56:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:21][root][INFO] - Training Epoch: 2/2, step 2408/7134 completed (loss: 0.08443967252969742, acc: 0.9723684191703796)
[2025-02-13 03:56:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:21][root][INFO] - Training Epoch: 2/2, step 2409/7134 completed (loss: 0.016381148248910904, acc: 0.9959016442298889)
[2025-02-13 03:56:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:22][root][INFO] - Training Epoch: 2/2, step 2410/7134 completed (loss: 0.023814378306269646, acc: 0.9885222315788269)
[2025-02-13 03:56:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:22][root][INFO] - Training Epoch: 2/2, step 2411/7134 completed (loss: 0.01599481888115406, acc: 0.9935317039489746)
[2025-02-13 03:56:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:22][root][INFO] - Training Epoch: 2/2, step 2412/7134 completed (loss: 0.02607002668082714, acc: 0.9923076629638672)
[2025-02-13 03:56:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:23][root][INFO] - Training Epoch: 2/2, step 2413/7134 completed (loss: 0.012342424131929874, acc: 0.9961880445480347)
[2025-02-13 03:56:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:23][root][INFO] - Training Epoch: 2/2, step 2414/7134 completed (loss: 0.016656560823321342, acc: 0.9925373196601868)
[2025-02-13 03:56:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:24][root][INFO] - Training Epoch: 2/2, step 2415/7134 completed (loss: 0.04426233470439911, acc: 0.9876543283462524)
[2025-02-13 03:56:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:24][root][INFO] - Training Epoch: 2/2, step 2416/7134 completed (loss: 0.021067548543214798, acc: 0.9917012453079224)
[2025-02-13 03:56:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:25][root][INFO] - Training Epoch: 2/2, step 2417/7134 completed (loss: 0.049908410757780075, acc: 0.9910813570022583)
[2025-02-13 03:56:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:25][root][INFO] - Training Epoch: 2/2, step 2418/7134 completed (loss: 0.029443947598338127, acc: 0.9873417615890503)
[2025-02-13 03:56:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:26][root][INFO] - Training Epoch: 2/2, step 2419/7134 completed (loss: 0.022659553214907646, acc: 0.9928469061851501)
[2025-02-13 03:56:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:26][root][INFO] - Training Epoch: 2/2, step 2420/7134 completed (loss: 0.020878316834568977, acc: 0.9933664798736572)
[2025-02-13 03:56:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:27][root][INFO] - Training Epoch: 2/2, step 2421/7134 completed (loss: 0.023324374109506607, acc: 0.9925373196601868)
[2025-02-13 03:56:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:27][root][INFO] - Training Epoch: 2/2, step 2422/7134 completed (loss: 0.012142475694417953, acc: 0.9952718615531921)
[2025-02-13 03:56:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:28][root][INFO] - Training Epoch: 2/2, step 2423/7134 completed (loss: 0.017962994053959846, acc: 0.9930264949798584)
[2025-02-13 03:56:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:28][root][INFO] - Training Epoch: 2/2, step 2424/7134 completed (loss: 0.019893618300557137, acc: 0.9943820238113403)
[2025-02-13 03:56:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:28][root][INFO] - Training Epoch: 2/2, step 2425/7134 completed (loss: 0.04345013573765755, acc: 0.9866666793823242)
[2025-02-13 03:56:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:29][root][INFO] - Training Epoch: 2/2, step 2426/7134 completed (loss: 0.014470956288278103, acc: 0.9952437281608582)
[2025-02-13 03:56:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:29][root][INFO] - Training Epoch: 2/2, step 2427/7134 completed (loss: 0.01483007613569498, acc: 0.9941995143890381)
[2025-02-13 03:56:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:30][root][INFO] - Training Epoch: 2/2, step 2428/7134 completed (loss: 0.021227460354566574, acc: 0.993630588054657)
[2025-02-13 03:56:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:30][root][INFO] - Training Epoch: 2/2, step 2429/7134 completed (loss: 0.03454696387052536, acc: 0.9890965819358826)
[2025-02-13 03:56:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:31][root][INFO] - Training Epoch: 2/2, step 2430/7134 completed (loss: 0.027077317237854004, acc: 0.992548406124115)
[2025-02-13 03:56:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:31][root][INFO] - Training Epoch: 2/2, step 2431/7134 completed (loss: 0.0303971990942955, acc: 0.992668628692627)
[2025-02-13 03:56:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:32][root][INFO] - Training Epoch: 2/2, step 2432/7134 completed (loss: 0.058474987745285034, acc: 0.9864176511764526)
[2025-02-13 03:56:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:32][root][INFO] - Training Epoch: 2/2, step 2433/7134 completed (loss: 0.047170016914606094, acc: 0.9914772510528564)
[2025-02-13 03:56:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:32][root][INFO] - Training Epoch: 2/2, step 2434/7134 completed (loss: 0.05926327034831047, acc: 0.9860248565673828)
[2025-02-13 03:56:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:33][root][INFO] - Training Epoch: 2/2, step 2435/7134 completed (loss: 0.020352067425847054, acc: 0.9960474371910095)
[2025-02-13 03:56:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:33][root][INFO] - Training Epoch: 2/2, step 2436/7134 completed (loss: 0.02391715906560421, acc: 0.9930151104927063)
[2025-02-13 03:56:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:34][root][INFO] - Training Epoch: 2/2, step 2437/7134 completed (loss: 0.022315876558423042, acc: 0.9894419312477112)
[2025-02-13 03:56:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:34][root][INFO] - Training Epoch: 2/2, step 2438/7134 completed (loss: 0.03631598502397537, acc: 0.9916267991065979)
[2025-02-13 03:56:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:35][root][INFO] - Training Epoch: 2/2, step 2439/7134 completed (loss: 0.03214197978377342, acc: 0.9903121590614319)
[2025-02-13 03:56:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:35][root][INFO] - Training Epoch: 2/2, step 2440/7134 completed (loss: 0.02798311971127987, acc: 0.9887892603874207)
[2025-02-13 03:56:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:36][root][INFO] - Training Epoch: 2/2, step 2441/7134 completed (loss: 0.10303008556365967, acc: 0.9750480055809021)
[2025-02-13 03:56:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:36][root][INFO] - Training Epoch: 2/2, step 2442/7134 completed (loss: 0.03746314346790314, acc: 0.9860140085220337)
[2025-02-13 03:56:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:37][root][INFO] - Training Epoch: 2/2, step 2443/7134 completed (loss: 0.021003374829888344, acc: 0.9930459260940552)
[2025-02-13 03:56:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:37][root][INFO] - Training Epoch: 2/2, step 2444/7134 completed (loss: 0.03575552627444267, acc: 0.9867424368858337)
[2025-02-13 03:56:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:37][root][INFO] - Training Epoch: 2/2, step 2445/7134 completed (loss: 0.043888259679079056, acc: 0.9860405921936035)
[2025-02-13 03:56:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:38][root][INFO] - Training Epoch: 2/2, step 2446/7134 completed (loss: 0.07869983464479446, acc: 0.9791666865348816)
[2025-02-13 03:56:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:38][root][INFO] - Training Epoch: 2/2, step 2447/7134 completed (loss: 0.032236989587545395, acc: 0.985571563243866)
[2025-02-13 03:56:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:39][root][INFO] - Training Epoch: 2/2, step 2448/7134 completed (loss: 0.02711939997971058, acc: 0.990641713142395)
[2025-02-13 03:56:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:39][root][INFO] - Training Epoch: 2/2, step 2449/7134 completed (loss: 0.04042505472898483, acc: 0.9893898963928223)
[2025-02-13 03:56:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:40][root][INFO] - Training Epoch: 2/2, step 2450/7134 completed (loss: 0.02336733043193817, acc: 0.9936467409133911)
[2025-02-13 03:56:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:40][root][INFO] - Training Epoch: 2/2, step 2451/7134 completed (loss: 0.014303908683359623, acc: 0.9961783289909363)
[2025-02-13 03:56:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:41][root][INFO] - Training Epoch: 2/2, step 2452/7134 completed (loss: 0.02636932022869587, acc: 0.9869961142539978)
[2025-02-13 03:56:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:41][root][INFO] - Training Epoch: 2/2, step 2453/7134 completed (loss: 0.033843349665403366, acc: 0.9896774291992188)
[2025-02-13 03:56:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:41][root][INFO] - Training Epoch: 2/2, step 2454/7134 completed (loss: 0.03880104050040245, acc: 0.9899665713310242)
[2025-02-13 03:56:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:42][root][INFO] - Training Epoch: 2/2, step 2455/7134 completed (loss: 0.018479734659194946, acc: 0.9957507252693176)
[2025-02-13 03:56:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:42][root][INFO] - Training Epoch: 2/2, step 2456/7134 completed (loss: 0.043509308248758316, acc: 0.9896238446235657)
[2025-02-13 03:56:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:43][root][INFO] - Training Epoch: 2/2, step 2457/7134 completed (loss: 0.04040425270795822, acc: 0.9900166392326355)
[2025-02-13 03:56:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:43][root][INFO] - Training Epoch: 2/2, step 2458/7134 completed (loss: 0.06896018981933594, acc: 0.9905660152435303)
[2025-02-13 03:56:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:44][root][INFO] - Training Epoch: 2/2, step 2459/7134 completed (loss: 0.008582296781241894, acc: 1.0)
[2025-02-13 03:56:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:44][root][INFO] - Training Epoch: 2/2, step 2460/7134 completed (loss: 0.02229388989508152, acc: 0.9944674968719482)
[2025-02-13 03:56:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:45][root][INFO] - Training Epoch: 2/2, step 2461/7134 completed (loss: 0.015049289911985397, acc: 0.9956521987915039)
[2025-02-13 03:56:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:45][root][INFO] - Training Epoch: 2/2, step 2462/7134 completed (loss: 0.04237142950296402, acc: 0.9862068891525269)
[2025-02-13 03:56:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:46][root][INFO] - Training Epoch: 2/2, step 2463/7134 completed (loss: 0.010462292470037937, acc: 0.9969742894172668)
[2025-02-13 03:56:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:46][root][INFO] - Training Epoch: 2/2, step 2464/7134 completed (loss: 0.0136249465867877, acc: 0.9943820238113403)
[2025-02-13 03:56:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:46][root][INFO] - Training Epoch: 2/2, step 2465/7134 completed (loss: 0.02562970109283924, acc: 0.9920529723167419)
[2025-02-13 03:56:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:47][root][INFO] - Training Epoch: 2/2, step 2466/7134 completed (loss: 0.013211477547883987, acc: 0.9973509907722473)
[2025-02-13 03:56:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:47][root][INFO] - Training Epoch: 2/2, step 2467/7134 completed (loss: 0.01975419744849205, acc: 0.9948586225509644)
[2025-02-13 03:56:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:48][root][INFO] - Training Epoch: 2/2, step 2468/7134 completed (loss: 0.010730485431849957, acc: 0.9944055676460266)
[2025-02-13 03:56:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:48][root][INFO] - Training Epoch: 2/2, step 2469/7134 completed (loss: 0.009080654010176659, acc: 0.9986357688903809)
[2025-02-13 03:56:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:49][root][INFO] - Training Epoch: 2/2, step 2470/7134 completed (loss: 0.03222693130373955, acc: 0.9932659864425659)
[2025-02-13 03:56:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:49][root][INFO] - Training Epoch: 2/2, step 2471/7134 completed (loss: 0.010868779383599758, acc: 0.9966555237770081)
[2025-02-13 03:56:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:49][root][INFO] - Training Epoch: 2/2, step 2472/7134 completed (loss: 0.007728401105850935, acc: 0.9984848499298096)
[2025-02-13 03:56:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:50][root][INFO] - Training Epoch: 2/2, step 2473/7134 completed (loss: 0.022317301481962204, acc: 0.9918032884597778)
[2025-02-13 03:56:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:50][root][INFO] - Training Epoch: 2/2, step 2474/7134 completed (loss: 0.013489019125699997, acc: 0.9957746267318726)
[2025-02-13 03:56:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:51][root][INFO] - Training Epoch: 2/2, step 2475/7134 completed (loss: 0.014571528881788254, acc: 0.994854211807251)
[2025-02-13 03:56:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:51][root][INFO] - Training Epoch: 2/2, step 2476/7134 completed (loss: 0.022699132561683655, acc: 0.9902507066726685)
[2025-02-13 03:56:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:51][root][INFO] - Training Epoch: 2/2, step 2477/7134 completed (loss: 0.018157636746764183, acc: 0.9944751262664795)
[2025-02-13 03:56:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:52][root][INFO] - Training Epoch: 2/2, step 2478/7134 completed (loss: 0.007501034531742334, acc: 0.9968847632408142)
[2025-02-13 03:56:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:52][root][INFO] - Training Epoch: 2/2, step 2479/7134 completed (loss: 0.04896697402000427, acc: 0.9872813820838928)
[2025-02-13 03:56:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:53][root][INFO] - Training Epoch: 2/2, step 2480/7134 completed (loss: 0.0035117981024086475, acc: 1.0)
[2025-02-13 03:56:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:53][root][INFO] - Training Epoch: 2/2, step 2481/7134 completed (loss: 0.022278551012277603, acc: 0.9850746393203735)
[2025-02-13 03:56:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:54][root][INFO] - Training Epoch: 2/2, step 2482/7134 completed (loss: 0.025904761627316475, acc: 0.9887640476226807)
[2025-02-13 03:56:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:54][root][INFO] - Training Epoch: 2/2, step 2483/7134 completed (loss: 0.013979331590235233, acc: 0.9972260594367981)
[2025-02-13 03:56:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:54][root][INFO] - Training Epoch: 2/2, step 2484/7134 completed (loss: 0.021775146946310997, acc: 0.9930555820465088)
[2025-02-13 03:56:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:55][root][INFO] - Training Epoch: 2/2, step 2485/7134 completed (loss: 0.019923042505979538, acc: 0.9968253970146179)
[2025-02-13 03:56:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:55][root][INFO] - Training Epoch: 2/2, step 2486/7134 completed (loss: 0.02294791489839554, acc: 0.9971264600753784)
[2025-02-13 03:56:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:56][root][INFO] - Training Epoch: 2/2, step 2487/7134 completed (loss: 0.05501343309879303, acc: 0.9858267903327942)
[2025-02-13 03:56:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:56][root][INFO] - Training Epoch: 2/2, step 2488/7134 completed (loss: 0.015045026317238808, acc: 0.9958563446998596)
[2025-02-13 03:56:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:56][root][INFO] - Training Epoch: 2/2, step 2489/7134 completed (loss: 0.008074226789176464, acc: 0.9986072182655334)
[2025-02-13 03:56:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:57][root][INFO] - Training Epoch: 2/2, step 2490/7134 completed (loss: 0.012286989949643612, acc: 0.995720386505127)
[2025-02-13 03:56:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:57][root][INFO] - Training Epoch: 2/2, step 2491/7134 completed (loss: 0.07601470500230789, acc: 0.9828042387962341)
[2025-02-13 03:56:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:58][root][INFO] - Training Epoch: 2/2, step 2492/7134 completed (loss: 0.02459520660340786, acc: 0.9913151264190674)
[2025-02-13 03:56:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:58][root][INFO] - Training Epoch: 2/2, step 2493/7134 completed (loss: 0.03191474825143814, acc: 0.987089216709137)
[2025-02-13 03:56:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:59][root][INFO] - Training Epoch: 2/2, step 2494/7134 completed (loss: 0.03872934356331825, acc: 0.987484335899353)
[2025-02-13 03:56:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:59][root][INFO] - Training Epoch: 2/2, step 2495/7134 completed (loss: 0.035892970860004425, acc: 0.9905020594596863)
[2025-02-13 03:56:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:56:59][root][INFO] - Training Epoch: 2/2, step 2496/7134 completed (loss: 0.023626914247870445, acc: 0.9901685118675232)
[2025-02-13 03:57:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:00][root][INFO] - Training Epoch: 2/2, step 2497/7134 completed (loss: 0.02960006520152092, acc: 0.9914841651916504)
[2025-02-13 03:57:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:00][root][INFO] - Training Epoch: 2/2, step 2498/7134 completed (loss: 0.01666911318898201, acc: 0.9954493641853333)
[2025-02-13 03:57:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:01][root][INFO] - Training Epoch: 2/2, step 2499/7134 completed (loss: 0.017154162749648094, acc: 0.9939613342285156)
[2025-02-13 03:57:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:01][root][INFO] - Training Epoch: 2/2, step 2500/7134 completed (loss: 0.017106981948018074, acc: 0.993565022945404)
[2025-02-13 03:57:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:02][root][INFO] - Training Epoch: 2/2, step 2501/7134 completed (loss: 0.03128499165177345, acc: 0.9885583519935608)
[2025-02-13 03:57:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:02][root][INFO] - Training Epoch: 2/2, step 2502/7134 completed (loss: 0.01978175714612007, acc: 0.9929328560829163)
[2025-02-13 03:57:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:03][root][INFO] - Training Epoch: 2/2, step 2503/7134 completed (loss: 0.027248749509453773, acc: 0.994301974773407)
[2025-02-13 03:57:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:03][root][INFO] - Training Epoch: 2/2, step 2504/7134 completed (loss: 0.021255530416965485, acc: 0.9919999837875366)
[2025-02-13 03:57:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:04][root][INFO] - Training Epoch: 2/2, step 2505/7134 completed (loss: 0.016252918168902397, acc: 0.9931740760803223)
[2025-02-13 03:57:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:04][root][INFO] - Training Epoch: 2/2, step 2506/7134 completed (loss: 0.020676538348197937, acc: 0.9919871687889099)
[2025-02-13 03:57:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:05][root][INFO] - Training Epoch: 2/2, step 2507/7134 completed (loss: 0.006562250666320324, acc: 0.9976771473884583)
[2025-02-13 03:57:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:05][root][INFO] - Training Epoch: 2/2, step 2508/7134 completed (loss: 0.027148209512233734, acc: 0.9919354915618896)
[2025-02-13 03:57:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:05][root][INFO] - Training Epoch: 2/2, step 2509/7134 completed (loss: 0.00942491739988327, acc: 0.9975520372390747)
[2025-02-13 03:57:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:06][root][INFO] - Training Epoch: 2/2, step 2510/7134 completed (loss: 0.018091220408678055, acc: 0.9939302206039429)
[2025-02-13 03:57:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:06][root][INFO] - Training Epoch: 2/2, step 2511/7134 completed (loss: 0.032365575432777405, acc: 0.9910314083099365)
[2025-02-13 03:57:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:07][root][INFO] - Training Epoch: 2/2, step 2512/7134 completed (loss: 0.05620769038796425, acc: 0.9878542423248291)
[2025-02-13 03:57:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:07][root][INFO] - Training Epoch: 2/2, step 2513/7134 completed (loss: 0.01764606684446335, acc: 0.9952606558799744)
[2025-02-13 03:57:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:08][root][INFO] - Training Epoch: 2/2, step 2514/7134 completed (loss: 0.01467587798833847, acc: 0.9941657185554504)
[2025-02-13 03:57:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:08][root][INFO] - Training Epoch: 2/2, step 2515/7134 completed (loss: 0.005830361042171717, acc: 1.0)
[2025-02-13 03:57:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:09][root][INFO] - Training Epoch: 2/2, step 2516/7134 completed (loss: 0.019250841811299324, acc: 0.9943181872367859)
[2025-02-13 03:57:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:09][root][INFO] - Training Epoch: 2/2, step 2517/7134 completed (loss: 0.02352156490087509, acc: 0.9942594766616821)
[2025-02-13 03:57:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:09][root][INFO] - Training Epoch: 2/2, step 2518/7134 completed (loss: 0.01811392419040203, acc: 0.9928469061851501)
[2025-02-13 03:57:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:10][root][INFO] - Training Epoch: 2/2, step 2519/7134 completed (loss: 0.003943283576518297, acc: 0.9986168742179871)
[2025-02-13 03:57:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:10][root][INFO] - Training Epoch: 2/2, step 2520/7134 completed (loss: 0.013157697394490242, acc: 0.995726466178894)
[2025-02-13 03:57:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:11][root][INFO] - Training Epoch: 2/2, step 2521/7134 completed (loss: 0.03769868612289429, acc: 0.9897040128707886)
[2025-02-13 03:57:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:11][root][INFO] - Training Epoch: 2/2, step 2522/7134 completed (loss: 0.029335418716073036, acc: 0.9914039969444275)
[2025-02-13 03:57:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:12][root][INFO] - Training Epoch: 2/2, step 2523/7134 completed (loss: 0.035904571413993835, acc: 0.9947643876075745)
[2025-02-13 03:57:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:12][root][INFO] - Training Epoch: 2/2, step 2524/7134 completed (loss: 0.004516259767115116, acc: 1.0)
[2025-02-13 03:57:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:12][root][INFO] - Training Epoch: 2/2, step 2525/7134 completed (loss: 0.0163966603577137, acc: 0.994991660118103)
[2025-02-13 03:57:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:13][root][INFO] - Training Epoch: 2/2, step 2526/7134 completed (loss: 0.0039538112469017506, acc: 1.0)
[2025-02-13 03:57:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:13][root][INFO] - Training Epoch: 2/2, step 2527/7134 completed (loss: 0.014682288281619549, acc: 0.995121955871582)
[2025-02-13 03:57:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:14][root][INFO] - Training Epoch: 2/2, step 2528/7134 completed (loss: 0.009242911823093891, acc: 0.99863201379776)
[2025-02-13 03:57:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:14][root][INFO] - Training Epoch: 2/2, step 2529/7134 completed (loss: 0.010992870666086674, acc: 0.998670220375061)
[2025-02-13 03:57:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:15][root][INFO] - Training Epoch: 2/2, step 2530/7134 completed (loss: 0.016805259510874748, acc: 0.9948052167892456)
[2025-02-13 03:57:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:15][root][INFO] - Training Epoch: 2/2, step 2531/7134 completed (loss: 0.005788093898445368, acc: 0.9972375631332397)
[2025-02-13 03:57:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:15][root][INFO] - Training Epoch: 2/2, step 2532/7134 completed (loss: 0.005090476479381323, acc: 0.9984591603279114)
[2025-02-13 03:57:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:16][root][INFO] - Training Epoch: 2/2, step 2533/7134 completed (loss: 0.01683769002556801, acc: 0.993966817855835)
[2025-02-13 03:57:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:16][root][INFO] - Training Epoch: 2/2, step 2534/7134 completed (loss: 0.037121955305337906, acc: 0.993630588054657)
[2025-02-13 03:57:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:17][root][INFO] - Training Epoch: 2/2, step 2535/7134 completed (loss: 0.01898634433746338, acc: 0.9922928810119629)
[2025-02-13 03:57:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:17][root][INFO] - Training Epoch: 2/2, step 2536/7134 completed (loss: 0.012903030961751938, acc: 0.9950799345970154)
[2025-02-13 03:57:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:18][root][INFO] - Training Epoch: 2/2, step 2537/7134 completed (loss: 0.03691378980875015, acc: 0.9892966151237488)
[2025-02-13 03:57:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:18][root][INFO] - Training Epoch: 2/2, step 2538/7134 completed (loss: 0.03400089964270592, acc: 0.9886685609817505)
[2025-02-13 03:57:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:18][root][INFO] - Training Epoch: 2/2, step 2539/7134 completed (loss: 0.011476289480924606, acc: 0.9946428537368774)
[2025-02-13 03:57:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:19][root][INFO] - Training Epoch: 2/2, step 2540/7134 completed (loss: 0.04092457890510559, acc: 0.9877675771713257)
[2025-02-13 03:57:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:19][root][INFO] - Training Epoch: 2/2, step 2541/7134 completed (loss: 0.010751795023679733, acc: 0.9950330853462219)
[2025-02-13 03:57:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:20][root][INFO] - Training Epoch: 2/2, step 2542/7134 completed (loss: 0.05382850021123886, acc: 0.9911242723464966)
[2025-02-13 03:57:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:20][root][INFO] - Training Epoch: 2/2, step 2543/7134 completed (loss: 0.06435364484786987, acc: 0.9904371500015259)
[2025-02-13 03:57:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:21][root][INFO] - Training Epoch: 2/2, step 2544/7134 completed (loss: 0.032199375331401825, acc: 0.9909793734550476)
[2025-02-13 03:57:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:21][root][INFO] - Training Epoch: 2/2, step 2545/7134 completed (loss: 0.02448309399187565, acc: 0.9895287752151489)
[2025-02-13 03:57:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:22][root][INFO] - Training Epoch: 2/2, step 2546/7134 completed (loss: 0.09443318098783493, acc: 0.9699879884719849)
[2025-02-13 03:57:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:22][root][INFO] - Training Epoch: 2/2, step 2547/7134 completed (loss: 0.03424094244837761, acc: 0.988727867603302)
[2025-02-13 03:57:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:22][root][INFO] - Training Epoch: 2/2, step 2548/7134 completed (loss: 0.016750788316130638, acc: 0.9960629940032959)
[2025-02-13 03:57:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:23][root][INFO] - Training Epoch: 2/2, step 2549/7134 completed (loss: 0.02919331192970276, acc: 0.990791916847229)
[2025-02-13 03:57:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:23][root][INFO] - Training Epoch: 2/2, step 2550/7134 completed (loss: 0.004272714257240295, acc: 1.0)
[2025-02-13 03:57:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:24][root][INFO] - Training Epoch: 2/2, step 2551/7134 completed (loss: 0.02814420685172081, acc: 0.9917491674423218)
[2025-02-13 03:57:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:24][root][INFO] - Training Epoch: 2/2, step 2552/7134 completed (loss: 0.005217967554926872, acc: 1.0)
[2025-02-13 03:57:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:24][root][INFO] - Training Epoch: 2/2, step 2553/7134 completed (loss: 0.039481401443481445, acc: 0.9909365773200989)
[2025-02-13 03:57:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:25][root][INFO] - Training Epoch: 2/2, step 2554/7134 completed (loss: 0.05032691732048988, acc: 0.9899497628211975)
[2025-02-13 03:57:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:25][root][INFO] - Training Epoch: 2/2, step 2555/7134 completed (loss: 0.016419213265180588, acc: 0.9958677887916565)
[2025-02-13 03:57:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:26][root][INFO] - Training Epoch: 2/2, step 2556/7134 completed (loss: 0.03969888016581535, acc: 0.9895178079605103)
[2025-02-13 03:57:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:26][root][INFO] - Training Epoch: 2/2, step 2557/7134 completed (loss: 0.027766600251197815, acc: 0.9967213273048401)
[2025-02-13 03:57:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:27][root][INFO] - Training Epoch: 2/2, step 2558/7134 completed (loss: 0.0066572148352861404, acc: 1.0)
[2025-02-13 03:57:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:27][root][INFO] - Training Epoch: 2/2, step 2559/7134 completed (loss: 0.0103719187900424, acc: 0.9972936511039734)
[2025-02-13 03:57:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:28][root][INFO] - Training Epoch: 2/2, step 2560/7134 completed (loss: 0.016025271266698837, acc: 0.9918699264526367)
[2025-02-13 03:57:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:28][root][INFO] - Training Epoch: 2/2, step 2561/7134 completed (loss: 0.03701317682862282, acc: 0.9891067743301392)
[2025-02-13 03:57:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:28][root][INFO] - Training Epoch: 2/2, step 2562/7134 completed (loss: 0.016467811539769173, acc: 0.9948717951774597)
[2025-02-13 03:57:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:29][root][INFO] - Training Epoch: 2/2, step 2563/7134 completed (loss: 0.024690845981240273, acc: 0.9956011772155762)
[2025-02-13 03:57:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:29][root][INFO] - Training Epoch: 2/2, step 2564/7134 completed (loss: 0.0750347301363945, acc: 0.9848993420600891)
[2025-02-13 03:57:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:30][root][INFO] - Training Epoch: 2/2, step 2565/7134 completed (loss: 0.01645774580538273, acc: 0.995468258857727)
[2025-02-13 03:57:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:30][root][INFO] - Training Epoch: 2/2, step 2566/7134 completed (loss: 0.01730441115796566, acc: 0.9943820238113403)
[2025-02-13 03:57:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:31][root][INFO] - Training Epoch: 2/2, step 2567/7134 completed (loss: 0.02278827130794525, acc: 0.9929453134536743)
[2025-02-13 03:57:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:31][root][INFO] - Training Epoch: 2/2, step 2568/7134 completed (loss: 0.03797391802072525, acc: 0.986994206905365)
[2025-02-13 03:57:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:31][root][INFO] - Training Epoch: 2/2, step 2569/7134 completed (loss: 0.026489807292819023, acc: 0.9924699068069458)
[2025-02-13 03:57:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:32][root][INFO] - Training Epoch: 2/2, step 2570/7134 completed (loss: 0.03143618628382683, acc: 0.9940029978752136)
[2025-02-13 03:57:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:32][root][INFO] - Training Epoch: 2/2, step 2571/7134 completed (loss: 0.02031516283750534, acc: 0.994727611541748)
[2025-02-13 03:57:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:33][root][INFO] - Training Epoch: 2/2, step 2572/7134 completed (loss: 0.01584312506020069, acc: 0.9921135902404785)
[2025-02-13 03:57:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:33][root][INFO] - Training Epoch: 2/2, step 2573/7134 completed (loss: 0.03382688760757446, acc: 0.9918032884597778)
[2025-02-13 03:57:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:34][root][INFO] - Training Epoch: 2/2, step 2574/7134 completed (loss: 0.01738703064620495, acc: 0.9956331849098206)
[2025-02-13 03:57:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:34][root][INFO] - Training Epoch: 2/2, step 2575/7134 completed (loss: 0.01606392301619053, acc: 0.9944238066673279)
[2025-02-13 03:57:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:34][root][INFO] - Training Epoch: 2/2, step 2576/7134 completed (loss: 0.016501223668456078, acc: 0.9965517520904541)
[2025-02-13 03:57:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:35][root][INFO] - Training Epoch: 2/2, step 2577/7134 completed (loss: 0.054465729743242264, acc: 0.9863247871398926)
[2025-02-13 03:57:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:35][root][INFO] - Training Epoch: 2/2, step 2578/7134 completed (loss: 0.04510865360498428, acc: 0.9800918698310852)
[2025-02-13 03:57:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:36][root][INFO] - Training Epoch: 2/2, step 2579/7134 completed (loss: 0.05179247632622719, acc: 0.9857594966888428)
[2025-02-13 03:57:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:36][root][INFO] - Training Epoch: 2/2, step 2580/7134 completed (loss: 0.034641750156879425, acc: 0.9908615946769714)
[2025-02-13 03:57:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:37][root][INFO] - Training Epoch: 2/2, step 2581/7134 completed (loss: 0.026024622842669487, acc: 0.9905787110328674)
[2025-02-13 03:57:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:37][root][INFO] - Training Epoch: 2/2, step 2582/7134 completed (loss: 0.034841135144233704, acc: 0.9878048896789551)
[2025-02-13 03:57:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:38][root][INFO] - Training Epoch: 2/2, step 2583/7134 completed (loss: 0.023471774533391, acc: 0.991465151309967)
[2025-02-13 03:57:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:38][root][INFO] - Training Epoch: 2/2, step 2584/7134 completed (loss: 0.05192752927541733, acc: 0.9825479984283447)
[2025-02-13 03:57:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:38][root][INFO] - Training Epoch: 2/2, step 2585/7134 completed (loss: 0.04714176431298256, acc: 0.9899280667304993)
[2025-02-13 03:57:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:39][root][INFO] - Training Epoch: 2/2, step 2586/7134 completed (loss: 0.045541733503341675, acc: 0.9876760840415955)
[2025-02-13 03:57:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:39][root][INFO] - Training Epoch: 2/2, step 2587/7134 completed (loss: 0.03035666048526764, acc: 0.9914004802703857)
[2025-02-13 03:57:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:40][root][INFO] - Training Epoch: 2/2, step 2588/7134 completed (loss: 0.007216025609523058, acc: 0.9986357688903809)
[2025-02-13 03:57:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:40][root][INFO] - Training Epoch: 2/2, step 2589/7134 completed (loss: 0.03817461058497429, acc: 0.9911660552024841)
[2025-02-13 03:57:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:41][root][INFO] - Training Epoch: 2/2, step 2590/7134 completed (loss: 0.025563769042491913, acc: 0.9934640526771545)
[2025-02-13 03:57:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:41][root][INFO] - Training Epoch: 2/2, step 2591/7134 completed (loss: 0.04947580024600029, acc: 0.9833759665489197)
[2025-02-13 03:57:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:42][root][INFO] - Training Epoch: 2/2, step 2592/7134 completed (loss: 0.03500914201140404, acc: 0.9938499331474304)
[2025-02-13 03:57:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:42][root][INFO] - Training Epoch: 2/2, step 2593/7134 completed (loss: 0.03118206188082695, acc: 0.9947643876075745)
[2025-02-13 03:57:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:43][root][INFO] - Training Epoch: 2/2, step 2594/7134 completed (loss: 0.030476626008749008, acc: 0.9877675771713257)
[2025-02-13 03:57:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:43][root][INFO] - Training Epoch: 2/2, step 2595/7134 completed (loss: 0.012504125013947487, acc: 0.9973118305206299)
[2025-02-13 03:57:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:44][root][INFO] - Training Epoch: 2/2, step 2596/7134 completed (loss: 0.037956707179546356, acc: 0.9899749159812927)
[2025-02-13 03:57:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:44][root][INFO] - Training Epoch: 2/2, step 2597/7134 completed (loss: 0.03257392719388008, acc: 0.9905405640602112)
[2025-02-13 03:57:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:44][root][INFO] - Training Epoch: 2/2, step 2598/7134 completed (loss: 0.046085309237241745, acc: 0.9891641139984131)
[2025-02-13 03:57:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:45][root][INFO] - Training Epoch: 2/2, step 2599/7134 completed (loss: 0.0029261745512485504, acc: 1.0)
[2025-02-13 03:57:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:45][root][INFO] - Training Epoch: 2/2, step 2600/7134 completed (loss: 0.0483701266348362, acc: 0.98591548204422)
[2025-02-13 03:57:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:46][root][INFO] - Training Epoch: 2/2, step 2601/7134 completed (loss: 0.03434516862034798, acc: 0.9864681959152222)
[2025-02-13 03:57:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:46][root][INFO] - Training Epoch: 2/2, step 2602/7134 completed (loss: 0.03856949135661125, acc: 0.9878934621810913)
[2025-02-13 03:57:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:47][root][INFO] - Training Epoch: 2/2, step 2603/7134 completed (loss: 0.03213934972882271, acc: 0.9906976819038391)
[2025-02-13 03:57:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:47][root][INFO] - Training Epoch: 2/2, step 2604/7134 completed (loss: 0.023942856118083, acc: 0.9923245906829834)
[2025-02-13 03:57:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:48][root][INFO] - Training Epoch: 2/2, step 2605/7134 completed (loss: 0.033500537276268005, acc: 0.9891696572303772)
[2025-02-13 03:57:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:48][root][INFO] - Training Epoch: 2/2, step 2606/7134 completed (loss: 0.017499348148703575, acc: 0.9928057789802551)
[2025-02-13 03:57:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:49][root][INFO] - Training Epoch: 2/2, step 2607/7134 completed (loss: 0.02374754473567009, acc: 0.9905882477760315)
[2025-02-13 03:57:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:49][root][INFO] - Training Epoch: 2/2, step 2608/7134 completed (loss: 0.03077523782849312, acc: 0.9876390695571899)
[2025-02-13 03:57:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:50][root][INFO] - Training Epoch: 2/2, step 2609/7134 completed (loss: 0.034302257001399994, acc: 0.9902507066726685)
[2025-02-13 03:57:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:50][root][INFO] - Training Epoch: 2/2, step 2610/7134 completed (loss: 0.05315496027469635, acc: 0.989313006401062)
[2025-02-13 03:57:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:51][root][INFO] - Training Epoch: 2/2, step 2611/7134 completed (loss: 0.06748066842556, acc: 0.9804822206497192)
[2025-02-13 03:57:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:51][root][INFO] - Training Epoch: 2/2, step 2612/7134 completed (loss: 0.03886152058839798, acc: 0.9886105060577393)
[2025-02-13 03:57:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:51][root][INFO] - Training Epoch: 2/2, step 2613/7134 completed (loss: 0.019936945289373398, acc: 0.9924050569534302)
[2025-02-13 03:57:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:52][root][INFO] - Training Epoch: 2/2, step 2614/7134 completed (loss: 0.0753181204199791, acc: 0.9783599376678467)
[2025-02-13 03:57:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:52][root][INFO] - Training Epoch: 2/2, step 2615/7134 completed (loss: 0.03316371887922287, acc: 0.9897172451019287)
[2025-02-13 03:57:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:53][root][INFO] - Training Epoch: 2/2, step 2616/7134 completed (loss: 0.055095188319683075, acc: 0.9814814925193787)
[2025-02-13 03:57:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:53][root][INFO] - Training Epoch: 2/2, step 2617/7134 completed (loss: 0.046992890536785126, acc: 0.9838056564331055)
[2025-02-13 03:57:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:54][root][INFO] - Training Epoch: 2/2, step 2618/7134 completed (loss: 0.0187725480645895, acc: 0.9938398599624634)
[2025-02-13 03:57:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:54][root][INFO] - Training Epoch: 2/2, step 2619/7134 completed (loss: 0.03315923362970352, acc: 0.9865771532058716)
[2025-02-13 03:57:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:55][root][INFO] - Training Epoch: 2/2, step 2620/7134 completed (loss: 0.04059880971908569, acc: 0.9876084327697754)
[2025-02-13 03:57:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:55][root][INFO] - Training Epoch: 2/2, step 2621/7134 completed (loss: 0.02725565806031227, acc: 0.9940758347511292)
[2025-02-13 03:57:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:56][root][INFO] - Training Epoch: 2/2, step 2622/7134 completed (loss: 0.03660658374428749, acc: 0.9918166995048523)
[2025-02-13 03:57:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:56][root][INFO] - Training Epoch: 2/2, step 2623/7134 completed (loss: 0.03595040738582611, acc: 0.9885057210922241)
[2025-02-13 03:57:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:57][root][INFO] - Training Epoch: 2/2, step 2624/7134 completed (loss: 0.03691200539469719, acc: 0.9898762702941895)
[2025-02-13 03:57:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:57][root][INFO] - Training Epoch: 2/2, step 2625/7134 completed (loss: 0.03730947896838188, acc: 0.9888888597488403)
[2025-02-13 03:57:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:58][root][INFO] - Training Epoch: 2/2, step 2626/7134 completed (loss: 0.018359769135713577, acc: 0.9925742745399475)
[2025-02-13 03:57:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:58][root][INFO] - Training Epoch: 2/2, step 2627/7134 completed (loss: 0.02105564996600151, acc: 0.9918808937072754)
[2025-02-13 03:57:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:59][root][INFO] - Training Epoch: 2/2, step 2628/7134 completed (loss: 0.04363671690225601, acc: 0.9861111044883728)
[2025-02-13 03:57:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:57:59][root][INFO] - Training Epoch: 2/2, step 2629/7134 completed (loss: 0.0355207696557045, acc: 0.9898862242698669)
[2025-02-13 03:57:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:00][root][INFO] - Training Epoch: 2/2, step 2630/7134 completed (loss: 0.025969810783863068, acc: 0.9904153347015381)
[2025-02-13 03:58:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:00][root][INFO] - Training Epoch: 2/2, step 2631/7134 completed (loss: 0.02118063159286976, acc: 0.9929824471473694)
[2025-02-13 03:58:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:00][root][INFO] - Training Epoch: 2/2, step 2632/7134 completed (loss: 0.005942144896835089, acc: 1.0)
[2025-02-13 03:58:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:01][root][INFO] - Training Epoch: 2/2, step 2633/7134 completed (loss: 0.02999453991651535, acc: 0.9900850057601929)
[2025-02-13 03:58:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:01][root][INFO] - Training Epoch: 2/2, step 2634/7134 completed (loss: 0.013813466764986515, acc: 0.9940029978752136)
[2025-02-13 03:58:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:02][root][INFO] - Training Epoch: 2/2, step 2635/7134 completed (loss: 0.06141863018274307, acc: 0.980555534362793)
[2025-02-13 03:58:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:02][root][INFO] - Training Epoch: 2/2, step 2636/7134 completed (loss: 0.04746127128601074, acc: 0.9871976971626282)
[2025-02-13 03:58:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:03][root][INFO] - Training Epoch: 2/2, step 2637/7134 completed (loss: 0.13880151510238647, acc: 0.971137523651123)
[2025-02-13 03:58:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:03][root][INFO] - Training Epoch: 2/2, step 2638/7134 completed (loss: 0.0723036453127861, acc: 0.9804432988166809)
[2025-02-13 03:58:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:04][root][INFO] - Training Epoch: 2/2, step 2639/7134 completed (loss: 0.04576214775443077, acc: 0.9834024906158447)
[2025-02-13 03:58:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:04][root][INFO] - Training Epoch: 2/2, step 2640/7134 completed (loss: 0.06761866062879562, acc: 0.9812734127044678)
[2025-02-13 03:58:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:05][root][INFO] - Training Epoch: 2/2, step 2641/7134 completed (loss: 0.0370677150785923, acc: 0.9882179498672485)
[2025-02-13 03:58:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:05][root][INFO] - Training Epoch: 2/2, step 2642/7134 completed (loss: 0.07081490755081177, acc: 0.9768683314323425)
[2025-02-13 03:58:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:05][root][INFO] - Training Epoch: 2/2, step 2643/7134 completed (loss: 0.03992753103375435, acc: 0.9867109656333923)
[2025-02-13 03:58:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:06][root][INFO] - Training Epoch: 2/2, step 2644/7134 completed (loss: 0.05515246093273163, acc: 0.9854369163513184)
[2025-02-13 03:58:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:06][root][INFO] - Training Epoch: 2/2, step 2645/7134 completed (loss: 0.03424068167805672, acc: 0.9927113652229309)
[2025-02-13 03:58:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:07][root][INFO] - Training Epoch: 2/2, step 2646/7134 completed (loss: 0.12487627565860748, acc: 0.9649681448936462)
[2025-02-13 03:58:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:07][root][INFO] - Training Epoch: 2/2, step 2647/7134 completed (loss: 0.030136264860630035, acc: 0.9895969033241272)
[2025-02-13 03:58:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:08][root][INFO] - Training Epoch: 2/2, step 2648/7134 completed (loss: 0.02367241680622101, acc: 0.9935483932495117)
[2025-02-13 03:58:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:08][root][INFO] - Training Epoch: 2/2, step 2649/7134 completed (loss: 0.0924096331000328, acc: 0.9672130942344666)
[2025-02-13 03:58:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:09][root][INFO] - Training Epoch: 2/2, step 2650/7134 completed (loss: 0.07186693698167801, acc: 0.9818456768989563)
[2025-02-13 03:58:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:09][root][INFO] - Training Epoch: 2/2, step 2651/7134 completed (loss: 0.03512677922844887, acc: 0.9916567206382751)
[2025-02-13 03:58:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:09][root][INFO] - Training Epoch: 2/2, step 2652/7134 completed (loss: 0.03343096375465393, acc: 0.9886506795883179)
[2025-02-13 03:58:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:10][root][INFO] - Training Epoch: 2/2, step 2653/7134 completed (loss: 0.03649483248591423, acc: 0.9851973652839661)
[2025-02-13 03:58:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:10][root][INFO] - Training Epoch: 2/2, step 2654/7134 completed (loss: 0.026764722540974617, acc: 0.9901639223098755)
[2025-02-13 03:58:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:11][root][INFO] - Training Epoch: 2/2, step 2655/7134 completed (loss: 0.027123847976326942, acc: 0.9894099831581116)
[2025-02-13 03:58:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:11][root][INFO] - Training Epoch: 2/2, step 2656/7134 completed (loss: 0.04497687146067619, acc: 0.9902642369270325)
[2025-02-13 03:58:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:12][root][INFO] - Training Epoch: 2/2, step 2657/7134 completed (loss: 0.025612005963921547, acc: 0.9887096881866455)
[2025-02-13 03:58:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:12][root][INFO] - Training Epoch: 2/2, step 2658/7134 completed (loss: 0.08948775380849838, acc: 0.9746328592300415)
[2025-02-13 03:58:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:13][root][INFO] - Training Epoch: 2/2, step 2659/7134 completed (loss: 0.038454342633485794, acc: 0.9942528605461121)
[2025-02-13 03:58:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:13][root][INFO] - Training Epoch: 2/2, step 2660/7134 completed (loss: 0.022825224325060844, acc: 0.9919354915618896)
[2025-02-13 03:58:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:14][root][INFO] - Training Epoch: 2/2, step 2661/7134 completed (loss: 0.022812124341726303, acc: 0.9929873943328857)
[2025-02-13 03:58:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:14][root][INFO] - Training Epoch: 2/2, step 2662/7134 completed (loss: 0.017954440787434578, acc: 0.9928876161575317)
[2025-02-13 03:58:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:15][root][INFO] - Training Epoch: 2/2, step 2663/7134 completed (loss: 0.019576137885451317, acc: 0.9953216314315796)
[2025-02-13 03:58:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:15][root][INFO] - Training Epoch: 2/2, step 2664/7134 completed (loss: 0.006361894775182009, acc: 0.9974193572998047)
[2025-02-13 03:58:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:15][root][INFO] - Training Epoch: 2/2, step 2665/7134 completed (loss: 0.010035185143351555, acc: 0.9947916865348816)
[2025-02-13 03:58:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:16][root][INFO] - Training Epoch: 2/2, step 2666/7134 completed (loss: 0.008894739672541618, acc: 0.9987760186195374)
[2025-02-13 03:58:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:16][root][INFO] - Training Epoch: 2/2, step 2667/7134 completed (loss: 0.007831809110939503, acc: 0.9971671104431152)
[2025-02-13 03:58:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:17][root][INFO] - Training Epoch: 2/2, step 2668/7134 completed (loss: 0.025639047846198082, acc: 0.9932705163955688)
[2025-02-13 03:58:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:17][root][INFO] - Training Epoch: 2/2, step 2669/7134 completed (loss: 0.005477771162986755, acc: 0.9976984858512878)
[2025-02-13 03:58:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:18][root][INFO] - Training Epoch: 2/2, step 2670/7134 completed (loss: 0.021944819018244743, acc: 0.993446946144104)
[2025-02-13 03:58:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:18][root][INFO] - Training Epoch: 2/2, step 2671/7134 completed (loss: 0.01422798726707697, acc: 0.9973261952400208)
[2025-02-13 03:58:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:19][root][INFO] - Training Epoch: 2/2, step 2672/7134 completed (loss: 0.012524261139333248, acc: 0.9985422492027283)
[2025-02-13 03:58:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:19][root][INFO] - Training Epoch: 2/2, step 2673/7134 completed (loss: 0.00800660066306591, acc: 0.9962121248245239)
[2025-02-13 03:58:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:19][root][INFO] - Training Epoch: 2/2, step 2674/7134 completed (loss: 0.007061939220875502, acc: 0.9972106218338013)
[2025-02-13 03:58:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:20][root][INFO] - Training Epoch: 2/2, step 2675/7134 completed (loss: 0.0034844260662794113, acc: 1.0)
[2025-02-13 03:58:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:20][root][INFO] - Training Epoch: 2/2, step 2676/7134 completed (loss: 0.015476401895284653, acc: 0.9922077655792236)
[2025-02-13 03:58:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:21][root][INFO] - Training Epoch: 2/2, step 2677/7134 completed (loss: 0.008358125574886799, acc: 0.9970238208770752)
[2025-02-13 03:58:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:21][root][INFO] - Training Epoch: 2/2, step 2678/7134 completed (loss: 0.0119782704859972, acc: 0.9973614811897278)
[2025-02-13 03:58:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:22][root][INFO] - Training Epoch: 2/2, step 2679/7134 completed (loss: 0.014079508371651173, acc: 0.9977553486824036)
[2025-02-13 03:58:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:22][root][INFO] - Training Epoch: 2/2, step 2680/7134 completed (loss: 0.010049988515675068, acc: 0.995067834854126)
[2025-02-13 03:58:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:23][root][INFO] - Training Epoch: 2/2, step 2681/7134 completed (loss: 0.00679097231477499, acc: 0.9984825253486633)
[2025-02-13 03:58:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:23][root][INFO] - Training Epoch: 2/2, step 2682/7134 completed (loss: 0.006898804567754269, acc: 0.9984050989151001)
[2025-02-13 03:58:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:23][root][INFO] - Training Epoch: 2/2, step 2683/7134 completed (loss: 0.002802477451041341, acc: 1.0)
[2025-02-13 03:58:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:24][root][INFO] - Training Epoch: 2/2, step 2684/7134 completed (loss: 0.024805501103401184, acc: 0.9946236610412598)
[2025-02-13 03:58:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:24][root][INFO] - Training Epoch: 2/2, step 2685/7134 completed (loss: 0.010226028971374035, acc: 0.9973992109298706)
[2025-02-13 03:58:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:25][root][INFO] - Training Epoch: 2/2, step 2686/7134 completed (loss: 0.033161792904138565, acc: 0.9907407164573669)
[2025-02-13 03:58:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:25][root][INFO] - Training Epoch: 2/2, step 2687/7134 completed (loss: 0.024763843044638634, acc: 0.9893993139266968)
[2025-02-13 03:58:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:26][root][INFO] - Training Epoch: 2/2, step 2688/7134 completed (loss: 0.027104632928967476, acc: 0.9937106966972351)
[2025-02-13 03:58:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:26][root][INFO] - Training Epoch: 2/2, step 2689/7134 completed (loss: 0.022432735189795494, acc: 0.9963833689689636)
[2025-02-13 03:58:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:27][root][INFO] - Training Epoch: 2/2, step 2690/7134 completed (loss: 0.04058433696627617, acc: 0.9860896468162537)
[2025-02-13 03:58:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:27][root][INFO] - Training Epoch: 2/2, step 2691/7134 completed (loss: 0.06865029036998749, acc: 0.98828125)
[2025-02-13 03:58:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:27][root][INFO] - Training Epoch: 2/2, step 2692/7134 completed (loss: 0.0832892581820488, acc: 0.9776536226272583)
[2025-02-13 03:58:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:28][root][INFO] - Training Epoch: 2/2, step 2693/7134 completed (loss: 0.05969204008579254, acc: 0.9846153855323792)
[2025-02-13 03:58:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:28][root][INFO] - Training Epoch: 2/2, step 2694/7134 completed (loss: 0.09748206287622452, acc: 0.9838235378265381)
[2025-02-13 03:58:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:29][root][INFO] - Training Epoch: 2/2, step 2695/7134 completed (loss: 0.02718636952340603, acc: 0.9932998418807983)
[2025-02-13 03:58:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:29][root][INFO] - Training Epoch: 2/2, step 2696/7134 completed (loss: 0.05682075768709183, acc: 0.9825119376182556)
[2025-02-13 03:58:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:30][root][INFO] - Training Epoch: 2/2, step 2697/7134 completed (loss: 0.05796163156628609, acc: 0.9830508232116699)
[2025-02-13 03:58:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:30][root][INFO] - Training Epoch: 2/2, step 2698/7134 completed (loss: 0.027971358969807625, acc: 0.9914529919624329)
[2025-02-13 03:58:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:31][root][INFO] - Training Epoch: 2/2, step 2699/7134 completed (loss: 0.02312110736966133, acc: 0.9911699891090393)
[2025-02-13 03:58:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:31][root][INFO] - Training Epoch: 2/2, step 2700/7134 completed (loss: 0.035768501460552216, acc: 0.9910979270935059)
[2025-02-13 03:58:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:31][root][INFO] - Training Epoch: 2/2, step 2701/7134 completed (loss: 0.04341983422636986, acc: 0.9848739504814148)
[2025-02-13 03:58:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:32][root][INFO] - Training Epoch: 2/2, step 2702/7134 completed (loss: 0.050367023795843124, acc: 0.984375)
[2025-02-13 03:58:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:32][root][INFO] - Training Epoch: 2/2, step 2703/7134 completed (loss: 0.009586353786289692, acc: 0.9968152642250061)
[2025-02-13 03:58:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:33][root][INFO] - Training Epoch: 2/2, step 2704/7134 completed (loss: 0.04502056539058685, acc: 0.9919137358665466)
[2025-02-13 03:58:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:33][root][INFO] - Training Epoch: 2/2, step 2705/7134 completed (loss: 0.04763510450720787, acc: 0.9861111044883728)
[2025-02-13 03:58:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:34][root][INFO] - Training Epoch: 2/2, step 2706/7134 completed (loss: 0.03567226603627205, acc: 0.9869375824928284)
[2025-02-13 03:58:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:34][root][INFO] - Training Epoch: 2/2, step 2707/7134 completed (loss: 0.027106786146759987, acc: 0.9913793206214905)
[2025-02-13 03:58:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:35][root][INFO] - Training Epoch: 2/2, step 2708/7134 completed (loss: 0.05481799691915512, acc: 0.9817671775817871)
[2025-02-13 03:58:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:35][root][INFO] - Training Epoch: 2/2, step 2709/7134 completed (loss: 0.027275871485471725, acc: 0.9951298832893372)
[2025-02-13 03:58:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:36][root][INFO] - Training Epoch: 2/2, step 2710/7134 completed (loss: 0.014469576068222523, acc: 0.997668981552124)
[2025-02-13 03:58:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:36][root][INFO] - Training Epoch: 2/2, step 2711/7134 completed (loss: 0.018232466652989388, acc: 0.9929245114326477)
[2025-02-13 03:58:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:37][root][INFO] - Training Epoch: 2/2, step 2712/7134 completed (loss: 0.021556807681918144, acc: 0.9933035969734192)
[2025-02-13 03:58:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:37][root][INFO] - Training Epoch: 2/2, step 2713/7134 completed (loss: 0.04161609709262848, acc: 0.9944289922714233)
[2025-02-13 03:58:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:37][root][INFO] - Training Epoch: 2/2, step 2714/7134 completed (loss: 0.030016986653208733, acc: 0.9916805028915405)
[2025-02-13 03:58:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:38][root][INFO] - Training Epoch: 2/2, step 2715/7134 completed (loss: 0.02215636894106865, acc: 0.9939849376678467)
[2025-02-13 03:58:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:38][root][INFO] - Training Epoch: 2/2, step 2716/7134 completed (loss: 0.031891174614429474, acc: 0.9895150661468506)
[2025-02-13 03:58:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:39][root][INFO] - Training Epoch: 2/2, step 2717/7134 completed (loss: 0.04359840229153633, acc: 0.9897330403327942)
[2025-02-13 03:58:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:39][root][INFO] - Training Epoch: 2/2, step 2718/7134 completed (loss: 0.088186115026474, acc: 0.9735915660858154)
[2025-02-13 03:58:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:40][root][INFO] - Training Epoch: 2/2, step 2719/7134 completed (loss: 0.03672799468040466, acc: 0.9932318329811096)
[2025-02-13 03:58:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:40][root][INFO] - Training Epoch: 2/2, step 2720/7134 completed (loss: 0.02757793851196766, acc: 0.9905362725257874)
[2025-02-13 03:58:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:41][root][INFO] - Training Epoch: 2/2, step 2721/7134 completed (loss: 0.044978078454732895, acc: 0.9942029118537903)
[2025-02-13 03:58:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:41][root][INFO] - Training Epoch: 2/2, step 2722/7134 completed (loss: 0.08602369576692581, acc: 0.9826086759567261)
[2025-02-13 03:58:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:41][root][INFO] - Training Epoch: 2/2, step 2723/7134 completed (loss: 0.07234187424182892, acc: 0.9827798008918762)
[2025-02-13 03:58:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:42][root][INFO] - Training Epoch: 2/2, step 2724/7134 completed (loss: 0.031003175303339958, acc: 0.9939246773719788)
[2025-02-13 03:58:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:42][root][INFO] - Training Epoch: 2/2, step 2725/7134 completed (loss: 0.010814359411597252, acc: 0.9967159032821655)
[2025-02-13 03:58:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:43][root][INFO] - Training Epoch: 2/2, step 2726/7134 completed (loss: 0.05481036379933357, acc: 0.990774929523468)
[2025-02-13 03:58:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:43][root][INFO] - Training Epoch: 2/2, step 2727/7134 completed (loss: 0.024025164544582367, acc: 0.9925925731658936)
[2025-02-13 03:58:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:44][root][INFO] - Training Epoch: 2/2, step 2728/7134 completed (loss: 0.08444825559854507, acc: 0.9770641922950745)
[2025-02-13 03:58:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:44][root][INFO] - Training Epoch: 2/2, step 2729/7134 completed (loss: 0.051546741276979446, acc: 0.9825783967971802)
[2025-02-13 03:58:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:45][root][INFO] - Training Epoch: 2/2, step 2730/7134 completed (loss: 0.0560188926756382, acc: 0.981249988079071)
[2025-02-13 03:58:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:45][root][INFO] - Training Epoch: 2/2, step 2731/7134 completed (loss: 0.07064960151910782, acc: 0.9805352687835693)
[2025-02-13 03:58:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:45][root][INFO] - Training Epoch: 2/2, step 2732/7134 completed (loss: 0.037213973701000214, acc: 0.989847719669342)
[2025-02-13 03:58:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:46][root][INFO] - Training Epoch: 2/2, step 2733/7134 completed (loss: 0.04262135550379753, acc: 0.9864314794540405)
[2025-02-13 03:58:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:46][root][INFO] - Training Epoch: 2/2, step 2734/7134 completed (loss: 0.03995520994067192, acc: 0.9919517040252686)
[2025-02-13 03:58:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:47][root][INFO] - Training Epoch: 2/2, step 2735/7134 completed (loss: 0.02961842156946659, acc: 0.9932249188423157)
[2025-02-13 03:58:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:47][root][INFO] - Training Epoch: 2/2, step 2736/7134 completed (loss: 0.04814097285270691, acc: 0.9850746393203735)
[2025-02-13 03:58:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:48][root][INFO] - Training Epoch: 2/2, step 2737/7134 completed (loss: 0.02772505208849907, acc: 0.9917491674423218)
[2025-02-13 03:58:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:48][root][INFO] - Training Epoch: 2/2, step 2738/7134 completed (loss: 0.021873582154512405, acc: 0.9900000095367432)
[2025-02-13 03:58:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:49][root][INFO] - Training Epoch: 2/2, step 2739/7134 completed (loss: 0.0421542152762413, acc: 0.9872727394104004)
[2025-02-13 03:58:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:49][root][INFO] - Training Epoch: 2/2, step 2740/7134 completed (loss: 0.011026973836123943, acc: 0.9956458806991577)
[2025-02-13 03:58:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:49][root][INFO] - Training Epoch: 2/2, step 2741/7134 completed (loss: 0.025673819705843925, acc: 0.9906542301177979)
[2025-02-13 03:58:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:50][root][INFO] - Training Epoch: 2/2, step 2742/7134 completed (loss: 0.012266358360648155, acc: 0.9958333373069763)
[2025-02-13 03:58:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:50][root][INFO] - Training Epoch: 2/2, step 2743/7134 completed (loss: 0.0045855226926505566, acc: 0.9973683953285217)
[2025-02-13 03:58:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:51][root][INFO] - Training Epoch: 2/2, step 2744/7134 completed (loss: 0.008246262557804585, acc: 1.0)
[2025-02-13 03:58:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:51][root][INFO] - Training Epoch: 2/2, step 2745/7134 completed (loss: 0.06148906424641609, acc: 0.9843205809593201)
[2025-02-13 03:58:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:51][root][INFO] - Training Epoch: 2/2, step 2746/7134 completed (loss: 0.016312668099999428, acc: 0.9935897588729858)
[2025-02-13 03:58:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:52][root][INFO] - Training Epoch: 2/2, step 2747/7134 completed (loss: 0.015544622205197811, acc: 0.9950860142707825)
[2025-02-13 03:58:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:52][root][INFO] - Training Epoch: 2/2, step 2748/7134 completed (loss: 0.008201791904866695, acc: 1.0)
[2025-02-13 03:58:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:53][root][INFO] - Training Epoch: 2/2, step 2749/7134 completed (loss: 0.030496269464492798, acc: 0.9918032884597778)
[2025-02-13 03:58:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:53][root][INFO] - Training Epoch: 2/2, step 2750/7134 completed (loss: 0.02106590010225773, acc: 0.9924812316894531)
[2025-02-13 03:58:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:53][root][INFO] - Training Epoch: 2/2, step 2751/7134 completed (loss: 0.03226051852107048, acc: 0.9949367046356201)
[2025-02-13 03:58:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:54][root][INFO] - Training Epoch: 2/2, step 2752/7134 completed (loss: 0.029544096440076828, acc: 0.9885931611061096)
[2025-02-13 03:58:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:54][root][INFO] - Training Epoch: 2/2, step 2753/7134 completed (loss: 0.05654904991388321, acc: 0.9824561476707458)
[2025-02-13 03:58:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:55][root][INFO] - Training Epoch: 2/2, step 2754/7134 completed (loss: 0.038124363869428635, acc: 0.9878296256065369)
[2025-02-13 03:58:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:55][root][INFO] - Training Epoch: 2/2, step 2755/7134 completed (loss: 0.038079921156167984, acc: 0.9941860437393188)
[2025-02-13 03:58:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:56][root][INFO] - Training Epoch: 2/2, step 2756/7134 completed (loss: 0.06104793772101402, acc: 0.9809523820877075)
[2025-02-13 03:58:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:56][root][INFO] - Training Epoch: 2/2, step 2757/7134 completed (loss: 0.04500826448202133, acc: 0.9863387942314148)
[2025-02-13 03:58:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:56][root][INFO] - Training Epoch: 2/2, step 2758/7134 completed (loss: 0.031817566603422165, acc: 0.9885844588279724)
[2025-02-13 03:58:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:57][root][INFO] - Training Epoch: 2/2, step 2759/7134 completed (loss: 0.044773466885089874, acc: 0.9866220951080322)
[2025-02-13 03:58:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:57][root][INFO] - Training Epoch: 2/2, step 2760/7134 completed (loss: 0.03899521753191948, acc: 0.9841269850730896)
[2025-02-13 03:58:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:58][root][INFO] - Training Epoch: 2/2, step 2761/7134 completed (loss: 0.06569214910268784, acc: 0.9848484992980957)
[2025-02-13 03:58:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:58][root][INFO] - Training Epoch: 2/2, step 2762/7134 completed (loss: 0.04611912742257118, acc: 0.9884925484657288)
[2025-02-13 03:58:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:58][root][INFO] - Training Epoch: 2/2, step 2763/7134 completed (loss: 0.03433629497885704, acc: 0.9924623370170593)
[2025-02-13 03:58:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:59][root][INFO] - Training Epoch: 2/2, step 2764/7134 completed (loss: 0.07731930166482925, acc: 0.9749670624732971)
[2025-02-13 03:58:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:58:59][root][INFO] - Training Epoch: 2/2, step 2765/7134 completed (loss: 0.018847007304430008, acc: 0.9971264600753784)
[2025-02-13 03:59:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:00][root][INFO] - Training Epoch: 2/2, step 2766/7134 completed (loss: 0.025277044624090195, acc: 0.9919224381446838)
[2025-02-13 03:59:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:00][root][INFO] - Training Epoch: 2/2, step 2767/7134 completed (loss: 0.0767408162355423, acc: 0.9814814925193787)
[2025-02-13 03:59:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:01][root][INFO] - Training Epoch: 2/2, step 2768/7134 completed (loss: 0.04165913164615631, acc: 0.985855758190155)
[2025-02-13 03:59:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:01][root][INFO] - Training Epoch: 2/2, step 2769/7134 completed (loss: 0.031145574524998665, acc: 0.9887387156486511)
[2025-02-13 03:59:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:02][root][INFO] - Training Epoch: 2/2, step 2770/7134 completed (loss: 0.014619427733123302, acc: 0.9972183704376221)
[2025-02-13 03:59:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:02][root][INFO] - Training Epoch: 2/2, step 2771/7134 completed (loss: 0.03731315955519676, acc: 0.9892818927764893)
[2025-02-13 03:59:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:03][root][INFO] - Training Epoch: 2/2, step 2772/7134 completed (loss: 0.029346732422709465, acc: 0.991051435470581)
[2025-02-13 03:59:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:03][root][INFO] - Training Epoch: 2/2, step 2773/7134 completed (loss: 0.01927129551768303, acc: 0.9919999837875366)
[2025-02-13 03:59:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:04][root][INFO] - Training Epoch: 2/2, step 2774/7134 completed (loss: 0.03911159187555313, acc: 0.9901315569877625)
[2025-02-13 03:59:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:04][root][INFO] - Training Epoch: 2/2, step 2775/7134 completed (loss: 0.03138525038957596, acc: 0.991411030292511)
[2025-02-13 03:59:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:05][root][INFO] - Training Epoch: 2/2, step 2776/7134 completed (loss: 0.013990046456456184, acc: 0.9959100484848022)
[2025-02-13 03:59:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:05][root][INFO] - Training Epoch: 2/2, step 2777/7134 completed (loss: 0.025581827387213707, acc: 0.992682933807373)
[2025-02-13 03:59:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:06][root][INFO] - Training Epoch: 2/2, step 2778/7134 completed (loss: 0.032748669385910034, acc: 0.9907299876213074)
[2025-02-13 03:59:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:06][root][INFO] - Training Epoch: 2/2, step 2779/7134 completed (loss: 0.049432311207056046, acc: 0.9883966445922852)
[2025-02-13 03:59:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:06][root][INFO] - Training Epoch: 2/2, step 2780/7134 completed (loss: 0.03331549093127251, acc: 0.9894514679908752)
[2025-02-13 03:59:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:07][root][INFO] - Training Epoch: 2/2, step 2781/7134 completed (loss: 0.019705872982740402, acc: 0.9961140155792236)
[2025-02-13 03:59:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:07][root][INFO] - Training Epoch: 2/2, step 2782/7134 completed (loss: 0.02229749970138073, acc: 0.9957982897758484)
[2025-02-13 03:59:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:08][root][INFO] - Training Epoch: 2/2, step 2783/7134 completed (loss: 0.015660004690289497, acc: 0.9969969987869263)
[2025-02-13 03:59:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:08][root][INFO] - Training Epoch: 2/2, step 2784/7134 completed (loss: 0.01891409605741501, acc: 0.995793879032135)
[2025-02-13 03:59:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:09][root][INFO] - Training Epoch: 2/2, step 2785/7134 completed (loss: 0.019184892997145653, acc: 0.9967249035835266)
[2025-02-13 03:59:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:09][root][INFO] - Training Epoch: 2/2, step 2786/7134 completed (loss: 0.030027925968170166, acc: 0.9906542301177979)
[2025-02-13 03:59:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:10][root][INFO] - Training Epoch: 2/2, step 2787/7134 completed (loss: 0.015867777168750763, acc: 0.9973439574241638)
[2025-02-13 03:59:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:10][root][INFO] - Training Epoch: 2/2, step 2788/7134 completed (loss: 0.025636782869696617, acc: 0.9921466112136841)
[2025-02-13 03:59:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:11][root][INFO] - Training Epoch: 2/2, step 2789/7134 completed (loss: 0.11083854734897614, acc: 0.9625360369682312)
[2025-02-13 03:59:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:11][root][INFO] - Training Epoch: 2/2, step 2790/7134 completed (loss: 0.09713797271251678, acc: 0.9757174253463745)
[2025-02-13 03:59:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:12][root][INFO] - Training Epoch: 2/2, step 2791/7134 completed (loss: 0.0384429395198822, acc: 0.9865471124649048)
[2025-02-13 03:59:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:12][root][INFO] - Training Epoch: 2/2, step 2792/7134 completed (loss: 0.035134635865688324, acc: 0.9930151104927063)
[2025-02-13 03:59:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:13][root][INFO] - Training Epoch: 2/2, step 2793/7134 completed (loss: 0.046456556767225266, acc: 0.9838056564331055)
[2025-02-13 03:59:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:13][root][INFO] - Training Epoch: 2/2, step 2794/7134 completed (loss: 0.009719951078295708, acc: 1.0)
[2025-02-13 03:59:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:13][root][INFO] - Training Epoch: 2/2, step 2795/7134 completed (loss: 0.07454968988895416, acc: 0.9791183471679688)
[2025-02-13 03:59:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:14][root][INFO] - Training Epoch: 2/2, step 2796/7134 completed (loss: 0.09979559481143951, acc: 0.960829496383667)
[2025-02-13 03:59:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:14][root][INFO] - Training Epoch: 2/2, step 2797/7134 completed (loss: 0.02858324535191059, acc: 0.9936708807945251)
[2025-02-13 03:59:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:15][root][INFO] - Training Epoch: 2/2, step 2798/7134 completed (loss: 0.05191804841160774, acc: 0.9831932783126831)
[2025-02-13 03:59:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:15][root][INFO] - Training Epoch: 2/2, step 2799/7134 completed (loss: 0.03368324041366577, acc: 0.9929947257041931)
[2025-02-13 03:59:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:15][root][INFO] - Training Epoch: 2/2, step 2800/7134 completed (loss: 0.07732722163200378, acc: 0.9718804955482483)
[2025-02-13 03:59:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:16][root][INFO] - Training Epoch: 2/2, step 2801/7134 completed (loss: 0.0417444221675396, acc: 0.9859334826469421)
[2025-02-13 03:59:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:16][root][INFO] - Training Epoch: 2/2, step 2802/7134 completed (loss: 0.03064163215458393, acc: 0.9900124669075012)
[2025-02-13 03:59:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:17][root][INFO] - Training Epoch: 2/2, step 2803/7134 completed (loss: 0.006924170069396496, acc: 0.99858558177948)
[2025-02-13 03:59:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:17][root][INFO] - Training Epoch: 2/2, step 2804/7134 completed (loss: 0.02986604906618595, acc: 0.9938119053840637)
[2025-02-13 03:59:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:18][root][INFO] - Training Epoch: 2/2, step 2805/7134 completed (loss: 0.01546387281268835, acc: 0.9960474371910095)
[2025-02-13 03:59:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:18][root][INFO] - Training Epoch: 2/2, step 2806/7134 completed (loss: 0.04977011680603027, acc: 0.9885246157646179)
[2025-02-13 03:59:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:19][root][INFO] - Training Epoch: 2/2, step 2807/7134 completed (loss: 0.028771406039595604, acc: 0.987525999546051)
[2025-02-13 03:59:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:19][root][INFO] - Training Epoch: 2/2, step 2808/7134 completed (loss: 0.015342256985604763, acc: 0.9954285621643066)
[2025-02-13 03:59:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:20][root][INFO] - Training Epoch: 2/2, step 2809/7134 completed (loss: 0.030074788257479668, acc: 0.9893048405647278)
[2025-02-13 03:59:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:20][root][INFO] - Training Epoch: 2/2, step 2810/7134 completed (loss: 0.038560591638088226, acc: 0.989311158657074)
[2025-02-13 03:59:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:21][root][INFO] - Training Epoch: 2/2, step 2811/7134 completed (loss: 0.007490908727049828, acc: 0.9976133704185486)
[2025-02-13 03:59:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:21][root][INFO] - Training Epoch: 2/2, step 2812/7134 completed (loss: 0.014894145540893078, acc: 0.9938271641731262)
[2025-02-13 03:59:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:21][root][INFO] - Training Epoch: 2/2, step 2813/7134 completed (loss: 0.01082650013267994, acc: 0.996221661567688)
[2025-02-13 03:59:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:22][root][INFO] - Training Epoch: 2/2, step 2814/7134 completed (loss: 0.02505243383347988, acc: 0.9935064911842346)
[2025-02-13 03:59:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:22][root][INFO] - Training Epoch: 2/2, step 2815/7134 completed (loss: 0.03234442323446274, acc: 0.990510106086731)
[2025-02-13 03:59:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:23][root][INFO] - Training Epoch: 2/2, step 2816/7134 completed (loss: 0.03656945377588272, acc: 0.9855072498321533)
[2025-02-13 03:59:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:23][root][INFO] - Training Epoch: 2/2, step 2817/7134 completed (loss: 0.04267890006303787, acc: 0.9884696006774902)
[2025-02-13 03:59:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:24][root][INFO] - Training Epoch: 2/2, step 2818/7134 completed (loss: 0.020394818857312202, acc: 0.9923245906829834)
[2025-02-13 03:59:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:24][root][INFO] - Training Epoch: 2/2, step 2819/7134 completed (loss: 0.007227504160255194, acc: 0.9967690110206604)
[2025-02-13 03:59:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:25][root][INFO] - Training Epoch: 2/2, step 2820/7134 completed (loss: 0.04753712937235832, acc: 0.9852724671363831)
[2025-02-13 03:59:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:25][root][INFO] - Training Epoch: 2/2, step 2821/7134 completed (loss: 0.03342027589678764, acc: 0.9892473220825195)
[2025-02-13 03:59:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:26][root][INFO] - Training Epoch: 2/2, step 2822/7134 completed (loss: 0.059859588742256165, acc: 0.982300877571106)
[2025-02-13 03:59:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:26][root][INFO] - Training Epoch: 2/2, step 2823/7134 completed (loss: 0.04467148333787918, acc: 0.9773049354553223)
[2025-02-13 03:59:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:27][root][INFO] - Training Epoch: 2/2, step 2824/7134 completed (loss: 0.07465732842683792, acc: 0.9707792401313782)
[2025-02-13 03:59:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:27][root][INFO] - Training Epoch: 2/2, step 2825/7134 completed (loss: 0.056930337101221085, acc: 0.9892617464065552)
[2025-02-13 03:59:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:27][root][INFO] - Training Epoch: 2/2, step 2826/7134 completed (loss: 0.05911194160580635, acc: 0.9849397540092468)
[2025-02-13 03:59:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:28][root][INFO] - Training Epoch: 2/2, step 2827/7134 completed (loss: 0.037220779806375504, acc: 0.9872286319732666)
[2025-02-13 03:59:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:28][root][INFO] - Training Epoch: 2/2, step 2828/7134 completed (loss: 0.029647603631019592, acc: 0.9919999837875366)
[2025-02-13 03:59:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:29][root][INFO] - Training Epoch: 2/2, step 2829/7134 completed (loss: 0.048786282539367676, acc: 0.9877216815948486)
[2025-02-13 03:59:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:29][root][INFO] - Training Epoch: 2/2, step 2830/7134 completed (loss: 0.008321099914610386, acc: 1.0)
[2025-02-13 03:59:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:30][root][INFO] - Training Epoch: 2/2, step 2831/7134 completed (loss: 0.016514694318175316, acc: 0.9968404173851013)
[2025-02-13 03:59:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:30][root][INFO] - Training Epoch: 2/2, step 2832/7134 completed (loss: 0.04266897961497307, acc: 0.9890109896659851)
[2025-02-13 03:59:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:31][root][INFO] - Training Epoch: 2/2, step 2833/7134 completed (loss: 0.020983772352337837, acc: 0.9941520690917969)
[2025-02-13 03:59:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:31][root][INFO] - Training Epoch: 2/2, step 2834/7134 completed (loss: 0.07290900498628616, acc: 0.9796954393386841)
[2025-02-13 03:59:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:32][root][INFO] - Training Epoch: 2/2, step 2835/7134 completed (loss: 0.03350299596786499, acc: 0.9919224381446838)
[2025-02-13 03:59:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:32][root][INFO] - Training Epoch: 2/2, step 2836/7134 completed (loss: 0.030727112665772438, acc: 0.9905213117599487)
[2025-02-13 03:59:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:32][root][INFO] - Training Epoch: 2/2, step 2837/7134 completed (loss: 0.013066977262496948, acc: 0.995945930480957)
[2025-02-13 03:59:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:33][root][INFO] - Training Epoch: 2/2, step 2838/7134 completed (loss: 0.05210019648075104, acc: 0.9843096137046814)
[2025-02-13 03:59:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:33][root][INFO] - Training Epoch: 2/2, step 2839/7134 completed (loss: 0.03452174365520477, acc: 0.9939302206039429)
[2025-02-13 03:59:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:34][root][INFO] - Training Epoch: 2/2, step 2840/7134 completed (loss: 0.020197443664073944, acc: 0.9944751262664795)
[2025-02-13 03:59:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:34][root][INFO] - Training Epoch: 2/2, step 2841/7134 completed (loss: 0.011996542103588581, acc: 0.9953917264938354)
[2025-02-13 03:59:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:35][root][INFO] - Training Epoch: 2/2, step 2842/7134 completed (loss: 0.02489364705979824, acc: 0.9935064911842346)
[2025-02-13 03:59:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:35][root][INFO] - Training Epoch: 2/2, step 2843/7134 completed (loss: 0.01833229698240757, acc: 0.9970588088035583)
[2025-02-13 03:59:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:36][root][INFO] - Training Epoch: 2/2, step 2844/7134 completed (loss: 0.07494048774242401, acc: 0.9808481335639954)
[2025-02-13 03:59:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:36][root][INFO] - Training Epoch: 2/2, step 2845/7134 completed (loss: 0.03949163481593132, acc: 0.9913344979286194)
[2025-02-13 03:59:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:37][root][INFO] - Training Epoch: 2/2, step 2846/7134 completed (loss: 0.047233954071998596, acc: 0.9884678721427917)
[2025-02-13 03:59:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:37][root][INFO] - Training Epoch: 2/2, step 2847/7134 completed (loss: 0.03748098015785217, acc: 0.9897360801696777)
[2025-02-13 03:59:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:37][root][INFO] - Training Epoch: 2/2, step 2848/7134 completed (loss: 0.023374086245894432, acc: 0.9930070042610168)
[2025-02-13 03:59:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:38][root][INFO] - Training Epoch: 2/2, step 2849/7134 completed (loss: 0.05235995352268219, acc: 0.9889349937438965)
[2025-02-13 03:59:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:38][root][INFO] - Training Epoch: 2/2, step 2850/7134 completed (loss: 0.0255221426486969, acc: 0.9935483932495117)
[2025-02-13 03:59:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:39][root][INFO] - Training Epoch: 2/2, step 2851/7134 completed (loss: 0.018443681299686432, acc: 0.9944751262664795)
[2025-02-13 03:59:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:39][root][INFO] - Training Epoch: 2/2, step 2852/7134 completed (loss: 0.014428273774683475, acc: 0.9960238337516785)
[2025-02-13 03:59:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:40][root][INFO] - Training Epoch: 2/2, step 2853/7134 completed (loss: 0.028514590114355087, acc: 0.989847719669342)
[2025-02-13 03:59:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:40][root][INFO] - Training Epoch: 2/2, step 2854/7134 completed (loss: 0.03190070763230324, acc: 0.9910846948623657)
[2025-02-13 03:59:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:40][root][INFO] - Training Epoch: 2/2, step 2855/7134 completed (loss: 0.03213738277554512, acc: 0.9903069734573364)
[2025-02-13 03:59:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:41][root][INFO] - Training Epoch: 2/2, step 2856/7134 completed (loss: 0.004567011725157499, acc: 0.9985895752906799)
[2025-02-13 03:59:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:41][root][INFO] - Training Epoch: 2/2, step 2857/7134 completed (loss: 0.039641451090574265, acc: 0.9898843765258789)
[2025-02-13 03:59:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:42][root][INFO] - Training Epoch: 2/2, step 2858/7134 completed (loss: 0.008203485049307346, acc: 0.9984227418899536)
[2025-02-13 03:59:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:42][root][INFO] - Training Epoch: 2/2, step 2859/7134 completed (loss: 0.04885474964976311, acc: 0.9860529899597168)
[2025-02-13 03:59:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:43][root][INFO] - Training Epoch: 2/2, step 2860/7134 completed (loss: 0.018241509795188904, acc: 0.9926605224609375)
[2025-02-13 03:59:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:43][root][INFO] - Training Epoch: 2/2, step 2861/7134 completed (loss: 0.004750838037580252, acc: 1.0)
[2025-02-13 03:59:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:43][root][INFO] - Training Epoch: 2/2, step 2862/7134 completed (loss: 0.017382174730300903, acc: 0.9965694546699524)
[2025-02-13 03:59:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:44][root][INFO] - Training Epoch: 2/2, step 2863/7134 completed (loss: 0.011826506815850735, acc: 0.9949044585227966)
[2025-02-13 03:59:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:44][root][INFO] - Training Epoch: 2/2, step 2864/7134 completed (loss: 0.02897370606660843, acc: 0.9948052167892456)
[2025-02-13 03:59:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:45][root][INFO] - Training Epoch: 2/2, step 2865/7134 completed (loss: 0.008070690557360649, acc: 0.997826099395752)
[2025-02-13 03:59:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:45][root][INFO] - Training Epoch: 2/2, step 2866/7134 completed (loss: 0.031228143721818924, acc: 0.9925261735916138)
[2025-02-13 03:59:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:46][root][INFO] - Training Epoch: 2/2, step 2867/7134 completed (loss: 0.01363567914813757, acc: 0.9938650131225586)
[2025-02-13 03:59:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:46][root][INFO] - Training Epoch: 2/2, step 2868/7134 completed (loss: 0.004834460560232401, acc: 1.0)
[2025-02-13 03:59:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:46][root][INFO] - Training Epoch: 2/2, step 2869/7134 completed (loss: 0.046250756829977036, acc: 0.9926793575286865)
[2025-02-13 03:59:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:47][root][INFO] - Training Epoch: 2/2, step 2870/7134 completed (loss: 0.04440917819738388, acc: 0.9958071112632751)
[2025-02-13 03:59:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:47][root][INFO] - Training Epoch: 2/2, step 2871/7134 completed (loss: 0.02733287215232849, acc: 0.9937597513198853)
[2025-02-13 03:59:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:48][root][INFO] - Training Epoch: 2/2, step 2872/7134 completed (loss: 0.003497426165267825, acc: 1.0)
[2025-02-13 03:59:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:48][root][INFO] - Training Epoch: 2/2, step 2873/7134 completed (loss: 0.016687164083123207, acc: 0.9908758997917175)
[2025-02-13 03:59:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:49][root][INFO] - Training Epoch: 2/2, step 2874/7134 completed (loss: 0.016799213364720345, acc: 0.9937597513198853)
[2025-02-13 03:59:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:49][root][INFO] - Training Epoch: 2/2, step 2875/7134 completed (loss: 0.013659673742949963, acc: 0.9957325458526611)
[2025-02-13 03:59:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:49][root][INFO] - Training Epoch: 2/2, step 2876/7134 completed (loss: 0.07922837138175964, acc: 0.9838709831237793)
[2025-02-13 03:59:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:50][root][INFO] - Training Epoch: 2/2, step 2877/7134 completed (loss: 0.008986764587461948, acc: 0.9978166222572327)
[2025-02-13 03:59:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:50][root][INFO] - Training Epoch: 2/2, step 2878/7134 completed (loss: 0.021822461858391762, acc: 0.9978813529014587)
[2025-02-13 03:59:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:51][root][INFO] - Training Epoch: 2/2, step 2879/7134 completed (loss: 0.009806226007640362, acc: 0.9984325766563416)
[2025-02-13 03:59:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:51][root][INFO] - Training Epoch: 2/2, step 2880/7134 completed (loss: 0.006728392094373703, acc: 0.9984375238418579)
[2025-02-13 03:59:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:52][root][INFO] - Training Epoch: 2/2, step 2881/7134 completed (loss: 0.020612068474292755, acc: 0.9928443431854248)
[2025-02-13 03:59:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:52][root][INFO] - Training Epoch: 2/2, step 2882/7134 completed (loss: 0.04105277359485626, acc: 0.9889025688171387)
[2025-02-13 03:59:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:52][root][INFO] - Training Epoch: 2/2, step 2883/7134 completed (loss: 0.020706387236714363, acc: 0.9947090148925781)
[2025-02-13 03:59:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:53][root][INFO] - Training Epoch: 2/2, step 2884/7134 completed (loss: 0.06275249272584915, acc: 0.9819004535675049)
[2025-02-13 03:59:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:53][root][INFO] - Training Epoch: 2/2, step 2885/7134 completed (loss: 0.026821523904800415, acc: 0.9939302206039429)
[2025-02-13 03:59:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:54][root][INFO] - Training Epoch: 2/2, step 2886/7134 completed (loss: 0.07072817534208298, acc: 0.9820895791053772)
[2025-02-13 03:59:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:54][root][INFO] - Training Epoch: 2/2, step 2887/7134 completed (loss: 0.03975585103034973, acc: 0.9893491268157959)
[2025-02-13 03:59:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:55][root][INFO] - Training Epoch: 2/2, step 2888/7134 completed (loss: 0.06329481303691864, acc: 0.9847434163093567)
[2025-02-13 03:59:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:55][root][INFO] - Training Epoch: 2/2, step 2889/7134 completed (loss: 0.010808986611664295, acc: 0.9958449006080627)
[2025-02-13 03:59:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:56][root][INFO] - Training Epoch: 2/2, step 2890/7134 completed (loss: 0.015089142136275768, acc: 0.9960052967071533)
[2025-02-13 03:59:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:56][root][INFO] - Training Epoch: 2/2, step 2891/7134 completed (loss: 0.03408300131559372, acc: 0.9907692074775696)
[2025-02-13 03:59:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:56][root][INFO] - Training Epoch: 2/2, step 2892/7134 completed (loss: 0.057385966181755066, acc: 0.983627200126648)
[2025-02-13 03:59:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:57][root][INFO] - Training Epoch: 2/2, step 2893/7134 completed (loss: 0.035374268889427185, acc: 0.9861286282539368)
[2025-02-13 03:59:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:57][root][INFO] - Training Epoch: 2/2, step 2894/7134 completed (loss: 0.030124086886644363, acc: 0.9865546226501465)
[2025-02-13 03:59:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:58][root][INFO] - Training Epoch: 2/2, step 2895/7134 completed (loss: 0.016300370916724205, acc: 0.9943289160728455)
[2025-02-13 03:59:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:58][root][INFO] - Training Epoch: 2/2, step 2896/7134 completed (loss: 0.01841844618320465, acc: 0.9961832165718079)
[2025-02-13 03:59:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:59][root][INFO] - Training Epoch: 2/2, step 2897/7134 completed (loss: 0.029376251623034477, acc: 0.9911190271377563)
[2025-02-13 03:59:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 03:59:59][root][INFO] - Training Epoch: 2/2, step 2898/7134 completed (loss: 0.025282304733991623, acc: 0.9934640526771545)
[2025-02-13 03:59:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:00][root][INFO] - Training Epoch: 2/2, step 2899/7134 completed (loss: 0.012233643792569637, acc: 0.9941860437393188)
[2025-02-13 04:00:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:00][root][INFO] - Training Epoch: 2/2, step 2900/7134 completed (loss: 0.025162598118185997, acc: 0.9962962865829468)
[2025-02-13 04:00:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:00][root][INFO] - Training Epoch: 2/2, step 2901/7134 completed (loss: 0.028951799497008324, acc: 0.9920381903648376)
[2025-02-13 04:00:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:01][root][INFO] - Training Epoch: 2/2, step 2902/7134 completed (loss: 0.017722956836223602, acc: 0.9924812316894531)
[2025-02-13 04:00:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:01][root][INFO] - Training Epoch: 2/2, step 2903/7134 completed (loss: 0.03775429353117943, acc: 0.9865047335624695)
[2025-02-13 04:00:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:02][root][INFO] - Training Epoch: 2/2, step 2904/7134 completed (loss: 0.006888147909194231, acc: 0.9975031018257141)
[2025-02-13 04:00:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:02][root][INFO] - Training Epoch: 2/2, step 2905/7134 completed (loss: 0.034376662224531174, acc: 0.9899874925613403)
[2025-02-13 04:00:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:03][root][INFO] - Training Epoch: 2/2, step 2906/7134 completed (loss: 0.003674084786325693, acc: 1.0)
[2025-02-13 04:00:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:03][root][INFO] - Training Epoch: 2/2, step 2907/7134 completed (loss: 0.04253169521689415, acc: 0.9851951599121094)
[2025-02-13 04:00:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:04][root][INFO] - Training Epoch: 2/2, step 2908/7134 completed (loss: 0.019221000373363495, acc: 0.9974126815795898)
[2025-02-13 04:00:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:04][root][INFO] - Training Epoch: 2/2, step 2909/7134 completed (loss: 0.020737795159220695, acc: 0.9952038526535034)
[2025-02-13 04:00:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:05][root][INFO] - Training Epoch: 2/2, step 2910/7134 completed (loss: 0.04731954634189606, acc: 0.9892933368682861)
[2025-02-13 04:00:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:05][root][INFO] - Training Epoch: 2/2, step 2911/7134 completed (loss: 0.007134647574275732, acc: 0.9986577033996582)
[2025-02-13 04:00:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:06][root][INFO] - Training Epoch: 2/2, step 2912/7134 completed (loss: 0.017579765990376472, acc: 0.9917241334915161)
[2025-02-13 04:00:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:06][root][INFO] - Training Epoch: 2/2, step 2913/7134 completed (loss: 0.02241240255534649, acc: 0.995726466178894)
[2025-02-13 04:00:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:06][root][INFO] - Training Epoch: 2/2, step 2914/7134 completed (loss: 0.021048851311206818, acc: 0.9963811635971069)
[2025-02-13 04:00:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:07][root][INFO] - Training Epoch: 2/2, step 2915/7134 completed (loss: 0.06345760822296143, acc: 0.9855907559394836)
[2025-02-13 04:00:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:07][root][INFO] - Training Epoch: 2/2, step 2916/7134 completed (loss: 0.04587371647357941, acc: 0.9914089441299438)
[2025-02-13 04:00:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:08][root][INFO] - Training Epoch: 2/2, step 2917/7134 completed (loss: 0.016900382936000824, acc: 0.9899497628211975)
[2025-02-13 04:00:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:08][root][INFO] - Training Epoch: 2/2, step 2918/7134 completed (loss: 0.015603365376591682, acc: 0.9953380227088928)
[2025-02-13 04:00:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:09][root][INFO] - Training Epoch: 2/2, step 2919/7134 completed (loss: 0.04622717201709747, acc: 0.9858989715576172)
[2025-02-13 04:00:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:09][root][INFO] - Training Epoch: 2/2, step 2920/7134 completed (loss: 0.018691103905439377, acc: 0.9947019815444946)
[2025-02-13 04:00:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:10][root][INFO] - Training Epoch: 2/2, step 2921/7134 completed (loss: 0.015238098800182343, acc: 0.9942528605461121)
[2025-02-13 04:00:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:10][root][INFO] - Training Epoch: 2/2, step 2922/7134 completed (loss: 0.04961688444018364, acc: 0.9886934757232666)
[2025-02-13 04:00:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:11][root][INFO] - Training Epoch: 2/2, step 2923/7134 completed (loss: 0.12671296298503876, acc: 0.9769585132598877)
[2025-02-13 04:00:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:11][root][INFO] - Training Epoch: 2/2, step 2924/7134 completed (loss: 0.011225007474422455, acc: 0.9976415038108826)
[2025-02-13 04:00:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:11][root][INFO] - Training Epoch: 2/2, step 2925/7134 completed (loss: 0.02662559039890766, acc: 0.9926289916038513)
[2025-02-13 04:00:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:12][root][INFO] - Training Epoch: 2/2, step 2926/7134 completed (loss: 0.12367133051156998, acc: 0.9771634340286255)
[2025-02-13 04:00:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:12][root][INFO] - Training Epoch: 2/2, step 2927/7134 completed (loss: 0.01335951965302229, acc: 0.996372401714325)
[2025-02-13 04:00:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:13][root][INFO] - Training Epoch: 2/2, step 2928/7134 completed (loss: 0.019750138744711876, acc: 0.9939024448394775)
[2025-02-13 04:00:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:13][root][INFO] - Training Epoch: 2/2, step 2929/7134 completed (loss: 0.023585021495819092, acc: 0.9934725761413574)
[2025-02-13 04:00:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:14][root][INFO] - Training Epoch: 2/2, step 2930/7134 completed (loss: 0.017545737326145172, acc: 0.9955947399139404)
[2025-02-13 04:00:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:14][root][INFO] - Training Epoch: 2/2, step 2931/7134 completed (loss: 0.042859651148319244, acc: 0.9865471124649048)
[2025-02-13 04:00:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:15][root][INFO] - Training Epoch: 2/2, step 2932/7134 completed (loss: 0.017916910350322723, acc: 0.9953650236129761)
[2025-02-13 04:00:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:15][root][INFO] - Training Epoch: 2/2, step 2933/7134 completed (loss: 0.021538274362683296, acc: 0.9913420081138611)
[2025-02-13 04:00:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:16][root][INFO] - Training Epoch: 2/2, step 2934/7134 completed (loss: 0.013213666155934334, acc: 0.995768666267395)
[2025-02-13 04:00:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:16][root][INFO] - Training Epoch: 2/2, step 2935/7134 completed (loss: 0.020321527495980263, acc: 0.9911894202232361)
[2025-02-13 04:00:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:17][root][INFO] - Training Epoch: 2/2, step 2936/7134 completed (loss: 0.030514679849147797, acc: 0.9908443689346313)
[2025-02-13 04:00:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:17][root][INFO] - Training Epoch: 2/2, step 2937/7134 completed (loss: 0.038407083600759506, acc: 0.9885222315788269)
[2025-02-13 04:00:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:17][root][INFO] - Training Epoch: 2/2, step 2938/7134 completed (loss: 0.02794828824698925, acc: 0.9966480731964111)
[2025-02-13 04:00:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:18][root][INFO] - Training Epoch: 2/2, step 2939/7134 completed (loss: 0.023864097893238068, acc: 0.995192289352417)
[2025-02-13 04:00:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:18][root][INFO] - Training Epoch: 2/2, step 2940/7134 completed (loss: 0.03893731161952019, acc: 0.9904240965843201)
[2025-02-13 04:00:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:19][root][INFO] - Training Epoch: 2/2, step 2941/7134 completed (loss: 0.0193608570843935, acc: 0.9927431344985962)
[2025-02-13 04:00:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:19][root][INFO] - Training Epoch: 2/2, step 2942/7134 completed (loss: 0.030791468918323517, acc: 0.9925705790519714)
[2025-02-13 04:00:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:19][root][INFO] - Training Epoch: 2/2, step 2943/7134 completed (loss: 0.008178655989468098, acc: 0.9966273307800293)
[2025-02-13 04:00:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:20][root][INFO] - Training Epoch: 2/2, step 2944/7134 completed (loss: 0.024630233645439148, acc: 0.9911894202232361)
[2025-02-13 04:00:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:20][root][INFO] - Training Epoch: 2/2, step 2945/7134 completed (loss: 0.012274234555661678, acc: 0.9953343868255615)
[2025-02-13 04:00:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:21][root][INFO] - Training Epoch: 2/2, step 2946/7134 completed (loss: 0.02626175433397293, acc: 0.9941860437393188)
[2025-02-13 04:00:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:21][root][INFO] - Training Epoch: 2/2, step 2947/7134 completed (loss: 0.03977368026971817, acc: 0.9874213933944702)
[2025-02-13 04:00:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:22][root][INFO] - Training Epoch: 2/2, step 2948/7134 completed (loss: 0.041753027588129044, acc: 0.9851729869842529)
[2025-02-13 04:00:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:22][root][INFO] - Training Epoch: 2/2, step 2949/7134 completed (loss: 0.006145200226455927, acc: 0.9969419240951538)
[2025-02-13 04:00:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:22][root][INFO] - Training Epoch: 2/2, step 2950/7134 completed (loss: 0.016164908185601234, acc: 0.9931034445762634)
[2025-02-13 04:00:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:23][root][INFO] - Training Epoch: 2/2, step 2951/7134 completed (loss: 0.0073437923565506935, acc: 0.9971014261245728)
[2025-02-13 04:00:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:23][root][INFO] - Training Epoch: 2/2, step 2952/7134 completed (loss: 0.016098203137516975, acc: 0.9925816059112549)
[2025-02-13 04:00:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:24][root][INFO] - Training Epoch: 2/2, step 2953/7134 completed (loss: 0.02540561929345131, acc: 0.9898648858070374)
[2025-02-13 04:00:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:24][root][INFO] - Training Epoch: 2/2, step 2954/7134 completed (loss: 0.02288580685853958, acc: 0.9948520064353943)
[2025-02-13 04:00:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:25][root][INFO] - Training Epoch: 2/2, step 2955/7134 completed (loss: 0.022396033629775047, acc: 0.9957386255264282)
[2025-02-13 04:00:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:25][root][INFO] - Training Epoch: 2/2, step 2956/7134 completed (loss: 0.017943279817700386, acc: 0.9941434860229492)
[2025-02-13 04:00:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:25][root][INFO] - Training Epoch: 2/2, step 2957/7134 completed (loss: 0.009786506183445454, acc: 0.9973261952400208)
[2025-02-13 04:00:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:26][root][INFO] - Training Epoch: 2/2, step 2958/7134 completed (loss: 0.020277133211493492, acc: 0.9948717951774597)
[2025-02-13 04:00:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:26][root][INFO] - Training Epoch: 2/2, step 2959/7134 completed (loss: 0.015383086167275906, acc: 0.9926900863647461)
[2025-02-13 04:00:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:27][root][INFO] - Training Epoch: 2/2, step 2960/7134 completed (loss: 0.020251844078302383, acc: 0.9931129217147827)
[2025-02-13 04:00:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:27][root][INFO] - Training Epoch: 2/2, step 2961/7134 completed (loss: 0.020518172532320023, acc: 0.9931972622871399)
[2025-02-13 04:00:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:28][root][INFO] - Training Epoch: 2/2, step 2962/7134 completed (loss: 0.01195729523897171, acc: 0.9922380447387695)
[2025-02-13 04:00:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:28][root][INFO] - Training Epoch: 2/2, step 2963/7134 completed (loss: 0.01738101802766323, acc: 0.9934554696083069)
[2025-02-13 04:00:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:29][root][INFO] - Training Epoch: 2/2, step 2964/7134 completed (loss: 0.012950013391673565, acc: 0.9941434860229492)
[2025-02-13 04:00:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:29][root][INFO] - Training Epoch: 2/2, step 2965/7134 completed (loss: 0.05176195874810219, acc: 0.9910447597503662)
[2025-02-13 04:00:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:29][root][INFO] - Training Epoch: 2/2, step 2966/7134 completed (loss: 0.0055303205735981464, acc: 0.9978678226470947)
[2025-02-13 04:00:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:30][root][INFO] - Training Epoch: 2/2, step 2967/7134 completed (loss: 0.036712147295475006, acc: 0.9902234673500061)
[2025-02-13 04:00:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:30][root][INFO] - Training Epoch: 2/2, step 2968/7134 completed (loss: 0.09997411072254181, acc: 0.971061110496521)
[2025-02-13 04:00:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:31][root][INFO] - Training Epoch: 2/2, step 2969/7134 completed (loss: 0.19131574034690857, acc: 0.9585798978805542)
[2025-02-13 04:00:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:31][root][INFO] - Training Epoch: 2/2, step 2970/7134 completed (loss: 0.1989450603723526, acc: 0.9520696997642517)
[2025-02-13 04:00:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:32][root][INFO] - Training Epoch: 2/2, step 2971/7134 completed (loss: 0.045467473566532135, acc: 0.9881734848022461)
[2025-02-13 04:00:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:32][root][INFO] - Training Epoch: 2/2, step 2972/7134 completed (loss: 0.021751996129751205, acc: 0.9940652847290039)
[2025-02-13 04:00:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:32][root][INFO] - Training Epoch: 2/2, step 2973/7134 completed (loss: 0.0400497131049633, acc: 0.9886075854301453)
[2025-02-13 04:00:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:33][root][INFO] - Training Epoch: 2/2, step 2974/7134 completed (loss: 0.043175872415304184, acc: 0.9857327938079834)
[2025-02-13 04:00:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:33][root][INFO] - Training Epoch: 2/2, step 2975/7134 completed (loss: 0.030173594132065773, acc: 0.9917126893997192)
[2025-02-13 04:00:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:34][root][INFO] - Training Epoch: 2/2, step 2976/7134 completed (loss: 0.040003880858421326, acc: 0.9904255270957947)
[2025-02-13 04:00:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:34][root][INFO] - Training Epoch: 2/2, step 2977/7134 completed (loss: 0.03758340701460838, acc: 0.9884393215179443)
[2025-02-13 04:00:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:35][root][INFO] - Training Epoch: 2/2, step 2978/7134 completed (loss: 0.041647423058748245, acc: 0.9900709390640259)
[2025-02-13 04:00:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:35][root][INFO] - Training Epoch: 2/2, step 2979/7134 completed (loss: 0.011090810410678387, acc: 0.9968847632408142)
[2025-02-13 04:00:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:36][root][INFO] - Training Epoch: 2/2, step 2980/7134 completed (loss: 0.03078256919980049, acc: 0.9923567175865173)
[2025-02-13 04:00:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:36][root][INFO] - Training Epoch: 2/2, step 2981/7134 completed (loss: 0.06206957623362541, acc: 0.9864314794540405)
[2025-02-13 04:00:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:37][root][INFO] - Training Epoch: 2/2, step 2982/7134 completed (loss: 0.03704012930393219, acc: 0.9895287752151489)
[2025-02-13 04:00:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:37][root][INFO] - Training Epoch: 2/2, step 2983/7134 completed (loss: 0.07418173551559448, acc: 0.9814814925193787)
[2025-02-13 04:00:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:37][root][INFO] - Training Epoch: 2/2, step 2984/7134 completed (loss: 0.024924607947468758, acc: 0.9937106966972351)
[2025-02-13 04:00:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:38][root][INFO] - Training Epoch: 2/2, step 2985/7134 completed (loss: 0.029476726427674294, acc: 0.9928486347198486)
[2025-02-13 04:00:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:38][root][INFO] - Training Epoch: 2/2, step 2986/7134 completed (loss: 0.01604321599006653, acc: 0.9964538812637329)
[2025-02-13 04:00:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:39][root][INFO] - Training Epoch: 2/2, step 2987/7134 completed (loss: 0.02115422487258911, acc: 0.9964664578437805)
[2025-02-13 04:00:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:39][root][INFO] - Training Epoch: 2/2, step 2988/7134 completed (loss: 0.01826656423509121, acc: 0.9975669384002686)
[2025-02-13 04:00:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:40][root][INFO] - Training Epoch: 2/2, step 2989/7134 completed (loss: 0.01942579261958599, acc: 0.9947712421417236)
[2025-02-13 04:00:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:40][root][INFO] - Training Epoch: 2/2, step 2990/7134 completed (loss: 0.09610490500926971, acc: 0.9772727489471436)
[2025-02-13 04:00:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:41][root][INFO] - Training Epoch: 2/2, step 2991/7134 completed (loss: 0.11471788585186005, acc: 0.9653259515762329)
[2025-02-13 04:00:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:41][root][INFO] - Training Epoch: 2/2, step 2992/7134 completed (loss: 0.053944699466228485, acc: 0.9834087491035461)
[2025-02-13 04:00:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:41][root][INFO] - Training Epoch: 2/2, step 2993/7134 completed (loss: 0.029496129602193832, acc: 0.9938875436782837)
[2025-02-13 04:00:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:42][root][INFO] - Training Epoch: 2/2, step 2994/7134 completed (loss: 0.01565834879875183, acc: 0.9964028596878052)
[2025-02-13 04:00:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:42][root][INFO] - Training Epoch: 2/2, step 2995/7134 completed (loss: 0.029081586748361588, acc: 0.994350254535675)
[2025-02-13 04:00:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:43][root][INFO] - Training Epoch: 2/2, step 2996/7134 completed (loss: 0.03651253506541252, acc: 0.988304078578949)
[2025-02-13 04:00:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:43][root][INFO] - Training Epoch: 2/2, step 2997/7134 completed (loss: 0.014228250831365585, acc: 0.9989604949951172)
[2025-02-13 04:00:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:44][root][INFO] - Training Epoch: 2/2, step 2998/7134 completed (loss: 0.0185456071048975, acc: 0.996363639831543)
[2025-02-13 04:00:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:44][root][INFO] - Training Epoch: 2/2, step 2999/7134 completed (loss: 0.013844647444784641, acc: 0.9971056580543518)
[2025-02-13 04:00:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:45][root][INFO] - Training Epoch: 2/2, step 3000/7134 completed (loss: 0.024449842050671577, acc: 0.9935759902000427)
[2025-02-13 04:00:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:45][root][INFO] - Training Epoch: 2/2, step 3001/7134 completed (loss: 0.014673927798867226, acc: 0.9964454770088196)
[2025-02-13 04:00:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:46][root][INFO] - Training Epoch: 2/2, step 3002/7134 completed (loss: 0.018680159002542496, acc: 0.9943310618400574)
[2025-02-13 04:00:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:46][root][INFO] - Training Epoch: 2/2, step 3003/7134 completed (loss: 0.01023390144109726, acc: 0.9976798295974731)
[2025-02-13 04:00:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:46][root][INFO] - Training Epoch: 2/2, step 3004/7134 completed (loss: 0.007060490548610687, acc: 1.0)
[2025-02-13 04:00:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:47][root][INFO] - Training Epoch: 2/2, step 3005/7134 completed (loss: 0.006966919172555208, acc: 1.0)
[2025-02-13 04:00:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:47][root][INFO] - Training Epoch: 2/2, step 3006/7134 completed (loss: 0.009459290653467178, acc: 0.9961685538291931)
[2025-02-13 04:00:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:48][root][INFO] - Training Epoch: 2/2, step 3007/7134 completed (loss: 0.015036210417747498, acc: 0.9953161478042603)
[2025-02-13 04:00:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:48][root][INFO] - Training Epoch: 2/2, step 3008/7134 completed (loss: 0.04125915840268135, acc: 0.9902098178863525)
[2025-02-13 04:00:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:49][root][INFO] - Training Epoch: 2/2, step 3009/7134 completed (loss: 0.062470026314258575, acc: 0.9886075854301453)
[2025-02-13 04:00:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:49][root][INFO] - Training Epoch: 2/2, step 3010/7134 completed (loss: 0.0320640429854393, acc: 0.99005526304245)
[2025-02-13 04:00:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:50][root][INFO] - Training Epoch: 2/2, step 3011/7134 completed (loss: 0.07320494204759598, acc: 0.9814550876617432)
[2025-02-13 04:00:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:50][root][INFO] - Training Epoch: 2/2, step 3012/7134 completed (loss: 0.042754653841257095, acc: 0.9889065027236938)
[2025-02-13 04:00:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:51][root][INFO] - Training Epoch: 2/2, step 3013/7134 completed (loss: 0.014077399857342243, acc: 0.9976958632469177)
[2025-02-13 04:00:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:51][root][INFO] - Training Epoch: 2/2, step 3014/7134 completed (loss: 0.022999418899416924, acc: 0.9929659962654114)
[2025-02-13 04:00:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:51][root][INFO] - Training Epoch: 2/2, step 3015/7134 completed (loss: 0.014210335910320282, acc: 0.9958677887916565)
[2025-02-13 04:00:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:52][root][INFO] - Training Epoch: 2/2, step 3016/7134 completed (loss: 0.014667356386780739, acc: 0.997187077999115)
[2025-02-13 04:00:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:52][root][INFO] - Training Epoch: 2/2, step 3017/7134 completed (loss: 0.01886196807026863, acc: 0.993220329284668)
[2025-02-13 04:00:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:53][root][INFO] - Training Epoch: 2/2, step 3018/7134 completed (loss: 0.020183218643069267, acc: 0.9966942071914673)
[2025-02-13 04:00:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:53][root][INFO] - Training Epoch: 2/2, step 3019/7134 completed (loss: 0.01751246303319931, acc: 0.9950980544090271)
[2025-02-13 04:00:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:54][root][INFO] - Training Epoch: 2/2, step 3020/7134 completed (loss: 0.027192197740077972, acc: 0.9910873174667358)
[2025-02-13 04:00:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:54][root][INFO] - Training Epoch: 2/2, step 3021/7134 completed (loss: 0.017185773700475693, acc: 0.9945454597473145)
[2025-02-13 04:00:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:54][root][INFO] - Training Epoch: 2/2, step 3022/7134 completed (loss: 0.026715628802776337, acc: 0.9967845678329468)
[2025-02-13 04:00:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:55][root][INFO] - Training Epoch: 2/2, step 3023/7134 completed (loss: 0.01984565518796444, acc: 0.9920477271080017)
[2025-02-13 04:00:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:55][root][INFO] - Training Epoch: 2/2, step 3024/7134 completed (loss: 0.003696525003761053, acc: 1.0)
[2025-02-13 04:00:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:56][root][INFO] - Training Epoch: 2/2, step 3025/7134 completed (loss: 0.04486681520938873, acc: 0.9879310131072998)
[2025-02-13 04:00:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:56][root][INFO] - Training Epoch: 2/2, step 3026/7134 completed (loss: 0.0063616251572966576, acc: 0.9982046484947205)
[2025-02-13 04:00:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:57][root][INFO] - Training Epoch: 2/2, step 3027/7134 completed (loss: 0.03948763385415077, acc: 0.991919219493866)
[2025-02-13 04:00:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:57][root][INFO] - Training Epoch: 2/2, step 3028/7134 completed (loss: 0.007616761140525341, acc: 0.997732400894165)
[2025-02-13 04:00:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:57][root][INFO] - Training Epoch: 2/2, step 3029/7134 completed (loss: 0.036230482161045074, acc: 0.9938176274299622)
[2025-02-13 04:00:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:58][root][INFO] - Training Epoch: 2/2, step 3030/7134 completed (loss: 0.014998766593635082, acc: 0.9954198598861694)
[2025-02-13 04:00:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:58][root][INFO] - Training Epoch: 2/2, step 3031/7134 completed (loss: 0.010449906811118126, acc: 0.9957627058029175)
[2025-02-13 04:00:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:59][root][INFO] - Training Epoch: 2/2, step 3032/7134 completed (loss: 0.042419884353876114, acc: 0.9927404522895813)
[2025-02-13 04:00:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:59][root][INFO] - Training Epoch: 2/2, step 3033/7134 completed (loss: 0.020325349643826485, acc: 0.9911816716194153)
[2025-02-13 04:00:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:00:59][root][INFO] - Training Epoch: 2/2, step 3034/7134 completed (loss: 0.02630798891186714, acc: 0.9936169981956482)
[2025-02-13 04:01:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:00][root][INFO] - Training Epoch: 2/2, step 3035/7134 completed (loss: 0.006260364782065153, acc: 0.9984251856803894)
[2025-02-13 04:01:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:00][root][INFO] - Training Epoch: 2/2, step 3036/7134 completed (loss: 0.04143308103084564, acc: 0.9916387796401978)
[2025-02-13 04:01:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:01][root][INFO] - Training Epoch: 2/2, step 3037/7134 completed (loss: 0.01909017562866211, acc: 0.9938367009162903)
[2025-02-13 04:01:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:01][root][INFO] - Training Epoch: 2/2, step 3038/7134 completed (loss: 0.011091489344835281, acc: 0.9965217113494873)
[2025-02-13 04:01:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:02][root][INFO] - Training Epoch: 2/2, step 3039/7134 completed (loss: 0.03622548654675484, acc: 0.9954057931900024)
[2025-02-13 04:01:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:02][root][INFO] - Training Epoch: 2/2, step 3040/7134 completed (loss: 0.02043018862605095, acc: 0.994434118270874)
[2025-02-13 04:01:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:02][root][INFO] - Training Epoch: 2/2, step 3041/7134 completed (loss: 0.04893932491540909, acc: 0.9920254945755005)
[2025-02-13 04:01:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:03][root][INFO] - Training Epoch: 2/2, step 3042/7134 completed (loss: 0.013684802688658237, acc: 0.9955686926841736)
[2025-02-13 04:01:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:03][root][INFO] - Training Epoch: 2/2, step 3043/7134 completed (loss: 0.007520653773099184, acc: 0.9961977005004883)
[2025-02-13 04:01:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:04][root][INFO] - Training Epoch: 2/2, step 3044/7134 completed (loss: 0.019814711064100266, acc: 0.9965457916259766)
[2025-02-13 04:01:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:04][root][INFO] - Training Epoch: 2/2, step 3045/7134 completed (loss: 0.010073456913232803, acc: 0.998305082321167)
[2025-02-13 04:01:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:04][root][INFO] - Training Epoch: 2/2, step 3046/7134 completed (loss: 0.024533558636903763, acc: 0.9931694269180298)
[2025-02-13 04:01:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:05][root][INFO] - Training Epoch: 2/2, step 3047/7134 completed (loss: 0.006939006969332695, acc: 0.9974842667579651)
[2025-02-13 04:01:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:05][root][INFO] - Training Epoch: 2/2, step 3048/7134 completed (loss: 0.01100364699959755, acc: 0.996221661567688)
[2025-02-13 04:01:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:06][root][INFO] - Training Epoch: 2/2, step 3049/7134 completed (loss: 0.013065195642411709, acc: 0.9970930218696594)
[2025-02-13 04:01:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:06][root][INFO] - Training Epoch: 2/2, step 3050/7134 completed (loss: 0.01874818466603756, acc: 0.9940617680549622)
[2025-02-13 04:01:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:07][root][INFO] - Training Epoch: 2/2, step 3051/7134 completed (loss: 0.027820128947496414, acc: 0.9909774661064148)
[2025-02-13 04:01:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:07][root][INFO] - Training Epoch: 2/2, step 3052/7134 completed (loss: 0.019336771219968796, acc: 0.9935566782951355)
[2025-02-13 04:01:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:08][root][INFO] - Training Epoch: 2/2, step 3053/7134 completed (loss: 0.013610947877168655, acc: 0.9927536249160767)
[2025-02-13 04:01:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:08][root][INFO] - Training Epoch: 2/2, step 3054/7134 completed (loss: 0.040179360657930374, acc: 0.9896103739738464)
[2025-02-13 04:01:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:08][root][INFO] - Training Epoch: 2/2, step 3055/7134 completed (loss: 0.02195642702281475, acc: 0.9948717951774597)
[2025-02-13 04:01:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:09][root][INFO] - Training Epoch: 2/2, step 3056/7134 completed (loss: 0.01016332022845745, acc: 0.9948586225509644)
[2025-02-13 04:01:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:09][root][INFO] - Training Epoch: 2/2, step 3057/7134 completed (loss: 0.009244519285857677, acc: 0.9972260594367981)
[2025-02-13 04:01:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:10][root][INFO] - Training Epoch: 2/2, step 3058/7134 completed (loss: 0.04417019337415695, acc: 0.9909326434135437)
[2025-02-13 04:01:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:10][root][INFO] - Training Epoch: 2/2, step 3059/7134 completed (loss: 0.010931531898677349, acc: 0.9945205450057983)
[2025-02-13 04:01:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:11][root][INFO] - Training Epoch: 2/2, step 3060/7134 completed (loss: 0.01836533099412918, acc: 0.9928401112556458)
[2025-02-13 04:01:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:11][root][INFO] - Training Epoch: 2/2, step 3061/7134 completed (loss: 0.014010646380484104, acc: 0.9939393997192383)
[2025-02-13 04:01:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:12][root][INFO] - Training Epoch: 2/2, step 3062/7134 completed (loss: 0.006819088011980057, acc: 0.9986522793769836)
[2025-02-13 04:01:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:12][root][INFO] - Training Epoch: 2/2, step 3063/7134 completed (loss: 0.018715424463152885, acc: 0.9948652386665344)
[2025-02-13 04:01:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:12][root][INFO] - Training Epoch: 2/2, step 3064/7134 completed (loss: 0.01202577818185091, acc: 0.997183084487915)
[2025-02-13 04:01:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:13][root][INFO] - Training Epoch: 2/2, step 3065/7134 completed (loss: 0.009831283241510391, acc: 0.9959677457809448)
[2025-02-13 04:01:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:13][root][INFO] - Training Epoch: 2/2, step 3066/7134 completed (loss: 0.007812665775418282, acc: 0.9967741966247559)
[2025-02-13 04:01:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:14][root][INFO] - Training Epoch: 2/2, step 3067/7134 completed (loss: 0.012700187973678112, acc: 0.996363639831543)
[2025-02-13 04:01:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:14][root][INFO] - Training Epoch: 2/2, step 3068/7134 completed (loss: 0.021891100332140923, acc: 0.9905277490615845)
[2025-02-13 04:01:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:14][root][INFO] - Training Epoch: 2/2, step 3069/7134 completed (loss: 0.016520801931619644, acc: 0.9953917264938354)
[2025-02-13 04:01:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:15][root][INFO] - Training Epoch: 2/2, step 3070/7134 completed (loss: 0.012424948625266552, acc: 0.9936373233795166)
[2025-02-13 04:01:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:15][root][INFO] - Training Epoch: 2/2, step 3071/7134 completed (loss: 0.01329171285033226, acc: 0.9961389899253845)
[2025-02-13 04:01:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:16][root][INFO] - Training Epoch: 2/2, step 3072/7134 completed (loss: 0.015501882880926132, acc: 0.9976662993431091)
[2025-02-13 04:01:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:16][root][INFO] - Training Epoch: 2/2, step 3073/7134 completed (loss: 0.009435757994651794, acc: 0.9961832165718079)
[2025-02-13 04:01:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:17][root][INFO] - Training Epoch: 2/2, step 3074/7134 completed (loss: 0.008256973698735237, acc: 0.9966722130775452)
[2025-02-13 04:01:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:17][root][INFO] - Training Epoch: 2/2, step 3075/7134 completed (loss: 0.011082546785473824, acc: 0.9972222447395325)
[2025-02-13 04:01:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:18][root][INFO] - Training Epoch: 2/2, step 3076/7134 completed (loss: 0.025211075320839882, acc: 0.9911209940910339)
[2025-02-13 04:01:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:18][root][INFO] - Training Epoch: 2/2, step 3077/7134 completed (loss: 0.04105345159769058, acc: 0.985792338848114)
[2025-02-13 04:01:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:19][root][INFO] - Training Epoch: 2/2, step 3078/7134 completed (loss: 0.04520653933286667, acc: 0.985792338848114)
[2025-02-13 04:01:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:19][root][INFO] - Training Epoch: 2/2, step 3079/7134 completed (loss: 0.02206011489033699, acc: 0.9925925731658936)
[2025-02-13 04:01:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:19][root][INFO] - Training Epoch: 2/2, step 3080/7134 completed (loss: 0.03988627344369888, acc: 0.9880043864250183)
[2025-02-13 04:01:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:20][root][INFO] - Training Epoch: 2/2, step 3081/7134 completed (loss: 0.03766569495201111, acc: 0.9913793206214905)
[2025-02-13 04:01:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:20][root][INFO] - Training Epoch: 2/2, step 3082/7134 completed (loss: 0.03642025217413902, acc: 0.9855595827102661)
[2025-02-13 04:01:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:21][root][INFO] - Training Epoch: 2/2, step 3083/7134 completed (loss: 0.02925368957221508, acc: 0.9908376932144165)
[2025-02-13 04:01:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:21][root][INFO] - Training Epoch: 2/2, step 3084/7134 completed (loss: 0.04094802215695381, acc: 0.988095223903656)
[2025-02-13 04:01:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:22][root][INFO] - Training Epoch: 2/2, step 3085/7134 completed (loss: 0.03171456232666969, acc: 0.9927007555961609)
[2025-02-13 04:01:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:22][root][INFO] - Training Epoch: 2/2, step 3086/7134 completed (loss: 0.046325866132974625, acc: 0.9831649661064148)
[2025-02-13 04:01:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:23][root][INFO] - Training Epoch: 2/2, step 3087/7134 completed (loss: 0.03168046846985817, acc: 0.9885222315788269)
[2025-02-13 04:01:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:23][root][INFO] - Training Epoch: 2/2, step 3088/7134 completed (loss: 0.029129939153790474, acc: 0.9932356476783752)
[2025-02-13 04:01:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:23][root][INFO] - Training Epoch: 2/2, step 3089/7134 completed (loss: 0.031839001923799515, acc: 0.9892125129699707)
[2025-02-13 04:01:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:24][root][INFO] - Training Epoch: 2/2, step 3090/7134 completed (loss: 0.06169169768691063, acc: 0.9836956262588501)
[2025-02-13 04:01:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:24][root][INFO] - Training Epoch: 2/2, step 3091/7134 completed (loss: 0.03587125614285469, acc: 0.9896313548088074)
[2025-02-13 04:01:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:25][root][INFO] - Training Epoch: 2/2, step 3092/7134 completed (loss: 0.04047756269574165, acc: 0.9852125644683838)
[2025-02-13 04:01:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:25][root][INFO] - Training Epoch: 2/2, step 3093/7134 completed (loss: 0.03033633530139923, acc: 0.99227374792099)
[2025-02-13 04:01:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:26][root][INFO] - Training Epoch: 2/2, step 3094/7134 completed (loss: 0.011384421028196812, acc: 0.9959127902984619)
[2025-02-13 04:01:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:26][root][INFO] - Training Epoch: 2/2, step 3095/7134 completed (loss: 0.039724595844745636, acc: 0.9916666746139526)
[2025-02-13 04:01:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:27][root][INFO] - Training Epoch: 2/2, step 3096/7134 completed (loss: 0.061009444296360016, acc: 0.9794608354568481)
[2025-02-13 04:01:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:27][root][INFO] - Training Epoch: 2/2, step 3097/7134 completed (loss: 0.053444456309080124, acc: 0.9857549667358398)
[2025-02-13 04:01:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:28][root][INFO] - Training Epoch: 2/2, step 3098/7134 completed (loss: 0.01161169447004795, acc: 0.9968553185462952)
[2025-02-13 04:01:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:28][root][INFO] - Training Epoch: 2/2, step 3099/7134 completed (loss: 0.01433535385876894, acc: 0.9935170412063599)
[2025-02-13 04:01:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:28][root][INFO] - Training Epoch: 2/2, step 3100/7134 completed (loss: 0.028618061915040016, acc: 0.9914893507957458)
[2025-02-13 04:01:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:29][root][INFO] - Training Epoch: 2/2, step 3101/7134 completed (loss: 0.017409149557352066, acc: 0.9939172863960266)
[2025-02-13 04:01:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:29][root][INFO] - Training Epoch: 2/2, step 3102/7134 completed (loss: 0.014372900128364563, acc: 0.9940740466117859)
[2025-02-13 04:01:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:30][root][INFO] - Training Epoch: 2/2, step 3103/7134 completed (loss: 0.01590067334473133, acc: 0.9978700876235962)
[2025-02-13 04:01:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:30][root][INFO] - Training Epoch: 2/2, step 3104/7134 completed (loss: 0.027616769075393677, acc: 0.9961832165718079)
[2025-02-13 04:01:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:31][root][INFO] - Training Epoch: 2/2, step 3105/7134 completed (loss: 0.028037987649440765, acc: 0.9934065937995911)
[2025-02-13 04:01:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:31][root][INFO] - Training Epoch: 2/2, step 3106/7134 completed (loss: 0.0070936535485088825, acc: 1.0)
[2025-02-13 04:01:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:32][root][INFO] - Training Epoch: 2/2, step 3107/7134 completed (loss: 0.011978222988545895, acc: 0.9960784316062927)
[2025-02-13 04:01:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:32][root][INFO] - Training Epoch: 2/2, step 3108/7134 completed (loss: 0.03137490153312683, acc: 0.9938461780548096)
[2025-02-13 04:01:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:32][root][INFO] - Training Epoch: 2/2, step 3109/7134 completed (loss: 0.020413992926478386, acc: 0.9963099360466003)
[2025-02-13 04:01:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:33][root][INFO] - Training Epoch: 2/2, step 3110/7134 completed (loss: 0.012815176509320736, acc: 0.995121955871582)
[2025-02-13 04:01:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:33][root][INFO] - Training Epoch: 2/2, step 3111/7134 completed (loss: 0.09046425670385361, acc: 0.9742268323898315)
[2025-02-13 04:01:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:34][root][INFO] - Training Epoch: 2/2, step 3112/7134 completed (loss: 0.03164384886622429, acc: 0.9964912533760071)
[2025-02-13 04:01:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:34][root][INFO] - Training Epoch: 2/2, step 3113/7134 completed (loss: 0.018058786168694496, acc: 0.992977499961853)
[2025-02-13 04:01:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:35][root][INFO] - Training Epoch: 2/2, step 3114/7134 completed (loss: 0.02960391528904438, acc: 0.9913793206214905)
[2025-02-13 04:01:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:35][root][INFO] - Training Epoch: 2/2, step 3115/7134 completed (loss: 0.04751892015337944, acc: 0.9938271641731262)
[2025-02-13 04:01:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:35][root][INFO] - Training Epoch: 2/2, step 3116/7134 completed (loss: 0.027777688577771187, acc: 0.9910256266593933)
[2025-02-13 04:01:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:36][root][INFO] - Training Epoch: 2/2, step 3117/7134 completed (loss: 0.05912601575255394, acc: 0.99048912525177)
[2025-02-13 04:01:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:36][root][INFO] - Training Epoch: 2/2, step 3118/7134 completed (loss: 0.0417427234351635, acc: 0.990777313709259)
[2025-02-13 04:01:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:37][root][INFO] - Training Epoch: 2/2, step 3119/7134 completed (loss: 0.04214287921786308, acc: 0.9844357967376709)
[2025-02-13 04:01:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:37][root][INFO] - Training Epoch: 2/2, step 3120/7134 completed (loss: 0.04336521774530411, acc: 0.994358241558075)
[2025-02-13 04:01:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:38][root][INFO] - Training Epoch: 2/2, step 3121/7134 completed (loss: 0.028057217597961426, acc: 0.9922879338264465)
[2025-02-13 04:01:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:38][root][INFO] - Training Epoch: 2/2, step 3122/7134 completed (loss: 0.023153698071837425, acc: 0.9941860437393188)
[2025-02-13 04:01:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:39][root][INFO] - Training Epoch: 2/2, step 3123/7134 completed (loss: 0.04588097706437111, acc: 0.9850746393203735)
[2025-02-13 04:01:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:39][root][INFO] - Training Epoch: 2/2, step 3124/7134 completed (loss: 0.0310300774872303, acc: 0.9950494766235352)
[2025-02-13 04:01:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:39][root][INFO] - Training Epoch: 2/2, step 3125/7134 completed (loss: 0.01558327954262495, acc: 0.9909228682518005)
[2025-02-13 04:01:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:40][root][INFO] - Training Epoch: 2/2, step 3126/7134 completed (loss: 0.02591991052031517, acc: 0.9923760890960693)
[2025-02-13 04:01:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:40][root][INFO] - Training Epoch: 2/2, step 3127/7134 completed (loss: 0.024529434740543365, acc: 0.9906976819038391)
[2025-02-13 04:01:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:41][root][INFO] - Training Epoch: 2/2, step 3128/7134 completed (loss: 0.01856641098856926, acc: 0.9909677505493164)
[2025-02-13 04:01:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:41][root][INFO] - Training Epoch: 2/2, step 3129/7134 completed (loss: 0.01913030631840229, acc: 0.993819534778595)
[2025-02-13 04:01:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:42][root][INFO] - Training Epoch: 2/2, step 3130/7134 completed (loss: 0.0178853627294302, acc: 0.9919725060462952)
[2025-02-13 04:01:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:42][root][INFO] - Training Epoch: 2/2, step 3131/7134 completed (loss: 0.03872564062476158, acc: 0.9884058237075806)
[2025-02-13 04:01:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:43][root][INFO] - Training Epoch: 2/2, step 3132/7134 completed (loss: 0.0099593885242939, acc: 0.9934924244880676)
[2025-02-13 04:01:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:43][root][INFO] - Training Epoch: 2/2, step 3133/7134 completed (loss: 0.07507920265197754, acc: 0.9807692170143127)
[2025-02-13 04:01:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:44][root][INFO] - Training Epoch: 2/2, step 3134/7134 completed (loss: 0.022930283099412918, acc: 0.9963369965553284)
[2025-02-13 04:01:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:44][root][INFO] - Training Epoch: 2/2, step 3135/7134 completed (loss: 0.02904989942908287, acc: 0.988452672958374)
[2025-02-13 04:01:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:44][root][INFO] - Training Epoch: 2/2, step 3136/7134 completed (loss: 0.019772054627537727, acc: 0.9947826266288757)
[2025-02-13 04:01:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:45][root][INFO] - Training Epoch: 2/2, step 3137/7134 completed (loss: 0.03323984146118164, acc: 0.988095223903656)
[2025-02-13 04:01:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:45][root][INFO] - Training Epoch: 2/2, step 3138/7134 completed (loss: 0.031627848744392395, acc: 0.9911764860153198)
[2025-02-13 04:01:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:46][root][INFO] - Training Epoch: 2/2, step 3139/7134 completed (loss: 0.06920318305492401, acc: 0.9785407781600952)
[2025-02-13 04:01:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:46][root][INFO] - Training Epoch: 2/2, step 3140/7134 completed (loss: 0.020909341052174568, acc: 0.994163453578949)
[2025-02-13 04:01:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:47][root][INFO] - Training Epoch: 2/2, step 3141/7134 completed (loss: 0.04263812676072121, acc: 0.9912587404251099)
[2025-02-13 04:01:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:47][root][INFO] - Training Epoch: 2/2, step 3142/7134 completed (loss: 0.013373823836445808, acc: 0.9970104694366455)
[2025-02-13 04:01:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:48][root][INFO] - Training Epoch: 2/2, step 3143/7134 completed (loss: 0.013225242495536804, acc: 0.9967845678329468)
[2025-02-13 04:01:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:48][root][INFO] - Training Epoch: 2/2, step 3144/7134 completed (loss: 0.014137434773147106, acc: 0.9952830076217651)
[2025-02-13 04:01:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:48][root][INFO] - Training Epoch: 2/2, step 3145/7134 completed (loss: 0.04274765029549599, acc: 0.9868708848953247)
[2025-02-13 04:01:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:49][root][INFO] - Training Epoch: 2/2, step 3146/7134 completed (loss: 0.015222002752125263, acc: 0.9947916865348816)
[2025-02-13 04:01:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:49][root][INFO] - Training Epoch: 2/2, step 3147/7134 completed (loss: 0.023312540724873543, acc: 0.9919742941856384)
[2025-02-13 04:01:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:50][root][INFO] - Training Epoch: 2/2, step 3148/7134 completed (loss: 0.015522635541856289, acc: 0.9936224222183228)
[2025-02-13 04:01:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:50][root][INFO] - Training Epoch: 2/2, step 3149/7134 completed (loss: 0.009438053704798222, acc: 0.9970717430114746)
[2025-02-13 04:01:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:51][root][INFO] - Training Epoch: 2/2, step 3150/7134 completed (loss: 0.09553521126508713, acc: 0.9842209219932556)
[2025-02-13 04:01:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:51][root][INFO] - Training Epoch: 2/2, step 3151/7134 completed (loss: 0.0475129596889019, acc: 0.9865900278091431)
[2025-02-13 04:01:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:52][root][INFO] - Training Epoch: 2/2, step 3152/7134 completed (loss: 0.0270512867718935, acc: 0.9863636493682861)
[2025-02-13 04:01:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:52][root][INFO] - Training Epoch: 2/2, step 3153/7134 completed (loss: 0.03650142624974251, acc: 0.9887217879295349)
[2025-02-13 04:01:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:52][root][INFO] - Training Epoch: 2/2, step 3154/7134 completed (loss: 0.03522338345646858, acc: 0.9911190271377563)
[2025-02-13 04:01:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:53][root][INFO] - Training Epoch: 2/2, step 3155/7134 completed (loss: 0.06294012814760208, acc: 0.9817444086074829)
[2025-02-13 04:01:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:53][root][INFO] - Training Epoch: 2/2, step 3156/7134 completed (loss: 0.06819568574428558, acc: 0.9806835055351257)
[2025-02-13 04:01:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:54][root][INFO] - Training Epoch: 2/2, step 3157/7134 completed (loss: 0.013343434780836105, acc: 0.9978494644165039)
[2025-02-13 04:01:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:54][root][INFO] - Training Epoch: 2/2, step 3158/7134 completed (loss: 0.013908633962273598, acc: 0.9934640526771545)
[2025-02-13 04:01:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:55][root][INFO] - Training Epoch: 2/2, step 3159/7134 completed (loss: 0.03525049611926079, acc: 0.9877451062202454)
[2025-02-13 04:01:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:55][root][INFO] - Training Epoch: 2/2, step 3160/7134 completed (loss: 0.007941956631839275, acc: 0.9972337484359741)
[2025-02-13 04:01:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:55][root][INFO] - Training Epoch: 2/2, step 3161/7134 completed (loss: 0.020957624539732933, acc: 0.9929971694946289)
[2025-02-13 04:01:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:56][root][INFO] - Training Epoch: 2/2, step 3162/7134 completed (loss: 0.01068216934800148, acc: 0.9970845580101013)
[2025-02-13 04:01:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:56][root][INFO] - Training Epoch: 2/2, step 3163/7134 completed (loss: 0.023239009082317352, acc: 0.994991660118103)
[2025-02-13 04:01:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:57][root][INFO] - Training Epoch: 2/2, step 3164/7134 completed (loss: 0.0515715517103672, acc: 0.9899497628211975)
[2025-02-13 04:01:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:57][root][INFO] - Training Epoch: 2/2, step 3165/7134 completed (loss: 0.023130469024181366, acc: 0.9933221936225891)
[2025-02-13 04:01:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:58][root][INFO] - Training Epoch: 2/2, step 3166/7134 completed (loss: 0.050703566521406174, acc: 0.9849624037742615)
[2025-02-13 04:01:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:58][root][INFO] - Training Epoch: 2/2, step 3167/7134 completed (loss: 0.07882276922464371, acc: 0.9750000238418579)
[2025-02-13 04:01:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:58][root][INFO] - Training Epoch: 2/2, step 3168/7134 completed (loss: 0.05252130329608917, acc: 0.9832317233085632)
[2025-02-13 04:01:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:59][root][INFO] - Training Epoch: 2/2, step 3169/7134 completed (loss: 0.04416710138320923, acc: 0.9836065769195557)
[2025-02-13 04:01:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:01:59][root][INFO] - Training Epoch: 2/2, step 3170/7134 completed (loss: 0.021982591599225998, acc: 0.9927272796630859)
[2025-02-13 04:01:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:00][root][INFO] - Training Epoch: 2/2, step 3171/7134 completed (loss: 0.033979449421167374, acc: 0.9895150661468506)
[2025-02-13 04:02:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:00][root][INFO] - Training Epoch: 2/2, step 3172/7134 completed (loss: 0.0038829301483929157, acc: 0.998084306716919)
[2025-02-13 04:02:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:01][root][INFO] - Training Epoch: 2/2, step 3173/7134 completed (loss: 0.02976900525391102, acc: 0.9846938848495483)
[2025-02-13 04:02:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:01][root][INFO] - Training Epoch: 2/2, step 3174/7134 completed (loss: 0.045010700821876526, acc: 0.9837278127670288)
[2025-02-13 04:02:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:02][root][INFO] - Training Epoch: 2/2, step 3175/7134 completed (loss: 0.04943137243390083, acc: 0.98591548204422)
[2025-02-13 04:02:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:02][root][INFO] - Training Epoch: 2/2, step 3176/7134 completed (loss: 0.0581735260784626, acc: 0.9864661693572998)
[2025-02-13 04:02:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:02][root][INFO] - Training Epoch: 2/2, step 3177/7134 completed (loss: 0.051582612097263336, acc: 0.9895833134651184)
[2025-02-13 04:02:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:03][root][INFO] - Training Epoch: 2/2, step 3178/7134 completed (loss: 0.012258537113666534, acc: 0.9983579516410828)
[2025-02-13 04:02:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:03][root][INFO] - Training Epoch: 2/2, step 3179/7134 completed (loss: 0.0645684003829956, acc: 0.9817578792572021)
[2025-02-13 04:02:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:04][root][INFO] - Training Epoch: 2/2, step 3180/7134 completed (loss: 0.02406090870499611, acc: 0.9942857027053833)
[2025-02-13 04:02:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:04][root][INFO] - Training Epoch: 2/2, step 3181/7134 completed (loss: 0.03066345863044262, acc: 0.9906396269798279)
[2025-02-13 04:02:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:05][root][INFO] - Training Epoch: 2/2, step 3182/7134 completed (loss: 0.030599381774663925, acc: 0.995529055595398)
[2025-02-13 04:02:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:05][root][INFO] - Training Epoch: 2/2, step 3183/7134 completed (loss: 0.03788997232913971, acc: 0.9915397763252258)
[2025-02-13 04:02:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:05][root][INFO] - Training Epoch: 2/2, step 3184/7134 completed (loss: 0.03658776730298996, acc: 0.9910714030265808)
[2025-02-13 04:02:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:06][root][INFO] - Training Epoch: 2/2, step 3185/7134 completed (loss: 0.006478555500507355, acc: 0.9985755085945129)
[2025-02-13 04:02:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:06][root][INFO] - Training Epoch: 2/2, step 3186/7134 completed (loss: 0.013847356662154198, acc: 0.995555579662323)
[2025-02-13 04:02:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:07][root][INFO] - Training Epoch: 2/2, step 3187/7134 completed (loss: 0.05361758545041084, acc: 0.9857594966888428)
[2025-02-13 04:02:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:07][root][INFO] - Training Epoch: 2/2, step 3188/7134 completed (loss: 0.029111923649907112, acc: 0.9935317039489746)
[2025-02-13 04:02:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:08][root][INFO] - Training Epoch: 2/2, step 3189/7134 completed (loss: 0.0380580797791481, acc: 0.9898697733879089)
[2025-02-13 04:02:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:08][root][INFO] - Training Epoch: 2/2, step 3190/7134 completed (loss: 0.027342619374394417, acc: 0.9936708807945251)
[2025-02-13 04:02:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:08][root][INFO] - Training Epoch: 2/2, step 3191/7134 completed (loss: 0.01845177821815014, acc: 0.9957143068313599)
[2025-02-13 04:02:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:09][root][INFO] - Training Epoch: 2/2, step 3192/7134 completed (loss: 0.037775009870529175, acc: 0.992438554763794)
[2025-02-13 04:02:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:09][root][INFO] - Training Epoch: 2/2, step 3193/7134 completed (loss: 0.04819891229271889, acc: 0.985358715057373)
[2025-02-13 04:02:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:10][root][INFO] - Training Epoch: 2/2, step 3194/7134 completed (loss: 0.03174449875950813, acc: 0.9885621070861816)
[2025-02-13 04:02:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:10][root][INFO] - Training Epoch: 2/2, step 3195/7134 completed (loss: 0.02505669742822647, acc: 0.9931389093399048)
[2025-02-13 04:02:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:11][root][INFO] - Training Epoch: 2/2, step 3196/7134 completed (loss: 0.011881302110850811, acc: 0.9962616562843323)
[2025-02-13 04:02:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:11][root][INFO] - Training Epoch: 2/2, step 3197/7134 completed (loss: 0.027604443952441216, acc: 0.9957746267318726)
[2025-02-13 04:02:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:11][root][INFO] - Training Epoch: 2/2, step 3198/7134 completed (loss: 0.01934272050857544, acc: 0.9939485788345337)
[2025-02-13 04:02:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:12][root][INFO] - Training Epoch: 2/2, step 3199/7134 completed (loss: 0.011728016659617424, acc: 0.997063159942627)
[2025-02-13 04:02:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:12][root][INFO] - Training Epoch: 2/2, step 3200/7134 completed (loss: 0.020473403856158257, acc: 0.994727611541748)
[2025-02-13 04:02:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:13][root][INFO] - Training Epoch: 2/2, step 3201/7134 completed (loss: 0.07153947651386261, acc: 0.9826464056968689)
[2025-02-13 04:02:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:13][root][INFO] - Training Epoch: 2/2, step 3202/7134 completed (loss: 0.021235758438706398, acc: 0.9958506226539612)
[2025-02-13 04:02:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:14][root][INFO] - Training Epoch: 2/2, step 3203/7134 completed (loss: 0.02401486225426197, acc: 0.9952606558799744)
[2025-02-13 04:02:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:14][root][INFO] - Training Epoch: 2/2, step 3204/7134 completed (loss: 0.057940367609262466, acc: 0.981203019618988)
[2025-02-13 04:02:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:14][root][INFO] - Training Epoch: 2/2, step 3205/7134 completed (loss: 0.0056752488017082214, acc: 1.0)
[2025-02-13 04:02:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:15][root][INFO] - Training Epoch: 2/2, step 3206/7134 completed (loss: 0.033622417598962784, acc: 0.9927641153335571)
[2025-02-13 04:02:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:15][root][INFO] - Training Epoch: 2/2, step 3207/7134 completed (loss: 0.019734378904104233, acc: 0.9892802238464355)
[2025-02-13 04:02:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:16][root][INFO] - Training Epoch: 2/2, step 3208/7134 completed (loss: 0.05316859483718872, acc: 0.9869281053543091)
[2025-02-13 04:02:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:16][root][INFO] - Training Epoch: 2/2, step 3209/7134 completed (loss: 0.013363104313611984, acc: 0.9955357313156128)
[2025-02-13 04:02:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:17][root][INFO] - Training Epoch: 2/2, step 3210/7134 completed (loss: 0.004470105282962322, acc: 1.0)
[2025-02-13 04:02:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:17][root][INFO] - Training Epoch: 2/2, step 3211/7134 completed (loss: 0.030690139159560204, acc: 0.9937629699707031)
[2025-02-13 04:02:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:17][root][INFO] - Training Epoch: 2/2, step 3212/7134 completed (loss: 0.005567910615354776, acc: 1.0)
[2025-02-13 04:02:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:18][root][INFO] - Training Epoch: 2/2, step 3213/7134 completed (loss: 0.025518054142594337, acc: 0.9928264021873474)
[2025-02-13 04:02:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:18][root][INFO] - Training Epoch: 2/2, step 3214/7134 completed (loss: 0.03343067318201065, acc: 0.9919678568840027)
[2025-02-13 04:02:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:19][root][INFO] - Training Epoch: 2/2, step 3215/7134 completed (loss: 0.025044236332178116, acc: 0.9957982897758484)
[2025-02-13 04:02:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:19][root][INFO] - Training Epoch: 2/2, step 3216/7134 completed (loss: 0.007060232572257519, acc: 0.9985673427581787)
[2025-02-13 04:02:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:20][root][INFO] - Training Epoch: 2/2, step 3217/7134 completed (loss: 0.011496939696371555, acc: 0.996129035949707)
[2025-02-13 04:02:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:20][root][INFO] - Training Epoch: 2/2, step 3218/7134 completed (loss: 0.05041103437542915, acc: 0.9827855825424194)
[2025-02-13 04:02:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:20][root][INFO] - Training Epoch: 2/2, step 3219/7134 completed (loss: 0.031157005578279495, acc: 0.9881831407546997)
[2025-02-13 04:02:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:21][root][INFO] - Training Epoch: 2/2, step 3220/7134 completed (loss: 0.027555925771594048, acc: 0.9959946870803833)
[2025-02-13 04:02:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:21][root][INFO] - Training Epoch: 2/2, step 3221/7134 completed (loss: 0.024900922551751137, acc: 0.9917808175086975)
[2025-02-13 04:02:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:22][root][INFO] - Training Epoch: 2/2, step 3222/7134 completed (loss: 0.03885653614997864, acc: 0.9899874925613403)
[2025-02-13 04:02:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:22][root][INFO] - Training Epoch: 2/2, step 3223/7134 completed (loss: 0.053664300590753555, acc: 0.9888888597488403)
[2025-02-13 04:02:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:23][root][INFO] - Training Epoch: 2/2, step 3224/7134 completed (loss: 0.045388903468847275, acc: 0.9913169145584106)
[2025-02-13 04:02:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:23][root][INFO] - Training Epoch: 2/2, step 3225/7134 completed (loss: 0.02098768949508667, acc: 0.991150438785553)
[2025-02-13 04:02:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:23][root][INFO] - Training Epoch: 2/2, step 3226/7134 completed (loss: 0.027559660375118256, acc: 0.9874326586723328)
[2025-02-13 04:02:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:24][root][INFO] - Training Epoch: 2/2, step 3227/7134 completed (loss: 0.04923113435506821, acc: 0.9857142567634583)
[2025-02-13 04:02:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:24][root][INFO] - Training Epoch: 2/2, step 3228/7134 completed (loss: 0.025193290784955025, acc: 0.9921156167984009)
[2025-02-13 04:02:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:25][root][INFO] - Training Epoch: 2/2, step 3229/7134 completed (loss: 0.02245124615728855, acc: 0.9926560521125793)
[2025-02-13 04:02:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:25][root][INFO] - Training Epoch: 2/2, step 3230/7134 completed (loss: 0.036943163722753525, acc: 0.9896449446678162)
[2025-02-13 04:02:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:26][root][INFO] - Training Epoch: 2/2, step 3231/7134 completed (loss: 0.040895506739616394, acc: 0.9908814430236816)
[2025-02-13 04:02:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:26][root][INFO] - Training Epoch: 2/2, step 3232/7134 completed (loss: 0.010341952554881573, acc: 0.9977553486824036)
[2025-02-13 04:02:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:27][root][INFO] - Training Epoch: 2/2, step 3233/7134 completed (loss: 0.049990035593509674, acc: 0.9921104311943054)
[2025-02-13 04:02:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:27][root][INFO] - Training Epoch: 2/2, step 3234/7134 completed (loss: 0.00969680491834879, acc: 0.998420238494873)
[2025-02-13 04:02:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:27][root][INFO] - Training Epoch: 2/2, step 3235/7134 completed (loss: 0.018231745809316635, acc: 0.9980952143669128)
[2025-02-13 04:02:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:28][root][INFO] - Training Epoch: 2/2, step 3236/7134 completed (loss: 0.060822319239377975, acc: 0.9867452383041382)
[2025-02-13 04:02:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:28][root][INFO] - Training Epoch: 2/2, step 3237/7134 completed (loss: 0.015023070387542248, acc: 0.9967793822288513)
[2025-02-13 04:02:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:29][root][INFO] - Training Epoch: 2/2, step 3238/7134 completed (loss: 0.0301299337297678, acc: 0.9907975196838379)
[2025-02-13 04:02:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:29][root][INFO] - Training Epoch: 2/2, step 3239/7134 completed (loss: 0.01086178608238697, acc: 0.9952681660652161)
[2025-02-13 04:02:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:29][root][INFO] - Training Epoch: 2/2, step 3240/7134 completed (loss: 0.019764980301260948, acc: 0.994020938873291)
[2025-02-13 04:02:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:30][root][INFO] - Training Epoch: 2/2, step 3241/7134 completed (loss: 0.01222154963761568, acc: 0.9940652847290039)
[2025-02-13 04:02:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:30][root][INFO] - Training Epoch: 2/2, step 3242/7134 completed (loss: 0.007198510691523552, acc: 0.9967319965362549)
[2025-02-13 04:02:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:31][root][INFO] - Training Epoch: 2/2, step 3243/7134 completed (loss: 0.018371352925896645, acc: 0.9945504069328308)
[2025-02-13 04:02:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:31][root][INFO] - Training Epoch: 2/2, step 3244/7134 completed (loss: 0.01924816519021988, acc: 0.9942611455917358)
[2025-02-13 04:02:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:32][root][INFO] - Training Epoch: 2/2, step 3245/7134 completed (loss: 0.008785106241703033, acc: 0.998711347579956)
[2025-02-13 04:02:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:32][root][INFO] - Training Epoch: 2/2, step 3246/7134 completed (loss: 0.03629390895366669, acc: 0.9884615540504456)
[2025-02-13 04:02:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:32][root][INFO] - Training Epoch: 2/2, step 3247/7134 completed (loss: 0.015609228052198887, acc: 0.9951377511024475)
[2025-02-13 04:02:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:33][root][INFO] - Training Epoch: 2/2, step 3248/7134 completed (loss: 0.012794350273907185, acc: 0.9967159032821655)
[2025-02-13 04:02:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:33][root][INFO] - Training Epoch: 2/2, step 3249/7134 completed (loss: 0.015477332286536694, acc: 0.9958391189575195)
[2025-02-13 04:02:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:34][root][INFO] - Training Epoch: 2/2, step 3250/7134 completed (loss: 0.030412493273615837, acc: 0.9841726422309875)
[2025-02-13 04:02:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:34][root][INFO] - Training Epoch: 2/2, step 3251/7134 completed (loss: 0.009082543663680553, acc: 0.9971469044685364)
[2025-02-13 04:02:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:35][root][INFO] - Training Epoch: 2/2, step 3252/7134 completed (loss: 0.0035596489906311035, acc: 0.99858158826828)
[2025-02-13 04:02:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:35][root][INFO] - Training Epoch: 2/2, step 3253/7134 completed (loss: 0.014116741716861725, acc: 0.995184600353241)
[2025-02-13 04:02:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:35][root][INFO] - Training Epoch: 2/2, step 3254/7134 completed (loss: 0.0062559498474001884, acc: 1.0)
[2025-02-13 04:02:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:36][root][INFO] - Training Epoch: 2/2, step 3255/7134 completed (loss: 0.007449795957654715, acc: 0.9946737885475159)
[2025-02-13 04:02:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:36][root][INFO] - Training Epoch: 2/2, step 3256/7134 completed (loss: 0.021054230630397797, acc: 0.9970370531082153)
[2025-02-13 04:02:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:37][root][INFO] - Training Epoch: 2/2, step 3257/7134 completed (loss: 0.017135895788669586, acc: 0.994452178478241)
[2025-02-13 04:02:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:37][root][INFO] - Training Epoch: 2/2, step 3258/7134 completed (loss: 0.020091690123081207, acc: 0.9923076629638672)
[2025-02-13 04:02:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:37][root][INFO] - Training Epoch: 2/2, step 3259/7134 completed (loss: 0.02017059549689293, acc: 0.9926470518112183)
[2025-02-13 04:02:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:38][root][INFO] - Training Epoch: 2/2, step 3260/7134 completed (loss: 0.06272397935390472, acc: 0.9881305694580078)
[2025-02-13 04:02:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:38][root][INFO] - Training Epoch: 2/2, step 3261/7134 completed (loss: 0.028206923976540565, acc: 0.9882550239562988)
[2025-02-13 04:02:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:39][root][INFO] - Training Epoch: 2/2, step 3262/7134 completed (loss: 0.015022206120193005, acc: 0.9927641153335571)
[2025-02-13 04:02:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:39][root][INFO] - Training Epoch: 2/2, step 3263/7134 completed (loss: 0.053147610276937485, acc: 0.9867841601371765)
[2025-02-13 04:02:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:40][root][INFO] - Training Epoch: 2/2, step 3264/7134 completed (loss: 0.012720166705548763, acc: 0.9944444298744202)
[2025-02-13 04:02:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:40][root][INFO] - Training Epoch: 2/2, step 3265/7134 completed (loss: 0.06790321320295334, acc: 0.9814814925193787)
[2025-02-13 04:02:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:40][root][INFO] - Training Epoch: 2/2, step 3266/7134 completed (loss: 0.03435918688774109, acc: 0.9909909963607788)
[2025-02-13 04:02:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:41][root][INFO] - Training Epoch: 2/2, step 3267/7134 completed (loss: 0.05363739654421806, acc: 0.982807993888855)
[2025-02-13 04:02:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:41][root][INFO] - Training Epoch: 2/2, step 3268/7134 completed (loss: 0.04780649021267891, acc: 0.9845916628837585)
[2025-02-13 04:02:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:42][root][INFO] - Training Epoch: 2/2, step 3269/7134 completed (loss: 0.04207775369286537, acc: 0.9886363744735718)
[2025-02-13 04:02:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:42][root][INFO] - Training Epoch: 2/2, step 3270/7134 completed (loss: 0.013910297304391861, acc: 0.9964115023612976)
[2025-02-13 04:02:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:43][root][INFO] - Training Epoch: 2/2, step 3271/7134 completed (loss: 0.005713543854653835, acc: 0.9966555237770081)
[2025-02-13 04:02:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:43][root][INFO] - Training Epoch: 2/2, step 3272/7134 completed (loss: 0.022020189091563225, acc: 0.9969465732574463)
[2025-02-13 04:02:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:43][root][INFO] - Training Epoch: 2/2, step 3273/7134 completed (loss: 0.010387726128101349, acc: 0.9961685538291931)
[2025-02-13 04:02:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:44][root][INFO] - Training Epoch: 2/2, step 3274/7134 completed (loss: 0.04242059588432312, acc: 0.9897210001945496)
[2025-02-13 04:02:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:44][root][INFO] - Training Epoch: 2/2, step 3275/7134 completed (loss: 0.11567535251379013, acc: 0.9766082167625427)
[2025-02-13 04:02:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:45][root][INFO] - Training Epoch: 2/2, step 3276/7134 completed (loss: 0.047458868473768234, acc: 0.9834710955619812)
[2025-02-13 04:02:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:45][root][INFO] - Training Epoch: 2/2, step 3277/7134 completed (loss: 0.026997089385986328, acc: 0.9908536672592163)
[2025-02-13 04:02:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:46][root][INFO] - Training Epoch: 2/2, step 3278/7134 completed (loss: 0.028449755162000656, acc: 0.9847036600112915)
[2025-02-13 04:02:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:46][root][INFO] - Training Epoch: 2/2, step 3279/7134 completed (loss: 0.031907614320516586, acc: 0.992546558380127)
[2025-02-13 04:02:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:46][root][INFO] - Training Epoch: 2/2, step 3280/7134 completed (loss: 0.09899850189685822, acc: 0.9730290174484253)
[2025-02-13 04:02:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:47][root][INFO] - Training Epoch: 2/2, step 3281/7134 completed (loss: 0.04372839629650116, acc: 0.9898550510406494)
[2025-02-13 04:02:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:47][root][INFO] - Training Epoch: 2/2, step 3282/7134 completed (loss: 0.01872948370873928, acc: 0.9889240264892578)
[2025-02-13 04:02:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:48][root][INFO] - Training Epoch: 2/2, step 3283/7134 completed (loss: 0.03754989802837372, acc: 0.985567033290863)
[2025-02-13 04:02:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:48][root][INFO] - Training Epoch: 2/2, step 3284/7134 completed (loss: 0.032194510102272034, acc: 0.9939117431640625)
[2025-02-13 04:02:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:48][root][INFO] - Training Epoch: 2/2, step 3285/7134 completed (loss: 0.022581759840250015, acc: 0.995356023311615)
[2025-02-13 04:02:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:49][root][INFO] - Training Epoch: 2/2, step 3286/7134 completed (loss: 0.024257121607661247, acc: 0.995488703250885)
[2025-02-13 04:02:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:49][root][INFO] - Training Epoch: 2/2, step 3287/7134 completed (loss: 0.07811493426561356, acc: 0.9802631735801697)
[2025-02-13 04:02:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:50][root][INFO] - Training Epoch: 2/2, step 3288/7134 completed (loss: 0.028209811076521873, acc: 0.9910179376602173)
[2025-02-13 04:02:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:50][root][INFO] - Training Epoch: 2/2, step 3289/7134 completed (loss: 0.05133737996220589, acc: 0.9859648942947388)
[2025-02-13 04:02:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:51][root][INFO] - Training Epoch: 2/2, step 3290/7134 completed (loss: 0.04797607287764549, acc: 0.9861111044883728)
[2025-02-13 04:02:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:51][root][INFO] - Training Epoch: 2/2, step 3291/7134 completed (loss: 0.03754386678338051, acc: 0.9821826219558716)
[2025-02-13 04:02:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:51][root][INFO] - Training Epoch: 2/2, step 3292/7134 completed (loss: 0.020051542669534683, acc: 0.9947090148925781)
[2025-02-13 04:02:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:52][root][INFO] - Training Epoch: 2/2, step 3293/7134 completed (loss: 0.028814513236284256, acc: 0.9882006049156189)
[2025-02-13 04:02:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:52][root][INFO] - Training Epoch: 2/2, step 3294/7134 completed (loss: 0.05548202618956566, acc: 0.9816232919692993)
[2025-02-13 04:02:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:53][root][INFO] - Training Epoch: 2/2, step 3295/7134 completed (loss: 0.013777134008705616, acc: 0.9948805570602417)
[2025-02-13 04:02:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:53][root][INFO] - Training Epoch: 2/2, step 3296/7134 completed (loss: 0.03727664053440094, acc: 0.9835466146469116)
[2025-02-13 04:02:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:53][root][INFO] - Training Epoch: 2/2, step 3297/7134 completed (loss: 0.030997417867183685, acc: 0.9906103014945984)
[2025-02-13 04:02:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:54][root][INFO] - Training Epoch: 2/2, step 3298/7134 completed (loss: 0.0714314728975296, acc: 0.9840255379676819)
[2025-02-13 04:02:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:54][root][INFO] - Training Epoch: 2/2, step 3299/7134 completed (loss: 0.03117479383945465, acc: 0.9882550239562988)
[2025-02-13 04:02:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:55][root][INFO] - Training Epoch: 2/2, step 3300/7134 completed (loss: 0.042744558304548264, acc: 0.9893048405647278)
[2025-02-13 04:02:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:55][root][INFO] - Training Epoch: 2/2, step 3301/7134 completed (loss: 0.024903295561671257, acc: 0.9958506226539612)
[2025-02-13 04:02:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:55][root][INFO] - Training Epoch: 2/2, step 3302/7134 completed (loss: 0.032551221549510956, acc: 0.9949495196342468)
[2025-02-13 04:02:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:56][root][INFO] - Training Epoch: 2/2, step 3303/7134 completed (loss: 0.021931568160653114, acc: 0.9933920502662659)
[2025-02-13 04:02:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:56][root][INFO] - Training Epoch: 2/2, step 3304/7134 completed (loss: 0.01694072224199772, acc: 0.9940333962440491)
[2025-02-13 04:02:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:57][root][INFO] - Training Epoch: 2/2, step 3305/7134 completed (loss: 0.024000946432352066, acc: 0.9909502267837524)
[2025-02-13 04:02:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:57][root][INFO] - Training Epoch: 2/2, step 3306/7134 completed (loss: 0.02083722874522209, acc: 0.990920901298523)
[2025-02-13 04:02:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:58][root][INFO] - Training Epoch: 2/2, step 3307/7134 completed (loss: 0.022759122774004936, acc: 0.9915397763252258)
[2025-02-13 04:02:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:58][root][INFO] - Training Epoch: 2/2, step 3308/7134 completed (loss: 0.027348220348358154, acc: 0.9913294911384583)
[2025-02-13 04:02:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:58][root][INFO] - Training Epoch: 2/2, step 3309/7134 completed (loss: 0.03544298931956291, acc: 0.991150438785553)
[2025-02-13 04:02:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:59][root][INFO] - Training Epoch: 2/2, step 3310/7134 completed (loss: 0.02804151549935341, acc: 0.9914712309837341)
[2025-02-13 04:02:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:02:59][root][INFO] - Training Epoch: 2/2, step 3311/7134 completed (loss: 0.024387894198298454, acc: 0.9892473220825195)
[2025-02-13 04:02:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:00][root][INFO] - Training Epoch: 2/2, step 3312/7134 completed (loss: 0.01468517817556858, acc: 0.9945454597473145)
[2025-02-13 04:03:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:00][root][INFO] - Training Epoch: 2/2, step 3313/7134 completed (loss: 0.009124362841248512, acc: 0.9976133704185486)
[2025-02-13 04:03:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:01][root][INFO] - Training Epoch: 2/2, step 3314/7134 completed (loss: 0.01878967508673668, acc: 0.9963768124580383)
[2025-02-13 04:03:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:01][root][INFO] - Training Epoch: 2/2, step 3315/7134 completed (loss: 0.0545264333486557, acc: 0.9850746393203735)
[2025-02-13 04:03:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:01][root][INFO] - Training Epoch: 2/2, step 3316/7134 completed (loss: 0.022385593503713608, acc: 0.9949579834938049)
[2025-02-13 04:03:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:02][root][INFO] - Training Epoch: 2/2, step 3317/7134 completed (loss: 0.053685758262872696, acc: 0.9903846383094788)
[2025-02-13 04:03:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:02][root][INFO] - Training Epoch: 2/2, step 3318/7134 completed (loss: 0.020221203565597534, acc: 0.9963898658752441)
[2025-02-13 04:03:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:03][root][INFO] - Training Epoch: 2/2, step 3319/7134 completed (loss: 0.02938411943614483, acc: 0.9918566942214966)
[2025-02-13 04:03:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:03][root][INFO] - Training Epoch: 2/2, step 3320/7134 completed (loss: 0.05392567440867424, acc: 0.9921466112136841)
[2025-02-13 04:03:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:04][root][INFO] - Training Epoch: 2/2, step 3321/7134 completed (loss: 0.06282270699739456, acc: 0.9834862351417542)
[2025-02-13 04:03:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:04][root][INFO] - Training Epoch: 2/2, step 3322/7134 completed (loss: 0.018852118402719498, acc: 0.9924924969673157)
[2025-02-13 04:03:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:05][root][INFO] - Training Epoch: 2/2, step 3323/7134 completed (loss: 0.022352837026119232, acc: 0.9903448224067688)
[2025-02-13 04:03:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:05][root][INFO] - Training Epoch: 2/2, step 3324/7134 completed (loss: 0.04807492345571518, acc: 0.987043559551239)
[2025-02-13 04:03:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:05][root][INFO] - Training Epoch: 2/2, step 3325/7134 completed (loss: 0.018682075664401054, acc: 0.9946236610412598)
[2025-02-13 04:03:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:06][root][INFO] - Training Epoch: 2/2, step 3326/7134 completed (loss: 0.05158187448978424, acc: 0.98740154504776)
[2025-02-13 04:03:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:06][root][INFO] - Training Epoch: 2/2, step 3327/7134 completed (loss: 0.028773752972483635, acc: 0.9891745448112488)
[2025-02-13 04:03:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:07][root][INFO] - Training Epoch: 2/2, step 3328/7134 completed (loss: 0.03495099022984505, acc: 0.9924812316894531)
[2025-02-13 04:03:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:07][root][INFO] - Training Epoch: 2/2, step 3329/7134 completed (loss: 0.016439739614725113, acc: 0.9963459372520447)
[2025-02-13 04:03:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:08][root][INFO] - Training Epoch: 2/2, step 3330/7134 completed (loss: 0.04468284547328949, acc: 0.9884792566299438)
[2025-02-13 04:03:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:08][root][INFO] - Training Epoch: 2/2, step 3331/7134 completed (loss: 0.047354377806186676, acc: 0.9869186282157898)
[2025-02-13 04:03:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:09][root][INFO] - Training Epoch: 2/2, step 3332/7134 completed (loss: 0.09857206791639328, acc: 0.9757785201072693)
[2025-02-13 04:03:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:09][root][INFO] - Training Epoch: 2/2, step 3333/7134 completed (loss: 0.04195793345570564, acc: 0.9959016442298889)
[2025-02-13 04:03:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:09][root][INFO] - Training Epoch: 2/2, step 3334/7134 completed (loss: 0.039988819509744644, acc: 0.9891892075538635)
[2025-02-13 04:03:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:10][root][INFO] - Training Epoch: 2/2, step 3335/7134 completed (loss: 0.030454376712441444, acc: 0.9920886158943176)
[2025-02-13 04:03:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:10][root][INFO] - Training Epoch: 2/2, step 3336/7134 completed (loss: 0.0071970997378230095, acc: 0.9985422492027283)
[2025-02-13 04:03:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:11][root][INFO] - Training Epoch: 2/2, step 3337/7134 completed (loss: 0.008397850207984447, acc: 0.9960552453994751)
[2025-02-13 04:03:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:11][root][INFO] - Training Epoch: 2/2, step 3338/7134 completed (loss: 0.008783177472651005, acc: 0.9975154995918274)
[2025-02-13 04:03:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:12][root][INFO] - Training Epoch: 2/2, step 3339/7134 completed (loss: 0.040809452533721924, acc: 0.9868735074996948)
[2025-02-13 04:03:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:12][root][INFO] - Training Epoch: 2/2, step 3340/7134 completed (loss: 0.07536397874355316, acc: 0.9773755669593811)
[2025-02-13 04:03:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:13][root][INFO] - Training Epoch: 2/2, step 3341/7134 completed (loss: 0.03988118842244148, acc: 0.9866666793823242)
[2025-02-13 04:03:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:13][root][INFO] - Training Epoch: 2/2, step 3342/7134 completed (loss: 0.021195925772190094, acc: 0.9949324131011963)
[2025-02-13 04:03:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:13][root][INFO] - Training Epoch: 2/2, step 3343/7134 completed (loss: 0.023331480100750923, acc: 0.9919354915618896)
[2025-02-13 04:03:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:14][root][INFO] - Training Epoch: 2/2, step 3344/7134 completed (loss: 0.015165446326136589, acc: 0.994854211807251)
[2025-02-13 04:03:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:14][root][INFO] - Training Epoch: 2/2, step 3345/7134 completed (loss: 0.04938298836350441, acc: 0.9918962717056274)
[2025-02-13 04:03:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:15][root][INFO] - Training Epoch: 2/2, step 3346/7134 completed (loss: 0.023164402693510056, acc: 0.9912280440330505)
[2025-02-13 04:03:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:15][root][INFO] - Training Epoch: 2/2, step 3347/7134 completed (loss: 0.05870024487376213, acc: 0.9804560542106628)
[2025-02-13 04:03:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:16][root][INFO] - Training Epoch: 2/2, step 3348/7134 completed (loss: 0.029316838830709457, acc: 0.995555579662323)
[2025-02-13 04:03:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:16][root][INFO] - Training Epoch: 2/2, step 3349/7134 completed (loss: 0.013353491201996803, acc: 0.9964726567268372)
[2025-02-13 04:03:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:17][root][INFO] - Training Epoch: 2/2, step 3350/7134 completed (loss: 0.021043704822659492, acc: 0.9939024448394775)
[2025-02-13 04:03:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:17][root][INFO] - Training Epoch: 2/2, step 3351/7134 completed (loss: 0.03812733665108681, acc: 0.9946949481964111)
[2025-02-13 04:03:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:17][root][INFO] - Training Epoch: 2/2, step 3352/7134 completed (loss: 0.015044238418340683, acc: 0.9965217113494873)
[2025-02-13 04:03:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:18][root][INFO] - Training Epoch: 2/2, step 3353/7134 completed (loss: 0.039345864206552505, acc: 0.990138053894043)
[2025-02-13 04:03:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:18][root][INFO] - Training Epoch: 2/2, step 3354/7134 completed (loss: 0.023498859256505966, acc: 0.9922480583190918)
[2025-02-13 04:03:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:19][root][INFO] - Training Epoch: 2/2, step 3355/7134 completed (loss: 0.009164782240986824, acc: 0.995768666267395)
[2025-02-13 04:03:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:19][root][INFO] - Training Epoch: 2/2, step 3356/7134 completed (loss: 0.004404956940561533, acc: 1.0)
[2025-02-13 04:03:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:20][root][INFO] - Training Epoch: 2/2, step 3357/7134 completed (loss: 0.022138798609375954, acc: 0.9927849769592285)
[2025-02-13 04:03:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:20][root][INFO] - Training Epoch: 2/2, step 3358/7134 completed (loss: 0.029045917093753815, acc: 0.9896193742752075)
[2025-02-13 04:03:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:20][root][INFO] - Training Epoch: 2/2, step 3359/7134 completed (loss: 0.011588645167648792, acc: 0.9967637658119202)
[2025-02-13 04:03:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:21][root][INFO] - Training Epoch: 2/2, step 3360/7134 completed (loss: 0.06526560336351395, acc: 0.9828326106071472)
[2025-02-13 04:03:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:21][root][INFO] - Training Epoch: 2/2, step 3361/7134 completed (loss: 0.030799398198723793, acc: 0.991051435470581)
[2025-02-13 04:03:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:22][root][INFO] - Training Epoch: 2/2, step 3362/7134 completed (loss: 0.035780999809503555, acc: 0.9909297227859497)
[2025-02-13 04:03:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:22][root][INFO] - Training Epoch: 2/2, step 3363/7134 completed (loss: 0.006297146435827017, acc: 1.0)
[2025-02-13 04:03:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:22][root][INFO] - Training Epoch: 2/2, step 3364/7134 completed (loss: 0.07350147515535355, acc: 0.9800307154655457)
[2025-02-13 04:03:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:23][root][INFO] - Training Epoch: 2/2, step 3365/7134 completed (loss: 0.05258282274007797, acc: 0.983132541179657)
[2025-02-13 04:03:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:23][root][INFO] - Training Epoch: 2/2, step 3366/7134 completed (loss: 0.016181163489818573, acc: 0.9932735562324524)
[2025-02-13 04:03:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:24][root][INFO] - Training Epoch: 2/2, step 3367/7134 completed (loss: 0.022498052567243576, acc: 0.994490385055542)
[2025-02-13 04:03:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:24][root][INFO] - Training Epoch: 2/2, step 3368/7134 completed (loss: 0.03257211297750473, acc: 0.9906396269798279)
[2025-02-13 04:03:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:25][root][INFO] - Training Epoch: 2/2, step 3369/7134 completed (loss: 0.04357941076159477, acc: 0.9870503544807434)
[2025-02-13 04:03:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:25][root][INFO] - Training Epoch: 2/2, step 3370/7134 completed (loss: 0.018417024984955788, acc: 0.995708167552948)
[2025-02-13 04:03:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:26][root][INFO] - Training Epoch: 2/2, step 3371/7134 completed (loss: 0.007982298731803894, acc: 0.9983552694320679)
[2025-02-13 04:03:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:26][root][INFO] - Training Epoch: 2/2, step 3372/7134 completed (loss: 0.017909735441207886, acc: 0.9968652129173279)
[2025-02-13 04:03:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:26][root][INFO] - Training Epoch: 2/2, step 3373/7134 completed (loss: 0.008292230777442455, acc: 0.9984567761421204)
[2025-02-13 04:03:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:27][root][INFO] - Training Epoch: 2/2, step 3374/7134 completed (loss: 0.00852862000465393, acc: 0.9986072182655334)
[2025-02-13 04:03:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:27][root][INFO] - Training Epoch: 2/2, step 3375/7134 completed (loss: 0.013565856963396072, acc: 0.9971056580543518)
[2025-02-13 04:03:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:28][root][INFO] - Training Epoch: 2/2, step 3376/7134 completed (loss: 0.011426758952438831, acc: 0.9987228512763977)
[2025-02-13 04:03:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:28][root][INFO] - Training Epoch: 2/2, step 3377/7134 completed (loss: 0.01913667656481266, acc: 0.9976162314414978)
[2025-02-13 04:03:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:29][root][INFO] - Training Epoch: 2/2, step 3378/7134 completed (loss: 0.004169258289039135, acc: 0.9986979365348816)
[2025-02-13 04:03:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:29][root][INFO] - Training Epoch: 2/2, step 3379/7134 completed (loss: 0.011797452345490456, acc: 0.9984962344169617)
[2025-02-13 04:03:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:30][root][INFO] - Training Epoch: 2/2, step 3380/7134 completed (loss: 0.012742927297949791, acc: 0.9953846335411072)
[2025-02-13 04:03:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:30][root][INFO] - Training Epoch: 2/2, step 3381/7134 completed (loss: 0.03902456536889076, acc: 0.9883720874786377)
[2025-02-13 04:03:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:30][root][INFO] - Training Epoch: 2/2, step 3382/7134 completed (loss: 0.005593182053416967, acc: 0.9965397715568542)
[2025-02-13 04:03:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:31][root][INFO] - Training Epoch: 2/2, step 3383/7134 completed (loss: 0.018299641087651253, acc: 0.9956958293914795)
[2025-02-13 04:03:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:31][root][INFO] - Training Epoch: 2/2, step 3384/7134 completed (loss: 0.02314329333603382, acc: 0.9938837885856628)
[2025-02-13 04:03:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:32][root][INFO] - Training Epoch: 2/2, step 3385/7134 completed (loss: 0.03586544468998909, acc: 0.9928057789802551)
[2025-02-13 04:03:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:32][root][INFO] - Training Epoch: 2/2, step 3386/7134 completed (loss: 0.0139158321544528, acc: 0.9957627058029175)
[2025-02-13 04:03:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:33][root][INFO] - Training Epoch: 2/2, step 3387/7134 completed (loss: 0.04012741521000862, acc: 0.9931972622871399)
[2025-02-13 04:03:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:33][root][INFO] - Training Epoch: 2/2, step 3388/7134 completed (loss: 0.02992667257785797, acc: 0.9891501069068909)
[2025-02-13 04:03:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:34][root][INFO] - Training Epoch: 2/2, step 3389/7134 completed (loss: 0.019481725990772247, acc: 0.9973649382591248)
[2025-02-13 04:03:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:34][root][INFO] - Training Epoch: 2/2, step 3390/7134 completed (loss: 0.010562881827354431, acc: 0.9974093437194824)
[2025-02-13 04:03:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:35][root][INFO] - Training Epoch: 2/2, step 3391/7134 completed (loss: 0.002142735291272402, acc: 1.0)
[2025-02-13 04:03:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:35][root][INFO] - Training Epoch: 2/2, step 3392/7134 completed (loss: 0.012673486024141312, acc: 0.9968553185462952)
[2025-02-13 04:03:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:35][root][INFO] - Training Epoch: 2/2, step 3393/7134 completed (loss: 0.016747737303376198, acc: 0.9938271641731262)
[2025-02-13 04:03:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:36][root][INFO] - Training Epoch: 2/2, step 3394/7134 completed (loss: 0.023394325748085976, acc: 0.9905063509941101)
[2025-02-13 04:03:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:36][root][INFO] - Training Epoch: 2/2, step 3395/7134 completed (loss: 0.049390215426683426, acc: 0.9788960814476013)
[2025-02-13 04:03:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:37][root][INFO] - Training Epoch: 2/2, step 3396/7134 completed (loss: 0.025740738958120346, acc: 0.9940119981765747)
[2025-02-13 04:03:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:37][root][INFO] - Training Epoch: 2/2, step 3397/7134 completed (loss: 0.01882912591099739, acc: 0.9963325262069702)
[2025-02-13 04:03:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:38][root][INFO] - Training Epoch: 2/2, step 3398/7134 completed (loss: 0.021925145760178566, acc: 0.9946236610412598)
[2025-02-13 04:03:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:38][root][INFO] - Training Epoch: 2/2, step 3399/7134 completed (loss: 0.0781698003411293, acc: 0.9724264740943909)
[2025-02-13 04:03:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:38][root][INFO] - Training Epoch: 2/2, step 3400/7134 completed (loss: 0.025488179177045822, acc: 0.9918864369392395)
[2025-02-13 04:03:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:39][root][INFO] - Training Epoch: 2/2, step 3401/7134 completed (loss: 0.027661927044391632, acc: 0.9895104765892029)
[2025-02-13 04:03:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:39][root][INFO] - Training Epoch: 2/2, step 3402/7134 completed (loss: 0.0413656160235405, acc: 0.9934123754501343)
[2025-02-13 04:03:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:40][root][INFO] - Training Epoch: 2/2, step 3403/7134 completed (loss: 0.07690875977277756, acc: 0.98046875)
[2025-02-13 04:03:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:40][root][INFO] - Training Epoch: 2/2, step 3404/7134 completed (loss: 0.01650654338300228, acc: 0.9955157041549683)
[2025-02-13 04:03:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:41][root][INFO] - Training Epoch: 2/2, step 3405/7134 completed (loss: 0.06484760344028473, acc: 0.9856114983558655)
[2025-02-13 04:03:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:41][root][INFO] - Training Epoch: 2/2, step 3406/7134 completed (loss: 0.034287966787815094, acc: 0.9884020686149597)
[2025-02-13 04:03:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:42][root][INFO] - Training Epoch: 2/2, step 3407/7134 completed (loss: 0.032099101692438126, acc: 0.9906396269798279)
[2025-02-13 04:03:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:42][root][INFO] - Training Epoch: 2/2, step 3408/7134 completed (loss: 0.022070473060011864, acc: 0.9907833933830261)
[2025-02-13 04:03:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:43][root][INFO] - Training Epoch: 2/2, step 3409/7134 completed (loss: 0.019952254369854927, acc: 0.9959404468536377)
[2025-02-13 04:03:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:43][root][INFO] - Training Epoch: 2/2, step 3410/7134 completed (loss: 0.018316755071282387, acc: 0.997245192527771)
[2025-02-13 04:03:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:43][root][INFO] - Training Epoch: 2/2, step 3411/7134 completed (loss: 0.03248714283108711, acc: 0.9935317039489746)
[2025-02-13 04:03:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:44][root][INFO] - Training Epoch: 2/2, step 3412/7134 completed (loss: 0.031660497188568115, acc: 0.9905481934547424)
[2025-02-13 04:03:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:44][root][INFO] - Training Epoch: 2/2, step 3413/7134 completed (loss: 0.013933029025793076, acc: 0.9940000176429749)
[2025-02-13 04:03:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:45][root][INFO] - Training Epoch: 2/2, step 3414/7134 completed (loss: 0.02758297696709633, acc: 0.9866443872451782)
[2025-02-13 04:03:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:45][root][INFO] - Training Epoch: 2/2, step 3415/7134 completed (loss: 0.0171060673892498, acc: 0.9935897588729858)
[2025-02-13 04:03:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:46][root][INFO] - Training Epoch: 2/2, step 3416/7134 completed (loss: 0.02886473946273327, acc: 0.9879952073097229)
[2025-02-13 04:03:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:46][root][INFO] - Training Epoch: 2/2, step 3417/7134 completed (loss: 0.01912195421755314, acc: 0.9959514141082764)
[2025-02-13 04:03:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:47][root][INFO] - Training Epoch: 2/2, step 3418/7134 completed (loss: 0.01649720035493374, acc: 0.9970545172691345)
[2025-02-13 04:03:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:47][root][INFO] - Training Epoch: 2/2, step 3419/7134 completed (loss: 0.005971251521259546, acc: 0.9971181750297546)
[2025-02-13 04:03:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:47][root][INFO] - Training Epoch: 2/2, step 3420/7134 completed (loss: 0.006372265517711639, acc: 0.998420238494873)
[2025-02-13 04:03:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:48][root][INFO] - Training Epoch: 2/2, step 3421/7134 completed (loss: 0.019093379378318787, acc: 0.996889591217041)
[2025-02-13 04:03:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:48][root][INFO] - Training Epoch: 2/2, step 3422/7134 completed (loss: 0.005755830556154251, acc: 0.996497392654419)
[2025-02-13 04:03:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:49][root][INFO] - Training Epoch: 2/2, step 3423/7134 completed (loss: 0.014824170619249344, acc: 0.9954545497894287)
[2025-02-13 04:03:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:49][root][INFO] - Training Epoch: 2/2, step 3424/7134 completed (loss: 0.04479396343231201, acc: 0.9819276928901672)
[2025-02-13 04:03:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:50][root][INFO] - Training Epoch: 2/2, step 3425/7134 completed (loss: 0.017221082001924515, acc: 0.9948717951774597)
[2025-02-13 04:03:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:50][root][INFO] - Training Epoch: 2/2, step 3426/7134 completed (loss: 0.03404248133301735, acc: 0.9972413778305054)
[2025-02-13 04:03:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:50][root][INFO] - Training Epoch: 2/2, step 3427/7134 completed (loss: 0.02477772906422615, acc: 0.9968404173851013)
[2025-02-13 04:03:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:51][root][INFO] - Training Epoch: 2/2, step 3428/7134 completed (loss: 0.022200457751750946, acc: 0.9957020282745361)
[2025-02-13 04:03:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:51][root][INFO] - Training Epoch: 2/2, step 3429/7134 completed (loss: 0.0276967640966177, acc: 0.9870550036430359)
[2025-02-13 04:03:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:52][root][INFO] - Training Epoch: 2/2, step 3430/7134 completed (loss: 0.055672891438007355, acc: 0.9886547923088074)
[2025-02-13 04:03:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:52][root][INFO] - Training Epoch: 2/2, step 3431/7134 completed (loss: 0.025021931156516075, acc: 0.9949066042900085)
[2025-02-13 04:03:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:53][root][INFO] - Training Epoch: 2/2, step 3432/7134 completed (loss: 0.022157177329063416, acc: 0.9945130348205566)
[2025-02-13 04:03:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:53][root][INFO] - Training Epoch: 2/2, step 3433/7134 completed (loss: 0.052075982093811035, acc: 0.9852941036224365)
[2025-02-13 04:03:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:54][root][INFO] - Training Epoch: 2/2, step 3434/7134 completed (loss: 0.012864543125033379, acc: 0.996820330619812)
[2025-02-13 04:03:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:54][root][INFO] - Training Epoch: 2/2, step 3435/7134 completed (loss: 0.03840411454439163, acc: 0.9871382713317871)
[2025-02-13 04:03:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:54][root][INFO] - Training Epoch: 2/2, step 3436/7134 completed (loss: 0.02340741828083992, acc: 0.9918699264526367)
[2025-02-13 04:03:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:55][root][INFO] - Training Epoch: 2/2, step 3437/7134 completed (loss: 0.010754363611340523, acc: 0.99609375)
[2025-02-13 04:03:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:55][root][INFO] - Training Epoch: 2/2, step 3438/7134 completed (loss: 0.028268849477171898, acc: 0.9939393997192383)
[2025-02-13 04:03:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:56][root][INFO] - Training Epoch: 2/2, step 3439/7134 completed (loss: 0.028267716988921165, acc: 0.994575023651123)
[2025-02-13 04:03:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:56][root][INFO] - Training Epoch: 2/2, step 3440/7134 completed (loss: 0.048251423984766006, acc: 0.9773662686347961)
[2025-02-13 04:03:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:57][root][INFO] - Training Epoch: 2/2, step 3441/7134 completed (loss: 0.03445003926753998, acc: 0.9884393215179443)
[2025-02-13 04:03:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:57][root][INFO] - Training Epoch: 2/2, step 3442/7134 completed (loss: 0.015599166974425316, acc: 0.9963503479957581)
[2025-02-13 04:03:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:57][root][INFO] - Training Epoch: 2/2, step 3443/7134 completed (loss: 0.04931608960032463, acc: 0.9887640476226807)
[2025-02-13 04:03:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:58][root][INFO] - Training Epoch: 2/2, step 3444/7134 completed (loss: 0.05691465362906456, acc: 0.9905063509941101)
[2025-02-13 04:03:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:58][root][INFO] - Training Epoch: 2/2, step 3445/7134 completed (loss: 0.07728270441293716, acc: 0.9827957153320312)
[2025-02-13 04:03:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:59][root][INFO] - Training Epoch: 2/2, step 3446/7134 completed (loss: 0.07292722910642624, acc: 0.9832776188850403)
[2025-02-13 04:03:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:59][root][INFO] - Training Epoch: 2/2, step 3447/7134 completed (loss: 0.04445290192961693, acc: 0.9853420257568359)
[2025-02-13 04:03:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:03:59][root][INFO] - Training Epoch: 2/2, step 3448/7134 completed (loss: 0.021517783403396606, acc: 0.9909909963607788)
[2025-02-13 04:04:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:00][root][INFO] - Training Epoch: 2/2, step 3449/7134 completed (loss: 0.03344627097249031, acc: 0.9923469424247742)
[2025-02-13 04:04:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:00][root][INFO] - Training Epoch: 2/2, step 3450/7134 completed (loss: 0.0283711738884449, acc: 0.9898734092712402)
[2025-02-13 04:04:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:01][root][INFO] - Training Epoch: 2/2, step 3451/7134 completed (loss: 0.02039378322660923, acc: 0.9946996569633484)
[2025-02-13 04:04:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:01][root][INFO] - Training Epoch: 2/2, step 3452/7134 completed (loss: 0.028704682365059853, acc: 0.9913232326507568)
[2025-02-13 04:04:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:02][root][INFO] - Training Epoch: 2/2, step 3453/7134 completed (loss: 0.03706026077270508, acc: 0.9932088255882263)
[2025-02-13 04:04:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:02][root][INFO] - Training Epoch: 2/2, step 3454/7134 completed (loss: 0.02593427337706089, acc: 0.9915373921394348)
[2025-02-13 04:04:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:02][root][INFO] - Training Epoch: 2/2, step 3455/7134 completed (loss: 0.0066750808618962765, acc: 1.0)
[2025-02-13 04:04:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:03][root][INFO] - Training Epoch: 2/2, step 3456/7134 completed (loss: 0.022008683532476425, acc: 0.9952940940856934)
[2025-02-13 04:04:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:03][root][INFO] - Training Epoch: 2/2, step 3457/7134 completed (loss: 0.009807802736759186, acc: 0.998305082321167)
[2025-02-13 04:04:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:04][root][INFO] - Training Epoch: 2/2, step 3458/7134 completed (loss: 0.042154405266046524, acc: 0.9905277490615845)
[2025-02-13 04:04:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:04][root][INFO] - Training Epoch: 2/2, step 3459/7134 completed (loss: 0.030524780973792076, acc: 0.990867555141449)
[2025-02-13 04:04:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:05][root][INFO] - Training Epoch: 2/2, step 3460/7134 completed (loss: 0.10194969177246094, acc: 0.9711538553237915)
[2025-02-13 04:04:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:05][root][INFO] - Training Epoch: 2/2, step 3461/7134 completed (loss: 0.17680583894252777, acc: 0.9632183909416199)
[2025-02-13 04:04:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:05][root][INFO] - Training Epoch: 2/2, step 3462/7134 completed (loss: 0.07743588089942932, acc: 0.9827315807342529)
[2025-02-13 04:04:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:06][root][INFO] - Training Epoch: 2/2, step 3463/7134 completed (loss: 0.014727660454809666, acc: 0.993966817855835)
[2025-02-13 04:04:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:06][root][INFO] - Training Epoch: 2/2, step 3464/7134 completed (loss: 0.03658871725201607, acc: 0.9879356622695923)
[2025-02-13 04:04:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:07][root][INFO] - Training Epoch: 2/2, step 3465/7134 completed (loss: 0.05633397027850151, acc: 0.9837067127227783)
[2025-02-13 04:04:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:07][root][INFO] - Training Epoch: 2/2, step 3466/7134 completed (loss: 0.013127838261425495, acc: 0.996830403804779)
[2025-02-13 04:04:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:08][root][INFO] - Training Epoch: 2/2, step 3467/7134 completed (loss: 0.024457799270749092, acc: 0.987261176109314)
[2025-02-13 04:04:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:08][root][INFO] - Training Epoch: 2/2, step 3468/7134 completed (loss: 0.008905196562409401, acc: 0.9971056580543518)
[2025-02-13 04:04:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:08][root][INFO] - Training Epoch: 2/2, step 3469/7134 completed (loss: 0.03134186565876007, acc: 0.9886792302131653)
[2025-02-13 04:04:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:09][root][INFO] - Training Epoch: 2/2, step 3470/7134 completed (loss: 0.008835166692733765, acc: 1.0)
[2025-02-13 04:04:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:09][root][INFO] - Training Epoch: 2/2, step 3471/7134 completed (loss: 0.05794810876250267, acc: 0.9917126893997192)
[2025-02-13 04:04:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:10][root][INFO] - Training Epoch: 2/2, step 3472/7134 completed (loss: 0.010870522819459438, acc: 0.997019350528717)
[2025-02-13 04:04:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:10][root][INFO] - Training Epoch: 2/2, step 3473/7134 completed (loss: 0.020752187818288803, acc: 0.9933775067329407)
[2025-02-13 04:04:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:11][root][INFO] - Training Epoch: 2/2, step 3474/7134 completed (loss: 0.05123560503125191, acc: 0.9893993139266968)
[2025-02-13 04:04:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:11][root][INFO] - Training Epoch: 2/2, step 3475/7134 completed (loss: 0.0231802798807621, acc: 0.9946714043617249)
[2025-02-13 04:04:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:12][root][INFO] - Training Epoch: 2/2, step 3476/7134 completed (loss: 0.037393953651189804, acc: 0.9845890402793884)
[2025-02-13 04:04:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:12][root][INFO] - Training Epoch: 2/2, step 3477/7134 completed (loss: 0.03612319380044937, acc: 0.9917762875556946)
[2025-02-13 04:04:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:12][root][INFO] - Training Epoch: 2/2, step 3478/7134 completed (loss: 0.01584036834537983, acc: 0.9955089688301086)
[2025-02-13 04:04:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:13][root][INFO] - Training Epoch: 2/2, step 3479/7134 completed (loss: 0.0026177712716162205, acc: 1.0)
[2025-02-13 04:04:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:13][root][INFO] - Training Epoch: 2/2, step 3480/7134 completed (loss: 0.020595984533429146, acc: 0.9959127902984619)
[2025-02-13 04:04:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:14][root][INFO] - Training Epoch: 2/2, step 3481/7134 completed (loss: 0.033282361924648285, acc: 0.9964538812637329)
[2025-02-13 04:04:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:14][root][INFO] - Training Epoch: 2/2, step 3482/7134 completed (loss: 0.005550317000597715, acc: 0.9982014298439026)
[2025-02-13 04:04:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:15][root][INFO] - Training Epoch: 2/2, step 3483/7134 completed (loss: 0.003766972105950117, acc: 0.9984591603279114)
[2025-02-13 04:04:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:15][root][INFO] - Training Epoch: 2/2, step 3484/7134 completed (loss: 0.01839541457593441, acc: 0.9941002726554871)
[2025-02-13 04:04:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:15][root][INFO] - Training Epoch: 2/2, step 3485/7134 completed (loss: 0.019327519461512566, acc: 0.9942280054092407)
[2025-02-13 04:04:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:16][root][INFO] - Training Epoch: 2/2, step 3486/7134 completed (loss: 0.009496866725385189, acc: 0.9982014298439026)
[2025-02-13 04:04:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:16][root][INFO] - Training Epoch: 2/2, step 3487/7134 completed (loss: 0.026255382224917412, acc: 0.9896907210350037)
[2025-02-13 04:04:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:17][root][INFO] - Training Epoch: 2/2, step 3488/7134 completed (loss: 0.017125437036156654, acc: 0.9934210777282715)
[2025-02-13 04:04:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:17][root][INFO] - Training Epoch: 2/2, step 3489/7134 completed (loss: 0.05009634420275688, acc: 0.988950252532959)
[2025-02-13 04:04:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:18][root][INFO] - Training Epoch: 2/2, step 3490/7134 completed (loss: 0.05758090689778328, acc: 0.9881831407546997)
[2025-02-13 04:04:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:18][root][INFO] - Training Epoch: 2/2, step 3491/7134 completed (loss: 0.038536302745342255, acc: 0.9914893507957458)
[2025-02-13 04:04:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:18][root][INFO] - Training Epoch: 2/2, step 3492/7134 completed (loss: 0.022022301331162453, acc: 0.9897330403327942)
[2025-02-13 04:04:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:19][root][INFO] - Training Epoch: 2/2, step 3493/7134 completed (loss: 0.010109035298228264, acc: 0.9957982897758484)
[2025-02-13 04:04:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:19][root][INFO] - Training Epoch: 2/2, step 3494/7134 completed (loss: 0.018061529844999313, acc: 0.9948275685310364)
[2025-02-13 04:04:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:20][root][INFO] - Training Epoch: 2/2, step 3495/7134 completed (loss: 0.007867665030062199, acc: 1.0)
[2025-02-13 04:04:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:20][root][INFO] - Training Epoch: 2/2, step 3496/7134 completed (loss: 0.015507812611758709, acc: 0.9909909963607788)
[2025-02-13 04:04:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:21][root][INFO] - Training Epoch: 2/2, step 3497/7134 completed (loss: 0.019026482477784157, acc: 0.997032642364502)
[2025-02-13 04:04:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:21][root][INFO] - Training Epoch: 2/2, step 3498/7134 completed (loss: 0.02574775367975235, acc: 0.9878048896789551)
[2025-02-13 04:04:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:21][root][INFO] - Training Epoch: 2/2, step 3499/7134 completed (loss: 0.01495380513370037, acc: 0.9967585206031799)
[2025-02-13 04:04:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:22][root][INFO] - Training Epoch: 2/2, step 3500/7134 completed (loss: 0.03242695331573486, acc: 0.9867841601371765)
[2025-02-13 04:04:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:22][root][INFO] - Training Epoch: 2/2, step 3501/7134 completed (loss: 0.0874108225107193, acc: 0.9738430380821228)
[2025-02-13 04:04:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:23][root][INFO] - Training Epoch: 2/2, step 3502/7134 completed (loss: 0.06908293813467026, acc: 0.9824561476707458)
[2025-02-13 04:04:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:23][root][INFO] - Training Epoch: 2/2, step 3503/7134 completed (loss: 0.028372053056955338, acc: 0.9948006868362427)
[2025-02-13 04:04:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:24][root][INFO] - Training Epoch: 2/2, step 3504/7134 completed (loss: 0.003526131622493267, acc: 0.9984917044639587)
[2025-02-13 04:04:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:24][root][INFO] - Training Epoch: 2/2, step 3505/7134 completed (loss: 0.017191626131534576, acc: 0.9946332573890686)
[2025-02-13 04:04:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:24][root][INFO] - Training Epoch: 2/2, step 3506/7134 completed (loss: 0.02519921213388443, acc: 0.9943289160728455)
[2025-02-13 04:04:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:25][root][INFO] - Training Epoch: 2/2, step 3507/7134 completed (loss: 0.006185173988342285, acc: 0.998344361782074)
[2025-02-13 04:04:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:25][root][INFO] - Training Epoch: 2/2, step 3508/7134 completed (loss: 0.026391342282295227, acc: 0.9941520690917969)
[2025-02-13 04:04:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:26][root][INFO] - Training Epoch: 2/2, step 3509/7134 completed (loss: 0.013867498375475407, acc: 0.9958563446998596)
[2025-02-13 04:04:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:26][root][INFO] - Training Epoch: 2/2, step 3510/7134 completed (loss: 0.019956808537244797, acc: 0.994854211807251)
[2025-02-13 04:04:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:26][root][INFO] - Training Epoch: 2/2, step 3511/7134 completed (loss: 0.009934251196682453, acc: 0.9981982111930847)
[2025-02-13 04:04:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:27][root][INFO] - Training Epoch: 2/2, step 3512/7134 completed (loss: 0.019783280789852142, acc: 0.995245635509491)
[2025-02-13 04:04:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:27][root][INFO] - Training Epoch: 2/2, step 3513/7134 completed (loss: 0.013170679099857807, acc: 0.9946902394294739)
[2025-02-13 04:04:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:28][root][INFO] - Training Epoch: 2/2, step 3514/7134 completed (loss: 0.013582399114966393, acc: 0.9976958632469177)
[2025-02-13 04:04:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:28][root][INFO] - Training Epoch: 2/2, step 3515/7134 completed (loss: 0.0027476868126541376, acc: 1.0)
[2025-02-13 04:04:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:29][root][INFO] - Training Epoch: 2/2, step 3516/7134 completed (loss: 0.016196634620428085, acc: 0.9956663250923157)
[2025-02-13 04:04:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:29][root][INFO] - Training Epoch: 2/2, step 3517/7134 completed (loss: 0.0030772858299314976, acc: 0.9988080859184265)
[2025-02-13 04:04:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:30][root][INFO] - Training Epoch: 2/2, step 3518/7134 completed (loss: 0.015770714730024338, acc: 0.9926062822341919)
[2025-02-13 04:04:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:30][root][INFO] - Training Epoch: 2/2, step 3519/7134 completed (loss: 0.011367129161953926, acc: 0.9977527856826782)
[2025-02-13 04:04:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:30][root][INFO] - Training Epoch: 2/2, step 3520/7134 completed (loss: 0.043184056878089905, acc: 0.9941176176071167)
[2025-02-13 04:04:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:31][root][INFO] - Training Epoch: 2/2, step 3521/7134 completed (loss: 0.0026398450136184692, acc: 1.0)
[2025-02-13 04:04:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:31][root][INFO] - Training Epoch: 2/2, step 3522/7134 completed (loss: 0.0036065734457224607, acc: 0.9973261952400208)
[2025-02-13 04:04:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:32][root][INFO] - Training Epoch: 2/2, step 3523/7134 completed (loss: 0.002143668010830879, acc: 1.0)
[2025-02-13 04:04:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:32][root][INFO] - Training Epoch: 2/2, step 3524/7134 completed (loss: 0.017732208594679832, acc: 0.9947916865348816)
[2025-02-13 04:04:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:33][root][INFO] - Training Epoch: 2/2, step 3525/7134 completed (loss: 0.008728105574846268, acc: 0.9983525276184082)
[2025-02-13 04:04:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:33][root][INFO] - Training Epoch: 2/2, step 3526/7134 completed (loss: 0.03222649171948433, acc: 0.9898605942726135)
[2025-02-13 04:04:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:34][root][INFO] - Training Epoch: 2/2, step 3527/7134 completed (loss: 0.022459641098976135, acc: 0.99615877866745)
[2025-02-13 04:04:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:34][root][INFO] - Training Epoch: 2/2, step 3528/7134 completed (loss: 0.0010197687661275268, acc: 1.0)
[2025-02-13 04:04:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:35][root][INFO] - Training Epoch: 2/2, step 3529/7134 completed (loss: 0.00774378702044487, acc: 0.9952940940856934)
[2025-02-13 04:04:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:35][root][INFO] - Training Epoch: 2/2, step 3530/7134 completed (loss: 0.007525802589952946, acc: 0.9965596199035645)
[2025-02-13 04:04:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:35][root][INFO] - Training Epoch: 2/2, step 3531/7134 completed (loss: 0.01277987752109766, acc: 0.9969879388809204)
[2025-02-13 04:04:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:36][root][INFO] - Training Epoch: 2/2, step 3532/7134 completed (loss: 0.0026053357869386673, acc: 1.0)
[2025-02-13 04:04:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:36][root][INFO] - Training Epoch: 2/2, step 3533/7134 completed (loss: 0.009430916048586369, acc: 0.9965075850486755)
[2025-02-13 04:04:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:37][root][INFO] - Training Epoch: 2/2, step 3534/7134 completed (loss: 0.01862614043056965, acc: 0.9944367408752441)
[2025-02-13 04:04:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:37][root][INFO] - Training Epoch: 2/2, step 3535/7134 completed (loss: 0.0276488084346056, acc: 0.9953917264938354)
[2025-02-13 04:04:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:38][root][INFO] - Training Epoch: 2/2, step 3536/7134 completed (loss: 0.04096386581659317, acc: 0.9890710115432739)
[2025-02-13 04:04:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:38][root][INFO] - Training Epoch: 2/2, step 3537/7134 completed (loss: 0.017681829631328583, acc: 0.9966443181037903)
[2025-02-13 04:04:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:39][root][INFO] - Training Epoch: 2/2, step 3538/7134 completed (loss: 0.0421370267868042, acc: 0.9961977005004883)
[2025-02-13 04:04:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:39][root][INFO] - Training Epoch: 2/2, step 3539/7134 completed (loss: 0.0038791862316429615, acc: 1.0)
[2025-02-13 04:04:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:39][root][INFO] - Training Epoch: 2/2, step 3540/7134 completed (loss: 0.03754355013370514, acc: 0.9863429665565491)
[2025-02-13 04:04:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:40][root][INFO] - Training Epoch: 2/2, step 3541/7134 completed (loss: 0.022674864158034325, acc: 0.9911894202232361)
[2025-02-13 04:04:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:40][root][INFO] - Training Epoch: 2/2, step 3542/7134 completed (loss: 0.012649917975068092, acc: 0.9953846335411072)
[2025-02-13 04:04:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:41][root][INFO] - Training Epoch: 2/2, step 3543/7134 completed (loss: 0.029290128499269485, acc: 0.9894366264343262)
[2025-02-13 04:04:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:41][root][INFO] - Training Epoch: 2/2, step 3544/7134 completed (loss: 0.024102939292788506, acc: 0.9923076629638672)
[2025-02-13 04:04:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:42][root][INFO] - Training Epoch: 2/2, step 3545/7134 completed (loss: 0.013442384079098701, acc: 0.9968944191932678)
[2025-02-13 04:04:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:42][root][INFO] - Training Epoch: 2/2, step 3546/7134 completed (loss: 0.017206404358148575, acc: 0.9917491674423218)
[2025-02-13 04:04:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:42][root][INFO] - Training Epoch: 2/2, step 3547/7134 completed (loss: 0.03131341561675072, acc: 0.9902098178863525)
[2025-02-13 04:04:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:43][root][INFO] - Training Epoch: 2/2, step 3548/7134 completed (loss: 0.02491694688796997, acc: 0.9933628439903259)
[2025-02-13 04:04:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:43][root][INFO] - Training Epoch: 2/2, step 3549/7134 completed (loss: 0.025316454470157623, acc: 0.9929453134536743)
[2025-02-13 04:04:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:44][root][INFO] - Training Epoch: 2/2, step 3550/7134 completed (loss: 0.008759872987866402, acc: 0.996497392654419)
[2025-02-13 04:04:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:44][root][INFO] - Training Epoch: 2/2, step 3551/7134 completed (loss: 0.016836930066347122, acc: 0.9949495196342468)
[2025-02-13 04:04:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:45][root][INFO] - Training Epoch: 2/2, step 3552/7134 completed (loss: 0.07870083302259445, acc: 0.983146071434021)
[2025-02-13 04:04:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:45][root][INFO] - Training Epoch: 2/2, step 3553/7134 completed (loss: 0.05369154363870621, acc: 0.9884868264198303)
[2025-02-13 04:04:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:45][root][INFO] - Training Epoch: 2/2, step 3554/7134 completed (loss: 0.036496106535196304, acc: 0.990963876247406)
[2025-02-13 04:04:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:46][root][INFO] - Training Epoch: 2/2, step 3555/7134 completed (loss: 0.023791858926415443, acc: 0.9937205910682678)
[2025-02-13 04:04:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:46][root][INFO] - Training Epoch: 2/2, step 3556/7134 completed (loss: 0.034513551741838455, acc: 0.9904030561447144)
[2025-02-13 04:04:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:47][root][INFO] - Training Epoch: 2/2, step 3557/7134 completed (loss: 0.026774080470204353, acc: 0.9927404522895813)
[2025-02-13 04:04:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:47][root][INFO] - Training Epoch: 2/2, step 3558/7134 completed (loss: 0.03653128817677498, acc: 0.9896907210350037)
[2025-02-13 04:04:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:48][root][INFO] - Training Epoch: 2/2, step 3559/7134 completed (loss: 0.00904629286378622, acc: 0.9970930218696594)
[2025-02-13 04:04:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:48][root][INFO] - Training Epoch: 2/2, step 3560/7134 completed (loss: 0.0071105253882706165, acc: 1.0)
[2025-02-13 04:04:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:48][root][INFO] - Training Epoch: 2/2, step 3561/7134 completed (loss: 0.03097516857087612, acc: 0.9887955188751221)
[2025-02-13 04:04:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:49][root][INFO] - Training Epoch: 2/2, step 3562/7134 completed (loss: 0.023654017597436905, acc: 0.9928571581840515)
[2025-02-13 04:04:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:49][root][INFO] - Training Epoch: 2/2, step 3563/7134 completed (loss: 0.012176159769296646, acc: 0.998410165309906)
[2025-02-13 04:04:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:04:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:05:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:06:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:07:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:08:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:04][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.0422, device='cuda:0') eval_epoch_loss=tensor(0.0413, device='cuda:0') eval_epoch_acc=tensor(0.9888, device='cuda:0')
[2025-02-13 04:09:04][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-13 04:09:04][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-13 04:09:04][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme/asr_epoch_2_step_3564_loss_0.04128989577293396/model.pt
[2025-02-13 04:09:04][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme directory
[2025-02-13 04:09:04][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 2 is 0.04128989577293396
[2025-02-13 04:09:04][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.9887902140617371
[2025-02-13 04:09:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:05][root][INFO] - Training Epoch: 2/2, step 3564/7134 completed (loss: 0.040171630680561066, acc: 0.9906014800071716)
[2025-02-13 04:09:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:05][root][INFO] - Training Epoch: 2/2, step 3565/7134 completed (loss: 0.05456065759062767, acc: 0.9793233275413513)
[2025-02-13 04:09:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:06][root][INFO] - Training Epoch: 2/2, step 3566/7134 completed (loss: 0.04188084229826927, acc: 0.9860557913780212)
[2025-02-13 04:09:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:06][root][INFO] - Training Epoch: 2/2, step 3567/7134 completed (loss: 0.06367918848991394, acc: 0.9857697486877441)
[2025-02-13 04:09:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:06][root][INFO] - Training Epoch: 2/2, step 3568/7134 completed (loss: 0.053683530539274216, acc: 0.9852700233459473)
[2025-02-13 04:09:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:07][root][INFO] - Training Epoch: 2/2, step 3569/7134 completed (loss: 0.03275255113840103, acc: 0.9874551892280579)
[2025-02-13 04:09:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:07][root][INFO] - Training Epoch: 2/2, step 3570/7134 completed (loss: 0.07000471651554108, acc: 0.9867768883705139)
[2025-02-13 04:09:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:08][root][INFO] - Training Epoch: 2/2, step 3571/7134 completed (loss: 0.058619968593120575, acc: 0.9867374300956726)
[2025-02-13 04:09:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:08][root][INFO] - Training Epoch: 2/2, step 3572/7134 completed (loss: 0.010912994854152203, acc: 0.9986824989318848)
[2025-02-13 04:09:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:09][root][INFO] - Training Epoch: 2/2, step 3573/7134 completed (loss: 0.03729483112692833, acc: 0.9837037324905396)
[2025-02-13 04:09:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:09][root][INFO] - Training Epoch: 2/2, step 3574/7134 completed (loss: 0.02895023487508297, acc: 0.9885057210922241)
[2025-02-13 04:09:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:10][root][INFO] - Training Epoch: 2/2, step 3575/7134 completed (loss: 0.07968108355998993, acc: 0.9777015447616577)
[2025-02-13 04:09:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:10][root][INFO] - Training Epoch: 2/2, step 3576/7134 completed (loss: 0.031290095299482346, acc: 0.988950252532959)
[2025-02-13 04:09:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:10][root][INFO] - Training Epoch: 2/2, step 3577/7134 completed (loss: 0.03498257324099541, acc: 0.991150438785553)
[2025-02-13 04:09:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:11][root][INFO] - Training Epoch: 2/2, step 3578/7134 completed (loss: 0.024658577516674995, acc: 0.9898107647895813)
[2025-02-13 04:09:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:11][root][INFO] - Training Epoch: 2/2, step 3579/7134 completed (loss: 0.039758093655109406, acc: 0.9879999756813049)
[2025-02-13 04:09:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:12][root][INFO] - Training Epoch: 2/2, step 3580/7134 completed (loss: 0.04841168224811554, acc: 0.9842519760131836)
[2025-02-13 04:09:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:12][root][INFO] - Training Epoch: 2/2, step 3581/7134 completed (loss: 0.0512542650103569, acc: 0.9867452383041382)
[2025-02-13 04:09:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:13][root][INFO] - Training Epoch: 2/2, step 3582/7134 completed (loss: 0.046954575926065445, acc: 0.9834123253822327)
[2025-02-13 04:09:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:13][root][INFO] - Training Epoch: 2/2, step 3583/7134 completed (loss: 0.050554465502500534, acc: 0.9832826852798462)
[2025-02-13 04:09:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:13][root][INFO] - Training Epoch: 2/2, step 3584/7134 completed (loss: 0.055255234241485596, acc: 0.9920634627342224)
[2025-02-13 04:09:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:14][root][INFO] - Training Epoch: 2/2, step 3585/7134 completed (loss: 0.04349522665143013, acc: 0.9895833134651184)
[2025-02-13 04:09:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:14][root][INFO] - Training Epoch: 2/2, step 3586/7134 completed (loss: 0.039711400866508484, acc: 0.9839486479759216)
[2025-02-13 04:09:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:15][root][INFO] - Training Epoch: 2/2, step 3587/7134 completed (loss: 0.03905683755874634, acc: 0.989847719669342)
[2025-02-13 04:09:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:15][root][INFO] - Training Epoch: 2/2, step 3588/7134 completed (loss: 0.049006734043359756, acc: 0.9866864085197449)
[2025-02-13 04:09:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:16][root][INFO] - Training Epoch: 2/2, step 3589/7134 completed (loss: 0.06153062358498573, acc: 0.9894099831581116)
[2025-02-13 04:09:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:16][root][INFO] - Training Epoch: 2/2, step 3590/7134 completed (loss: 0.01738327369093895, acc: 0.992977499961853)
[2025-02-13 04:09:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:16][root][INFO] - Training Epoch: 2/2, step 3591/7134 completed (loss: 0.023922383785247803, acc: 0.9939939975738525)
[2025-02-13 04:09:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:17][root][INFO] - Training Epoch: 2/2, step 3592/7134 completed (loss: 0.013752653263509274, acc: 0.9933884143829346)
[2025-02-13 04:09:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:17][root][INFO] - Training Epoch: 2/2, step 3593/7134 completed (loss: 0.03426172584295273, acc: 0.9912434220314026)
[2025-02-13 04:09:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:18][root][INFO] - Training Epoch: 2/2, step 3594/7134 completed (loss: 0.020448505878448486, acc: 0.9935064911842346)
[2025-02-13 04:09:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:18][root][INFO] - Training Epoch: 2/2, step 3595/7134 completed (loss: 0.030018819496035576, acc: 0.9892802238464355)
[2025-02-13 04:09:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:19][root][INFO] - Training Epoch: 2/2, step 3596/7134 completed (loss: 0.023036785423755646, acc: 0.993122398853302)
[2025-02-13 04:09:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:19][root][INFO] - Training Epoch: 2/2, step 3597/7134 completed (loss: 0.018252849578857422, acc: 0.9956834316253662)
[2025-02-13 04:09:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:20][root][INFO] - Training Epoch: 2/2, step 3598/7134 completed (loss: 0.04246310517191887, acc: 0.9885583519935608)
[2025-02-13 04:09:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:20][root][INFO] - Training Epoch: 2/2, step 3599/7134 completed (loss: 0.04801696538925171, acc: 0.9873896837234497)
[2025-02-13 04:09:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:20][root][INFO] - Training Epoch: 2/2, step 3600/7134 completed (loss: 0.0421711802482605, acc: 0.9876543283462524)
[2025-02-13 04:09:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:21][root][INFO] - Training Epoch: 2/2, step 3601/7134 completed (loss: 0.09236649423837662, acc: 0.9765517115592957)
[2025-02-13 04:09:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:21][root][INFO] - Training Epoch: 2/2, step 3602/7134 completed (loss: 0.06959177553653717, acc: 0.9801633358001709)
[2025-02-13 04:09:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:22][root][INFO] - Training Epoch: 2/2, step 3603/7134 completed (loss: 0.016580622643232346, acc: 0.9937499761581421)
[2025-02-13 04:09:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:22][root][INFO] - Training Epoch: 2/2, step 3604/7134 completed (loss: 0.021446766331791878, acc: 0.9887429475784302)
[2025-02-13 04:09:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:23][root][INFO] - Training Epoch: 2/2, step 3605/7134 completed (loss: 0.040566544979810715, acc: 0.9896480441093445)
[2025-02-13 04:09:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:23][root][INFO] - Training Epoch: 2/2, step 3606/7134 completed (loss: 0.040872037410736084, acc: 0.9893428087234497)
[2025-02-13 04:09:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:24][root][INFO] - Training Epoch: 2/2, step 3607/7134 completed (loss: 0.027274135500192642, acc: 0.994966447353363)
[2025-02-13 04:09:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:24][root][INFO] - Training Epoch: 2/2, step 3608/7134 completed (loss: 0.0700291320681572, acc: 0.9856194853782654)
[2025-02-13 04:09:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:25][root][INFO] - Training Epoch: 2/2, step 3609/7134 completed (loss: 0.038961004465818405, acc: 0.9911167621612549)
[2025-02-13 04:09:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:25][root][INFO] - Training Epoch: 2/2, step 3610/7134 completed (loss: 0.05817822366952896, acc: 0.9805194735527039)
[2025-02-13 04:09:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:25][root][INFO] - Training Epoch: 2/2, step 3611/7134 completed (loss: 0.027313414961099625, acc: 0.9919354915618896)
[2025-02-13 04:09:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:26][root][INFO] - Training Epoch: 2/2, step 3612/7134 completed (loss: 0.061951376497745514, acc: 0.9792531132698059)
[2025-02-13 04:09:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:26][root][INFO] - Training Epoch: 2/2, step 3613/7134 completed (loss: 0.039592355489730835, acc: 0.9847328066825867)
[2025-02-13 04:09:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:27][root][INFO] - Training Epoch: 2/2, step 3614/7134 completed (loss: 0.031305942684412, acc: 0.9892904758453369)
[2025-02-13 04:09:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:27][root][INFO] - Training Epoch: 2/2, step 3615/7134 completed (loss: 0.02236930839717388, acc: 0.9969419240951538)
[2025-02-13 04:09:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:28][root][INFO] - Training Epoch: 2/2, step 3616/7134 completed (loss: 0.03559628874063492, acc: 0.9843013882637024)
[2025-02-13 04:09:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:28][root][INFO] - Training Epoch: 2/2, step 3617/7134 completed (loss: 0.027559200301766396, acc: 0.9925742745399475)
[2025-02-13 04:09:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:29][root][INFO] - Training Epoch: 2/2, step 3618/7134 completed (loss: 0.07389231026172638, acc: 0.9787581562995911)
[2025-02-13 04:09:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:29][root][INFO] - Training Epoch: 2/2, step 3619/7134 completed (loss: 0.0731460377573967, acc: 0.9750000238418579)
[2025-02-13 04:09:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:30][root][INFO] - Training Epoch: 2/2, step 3620/7134 completed (loss: 0.03455333039164543, acc: 0.9887482523918152)
[2025-02-13 04:09:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:30][root][INFO] - Training Epoch: 2/2, step 3621/7134 completed (loss: 0.03161664307117462, acc: 0.9875173568725586)
[2025-02-13 04:09:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:30][root][INFO] - Training Epoch: 2/2, step 3622/7134 completed (loss: 0.026933835819363594, acc: 0.9894894957542419)
[2025-02-13 04:09:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:31][root][INFO] - Training Epoch: 2/2, step 3623/7134 completed (loss: 0.009697701781988144, acc: 1.0)
[2025-02-13 04:09:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:31][root][INFO] - Training Epoch: 2/2, step 3624/7134 completed (loss: 0.029071757569909096, acc: 0.9900000095367432)
[2025-02-13 04:09:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:32][root][INFO] - Training Epoch: 2/2, step 3625/7134 completed (loss: 0.048905957490205765, acc: 0.9778671860694885)
[2025-02-13 04:09:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:32][root][INFO] - Training Epoch: 2/2, step 3626/7134 completed (loss: 0.12236429750919342, acc: 0.9679999947547913)
[2025-02-13 04:09:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:33][root][INFO] - Training Epoch: 2/2, step 3627/7134 completed (loss: 0.04837259277701378, acc: 0.9858155846595764)
[2025-02-13 04:09:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:33][root][INFO] - Training Epoch: 2/2, step 3628/7134 completed (loss: 0.11920664459466934, acc: 0.9659090638160706)
[2025-02-13 04:09:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:33][root][INFO] - Training Epoch: 2/2, step 3629/7134 completed (loss: 0.0902610719203949, acc: 0.9735772609710693)
[2025-02-13 04:09:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:34][root][INFO] - Training Epoch: 2/2, step 3630/7134 completed (loss: 0.05319812148809433, acc: 0.9793281555175781)
[2025-02-13 04:09:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:34][root][INFO] - Training Epoch: 2/2, step 3631/7134 completed (loss: 0.05109303444623947, acc: 0.9852458834648132)
[2025-02-13 04:09:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:35][root][INFO] - Training Epoch: 2/2, step 3632/7134 completed (loss: 0.03525643050670624, acc: 0.9939637780189514)
[2025-02-13 04:09:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:35][root][INFO] - Training Epoch: 2/2, step 3633/7134 completed (loss: 0.06106511130928993, acc: 0.9870874881744385)
[2025-02-13 04:09:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:35][root][INFO] - Training Epoch: 2/2, step 3634/7134 completed (loss: 0.07964396476745605, acc: 0.9878542423248291)
[2025-02-13 04:09:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:36][root][INFO] - Training Epoch: 2/2, step 3635/7134 completed (loss: 0.013671094551682472, acc: 0.9946236610412598)
[2025-02-13 04:09:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:36][root][INFO] - Training Epoch: 2/2, step 3636/7134 completed (loss: 0.020123779773712158, acc: 0.9940944910049438)
[2025-02-13 04:09:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:37][root][INFO] - Training Epoch: 2/2, step 3637/7134 completed (loss: 0.02900826744735241, acc: 0.9919517040252686)
[2025-02-13 04:09:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:37][root][INFO] - Training Epoch: 2/2, step 3638/7134 completed (loss: 0.01652485877275467, acc: 0.9952940940856934)
[2025-02-13 04:09:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:37][root][INFO] - Training Epoch: 2/2, step 3639/7134 completed (loss: 0.012032190337777138, acc: 0.9977477192878723)
[2025-02-13 04:09:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:38][root][INFO] - Training Epoch: 2/2, step 3640/7134 completed (loss: 0.024544693529605865, acc: 0.9923664331436157)
[2025-02-13 04:09:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:38][root][INFO] - Training Epoch: 2/2, step 3641/7134 completed (loss: 0.02451738528907299, acc: 0.994163453578949)
[2025-02-13 04:09:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:39][root][INFO] - Training Epoch: 2/2, step 3642/7134 completed (loss: 0.015878349542617798, acc: 0.9953632354736328)
[2025-02-13 04:09:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:39][root][INFO] - Training Epoch: 2/2, step 3643/7134 completed (loss: 0.03264269605278969, acc: 0.9914089441299438)
[2025-02-13 04:09:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:40][root][INFO] - Training Epoch: 2/2, step 3644/7134 completed (loss: 0.0537162646651268, acc: 0.9869918823242188)
[2025-02-13 04:09:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:40][root][INFO] - Training Epoch: 2/2, step 3645/7134 completed (loss: 0.026931969448924065, acc: 0.9947183132171631)
[2025-02-13 04:09:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:40][root][INFO] - Training Epoch: 2/2, step 3646/7134 completed (loss: 0.09467581659555435, acc: 0.9756944179534912)
[2025-02-13 04:09:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:41][root][INFO] - Training Epoch: 2/2, step 3647/7134 completed (loss: 0.012105218134820461, acc: 0.9974160194396973)
[2025-02-13 04:09:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:41][root][INFO] - Training Epoch: 2/2, step 3648/7134 completed (loss: 0.04298613220453262, acc: 0.9821673631668091)
[2025-02-13 04:09:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:42][root][INFO] - Training Epoch: 2/2, step 3649/7134 completed (loss: 0.061209339648485184, acc: 0.9873949289321899)
[2025-02-13 04:09:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:42][root][INFO] - Training Epoch: 2/2, step 3650/7134 completed (loss: 0.04242510348558426, acc: 0.9882352948188782)
[2025-02-13 04:09:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:43][root][INFO] - Training Epoch: 2/2, step 3651/7134 completed (loss: 0.06596332788467407, acc: 0.9879356622695923)
[2025-02-13 04:09:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:43][root][INFO] - Training Epoch: 2/2, step 3652/7134 completed (loss: 0.04627280682325363, acc: 0.9879336357116699)
[2025-02-13 04:09:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:44][root][INFO] - Training Epoch: 2/2, step 3653/7134 completed (loss: 0.07393347471952438, acc: 0.9834087491035461)
[2025-02-13 04:09:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:44][root][INFO] - Training Epoch: 2/2, step 3654/7134 completed (loss: 0.023382393643260002, acc: 0.9913644194602966)
[2025-02-13 04:09:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:44][root][INFO] - Training Epoch: 2/2, step 3655/7134 completed (loss: 0.04825940355658531, acc: 0.9862637519836426)
[2025-02-13 04:09:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:45][root][INFO] - Training Epoch: 2/2, step 3656/7134 completed (loss: 0.08801183104515076, acc: 0.9818181991577148)
[2025-02-13 04:09:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:45][root][INFO] - Training Epoch: 2/2, step 3657/7134 completed (loss: 0.01062318030744791, acc: 0.9967948794364929)
[2025-02-13 04:09:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:46][root][INFO] - Training Epoch: 2/2, step 3658/7134 completed (loss: 0.04036670923233032, acc: 0.9906542301177979)
[2025-02-13 04:09:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:46][root][INFO] - Training Epoch: 2/2, step 3659/7134 completed (loss: 0.025862133130431175, acc: 0.9895366430282593)
[2025-02-13 04:09:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:46][root][INFO] - Training Epoch: 2/2, step 3660/7134 completed (loss: 0.056577783077955246, acc: 0.9868852496147156)
[2025-02-13 04:09:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:47][root][INFO] - Training Epoch: 2/2, step 3661/7134 completed (loss: 0.03120800107717514, acc: 0.9892857074737549)
[2025-02-13 04:09:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:47][root][INFO] - Training Epoch: 2/2, step 3662/7134 completed (loss: 0.05868779867887497, acc: 0.9817073345184326)
[2025-02-13 04:09:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:48][root][INFO] - Training Epoch: 2/2, step 3663/7134 completed (loss: 0.02480420656502247, acc: 0.9963369965553284)
[2025-02-13 04:09:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:48][root][INFO] - Training Epoch: 2/2, step 3664/7134 completed (loss: 0.040473803877830505, acc: 0.9924012422561646)
[2025-02-13 04:09:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:49][root][INFO] - Training Epoch: 2/2, step 3665/7134 completed (loss: 0.029605260118842125, acc: 0.9950739145278931)
[2025-02-13 04:09:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:49][root][INFO] - Training Epoch: 2/2, step 3666/7134 completed (loss: 0.035588037222623825, acc: 0.989051103591919)
[2025-02-13 04:09:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:49][root][INFO] - Training Epoch: 2/2, step 3667/7134 completed (loss: 0.12356018275022507, acc: 0.9726651310920715)
[2025-02-13 04:09:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:50][root][INFO] - Training Epoch: 2/2, step 3668/7134 completed (loss: 0.03413666784763336, acc: 0.9947643876075745)
[2025-02-13 04:09:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:50][root][INFO] - Training Epoch: 2/2, step 3669/7134 completed (loss: 0.009965112432837486, acc: 0.996219277381897)
[2025-02-13 04:09:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:51][root][INFO] - Training Epoch: 2/2, step 3670/7134 completed (loss: 0.02928941138088703, acc: 0.9931318759918213)
[2025-02-13 04:09:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:51][root][INFO] - Training Epoch: 2/2, step 3671/7134 completed (loss: 0.0370522178709507, acc: 0.9898843765258789)
[2025-02-13 04:09:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:52][root][INFO] - Training Epoch: 2/2, step 3672/7134 completed (loss: 0.01910359039902687, acc: 0.9949495196342468)
[2025-02-13 04:09:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:52][root][INFO] - Training Epoch: 2/2, step 3673/7134 completed (loss: 0.04294843226671219, acc: 0.9816053509712219)
[2025-02-13 04:09:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:52][root][INFO] - Training Epoch: 2/2, step 3674/7134 completed (loss: 0.023786693811416626, acc: 0.9937008023262024)
[2025-02-13 04:09:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:53][root][INFO] - Training Epoch: 2/2, step 3675/7134 completed (loss: 0.008862486109137535, acc: 0.9987244606018066)
[2025-02-13 04:09:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:53][root][INFO] - Training Epoch: 2/2, step 3676/7134 completed (loss: 0.011722086928784847, acc: 0.9985422492027283)
[2025-02-13 04:09:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:54][root][INFO] - Training Epoch: 2/2, step 3677/7134 completed (loss: 0.03174861520528793, acc: 0.9863013625144958)
[2025-02-13 04:09:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:54][root][INFO] - Training Epoch: 2/2, step 3678/7134 completed (loss: 0.023517772555351257, acc: 0.9919028282165527)
[2025-02-13 04:09:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:55][root][INFO] - Training Epoch: 2/2, step 3679/7134 completed (loss: 0.007400218863040209, acc: 1.0)
[2025-02-13 04:09:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:55][root][INFO] - Training Epoch: 2/2, step 3680/7134 completed (loss: 0.03484724834561348, acc: 0.9886363744735718)
[2025-02-13 04:09:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:55][root][INFO] - Training Epoch: 2/2, step 3681/7134 completed (loss: 0.024597637355327606, acc: 0.9902234673500061)
[2025-02-13 04:09:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:56][root][INFO] - Training Epoch: 2/2, step 3682/7134 completed (loss: 0.014303397387266159, acc: 0.9960474371910095)
[2025-02-13 04:09:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:56][root][INFO] - Training Epoch: 2/2, step 3683/7134 completed (loss: 0.014186250977218151, acc: 0.9968503713607788)
[2025-02-13 04:09:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:57][root][INFO] - Training Epoch: 2/2, step 3684/7134 completed (loss: 0.007828004658222198, acc: 0.9985569715499878)
[2025-02-13 04:09:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:57][root][INFO] - Training Epoch: 2/2, step 3685/7134 completed (loss: 0.01546283345669508, acc: 0.9922928810119629)
[2025-02-13 04:09:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:58][root][INFO] - Training Epoch: 2/2, step 3686/7134 completed (loss: 0.011817836202681065, acc: 0.995502233505249)
[2025-02-13 04:09:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:58][root][INFO] - Training Epoch: 2/2, step 3687/7134 completed (loss: 0.012215999886393547, acc: 0.9938461780548096)
[2025-02-13 04:09:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:58][root][INFO] - Training Epoch: 2/2, step 3688/7134 completed (loss: 0.010826325044035912, acc: 0.993914783000946)
[2025-02-13 04:09:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:59][root][INFO] - Training Epoch: 2/2, step 3689/7134 completed (loss: 0.016445709392428398, acc: 0.991830050945282)
[2025-02-13 04:09:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:09:59][root][INFO] - Training Epoch: 2/2, step 3690/7134 completed (loss: 0.0074758343398571014, acc: 0.9942775368690491)
[2025-02-13 04:10:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:00][root][INFO] - Training Epoch: 2/2, step 3691/7134 completed (loss: 0.02749510109424591, acc: 0.9925558567047119)
[2025-02-13 04:10:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:00][root][INFO] - Training Epoch: 2/2, step 3692/7134 completed (loss: 0.036840107291936874, acc: 0.9893454909324646)
[2025-02-13 04:10:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:01][root][INFO] - Training Epoch: 2/2, step 3693/7134 completed (loss: 0.01941322535276413, acc: 0.9936908483505249)
[2025-02-13 04:10:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:01][root][INFO] - Training Epoch: 2/2, step 3694/7134 completed (loss: 0.022211341187357903, acc: 0.9943100810050964)
[2025-02-13 04:10:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:02][root][INFO] - Training Epoch: 2/2, step 3695/7134 completed (loss: 0.015236936509609222, acc: 0.9929178357124329)
[2025-02-13 04:10:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:02][root][INFO] - Training Epoch: 2/2, step 3696/7134 completed (loss: 0.007019365206360817, acc: 0.9974293112754822)
[2025-02-13 04:10:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:02][root][INFO] - Training Epoch: 2/2, step 3697/7134 completed (loss: 0.00977927166968584, acc: 0.9941691160202026)
[2025-02-13 04:10:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:03][root][INFO] - Training Epoch: 2/2, step 3698/7134 completed (loss: 0.022718520835042, acc: 0.9903692007064819)
[2025-02-13 04:10:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:03][root][INFO] - Training Epoch: 2/2, step 3699/7134 completed (loss: 0.0061974008567631245, acc: 0.9983108043670654)
[2025-02-13 04:10:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:04][root][INFO] - Training Epoch: 2/2, step 3700/7134 completed (loss: 0.015133149921894073, acc: 0.9956268072128296)
[2025-02-13 04:10:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:04][root][INFO] - Training Epoch: 2/2, step 3701/7134 completed (loss: 0.012948204763233662, acc: 0.996688723564148)
[2025-02-13 04:10:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:05][root][INFO] - Training Epoch: 2/2, step 3702/7134 completed (loss: 0.02697252295911312, acc: 0.9931129217147827)
[2025-02-13 04:10:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:05][root][INFO] - Training Epoch: 2/2, step 3703/7134 completed (loss: 0.022434020414948463, acc: 0.9936143159866333)
[2025-02-13 04:10:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:05][root][INFO] - Training Epoch: 2/2, step 3704/7134 completed (loss: 0.004581859335303307, acc: 0.9958158731460571)
[2025-02-13 04:10:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:06][root][INFO] - Training Epoch: 2/2, step 3705/7134 completed (loss: 0.035439249128103256, acc: 0.9888535141944885)
[2025-02-13 04:10:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:06][root][INFO] - Training Epoch: 2/2, step 3706/7134 completed (loss: 0.07694210857152939, acc: 0.9827798008918762)
[2025-02-13 04:10:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:07][root][INFO] - Training Epoch: 2/2, step 3707/7134 completed (loss: 0.009974529966711998, acc: 0.9957924485206604)
[2025-02-13 04:10:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:07][root][INFO] - Training Epoch: 2/2, step 3708/7134 completed (loss: 0.05139857530593872, acc: 0.9857142567634583)
[2025-02-13 04:10:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:08][root][INFO] - Training Epoch: 2/2, step 3709/7134 completed (loss: 0.018145373091101646, acc: 0.993914783000946)
[2025-02-13 04:10:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:08][root][INFO] - Training Epoch: 2/2, step 3710/7134 completed (loss: 0.022216921672225, acc: 0.9948770403862)
[2025-02-13 04:10:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:09][root][INFO] - Training Epoch: 2/2, step 3711/7134 completed (loss: 0.022859780117869377, acc: 0.9950371980667114)
[2025-02-13 04:10:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:09][root][INFO] - Training Epoch: 2/2, step 3712/7134 completed (loss: 0.024418028071522713, acc: 0.9928315281867981)
[2025-02-13 04:10:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:09][root][INFO] - Training Epoch: 2/2, step 3713/7134 completed (loss: 0.032636821269989014, acc: 0.9934318661689758)
[2025-02-13 04:10:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:10][root][INFO] - Training Epoch: 2/2, step 3714/7134 completed (loss: 0.02457214519381523, acc: 0.9922651648521423)
[2025-02-13 04:10:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:10][root][INFO] - Training Epoch: 2/2, step 3715/7134 completed (loss: 0.017575664445757866, acc: 0.996372401714325)
[2025-02-13 04:10:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:11][root][INFO] - Training Epoch: 2/2, step 3716/7134 completed (loss: 0.025195028632879257, acc: 0.9913686513900757)
[2025-02-13 04:10:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:11][root][INFO] - Training Epoch: 2/2, step 3717/7134 completed (loss: 0.017696669325232506, acc: 0.9972677826881409)
[2025-02-13 04:10:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:12][root][INFO] - Training Epoch: 2/2, step 3718/7134 completed (loss: 0.03404472768306732, acc: 0.9906213283538818)
[2025-02-13 04:10:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:12][root][INFO] - Training Epoch: 2/2, step 3719/7134 completed (loss: 0.03852561116218567, acc: 0.9867647290229797)
[2025-02-13 04:10:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:13][root][INFO] - Training Epoch: 2/2, step 3720/7134 completed (loss: 0.013848558068275452, acc: 0.9956011772155762)
[2025-02-13 04:10:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:13][root][INFO] - Training Epoch: 2/2, step 3721/7134 completed (loss: 0.019949527457356453, acc: 0.9946737885475159)
[2025-02-13 04:10:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:14][root][INFO] - Training Epoch: 2/2, step 3722/7134 completed (loss: 0.014376025646924973, acc: 0.9950739145278931)
[2025-02-13 04:10:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:14][root][INFO] - Training Epoch: 2/2, step 3723/7134 completed (loss: 0.04309859871864319, acc: 0.9884892106056213)
[2025-02-13 04:10:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:14][root][INFO] - Training Epoch: 2/2, step 3724/7134 completed (loss: 0.0316033698618412, acc: 0.9913793206214905)
[2025-02-13 04:10:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:15][root][INFO] - Training Epoch: 2/2, step 3725/7134 completed (loss: 0.09241173416376114, acc: 0.982300877571106)
[2025-02-13 04:10:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:15][root][INFO] - Training Epoch: 2/2, step 3726/7134 completed (loss: 0.018192125484347343, acc: 0.9957491755485535)
[2025-02-13 04:10:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:16][root][INFO] - Training Epoch: 2/2, step 3727/7134 completed (loss: 0.023870203644037247, acc: 0.9946595430374146)
[2025-02-13 04:10:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:16][root][INFO] - Training Epoch: 2/2, step 3728/7134 completed (loss: 0.006752804387360811, acc: 0.9988725781440735)
[2025-02-13 04:10:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:17][root][INFO] - Training Epoch: 2/2, step 3729/7134 completed (loss: 0.02177208475768566, acc: 0.9909090995788574)
[2025-02-13 04:10:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:17][root][INFO] - Training Epoch: 2/2, step 3730/7134 completed (loss: 0.024043669924139977, acc: 0.9945725798606873)
[2025-02-13 04:10:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:18][root][INFO] - Training Epoch: 2/2, step 3731/7134 completed (loss: 0.0075910319574177265, acc: 0.9987546801567078)
[2025-02-13 04:10:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:18][root][INFO] - Training Epoch: 2/2, step 3732/7134 completed (loss: 0.0071715982630848885, acc: 0.9988649487495422)
[2025-02-13 04:10:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:19][root][INFO] - Training Epoch: 2/2, step 3733/7134 completed (loss: 0.03548372536897659, acc: 0.9901823401451111)
[2025-02-13 04:10:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:19][root][INFO] - Training Epoch: 2/2, step 3734/7134 completed (loss: 0.02869443967938423, acc: 0.9902912378311157)
[2025-02-13 04:10:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:19][root][INFO] - Training Epoch: 2/2, step 3735/7134 completed (loss: 0.049652066081762314, acc: 0.9864457845687866)
[2025-02-13 04:10:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:20][root][INFO] - Training Epoch: 2/2, step 3736/7134 completed (loss: 0.06033077836036682, acc: 0.9843478202819824)
[2025-02-13 04:10:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:20][root][INFO] - Training Epoch: 2/2, step 3737/7134 completed (loss: 0.05302774906158447, acc: 0.9864253401756287)
[2025-02-13 04:10:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:21][root][INFO] - Training Epoch: 2/2, step 3738/7134 completed (loss: 0.015864191576838493, acc: 0.9950494766235352)
[2025-02-13 04:10:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:21][root][INFO] - Training Epoch: 2/2, step 3739/7134 completed (loss: 0.029254047200083733, acc: 0.9943820238113403)
[2025-02-13 04:10:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:21][root][INFO] - Training Epoch: 2/2, step 3740/7134 completed (loss: 0.029853375628590584, acc: 0.9951298832893372)
[2025-02-13 04:10:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:22][root][INFO] - Training Epoch: 2/2, step 3741/7134 completed (loss: 0.03295950964093208, acc: 0.9928673505783081)
[2025-02-13 04:10:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:22][root][INFO] - Training Epoch: 2/2, step 3742/7134 completed (loss: 0.04552803188562393, acc: 0.988034188747406)
[2025-02-13 04:10:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:23][root][INFO] - Training Epoch: 2/2, step 3743/7134 completed (loss: 0.039036743342876434, acc: 0.9882006049156189)
[2025-02-13 04:10:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:23][root][INFO] - Training Epoch: 2/2, step 3744/7134 completed (loss: 0.04703710600733757, acc: 0.9894598126411438)
[2025-02-13 04:10:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:23][root][INFO] - Training Epoch: 2/2, step 3745/7134 completed (loss: 0.06429136544466019, acc: 0.9837586879730225)
[2025-02-13 04:10:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:24][root][INFO] - Training Epoch: 2/2, step 3746/7134 completed (loss: 0.04406536743044853, acc: 0.9882199168205261)
[2025-02-13 04:10:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:24][root][INFO] - Training Epoch: 2/2, step 3747/7134 completed (loss: 0.0406716950237751, acc: 0.9883381724357605)
[2025-02-13 04:10:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:25][root][INFO] - Training Epoch: 2/2, step 3748/7134 completed (loss: 0.05728291720151901, acc: 0.9876760840415955)
[2025-02-13 04:10:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:25][root][INFO] - Training Epoch: 2/2, step 3749/7134 completed (loss: 0.065107561647892, acc: 0.9850075244903564)
[2025-02-13 04:10:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:26][root][INFO] - Training Epoch: 2/2, step 3750/7134 completed (loss: 0.009937548078596592, acc: 0.9972106218338013)
[2025-02-13 04:10:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:26][root][INFO] - Training Epoch: 2/2, step 3751/7134 completed (loss: 0.04266219586133957, acc: 0.9871976971626282)
[2025-02-13 04:10:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:26][root][INFO] - Training Epoch: 2/2, step 3752/7134 completed (loss: 0.02814234048128128, acc: 0.9940298795700073)
[2025-02-13 04:10:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:27][root][INFO] - Training Epoch: 2/2, step 3753/7134 completed (loss: 0.06851648539304733, acc: 0.9790732264518738)
[2025-02-13 04:10:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:27][root][INFO] - Training Epoch: 2/2, step 3754/7134 completed (loss: 0.00970879103988409, acc: 0.9984825253486633)
[2025-02-13 04:10:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:28][root][INFO] - Training Epoch: 2/2, step 3755/7134 completed (loss: 0.026326928287744522, acc: 0.9962894320487976)
[2025-02-13 04:10:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:28][root][INFO] - Training Epoch: 2/2, step 3756/7134 completed (loss: 0.03161650151014328, acc: 0.9889196753501892)
[2025-02-13 04:10:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:29][root][INFO] - Training Epoch: 2/2, step 3757/7134 completed (loss: 0.059190232306718826, acc: 0.9786535501480103)
[2025-02-13 04:10:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:29][root][INFO] - Training Epoch: 2/2, step 3758/7134 completed (loss: 0.0269971564412117, acc: 0.9891892075538635)
[2025-02-13 04:10:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:29][root][INFO] - Training Epoch: 2/2, step 3759/7134 completed (loss: 0.03482934460043907, acc: 0.9881756901741028)
[2025-02-13 04:10:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:30][root][INFO] - Training Epoch: 2/2, step 3760/7134 completed (loss: 0.053458504378795624, acc: 0.9887005686759949)
[2025-02-13 04:10:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:30][root][INFO] - Training Epoch: 2/2, step 3761/7134 completed (loss: 0.029383741319179535, acc: 0.9922720193862915)
[2025-02-13 04:10:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:31][root][INFO] - Training Epoch: 2/2, step 3762/7134 completed (loss: 0.03183336183428764, acc: 0.9889958500862122)
[2025-02-13 04:10:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:31][root][INFO] - Training Epoch: 2/2, step 3763/7134 completed (loss: 0.01330025214701891, acc: 0.9938271641731262)
[2025-02-13 04:10:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:32][root][INFO] - Training Epoch: 2/2, step 3764/7134 completed (loss: 0.04328184947371483, acc: 0.9876881241798401)
[2025-02-13 04:10:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:32][root][INFO] - Training Epoch: 2/2, step 3765/7134 completed (loss: 0.013001054525375366, acc: 0.9963985681533813)
[2025-02-13 04:10:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:32][root][INFO] - Training Epoch: 2/2, step 3766/7134 completed (loss: 0.037944767624139786, acc: 0.9900285005569458)
[2025-02-13 04:10:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:33][root][INFO] - Training Epoch: 2/2, step 3767/7134 completed (loss: 0.011412817053496838, acc: 0.996259331703186)
[2025-02-13 04:10:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:33][root][INFO] - Training Epoch: 2/2, step 3768/7134 completed (loss: 0.008245655335485935, acc: 0.9979252815246582)
[2025-02-13 04:10:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:34][root][INFO] - Training Epoch: 2/2, step 3769/7134 completed (loss: 0.03049011155962944, acc: 0.989924430847168)
[2025-02-13 04:10:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:34][root][INFO] - Training Epoch: 2/2, step 3770/7134 completed (loss: 0.027074024081230164, acc: 0.9929378628730774)
[2025-02-13 04:10:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:35][root][INFO] - Training Epoch: 2/2, step 3771/7134 completed (loss: 0.013167958706617355, acc: 0.9961038827896118)
[2025-02-13 04:10:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:35][root][INFO] - Training Epoch: 2/2, step 3772/7134 completed (loss: 0.04026058688759804, acc: 0.988041877746582)
[2025-02-13 04:10:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:35][root][INFO] - Training Epoch: 2/2, step 3773/7134 completed (loss: 0.027804318815469742, acc: 0.9944196343421936)
[2025-02-13 04:10:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:36][root][INFO] - Training Epoch: 2/2, step 3774/7134 completed (loss: 0.017118263989686966, acc: 0.9941995143890381)
[2025-02-13 04:10:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:36][root][INFO] - Training Epoch: 2/2, step 3775/7134 completed (loss: 0.029226625338196754, acc: 0.9942693114280701)
[2025-02-13 04:10:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:37][root][INFO] - Training Epoch: 2/2, step 3776/7134 completed (loss: 0.023281723260879517, acc: 0.9940740466117859)
[2025-02-13 04:10:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:37][root][INFO] - Training Epoch: 2/2, step 3777/7134 completed (loss: 0.019578099250793457, acc: 0.9937343597412109)
[2025-02-13 04:10:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:38][root][INFO] - Training Epoch: 2/2, step 3778/7134 completed (loss: 0.04224149137735367, acc: 0.9886202216148376)
[2025-02-13 04:10:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:38][root][INFO] - Training Epoch: 2/2, step 3779/7134 completed (loss: 0.0271303690969944, acc: 0.9900285005569458)
[2025-02-13 04:10:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:39][root][INFO] - Training Epoch: 2/2, step 3780/7134 completed (loss: 0.02294708415865898, acc: 0.9913544654846191)
[2025-02-13 04:10:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:39][root][INFO] - Training Epoch: 2/2, step 3781/7134 completed (loss: 0.034285493195056915, acc: 0.9876543283462524)
[2025-02-13 04:10:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:39][root][INFO] - Training Epoch: 2/2, step 3782/7134 completed (loss: 0.03481840342283249, acc: 0.9857549667358398)
[2025-02-13 04:10:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:40][root][INFO] - Training Epoch: 2/2, step 3783/7134 completed (loss: 0.03435729816555977, acc: 0.9883871078491211)
[2025-02-13 04:10:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:40][root][INFO] - Training Epoch: 2/2, step 3784/7134 completed (loss: 0.04087606817483902, acc: 0.989180862903595)
[2025-02-13 04:10:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:41][root][INFO] - Training Epoch: 2/2, step 3785/7134 completed (loss: 0.011026274412870407, acc: 0.9953197836875916)
[2025-02-13 04:10:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:41][root][INFO] - Training Epoch: 2/2, step 3786/7134 completed (loss: 0.03158974647521973, acc: 0.9883570671081543)
[2025-02-13 04:10:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:41][root][INFO] - Training Epoch: 2/2, step 3787/7134 completed (loss: 0.017121659591794014, acc: 0.9968101978302002)
[2025-02-13 04:10:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:42][root][INFO] - Training Epoch: 2/2, step 3788/7134 completed (loss: 0.02336786314845085, acc: 0.9946019053459167)
[2025-02-13 04:10:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:42][root][INFO] - Training Epoch: 2/2, step 3789/7134 completed (loss: 0.03421625867486, acc: 0.9901546835899353)
[2025-02-13 04:10:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:43][root][INFO] - Training Epoch: 2/2, step 3790/7134 completed (loss: 0.08357731997966766, acc: 0.9822559952735901)
[2025-02-13 04:10:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:43][root][INFO] - Training Epoch: 2/2, step 3791/7134 completed (loss: 0.015599886886775494, acc: 0.996129035949707)
[2025-02-13 04:10:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:44][root][INFO] - Training Epoch: 2/2, step 3792/7134 completed (loss: 0.01267757173627615, acc: 0.9967159032821655)
[2025-02-13 04:10:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:44][root][INFO] - Training Epoch: 2/2, step 3793/7134 completed (loss: 0.026787973940372467, acc: 0.9918919205665588)
[2025-02-13 04:10:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:44][root][INFO] - Training Epoch: 2/2, step 3794/7134 completed (loss: 0.028845323249697685, acc: 0.9896238446235657)
[2025-02-13 04:10:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:45][root][INFO] - Training Epoch: 2/2, step 3795/7134 completed (loss: 0.023984571918845177, acc: 0.9927710890769958)
[2025-02-13 04:10:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:45][root][INFO] - Training Epoch: 2/2, step 3796/7134 completed (loss: 0.01671558804810047, acc: 0.9949748516082764)
[2025-02-13 04:10:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:46][root][INFO] - Training Epoch: 2/2, step 3797/7134 completed (loss: 0.017856836318969727, acc: 0.9908735156059265)
[2025-02-13 04:10:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:46][root][INFO] - Training Epoch: 2/2, step 3798/7134 completed (loss: 0.03833131119608879, acc: 0.9913793206214905)
[2025-02-13 04:10:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:47][root][INFO] - Training Epoch: 2/2, step 3799/7134 completed (loss: 0.04153677076101303, acc: 0.9897040128707886)
[2025-02-13 04:10:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:47][root][INFO] - Training Epoch: 2/2, step 3800/7134 completed (loss: 0.031524594873189926, acc: 0.9923547506332397)
[2025-02-13 04:10:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:47][root][INFO] - Training Epoch: 2/2, step 3801/7134 completed (loss: 0.014572707936167717, acc: 0.9962962865829468)
[2025-02-13 04:10:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:48][root][INFO] - Training Epoch: 2/2, step 3802/7134 completed (loss: 0.021567629650235176, acc: 0.9940898418426514)
[2025-02-13 04:10:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:48][root][INFO] - Training Epoch: 2/2, step 3803/7134 completed (loss: 0.029092960059642792, acc: 0.994397759437561)
[2025-02-13 04:10:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:49][root][INFO] - Training Epoch: 2/2, step 3804/7134 completed (loss: 0.02440163679420948, acc: 0.9922239780426025)
[2025-02-13 04:10:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:49][root][INFO] - Training Epoch: 2/2, step 3805/7134 completed (loss: 0.010724717751145363, acc: 0.996073305606842)
[2025-02-13 04:10:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:50][root][INFO] - Training Epoch: 2/2, step 3806/7134 completed (loss: 0.024452319368720055, acc: 0.990138053894043)
[2025-02-13 04:10:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:50][root][INFO] - Training Epoch: 2/2, step 3807/7134 completed (loss: 0.01017433125525713, acc: 0.9985380172729492)
[2025-02-13 04:10:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:51][root][INFO] - Training Epoch: 2/2, step 3808/7134 completed (loss: 0.033358413726091385, acc: 0.9873417615890503)
[2025-02-13 04:10:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:51][root][INFO] - Training Epoch: 2/2, step 3809/7134 completed (loss: 0.034609921276569366, acc: 0.989847719669342)
[2025-02-13 04:10:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:51][root][INFO] - Training Epoch: 2/2, step 3810/7134 completed (loss: 0.020688116550445557, acc: 0.9940476417541504)
[2025-02-13 04:10:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:52][root][INFO] - Training Epoch: 2/2, step 3811/7134 completed (loss: 0.039037078619003296, acc: 0.9862068891525269)
[2025-02-13 04:10:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:52][root][INFO] - Training Epoch: 2/2, step 3812/7134 completed (loss: 0.033743537962436676, acc: 0.989708423614502)
[2025-02-13 04:10:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:53][root][INFO] - Training Epoch: 2/2, step 3813/7134 completed (loss: 0.03954639285802841, acc: 0.9868074059486389)
[2025-02-13 04:10:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:53][root][INFO] - Training Epoch: 2/2, step 3814/7134 completed (loss: 0.01481085829436779, acc: 0.9946380853652954)
[2025-02-13 04:10:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:54][root][INFO] - Training Epoch: 2/2, step 3815/7134 completed (loss: 0.0268235020339489, acc: 0.9881481528282166)
[2025-02-13 04:10:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:54][root][INFO] - Training Epoch: 2/2, step 3816/7134 completed (loss: 0.01291972678154707, acc: 0.9952681660652161)
[2025-02-13 04:10:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:54][root][INFO] - Training Epoch: 2/2, step 3817/7134 completed (loss: 0.041726820170879364, acc: 0.9832134246826172)
[2025-02-13 04:10:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:55][root][INFO] - Training Epoch: 2/2, step 3818/7134 completed (loss: 0.043976813554763794, acc: 0.9898989796638489)
[2025-02-13 04:10:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:55][root][INFO] - Training Epoch: 2/2, step 3819/7134 completed (loss: 0.044533103704452515, acc: 0.9839357137680054)
[2025-02-13 04:10:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:56][root][INFO] - Training Epoch: 2/2, step 3820/7134 completed (loss: 0.031726911664009094, acc: 0.9857482314109802)
[2025-02-13 04:10:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:56][root][INFO] - Training Epoch: 2/2, step 3821/7134 completed (loss: 0.017961543053388596, acc: 0.9915397763252258)
[2025-02-13 04:10:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:56][root][INFO] - Training Epoch: 2/2, step 3822/7134 completed (loss: 0.027597254142165184, acc: 0.9882121682167053)
[2025-02-13 04:10:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:57][root][INFO] - Training Epoch: 2/2, step 3823/7134 completed (loss: 0.014294181019067764, acc: 0.9930843710899353)
[2025-02-13 04:10:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:57][root][INFO] - Training Epoch: 2/2, step 3824/7134 completed (loss: 0.06528157740831375, acc: 0.9881955981254578)
[2025-02-13 04:10:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:58][root][INFO] - Training Epoch: 2/2, step 3825/7134 completed (loss: 0.022279605269432068, acc: 0.9940119981765747)
[2025-02-13 04:10:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:58][root][INFO] - Training Epoch: 2/2, step 3826/7134 completed (loss: 0.02194136381149292, acc: 0.9941176176071167)
[2025-02-13 04:10:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:59][root][INFO] - Training Epoch: 2/2, step 3827/7134 completed (loss: 0.026411617174744606, acc: 0.994397759437561)
[2025-02-13 04:10:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:59][root][INFO] - Training Epoch: 2/2, step 3828/7134 completed (loss: 0.019426867365837097, acc: 0.9973474740982056)
[2025-02-13 04:10:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:10:59][root][INFO] - Training Epoch: 2/2, step 3829/7134 completed (loss: 0.01619969680905342, acc: 0.9969834089279175)
[2025-02-13 04:11:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:00][root][INFO] - Training Epoch: 2/2, step 3830/7134 completed (loss: 0.03958982974290848, acc: 0.9899497628211975)
[2025-02-13 04:11:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:00][root][INFO] - Training Epoch: 2/2, step 3831/7134 completed (loss: 0.0239862147718668, acc: 0.9924924969673157)
[2025-02-13 04:11:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:01][root][INFO] - Training Epoch: 2/2, step 3832/7134 completed (loss: 0.022760439664125443, acc: 0.994106113910675)
[2025-02-13 04:11:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:01][root][INFO] - Training Epoch: 2/2, step 3833/7134 completed (loss: 0.009274391457438469, acc: 0.9950494766235352)
[2025-02-13 04:11:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:02][root][INFO] - Training Epoch: 2/2, step 3834/7134 completed (loss: 0.004173296969383955, acc: 0.998701274394989)
[2025-02-13 04:11:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:02][root][INFO] - Training Epoch: 2/2, step 3835/7134 completed (loss: 0.006435934919863939, acc: 0.9975728392601013)
[2025-02-13 04:11:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:02][root][INFO] - Training Epoch: 2/2, step 3836/7134 completed (loss: 0.03616088256239891, acc: 0.991304337978363)
[2025-02-13 04:11:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:03][root][INFO] - Training Epoch: 2/2, step 3837/7134 completed (loss: 0.021442823112010956, acc: 0.9971264600753784)
[2025-02-13 04:11:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:03][root][INFO] - Training Epoch: 2/2, step 3838/7134 completed (loss: 0.005383125972002745, acc: 0.9986720085144043)
[2025-02-13 04:11:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:04][root][INFO] - Training Epoch: 2/2, step 3839/7134 completed (loss: 0.0221147071570158, acc: 0.9944751262664795)
[2025-02-13 04:11:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:04][root][INFO] - Training Epoch: 2/2, step 3840/7134 completed (loss: 0.011341370642185211, acc: 0.9986613392829895)
[2025-02-13 04:11:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:05][root][INFO] - Training Epoch: 2/2, step 3841/7134 completed (loss: 0.03513411805033684, acc: 0.9889011979103088)
[2025-02-13 04:11:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:05][root][INFO] - Training Epoch: 2/2, step 3842/7134 completed (loss: 0.0012624658411368728, acc: 1.0)
[2025-02-13 04:11:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:06][root][INFO] - Training Epoch: 2/2, step 3843/7134 completed (loss: 0.01961640454828739, acc: 0.9958289861679077)
[2025-02-13 04:11:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:06][root][INFO] - Training Epoch: 2/2, step 3844/7134 completed (loss: 0.02056020125746727, acc: 0.9974586963653564)
[2025-02-13 04:11:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:07][root][INFO] - Training Epoch: 2/2, step 3845/7134 completed (loss: 0.023582708090543747, acc: 0.9932659864425659)
[2025-02-13 04:11:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:07][root][INFO] - Training Epoch: 2/2, step 3846/7134 completed (loss: 0.012455218471586704, acc: 0.9966044425964355)
[2025-02-13 04:11:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:07][root][INFO] - Training Epoch: 2/2, step 3847/7134 completed (loss: 0.013357152231037617, acc: 0.995199978351593)
[2025-02-13 04:11:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:08][root][INFO] - Training Epoch: 2/2, step 3848/7134 completed (loss: 0.02171075902879238, acc: 0.9974554777145386)
[2025-02-13 04:11:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:08][root][INFO] - Training Epoch: 2/2, step 3849/7134 completed (loss: 0.03445480391383171, acc: 0.9893048405647278)
[2025-02-13 04:11:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:09][root][INFO] - Training Epoch: 2/2, step 3850/7134 completed (loss: 0.051638413220644, acc: 0.9902200698852539)
[2025-02-13 04:11:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:09][root][INFO] - Training Epoch: 2/2, step 3851/7134 completed (loss: 0.03225371614098549, acc: 0.9860529899597168)
[2025-02-13 04:11:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:10][root][INFO] - Training Epoch: 2/2, step 3852/7134 completed (loss: 0.01855439692735672, acc: 0.9935691356658936)
[2025-02-13 04:11:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:10][root][INFO] - Training Epoch: 2/2, step 3853/7134 completed (loss: 0.03812280669808388, acc: 0.9859353303909302)
[2025-02-13 04:11:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:11][root][INFO] - Training Epoch: 2/2, step 3854/7134 completed (loss: 0.01591421850025654, acc: 0.9943820238113403)
[2025-02-13 04:11:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:11][root][INFO] - Training Epoch: 2/2, step 3855/7134 completed (loss: 0.03430125117301941, acc: 0.9900249242782593)
[2025-02-13 04:11:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:11][root][INFO] - Training Epoch: 2/2, step 3856/7134 completed (loss: 0.027856260538101196, acc: 0.9902371168136597)
[2025-02-13 04:11:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:12][root][INFO] - Training Epoch: 2/2, step 3857/7134 completed (loss: 0.013201138935983181, acc: 0.9951768517494202)
[2025-02-13 04:11:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:12][root][INFO] - Training Epoch: 2/2, step 3858/7134 completed (loss: 0.03675871714949608, acc: 0.9915013909339905)
[2025-02-13 04:11:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:13][root][INFO] - Training Epoch: 2/2, step 3859/7134 completed (loss: 0.03389819711446762, acc: 0.9885495901107788)
[2025-02-13 04:11:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:13][root][INFO] - Training Epoch: 2/2, step 3860/7134 completed (loss: 0.021896015852689743, acc: 0.994413435459137)
[2025-02-13 04:11:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:14][root][INFO] - Training Epoch: 2/2, step 3861/7134 completed (loss: 0.023429447785019875, acc: 0.9918367266654968)
[2025-02-13 04:11:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:14][root][INFO] - Training Epoch: 2/2, step 3862/7134 completed (loss: 0.03832056745886803, acc: 0.9868637323379517)
[2025-02-13 04:11:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:14][root][INFO] - Training Epoch: 2/2, step 3863/7134 completed (loss: 0.017088960856199265, acc: 0.9938271641731262)
[2025-02-13 04:11:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:15][root][INFO] - Training Epoch: 2/2, step 3864/7134 completed (loss: 0.02797159180045128, acc: 0.9947780966758728)
[2025-02-13 04:11:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:15][root][INFO] - Training Epoch: 2/2, step 3865/7134 completed (loss: 0.020433053374290466, acc: 0.9920318722724915)
[2025-02-13 04:11:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:16][root][INFO] - Training Epoch: 2/2, step 3866/7134 completed (loss: 0.038719143718481064, acc: 0.990231990814209)
[2025-02-13 04:11:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:16][root][INFO] - Training Epoch: 2/2, step 3867/7134 completed (loss: 0.029260937124490738, acc: 0.9905303120613098)
[2025-02-13 04:11:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:17][root][INFO] - Training Epoch: 2/2, step 3868/7134 completed (loss: 0.0252340380102396, acc: 0.9932318329811096)
[2025-02-13 04:11:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:17][root][INFO] - Training Epoch: 2/2, step 3869/7134 completed (loss: 0.022781528532505035, acc: 0.9939485788345337)
[2025-02-13 04:11:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:18][root][INFO] - Training Epoch: 2/2, step 3870/7134 completed (loss: 0.053601428866386414, acc: 0.987500011920929)
[2025-02-13 04:11:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:18][root][INFO] - Training Epoch: 2/2, step 3871/7134 completed (loss: 0.033323515206575394, acc: 0.9912717938423157)
[2025-02-13 04:11:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:18][root][INFO] - Training Epoch: 2/2, step 3872/7134 completed (loss: 0.04494386911392212, acc: 0.9892933368682861)
[2025-02-13 04:11:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:19][root][INFO] - Training Epoch: 2/2, step 3873/7134 completed (loss: 0.030688460916280746, acc: 0.9910485744476318)
[2025-02-13 04:11:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:19][root][INFO] - Training Epoch: 2/2, step 3874/7134 completed (loss: 0.04291433468461037, acc: 0.9875518679618835)
[2025-02-13 04:11:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:20][root][INFO] - Training Epoch: 2/2, step 3875/7134 completed (loss: 0.04979972168803215, acc: 0.9893617033958435)
[2025-02-13 04:11:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:20][root][INFO] - Training Epoch: 2/2, step 3876/7134 completed (loss: 0.017566215246915817, acc: 0.9906191229820251)
[2025-02-13 04:11:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:21][root][INFO] - Training Epoch: 2/2, step 3877/7134 completed (loss: 0.04485737904906273, acc: 0.9895561337471008)
[2025-02-13 04:11:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:21][root][INFO] - Training Epoch: 2/2, step 3878/7134 completed (loss: 0.05262935906648636, acc: 0.986522912979126)
[2025-02-13 04:11:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:22][root][INFO] - Training Epoch: 2/2, step 3879/7134 completed (loss: 0.010683169588446617, acc: 0.9981516003608704)
[2025-02-13 04:11:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:22][root][INFO] - Training Epoch: 2/2, step 3880/7134 completed (loss: 0.07789574563503265, acc: 0.9779735803604126)
[2025-02-13 04:11:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:22][root][INFO] - Training Epoch: 2/2, step 3881/7134 completed (loss: 0.08240196108818054, acc: 0.9826666712760925)
[2025-02-13 04:11:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:23][root][INFO] - Training Epoch: 2/2, step 3882/7134 completed (loss: 0.0293610617518425, acc: 0.9937499761581421)
[2025-02-13 04:11:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:23][root][INFO] - Training Epoch: 2/2, step 3883/7134 completed (loss: 0.006676284596323967, acc: 0.9986013770103455)
[2025-02-13 04:11:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:24][root][INFO] - Training Epoch: 2/2, step 3884/7134 completed (loss: 0.01363770104944706, acc: 0.9972789287567139)
[2025-02-13 04:11:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:24][root][INFO] - Training Epoch: 2/2, step 3885/7134 completed (loss: 0.018248211592435837, acc: 0.9934725761413574)
[2025-02-13 04:11:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:25][root][INFO] - Training Epoch: 2/2, step 3886/7134 completed (loss: 0.014981660060584545, acc: 0.9939613342285156)
[2025-02-13 04:11:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:25][root][INFO] - Training Epoch: 2/2, step 3887/7134 completed (loss: 0.019316446036100388, acc: 0.9954751133918762)
[2025-02-13 04:11:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:26][root][INFO] - Training Epoch: 2/2, step 3888/7134 completed (loss: 0.02443753182888031, acc: 0.9912499785423279)
[2025-02-13 04:11:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:26][root][INFO] - Training Epoch: 2/2, step 3889/7134 completed (loss: 0.0151266073808074, acc: 0.9974457025527954)
[2025-02-13 04:11:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:26][root][INFO] - Training Epoch: 2/2, step 3890/7134 completed (loss: 0.016951093450188637, acc: 0.9951865077018738)
[2025-02-13 04:11:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:27][root][INFO] - Training Epoch: 2/2, step 3891/7134 completed (loss: 0.011959823779761791, acc: 0.9960784316062927)
[2025-02-13 04:11:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:27][root][INFO] - Training Epoch: 2/2, step 3892/7134 completed (loss: 0.012171674519777298, acc: 0.9969230890274048)
[2025-02-13 04:11:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:28][root][INFO] - Training Epoch: 2/2, step 3893/7134 completed (loss: 0.06263166666030884, acc: 0.9768041372299194)
[2025-02-13 04:11:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:28][root][INFO] - Training Epoch: 2/2, step 3894/7134 completed (loss: 0.006242363713681698, acc: 1.0)
[2025-02-13 04:11:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:29][root][INFO] - Training Epoch: 2/2, step 3895/7134 completed (loss: 0.016295580193400383, acc: 0.996129035949707)
[2025-02-13 04:11:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:29][root][INFO] - Training Epoch: 2/2, step 3896/7134 completed (loss: 0.020649375393986702, acc: 0.9966139793395996)
[2025-02-13 04:11:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:30][root][INFO] - Training Epoch: 2/2, step 3897/7134 completed (loss: 0.053810346871614456, acc: 0.983098566532135)
[2025-02-13 04:11:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:30][root][INFO] - Training Epoch: 2/2, step 3898/7134 completed (loss: 0.02828175388276577, acc: 0.9949109554290771)
[2025-02-13 04:11:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:31][root][INFO] - Training Epoch: 2/2, step 3899/7134 completed (loss: 0.02271539904177189, acc: 0.9948253631591797)
[2025-02-13 04:11:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:31][root][INFO] - Training Epoch: 2/2, step 3900/7134 completed (loss: 0.015351603738963604, acc: 0.9942062497138977)
[2025-02-13 04:11:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:31][root][INFO] - Training Epoch: 2/2, step 3901/7134 completed (loss: 0.07096302509307861, acc: 0.9851751923561096)
[2025-02-13 04:11:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:32][root][INFO] - Training Epoch: 2/2, step 3902/7134 completed (loss: 0.06815288960933685, acc: 0.9841059446334839)
[2025-02-13 04:11:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:32][root][INFO] - Training Epoch: 2/2, step 3903/7134 completed (loss: 0.06312483549118042, acc: 0.9812332391738892)
[2025-02-13 04:11:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:33][root][INFO] - Training Epoch: 2/2, step 3904/7134 completed (loss: 0.03132638335227966, acc: 0.9923896789550781)
[2025-02-13 04:11:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:33][root][INFO] - Training Epoch: 2/2, step 3905/7134 completed (loss: 0.012262307107448578, acc: 0.9971791505813599)
[2025-02-13 04:11:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:34][root][INFO] - Training Epoch: 2/2, step 3906/7134 completed (loss: 0.01045407634228468, acc: 0.9971264600753784)
[2025-02-13 04:11:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:34][root][INFO] - Training Epoch: 2/2, step 3907/7134 completed (loss: 0.03399203345179558, acc: 0.9884318709373474)
[2025-02-13 04:11:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:35][root][INFO] - Training Epoch: 2/2, step 3908/7134 completed (loss: 0.05031782016158104, acc: 0.9846860766410828)
[2025-02-13 04:11:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:35][root][INFO] - Training Epoch: 2/2, step 3909/7134 completed (loss: 0.0231944527477026, acc: 0.9919224381446838)
[2025-02-13 04:11:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:35][root][INFO] - Training Epoch: 2/2, step 3910/7134 completed (loss: 0.03134233132004738, acc: 0.9874826073646545)
[2025-02-13 04:11:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:36][root][INFO] - Training Epoch: 2/2, step 3911/7134 completed (loss: 0.021314509212970734, acc: 0.9951456189155579)
[2025-02-13 04:11:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:36][root][INFO] - Training Epoch: 2/2, step 3912/7134 completed (loss: 0.009395611472427845, acc: 0.9955357313156128)
[2025-02-13 04:11:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:37][root][INFO] - Training Epoch: 2/2, step 3913/7134 completed (loss: 0.027962401509284973, acc: 0.9923076629638672)
[2025-02-13 04:11:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:37][root][INFO] - Training Epoch: 2/2, step 3914/7134 completed (loss: 0.030455859377980232, acc: 0.9865319728851318)
[2025-02-13 04:11:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:38][root][INFO] - Training Epoch: 2/2, step 3915/7134 completed (loss: 0.011847647838294506, acc: 0.994358241558075)
[2025-02-13 04:11:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:38][root][INFO] - Training Epoch: 2/2, step 3916/7134 completed (loss: 0.01421231310814619, acc: 0.9949044585227966)
[2025-02-13 04:11:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:39][root][INFO] - Training Epoch: 2/2, step 3917/7134 completed (loss: 0.009029769338667393, acc: 0.9987966418266296)
[2025-02-13 04:11:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:39][root][INFO] - Training Epoch: 2/2, step 3918/7134 completed (loss: 0.027264101430773735, acc: 0.9934895634651184)
[2025-02-13 04:11:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:39][root][INFO] - Training Epoch: 2/2, step 3919/7134 completed (loss: 0.04861147701740265, acc: 0.9885993599891663)
[2025-02-13 04:11:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:40][root][INFO] - Training Epoch: 2/2, step 3920/7134 completed (loss: 0.030602436512708664, acc: 0.9875389337539673)
[2025-02-13 04:11:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:40][root][INFO] - Training Epoch: 2/2, step 3921/7134 completed (loss: 0.01977991871535778, acc: 0.9940000176429749)
[2025-02-13 04:11:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:41][root][INFO] - Training Epoch: 2/2, step 3922/7134 completed (loss: 0.06348828971385956, acc: 0.9706498980522156)
[2025-02-13 04:11:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:41][root][INFO] - Training Epoch: 2/2, step 3923/7134 completed (loss: 0.07845728099346161, acc: 0.9863013625144958)
[2025-02-13 04:11:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:42][root][INFO] - Training Epoch: 2/2, step 3924/7134 completed (loss: 0.027565792202949524, acc: 0.9942528605461121)
[2025-02-13 04:11:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:42][root][INFO] - Training Epoch: 2/2, step 3925/7134 completed (loss: 0.04304364696145058, acc: 0.9868228435516357)
[2025-02-13 04:11:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:42][root][INFO] - Training Epoch: 2/2, step 3926/7134 completed (loss: 0.05054759979248047, acc: 0.9884892106056213)
[2025-02-13 04:11:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:43][root][INFO] - Training Epoch: 2/2, step 3927/7134 completed (loss: 0.021728016436100006, acc: 0.9917469024658203)
[2025-02-13 04:11:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:43][root][INFO] - Training Epoch: 2/2, step 3928/7134 completed (loss: 0.04632297158241272, acc: 0.9856321811676025)
[2025-02-13 04:11:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:44][root][INFO] - Training Epoch: 2/2, step 3929/7134 completed (loss: 0.030266929417848587, acc: 0.9904610514640808)
[2025-02-13 04:11:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:44][root][INFO] - Training Epoch: 2/2, step 3930/7134 completed (loss: 0.02361094392836094, acc: 0.9894958138465881)
[2025-02-13 04:11:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:45][root][INFO] - Training Epoch: 2/2, step 3931/7134 completed (loss: 0.010486640967428684, acc: 0.9970104694366455)
[2025-02-13 04:11:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:45][root][INFO] - Training Epoch: 2/2, step 3932/7134 completed (loss: 0.02613171376287937, acc: 0.9822134375572205)
[2025-02-13 04:11:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:45][root][INFO] - Training Epoch: 2/2, step 3933/7134 completed (loss: 0.010978281497955322, acc: 0.9934210777282715)
[2025-02-13 04:11:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:46][root][INFO] - Training Epoch: 2/2, step 3934/7134 completed (loss: 0.013468853197991848, acc: 0.9953161478042603)
[2025-02-13 04:11:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:46][root][INFO] - Training Epoch: 2/2, step 3935/7134 completed (loss: 0.005574787501245737, acc: 1.0)
[2025-02-13 04:11:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:47][root][INFO] - Training Epoch: 2/2, step 3936/7134 completed (loss: 0.03439939767122269, acc: 0.9867924451828003)
[2025-02-13 04:11:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:47][root][INFO] - Training Epoch: 2/2, step 3937/7134 completed (loss: 0.03184030205011368, acc: 0.9897959232330322)
[2025-02-13 04:11:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:47][root][INFO] - Training Epoch: 2/2, step 3938/7134 completed (loss: 0.009701281785964966, acc: 0.9977375268936157)
[2025-02-13 04:11:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:48][root][INFO] - Training Epoch: 2/2, step 3939/7134 completed (loss: 0.05516960471868515, acc: 0.9796954393386841)
[2025-02-13 04:11:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:48][root][INFO] - Training Epoch: 2/2, step 3940/7134 completed (loss: 0.030206363648176193, acc: 0.9874371886253357)
[2025-02-13 04:11:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:49][root][INFO] - Training Epoch: 2/2, step 3941/7134 completed (loss: 0.02014300785958767, acc: 0.9939024448394775)
[2025-02-13 04:11:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:49][root][INFO] - Training Epoch: 2/2, step 3942/7134 completed (loss: 0.015054836869239807, acc: 0.993852436542511)
[2025-02-13 04:11:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:49][root][INFO] - Training Epoch: 2/2, step 3943/7134 completed (loss: 0.012116046622395515, acc: 0.9961389899253845)
[2025-02-13 04:11:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:50][root][INFO] - Training Epoch: 2/2, step 3944/7134 completed (loss: 0.02172401174902916, acc: 0.9944649338722229)
[2025-02-13 04:11:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:50][root][INFO] - Training Epoch: 2/2, step 3945/7134 completed (loss: 0.010362138040363789, acc: 0.9954268336296082)
[2025-02-13 04:11:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:51][root][INFO] - Training Epoch: 2/2, step 3946/7134 completed (loss: 0.010558290407061577, acc: 0.9958246350288391)
[2025-02-13 04:11:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:51][root][INFO] - Training Epoch: 2/2, step 3947/7134 completed (loss: 0.01226309034973383, acc: 0.9922279715538025)
[2025-02-13 04:11:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:51][root][INFO] - Training Epoch: 2/2, step 3948/7134 completed (loss: 0.005999553017318249, acc: 0.9974874258041382)
[2025-02-13 04:11:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:52][root][INFO] - Training Epoch: 2/2, step 3949/7134 completed (loss: 0.021321754902601242, acc: 0.9944030046463013)
[2025-02-13 04:11:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:52][root][INFO] - Training Epoch: 2/2, step 3950/7134 completed (loss: 0.024597832933068275, acc: 0.9955357313156128)
[2025-02-13 04:11:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:53][root][INFO] - Training Epoch: 2/2, step 3951/7134 completed (loss: 0.02265436202287674, acc: 0.9924812316894531)
[2025-02-13 04:11:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:53][root][INFO] - Training Epoch: 2/2, step 3952/7134 completed (loss: 0.006520658731460571, acc: 0.9974358677864075)
[2025-02-13 04:11:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:53][root][INFO] - Training Epoch: 2/2, step 3953/7134 completed (loss: 0.03465469926595688, acc: 0.9943073987960815)
[2025-02-13 04:11:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:54][root][INFO] - Training Epoch: 2/2, step 3954/7134 completed (loss: 0.0226882491260767, acc: 0.9922680258750916)
[2025-02-13 04:11:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:54][root][INFO] - Training Epoch: 2/2, step 3955/7134 completed (loss: 0.025347532704472542, acc: 0.9881955981254578)
[2025-02-13 04:11:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:55][root][INFO] - Training Epoch: 2/2, step 3956/7134 completed (loss: 0.008887605741620064, acc: 0.9983999729156494)
[2025-02-13 04:11:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:55][root][INFO] - Training Epoch: 2/2, step 3957/7134 completed (loss: 0.021006794646382332, acc: 0.9887387156486511)
[2025-02-13 04:11:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:56][root][INFO] - Training Epoch: 2/2, step 3958/7134 completed (loss: 0.03724657744169235, acc: 0.9839743375778198)
[2025-02-13 04:11:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:56][root][INFO] - Training Epoch: 2/2, step 3959/7134 completed (loss: 0.006407657638192177, acc: 1.0)
[2025-02-13 04:11:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:56][root][INFO] - Training Epoch: 2/2, step 3960/7134 completed (loss: 0.03284769877791405, acc: 0.9914529919624329)
[2025-02-13 04:11:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:57][root][INFO] - Training Epoch: 2/2, step 3961/7134 completed (loss: 0.01085832342505455, acc: 0.995502233505249)
[2025-02-13 04:11:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:57][root][INFO] - Training Epoch: 2/2, step 3962/7134 completed (loss: 0.01024092361330986, acc: 0.9951456189155579)
[2025-02-13 04:11:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:58][root][INFO] - Training Epoch: 2/2, step 3963/7134 completed (loss: 0.061184339225292206, acc: 0.9868035316467285)
[2025-02-13 04:11:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:58][root][INFO] - Training Epoch: 2/2, step 3964/7134 completed (loss: 0.024916641414165497, acc: 0.9925925731658936)
[2025-02-13 04:11:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:59][root][INFO] - Training Epoch: 2/2, step 3965/7134 completed (loss: 0.02032470889389515, acc: 0.9942775368690491)
[2025-02-13 04:11:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:59][root][INFO] - Training Epoch: 2/2, step 3966/7134 completed (loss: 0.013441142626106739, acc: 0.994584858417511)
[2025-02-13 04:11:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:11:59][root][INFO] - Training Epoch: 2/2, step 3967/7134 completed (loss: 0.07587023079395294, acc: 0.984402060508728)
[2025-02-13 04:12:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:00][root][INFO] - Training Epoch: 2/2, step 3968/7134 completed (loss: 0.010114758275449276, acc: 0.9947984218597412)
[2025-02-13 04:12:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:00][root][INFO] - Training Epoch: 2/2, step 3969/7134 completed (loss: 0.021899333223700523, acc: 0.9895287752151489)
[2025-02-13 04:12:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:01][root][INFO] - Training Epoch: 2/2, step 3970/7134 completed (loss: 0.053323034197092056, acc: 0.9913669228553772)
[2025-02-13 04:12:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:01][root][INFO] - Training Epoch: 2/2, step 3971/7134 completed (loss: 0.10181578993797302, acc: 0.9795396327972412)
[2025-02-13 04:12:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:01][root][INFO] - Training Epoch: 2/2, step 3972/7134 completed (loss: 0.11257417500019073, acc: 0.9742268323898315)
[2025-02-13 04:12:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:02][root][INFO] - Training Epoch: 2/2, step 3973/7134 completed (loss: 0.04234016686677933, acc: 0.9879102110862732)
[2025-02-13 04:12:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:02][root][INFO] - Training Epoch: 2/2, step 3974/7134 completed (loss: 0.08496052026748657, acc: 0.9756097793579102)
[2025-02-13 04:12:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:03][root][INFO] - Training Epoch: 2/2, step 3975/7134 completed (loss: 0.01881726272404194, acc: 0.9939576983451843)
[2025-02-13 04:12:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:03][root][INFO] - Training Epoch: 2/2, step 3976/7134 completed (loss: 0.0816330760717392, acc: 0.982300877571106)
[2025-02-13 04:12:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:04][root][INFO] - Training Epoch: 2/2, step 3977/7134 completed (loss: 0.02369849756360054, acc: 0.9930434823036194)
[2025-02-13 04:12:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:04][root][INFO] - Training Epoch: 2/2, step 3978/7134 completed (loss: 0.010209976695477962, acc: 0.9974126815795898)
[2025-02-13 04:12:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:05][root][INFO] - Training Epoch: 2/2, step 3979/7134 completed (loss: 0.04163004085421562, acc: 0.9890710115432739)
[2025-02-13 04:12:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:05][root][INFO] - Training Epoch: 2/2, step 3980/7134 completed (loss: 0.021668769419193268, acc: 0.9923780560493469)
[2025-02-13 04:12:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:05][root][INFO] - Training Epoch: 2/2, step 3981/7134 completed (loss: 0.055624742060899734, acc: 0.9862385392189026)
[2025-02-13 04:12:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:06][root][INFO] - Training Epoch: 2/2, step 3982/7134 completed (loss: 0.05985592305660248, acc: 0.9885222315788269)
[2025-02-13 04:12:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:06][root][INFO] - Training Epoch: 2/2, step 3983/7134 completed (loss: 0.18702057003974915, acc: 0.957446813583374)
[2025-02-13 04:12:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:07][root][INFO] - Training Epoch: 2/2, step 3984/7134 completed (loss: 0.06365389376878738, acc: 0.9857142567634583)
[2025-02-13 04:12:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:07][root][INFO] - Training Epoch: 2/2, step 3985/7134 completed (loss: 0.018843047320842743, acc: 0.9946523904800415)
[2025-02-13 04:12:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:08][root][INFO] - Training Epoch: 2/2, step 3986/7134 completed (loss: 0.13179545104503632, acc: 0.9696969985961914)
[2025-02-13 04:12:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:08][root][INFO] - Training Epoch: 2/2, step 3987/7134 completed (loss: 0.04350563883781433, acc: 0.9904030561447144)
[2025-02-13 04:12:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:08][root][INFO] - Training Epoch: 2/2, step 3988/7134 completed (loss: 0.06005463749170303, acc: 0.9873577952384949)
[2025-02-13 04:12:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:09][root][INFO] - Training Epoch: 2/2, step 3989/7134 completed (loss: 0.10096035897731781, acc: 0.9786780476570129)
[2025-02-13 04:12:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:09][root][INFO] - Training Epoch: 2/2, step 3990/7134 completed (loss: 0.027639806270599365, acc: 0.9917080998420715)
[2025-02-13 04:12:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:10][root][INFO] - Training Epoch: 2/2, step 3991/7134 completed (loss: 0.028077950701117516, acc: 0.9933993220329285)
[2025-02-13 04:12:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:10][root][INFO] - Training Epoch: 2/2, step 3992/7134 completed (loss: 0.03329257294535637, acc: 0.9918919205665588)
[2025-02-13 04:12:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:11][root][INFO] - Training Epoch: 2/2, step 3993/7134 completed (loss: 0.039793405681848526, acc: 0.9908257126808167)
[2025-02-13 04:12:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:11][root][INFO] - Training Epoch: 2/2, step 3994/7134 completed (loss: 0.04752940312027931, acc: 0.9904761910438538)
[2025-02-13 04:12:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:11][root][INFO] - Training Epoch: 2/2, step 3995/7134 completed (loss: 0.07512607425451279, acc: 0.9825581312179565)
[2025-02-13 04:12:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:12][root][INFO] - Training Epoch: 2/2, step 3996/7134 completed (loss: 0.027994534000754356, acc: 0.9938398599624634)
[2025-02-13 04:12:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:12][root][INFO] - Training Epoch: 2/2, step 3997/7134 completed (loss: 0.1148480772972107, acc: 0.9758241772651672)
[2025-02-13 04:12:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:12][root][INFO] - Training Epoch: 2/2, step 3998/7134 completed (loss: 0.05932769924402237, acc: 0.9846827387809753)
[2025-02-13 04:12:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:13][root][INFO] - Training Epoch: 2/2, step 3999/7134 completed (loss: 0.04568495973944664, acc: 0.9840319156646729)
[2025-02-13 04:12:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:13][root][INFO] - Training Epoch: 2/2, step 4000/7134 completed (loss: 0.02878100797533989, acc: 0.9942085146903992)
[2025-02-13 04:12:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:14][root][INFO] - Training Epoch: 2/2, step 4001/7134 completed (loss: 0.052186738699674606, acc: 0.9840954542160034)
[2025-02-13 04:12:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:14][root][INFO] - Training Epoch: 2/2, step 4002/7134 completed (loss: 0.06553792953491211, acc: 0.9855421781539917)
[2025-02-13 04:12:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:14][root][INFO] - Training Epoch: 2/2, step 4003/7134 completed (loss: 0.07338718324899673, acc: 0.9797047972679138)
[2025-02-13 04:12:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:15][root][INFO] - Training Epoch: 2/2, step 4004/7134 completed (loss: 0.030400056391954422, acc: 0.9930070042610168)
[2025-02-13 04:12:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:15][root][INFO] - Training Epoch: 2/2, step 4005/7134 completed (loss: 0.006596916355192661, acc: 1.0)
[2025-02-13 04:12:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:16][root][INFO] - Training Epoch: 2/2, step 4006/7134 completed (loss: 0.01460547000169754, acc: 0.9979188442230225)
[2025-02-13 04:12:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:16][root][INFO] - Training Epoch: 2/2, step 4007/7134 completed (loss: 0.026167264208197594, acc: 0.9925037622451782)
[2025-02-13 04:12:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:17][root][INFO] - Training Epoch: 2/2, step 4008/7134 completed (loss: 0.01380757987499237, acc: 0.9957716464996338)
[2025-02-13 04:12:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:17][root][INFO] - Training Epoch: 2/2, step 4009/7134 completed (loss: 0.01671781577169895, acc: 0.9946737885475159)
[2025-02-13 04:12:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:18][root][INFO] - Training Epoch: 2/2, step 4010/7134 completed (loss: 0.016978824511170387, acc: 0.9977628588676453)
[2025-02-13 04:12:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:18][root][INFO] - Training Epoch: 2/2, step 4011/7134 completed (loss: 0.02867194637656212, acc: 0.9933599233627319)
[2025-02-13 04:12:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:18][root][INFO] - Training Epoch: 2/2, step 4012/7134 completed (loss: 0.010495676659047604, acc: 0.9957627058029175)
[2025-02-13 04:12:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:19][root][INFO] - Training Epoch: 2/2, step 4013/7134 completed (loss: 0.005279574543237686, acc: 1.0)
[2025-02-13 04:12:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:19][root][INFO] - Training Epoch: 2/2, step 4014/7134 completed (loss: 0.017697159200906754, acc: 0.9952977895736694)
[2025-02-13 04:12:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:20][root][INFO] - Training Epoch: 2/2, step 4015/7134 completed (loss: 0.020644202828407288, acc: 0.996927797794342)
[2025-02-13 04:12:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:20][root][INFO] - Training Epoch: 2/2, step 4016/7134 completed (loss: 0.025476254522800446, acc: 0.9910714030265808)
[2025-02-13 04:12:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:21][root][INFO] - Training Epoch: 2/2, step 4017/7134 completed (loss: 0.023337537422776222, acc: 0.9965357780456543)
[2025-02-13 04:12:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:21][root][INFO] - Training Epoch: 2/2, step 4018/7134 completed (loss: 0.012953232042491436, acc: 0.996363639831543)
[2025-02-13 04:12:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:22][root][INFO] - Training Epoch: 2/2, step 4019/7134 completed (loss: 0.03842877224087715, acc: 0.989847719669342)
[2025-02-13 04:12:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:22][root][INFO] - Training Epoch: 2/2, step 4020/7134 completed (loss: 0.030678454786539078, acc: 0.9877049326896667)
[2025-02-13 04:12:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:22][root][INFO] - Training Epoch: 2/2, step 4021/7134 completed (loss: 0.013434645719826221, acc: 0.9928698539733887)
[2025-02-13 04:12:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:23][root][INFO] - Training Epoch: 2/2, step 4022/7134 completed (loss: 0.005075611639767885, acc: 0.9975460171699524)
[2025-02-13 04:12:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:23][root][INFO] - Training Epoch: 2/2, step 4023/7134 completed (loss: 0.020482076331973076, acc: 0.9911949634552002)
[2025-02-13 04:12:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:24][root][INFO] - Training Epoch: 2/2, step 4024/7134 completed (loss: 0.02890603430569172, acc: 0.9881889820098877)
[2025-02-13 04:12:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:24][root][INFO] - Training Epoch: 2/2, step 4025/7134 completed (loss: 0.040538184344768524, acc: 0.9898074865341187)
[2025-02-13 04:12:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:25][root][INFO] - Training Epoch: 2/2, step 4026/7134 completed (loss: 0.07816221565008163, acc: 0.982300877571106)
[2025-02-13 04:12:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:25][root][INFO] - Training Epoch: 2/2, step 4027/7134 completed (loss: 0.03483305126428604, acc: 0.9880095720291138)
[2025-02-13 04:12:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:26][root][INFO] - Training Epoch: 2/2, step 4028/7134 completed (loss: 0.039069000631570816, acc: 0.9931153059005737)
[2025-02-13 04:12:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:26][root][INFO] - Training Epoch: 2/2, step 4029/7134 completed (loss: 0.02005246840417385, acc: 0.9913899302482605)
[2025-02-13 04:12:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:26][root][INFO] - Training Epoch: 2/2, step 4030/7134 completed (loss: 0.005316772032529116, acc: 0.9979466199874878)
[2025-02-13 04:12:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:27][root][INFO] - Training Epoch: 2/2, step 4031/7134 completed (loss: 0.00819477904587984, acc: 0.9970674514770508)
[2025-02-13 04:12:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:27][root][INFO] - Training Epoch: 2/2, step 4032/7134 completed (loss: 0.05090472474694252, acc: 0.9839357137680054)
[2025-02-13 04:12:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:28][root][INFO] - Training Epoch: 2/2, step 4033/7134 completed (loss: 0.006627832539379597, acc: 0.9987421631813049)
[2025-02-13 04:12:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:28][root][INFO] - Training Epoch: 2/2, step 4034/7134 completed (loss: 0.01186610572040081, acc: 0.9963459372520447)
[2025-02-13 04:12:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:29][root][INFO] - Training Epoch: 2/2, step 4035/7134 completed (loss: 0.0216669999063015, acc: 0.995277464389801)
[2025-02-13 04:12:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:29][root][INFO] - Training Epoch: 2/2, step 4036/7134 completed (loss: 0.03321485593914986, acc: 0.9908424615859985)
[2025-02-13 04:12:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:30][root][INFO] - Training Epoch: 2/2, step 4037/7134 completed (loss: 0.010117869824171066, acc: 0.9975932836532593)
[2025-02-13 04:12:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:30][root][INFO] - Training Epoch: 2/2, step 4038/7134 completed (loss: 0.03347465768456459, acc: 0.9903714060783386)
[2025-02-13 04:12:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:30][root][INFO] - Training Epoch: 2/2, step 4039/7134 completed (loss: 0.03259619325399399, acc: 0.9900709390640259)
[2025-02-13 04:12:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:31][root][INFO] - Training Epoch: 2/2, step 4040/7134 completed (loss: 0.007180217653512955, acc: 0.9975369572639465)
[2025-02-13 04:12:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:31][root][INFO] - Training Epoch: 2/2, step 4041/7134 completed (loss: 0.03480600193142891, acc: 0.9905914068222046)
[2025-02-13 04:12:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:32][root][INFO] - Training Epoch: 2/2, step 4042/7134 completed (loss: 0.019138628616929054, acc: 0.9926470518112183)
[2025-02-13 04:12:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:32][root][INFO] - Training Epoch: 2/2, step 4043/7134 completed (loss: 0.014693343080580235, acc: 0.9926578402519226)
[2025-02-13 04:12:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:33][root][INFO] - Training Epoch: 2/2, step 4044/7134 completed (loss: 0.007818005979061127, acc: 0.9949495196342468)
[2025-02-13 04:12:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:33][root][INFO] - Training Epoch: 2/2, step 4045/7134 completed (loss: 0.03385981544852257, acc: 0.9916666746139526)
[2025-02-13 04:12:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:33][root][INFO] - Training Epoch: 2/2, step 4046/7134 completed (loss: 0.027984576299786568, acc: 0.9917582273483276)
[2025-02-13 04:12:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:34][root][INFO] - Training Epoch: 2/2, step 4047/7134 completed (loss: 0.035136885941028595, acc: 0.9940333962440491)
[2025-02-13 04:12:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:34][root][INFO] - Training Epoch: 2/2, step 4048/7134 completed (loss: 0.01748623326420784, acc: 0.9953380227088928)
[2025-02-13 04:12:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:35][root][INFO] - Training Epoch: 2/2, step 4049/7134 completed (loss: 0.02512551285326481, acc: 0.9931034445762634)
[2025-02-13 04:12:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:35][root][INFO] - Training Epoch: 2/2, step 4050/7134 completed (loss: 0.027383195236325264, acc: 0.9914893507957458)
[2025-02-13 04:12:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:36][root][INFO] - Training Epoch: 2/2, step 4051/7134 completed (loss: 0.05644012987613678, acc: 0.9928229451179504)
[2025-02-13 04:12:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:36][root][INFO] - Training Epoch: 2/2, step 4052/7134 completed (loss: 0.017016373574733734, acc: 0.9935400485992432)
[2025-02-13 04:12:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:37][root][INFO] - Training Epoch: 2/2, step 4053/7134 completed (loss: 0.030287573114037514, acc: 0.9893898963928223)
[2025-02-13 04:12:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:37][root][INFO] - Training Epoch: 2/2, step 4054/7134 completed (loss: 0.029186490923166275, acc: 0.9959999918937683)
[2025-02-13 04:12:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:37][root][INFO] - Training Epoch: 2/2, step 4055/7134 completed (loss: 0.020599082112312317, acc: 0.9902912378311157)
[2025-02-13 04:12:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:38][root][INFO] - Training Epoch: 2/2, step 4056/7134 completed (loss: 0.016014670953154564, acc: 0.996363639831543)
[2025-02-13 04:12:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:38][root][INFO] - Training Epoch: 2/2, step 4057/7134 completed (loss: 0.03549816831946373, acc: 0.9916550517082214)
[2025-02-13 04:12:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:39][root][INFO] - Training Epoch: 2/2, step 4058/7134 completed (loss: 0.030886415392160416, acc: 0.9847036600112915)
[2025-02-13 04:12:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:39][root][INFO] - Training Epoch: 2/2, step 4059/7134 completed (loss: 0.029958464205265045, acc: 0.9908758997917175)
[2025-02-13 04:12:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:40][root][INFO] - Training Epoch: 2/2, step 4060/7134 completed (loss: 0.014561458490788937, acc: 0.9938176274299622)
[2025-02-13 04:12:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:40][root][INFO] - Training Epoch: 2/2, step 4061/7134 completed (loss: 0.018710890784859657, acc: 0.994301974773407)
[2025-02-13 04:12:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:40][root][INFO] - Training Epoch: 2/2, step 4062/7134 completed (loss: 0.004636690486222506, acc: 1.0)
[2025-02-13 04:12:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:41][root][INFO] - Training Epoch: 2/2, step 4063/7134 completed (loss: 0.00926897767931223, acc: 0.9964664578437805)
[2025-02-13 04:12:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:41][root][INFO] - Training Epoch: 2/2, step 4064/7134 completed (loss: 0.00856949482113123, acc: 0.9969742894172668)
[2025-02-13 04:12:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:42][root][INFO] - Training Epoch: 2/2, step 4065/7134 completed (loss: 0.015526501461863518, acc: 0.9972413778305054)
[2025-02-13 04:12:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:42][root][INFO] - Training Epoch: 2/2, step 4066/7134 completed (loss: 0.005156930070370436, acc: 0.9986110925674438)
[2025-02-13 04:12:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:43][root][INFO] - Training Epoch: 2/2, step 4067/7134 completed (loss: 0.025999601930379868, acc: 0.9950980544090271)
[2025-02-13 04:12:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:43][root][INFO] - Training Epoch: 2/2, step 4068/7134 completed (loss: 0.03221229836344719, acc: 0.9944444298744202)
[2025-02-13 04:12:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:43][root][INFO] - Training Epoch: 2/2, step 4069/7134 completed (loss: 0.017727186903357506, acc: 0.9953051805496216)
[2025-02-13 04:12:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:44][root][INFO] - Training Epoch: 2/2, step 4070/7134 completed (loss: 0.016583630815148354, acc: 0.9954338073730469)
[2025-02-13 04:12:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:44][root][INFO] - Training Epoch: 2/2, step 4071/7134 completed (loss: 0.018709415569901466, acc: 0.9917627573013306)
[2025-02-13 04:12:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:45][root][INFO] - Training Epoch: 2/2, step 4072/7134 completed (loss: 0.00840869639068842, acc: 0.9985569715499878)
[2025-02-13 04:12:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:45][root][INFO] - Training Epoch: 2/2, step 4073/7134 completed (loss: 0.011090526357293129, acc: 0.9956896305084229)
[2025-02-13 04:12:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:45][root][INFO] - Training Epoch: 2/2, step 4074/7134 completed (loss: 0.016348211094737053, acc: 0.9942775368690491)
[2025-02-13 04:12:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:46][root][INFO] - Training Epoch: 2/2, step 4075/7134 completed (loss: 0.006161433178931475, acc: 0.995502233505249)
[2025-02-13 04:12:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:46][root][INFO] - Training Epoch: 2/2, step 4076/7134 completed (loss: 0.008171945810317993, acc: 0.9971056580543518)
[2025-02-13 04:12:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:47][root][INFO] - Training Epoch: 2/2, step 4077/7134 completed (loss: 0.025476636365056038, acc: 0.9915397763252258)
[2025-02-13 04:12:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:47][root][INFO] - Training Epoch: 2/2, step 4078/7134 completed (loss: 0.01388491503894329, acc: 0.995726466178894)
[2025-02-13 04:12:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:48][root][INFO] - Training Epoch: 2/2, step 4079/7134 completed (loss: 0.015729445964097977, acc: 0.9946523904800415)
[2025-02-13 04:12:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:48][root][INFO] - Training Epoch: 2/2, step 4080/7134 completed (loss: 0.0064153182320296764, acc: 0.996835470199585)
[2025-02-13 04:12:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:48][root][INFO] - Training Epoch: 2/2, step 4081/7134 completed (loss: 0.0041297925636172295, acc: 0.9983498454093933)
[2025-02-13 04:12:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:49][root][INFO] - Training Epoch: 2/2, step 4082/7134 completed (loss: 0.03837212920188904, acc: 0.9870610237121582)
[2025-02-13 04:12:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:49][root][INFO] - Training Epoch: 2/2, step 4083/7134 completed (loss: 0.04831107333302498, acc: 0.985029935836792)
[2025-02-13 04:12:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:50][root][INFO] - Training Epoch: 2/2, step 4084/7134 completed (loss: 0.06446930021047592, acc: 0.9803921580314636)
[2025-02-13 04:12:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:50][root][INFO] - Training Epoch: 2/2, step 4085/7134 completed (loss: 0.026209209114313126, acc: 0.9905660152435303)
[2025-02-13 04:12:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:51][root][INFO] - Training Epoch: 2/2, step 4086/7134 completed (loss: 0.04771382361650467, acc: 0.9852700233459473)
[2025-02-13 04:12:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:51][root][INFO] - Training Epoch: 2/2, step 4087/7134 completed (loss: 0.02349064126610756, acc: 0.9974160194396973)
[2025-02-13 04:12:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:51][root][INFO] - Training Epoch: 2/2, step 4088/7134 completed (loss: 0.053346898406744, acc: 0.9825870394706726)
[2025-02-13 04:12:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:52][root][INFO] - Training Epoch: 2/2, step 4089/7134 completed (loss: 0.04988076165318489, acc: 0.9857904314994812)
[2025-02-13 04:12:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:52][root][INFO] - Training Epoch: 2/2, step 4090/7134 completed (loss: 0.08600779622793198, acc: 0.9788918495178223)
[2025-02-13 04:12:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:53][root][INFO] - Training Epoch: 2/2, step 4091/7134 completed (loss: 0.10681377351284027, acc: 0.9730185270309448)
[2025-02-13 04:12:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:53][root][INFO] - Training Epoch: 2/2, step 4092/7134 completed (loss: 0.040023379027843475, acc: 0.992277979850769)
[2025-02-13 04:12:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:54][root][INFO] - Training Epoch: 2/2, step 4093/7134 completed (loss: 0.01855641044676304, acc: 0.994535505771637)
[2025-02-13 04:12:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:54][root][INFO] - Training Epoch: 2/2, step 4094/7134 completed (loss: 0.04804008826613426, acc: 0.9879879951477051)
[2025-02-13 04:12:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:54][root][INFO] - Training Epoch: 2/2, step 4095/7134 completed (loss: 0.018554672598838806, acc: 0.9956584572792053)
[2025-02-13 04:12:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:55][root][INFO] - Training Epoch: 2/2, step 4096/7134 completed (loss: 0.04522150382399559, acc: 0.9910447597503662)
[2025-02-13 04:12:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:55][root][INFO] - Training Epoch: 2/2, step 4097/7134 completed (loss: 0.053741343319416046, acc: 0.9887096881866455)
[2025-02-13 04:12:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:56][root][INFO] - Training Epoch: 2/2, step 4098/7134 completed (loss: 0.030412405729293823, acc: 0.9945799708366394)
[2025-02-13 04:12:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:56][root][INFO] - Training Epoch: 2/2, step 4099/7134 completed (loss: 0.03367020562291145, acc: 0.9911971688270569)
[2025-02-13 04:12:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:57][root][INFO] - Training Epoch: 2/2, step 4100/7134 completed (loss: 0.0061200461350381374, acc: 0.998487114906311)
[2025-02-13 04:12:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:57][root][INFO] - Training Epoch: 2/2, step 4101/7134 completed (loss: 0.012861285358667374, acc: 0.9942965507507324)
[2025-02-13 04:12:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:57][root][INFO] - Training Epoch: 2/2, step 4102/7134 completed (loss: 0.015269183553755283, acc: 0.9963436722755432)
[2025-02-13 04:12:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:58][root][INFO] - Training Epoch: 2/2, step 4103/7134 completed (loss: 0.003972697537392378, acc: 1.0)
[2025-02-13 04:12:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:58][root][INFO] - Training Epoch: 2/2, step 4104/7134 completed (loss: 0.005242552608251572, acc: 1.0)
[2025-02-13 04:12:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:59][root][INFO] - Training Epoch: 2/2, step 4105/7134 completed (loss: 0.015094438567757607, acc: 0.9969788789749146)
[2025-02-13 04:12:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:12:59][root][INFO] - Training Epoch: 2/2, step 4106/7134 completed (loss: 0.02582545392215252, acc: 0.9938650131225586)
[2025-02-13 04:12:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:00][root][INFO] - Training Epoch: 2/2, step 4107/7134 completed (loss: 0.029818328097462654, acc: 0.9932773113250732)
[2025-02-13 04:13:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:00][root][INFO] - Training Epoch: 2/2, step 4108/7134 completed (loss: 0.01876653917133808, acc: 0.9935275316238403)
[2025-02-13 04:13:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:00][root][INFO] - Training Epoch: 2/2, step 4109/7134 completed (loss: 0.010964082553982735, acc: 0.9983948469161987)
[2025-02-13 04:13:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:01][root][INFO] - Training Epoch: 2/2, step 4110/7134 completed (loss: 0.0025272800121456385, acc: 1.0)
[2025-02-13 04:13:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:01][root][INFO] - Training Epoch: 2/2, step 4111/7134 completed (loss: 0.011937478557229042, acc: 0.9984227418899536)
[2025-02-13 04:13:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:02][root][INFO] - Training Epoch: 2/2, step 4112/7134 completed (loss: 0.020486220717430115, acc: 0.9964157938957214)
[2025-02-13 04:13:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:02][root][INFO] - Training Epoch: 2/2, step 4113/7134 completed (loss: 0.017968883737921715, acc: 0.9937984347343445)
[2025-02-13 04:13:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:03][root][INFO] - Training Epoch: 2/2, step 4114/7134 completed (loss: 0.07763489335775375, acc: 0.9796748161315918)
[2025-02-13 04:13:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:03][root][INFO] - Training Epoch: 2/2, step 4115/7134 completed (loss: 0.031148355454206467, acc: 0.9888059496879578)
[2025-02-13 04:13:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:03][root][INFO] - Training Epoch: 2/2, step 4116/7134 completed (loss: 0.004820927046239376, acc: 1.0)
[2025-02-13 04:13:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:04][root][INFO] - Training Epoch: 2/2, step 4117/7134 completed (loss: 0.010181767866015434, acc: 0.9968454241752625)
[2025-02-13 04:13:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:04][root][INFO] - Training Epoch: 2/2, step 4118/7134 completed (loss: 0.020054835826158524, acc: 0.9926289916038513)
[2025-02-13 04:13:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:05][root][INFO] - Training Epoch: 2/2, step 4119/7134 completed (loss: 0.04009581729769707, acc: 0.9903069734573364)
[2025-02-13 04:13:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:05][root][INFO] - Training Epoch: 2/2, step 4120/7134 completed (loss: 0.028931597247719765, acc: 0.9950576424598694)
[2025-02-13 04:13:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:05][root][INFO] - Training Epoch: 2/2, step 4121/7134 completed (loss: 0.037153229117393494, acc: 0.9928977489471436)
[2025-02-13 04:13:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:06][root][INFO] - Training Epoch: 2/2, step 4122/7134 completed (loss: 0.007790210656821728, acc: 0.9980915784835815)
[2025-02-13 04:13:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:06][root][INFO] - Training Epoch: 2/2, step 4123/7134 completed (loss: 0.0232224240899086, acc: 0.993966817855835)
[2025-02-13 04:13:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:07][root][INFO] - Training Epoch: 2/2, step 4124/7134 completed (loss: 0.023164773359894753, acc: 0.9929278492927551)
[2025-02-13 04:13:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:07][root][INFO] - Training Epoch: 2/2, step 4125/7134 completed (loss: 0.01845533773303032, acc: 0.9955882430076599)
[2025-02-13 04:13:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:08][root][INFO] - Training Epoch: 2/2, step 4126/7134 completed (loss: 0.009550857357680798, acc: 0.9985835552215576)
[2025-02-13 04:13:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:08][root][INFO] - Training Epoch: 2/2, step 4127/7134 completed (loss: 0.02465161494910717, acc: 0.9921875)
[2025-02-13 04:13:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:08][root][INFO] - Training Epoch: 2/2, step 4128/7134 completed (loss: 0.017950844019651413, acc: 0.9968701004981995)
[2025-02-13 04:13:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:09][root][INFO] - Training Epoch: 2/2, step 4129/7134 completed (loss: 0.014126526191830635, acc: 0.9925816059112549)
[2025-02-13 04:13:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:09][root][INFO] - Training Epoch: 2/2, step 4130/7134 completed (loss: 0.015335043892264366, acc: 0.9980732202529907)
[2025-02-13 04:13:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:10][root][INFO] - Training Epoch: 2/2, step 4131/7134 completed (loss: 0.013351138681173325, acc: 0.996303141117096)
[2025-02-13 04:13:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:10][root][INFO] - Training Epoch: 2/2, step 4132/7134 completed (loss: 0.012110201641917229, acc: 0.9955357313156128)
[2025-02-13 04:13:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:11][root][INFO] - Training Epoch: 2/2, step 4133/7134 completed (loss: 0.04223412275314331, acc: 0.9888888597488403)
[2025-02-13 04:13:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:11][root][INFO] - Training Epoch: 2/2, step 4134/7134 completed (loss: 0.04079500958323479, acc: 0.9810426831245422)
[2025-02-13 04:13:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:11][root][INFO] - Training Epoch: 2/2, step 4135/7134 completed (loss: 0.02032606303691864, acc: 0.9928951859474182)
[2025-02-13 04:13:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:12][root][INFO] - Training Epoch: 2/2, step 4136/7134 completed (loss: 0.00861768051981926, acc: 0.9982300996780396)
[2025-02-13 04:13:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:12][root][INFO] - Training Epoch: 2/2, step 4137/7134 completed (loss: 0.01946212165057659, acc: 0.990234375)
[2025-02-13 04:13:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:13][root][INFO] - Training Epoch: 2/2, step 4138/7134 completed (loss: 0.023225659504532814, acc: 0.9921630024909973)
[2025-02-13 04:13:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:13][root][INFO] - Training Epoch: 2/2, step 4139/7134 completed (loss: 0.01205521635711193, acc: 0.9978540539741516)
[2025-02-13 04:13:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:13][root][INFO] - Training Epoch: 2/2, step 4140/7134 completed (loss: 0.03568962588906288, acc: 0.9931034445762634)
[2025-02-13 04:13:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:14][root][INFO] - Training Epoch: 2/2, step 4141/7134 completed (loss: 0.04301465302705765, acc: 0.9899713397026062)
[2025-02-13 04:13:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:14][root][INFO] - Training Epoch: 2/2, step 4142/7134 completed (loss: 0.05851056054234505, acc: 0.9851258397102356)
[2025-02-13 04:13:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:15][root][INFO] - Training Epoch: 2/2, step 4143/7134 completed (loss: 0.027800235897302628, acc: 0.9938775300979614)
[2025-02-13 04:13:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:15][root][INFO] - Training Epoch: 2/2, step 4144/7134 completed (loss: 0.05258306488394737, acc: 0.9783599376678467)
[2025-02-13 04:13:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:16][root][INFO] - Training Epoch: 2/2, step 4145/7134 completed (loss: 0.03078785352408886, acc: 0.994584858417511)
[2025-02-13 04:13:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:16][root][INFO] - Training Epoch: 2/2, step 4146/7134 completed (loss: 0.06354635208845139, acc: 0.9858585596084595)
[2025-02-13 04:13:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:17][root][INFO] - Training Epoch: 2/2, step 4147/7134 completed (loss: 0.05692114681005478, acc: 0.9885057210922241)
[2025-02-13 04:13:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:17][root][INFO] - Training Epoch: 2/2, step 4148/7134 completed (loss: 0.09080978482961655, acc: 0.9710424542427063)
[2025-02-13 04:13:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:17][root][INFO] - Training Epoch: 2/2, step 4149/7134 completed (loss: 0.08041432499885559, acc: 0.981333315372467)
[2025-02-13 04:13:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:18][root][INFO] - Training Epoch: 2/2, step 4150/7134 completed (loss: 0.07704479992389679, acc: 0.9758551120758057)
[2025-02-13 04:13:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:18][root][INFO] - Training Epoch: 2/2, step 4151/7134 completed (loss: 0.1096198782324791, acc: 0.9818840622901917)
[2025-02-13 04:13:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:19][root][INFO] - Training Epoch: 2/2, step 4152/7134 completed (loss: 0.042232029139995575, acc: 0.989180862903595)
[2025-02-13 04:13:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:19][root][INFO] - Training Epoch: 2/2, step 4153/7134 completed (loss: 0.05462661013007164, acc: 0.9861963391304016)
[2025-02-13 04:13:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:20][root][INFO] - Training Epoch: 2/2, step 4154/7134 completed (loss: 0.03914343938231468, acc: 0.9895833134651184)
[2025-02-13 04:13:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:20][root][INFO] - Training Epoch: 2/2, step 4155/7134 completed (loss: 0.04291883111000061, acc: 0.9879931211471558)
[2025-02-13 04:13:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:21][root][INFO] - Training Epoch: 2/2, step 4156/7134 completed (loss: 0.04793234542012215, acc: 0.9860917925834656)
[2025-02-13 04:13:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:21][root][INFO] - Training Epoch: 2/2, step 4157/7134 completed (loss: 0.04861466959118843, acc: 0.9880239367485046)
[2025-02-13 04:13:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:21][root][INFO] - Training Epoch: 2/2, step 4158/7134 completed (loss: 0.07071314007043839, acc: 0.9820051193237305)
[2025-02-13 04:13:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:22][root][INFO] - Training Epoch: 2/2, step 4159/7134 completed (loss: 0.025599302724003792, acc: 0.9915110468864441)
[2025-02-13 04:13:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:22][root][INFO] - Training Epoch: 2/2, step 4160/7134 completed (loss: 0.037326980382204056, acc: 0.9905660152435303)
[2025-02-13 04:13:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:23][root][INFO] - Training Epoch: 2/2, step 4161/7134 completed (loss: 0.02677042782306671, acc: 0.9919678568840027)
[2025-02-13 04:13:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:23][root][INFO] - Training Epoch: 2/2, step 4162/7134 completed (loss: 0.041904449462890625, acc: 0.9895678162574768)
[2025-02-13 04:13:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:24][root][INFO] - Training Epoch: 2/2, step 4163/7134 completed (loss: 0.03918171674013138, acc: 0.9894099831581116)
[2025-02-13 04:13:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:24][root][INFO] - Training Epoch: 2/2, step 4164/7134 completed (loss: 0.04144733026623726, acc: 0.9912917017936707)
[2025-02-13 04:13:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:25][root][INFO] - Training Epoch: 2/2, step 4165/7134 completed (loss: 0.03458169847726822, acc: 0.9900124669075012)
[2025-02-13 04:13:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:25][root][INFO] - Training Epoch: 2/2, step 4166/7134 completed (loss: 0.036607079207897186, acc: 0.9867841601371765)
[2025-02-13 04:13:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:25][root][INFO] - Training Epoch: 2/2, step 4167/7134 completed (loss: 0.013821164146065712, acc: 0.9965277910232544)
[2025-02-13 04:13:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:26][root][INFO] - Training Epoch: 2/2, step 4168/7134 completed (loss: 0.025824349373579025, acc: 0.9947916865348816)
[2025-02-13 04:13:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:26][root][INFO] - Training Epoch: 2/2, step 4169/7134 completed (loss: 0.03238124027848244, acc: 0.9957761168479919)
[2025-02-13 04:13:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:27][root][INFO] - Training Epoch: 2/2, step 4170/7134 completed (loss: 0.03269033133983612, acc: 0.993630588054657)
[2025-02-13 04:13:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:27][root][INFO] - Training Epoch: 2/2, step 4171/7134 completed (loss: 0.019599584862589836, acc: 0.9953863620758057)
[2025-02-13 04:13:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:28][root][INFO] - Training Epoch: 2/2, step 4172/7134 completed (loss: 0.035300370305776596, acc: 0.9925187230110168)
[2025-02-13 04:13:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:28][root][INFO] - Training Epoch: 2/2, step 4173/7134 completed (loss: 0.044682301580905914, acc: 0.9935622215270996)
[2025-02-13 04:13:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:29][root][INFO] - Training Epoch: 2/2, step 4174/7134 completed (loss: 0.0034769750200212, acc: 1.0)
[2025-02-13 04:13:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:29][root][INFO] - Training Epoch: 2/2, step 4175/7134 completed (loss: 0.018730200827121735, acc: 0.993127167224884)
[2025-02-13 04:13:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:29][root][INFO] - Training Epoch: 2/2, step 4176/7134 completed (loss: 0.020888082683086395, acc: 0.995529055595398)
[2025-02-13 04:13:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:30][root][INFO] - Training Epoch: 2/2, step 4177/7134 completed (loss: 0.009423712268471718, acc: 0.9973718523979187)
[2025-02-13 04:13:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:30][root][INFO] - Training Epoch: 2/2, step 4178/7134 completed (loss: 0.015182863920927048, acc: 0.9939576983451843)
[2025-02-13 04:13:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:31][root][INFO] - Training Epoch: 2/2, step 4179/7134 completed (loss: 0.009693226777017117, acc: 0.9960629940032959)
[2025-02-13 04:13:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:31][root][INFO] - Training Epoch: 2/2, step 4180/7134 completed (loss: 0.009041889570653439, acc: 0.9957627058029175)
[2025-02-13 04:13:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:32][root][INFO] - Training Epoch: 2/2, step 4181/7134 completed (loss: 0.004049334675073624, acc: 1.0)
[2025-02-13 04:13:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:32][root][INFO] - Training Epoch: 2/2, step 4182/7134 completed (loss: 0.0036358560901135206, acc: 1.0)
[2025-02-13 04:13:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:33][root][INFO] - Training Epoch: 2/2, step 4183/7134 completed (loss: 0.01933768019080162, acc: 0.9944367408752441)
[2025-02-13 04:13:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:33][root][INFO] - Training Epoch: 2/2, step 4184/7134 completed (loss: 0.02282322198152542, acc: 0.9946949481964111)
[2025-02-13 04:13:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:34][root][INFO] - Training Epoch: 2/2, step 4185/7134 completed (loss: 0.006671368144452572, acc: 0.9977169036865234)
[2025-02-13 04:13:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:34][root][INFO] - Training Epoch: 2/2, step 4186/7134 completed (loss: 0.06524091958999634, acc: 0.9879662990570068)
[2025-02-13 04:13:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:34][root][INFO] - Training Epoch: 2/2, step 4187/7134 completed (loss: 0.0014627916971221566, acc: 1.0)
[2025-02-13 04:13:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:35][root][INFO] - Training Epoch: 2/2, step 4188/7134 completed (loss: 0.0030132518149912357, acc: 0.9987499713897705)
[2025-02-13 04:13:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:35][root][INFO] - Training Epoch: 2/2, step 4189/7134 completed (loss: 0.01735416240990162, acc: 0.9972028136253357)
[2025-02-13 04:13:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:36][root][INFO] - Training Epoch: 2/2, step 4190/7134 completed (loss: 0.00627404497936368, acc: 0.9985795617103577)
[2025-02-13 04:13:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:36][root][INFO] - Training Epoch: 2/2, step 4191/7134 completed (loss: 0.010028503835201263, acc: 0.9976958632469177)
[2025-02-13 04:13:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:37][root][INFO] - Training Epoch: 2/2, step 4192/7134 completed (loss: 0.010566459968686104, acc: 0.9968051314353943)
[2025-02-13 04:13:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:37][root][INFO] - Training Epoch: 2/2, step 4193/7134 completed (loss: 0.01389834750443697, acc: 0.9973261952400208)
[2025-02-13 04:13:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:38][root][INFO] - Training Epoch: 2/2, step 4194/7134 completed (loss: 0.012048158794641495, acc: 0.9969742894172668)
[2025-02-13 04:13:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:38][root][INFO] - Training Epoch: 2/2, step 4195/7134 completed (loss: 0.03320397436618805, acc: 0.9932249188423157)
[2025-02-13 04:13:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:38][root][INFO] - Training Epoch: 2/2, step 4196/7134 completed (loss: 0.006713103037327528, acc: 0.9965397715568542)
[2025-02-13 04:13:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:39][root][INFO] - Training Epoch: 2/2, step 4197/7134 completed (loss: 0.026472173631191254, acc: 0.9935317039489746)
[2025-02-13 04:13:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:39][root][INFO] - Training Epoch: 2/2, step 4198/7134 completed (loss: 0.013483592309057713, acc: 0.9986110925674438)
[2025-02-13 04:13:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:40][root][INFO] - Training Epoch: 2/2, step 4199/7134 completed (loss: 0.013107073493301868, acc: 0.9945255517959595)
[2025-02-13 04:13:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:40][root][INFO] - Training Epoch: 2/2, step 4200/7134 completed (loss: 0.032108552753925323, acc: 0.9910394549369812)
[2025-02-13 04:13:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:41][root][INFO] - Training Epoch: 2/2, step 4201/7134 completed (loss: 0.010028744116425514, acc: 0.9965753555297852)
[2025-02-13 04:13:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:41][root][INFO] - Training Epoch: 2/2, step 4202/7134 completed (loss: 0.05040818080306053, acc: 0.9864341020584106)
[2025-02-13 04:13:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:41][root][INFO] - Training Epoch: 2/2, step 4203/7134 completed (loss: 0.03520013764500618, acc: 0.9859594106674194)
[2025-02-13 04:13:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:42][root][INFO] - Training Epoch: 2/2, step 4204/7134 completed (loss: 0.05373833328485489, acc: 0.988304078578949)
[2025-02-13 04:13:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:42][root][INFO] - Training Epoch: 2/2, step 4205/7134 completed (loss: 0.030477171763777733, acc: 0.9899280667304993)
[2025-02-13 04:13:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:43][root][INFO] - Training Epoch: 2/2, step 4206/7134 completed (loss: 0.04055899754166603, acc: 0.9915373921394348)
[2025-02-13 04:13:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:43][root][INFO] - Training Epoch: 2/2, step 4207/7134 completed (loss: 0.031615227460861206, acc: 0.9952229261398315)
[2025-02-13 04:13:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:44][root][INFO] - Training Epoch: 2/2, step 4208/7134 completed (loss: 0.055962465703487396, acc: 0.9840510487556458)
[2025-02-13 04:13:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:44][root][INFO] - Training Epoch: 2/2, step 4209/7134 completed (loss: 0.04420248791575432, acc: 0.9886685609817505)
[2025-02-13 04:13:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:44][root][INFO] - Training Epoch: 2/2, step 4210/7134 completed (loss: 0.004583925940096378, acc: 1.0)
[2025-02-13 04:13:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:45][root][INFO] - Training Epoch: 2/2, step 4211/7134 completed (loss: 0.02198389731347561, acc: 0.9907833933830261)
[2025-02-13 04:13:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:45][root][INFO] - Training Epoch: 2/2, step 4212/7134 completed (loss: 0.010442366823554039, acc: 0.9967266917228699)
[2025-02-13 04:13:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:46][root][INFO] - Training Epoch: 2/2, step 4213/7134 completed (loss: 0.009511216543614864, acc: 0.9971387982368469)
[2025-02-13 04:13:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:46][root][INFO] - Training Epoch: 2/2, step 4214/7134 completed (loss: 0.04215868562459946, acc: 0.9850746393203735)
[2025-02-13 04:13:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:47][root][INFO] - Training Epoch: 2/2, step 4215/7134 completed (loss: 0.031027961522340775, acc: 0.9912087917327881)
[2025-02-13 04:13:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:47][root][INFO] - Training Epoch: 2/2, step 4216/7134 completed (loss: 0.029473066329956055, acc: 0.9908592104911804)
[2025-02-13 04:13:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:47][root][INFO] - Training Epoch: 2/2, step 4217/7134 completed (loss: 0.01636134460568428, acc: 0.9958041906356812)
[2025-02-13 04:13:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:48][root][INFO] - Training Epoch: 2/2, step 4218/7134 completed (loss: 0.042187802493572235, acc: 0.9938144087791443)
[2025-02-13 04:13:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:48][root][INFO] - Training Epoch: 2/2, step 4219/7134 completed (loss: 0.015987597405910492, acc: 0.9959839582443237)
[2025-02-13 04:13:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:49][root][INFO] - Training Epoch: 2/2, step 4220/7134 completed (loss: 0.032380521297454834, acc: 0.9913669228553772)
[2025-02-13 04:13:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:49][root][INFO] - Training Epoch: 2/2, step 4221/7134 completed (loss: 0.011601541191339493, acc: 0.9964601993560791)
[2025-02-13 04:13:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:49][root][INFO] - Training Epoch: 2/2, step 4222/7134 completed (loss: 0.08225078880786896, acc: 0.9866666793823242)
[2025-02-13 04:13:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:50][root][INFO] - Training Epoch: 2/2, step 4223/7134 completed (loss: 0.0366244874894619, acc: 0.9894039630889893)
[2025-02-13 04:13:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:50][root][INFO] - Training Epoch: 2/2, step 4224/7134 completed (loss: 0.05123286694288254, acc: 0.9848812222480774)
[2025-02-13 04:13:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:51][root][INFO] - Training Epoch: 2/2, step 4225/7134 completed (loss: 0.05521567538380623, acc: 0.9864176511764526)
[2025-02-13 04:13:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:51][root][INFO] - Training Epoch: 2/2, step 4226/7134 completed (loss: 0.052931420505046844, acc: 0.9859514832496643)
[2025-02-13 04:13:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:52][root][INFO] - Training Epoch: 2/2, step 4227/7134 completed (loss: 0.07478851079940796, acc: 0.9803439974784851)
[2025-02-13 04:13:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:52][root][INFO] - Training Epoch: 2/2, step 4228/7134 completed (loss: 0.08936017006635666, acc: 0.9741379022598267)
[2025-02-13 04:13:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:52][root][INFO] - Training Epoch: 2/2, step 4229/7134 completed (loss: 0.06392750889062881, acc: 0.9874213933944702)
[2025-02-13 04:13:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:53][root][INFO] - Training Epoch: 2/2, step 4230/7134 completed (loss: 0.059001676738262177, acc: 0.9852941036224365)
[2025-02-13 04:13:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:53][root][INFO] - Training Epoch: 2/2, step 4231/7134 completed (loss: 0.1150960773229599, acc: 0.9723756909370422)
[2025-02-13 04:13:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:54][root][INFO] - Training Epoch: 2/2, step 4232/7134 completed (loss: 0.07205904275178909, acc: 0.9770992398262024)
[2025-02-13 04:13:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:54][root][INFO] - Training Epoch: 2/2, step 4233/7134 completed (loss: 0.05188552662730217, acc: 0.9883138537406921)
[2025-02-13 04:13:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:55][root][INFO] - Training Epoch: 2/2, step 4234/7134 completed (loss: 0.03706137463450432, acc: 0.995230495929718)
[2025-02-13 04:13:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:55][root][INFO] - Training Epoch: 2/2, step 4235/7134 completed (loss: 0.03482179343700409, acc: 0.9905213117599487)
[2025-02-13 04:13:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:55][root][INFO] - Training Epoch: 2/2, step 4236/7134 completed (loss: 0.013316724449396133, acc: 0.997474730014801)
[2025-02-13 04:13:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:56][root][INFO] - Training Epoch: 2/2, step 4237/7134 completed (loss: 0.019899532198905945, acc: 0.9938144087791443)
[2025-02-13 04:13:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:56][root][INFO] - Training Epoch: 2/2, step 4238/7134 completed (loss: 0.02535817213356495, acc: 0.9967266917228699)
[2025-02-13 04:13:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:57][root][INFO] - Training Epoch: 2/2, step 4239/7134 completed (loss: 0.015549851581454277, acc: 0.9936000108718872)
[2025-02-13 04:13:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:57][root][INFO] - Training Epoch: 2/2, step 4240/7134 completed (loss: 0.03983275592327118, acc: 0.9919224381446838)
[2025-02-13 04:13:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:57][root][INFO] - Training Epoch: 2/2, step 4241/7134 completed (loss: 0.009564635343849659, acc: 1.0)
[2025-02-13 04:13:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:58][root][INFO] - Training Epoch: 2/2, step 4242/7134 completed (loss: 0.032247111201286316, acc: 0.9910913109779358)
[2025-02-13 04:13:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:58][root][INFO] - Training Epoch: 2/2, step 4243/7134 completed (loss: 0.04734967276453972, acc: 0.9891501069068909)
[2025-02-13 04:13:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:59][root][INFO] - Training Epoch: 2/2, step 4244/7134 completed (loss: 0.02389848418533802, acc: 0.9933481216430664)
[2025-02-13 04:13:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:59][root][INFO] - Training Epoch: 2/2, step 4245/7134 completed (loss: 0.02542443946003914, acc: 0.9926650524139404)
[2025-02-13 04:13:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:13:59][root][INFO] - Training Epoch: 2/2, step 4246/7134 completed (loss: 0.03035775013267994, acc: 0.995245635509491)
[2025-02-13 04:14:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:00][root][INFO] - Training Epoch: 2/2, step 4247/7134 completed (loss: 0.029103722423315048, acc: 0.9964349269866943)
[2025-02-13 04:14:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:00][root][INFO] - Training Epoch: 2/2, step 4248/7134 completed (loss: 0.04864649847149849, acc: 0.9871794581413269)
[2025-02-13 04:14:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:01][root][INFO] - Training Epoch: 2/2, step 4249/7134 completed (loss: 0.04496973752975464, acc: 0.9920634627342224)
[2025-02-13 04:14:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:01][root][INFO] - Training Epoch: 2/2, step 4250/7134 completed (loss: 0.04835471138358116, acc: 0.9893842935562134)
[2025-02-13 04:14:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:02][root][INFO] - Training Epoch: 2/2, step 4251/7134 completed (loss: 0.011674312874674797, acc: 0.9967105388641357)
[2025-02-13 04:14:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:02][root][INFO] - Training Epoch: 2/2, step 4252/7134 completed (loss: 0.020482754334807396, acc: 0.9936407208442688)
[2025-02-13 04:14:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:02][root][INFO] - Training Epoch: 2/2, step 4253/7134 completed (loss: 0.03511626273393631, acc: 0.989154040813446)
[2025-02-13 04:14:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:03][root][INFO] - Training Epoch: 2/2, step 4254/7134 completed (loss: 0.014163754880428314, acc: 0.9950124621391296)
[2025-02-13 04:14:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:03][root][INFO] - Training Epoch: 2/2, step 4255/7134 completed (loss: 0.02565848082304001, acc: 0.9948630332946777)
[2025-02-13 04:14:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:04][root][INFO] - Training Epoch: 2/2, step 4256/7134 completed (loss: 0.03425908461213112, acc: 0.9909502267837524)
[2025-02-13 04:14:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:04][root][INFO] - Training Epoch: 2/2, step 4257/7134 completed (loss: 0.011369866319000721, acc: 0.9964349269866943)
[2025-02-13 04:14:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:04][root][INFO] - Training Epoch: 2/2, step 4258/7134 completed (loss: 0.0056362394243478775, acc: 1.0)
[2025-02-13 04:14:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:05][root][INFO] - Training Epoch: 2/2, step 4259/7134 completed (loss: 0.057513874024152756, acc: 0.984240710735321)
[2025-02-13 04:14:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:05][root][INFO] - Training Epoch: 2/2, step 4260/7134 completed (loss: 0.012139424681663513, acc: 0.9917184114456177)
[2025-02-13 04:14:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:06][root][INFO] - Training Epoch: 2/2, step 4261/7134 completed (loss: 0.019797349348664284, acc: 0.9923518300056458)
[2025-02-13 04:14:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:06][root][INFO] - Training Epoch: 2/2, step 4262/7134 completed (loss: 0.029512425884604454, acc: 0.9874804615974426)
[2025-02-13 04:14:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:07][root][INFO] - Training Epoch: 2/2, step 4263/7134 completed (loss: 0.04901725426316261, acc: 0.9949324131011963)
[2025-02-13 04:14:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:07][root][INFO] - Training Epoch: 2/2, step 4264/7134 completed (loss: 0.029013674706220627, acc: 0.9899874925613403)
[2025-02-13 04:14:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:08][root][INFO] - Training Epoch: 2/2, step 4265/7134 completed (loss: 0.018238335847854614, acc: 0.993686854839325)
[2025-02-13 04:14:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:08][root][INFO] - Training Epoch: 2/2, step 4266/7134 completed (loss: 0.019129160791635513, acc: 0.9952095746994019)
[2025-02-13 04:14:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:08][root][INFO] - Training Epoch: 2/2, step 4267/7134 completed (loss: 0.023094715550541878, acc: 0.9939393997192383)
[2025-02-13 04:14:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:09][root][INFO] - Training Epoch: 2/2, step 4268/7134 completed (loss: 0.053870342671871185, acc: 0.9839679598808289)
[2025-02-13 04:14:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:09][root][INFO] - Training Epoch: 2/2, step 4269/7134 completed (loss: 0.05548642948269844, acc: 0.9841726422309875)
[2025-02-13 04:14:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:10][root][INFO] - Training Epoch: 2/2, step 4270/7134 completed (loss: 0.03310663253068924, acc: 0.98828125)
[2025-02-13 04:14:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:10][root][INFO] - Training Epoch: 2/2, step 4271/7134 completed (loss: 0.025027373805642128, acc: 0.9900373816490173)
[2025-02-13 04:14:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:11][root][INFO] - Training Epoch: 2/2, step 4272/7134 completed (loss: 0.022294621914625168, acc: 0.9953051805496216)
[2025-02-13 04:14:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:11][root][INFO] - Training Epoch: 2/2, step 4273/7134 completed (loss: 0.029191721230745316, acc: 0.9899623394012451)
[2025-02-13 04:14:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:12][root][INFO] - Training Epoch: 2/2, step 4274/7134 completed (loss: 0.05344904959201813, acc: 0.9836448431015015)
[2025-02-13 04:14:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:12][root][INFO] - Training Epoch: 2/2, step 4275/7134 completed (loss: 0.012356950901448727, acc: 0.997474730014801)
[2025-02-13 04:14:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:13][root][INFO] - Training Epoch: 2/2, step 4276/7134 completed (loss: 0.05008819326758385, acc: 0.9896907210350037)
[2025-02-13 04:14:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:13][root][INFO] - Training Epoch: 2/2, step 4277/7134 completed (loss: 0.01788250170648098, acc: 0.9968253970146179)
[2025-02-13 04:14:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:13][root][INFO] - Training Epoch: 2/2, step 4278/7134 completed (loss: 0.03569503501057625, acc: 0.9925925731658936)
[2025-02-13 04:14:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:14][root][INFO] - Training Epoch: 2/2, step 4279/7134 completed (loss: 0.028897646814584732, acc: 0.9940476417541504)
[2025-02-13 04:14:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:14][root][INFO] - Training Epoch: 2/2, step 4280/7134 completed (loss: 0.003924406599253416, acc: 1.0)
[2025-02-13 04:14:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:15][root][INFO] - Training Epoch: 2/2, step 4281/7134 completed (loss: 0.026097925379872322, acc: 0.9876237511634827)
[2025-02-13 04:14:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:15][root][INFO] - Training Epoch: 2/2, step 4282/7134 completed (loss: 0.013655181042850018, acc: 0.9962962865829468)
[2025-02-13 04:14:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:16][root][INFO] - Training Epoch: 2/2, step 4283/7134 completed (loss: 0.012933026999235153, acc: 0.9942660331726074)
[2025-02-13 04:14:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:16][root][INFO] - Training Epoch: 2/2, step 4284/7134 completed (loss: 0.02662089839577675, acc: 0.9981883764266968)
[2025-02-13 04:14:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:17][root][INFO] - Training Epoch: 2/2, step 4285/7134 completed (loss: 0.009627080522477627, acc: 0.9982876777648926)
[2025-02-13 04:14:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:17][root][INFO] - Training Epoch: 2/2, step 4286/7134 completed (loss: 0.01672748662531376, acc: 0.9928057789802551)
[2025-02-13 04:14:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:17][root][INFO] - Training Epoch: 2/2, step 4287/7134 completed (loss: 0.02280312217772007, acc: 0.9948453903198242)
[2025-02-13 04:14:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:18][root][INFO] - Training Epoch: 2/2, step 4288/7134 completed (loss: 0.01935553178191185, acc: 0.9929078221321106)
[2025-02-13 04:14:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:18][root][INFO] - Training Epoch: 2/2, step 4289/7134 completed (loss: 0.03497069329023361, acc: 0.9894737005233765)
[2025-02-13 04:14:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:19][root][INFO] - Training Epoch: 2/2, step 4290/7134 completed (loss: 0.015746723860502243, acc: 0.9959404468536377)
[2025-02-13 04:14:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:19][root][INFO] - Training Epoch: 2/2, step 4291/7134 completed (loss: 0.01992725022137165, acc: 0.9940405488014221)
[2025-02-13 04:14:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:20][root][INFO] - Training Epoch: 2/2, step 4292/7134 completed (loss: 0.023978613317012787, acc: 0.990963876247406)
[2025-02-13 04:14:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:20][root][INFO] - Training Epoch: 2/2, step 4293/7134 completed (loss: 0.017822450026869774, acc: 0.9939516186714172)
[2025-02-13 04:14:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:20][root][INFO] - Training Epoch: 2/2, step 4294/7134 completed (loss: 0.013575405813753605, acc: 0.9956331849098206)
[2025-02-13 04:14:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:21][root][INFO] - Training Epoch: 2/2, step 4295/7134 completed (loss: 0.029072867706418037, acc: 0.9908758997917175)
[2025-02-13 04:14:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:21][root][INFO] - Training Epoch: 2/2, step 4296/7134 completed (loss: 0.016493164002895355, acc: 0.9975669384002686)
[2025-02-13 04:14:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:22][root][INFO] - Training Epoch: 2/2, step 4297/7134 completed (loss: 0.007143163122236729, acc: 0.9976525902748108)
[2025-02-13 04:14:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:22][root][INFO] - Training Epoch: 2/2, step 4298/7134 completed (loss: 0.02732546441257, acc: 0.993122398853302)
[2025-02-13 04:14:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:23][root][INFO] - Training Epoch: 2/2, step 4299/7134 completed (loss: 0.020142920315265656, acc: 0.9928366541862488)
[2025-02-13 04:14:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:23][root][INFO] - Training Epoch: 2/2, step 4300/7134 completed (loss: 0.008018620312213898, acc: 0.9978070259094238)
[2025-02-13 04:14:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:23][root][INFO] - Training Epoch: 2/2, step 4301/7134 completed (loss: 0.015987124294042587, acc: 0.9950819611549377)
[2025-02-13 04:14:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:24][root][INFO] - Training Epoch: 2/2, step 4302/7134 completed (loss: 0.017248036339879036, acc: 0.9940298795700073)
[2025-02-13 04:14:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:24][root][INFO] - Training Epoch: 2/2, step 4303/7134 completed (loss: 0.026430845260620117, acc: 0.9940333962440491)
[2025-02-13 04:14:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:25][root][INFO] - Training Epoch: 2/2, step 4304/7134 completed (loss: 0.023279326036572456, acc: 0.9910846948623657)
[2025-02-13 04:14:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:25][root][INFO] - Training Epoch: 2/2, step 4305/7134 completed (loss: 0.008780990727245808, acc: 0.9976525902748108)
[2025-02-13 04:14:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:26][root][INFO] - Training Epoch: 2/2, step 4306/7134 completed (loss: 0.05436834320425987, acc: 0.9881734848022461)
[2025-02-13 04:14:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:26][root][INFO] - Training Epoch: 2/2, step 4307/7134 completed (loss: 0.049154818058013916, acc: 0.9855538010597229)
[2025-02-13 04:14:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:27][root][INFO] - Training Epoch: 2/2, step 4308/7134 completed (loss: 0.045407965779304504, acc: 0.9907192587852478)
[2025-02-13 04:14:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:27][root][INFO] - Training Epoch: 2/2, step 4309/7134 completed (loss: 0.03452443331480026, acc: 0.9913686513900757)
[2025-02-13 04:14:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:27][root][INFO] - Training Epoch: 2/2, step 4310/7134 completed (loss: 0.009178195148706436, acc: 0.9964747428894043)
[2025-02-13 04:14:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:28][root][INFO] - Training Epoch: 2/2, step 4311/7134 completed (loss: 0.007945364341139793, acc: 0.9971014261245728)
[2025-02-13 04:14:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:28][root][INFO] - Training Epoch: 2/2, step 4312/7134 completed (loss: 0.010345304384827614, acc: 0.9972752332687378)
[2025-02-13 04:14:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:29][root][INFO] - Training Epoch: 2/2, step 4313/7134 completed (loss: 0.007090000901371241, acc: 0.9974026083946228)
[2025-02-13 04:14:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:29][root][INFO] - Training Epoch: 2/2, step 4314/7134 completed (loss: 0.016919435933232307, acc: 0.9924471378326416)
[2025-02-13 04:14:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:30][root][INFO] - Training Epoch: 2/2, step 4315/7134 completed (loss: 0.025317130610346794, acc: 0.9921568632125854)
[2025-02-13 04:14:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:30][root][INFO] - Training Epoch: 2/2, step 4316/7134 completed (loss: 0.029237106442451477, acc: 0.9923469424247742)
[2025-02-13 04:14:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:31][root][INFO] - Training Epoch: 2/2, step 4317/7134 completed (loss: 0.027577919885516167, acc: 0.9950433969497681)
[2025-02-13 04:14:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:31][root][INFO] - Training Epoch: 2/2, step 4318/7134 completed (loss: 0.02132933959364891, acc: 0.9955157041549683)
[2025-02-13 04:14:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:31][root][INFO] - Training Epoch: 2/2, step 4319/7134 completed (loss: 0.00809731986373663, acc: 0.997770369052887)
[2025-02-13 04:14:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:32][root][INFO] - Training Epoch: 2/2, step 4320/7134 completed (loss: 0.01938905008137226, acc: 0.995945930480957)
[2025-02-13 04:14:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:32][root][INFO] - Training Epoch: 2/2, step 4321/7134 completed (loss: 0.024246150627732277, acc: 0.9907621145248413)
[2025-02-13 04:14:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:33][root][INFO] - Training Epoch: 2/2, step 4322/7134 completed (loss: 0.027170946821570396, acc: 0.9921383857727051)
[2025-02-13 04:14:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:33][root][INFO] - Training Epoch: 2/2, step 4323/7134 completed (loss: 0.05577584356069565, acc: 0.9807322025299072)
[2025-02-13 04:14:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:34][root][INFO] - Training Epoch: 2/2, step 4324/7134 completed (loss: 0.012585856951773167, acc: 0.9941860437393188)
[2025-02-13 04:14:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:34][root][INFO] - Training Epoch: 2/2, step 4325/7134 completed (loss: 0.02172001078724861, acc: 0.9920254945755005)
[2025-02-13 04:14:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:34][root][INFO] - Training Epoch: 2/2, step 4326/7134 completed (loss: 0.020247982814908028, acc: 0.9911971688270569)
[2025-02-13 04:14:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:35][root][INFO] - Training Epoch: 2/2, step 4327/7134 completed (loss: 0.008154945448040962, acc: 1.0)
[2025-02-13 04:14:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:35][root][INFO] - Training Epoch: 2/2, step 4328/7134 completed (loss: 0.04503578320145607, acc: 0.9868420958518982)
[2025-02-13 04:14:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:36][root][INFO] - Training Epoch: 2/2, step 4329/7134 completed (loss: 0.02266709879040718, acc: 0.9960681796073914)
[2025-02-13 04:14:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:36][root][INFO] - Training Epoch: 2/2, step 4330/7134 completed (loss: 0.028650104999542236, acc: 0.9914712309837341)
[2025-02-13 04:14:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:37][root][INFO] - Training Epoch: 2/2, step 4331/7134 completed (loss: 0.0042786384001374245, acc: 1.0)
[2025-02-13 04:14:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:37][root][INFO] - Training Epoch: 2/2, step 4332/7134 completed (loss: 0.026836426928639412, acc: 0.9885931611061096)
[2025-02-13 04:14:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:37][root][INFO] - Training Epoch: 2/2, step 4333/7134 completed (loss: 0.017978208139538765, acc: 0.9954476356506348)
[2025-02-13 04:14:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:38][root][INFO] - Training Epoch: 2/2, step 4334/7134 completed (loss: 0.02735556848347187, acc: 0.992514967918396)
[2025-02-13 04:14:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:38][root][INFO] - Training Epoch: 2/2, step 4335/7134 completed (loss: 0.03269472345709801, acc: 0.9899193644523621)
[2025-02-13 04:14:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:39][root][INFO] - Training Epoch: 2/2, step 4336/7134 completed (loss: 0.006978899706155062, acc: 0.9967637658119202)
[2025-02-13 04:14:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:39][root][INFO] - Training Epoch: 2/2, step 4337/7134 completed (loss: 0.012062206864356995, acc: 0.996835470199585)
[2025-02-13 04:14:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:40][root][INFO] - Training Epoch: 2/2, step 4338/7134 completed (loss: 0.00782283116132021, acc: 1.0)
[2025-02-13 04:14:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:40][root][INFO] - Training Epoch: 2/2, step 4339/7134 completed (loss: 0.026306889951229095, acc: 0.9927113652229309)
[2025-02-13 04:14:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:41][root][INFO] - Training Epoch: 2/2, step 4340/7134 completed (loss: 0.019182749092578888, acc: 0.9915540814399719)
[2025-02-13 04:14:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:41][root][INFO] - Training Epoch: 2/2, step 4341/7134 completed (loss: 0.035473085939884186, acc: 0.98959881067276)
[2025-02-13 04:14:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:41][root][INFO] - Training Epoch: 2/2, step 4342/7134 completed (loss: 0.053938694298267365, acc: 0.9887820482254028)
[2025-02-13 04:14:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:42][root][INFO] - Training Epoch: 2/2, step 4343/7134 completed (loss: 0.023154241964221, acc: 0.9966722130775452)
[2025-02-13 04:14:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:42][root][INFO] - Training Epoch: 2/2, step 4344/7134 completed (loss: 0.008188500069081783, acc: 0.9979209899902344)
[2025-02-13 04:14:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:43][root][INFO] - Training Epoch: 2/2, step 4345/7134 completed (loss: 0.017478210851550102, acc: 0.9937888383865356)
[2025-02-13 04:14:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:43][root][INFO] - Training Epoch: 2/2, step 4346/7134 completed (loss: 0.0031972185242921114, acc: 1.0)
[2025-02-13 04:14:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:44][root][INFO] - Training Epoch: 2/2, step 4347/7134 completed (loss: 0.053899917751550674, acc: 0.9886845946311951)
[2025-02-13 04:14:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:44][root][INFO] - Training Epoch: 2/2, step 4348/7134 completed (loss: 0.005152749363332987, acc: 1.0)
[2025-02-13 04:14:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:44][root][INFO] - Training Epoch: 2/2, step 4349/7134 completed (loss: 0.007887786254286766, acc: 0.9976905584335327)
[2025-02-13 04:14:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:45][root][INFO] - Training Epoch: 2/2, step 4350/7134 completed (loss: 0.029248187318444252, acc: 0.9934554696083069)
[2025-02-13 04:14:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:45][root][INFO] - Training Epoch: 2/2, step 4351/7134 completed (loss: 0.018765509128570557, acc: 0.9931412935256958)
[2025-02-13 04:14:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:46][root][INFO] - Training Epoch: 2/2, step 4352/7134 completed (loss: 0.025142692029476166, acc: 0.9911280274391174)
[2025-02-13 04:14:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:46][root][INFO] - Training Epoch: 2/2, step 4353/7134 completed (loss: 0.0346449576318264, acc: 0.990728497505188)
[2025-02-13 04:14:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:47][root][INFO] - Training Epoch: 2/2, step 4354/7134 completed (loss: 0.022117385640740395, acc: 0.9919999837875366)
[2025-02-13 04:14:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:47][root][INFO] - Training Epoch: 2/2, step 4355/7134 completed (loss: 0.02893773280084133, acc: 0.9899874925613403)
[2025-02-13 04:14:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:47][root][INFO] - Training Epoch: 2/2, step 4356/7134 completed (loss: 0.022570816799998283, acc: 0.9916368126869202)
[2025-02-13 04:14:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:48][root][INFO] - Training Epoch: 2/2, step 4357/7134 completed (loss: 0.029832549393177032, acc: 0.9921259880065918)
[2025-02-13 04:14:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:48][root][INFO] - Training Epoch: 2/2, step 4358/7134 completed (loss: 0.023624362424016, acc: 0.9916167855262756)
[2025-02-13 04:14:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:49][root][INFO] - Training Epoch: 2/2, step 4359/7134 completed (loss: 0.02614549919962883, acc: 0.9920760989189148)
[2025-02-13 04:14:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:49][root][INFO] - Training Epoch: 2/2, step 4360/7134 completed (loss: 0.05165977403521538, acc: 0.9846368432044983)
[2025-02-13 04:14:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:50][root][INFO] - Training Epoch: 2/2, step 4361/7134 completed (loss: 0.020358258858323097, acc: 0.9927745461463928)
[2025-02-13 04:14:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:50][root][INFO] - Training Epoch: 2/2, step 4362/7134 completed (loss: 0.031465087085962296, acc: 0.9876695275306702)
[2025-02-13 04:14:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:51][root][INFO] - Training Epoch: 2/2, step 4363/7134 completed (loss: 0.01675601489841938, acc: 0.9966942071914673)
[2025-02-13 04:14:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:51][root][INFO] - Training Epoch: 2/2, step 4364/7134 completed (loss: 0.0109100965783, acc: 0.9984375238418579)
[2025-02-13 04:14:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:51][root][INFO] - Training Epoch: 2/2, step 4365/7134 completed (loss: 0.018351705744862556, acc: 0.9968553185462952)
[2025-02-13 04:14:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:52][root][INFO] - Training Epoch: 2/2, step 4366/7134 completed (loss: 0.04727357625961304, acc: 0.9806700944900513)
[2025-02-13 04:14:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:52][root][INFO] - Training Epoch: 2/2, step 4367/7134 completed (loss: 0.02054920792579651, acc: 0.9914039969444275)
[2025-02-13 04:14:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:53][root][INFO] - Training Epoch: 2/2, step 4368/7134 completed (loss: 0.024081354960799217, acc: 0.989924430847168)
[2025-02-13 04:14:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:53][root][INFO] - Training Epoch: 2/2, step 4369/7134 completed (loss: 0.016703614965081215, acc: 0.9916666746139526)
[2025-02-13 04:14:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:54][root][INFO] - Training Epoch: 2/2, step 4370/7134 completed (loss: 0.04009679704904556, acc: 0.9886506795883179)
[2025-02-13 04:14:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:54][root][INFO] - Training Epoch: 2/2, step 4371/7134 completed (loss: 0.0350092351436615, acc: 0.9920844435691833)
[2025-02-13 04:14:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:55][root][INFO] - Training Epoch: 2/2, step 4372/7134 completed (loss: 0.027594199404120445, acc: 0.9931153059005737)
[2025-02-13 04:14:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:55][root][INFO] - Training Epoch: 2/2, step 4373/7134 completed (loss: 0.012887351214885712, acc: 0.9988109469413757)
[2025-02-13 04:14:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:55][root][INFO] - Training Epoch: 2/2, step 4374/7134 completed (loss: 0.02773597463965416, acc: 0.9884892106056213)
[2025-02-13 04:14:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:56][root][INFO] - Training Epoch: 2/2, step 4375/7134 completed (loss: 0.0148755619302392, acc: 0.9948520064353943)
[2025-02-13 04:14:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:56][root][INFO] - Training Epoch: 2/2, step 4376/7134 completed (loss: 0.036908071488142014, acc: 0.9909502267837524)
[2025-02-13 04:14:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:57][root][INFO] - Training Epoch: 2/2, step 4377/7134 completed (loss: 0.046060994267463684, acc: 0.9913473129272461)
[2025-02-13 04:14:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:57][root][INFO] - Training Epoch: 2/2, step 4378/7134 completed (loss: 0.023245560005307198, acc: 0.991830050945282)
[2025-02-13 04:14:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:58][root][INFO] - Training Epoch: 2/2, step 4379/7134 completed (loss: 0.05800718069076538, acc: 0.9810606241226196)
[2025-02-13 04:14:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:58][root][INFO] - Training Epoch: 2/2, step 4380/7134 completed (loss: 0.04050100967288017, acc: 0.9877408146858215)
[2025-02-13 04:14:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:58][root][INFO] - Training Epoch: 2/2, step 4381/7134 completed (loss: 0.014342222362756729, acc: 0.9958620667457581)
[2025-02-13 04:14:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:59][root][INFO] - Training Epoch: 2/2, step 4382/7134 completed (loss: 0.00888803880661726, acc: 0.998410165309906)
[2025-02-13 04:14:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:14:59][root][INFO] - Training Epoch: 2/2, step 4383/7134 completed (loss: 0.01854824088513851, acc: 0.9917469024658203)
[2025-02-13 04:14:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:00][root][INFO] - Training Epoch: 2/2, step 4384/7134 completed (loss: 0.01761786825954914, acc: 0.9962311387062073)
[2025-02-13 04:15:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:00][root][INFO] - Training Epoch: 2/2, step 4385/7134 completed (loss: 0.039567094296216965, acc: 0.9843546152114868)
[2025-02-13 04:15:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:01][root][INFO] - Training Epoch: 2/2, step 4386/7134 completed (loss: 0.04369347169995308, acc: 0.9834123253822327)
[2025-02-13 04:15:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:01][root][INFO] - Training Epoch: 2/2, step 4387/7134 completed (loss: 0.04584864526987076, acc: 0.9855072498321533)
[2025-02-13 04:15:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:02][root][INFO] - Training Epoch: 2/2, step 4388/7134 completed (loss: 0.028271548449993134, acc: 0.9922360181808472)
[2025-02-13 04:15:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:02][root][INFO] - Training Epoch: 2/2, step 4389/7134 completed (loss: 0.03778623789548874, acc: 0.9934747219085693)
[2025-02-13 04:15:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:02][root][INFO] - Training Epoch: 2/2, step 4390/7134 completed (loss: 0.05970697104930878, acc: 0.9859747290611267)
[2025-02-13 04:15:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:03][root][INFO] - Training Epoch: 2/2, step 4391/7134 completed (loss: 0.052917491644620895, acc: 0.9837133288383484)
[2025-02-13 04:15:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:03][root][INFO] - Training Epoch: 2/2, step 4392/7134 completed (loss: 0.02256997860968113, acc: 0.9950980544090271)
[2025-02-13 04:15:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:04][root][INFO] - Training Epoch: 2/2, step 4393/7134 completed (loss: 0.021321935579180717, acc: 0.9929676651954651)
[2025-02-13 04:15:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:04][root][INFO] - Training Epoch: 2/2, step 4394/7134 completed (loss: 0.0254666768014431, acc: 0.9929478168487549)
[2025-02-13 04:15:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:05][root][INFO] - Training Epoch: 2/2, step 4395/7134 completed (loss: 0.03782616928219795, acc: 0.9827315807342529)
[2025-02-13 04:15:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:05][root][INFO] - Training Epoch: 2/2, step 4396/7134 completed (loss: 0.06885568052530289, acc: 0.9810298085212708)
[2025-02-13 04:15:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:05][root][INFO] - Training Epoch: 2/2, step 4397/7134 completed (loss: 0.03224118798971176, acc: 0.9929412007331848)
[2025-02-13 04:15:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:06][root][INFO] - Training Epoch: 2/2, step 4398/7134 completed (loss: 0.02445572055876255, acc: 0.9949495196342468)
[2025-02-13 04:15:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:06][root][INFO] - Training Epoch: 2/2, step 4399/7134 completed (loss: 0.0297479759901762, acc: 0.9951768517494202)
[2025-02-13 04:15:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:07][root][INFO] - Training Epoch: 2/2, step 4400/7134 completed (loss: 0.062100350856781006, acc: 0.9888476133346558)
[2025-02-13 04:15:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:07][root][INFO] - Training Epoch: 2/2, step 4401/7134 completed (loss: 0.04661542549729347, acc: 0.9855072498321533)
[2025-02-13 04:15:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:08][root][INFO] - Training Epoch: 2/2, step 4402/7134 completed (loss: 0.07332248240709305, acc: 0.9781976938247681)
[2025-02-13 04:15:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:08][root][INFO] - Training Epoch: 2/2, step 4403/7134 completed (loss: 0.11259094625711441, acc: 0.9676165580749512)
[2025-02-13 04:15:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:08][root][INFO] - Training Epoch: 2/2, step 4404/7134 completed (loss: 0.10610347241163254, acc: 0.9720744490623474)
[2025-02-13 04:15:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:09][root][INFO] - Training Epoch: 2/2, step 4405/7134 completed (loss: 0.01833520643413067, acc: 0.9956803321838379)
[2025-02-13 04:15:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:09][root][INFO] - Training Epoch: 2/2, step 4406/7134 completed (loss: 0.02494698017835617, acc: 0.993318498134613)
[2025-02-13 04:15:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:10][root][INFO] - Training Epoch: 2/2, step 4407/7134 completed (loss: 0.02915818989276886, acc: 0.989595353603363)
[2025-02-13 04:15:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:10][root][INFO] - Training Epoch: 2/2, step 4408/7134 completed (loss: 0.051206305623054504, acc: 0.9871944189071655)
[2025-02-13 04:15:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:11][root][INFO] - Training Epoch: 2/2, step 4409/7134 completed (loss: 0.05445605888962746, acc: 0.9902200698852539)
[2025-02-13 04:15:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:11][root][INFO] - Training Epoch: 2/2, step 4410/7134 completed (loss: 0.050865352153778076, acc: 0.9892473220825195)
[2025-02-13 04:15:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:12][root][INFO] - Training Epoch: 2/2, step 4411/7134 completed (loss: 0.0829954594373703, acc: 0.9769784212112427)
[2025-02-13 04:15:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:12][root][INFO] - Training Epoch: 2/2, step 4412/7134 completed (loss: 0.12122656404972076, acc: 0.9595808386802673)
[2025-02-13 04:15:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:12][root][INFO] - Training Epoch: 2/2, step 4413/7134 completed (loss: 0.08553095906972885, acc: 0.9741496443748474)
[2025-02-13 04:15:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:13][root][INFO] - Training Epoch: 2/2, step 4414/7134 completed (loss: 0.026553167030215263, acc: 0.9928910136222839)
[2025-02-13 04:15:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:13][root][INFO] - Training Epoch: 2/2, step 4415/7134 completed (loss: 0.0420353077352047, acc: 0.9887920022010803)
[2025-02-13 04:15:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:14][root][INFO] - Training Epoch: 2/2, step 4416/7134 completed (loss: 0.03873518854379654, acc: 0.9918319582939148)
[2025-02-13 04:15:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:14][root][INFO] - Training Epoch: 2/2, step 4417/7134 completed (loss: 0.04553206264972687, acc: 0.9879807829856873)
[2025-02-13 04:15:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:15][root][INFO] - Training Epoch: 2/2, step 4418/7134 completed (loss: 0.04322842136025429, acc: 0.9934895634651184)
[2025-02-13 04:15:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:15][root][INFO] - Training Epoch: 2/2, step 4419/7134 completed (loss: 0.05883689597249031, acc: 0.9783653616905212)
[2025-02-13 04:15:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:16][root][INFO] - Training Epoch: 2/2, step 4420/7134 completed (loss: 0.04557951167225838, acc: 0.9820193648338318)
[2025-02-13 04:15:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:16][root][INFO] - Training Epoch: 2/2, step 4421/7134 completed (loss: 0.04138277843594551, acc: 0.9898762702941895)
[2025-02-13 04:15:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:16][root][INFO] - Training Epoch: 2/2, step 4422/7134 completed (loss: 0.05963538959622383, acc: 0.9827315807342529)
[2025-02-13 04:15:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:17][root][INFO] - Training Epoch: 2/2, step 4423/7134 completed (loss: 0.032118603587150574, acc: 0.990867555141449)
[2025-02-13 04:15:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:17][root][INFO] - Training Epoch: 2/2, step 4424/7134 completed (loss: 0.025750016793608665, acc: 0.9954954981803894)
[2025-02-13 04:15:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:18][root][INFO] - Training Epoch: 2/2, step 4425/7134 completed (loss: 0.028761180117726326, acc: 0.9906432628631592)
[2025-02-13 04:15:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:18][root][INFO] - Training Epoch: 2/2, step 4426/7134 completed (loss: 0.03817042335867882, acc: 0.988624632358551)
[2025-02-13 04:15:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:19][root][INFO] - Training Epoch: 2/2, step 4427/7134 completed (loss: 0.0686427503824234, acc: 0.983132541179657)
[2025-02-13 04:15:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:19][root][INFO] - Training Epoch: 2/2, step 4428/7134 completed (loss: 0.023256976157426834, acc: 0.9929577708244324)
[2025-02-13 04:15:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:20][root][INFO] - Training Epoch: 2/2, step 4429/7134 completed (loss: 0.025593917816877365, acc: 0.9896907210350037)
[2025-02-13 04:15:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:20][root][INFO] - Training Epoch: 2/2, step 4430/7134 completed (loss: 0.03513536602258682, acc: 0.9878048896789551)
[2025-02-13 04:15:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:20][root][INFO] - Training Epoch: 2/2, step 4431/7134 completed (loss: 0.038064371794462204, acc: 0.993779182434082)
[2025-02-13 04:15:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:21][root][INFO] - Training Epoch: 2/2, step 4432/7134 completed (loss: 0.011543980799615383, acc: 0.9938555955886841)
[2025-02-13 04:15:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:21][root][INFO] - Training Epoch: 2/2, step 4433/7134 completed (loss: 0.03088518977165222, acc: 0.9918414950370789)
[2025-02-13 04:15:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:22][root][INFO] - Training Epoch: 2/2, step 4434/7134 completed (loss: 0.021668938919901848, acc: 0.9939939975738525)
[2025-02-13 04:15:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:22][root][INFO] - Training Epoch: 2/2, step 4435/7134 completed (loss: 0.01704644411802292, acc: 0.9926062822341919)
[2025-02-13 04:15:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:23][root][INFO] - Training Epoch: 2/2, step 4436/7134 completed (loss: 0.05379404500126839, acc: 0.9846368432044983)
[2025-02-13 04:15:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:23][root][INFO] - Training Epoch: 2/2, step 4437/7134 completed (loss: 0.01679626666009426, acc: 0.9960629940032959)
[2025-02-13 04:15:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:23][root][INFO] - Training Epoch: 2/2, step 4438/7134 completed (loss: 0.030354434624314308, acc: 0.991525411605835)
[2025-02-13 04:15:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:24][root][INFO] - Training Epoch: 2/2, step 4439/7134 completed (loss: 0.01644383929669857, acc: 0.9902439117431641)
[2025-02-13 04:15:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:24][root][INFO] - Training Epoch: 2/2, step 4440/7134 completed (loss: 0.024247460067272186, acc: 0.9930939078330994)
[2025-02-13 04:15:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:25][root][INFO] - Training Epoch: 2/2, step 4441/7134 completed (loss: 0.030099714174866676, acc: 0.9892473220825195)
[2025-02-13 04:15:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:25][root][INFO] - Training Epoch: 2/2, step 4442/7134 completed (loss: 0.04690182954072952, acc: 0.9884169697761536)
[2025-02-13 04:15:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:26][root][INFO] - Training Epoch: 2/2, step 4443/7134 completed (loss: 0.022947926074266434, acc: 0.9948717951774597)
[2025-02-13 04:15:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:26][root][INFO] - Training Epoch: 2/2, step 4444/7134 completed (loss: 0.020376436412334442, acc: 0.9918434023857117)
[2025-02-13 04:15:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:26][root][INFO] - Training Epoch: 2/2, step 4445/7134 completed (loss: 0.01705457828938961, acc: 0.99301677942276)
[2025-02-13 04:15:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:27][root][INFO] - Training Epoch: 2/2, step 4446/7134 completed (loss: 0.024056553840637207, acc: 0.9919484853744507)
[2025-02-13 04:15:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:27][root][INFO] - Training Epoch: 2/2, step 4447/7134 completed (loss: 0.021818457171320915, acc: 0.9952830076217651)
[2025-02-13 04:15:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:28][root][INFO] - Training Epoch: 2/2, step 4448/7134 completed (loss: 0.040587861090898514, acc: 0.9853747487068176)
[2025-02-13 04:15:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:28][root][INFO] - Training Epoch: 2/2, step 4449/7134 completed (loss: 0.03339477628469467, acc: 0.9851301312446594)
[2025-02-13 04:15:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:29][root][INFO] - Training Epoch: 2/2, step 4450/7134 completed (loss: 0.01623675785958767, acc: 0.995555579662323)
[2025-02-13 04:15:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:29][root][INFO] - Training Epoch: 2/2, step 4451/7134 completed (loss: 0.07649896293878555, acc: 0.9784411191940308)
[2025-02-13 04:15:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:29][root][INFO] - Training Epoch: 2/2, step 4452/7134 completed (loss: 0.013125534169375896, acc: 0.9968000054359436)
[2025-02-13 04:15:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:30][root][INFO] - Training Epoch: 2/2, step 4453/7134 completed (loss: 0.0389396958053112, acc: 0.9890710115432739)
[2025-02-13 04:15:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:30][root][INFO] - Training Epoch: 2/2, step 4454/7134 completed (loss: 0.029911259189248085, acc: 0.9929178357124329)
[2025-02-13 04:15:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:31][root][INFO] - Training Epoch: 2/2, step 4455/7134 completed (loss: 0.0734899714589119, acc: 0.988950252532959)
[2025-02-13 04:15:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:31][root][INFO] - Training Epoch: 2/2, step 4456/7134 completed (loss: 0.02368669956922531, acc: 0.9951456189155579)
[2025-02-13 04:15:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:31][root][INFO] - Training Epoch: 2/2, step 4457/7134 completed (loss: 0.010198242031037807, acc: 0.9962546825408936)
[2025-02-13 04:15:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:32][root][INFO] - Training Epoch: 2/2, step 4458/7134 completed (loss: 0.0019014051649719477, acc: 1.0)
[2025-02-13 04:15:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:32][root][INFO] - Training Epoch: 2/2, step 4459/7134 completed (loss: 0.02014900930225849, acc: 0.9945054650306702)
[2025-02-13 04:15:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:33][root][INFO] - Training Epoch: 2/2, step 4460/7134 completed (loss: 0.01744360849261284, acc: 0.9938875436782837)
[2025-02-13 04:15:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:33][root][INFO] - Training Epoch: 2/2, step 4461/7134 completed (loss: 0.04443897679448128, acc: 0.9889298677444458)
[2025-02-13 04:15:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:34][root][INFO] - Training Epoch: 2/2, step 4462/7134 completed (loss: 0.019780995324254036, acc: 0.9945504069328308)
[2025-02-13 04:15:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:34][root][INFO] - Training Epoch: 2/2, step 4463/7134 completed (loss: 0.019110212102532387, acc: 0.9949685335159302)
[2025-02-13 04:15:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:34][root][INFO] - Training Epoch: 2/2, step 4464/7134 completed (loss: 0.030259516090154648, acc: 0.994940996170044)
[2025-02-13 04:15:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:35][root][INFO] - Training Epoch: 2/2, step 4465/7134 completed (loss: 0.009543538093566895, acc: 0.9961340427398682)
[2025-02-13 04:15:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:35][root][INFO] - Training Epoch: 2/2, step 4466/7134 completed (loss: 0.014583174139261246, acc: 0.9938271641731262)
[2025-02-13 04:15:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:36][root][INFO] - Training Epoch: 2/2, step 4467/7134 completed (loss: 0.029435670003294945, acc: 0.9912060499191284)
[2025-02-13 04:15:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:36][root][INFO] - Training Epoch: 2/2, step 4468/7134 completed (loss: 0.004769450053572655, acc: 0.9988109469413757)
[2025-02-13 04:15:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:37][root][INFO] - Training Epoch: 2/2, step 4469/7134 completed (loss: 0.03116748109459877, acc: 0.99190753698349)
[2025-02-13 04:15:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:37][root][INFO] - Training Epoch: 2/2, step 4470/7134 completed (loss: 0.010079117491841316, acc: 0.9966850876808167)
[2025-02-13 04:15:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:38][root][INFO] - Training Epoch: 2/2, step 4471/7134 completed (loss: 0.003700973466038704, acc: 0.9987714886665344)
[2025-02-13 04:15:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:38][root][INFO] - Training Epoch: 2/2, step 4472/7134 completed (loss: 0.024950098246335983, acc: 0.9934123754501343)
[2025-02-13 04:15:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:38][root][INFO] - Training Epoch: 2/2, step 4473/7134 completed (loss: 0.050809960812330246, acc: 0.9887955188751221)
[2025-02-13 04:15:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:39][root][INFO] - Training Epoch: 2/2, step 4474/7134 completed (loss: 0.03774888440966606, acc: 0.9860529899597168)
[2025-02-13 04:15:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:39][root][INFO] - Training Epoch: 2/2, step 4475/7134 completed (loss: 0.10061455518007278, acc: 0.9739454388618469)
[2025-02-13 04:15:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:40][root][INFO] - Training Epoch: 2/2, step 4476/7134 completed (loss: 0.057246189564466476, acc: 0.9780521392822266)
[2025-02-13 04:15:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:40][root][INFO] - Training Epoch: 2/2, step 4477/7134 completed (loss: 0.006678939796984196, acc: 0.9963325262069702)
[2025-02-13 04:15:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:41][root][INFO] - Training Epoch: 2/2, step 4478/7134 completed (loss: 0.017218267545104027, acc: 0.998763918876648)
[2025-02-13 04:15:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:41][root][INFO] - Training Epoch: 2/2, step 4479/7134 completed (loss: 0.010401531122624874, acc: 0.9959568977355957)
[2025-02-13 04:15:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:42][root][INFO] - Training Epoch: 2/2, step 4480/7134 completed (loss: 0.02297382242977619, acc: 0.9929906725883484)
[2025-02-13 04:15:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:42][root][INFO] - Training Epoch: 2/2, step 4481/7134 completed (loss: 0.024156585335731506, acc: 0.9941588640213013)
[2025-02-13 04:15:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:42][root][INFO] - Training Epoch: 2/2, step 4482/7134 completed (loss: 0.006973633076995611, acc: 0.9976019263267517)
[2025-02-13 04:15:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:43][root][INFO] - Training Epoch: 2/2, step 4483/7134 completed (loss: 0.015159661881625652, acc: 0.9955423474311829)
[2025-02-13 04:15:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:43][root][INFO] - Training Epoch: 2/2, step 4484/7134 completed (loss: 0.008352152071893215, acc: 0.9973545074462891)
[2025-02-13 04:15:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:44][root][INFO] - Training Epoch: 2/2, step 4485/7134 completed (loss: 0.005127187818288803, acc: 0.9984685778617859)
[2025-02-13 04:15:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:44][root][INFO] - Training Epoch: 2/2, step 4486/7134 completed (loss: 0.0024414805229753256, acc: 1.0)
[2025-02-13 04:15:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:45][root][INFO] - Training Epoch: 2/2, step 4487/7134 completed (loss: 0.008269642479717731, acc: 0.9986206889152527)
[2025-02-13 04:15:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:45][root][INFO] - Training Epoch: 2/2, step 4488/7134 completed (loss: 0.008836683817207813, acc: 0.9964028596878052)
[2025-02-13 04:15:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:45][root][INFO] - Training Epoch: 2/2, step 4489/7134 completed (loss: 0.009483939968049526, acc: 0.9982817769050598)
[2025-02-13 04:15:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:46][root][INFO] - Training Epoch: 2/2, step 4490/7134 completed (loss: 0.0061356076039373875, acc: 0.9962962865829468)
[2025-02-13 04:15:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:46][root][INFO] - Training Epoch: 2/2, step 4491/7134 completed (loss: 0.0018578782910481095, acc: 1.0)
[2025-02-13 04:15:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:47][root][INFO] - Training Epoch: 2/2, step 4492/7134 completed (loss: 0.006315584760159254, acc: 0.998161792755127)
[2025-02-13 04:15:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:47][root][INFO] - Training Epoch: 2/2, step 4493/7134 completed (loss: 0.006551938597112894, acc: 0.9986245036125183)
[2025-02-13 04:15:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:48][root][INFO] - Training Epoch: 2/2, step 4494/7134 completed (loss: 0.016474023461341858, acc: 0.9956584572792053)
[2025-02-13 04:15:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:48][root][INFO] - Training Epoch: 2/2, step 4495/7134 completed (loss: 0.00749224703758955, acc: 0.9987228512763977)
[2025-02-13 04:15:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:48][root][INFO] - Training Epoch: 2/2, step 4496/7134 completed (loss: 0.007971125654876232, acc: 0.9971510171890259)
[2025-02-13 04:15:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:49][root][INFO] - Training Epoch: 2/2, step 4497/7134 completed (loss: 0.010250890627503395, acc: 0.9973509907722473)
[2025-02-13 04:15:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:49][root][INFO] - Training Epoch: 2/2, step 4498/7134 completed (loss: 0.04859856888651848, acc: 0.9927431344985962)
[2025-02-13 04:15:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:50][root][INFO] - Training Epoch: 2/2, step 4499/7134 completed (loss: 0.027183376252651215, acc: 0.9947826266288757)
[2025-02-13 04:15:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:50][root][INFO] - Training Epoch: 2/2, step 4500/7134 completed (loss: 0.019360747188329697, acc: 0.9944055676460266)
[2025-02-13 04:15:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:51][root][INFO] - Training Epoch: 2/2, step 4501/7134 completed (loss: 0.013713199645280838, acc: 0.99726402759552)
[2025-02-13 04:15:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:51][root][INFO] - Training Epoch: 2/2, step 4502/7134 completed (loss: 0.013026829808950424, acc: 0.9970015287399292)
[2025-02-13 04:15:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:51][root][INFO] - Training Epoch: 2/2, step 4503/7134 completed (loss: 0.009949695318937302, acc: 0.9967897534370422)
[2025-02-13 04:15:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:52][root][INFO] - Training Epoch: 2/2, step 4504/7134 completed (loss: 0.02256902866065502, acc: 0.9925261735916138)
[2025-02-13 04:15:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:52][root][INFO] - Training Epoch: 2/2, step 4505/7134 completed (loss: 0.019293617457151413, acc: 0.9938575029373169)
[2025-02-13 04:15:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:53][root][INFO] - Training Epoch: 2/2, step 4506/7134 completed (loss: 0.011730071157217026, acc: 0.9974259734153748)
[2025-02-13 04:15:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:53][root][INFO] - Training Epoch: 2/2, step 4507/7134 completed (loss: 0.012344853952527046, acc: 0.9957325458526611)
[2025-02-13 04:15:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:54][root][INFO] - Training Epoch: 2/2, step 4508/7134 completed (loss: 0.02506210468709469, acc: 0.9914529919624329)
[2025-02-13 04:15:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:54][root][INFO] - Training Epoch: 2/2, step 4509/7134 completed (loss: 0.02465057373046875, acc: 0.990604043006897)
[2025-02-13 04:15:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:54][root][INFO] - Training Epoch: 2/2, step 4510/7134 completed (loss: 0.04040103778243065, acc: 0.9873060584068298)
[2025-02-13 04:15:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:55][root][INFO] - Training Epoch: 2/2, step 4511/7134 completed (loss: 0.050748273730278015, acc: 0.9859353303909302)
[2025-02-13 04:15:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:55][root][INFO] - Training Epoch: 2/2, step 4512/7134 completed (loss: 0.028080038726329803, acc: 0.9910394549369812)
[2025-02-13 04:15:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:56][root][INFO] - Training Epoch: 2/2, step 4513/7134 completed (loss: 0.017600953578948975, acc: 0.992977499961853)
[2025-02-13 04:15:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:56][root][INFO] - Training Epoch: 2/2, step 4514/7134 completed (loss: 0.06380670517683029, acc: 0.9810218811035156)
[2025-02-13 04:15:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:57][root][INFO] - Training Epoch: 2/2, step 4515/7134 completed (loss: 0.018849533051252365, acc: 0.9940298795700073)
[2025-02-13 04:15:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:57][root][INFO] - Training Epoch: 2/2, step 4516/7134 completed (loss: 0.012137031182646751, acc: 0.9963280558586121)
[2025-02-13 04:15:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:57][root][INFO] - Training Epoch: 2/2, step 4517/7134 completed (loss: 0.022435294464230537, acc: 0.9923175573348999)
[2025-02-13 04:15:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:58][root][INFO] - Training Epoch: 2/2, step 4518/7134 completed (loss: 0.032335951924324036, acc: 0.9897959232330322)
[2025-02-13 04:15:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:58][root][INFO] - Training Epoch: 2/2, step 4519/7134 completed (loss: 0.02541179209947586, acc: 0.993954062461853)
[2025-02-13 04:15:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:59][root][INFO] - Training Epoch: 2/2, step 4520/7134 completed (loss: 0.016296328976750374, acc: 0.9957627058029175)
[2025-02-13 04:15:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:15:59][root][INFO] - Training Epoch: 2/2, step 4521/7134 completed (loss: 0.034584660083055496, acc: 0.9915764331817627)
[2025-02-13 04:15:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:00][root][INFO] - Training Epoch: 2/2, step 4522/7134 completed (loss: 0.02771645411849022, acc: 0.991183876991272)
[2025-02-13 04:16:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:00][root][INFO] - Training Epoch: 2/2, step 4523/7134 completed (loss: 0.025489646941423416, acc: 0.9917159676551819)
[2025-02-13 04:16:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:01][root][INFO] - Training Epoch: 2/2, step 4524/7134 completed (loss: 0.04076908156275749, acc: 0.9878542423248291)
[2025-02-13 04:16:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:01][root][INFO] - Training Epoch: 2/2, step 4525/7134 completed (loss: 0.01570778526365757, acc: 0.9944444298744202)
[2025-02-13 04:16:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:01][root][INFO] - Training Epoch: 2/2, step 4526/7134 completed (loss: 0.03832830488681793, acc: 0.9883871078491211)
[2025-02-13 04:16:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:02][root][INFO] - Training Epoch: 2/2, step 4527/7134 completed (loss: 0.019864091649651527, acc: 0.9921962022781372)
[2025-02-13 04:16:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:02][root][INFO] - Training Epoch: 2/2, step 4528/7134 completed (loss: 0.031738847494125366, acc: 0.9871465563774109)
[2025-02-13 04:16:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:03][root][INFO] - Training Epoch: 2/2, step 4529/7134 completed (loss: 0.04740644246339798, acc: 0.9883419871330261)
[2025-02-13 04:16:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:03][root][INFO] - Training Epoch: 2/2, step 4530/7134 completed (loss: 0.013570314273238182, acc: 0.9985994100570679)
[2025-02-13 04:16:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:04][root][INFO] - Training Epoch: 2/2, step 4531/7134 completed (loss: 0.015402932651340961, acc: 0.9946140050888062)
[2025-02-13 04:16:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:04][root][INFO] - Training Epoch: 2/2, step 4532/7134 completed (loss: 0.04063891991972923, acc: 0.9890438318252563)
[2025-02-13 04:16:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:05][root][INFO] - Training Epoch: 2/2, step 4533/7134 completed (loss: 0.01629510335624218, acc: 0.9952550530433655)
[2025-02-13 04:16:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:05][root][INFO] - Training Epoch: 2/2, step 4534/7134 completed (loss: 0.01538984663784504, acc: 0.9941314458847046)
[2025-02-13 04:16:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:06][root][INFO] - Training Epoch: 2/2, step 4535/7134 completed (loss: 0.022842733189463615, acc: 0.9931662678718567)
[2025-02-13 04:16:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:06][root][INFO] - Training Epoch: 2/2, step 4536/7134 completed (loss: 0.026362212374806404, acc: 0.9952107071876526)
[2025-02-13 04:16:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:07][root][INFO] - Training Epoch: 2/2, step 4537/7134 completed (loss: 0.007885107770562172, acc: 0.9966480731964111)
[2025-02-13 04:16:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:07][root][INFO] - Training Epoch: 2/2, step 4538/7134 completed (loss: 0.025071289390325546, acc: 0.9954022765159607)
[2025-02-13 04:16:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:07][root][INFO] - Training Epoch: 2/2, step 4539/7134 completed (loss: 0.02309812232851982, acc: 0.9924487471580505)
[2025-02-13 04:16:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:08][root][INFO] - Training Epoch: 2/2, step 4540/7134 completed (loss: 0.02715316042304039, acc: 0.9913294911384583)
[2025-02-13 04:16:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:08][root][INFO] - Training Epoch: 2/2, step 4541/7134 completed (loss: 0.03636341542005539, acc: 0.987679660320282)
[2025-02-13 04:16:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:09][root][INFO] - Training Epoch: 2/2, step 4542/7134 completed (loss: 0.036688704043626785, acc: 0.9891774654388428)
[2025-02-13 04:16:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:09][root][INFO] - Training Epoch: 2/2, step 4543/7134 completed (loss: 0.04627588018774986, acc: 0.9857397675514221)
[2025-02-13 04:16:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:10][root][INFO] - Training Epoch: 2/2, step 4544/7134 completed (loss: 0.030478840693831444, acc: 0.9908536672592163)
[2025-02-13 04:16:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:10][root][INFO] - Training Epoch: 2/2, step 4545/7134 completed (loss: 0.03834526613354683, acc: 0.9903661012649536)
[2025-02-13 04:16:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:11][root][INFO] - Training Epoch: 2/2, step 4546/7134 completed (loss: 0.048585131764411926, acc: 0.9915966391563416)
[2025-02-13 04:16:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:11][root][INFO] - Training Epoch: 2/2, step 4547/7134 completed (loss: 0.022180715575814247, acc: 0.9965397715568542)
[2025-02-13 04:16:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:11][root][INFO] - Training Epoch: 2/2, step 4548/7134 completed (loss: 0.032865118235349655, acc: 0.9900497794151306)
[2025-02-13 04:16:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:12][root][INFO] - Training Epoch: 2/2, step 4549/7134 completed (loss: 0.033768512308597565, acc: 0.9828326106071472)
[2025-02-13 04:16:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:12][root][INFO] - Training Epoch: 2/2, step 4550/7134 completed (loss: 0.039860621094703674, acc: 0.9932998418807983)
[2025-02-13 04:16:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:13][root][INFO] - Training Epoch: 2/2, step 4551/7134 completed (loss: 0.025864046066999435, acc: 0.9942857027053833)
[2025-02-13 04:16:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:13][root][INFO] - Training Epoch: 2/2, step 4552/7134 completed (loss: 0.011533054523169994, acc: 0.9980952143669128)
[2025-02-13 04:16:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:13][root][INFO] - Training Epoch: 2/2, step 4553/7134 completed (loss: 0.034325264394283295, acc: 0.9915397763252258)
[2025-02-13 04:16:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:14][root][INFO] - Training Epoch: 2/2, step 4554/7134 completed (loss: 0.04066705331206322, acc: 0.9873873591423035)
[2025-02-13 04:16:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:14][root][INFO] - Training Epoch: 2/2, step 4555/7134 completed (loss: 0.022607961669564247, acc: 0.9916897416114807)
[2025-02-13 04:16:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:15][root][INFO] - Training Epoch: 2/2, step 4556/7134 completed (loss: 0.03914661705493927, acc: 0.9864661693572998)
[2025-02-13 04:16:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:15][root][INFO] - Training Epoch: 2/2, step 4557/7134 completed (loss: 0.043066609650850296, acc: 0.985401451587677)
[2025-02-13 04:16:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:15][root][INFO] - Training Epoch: 2/2, step 4558/7134 completed (loss: 0.038837406784296036, acc: 0.9915825128555298)
[2025-02-13 04:16:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:16][root][INFO] - Training Epoch: 2/2, step 4559/7134 completed (loss: 0.0484233982861042, acc: 0.9848739504814148)
[2025-02-13 04:16:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:16][root][INFO] - Training Epoch: 2/2, step 4560/7134 completed (loss: 0.017792483791708946, acc: 0.9927536249160767)
[2025-02-13 04:16:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:17][root][INFO] - Training Epoch: 2/2, step 4561/7134 completed (loss: 0.04044438526034355, acc: 0.9862744808197021)
[2025-02-13 04:16:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:17][root][INFO] - Training Epoch: 2/2, step 4562/7134 completed (loss: 0.03729930520057678, acc: 0.990791916847229)
[2025-02-13 04:16:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:18][root][INFO] - Training Epoch: 2/2, step 4563/7134 completed (loss: 0.02820282056927681, acc: 0.9908397197723389)
[2025-02-13 04:16:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:18][root][INFO] - Training Epoch: 2/2, step 4564/7134 completed (loss: 0.03930366411805153, acc: 0.9873060584068298)
[2025-02-13 04:16:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:19][root][INFO] - Training Epoch: 2/2, step 4565/7134 completed (loss: 0.022014444693922997, acc: 0.9923518300056458)
[2025-02-13 04:16:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:19][root][INFO] - Training Epoch: 2/2, step 4566/7134 completed (loss: 0.03010665439069271, acc: 0.9946236610412598)
[2025-02-13 04:16:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:19][root][INFO] - Training Epoch: 2/2, step 4567/7134 completed (loss: 0.01424560509622097, acc: 0.998161792755127)
[2025-02-13 04:16:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:20][root][INFO] - Training Epoch: 2/2, step 4568/7134 completed (loss: 0.03576109558343887, acc: 0.9885321259498596)
[2025-02-13 04:16:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:20][root][INFO] - Training Epoch: 2/2, step 4569/7134 completed (loss: 0.006408276967704296, acc: 0.998039186000824)
[2025-02-13 04:16:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:21][root][INFO] - Training Epoch: 2/2, step 4570/7134 completed (loss: 0.01728675328195095, acc: 0.9941691160202026)
[2025-02-13 04:16:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:21][root][INFO] - Training Epoch: 2/2, step 4571/7134 completed (loss: 0.020182112231850624, acc: 0.9961488842964172)
[2025-02-13 04:16:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:21][root][INFO] - Training Epoch: 2/2, step 4572/7134 completed (loss: 0.024216579273343086, acc: 0.9922118186950684)
[2025-02-13 04:16:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:22][root][INFO] - Training Epoch: 2/2, step 4573/7134 completed (loss: 0.013617589138448238, acc: 0.9957982897758484)
[2025-02-13 04:16:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:22][root][INFO] - Training Epoch: 2/2, step 4574/7134 completed (loss: 0.005915878806263208, acc: 0.9982331991195679)
[2025-02-13 04:16:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:23][root][INFO] - Training Epoch: 2/2, step 4575/7134 completed (loss: 0.030006205663084984, acc: 0.9893617033958435)
[2025-02-13 04:16:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:23][root][INFO] - Training Epoch: 2/2, step 4576/7134 completed (loss: 0.008137475699186325, acc: 0.9962359070777893)
[2025-02-13 04:16:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:24][root][INFO] - Training Epoch: 2/2, step 4577/7134 completed (loss: 0.025155646726489067, acc: 0.9915730357170105)
[2025-02-13 04:16:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:24][root][INFO] - Training Epoch: 2/2, step 4578/7134 completed (loss: 0.0071639325469732285, acc: 0.9970887899398804)
[2025-02-13 04:16:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:25][root][INFO] - Training Epoch: 2/2, step 4579/7134 completed (loss: 0.034206777811050415, acc: 0.9936608672142029)
[2025-02-13 04:16:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:25][root][INFO] - Training Epoch: 2/2, step 4580/7134 completed (loss: 0.015519867651164532, acc: 0.9940298795700073)
[2025-02-13 04:16:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:25][root][INFO] - Training Epoch: 2/2, step 4581/7134 completed (loss: 0.008505409583449364, acc: 0.998236358165741)
[2025-02-13 04:16:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:26][root][INFO] - Training Epoch: 2/2, step 4582/7134 completed (loss: 0.01440589502453804, acc: 0.9934210777282715)
[2025-02-13 04:16:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:26][root][INFO] - Training Epoch: 2/2, step 4583/7134 completed (loss: 0.004917467944324017, acc: 1.0)
[2025-02-13 04:16:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:27][root][INFO] - Training Epoch: 2/2, step 4584/7134 completed (loss: 0.0061465175822377205, acc: 0.998389720916748)
[2025-02-13 04:16:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:27][root][INFO] - Training Epoch: 2/2, step 4585/7134 completed (loss: 0.006897913757711649, acc: 0.9967637658119202)
[2025-02-13 04:16:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:28][root][INFO] - Training Epoch: 2/2, step 4586/7134 completed (loss: 0.01633770577609539, acc: 0.9960988163948059)
[2025-02-13 04:16:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:28][root][INFO] - Training Epoch: 2/2, step 4587/7134 completed (loss: 0.026634301990270615, acc: 0.9945205450057983)
[2025-02-13 04:16:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:28][root][INFO] - Training Epoch: 2/2, step 4588/7134 completed (loss: 0.03603576496243477, acc: 0.9869186282157898)
[2025-02-13 04:16:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:29][root][INFO] - Training Epoch: 2/2, step 4589/7134 completed (loss: 0.04860933497548103, acc: 0.9835329055786133)
[2025-02-13 04:16:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:29][root][INFO] - Training Epoch: 2/2, step 4590/7134 completed (loss: 0.030046893283724785, acc: 0.9944751262664795)
[2025-02-13 04:16:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:30][root][INFO] - Training Epoch: 2/2, step 4591/7134 completed (loss: 0.020092394202947617, acc: 0.9920844435691833)
[2025-02-13 04:16:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:30][root][INFO] - Training Epoch: 2/2, step 4592/7134 completed (loss: 0.032502759248018265, acc: 0.9840348362922668)
[2025-02-13 04:16:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:31][root][INFO] - Training Epoch: 2/2, step 4593/7134 completed (loss: 0.030414940789341927, acc: 0.9896296262741089)
[2025-02-13 04:16:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:31][root][INFO] - Training Epoch: 2/2, step 4594/7134 completed (loss: 0.0317118763923645, acc: 0.9933333396911621)
[2025-02-13 04:16:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:31][root][INFO] - Training Epoch: 2/2, step 4595/7134 completed (loss: 0.012829957529902458, acc: 0.9955423474311829)
[2025-02-13 04:16:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:32][root][INFO] - Training Epoch: 2/2, step 4596/7134 completed (loss: 0.01953282579779625, acc: 0.9904761910438538)
[2025-02-13 04:16:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:32][root][INFO] - Training Epoch: 2/2, step 4597/7134 completed (loss: 0.011971773579716682, acc: 0.9987849593162537)
[2025-02-13 04:16:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:33][root][INFO] - Training Epoch: 2/2, step 4598/7134 completed (loss: 0.0375988744199276, acc: 0.9926380515098572)
[2025-02-13 04:16:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:33][root][INFO] - Training Epoch: 2/2, step 4599/7134 completed (loss: 0.01897399127483368, acc: 0.995230495929718)
[2025-02-13 04:16:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:34][root][INFO] - Training Epoch: 2/2, step 4600/7134 completed (loss: 0.016352200880646706, acc: 0.9959404468536377)
[2025-02-13 04:16:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:34][root][INFO] - Training Epoch: 2/2, step 4601/7134 completed (loss: 0.03655474632978439, acc: 0.9908735156059265)
[2025-02-13 04:16:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:35][root][INFO] - Training Epoch: 2/2, step 4602/7134 completed (loss: 0.01930803619325161, acc: 0.9891008138656616)
[2025-02-13 04:16:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:35][root][INFO] - Training Epoch: 2/2, step 4603/7134 completed (loss: 0.020584214478731155, acc: 0.9917241334915161)
[2025-02-13 04:16:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:35][root][INFO] - Training Epoch: 2/2, step 4604/7134 completed (loss: 0.012873144820332527, acc: 0.995184600353241)
[2025-02-13 04:16:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:36][root][INFO] - Training Epoch: 2/2, step 4605/7134 completed (loss: 0.049080923199653625, acc: 0.9847161769866943)
[2025-02-13 04:16:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:36][root][INFO] - Training Epoch: 2/2, step 4606/7134 completed (loss: 0.015199411660432816, acc: 0.9954128265380859)
[2025-02-13 04:16:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:37][root][INFO] - Training Epoch: 2/2, step 4607/7134 completed (loss: 0.009099923074245453, acc: 0.9949495196342468)
[2025-02-13 04:16:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:37][root][INFO] - Training Epoch: 2/2, step 4608/7134 completed (loss: 0.00838466826826334, acc: 0.9970370531082153)
[2025-02-13 04:16:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:38][root][INFO] - Training Epoch: 2/2, step 4609/7134 completed (loss: 0.01913517341017723, acc: 0.9944953918457031)
[2025-02-13 04:16:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:38][root][INFO] - Training Epoch: 2/2, step 4610/7134 completed (loss: 0.017722338438034058, acc: 0.9933599233627319)
[2025-02-13 04:16:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:38][root][INFO] - Training Epoch: 2/2, step 4611/7134 completed (loss: 0.004823770839720964, acc: 1.0)
[2025-02-13 04:16:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:39][root][INFO] - Training Epoch: 2/2, step 4612/7134 completed (loss: 0.01582954451441765, acc: 0.9944238066673279)
[2025-02-13 04:16:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:39][root][INFO] - Training Epoch: 2/2, step 4613/7134 completed (loss: 0.02039109356701374, acc: 0.9947916865348816)
[2025-02-13 04:16:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:40][root][INFO] - Training Epoch: 2/2, step 4614/7134 completed (loss: 0.02658703364431858, acc: 0.9886363744735718)
[2025-02-13 04:16:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:40][root][INFO] - Training Epoch: 2/2, step 4615/7134 completed (loss: 0.030850674957036972, acc: 0.9891451597213745)
[2025-02-13 04:16:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:41][root][INFO] - Training Epoch: 2/2, step 4616/7134 completed (loss: 0.03765435889363289, acc: 0.9911242723464966)
[2025-02-13 04:16:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:41][root][INFO] - Training Epoch: 2/2, step 4617/7134 completed (loss: 0.01913563348352909, acc: 0.9939939975738525)
[2025-02-13 04:16:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:41][root][INFO] - Training Epoch: 2/2, step 4618/7134 completed (loss: 0.024067984893918037, acc: 0.9902912378311157)
[2025-02-13 04:16:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:42][root][INFO] - Training Epoch: 2/2, step 4619/7134 completed (loss: 0.0072375452145934105, acc: 1.0)
[2025-02-13 04:16:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:42][root][INFO] - Training Epoch: 2/2, step 4620/7134 completed (loss: 0.007363416254520416, acc: 0.9985507130622864)
[2025-02-13 04:16:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:43][root][INFO] - Training Epoch: 2/2, step 4621/7134 completed (loss: 0.01240123063325882, acc: 0.9953703880310059)
[2025-02-13 04:16:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:43][root][INFO] - Training Epoch: 2/2, step 4622/7134 completed (loss: 0.024929244071245193, acc: 0.9937577843666077)
[2025-02-13 04:16:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:44][root][INFO] - Training Epoch: 2/2, step 4623/7134 completed (loss: 0.03535204008221626, acc: 0.98562091588974)
[2025-02-13 04:16:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:44][root][INFO] - Training Epoch: 2/2, step 4624/7134 completed (loss: 0.04283737391233444, acc: 0.9876543283462524)
[2025-02-13 04:16:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:44][root][INFO] - Training Epoch: 2/2, step 4625/7134 completed (loss: 0.04568931460380554, acc: 0.9802955389022827)
[2025-02-13 04:16:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:45][root][INFO] - Training Epoch: 2/2, step 4626/7134 completed (loss: 0.03276771306991577, acc: 0.9892617464065552)
[2025-02-13 04:16:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:45][root][INFO] - Training Epoch: 2/2, step 4627/7134 completed (loss: 0.050401169806718826, acc: 0.987860381603241)
[2025-02-13 04:16:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:46][root][INFO] - Training Epoch: 2/2, step 4628/7134 completed (loss: 0.03326515108346939, acc: 0.9855072498321533)
[2025-02-13 04:16:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:46][root][INFO] - Training Epoch: 2/2, step 4629/7134 completed (loss: 0.03825916722416878, acc: 0.988034188747406)
[2025-02-13 04:16:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:47][root][INFO] - Training Epoch: 2/2, step 4630/7134 completed (loss: 0.03651559725403786, acc: 0.9867899417877197)
[2025-02-13 04:16:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:47][root][INFO] - Training Epoch: 2/2, step 4631/7134 completed (loss: 0.0165200624614954, acc: 0.9966813921928406)
[2025-02-13 04:16:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:48][root][INFO] - Training Epoch: 2/2, step 4632/7134 completed (loss: 0.025472618639469147, acc: 0.9941176176071167)
[2025-02-13 04:16:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:48][root][INFO] - Training Epoch: 2/2, step 4633/7134 completed (loss: 0.027419500052928925, acc: 0.99190753698349)
[2025-02-13 04:16:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:49][root][INFO] - Training Epoch: 2/2, step 4634/7134 completed (loss: 0.020014962181448936, acc: 0.9941747784614563)
[2025-02-13 04:16:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:49][root][INFO] - Training Epoch: 2/2, step 4635/7134 completed (loss: 0.05181692913174629, acc: 0.9893292784690857)
[2025-02-13 04:16:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:49][root][INFO] - Training Epoch: 2/2, step 4636/7134 completed (loss: 0.00783985573798418, acc: 1.0)
[2025-02-13 04:16:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:50][root][INFO] - Training Epoch: 2/2, step 4637/7134 completed (loss: 0.011318172328174114, acc: 0.9979838728904724)
[2025-02-13 04:16:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:50][root][INFO] - Training Epoch: 2/2, step 4638/7134 completed (loss: 0.027139462530612946, acc: 0.9907529950141907)
[2025-02-13 04:16:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:51][root][INFO] - Training Epoch: 2/2, step 4639/7134 completed (loss: 0.03174285590648651, acc: 0.9951338171958923)
[2025-02-13 04:16:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:51][root][INFO] - Training Epoch: 2/2, step 4640/7134 completed (loss: 0.03243601322174072, acc: 0.9910256266593933)
[2025-02-13 04:16:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:52][root][INFO] - Training Epoch: 2/2, step 4641/7134 completed (loss: 0.01265675388276577, acc: 0.9971590638160706)
[2025-02-13 04:16:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:52][root][INFO] - Training Epoch: 2/2, step 4642/7134 completed (loss: 0.0062557305209338665, acc: 0.9973545074462891)
[2025-02-13 04:16:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:52][root][INFO] - Training Epoch: 2/2, step 4643/7134 completed (loss: 0.019651565700769424, acc: 0.9952662587165833)
[2025-02-13 04:16:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:53][root][INFO] - Training Epoch: 2/2, step 4644/7134 completed (loss: 0.02039828523993492, acc: 0.9917159676551819)
[2025-02-13 04:16:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:53][root][INFO] - Training Epoch: 2/2, step 4645/7134 completed (loss: 0.00931859202682972, acc: 0.9958275556564331)
[2025-02-13 04:16:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:54][root][INFO] - Training Epoch: 2/2, step 4646/7134 completed (loss: 0.006182820536196232, acc: 1.0)
[2025-02-13 04:16:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:54][root][INFO] - Training Epoch: 2/2, step 4647/7134 completed (loss: 0.05381564423441887, acc: 0.9908854365348816)
[2025-02-13 04:16:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:55][root][INFO] - Training Epoch: 2/2, step 4648/7134 completed (loss: 0.024663593620061874, acc: 0.9890260696411133)
[2025-02-13 04:16:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:55][root][INFO] - Training Epoch: 2/2, step 4649/7134 completed (loss: 0.023010356351733208, acc: 0.9912434220314026)
[2025-02-13 04:16:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:56][root][INFO] - Training Epoch: 2/2, step 4650/7134 completed (loss: 0.027088062837719917, acc: 0.9866071343421936)
[2025-02-13 04:16:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:56][root][INFO] - Training Epoch: 2/2, step 4651/7134 completed (loss: 0.07187449187040329, acc: 0.9847908616065979)
[2025-02-13 04:16:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:57][root][INFO] - Training Epoch: 2/2, step 4652/7134 completed (loss: 0.019261790439486504, acc: 0.9939024448394775)
[2025-02-13 04:16:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:57][root][INFO] - Training Epoch: 2/2, step 4653/7134 completed (loss: 0.02415575087070465, acc: 0.9946236610412598)
[2025-02-13 04:16:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:57][root][INFO] - Training Epoch: 2/2, step 4654/7134 completed (loss: 0.04556382820010185, acc: 0.9878197312355042)
[2025-02-13 04:16:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:58][root][INFO] - Training Epoch: 2/2, step 4655/7134 completed (loss: 0.017612092196941376, acc: 0.99589604139328)
[2025-02-13 04:16:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:58][root][INFO] - Training Epoch: 2/2, step 4656/7134 completed (loss: 0.026290303096175194, acc: 0.9924242496490479)
[2025-02-13 04:16:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:59][root][INFO] - Training Epoch: 2/2, step 4657/7134 completed (loss: 0.04615354165434837, acc: 0.9858356714248657)
[2025-02-13 04:16:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:16:59][root][INFO] - Training Epoch: 2/2, step 4658/7134 completed (loss: 0.03460635989904404, acc: 0.9851149916648865)
[2025-02-13 04:16:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:00][root][INFO] - Training Epoch: 2/2, step 4659/7134 completed (loss: 0.041508618742227554, acc: 0.9858611822128296)
[2025-02-13 04:17:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:00][root][INFO] - Training Epoch: 2/2, step 4660/7134 completed (loss: 0.05888482555747032, acc: 0.9874607920646667)
[2025-02-13 04:17:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:01][root][INFO] - Training Epoch: 2/2, step 4661/7134 completed (loss: 0.03248747065663338, acc: 0.9923780560493469)
[2025-02-13 04:17:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:01][root][INFO] - Training Epoch: 2/2, step 4662/7134 completed (loss: 0.060933224856853485, acc: 0.9846938848495483)
[2025-02-13 04:17:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:01][root][INFO] - Training Epoch: 2/2, step 4663/7134 completed (loss: 0.05046211555600166, acc: 0.9879679083824158)
[2025-02-13 04:17:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:02][root][INFO] - Training Epoch: 2/2, step 4664/7134 completed (loss: 0.02044852264225483, acc: 0.9960370063781738)
[2025-02-13 04:17:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:02][root][INFO] - Training Epoch: 2/2, step 4665/7134 completed (loss: 0.017338719218969345, acc: 0.9959072470664978)
[2025-02-13 04:17:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:03][root][INFO] - Training Epoch: 2/2, step 4666/7134 completed (loss: 0.0392199382185936, acc: 0.9895697236061096)
[2025-02-13 04:17:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:03][root][INFO] - Training Epoch: 2/2, step 4667/7134 completed (loss: 0.045017220079898834, acc: 0.9879518151283264)
[2025-02-13 04:17:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:04][root][INFO] - Training Epoch: 2/2, step 4668/7134 completed (loss: 0.009374840185046196, acc: 0.9982078671455383)
[2025-02-13 04:17:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:04][root][INFO] - Training Epoch: 2/2, step 4669/7134 completed (loss: 0.019847538322210312, acc: 0.9940828680992126)
[2025-02-13 04:17:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:04][root][INFO] - Training Epoch: 2/2, step 4670/7134 completed (loss: 0.04007136449217796, acc: 0.988304078578949)
[2025-02-13 04:17:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:05][root][INFO] - Training Epoch: 2/2, step 4671/7134 completed (loss: 0.030865460634231567, acc: 0.9873060584068298)
[2025-02-13 04:17:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:05][root][INFO] - Training Epoch: 2/2, step 4672/7134 completed (loss: 0.03277052193880081, acc: 0.9878048896789551)
[2025-02-13 04:17:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:06][root][INFO] - Training Epoch: 2/2, step 4673/7134 completed (loss: 0.03451361134648323, acc: 0.9921996593475342)
[2025-02-13 04:17:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:06][root][INFO] - Training Epoch: 2/2, step 4674/7134 completed (loss: 0.031448885798454285, acc: 0.9920760989189148)
[2025-02-13 04:17:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:07][root][INFO] - Training Epoch: 2/2, step 4675/7134 completed (loss: 0.029896941035985947, acc: 0.9910827875137329)
[2025-02-13 04:17:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:07][root][INFO] - Training Epoch: 2/2, step 4676/7134 completed (loss: 0.008074373938143253, acc: 0.9984939694404602)
[2025-02-13 04:17:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:07][root][INFO] - Training Epoch: 2/2, step 4677/7134 completed (loss: 0.02815200388431549, acc: 0.9912280440330505)
[2025-02-13 04:17:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:08][root][INFO] - Training Epoch: 2/2, step 4678/7134 completed (loss: 0.04042381793260574, acc: 0.9910846948623657)
[2025-02-13 04:17:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:08][root][INFO] - Training Epoch: 2/2, step 4679/7134 completed (loss: 0.018857207149267197, acc: 0.9972413778305054)
[2025-02-13 04:17:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:09][root][INFO] - Training Epoch: 2/2, step 4680/7134 completed (loss: 0.0311940498650074, acc: 0.9883177280426025)
[2025-02-13 04:17:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:09][root][INFO] - Training Epoch: 2/2, step 4681/7134 completed (loss: 0.04403180256485939, acc: 0.9907692074775696)
[2025-02-13 04:17:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:10][root][INFO] - Training Epoch: 2/2, step 4682/7134 completed (loss: 0.01246592216193676, acc: 0.9975669384002686)
[2025-02-13 04:17:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:10][root][INFO] - Training Epoch: 2/2, step 4683/7134 completed (loss: 0.030865389853715897, acc: 0.9914407730102539)
[2025-02-13 04:17:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:10][root][INFO] - Training Epoch: 2/2, step 4684/7134 completed (loss: 0.05043399706482887, acc: 0.9811320900917053)
[2025-02-13 04:17:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:11][root][INFO] - Training Epoch: 2/2, step 4685/7134 completed (loss: 0.025450851768255234, acc: 0.9928278923034668)
[2025-02-13 04:17:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:11][root][INFO] - Training Epoch: 2/2, step 4686/7134 completed (loss: 0.023857448250055313, acc: 0.9941792488098145)
[2025-02-13 04:17:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:12][root][INFO] - Training Epoch: 2/2, step 4687/7134 completed (loss: 0.07729168236255646, acc: 0.9785276055335999)
[2025-02-13 04:17:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:12][root][INFO] - Training Epoch: 2/2, step 4688/7134 completed (loss: 0.09661050140857697, acc: 0.9671458005905151)
[2025-02-13 04:17:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:13][root][INFO] - Training Epoch: 2/2, step 4689/7134 completed (loss: 0.0774056538939476, acc: 0.9749631881713867)
[2025-02-13 04:17:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:13][root][INFO] - Training Epoch: 2/2, step 4690/7134 completed (loss: 0.07581865042448044, acc: 0.9832285046577454)
[2025-02-13 04:17:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:14][root][INFO] - Training Epoch: 2/2, step 4691/7134 completed (loss: 0.0273464173078537, acc: 0.9875283241271973)
[2025-02-13 04:17:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:14][root][INFO] - Training Epoch: 2/2, step 4692/7134 completed (loss: 0.08423201739788055, acc: 0.9766454100608826)
[2025-02-13 04:17:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:14][root][INFO] - Training Epoch: 2/2, step 4693/7134 completed (loss: 0.10072589665651321, acc: 0.9684210419654846)
[2025-02-13 04:17:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:15][root][INFO] - Training Epoch: 2/2, step 4694/7134 completed (loss: 0.11195232719182968, acc: 0.9858012199401855)
[2025-02-13 04:17:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:15][root][INFO] - Training Epoch: 2/2, step 4695/7134 completed (loss: 0.051799122244119644, acc: 0.9835796356201172)
[2025-02-13 04:17:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:16][root][INFO] - Training Epoch: 2/2, step 4696/7134 completed (loss: 0.023681722581386566, acc: 0.9940564632415771)
[2025-02-13 04:17:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:16][root][INFO] - Training Epoch: 2/2, step 4697/7134 completed (loss: 0.02976907044649124, acc: 0.9887217879295349)
[2025-02-13 04:17:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:17][root][INFO] - Training Epoch: 2/2, step 4698/7134 completed (loss: 0.0872185230255127, acc: 0.9751908183097839)
[2025-02-13 04:17:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:17][root][INFO] - Training Epoch: 2/2, step 4699/7134 completed (loss: 0.03347614035010338, acc: 0.9853747487068176)
[2025-02-13 04:17:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:17][root][INFO] - Training Epoch: 2/2, step 4700/7134 completed (loss: 0.03701429069042206, acc: 0.9895678162574768)
[2025-02-13 04:17:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:18][root][INFO] - Training Epoch: 2/2, step 4701/7134 completed (loss: 0.03580596297979355, acc: 0.9904153347015381)
[2025-02-13 04:17:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:18][root][INFO] - Training Epoch: 2/2, step 4702/7134 completed (loss: 0.015213655307888985, acc: 0.9952229261398315)
[2025-02-13 04:17:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:19][root][INFO] - Training Epoch: 2/2, step 4703/7134 completed (loss: 0.03766666725277901, acc: 0.9945945739746094)
[2025-02-13 04:17:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:19][root][INFO] - Training Epoch: 2/2, step 4704/7134 completed (loss: 0.01788415014743805, acc: 0.9929659962654114)
[2025-02-13 04:17:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:20][root][INFO] - Training Epoch: 2/2, step 4705/7134 completed (loss: 0.024751773104071617, acc: 0.9918128848075867)
[2025-02-13 04:17:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:20][root][INFO] - Training Epoch: 2/2, step 4706/7134 completed (loss: 0.048932984471321106, acc: 0.9890965819358826)
[2025-02-13 04:17:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:21][root][INFO] - Training Epoch: 2/2, step 4707/7134 completed (loss: 0.13393156230449677, acc: 0.9659863710403442)
[2025-02-13 04:17:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:21][root][INFO] - Training Epoch: 2/2, step 4708/7134 completed (loss: 0.015122118405997753, acc: 0.9925187230110168)
[2025-02-13 04:17:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:21][root][INFO] - Training Epoch: 2/2, step 4709/7134 completed (loss: 0.014285095036029816, acc: 0.9973649382591248)
[2025-02-13 04:17:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:22][root][INFO] - Training Epoch: 2/2, step 4710/7134 completed (loss: 0.02407153695821762, acc: 0.9927219748497009)
[2025-02-13 04:17:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:22][root][INFO] - Training Epoch: 2/2, step 4711/7134 completed (loss: 0.04529047757387161, acc: 0.9864176511764526)
[2025-02-13 04:17:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:23][root][INFO] - Training Epoch: 2/2, step 4712/7134 completed (loss: 0.06066906079649925, acc: 0.9803030490875244)
[2025-02-13 04:17:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:23][root][INFO] - Training Epoch: 2/2, step 4713/7134 completed (loss: 0.01599620282649994, acc: 0.9946428537368774)
[2025-02-13 04:17:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:24][root][INFO] - Training Epoch: 2/2, step 4714/7134 completed (loss: 0.04253816232085228, acc: 0.9850467443466187)
[2025-02-13 04:17:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:24][root][INFO] - Training Epoch: 2/2, step 4715/7134 completed (loss: 0.07542617619037628, acc: 0.9786780476570129)
[2025-02-13 04:17:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:24][root][INFO] - Training Epoch: 2/2, step 4716/7134 completed (loss: 0.005590618122369051, acc: 0.9984756112098694)
[2025-02-13 04:17:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:25][root][INFO] - Training Epoch: 2/2, step 4717/7134 completed (loss: 0.038319867104291916, acc: 0.995555579662323)
[2025-02-13 04:17:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:25][root][INFO] - Training Epoch: 2/2, step 4718/7134 completed (loss: 0.019620325416326523, acc: 0.9933775067329407)
[2025-02-13 04:17:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:26][root][INFO] - Training Epoch: 2/2, step 4719/7134 completed (loss: 0.021701572462916374, acc: 0.9941002726554871)
[2025-02-13 04:17:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:26][root][INFO] - Training Epoch: 2/2, step 4720/7134 completed (loss: 0.004687406122684479, acc: 1.0)
[2025-02-13 04:17:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:27][root][INFO] - Training Epoch: 2/2, step 4721/7134 completed (loss: 0.0034917197190225124, acc: 1.0)
[2025-02-13 04:17:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:27][root][INFO] - Training Epoch: 2/2, step 4722/7134 completed (loss: 0.009571786038577557, acc: 0.9955621361732483)
[2025-02-13 04:17:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:27][root][INFO] - Training Epoch: 2/2, step 4723/7134 completed (loss: 0.009634437039494514, acc: 0.9969512224197388)
[2025-02-13 04:17:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:28][root][INFO] - Training Epoch: 2/2, step 4724/7134 completed (loss: 0.024253960698843002, acc: 0.9911242723464966)
[2025-02-13 04:17:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:28][root][INFO] - Training Epoch: 2/2, step 4725/7134 completed (loss: 0.0055502611212432384, acc: 0.9986807107925415)
[2025-02-13 04:17:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:29][root][INFO] - Training Epoch: 2/2, step 4726/7134 completed (loss: 0.03772447630763054, acc: 0.9858155846595764)
[2025-02-13 04:17:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:29][root][INFO] - Training Epoch: 2/2, step 4727/7134 completed (loss: 0.031121887266635895, acc: 0.9905956387519836)
[2025-02-13 04:17:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:30][root][INFO] - Training Epoch: 2/2, step 4728/7134 completed (loss: 0.0209248885512352, acc: 0.9934102296829224)
[2025-02-13 04:17:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:30][root][INFO] - Training Epoch: 2/2, step 4729/7134 completed (loss: 0.012639275752007961, acc: 0.997183084487915)
[2025-02-13 04:17:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:30][root][INFO] - Training Epoch: 2/2, step 4730/7134 completed (loss: 0.02161347307264805, acc: 0.992343008518219)
[2025-02-13 04:17:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:31][root][INFO] - Training Epoch: 2/2, step 4731/7134 completed (loss: 0.028774380683898926, acc: 0.9954198598861694)
[2025-02-13 04:17:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:31][root][INFO] - Training Epoch: 2/2, step 4732/7134 completed (loss: 0.05590774491429329, acc: 0.9818511605262756)
[2025-02-13 04:17:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:32][root][INFO] - Training Epoch: 2/2, step 4733/7134 completed (loss: 0.007173147052526474, acc: 0.9967690110206604)
[2025-02-13 04:17:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:32][root][INFO] - Training Epoch: 2/2, step 4734/7134 completed (loss: 0.02777380682528019, acc: 0.9888712167739868)
[2025-02-13 04:17:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:32][root][INFO] - Training Epoch: 2/2, step 4735/7134 completed (loss: 0.02197827585041523, acc: 0.9924924969673157)
[2025-02-13 04:17:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:33][root][INFO] - Training Epoch: 2/2, step 4736/7134 completed (loss: 0.06198357790708542, acc: 0.9881656765937805)
[2025-02-13 04:17:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:33][root][INFO] - Training Epoch: 2/2, step 4737/7134 completed (loss: 0.023257547989487648, acc: 0.9937304258346558)
[2025-02-13 04:17:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:34][root][INFO] - Training Epoch: 2/2, step 4738/7134 completed (loss: 0.01671620085835457, acc: 0.9969651103019714)
[2025-02-13 04:17:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:34][root][INFO] - Training Epoch: 2/2, step 4739/7134 completed (loss: 0.06151191517710686, acc: 0.9776358008384705)
[2025-02-13 04:17:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:35][root][INFO] - Training Epoch: 2/2, step 4740/7134 completed (loss: 0.036435630172491074, acc: 0.9835329055786133)
[2025-02-13 04:17:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:35][root][INFO] - Training Epoch: 2/2, step 4741/7134 completed (loss: 0.025258304551243782, acc: 0.9926062822341919)
[2025-02-13 04:17:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:35][root][INFO] - Training Epoch: 2/2, step 4742/7134 completed (loss: 0.07100752741098404, acc: 0.982300877571106)
[2025-02-13 04:17:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:36][root][INFO] - Training Epoch: 2/2, step 4743/7134 completed (loss: 0.03068908303976059, acc: 0.9875444769859314)
[2025-02-13 04:17:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:36][root][INFO] - Training Epoch: 2/2, step 4744/7134 completed (loss: 0.01974252611398697, acc: 0.9917355179786682)
[2025-02-13 04:17:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:36][root][INFO] - Training Epoch: 2/2, step 4745/7134 completed (loss: 0.025847340002655983, acc: 0.9933110475540161)
[2025-02-13 04:17:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:37][root][INFO] - Training Epoch: 2/2, step 4746/7134 completed (loss: 0.028838519006967545, acc: 0.9926470518112183)
[2025-02-13 04:17:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:37][root][INFO] - Training Epoch: 2/2, step 4747/7134 completed (loss: 0.019816145300865173, acc: 0.997300922870636)
[2025-02-13 04:17:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:38][root][INFO] - Training Epoch: 2/2, step 4748/7134 completed (loss: 0.04344084486365318, acc: 0.982300877571106)
[2025-02-13 04:17:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:38][root][INFO] - Training Epoch: 2/2, step 4749/7134 completed (loss: 0.03255508095026016, acc: 0.9946666955947876)
[2025-02-13 04:17:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:39][root][INFO] - Training Epoch: 2/2, step 4750/7134 completed (loss: 0.06114254519343376, acc: 0.9845857620239258)
[2025-02-13 04:17:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:39][root][INFO] - Training Epoch: 2/2, step 4751/7134 completed (loss: 0.0153821911662817, acc: 0.9957567453384399)
[2025-02-13 04:17:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:40][root][INFO] - Training Epoch: 2/2, step 4752/7134 completed (loss: 0.04403892531991005, acc: 0.9907833933830261)
[2025-02-13 04:17:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:40][root][INFO] - Training Epoch: 2/2, step 4753/7134 completed (loss: 0.06357607245445251, acc: 0.9864457845687866)
[2025-02-13 04:17:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:40][root][INFO] - Training Epoch: 2/2, step 4754/7134 completed (loss: 0.014219523407518864, acc: 0.9941262602806091)
[2025-02-13 04:17:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:41][root][INFO] - Training Epoch: 2/2, step 4755/7134 completed (loss: 0.045473285019397736, acc: 0.9861303567886353)
[2025-02-13 04:17:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:41][root][INFO] - Training Epoch: 2/2, step 4756/7134 completed (loss: 0.05136241763830185, acc: 0.9874476790428162)
[2025-02-13 04:17:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:42][root][INFO] - Training Epoch: 2/2, step 4757/7134 completed (loss: 0.014522659592330456, acc: 0.9954075813293457)
[2025-02-13 04:17:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:42][root][INFO] - Training Epoch: 2/2, step 4758/7134 completed (loss: 0.024603039026260376, acc: 0.9929971694946289)
[2025-02-13 04:17:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:43][root][INFO] - Training Epoch: 2/2, step 4759/7134 completed (loss: 0.011501481756567955, acc: 0.998275876045227)
[2025-02-13 04:17:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:43][root][INFO] - Training Epoch: 2/2, step 4760/7134 completed (loss: 0.023430529981851578, acc: 0.9937888383865356)
[2025-02-13 04:17:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:43][root][INFO] - Training Epoch: 2/2, step 4761/7134 completed (loss: 0.04638045281171799, acc: 0.9908397197723389)
[2025-02-13 04:17:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:44][root][INFO] - Training Epoch: 2/2, step 4762/7134 completed (loss: 0.037010613828897476, acc: 0.9918919205665588)
[2025-02-13 04:17:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:44][root][INFO] - Training Epoch: 2/2, step 4763/7134 completed (loss: 0.03333481773734093, acc: 0.9908814430236816)
[2025-02-13 04:17:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:45][root][INFO] - Training Epoch: 2/2, step 4764/7134 completed (loss: 0.02821405977010727, acc: 0.990111231803894)
[2025-02-13 04:17:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:45][root][INFO] - Training Epoch: 2/2, step 4765/7134 completed (loss: 0.05081067606806755, acc: 0.9913669228553772)
[2025-02-13 04:17:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:46][root][INFO] - Training Epoch: 2/2, step 4766/7134 completed (loss: 0.025355005636811256, acc: 0.992277979850769)
[2025-02-13 04:17:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:46][root][INFO] - Training Epoch: 2/2, step 4767/7134 completed (loss: 0.03324652835726738, acc: 0.9898219108581543)
[2025-02-13 04:17:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:46][root][INFO] - Training Epoch: 2/2, step 4768/7134 completed (loss: 0.04710112139582634, acc: 0.9864498376846313)
[2025-02-13 04:17:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:47][root][INFO] - Training Epoch: 2/2, step 4769/7134 completed (loss: 0.04142964631319046, acc: 0.9903692007064819)
[2025-02-13 04:17:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:47][root][INFO] - Training Epoch: 2/2, step 4770/7134 completed (loss: 0.021572859957814217, acc: 0.9939393997192383)
[2025-02-13 04:17:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:48][root][INFO] - Training Epoch: 2/2, step 4771/7134 completed (loss: 0.03348444402217865, acc: 0.9869186282157898)
[2025-02-13 04:17:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:48][root][INFO] - Training Epoch: 2/2, step 4772/7134 completed (loss: 0.03646189719438553, acc: 0.9920254945755005)
[2025-02-13 04:17:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:48][root][INFO] - Training Epoch: 2/2, step 4773/7134 completed (loss: 0.02743961103260517, acc: 0.9917808175086975)
[2025-02-13 04:17:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:49][root][INFO] - Training Epoch: 2/2, step 4774/7134 completed (loss: 0.018126873299479485, acc: 0.9957864880561829)
[2025-02-13 04:17:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:49][root][INFO] - Training Epoch: 2/2, step 4775/7134 completed (loss: 0.038361962884664536, acc: 0.988727867603302)
[2025-02-13 04:17:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:50][root][INFO] - Training Epoch: 2/2, step 4776/7134 completed (loss: 0.03213964402675629, acc: 0.9887640476226807)
[2025-02-13 04:17:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:50][root][INFO] - Training Epoch: 2/2, step 4777/7134 completed (loss: 0.010309548117220402, acc: 0.9983305335044861)
[2025-02-13 04:17:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:51][root][INFO] - Training Epoch: 2/2, step 4778/7134 completed (loss: 0.02769964374601841, acc: 0.9934354424476624)
[2025-02-13 04:17:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:51][root][INFO] - Training Epoch: 2/2, step 4779/7134 completed (loss: 0.00912536308169365, acc: 0.9981583952903748)
[2025-02-13 04:17:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:52][root][INFO] - Training Epoch: 2/2, step 4780/7134 completed (loss: 0.005014281254261732, acc: 0.9968847632408142)
[2025-02-13 04:17:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:52][root][INFO] - Training Epoch: 2/2, step 4781/7134 completed (loss: 0.006941007915884256, acc: 0.9960861206054688)
[2025-02-13 04:17:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:52][root][INFO] - Training Epoch: 2/2, step 4782/7134 completed (loss: 0.009384702891111374, acc: 0.9983792304992676)
[2025-02-13 04:17:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:53][root][INFO] - Training Epoch: 2/2, step 4783/7134 completed (loss: 0.0691077709197998, acc: 0.982206404209137)
[2025-02-13 04:17:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:53][root][INFO] - Training Epoch: 2/2, step 4784/7134 completed (loss: 0.005179494619369507, acc: 0.9986357688903809)
[2025-02-13 04:17:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:54][root][INFO] - Training Epoch: 2/2, step 4785/7134 completed (loss: 0.03548463433980942, acc: 0.9887640476226807)
[2025-02-13 04:17:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:54][root][INFO] - Training Epoch: 2/2, step 4786/7134 completed (loss: 0.026155397295951843, acc: 0.9922360181808472)
[2025-02-13 04:17:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:55][root][INFO] - Training Epoch: 2/2, step 4787/7134 completed (loss: 0.0037843736354261637, acc: 0.9984177350997925)
[2025-02-13 04:17:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:55][root][INFO] - Training Epoch: 2/2, step 4788/7134 completed (loss: 0.0038359109312295914, acc: 0.9982269406318665)
[2025-02-13 04:17:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:55][root][INFO] - Training Epoch: 2/2, step 4789/7134 completed (loss: 0.0238251481205225, acc: 0.9952229261398315)
[2025-02-13 04:17:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:56][root][INFO] - Training Epoch: 2/2, step 4790/7134 completed (loss: 0.015323430299758911, acc: 0.9972826242446899)
[2025-02-13 04:17:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:56][root][INFO] - Training Epoch: 2/2, step 4791/7134 completed (loss: 0.016137482598423958, acc: 0.9962756037712097)
[2025-02-13 04:17:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:57][root][INFO] - Training Epoch: 2/2, step 4792/7134 completed (loss: 0.011175882071256638, acc: 0.9966555237770081)
[2025-02-13 04:17:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:57][root][INFO] - Training Epoch: 2/2, step 4793/7134 completed (loss: 0.036496225744485855, acc: 0.9863013625144958)
[2025-02-13 04:17:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:57][root][INFO] - Training Epoch: 2/2, step 4794/7134 completed (loss: 0.055314838886260986, acc: 0.9847618937492371)
[2025-02-13 04:17:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:58][root][INFO] - Training Epoch: 2/2, step 4795/7134 completed (loss: 0.017692381516098976, acc: 0.9929378628730774)
[2025-02-13 04:17:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:58][root][INFO] - Training Epoch: 2/2, step 4796/7134 completed (loss: 0.017247533425688744, acc: 0.9952830076217651)
[2025-02-13 04:17:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:59][root][INFO] - Training Epoch: 2/2, step 4797/7134 completed (loss: 0.01695629209280014, acc: 0.9937888383865356)
[2025-02-13 04:17:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:17:59][root][INFO] - Training Epoch: 2/2, step 4798/7134 completed (loss: 0.020073015242815018, acc: 0.9933422207832336)
[2025-02-13 04:17:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:00][root][INFO] - Training Epoch: 2/2, step 4799/7134 completed (loss: 0.02372623048722744, acc: 0.987270176410675)
[2025-02-13 04:18:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:00][root][INFO] - Training Epoch: 2/2, step 4800/7134 completed (loss: 0.033405058085918427, acc: 0.9940387606620789)
[2025-02-13 04:18:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:00][root][INFO] - Training Epoch: 2/2, step 4801/7134 completed (loss: 0.011709746904671192, acc: 0.9966063499450684)
[2025-02-13 04:18:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:01][root][INFO] - Training Epoch: 2/2, step 4802/7134 completed (loss: 0.03258989006280899, acc: 0.9908376932144165)
[2025-02-13 04:18:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:01][root][INFO] - Training Epoch: 2/2, step 4803/7134 completed (loss: 0.02771160379052162, acc: 0.9911764860153198)
[2025-02-13 04:18:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:02][root][INFO] - Training Epoch: 2/2, step 4804/7134 completed (loss: 0.0431537851691246, acc: 0.9876922965049744)
[2025-02-13 04:18:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:02][root][INFO] - Training Epoch: 2/2, step 4805/7134 completed (loss: 0.02240745536983013, acc: 0.9951456189155579)
[2025-02-13 04:18:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:03][root][INFO] - Training Epoch: 2/2, step 4806/7134 completed (loss: 0.02953813411295414, acc: 0.9933444261550903)
[2025-02-13 04:18:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:03][root][INFO] - Training Epoch: 2/2, step 4807/7134 completed (loss: 0.023164505138993263, acc: 0.996960461139679)
[2025-02-13 04:18:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:03][root][INFO] - Training Epoch: 2/2, step 4808/7134 completed (loss: 0.02474764548242092, acc: 0.991134762763977)
[2025-02-13 04:18:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:04][root][INFO] - Training Epoch: 2/2, step 4809/7134 completed (loss: 0.020108016207814217, acc: 0.9904761910438538)
[2025-02-13 04:18:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:04][root][INFO] - Training Epoch: 2/2, step 4810/7134 completed (loss: 0.0404963344335556, acc: 0.9942938685417175)
[2025-02-13 04:18:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:05][root][INFO] - Training Epoch: 2/2, step 4811/7134 completed (loss: 0.031014416366815567, acc: 0.9941860437393188)
[2025-02-13 04:18:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:05][root][INFO] - Training Epoch: 2/2, step 4812/7134 completed (loss: 0.09638354927301407, acc: 0.9860917925834656)
[2025-02-13 04:18:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:06][root][INFO] - Training Epoch: 2/2, step 4813/7134 completed (loss: 0.024593152105808258, acc: 0.9948052167892456)
[2025-02-13 04:18:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:06][root][INFO] - Training Epoch: 2/2, step 4814/7134 completed (loss: 0.023150676861405373, acc: 0.9948979616165161)
[2025-02-13 04:18:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:06][root][INFO] - Training Epoch: 2/2, step 4815/7134 completed (loss: 0.0020398192573338747, acc: 1.0)
[2025-02-13 04:18:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:07][root][INFO] - Training Epoch: 2/2, step 4816/7134 completed (loss: 0.005380823276937008, acc: 0.9962871074676514)
[2025-02-13 04:18:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:07][root][INFO] - Training Epoch: 2/2, step 4817/7134 completed (loss: 0.006057173479348421, acc: 0.9975903630256653)
[2025-02-13 04:18:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:08][root][INFO] - Training Epoch: 2/2, step 4818/7134 completed (loss: 0.012698986567556858, acc: 0.9984615445137024)
[2025-02-13 04:18:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:08][root][INFO] - Training Epoch: 2/2, step 4819/7134 completed (loss: 0.017813479527831078, acc: 0.993914783000946)
[2025-02-13 04:18:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:09][root][INFO] - Training Epoch: 2/2, step 4820/7134 completed (loss: 0.012570907361805439, acc: 0.9974259734153748)
[2025-02-13 04:18:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:09][root][INFO] - Training Epoch: 2/2, step 4821/7134 completed (loss: 0.015396637842059135, acc: 0.994413435459137)
[2025-02-13 04:18:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:09][root][INFO] - Training Epoch: 2/2, step 4822/7134 completed (loss: 0.0170741006731987, acc: 0.996219277381897)
[2025-02-13 04:18:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:10][root][INFO] - Training Epoch: 2/2, step 4823/7134 completed (loss: 0.05119171738624573, acc: 0.9873617887496948)
[2025-02-13 04:18:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:10][root][INFO] - Training Epoch: 2/2, step 4824/7134 completed (loss: 0.023671390488743782, acc: 0.990867555141449)
[2025-02-13 04:18:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:11][root][INFO] - Training Epoch: 2/2, step 4825/7134 completed (loss: 0.006682281848043203, acc: 0.9963503479957581)
[2025-02-13 04:18:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:11][root][INFO] - Training Epoch: 2/2, step 4826/7134 completed (loss: 0.009355283342301846, acc: 0.9978540539741516)
[2025-02-13 04:18:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:12][root][INFO] - Training Epoch: 2/2, step 4827/7134 completed (loss: 0.018274378031492233, acc: 0.991349458694458)
[2025-02-13 04:18:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:12][root][INFO] - Training Epoch: 2/2, step 4828/7134 completed (loss: 0.01932794228196144, acc: 0.9951397180557251)
[2025-02-13 04:18:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:12][root][INFO] - Training Epoch: 2/2, step 4829/7134 completed (loss: 0.011975817382335663, acc: 0.9974358677864075)
[2025-02-13 04:18:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:13][root][INFO] - Training Epoch: 2/2, step 4830/7134 completed (loss: 0.007553163915872574, acc: 0.9970414042472839)
[2025-02-13 04:18:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:13][root][INFO] - Training Epoch: 2/2, step 4831/7134 completed (loss: 0.007155068218708038, acc: 0.9970760345458984)
[2025-02-13 04:18:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:14][root][INFO] - Training Epoch: 2/2, step 4832/7134 completed (loss: 0.006980383303016424, acc: 0.9983713626861572)
[2025-02-13 04:18:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:14][root][INFO] - Training Epoch: 2/2, step 4833/7134 completed (loss: 0.008100125007331371, acc: 0.9949173927307129)
[2025-02-13 04:18:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:15][root][INFO] - Training Epoch: 2/2, step 4834/7134 completed (loss: 0.01590636745095253, acc: 0.9976190328598022)
[2025-02-13 04:18:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:15][root][INFO] - Training Epoch: 2/2, step 4835/7134 completed (loss: 0.009592767804861069, acc: 0.995945930480957)
[2025-02-13 04:18:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:15][root][INFO] - Training Epoch: 2/2, step 4836/7134 completed (loss: 0.00929439440369606, acc: 0.9969465732574463)
[2025-02-13 04:18:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:16][root][INFO] - Training Epoch: 2/2, step 4837/7134 completed (loss: 0.061340443789958954, acc: 0.9889705777168274)
[2025-02-13 04:18:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:16][root][INFO] - Training Epoch: 2/2, step 4838/7134 completed (loss: 0.016321003437042236, acc: 0.9935587644577026)
[2025-02-13 04:18:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:17][root][INFO] - Training Epoch: 2/2, step 4839/7134 completed (loss: 0.04581473022699356, acc: 0.9879518151283264)
[2025-02-13 04:18:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:17][root][INFO] - Training Epoch: 2/2, step 4840/7134 completed (loss: 0.027301443740725517, acc: 0.9942196607589722)
[2025-02-13 04:18:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:18][root][INFO] - Training Epoch: 2/2, step 4841/7134 completed (loss: 0.016905076801776886, acc: 0.9940387606620789)
[2025-02-13 04:18:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:18][root][INFO] - Training Epoch: 2/2, step 4842/7134 completed (loss: 0.042272426187992096, acc: 0.9885203838348389)
[2025-02-13 04:18:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:18][root][INFO] - Training Epoch: 2/2, step 4843/7134 completed (loss: 0.020118430256843567, acc: 0.9925705790519714)
[2025-02-13 04:18:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:19][root][INFO] - Training Epoch: 2/2, step 4844/7134 completed (loss: 0.013009399175643921, acc: 0.9951534867286682)
[2025-02-13 04:18:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:19][root][INFO] - Training Epoch: 2/2, step 4845/7134 completed (loss: 0.02680237591266632, acc: 0.9918032884597778)
[2025-02-13 04:18:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:20][root][INFO] - Training Epoch: 2/2, step 4846/7134 completed (loss: 0.02362750843167305, acc: 0.9905405640602112)
[2025-02-13 04:18:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:20][root][INFO] - Training Epoch: 2/2, step 4847/7134 completed (loss: 0.020732196047902107, acc: 0.9946091771125793)
[2025-02-13 04:18:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:21][root][INFO] - Training Epoch: 2/2, step 4848/7134 completed (loss: 0.030196690931916237, acc: 0.9944751262664795)
[2025-02-13 04:18:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:21][root][INFO] - Training Epoch: 2/2, step 4849/7134 completed (loss: 0.015034984797239304, acc: 0.9970282316207886)
[2025-02-13 04:18:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:21][root][INFO] - Training Epoch: 2/2, step 4850/7134 completed (loss: 0.030765414237976074, acc: 0.9914320707321167)
[2025-02-13 04:18:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:22][root][INFO] - Training Epoch: 2/2, step 4851/7134 completed (loss: 0.019674083217978477, acc: 0.9918224215507507)
[2025-02-13 04:18:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:22][root][INFO] - Training Epoch: 2/2, step 4852/7134 completed (loss: 0.025134872645139694, acc: 0.9880159497261047)
[2025-02-13 04:18:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:23][root][INFO] - Training Epoch: 2/2, step 4853/7134 completed (loss: 0.05116225779056549, acc: 0.984308123588562)
[2025-02-13 04:18:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:23][root][INFO] - Training Epoch: 2/2, step 4854/7134 completed (loss: 0.015268019400537014, acc: 0.9973333477973938)
[2025-02-13 04:18:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:24][root][INFO] - Training Epoch: 2/2, step 4855/7134 completed (loss: 0.016056301072239876, acc: 0.9951632618904114)
[2025-02-13 04:18:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:24][root][INFO] - Training Epoch: 2/2, step 4856/7134 completed (loss: 0.02307388000190258, acc: 0.9923664331436157)
[2025-02-13 04:18:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:25][root][INFO] - Training Epoch: 2/2, step 4857/7134 completed (loss: 0.012154306285083294, acc: 0.9942445755004883)
[2025-02-13 04:18:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:25][root][INFO] - Training Epoch: 2/2, step 4858/7134 completed (loss: 0.013515884056687355, acc: 0.994301974773407)
[2025-02-13 04:18:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:25][root][INFO] - Training Epoch: 2/2, step 4859/7134 completed (loss: 0.014381485059857368, acc: 0.9961389899253845)
[2025-02-13 04:18:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:26][root][INFO] - Training Epoch: 2/2, step 4860/7134 completed (loss: 0.010673262178897858, acc: 0.9959127902984619)
[2025-02-13 04:18:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:26][root][INFO] - Training Epoch: 2/2, step 4861/7134 completed (loss: 0.010892082937061787, acc: 0.9973045587539673)
[2025-02-13 04:18:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:27][root][INFO] - Training Epoch: 2/2, step 4862/7134 completed (loss: 0.01041342318058014, acc: 0.9971387982368469)
[2025-02-13 04:18:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:27][root][INFO] - Training Epoch: 2/2, step 4863/7134 completed (loss: 0.039887893944978714, acc: 0.9963503479957581)
[2025-02-13 04:18:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:28][root][INFO] - Training Epoch: 2/2, step 4864/7134 completed (loss: 0.01190160308033228, acc: 0.9984615445137024)
[2025-02-13 04:18:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:28][root][INFO] - Training Epoch: 2/2, step 4865/7134 completed (loss: 0.01908183842897415, acc: 0.9951159954071045)
[2025-02-13 04:18:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:29][root][INFO] - Training Epoch: 2/2, step 4866/7134 completed (loss: 0.017067773267626762, acc: 0.9961089491844177)
[2025-02-13 04:18:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:29][root][INFO] - Training Epoch: 2/2, step 4867/7134 completed (loss: 0.03324813395738602, acc: 0.9920634627342224)
[2025-02-13 04:18:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:29][root][INFO] - Training Epoch: 2/2, step 4868/7134 completed (loss: 0.03919726610183716, acc: 0.9864341020584106)
[2025-02-13 04:18:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:30][root][INFO] - Training Epoch: 2/2, step 4869/7134 completed (loss: 0.03915783762931824, acc: 0.9911110997200012)
[2025-02-13 04:18:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:30][root][INFO] - Training Epoch: 2/2, step 4870/7134 completed (loss: 0.010301678441464901, acc: 0.9976525902748108)
[2025-02-13 04:18:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:31][root][INFO] - Training Epoch: 2/2, step 4871/7134 completed (loss: 0.01645178161561489, acc: 0.9984567761421204)
[2025-02-13 04:18:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:31][root][INFO] - Training Epoch: 2/2, step 4872/7134 completed (loss: 0.01366156805306673, acc: 0.9972183704376221)
[2025-02-13 04:18:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:32][root][INFO] - Training Epoch: 2/2, step 4873/7134 completed (loss: 0.01544516533613205, acc: 0.9934895634651184)
[2025-02-13 04:18:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:32][root][INFO] - Training Epoch: 2/2, step 4874/7134 completed (loss: 0.01642700657248497, acc: 0.99314284324646)
[2025-02-13 04:18:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:32][root][INFO] - Training Epoch: 2/2, step 4875/7134 completed (loss: 0.014458290301263332, acc: 0.9930651783943176)
[2025-02-13 04:18:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:33][root][INFO] - Training Epoch: 2/2, step 4876/7134 completed (loss: 0.051553983241319656, acc: 0.985200822353363)
[2025-02-13 04:18:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:33][root][INFO] - Training Epoch: 2/2, step 4877/7134 completed (loss: 0.029198339208960533, acc: 0.9896774291992188)
[2025-02-13 04:18:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:34][root][INFO] - Training Epoch: 2/2, step 4878/7134 completed (loss: 0.018433257937431335, acc: 0.9941089749336243)
[2025-02-13 04:18:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:34][root][INFO] - Training Epoch: 2/2, step 4879/7134 completed (loss: 0.05301967263221741, acc: 0.983433723449707)
[2025-02-13 04:18:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:35][root][INFO] - Training Epoch: 2/2, step 4880/7134 completed (loss: 0.007932586595416069, acc: 0.9974059462547302)
[2025-02-13 04:18:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:35][root][INFO] - Training Epoch: 2/2, step 4881/7134 completed (loss: 0.02951921708881855, acc: 0.9927007555961609)
[2025-02-13 04:18:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:36][root][INFO] - Training Epoch: 2/2, step 4882/7134 completed (loss: 0.032616641372442245, acc: 0.9868420958518982)
[2025-02-13 04:18:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:36][root][INFO] - Training Epoch: 2/2, step 4883/7134 completed (loss: 0.01802954077720642, acc: 0.9941605925559998)
[2025-02-13 04:18:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:36][root][INFO] - Training Epoch: 2/2, step 4884/7134 completed (loss: 0.008785239420831203, acc: 0.996927797794342)
[2025-02-13 04:18:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:37][root][INFO] - Training Epoch: 2/2, step 4885/7134 completed (loss: 0.00819364283233881, acc: 0.9973614811897278)
[2025-02-13 04:18:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:37][root][INFO] - Training Epoch: 2/2, step 4886/7134 completed (loss: 0.02632194198668003, acc: 0.9897435903549194)
[2025-02-13 04:18:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:38][root][INFO] - Training Epoch: 2/2, step 4887/7134 completed (loss: 0.007470100652426481, acc: 0.9964157938957214)
[2025-02-13 04:18:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:38][root][INFO] - Training Epoch: 2/2, step 4888/7134 completed (loss: 0.026745885610580444, acc: 0.9945054650306702)
[2025-02-13 04:18:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:39][root][INFO] - Training Epoch: 2/2, step 4889/7134 completed (loss: 0.005578041076660156, acc: 0.9975932836532593)
[2025-02-13 04:18:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:39][root][INFO] - Training Epoch: 2/2, step 4890/7134 completed (loss: 0.025526951998472214, acc: 0.9952380657196045)
[2025-02-13 04:18:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:39][root][INFO] - Training Epoch: 2/2, step 4891/7134 completed (loss: 0.02604764886200428, acc: 0.9950248599052429)
[2025-02-13 04:18:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:40][root][INFO] - Training Epoch: 2/2, step 4892/7134 completed (loss: 0.03073766641318798, acc: 0.9932885766029358)
[2025-02-13 04:18:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:40][root][INFO] - Training Epoch: 2/2, step 4893/7134 completed (loss: 0.10909031331539154, acc: 0.9683377146720886)
[2025-02-13 04:18:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:41][root][INFO] - Training Epoch: 2/2, step 4894/7134 completed (loss: 0.008888586424291134, acc: 0.9971181750297546)
[2025-02-13 04:18:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:41][root][INFO] - Training Epoch: 2/2, step 4895/7134 completed (loss: 0.006166647654026747, acc: 0.9975000023841858)
[2025-02-13 04:18:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:42][root][INFO] - Training Epoch: 2/2, step 4896/7134 completed (loss: 0.004667333792895079, acc: 0.9985954761505127)
[2025-02-13 04:18:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:42][root][INFO] - Training Epoch: 2/2, step 4897/7134 completed (loss: 0.017754167318344116, acc: 0.9941520690917969)
[2025-02-13 04:18:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:43][root][INFO] - Training Epoch: 2/2, step 4898/7134 completed (loss: 0.019304191693663597, acc: 0.99622642993927)
[2025-02-13 04:18:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:43][root][INFO] - Training Epoch: 2/2, step 4899/7134 completed (loss: 0.019738364964723587, acc: 0.9941775798797607)
[2025-02-13 04:18:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:43][root][INFO] - Training Epoch: 2/2, step 4900/7134 completed (loss: 0.03560299798846245, acc: 0.9894737005233765)
[2025-02-13 04:18:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:44][root][INFO] - Training Epoch: 2/2, step 4901/7134 completed (loss: 0.013907331973314285, acc: 0.995488703250885)
[2025-02-13 04:18:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:44][root][INFO] - Training Epoch: 2/2, step 4902/7134 completed (loss: 0.00618491368368268, acc: 0.9986737370491028)
[2025-02-13 04:18:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:45][root][INFO] - Training Epoch: 2/2, step 4903/7134 completed (loss: 0.005540521815419197, acc: 0.9982078671455383)
[2025-02-13 04:18:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:45][root][INFO] - Training Epoch: 2/2, step 4904/7134 completed (loss: 0.0025767467450350523, acc: 1.0)
[2025-02-13 04:18:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:46][root][INFO] - Training Epoch: 2/2, step 4905/7134 completed (loss: 0.011935509741306305, acc: 0.9968152642250061)
[2025-02-13 04:18:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:46][root][INFO] - Training Epoch: 2/2, step 4906/7134 completed (loss: 0.01424355898052454, acc: 0.9966273307800293)
[2025-02-13 04:18:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:47][root][INFO] - Training Epoch: 2/2, step 4907/7134 completed (loss: 0.014489280991256237, acc: 0.9984177350997925)
[2025-02-13 04:18:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:47][root][INFO] - Training Epoch: 2/2, step 4908/7134 completed (loss: 0.004233562387526035, acc: 1.0)
[2025-02-13 04:18:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:47][root][INFO] - Training Epoch: 2/2, step 4909/7134 completed (loss: 0.003489401889964938, acc: 1.0)
[2025-02-13 04:18:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:48][root][INFO] - Training Epoch: 2/2, step 4910/7134 completed (loss: 0.024780945852398872, acc: 0.9921630024909973)
[2025-02-13 04:18:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:48][root][INFO] - Training Epoch: 2/2, step 4911/7134 completed (loss: 0.018249891698360443, acc: 0.9920844435691833)
[2025-02-13 04:18:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:49][root][INFO] - Training Epoch: 2/2, step 4912/7134 completed (loss: 0.014995641075074673, acc: 0.9963680505752563)
[2025-02-13 04:18:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:49][root][INFO] - Training Epoch: 2/2, step 4913/7134 completed (loss: 0.013211145997047424, acc: 0.9966216087341309)
[2025-02-13 04:18:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:50][root][INFO] - Training Epoch: 2/2, step 4914/7134 completed (loss: 0.023471534252166748, acc: 0.9954128265380859)
[2025-02-13 04:18:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:50][root][INFO] - Training Epoch: 2/2, step 4915/7134 completed (loss: 0.0014020001981407404, acc: 1.0)
[2025-02-13 04:18:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:50][root][INFO] - Training Epoch: 2/2, step 4916/7134 completed (loss: 0.027432288974523544, acc: 0.987864077091217)
[2025-02-13 04:18:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:51][root][INFO] - Training Epoch: 2/2, step 4917/7134 completed (loss: 0.017572583630681038, acc: 0.9944367408752441)
[2025-02-13 04:18:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:51][root][INFO] - Training Epoch: 2/2, step 4918/7134 completed (loss: 0.0018066467018797994, acc: 1.0)
[2025-02-13 04:18:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:52][root][INFO] - Training Epoch: 2/2, step 4919/7134 completed (loss: 0.028827281668782234, acc: 0.9907894730567932)
[2025-02-13 04:18:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:52][root][INFO] - Training Epoch: 2/2, step 4920/7134 completed (loss: 0.034008096903562546, acc: 0.9922480583190918)
[2025-02-13 04:18:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:53][root][INFO] - Training Epoch: 2/2, step 4921/7134 completed (loss: 0.02756926789879799, acc: 0.9942611455917358)
[2025-02-13 04:18:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:53][root][INFO] - Training Epoch: 2/2, step 4922/7134 completed (loss: 0.023703722283244133, acc: 0.9931972622871399)
[2025-02-13 04:18:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:54][root][INFO] - Training Epoch: 2/2, step 4923/7134 completed (loss: 0.007467896677553654, acc: 0.997063159942627)
[2025-02-13 04:18:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:54][root][INFO] - Training Epoch: 2/2, step 4924/7134 completed (loss: 0.025021744892001152, acc: 0.9940000176429749)
[2025-02-13 04:18:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:54][root][INFO] - Training Epoch: 2/2, step 4925/7134 completed (loss: 0.01936962828040123, acc: 0.9928469061851501)
[2025-02-13 04:18:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:55][root][INFO] - Training Epoch: 2/2, step 4926/7134 completed (loss: 0.010136229917407036, acc: 0.9956958293914795)
[2025-02-13 04:18:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:55][root][INFO] - Training Epoch: 2/2, step 4927/7134 completed (loss: 0.016775822266936302, acc: 0.994397759437561)
[2025-02-13 04:18:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:56][root][INFO] - Training Epoch: 2/2, step 4928/7134 completed (loss: 0.006874451879411936, acc: 0.9988412261009216)
[2025-02-13 04:18:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:56][root][INFO] - Training Epoch: 2/2, step 4929/7134 completed (loss: 0.01589222066104412, acc: 0.9937984347343445)
[2025-02-13 04:18:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:57][root][INFO] - Training Epoch: 2/2, step 4930/7134 completed (loss: 0.03699801117181778, acc: 0.993966817855835)
[2025-02-13 04:18:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:57][root][INFO] - Training Epoch: 2/2, step 4931/7134 completed (loss: 0.05653217434883118, acc: 0.9863013625144958)
[2025-02-13 04:18:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:57][root][INFO] - Training Epoch: 2/2, step 4932/7134 completed (loss: 0.022053521126508713, acc: 0.9932523369789124)
[2025-02-13 04:18:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:58][root][INFO] - Training Epoch: 2/2, step 4933/7134 completed (loss: 0.019163046032190323, acc: 0.993819534778595)
[2025-02-13 04:18:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:58][root][INFO] - Training Epoch: 2/2, step 4934/7134 completed (loss: 0.009148008190095425, acc: 0.9959404468536377)
[2025-02-13 04:18:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:59][root][INFO] - Training Epoch: 2/2, step 4935/7134 completed (loss: 0.025839634239673615, acc: 0.99589604139328)
[2025-02-13 04:18:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:18:59][root][INFO] - Training Epoch: 2/2, step 4936/7134 completed (loss: 0.02182300016283989, acc: 0.9907407164573669)
[2025-02-13 04:18:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:00][root][INFO] - Training Epoch: 2/2, step 4937/7134 completed (loss: 0.015646252781152725, acc: 0.994020938873291)
[2025-02-13 04:19:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:00][root][INFO] - Training Epoch: 2/2, step 4938/7134 completed (loss: 0.01629263535141945, acc: 0.9919028282165527)
[2025-02-13 04:19:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:00][root][INFO] - Training Epoch: 2/2, step 4939/7134 completed (loss: 0.024823186919093132, acc: 0.9885807633399963)
[2025-02-13 04:19:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:01][root][INFO] - Training Epoch: 2/2, step 4940/7134 completed (loss: 0.033597417175769806, acc: 0.989393949508667)
[2025-02-13 04:19:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:01][root][INFO] - Training Epoch: 2/2, step 4941/7134 completed (loss: 0.020507553592324257, acc: 0.9932705163955688)
[2025-02-13 04:19:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:02][root][INFO] - Training Epoch: 2/2, step 4942/7134 completed (loss: 0.02382742427289486, acc: 0.9932975769042969)
[2025-02-13 04:19:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:02][root][INFO] - Training Epoch: 2/2, step 4943/7134 completed (loss: 0.029125211760401726, acc: 0.9931600689888)
[2025-02-13 04:19:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:03][root][INFO] - Training Epoch: 2/2, step 4944/7134 completed (loss: 0.017056984826922417, acc: 0.9936842322349548)
[2025-02-13 04:19:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:03][root][INFO] - Training Epoch: 2/2, step 4945/7134 completed (loss: 0.012625806964933872, acc: 0.9959677457809448)
[2025-02-13 04:19:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:03][root][INFO] - Training Epoch: 2/2, step 4946/7134 completed (loss: 0.04797683656215668, acc: 0.9846153855323792)
[2025-02-13 04:19:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:04][root][INFO] - Training Epoch: 2/2, step 4947/7134 completed (loss: 0.016001809388399124, acc: 0.9933481216430664)
[2025-02-13 04:19:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:04][root][INFO] - Training Epoch: 2/2, step 4948/7134 completed (loss: 0.025868499651551247, acc: 0.9946523904800415)
[2025-02-13 04:19:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:05][root][INFO] - Training Epoch: 2/2, step 4949/7134 completed (loss: 0.016057441011071205, acc: 0.9968253970146179)
[2025-02-13 04:19:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:05][root][INFO] - Training Epoch: 2/2, step 4950/7134 completed (loss: 0.012188839726150036, acc: 0.9982486963272095)
[2025-02-13 04:19:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:06][root][INFO] - Training Epoch: 2/2, step 4951/7134 completed (loss: 0.01740584895014763, acc: 0.9886040091514587)
[2025-02-13 04:19:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:06][root][INFO] - Training Epoch: 2/2, step 4952/7134 completed (loss: 0.021911315619945526, acc: 0.996688723564148)
[2025-02-13 04:19:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:06][root][INFO] - Training Epoch: 2/2, step 4953/7134 completed (loss: 0.011040724813938141, acc: 0.9956427216529846)
[2025-02-13 04:19:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:07][root][INFO] - Training Epoch: 2/2, step 4954/7134 completed (loss: 0.00816652923822403, acc: 1.0)
[2025-02-13 04:19:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:07][root][INFO] - Training Epoch: 2/2, step 4955/7134 completed (loss: 0.008168979547917843, acc: 1.0)
[2025-02-13 04:19:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:08][root][INFO] - Training Epoch: 2/2, step 4956/7134 completed (loss: 0.013656336814165115, acc: 0.9971550703048706)
[2025-02-13 04:19:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:08][root][INFO] - Training Epoch: 2/2, step 4957/7134 completed (loss: 0.022869616746902466, acc: 0.9894179701805115)
[2025-02-13 04:19:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:09][root][INFO] - Training Epoch: 2/2, step 4958/7134 completed (loss: 0.014751804992556572, acc: 0.9942445755004883)
[2025-02-13 04:19:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:09][root][INFO] - Training Epoch: 2/2, step 4959/7134 completed (loss: 0.007121809758245945, acc: 0.9970588088035583)
[2025-02-13 04:19:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:09][root][INFO] - Training Epoch: 2/2, step 4960/7134 completed (loss: 0.007188939023762941, acc: 0.998275876045227)
[2025-02-13 04:19:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:10][root][INFO] - Training Epoch: 2/2, step 4961/7134 completed (loss: 0.007741121109575033, acc: 0.9963570237159729)
[2025-02-13 04:19:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:10][root][INFO] - Training Epoch: 2/2, step 4962/7134 completed (loss: 0.0023794376756995916, acc: 1.0)
[2025-02-13 04:19:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:11][root][INFO] - Training Epoch: 2/2, step 4963/7134 completed (loss: 0.015700196847319603, acc: 0.9951298832893372)
[2025-02-13 04:19:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:11][root][INFO] - Training Epoch: 2/2, step 4964/7134 completed (loss: 0.01573525182902813, acc: 0.9934959411621094)
[2025-02-13 04:19:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:12][root][INFO] - Training Epoch: 2/2, step 4965/7134 completed (loss: 0.026292677968740463, acc: 0.9915966391563416)
[2025-02-13 04:19:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:12][root][INFO] - Training Epoch: 2/2, step 4966/7134 completed (loss: 0.029074475169181824, acc: 0.9944598078727722)
[2025-02-13 04:19:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:12][root][INFO] - Training Epoch: 2/2, step 4967/7134 completed (loss: 0.012076294049620628, acc: 0.994140625)
[2025-02-13 04:19:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:13][root][INFO] - Training Epoch: 2/2, step 4968/7134 completed (loss: 0.055407337844371796, acc: 0.9839857816696167)
[2025-02-13 04:19:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:13][root][INFO] - Training Epoch: 2/2, step 4969/7134 completed (loss: 0.04466740041971207, acc: 0.9896907210350037)
[2025-02-13 04:19:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:14][root][INFO] - Training Epoch: 2/2, step 4970/7134 completed (loss: 0.020287832245230675, acc: 0.9933110475540161)
[2025-02-13 04:19:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:14][root][INFO] - Training Epoch: 2/2, step 4971/7134 completed (loss: 0.018026871606707573, acc: 0.991525411605835)
[2025-02-13 04:19:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:15][root][INFO] - Training Epoch: 2/2, step 4972/7134 completed (loss: 0.008033055812120438, acc: 0.9985895752906799)
[2025-02-13 04:19:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:15][root][INFO] - Training Epoch: 2/2, step 4973/7134 completed (loss: 0.0395931638777256, acc: 0.9897119402885437)
[2025-02-13 04:19:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:16][root][INFO] - Training Epoch: 2/2, step 4974/7134 completed (loss: 0.03281606361269951, acc: 0.9899799823760986)
[2025-02-13 04:19:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:16][root][INFO] - Training Epoch: 2/2, step 4975/7134 completed (loss: 0.04847242683172226, acc: 0.9872204661369324)
[2025-02-13 04:19:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:16][root][INFO] - Training Epoch: 2/2, step 4976/7134 completed (loss: 0.023241417482495308, acc: 0.9929742217063904)
[2025-02-13 04:19:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:17][root][INFO] - Training Epoch: 2/2, step 4977/7134 completed (loss: 0.060135334730148315, acc: 0.9870967864990234)
[2025-02-13 04:19:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:17][root][INFO] - Training Epoch: 2/2, step 4978/7134 completed (loss: 0.012321582064032555, acc: 0.9937888383865356)
[2025-02-13 04:19:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:18][root][INFO] - Training Epoch: 2/2, step 4979/7134 completed (loss: 0.0694967582821846, acc: 0.9835255146026611)
[2025-02-13 04:19:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:18][root][INFO] - Training Epoch: 2/2, step 4980/7134 completed (loss: 0.0373295359313488, acc: 0.990338146686554)
[2025-02-13 04:19:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:19][root][INFO] - Training Epoch: 2/2, step 4981/7134 completed (loss: 0.03740733489394188, acc: 0.9872408509254456)
[2025-02-13 04:19:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:19][root][INFO] - Training Epoch: 2/2, step 4982/7134 completed (loss: 0.049968697130680084, acc: 0.9878542423248291)
[2025-02-13 04:19:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:20][root][INFO] - Training Epoch: 2/2, step 4983/7134 completed (loss: 0.02908569760620594, acc: 0.9953271150588989)
[2025-02-13 04:19:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:20][root][INFO] - Training Epoch: 2/2, step 4984/7134 completed (loss: 0.045791223645210266, acc: 0.9958333373069763)
[2025-02-13 04:19:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:20][root][INFO] - Training Epoch: 2/2, step 4985/7134 completed (loss: 0.022192122414708138, acc: 0.9938837885856628)
[2025-02-13 04:19:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:21][root][INFO] - Training Epoch: 2/2, step 4986/7134 completed (loss: 0.03776633366942406, acc: 0.98959881067276)
[2025-02-13 04:19:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:21][root][INFO] - Training Epoch: 2/2, step 4987/7134 completed (loss: 0.021152419969439507, acc: 0.9940357804298401)
[2025-02-13 04:19:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:22][root][INFO] - Training Epoch: 2/2, step 4988/7134 completed (loss: 0.009694185107946396, acc: 0.9964664578437805)
[2025-02-13 04:19:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:22][root][INFO] - Training Epoch: 2/2, step 4989/7134 completed (loss: 0.01253232266753912, acc: 0.9950494766235352)
[2025-02-13 04:19:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:23][root][INFO] - Training Epoch: 2/2, step 4990/7134 completed (loss: 0.02435494214296341, acc: 0.9915540814399719)
[2025-02-13 04:19:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:23][root][INFO] - Training Epoch: 2/2, step 4991/7134 completed (loss: 0.012890362180769444, acc: 0.9949748516082764)
[2025-02-13 04:19:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:23][root][INFO] - Training Epoch: 2/2, step 4992/7134 completed (loss: 0.07187819480895996, acc: 0.9824945330619812)
[2025-02-13 04:19:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:24][root][INFO] - Training Epoch: 2/2, step 4993/7134 completed (loss: 0.0052638668566942215, acc: 1.0)
[2025-02-13 04:19:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:24][root][INFO] - Training Epoch: 2/2, step 4994/7134 completed (loss: 0.014317378401756287, acc: 0.9957447052001953)
[2025-02-13 04:19:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:25][root][INFO] - Training Epoch: 2/2, step 4995/7134 completed (loss: 0.02944493107497692, acc: 0.9918367266654968)
[2025-02-13 04:19:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:25][root][INFO] - Training Epoch: 2/2, step 4996/7134 completed (loss: 0.018117956817150116, acc: 0.9944649338722229)
[2025-02-13 04:19:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:26][root][INFO] - Training Epoch: 2/2, step 4997/7134 completed (loss: 0.03367399796843529, acc: 0.9931507110595703)
[2025-02-13 04:19:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:26][root][INFO] - Training Epoch: 2/2, step 4998/7134 completed (loss: 0.04348007217049599, acc: 0.9890710115432739)
[2025-02-13 04:19:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:26][root][INFO] - Training Epoch: 2/2, step 4999/7134 completed (loss: 0.010492458008229733, acc: 0.9965576529502869)
[2025-02-13 04:19:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:27][root][INFO] - Training Epoch: 2/2, step 5000/7134 completed (loss: 0.013591205701231956, acc: 0.9953488111495972)
[2025-02-13 04:19:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:27][root][INFO] - Training Epoch: 2/2, step 5001/7134 completed (loss: 0.04301860183477402, acc: 0.9898256063461304)
[2025-02-13 04:19:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:28][root][INFO] - Training Epoch: 2/2, step 5002/7134 completed (loss: 0.027533119544386864, acc: 0.9944238066673279)
[2025-02-13 04:19:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:28][root][INFO] - Training Epoch: 2/2, step 5003/7134 completed (loss: 0.013694646768271923, acc: 0.9983999729156494)
[2025-02-13 04:19:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:29][root][INFO] - Training Epoch: 2/2, step 5004/7134 completed (loss: 0.007747495546936989, acc: 0.9977924823760986)
[2025-02-13 04:19:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:29][root][INFO] - Training Epoch: 2/2, step 5005/7134 completed (loss: 0.026916179805994034, acc: 0.995121955871582)
[2025-02-13 04:19:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:29][root][INFO] - Training Epoch: 2/2, step 5006/7134 completed (loss: 0.012118534184992313, acc: 0.9985315799713135)
[2025-02-13 04:19:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:30][root][INFO] - Training Epoch: 2/2, step 5007/7134 completed (loss: 0.004205372184514999, acc: 1.0)
[2025-02-13 04:19:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:30][root][INFO] - Training Epoch: 2/2, step 5008/7134 completed (loss: 0.03877643123269081, acc: 0.9881889820098877)
[2025-02-13 04:19:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:31][root][INFO] - Training Epoch: 2/2, step 5009/7134 completed (loss: 0.05531567707657814, acc: 0.9875930547714233)
[2025-02-13 04:19:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:31][root][INFO] - Training Epoch: 2/2, step 5010/7134 completed (loss: 0.03255283460021019, acc: 0.9898785352706909)
[2025-02-13 04:19:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:32][root][INFO] - Training Epoch: 2/2, step 5011/7134 completed (loss: 0.03306598961353302, acc: 0.9934959411621094)
[2025-02-13 04:19:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:32][root][INFO] - Training Epoch: 2/2, step 5012/7134 completed (loss: 0.042105261236429214, acc: 0.9885057210922241)
[2025-02-13 04:19:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:32][root][INFO] - Training Epoch: 2/2, step 5013/7134 completed (loss: 0.006228295154869556, acc: 0.9979959726333618)
[2025-02-13 04:19:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:33][root][INFO] - Training Epoch: 2/2, step 5014/7134 completed (loss: 0.0215725339949131, acc: 0.9979507923126221)
[2025-02-13 04:19:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:33][root][INFO] - Training Epoch: 2/2, step 5015/7134 completed (loss: 0.06786680221557617, acc: 0.9862068891525269)
[2025-02-13 04:19:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:34][root][INFO] - Training Epoch: 2/2, step 5016/7134 completed (loss: 0.024739764630794525, acc: 0.9927667379379272)
[2025-02-13 04:19:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:34][root][INFO] - Training Epoch: 2/2, step 5017/7134 completed (loss: 0.010802359320223331, acc: 0.9963964223861694)
[2025-02-13 04:19:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:35][root][INFO] - Training Epoch: 2/2, step 5018/7134 completed (loss: 0.020939558744430542, acc: 0.98758864402771)
[2025-02-13 04:19:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:35][root][INFO] - Training Epoch: 2/2, step 5019/7134 completed (loss: 0.01668824814260006, acc: 0.994301974773407)
[2025-02-13 04:19:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:35][root][INFO] - Training Epoch: 2/2, step 5020/7134 completed (loss: 0.026198571547865868, acc: 0.9962499737739563)
[2025-02-13 04:19:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:36][root][INFO] - Training Epoch: 2/2, step 5021/7134 completed (loss: 0.022889675572514534, acc: 0.9954233169555664)
[2025-02-13 04:19:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:36][root][INFO] - Training Epoch: 2/2, step 5022/7134 completed (loss: 0.013416399247944355, acc: 0.9984567761421204)
[2025-02-13 04:19:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:37][root][INFO] - Training Epoch: 2/2, step 5023/7134 completed (loss: 0.029682226479053497, acc: 0.9924012422561646)
[2025-02-13 04:19:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:37][root][INFO] - Training Epoch: 2/2, step 5024/7134 completed (loss: 0.026322759687900543, acc: 0.9948520064353943)
[2025-02-13 04:19:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:38][root][INFO] - Training Epoch: 2/2, step 5025/7134 completed (loss: 0.008376441895961761, acc: 0.9981883764266968)
[2025-02-13 04:19:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:38][root][INFO] - Training Epoch: 2/2, step 5026/7134 completed (loss: 0.016923759132623672, acc: 0.996497392654419)
[2025-02-13 04:19:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:38][root][INFO] - Training Epoch: 2/2, step 5027/7134 completed (loss: 0.02058888040482998, acc: 0.994535505771637)
[2025-02-13 04:19:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:39][root][INFO] - Training Epoch: 2/2, step 5028/7134 completed (loss: 0.0146010247990489, acc: 0.9968652129173279)
[2025-02-13 04:19:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:39][root][INFO] - Training Epoch: 2/2, step 5029/7134 completed (loss: 0.012989815324544907, acc: 0.9962121248245239)
[2025-02-13 04:19:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:40][root][INFO] - Training Epoch: 2/2, step 5030/7134 completed (loss: 0.010178910568356514, acc: 0.9972714781761169)
[2025-02-13 04:19:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:40][root][INFO] - Training Epoch: 2/2, step 5031/7134 completed (loss: 0.05803636461496353, acc: 0.9914966225624084)
[2025-02-13 04:19:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:41][root][INFO] - Training Epoch: 2/2, step 5032/7134 completed (loss: 0.013794301077723503, acc: 0.9967532753944397)
[2025-02-13 04:19:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:41][root][INFO] - Training Epoch: 2/2, step 5033/7134 completed (loss: 0.007076405454427004, acc: 0.9983974099159241)
[2025-02-13 04:19:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:41][root][INFO] - Training Epoch: 2/2, step 5034/7134 completed (loss: 0.01804117113351822, acc: 0.9946808218955994)
[2025-02-13 04:19:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:42][root][INFO] - Training Epoch: 2/2, step 5035/7134 completed (loss: 0.002640856197103858, acc: 1.0)
[2025-02-13 04:19:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:42][root][INFO] - Training Epoch: 2/2, step 5036/7134 completed (loss: 0.033017996698617935, acc: 0.9955357313156128)
[2025-02-13 04:19:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:43][root][INFO] - Training Epoch: 2/2, step 5037/7134 completed (loss: 0.04908376187086105, acc: 0.9919571280479431)
[2025-02-13 04:19:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:43][root][INFO] - Training Epoch: 2/2, step 5038/7134 completed (loss: 0.0143808051943779, acc: 0.9977728128433228)
[2025-02-13 04:19:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:43][root][INFO] - Training Epoch: 2/2, step 5039/7134 completed (loss: 0.02888096123933792, acc: 0.9905063509941101)
[2025-02-13 04:19:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:44][root][INFO] - Training Epoch: 2/2, step 5040/7134 completed (loss: 0.008656598627567291, acc: 0.9980879426002502)
[2025-02-13 04:19:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:44][root][INFO] - Training Epoch: 2/2, step 5041/7134 completed (loss: 0.010063573718070984, acc: 0.9986737370491028)
[2025-02-13 04:19:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:45][root][INFO] - Training Epoch: 2/2, step 5042/7134 completed (loss: 0.04262590408325195, acc: 0.9906759858131409)
[2025-02-13 04:19:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:45][root][INFO] - Training Epoch: 2/2, step 5043/7134 completed (loss: 0.028492368757724762, acc: 0.992094874382019)
[2025-02-13 04:19:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:46][root][INFO] - Training Epoch: 2/2, step 5044/7134 completed (loss: 0.01237204298377037, acc: 0.9956140518188477)
[2025-02-13 04:19:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:46][root][INFO] - Training Epoch: 2/2, step 5045/7134 completed (loss: 0.03756777569651604, acc: 0.9865410327911377)
[2025-02-13 04:19:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:46][root][INFO] - Training Epoch: 2/2, step 5046/7134 completed (loss: 0.030692555010318756, acc: 0.9859648942947388)
[2025-02-13 04:19:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:47][root][INFO] - Training Epoch: 2/2, step 5047/7134 completed (loss: 0.034356262534856796, acc: 0.9933884143829346)
[2025-02-13 04:19:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:47][root][INFO] - Training Epoch: 2/2, step 5048/7134 completed (loss: 0.030483854934573174, acc: 0.9939117431640625)
[2025-02-13 04:19:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:48][root][INFO] - Training Epoch: 2/2, step 5049/7134 completed (loss: 0.038615334779024124, acc: 0.9904631972312927)
[2025-02-13 04:19:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:48][root][INFO] - Training Epoch: 2/2, step 5050/7134 completed (loss: 0.04098387435078621, acc: 0.9915611743927002)
[2025-02-13 04:19:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:49][root][INFO] - Training Epoch: 2/2, step 5051/7134 completed (loss: 0.03458024561405182, acc: 0.9942029118537903)
[2025-02-13 04:19:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:49][root][INFO] - Training Epoch: 2/2, step 5052/7134 completed (loss: 0.018170922994613647, acc: 0.989393949508667)
[2025-02-13 04:19:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:49][root][INFO] - Training Epoch: 2/2, step 5053/7134 completed (loss: 0.052437834441661835, acc: 0.9886792302131653)
[2025-02-13 04:19:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:50][root][INFO] - Training Epoch: 2/2, step 5054/7134 completed (loss: 0.028711337596178055, acc: 0.9905362725257874)
[2025-02-13 04:19:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:50][root][INFO] - Training Epoch: 2/2, step 5055/7134 completed (loss: 0.013725584372878075, acc: 0.9959183931350708)
[2025-02-13 04:19:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:51][root][INFO] - Training Epoch: 2/2, step 5056/7134 completed (loss: 0.05506085604429245, acc: 0.9857142567634583)
[2025-02-13 04:19:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:51][root][INFO] - Training Epoch: 2/2, step 5057/7134 completed (loss: 0.06043814867734909, acc: 0.9900426864624023)
[2025-02-13 04:19:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:51][root][INFO] - Training Epoch: 2/2, step 5058/7134 completed (loss: 0.05944976210594177, acc: 0.9826086759567261)
[2025-02-13 04:19:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:52][root][INFO] - Training Epoch: 2/2, step 5059/7134 completed (loss: 0.021542105823755264, acc: 0.993446946144104)
[2025-02-13 04:19:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:52][root][INFO] - Training Epoch: 2/2, step 5060/7134 completed (loss: 0.011321129277348518, acc: 0.9985141158103943)
[2025-02-13 04:19:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:53][root][INFO] - Training Epoch: 2/2, step 5061/7134 completed (loss: 0.01691211760044098, acc: 0.9935204982757568)
[2025-02-13 04:19:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:53][root][INFO] - Training Epoch: 2/2, step 5062/7134 completed (loss: 0.0473509356379509, acc: 0.9862778782844543)
[2025-02-13 04:19:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:54][root][INFO] - Training Epoch: 2/2, step 5063/7134 completed (loss: 0.028569677844643593, acc: 0.9916527271270752)
[2025-02-13 04:19:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:54][root][INFO] - Training Epoch: 2/2, step 5064/7134 completed (loss: 0.02362203225493431, acc: 0.9931507110595703)
[2025-02-13 04:19:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:55][root][INFO] - Training Epoch: 2/2, step 5065/7134 completed (loss: 0.03330457583069801, acc: 0.9929478168487549)
[2025-02-13 04:19:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:55][root][INFO] - Training Epoch: 2/2, step 5066/7134 completed (loss: 0.04739805683493614, acc: 0.9863842725753784)
[2025-02-13 04:19:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:55][root][INFO] - Training Epoch: 2/2, step 5067/7134 completed (loss: 0.019772369414567947, acc: 0.9961013793945312)
[2025-02-13 04:19:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:56][root][INFO] - Training Epoch: 2/2, step 5068/7134 completed (loss: 0.019211556762456894, acc: 0.9936808943748474)
[2025-02-13 04:19:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:56][root][INFO] - Training Epoch: 2/2, step 5069/7134 completed (loss: 0.011773898266255856, acc: 0.995312511920929)
[2025-02-13 04:19:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:57][root][INFO] - Training Epoch: 2/2, step 5070/7134 completed (loss: 0.004637148696929216, acc: 1.0)
[2025-02-13 04:19:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:57][root][INFO] - Training Epoch: 2/2, step 5071/7134 completed (loss: 0.033180978149175644, acc: 0.9929873943328857)
[2025-02-13 04:19:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:57][root][INFO] - Training Epoch: 2/2, step 5072/7134 completed (loss: 0.04409800469875336, acc: 0.9887076616287231)
[2025-02-13 04:19:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:58][root][INFO] - Training Epoch: 2/2, step 5073/7134 completed (loss: 0.030095327645540237, acc: 0.988394558429718)
[2025-02-13 04:19:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:58][root][INFO] - Training Epoch: 2/2, step 5074/7134 completed (loss: 0.024785365909337997, acc: 0.9933664798736572)
[2025-02-13 04:19:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:59][root][INFO] - Training Epoch: 2/2, step 5075/7134 completed (loss: 0.02897159568965435, acc: 0.9922480583190918)
[2025-02-13 04:19:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:19:59][root][INFO] - Training Epoch: 2/2, step 5076/7134 completed (loss: 0.03862350434064865, acc: 0.9904305934906006)
[2025-02-13 04:19:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:00][root][INFO] - Training Epoch: 2/2, step 5077/7134 completed (loss: 0.0076113175600767136, acc: 0.9952531456947327)
[2025-02-13 04:20:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:00][root][INFO] - Training Epoch: 2/2, step 5078/7134 completed (loss: 0.06210291013121605, acc: 0.9832636117935181)
[2025-02-13 04:20:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:01][root][INFO] - Training Epoch: 2/2, step 5079/7134 completed (loss: 0.016719624400138855, acc: 0.9959183931350708)
[2025-02-13 04:20:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:01][root][INFO] - Training Epoch: 2/2, step 5080/7134 completed (loss: 0.01150925736874342, acc: 0.9964538812637329)
[2025-02-13 04:20:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:01][root][INFO] - Training Epoch: 2/2, step 5081/7134 completed (loss: 0.006570303346961737, acc: 0.998641312122345)
[2025-02-13 04:20:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:02][root][INFO] - Training Epoch: 2/2, step 5082/7134 completed (loss: 0.015129266306757927, acc: 0.9948979616165161)
[2025-02-13 04:20:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:02][root][INFO] - Training Epoch: 2/2, step 5083/7134 completed (loss: 0.03404522314667702, acc: 0.9929676651954651)
[2025-02-13 04:20:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:03][root][INFO] - Training Epoch: 2/2, step 5084/7134 completed (loss: 0.015330447815358639, acc: 0.9953703880310059)
[2025-02-13 04:20:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:03][root][INFO] - Training Epoch: 2/2, step 5085/7134 completed (loss: 0.017219701781868935, acc: 0.9964497089385986)
[2025-02-13 04:20:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:04][root][INFO] - Training Epoch: 2/2, step 5086/7134 completed (loss: 0.02636478655040264, acc: 0.9923760890960693)
[2025-02-13 04:20:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:04][root][INFO] - Training Epoch: 2/2, step 5087/7134 completed (loss: 0.005294846836477518, acc: 0.9986613392829895)
[2025-02-13 04:20:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:04][root][INFO] - Training Epoch: 2/2, step 5088/7134 completed (loss: 0.005706231575459242, acc: 1.0)
[2025-02-13 04:20:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:05][root][INFO] - Training Epoch: 2/2, step 5089/7134 completed (loss: 0.01635955274105072, acc: 0.9975062608718872)
[2025-02-13 04:20:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:05][root][INFO] - Training Epoch: 2/2, step 5090/7134 completed (loss: 0.009776264429092407, acc: 0.9976662993431091)
[2025-02-13 04:20:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:06][root][INFO] - Training Epoch: 2/2, step 5091/7134 completed (loss: 0.009420287795364857, acc: 0.9986824989318848)
[2025-02-13 04:20:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:06][root][INFO] - Training Epoch: 2/2, step 5092/7134 completed (loss: 0.02736995369195938, acc: 0.9944674968719482)
[2025-02-13 04:20:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:07][root][INFO] - Training Epoch: 2/2, step 5093/7134 completed (loss: 0.023066414520144463, acc: 0.9917126893997192)
[2025-02-13 04:20:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:07][root][INFO] - Training Epoch: 2/2, step 5094/7134 completed (loss: 0.006780209019780159, acc: 0.9981818199157715)
[2025-02-13 04:20:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:08][root][INFO] - Training Epoch: 2/2, step 5095/7134 completed (loss: 0.0510074608027935, acc: 0.9828392863273621)
[2025-02-13 04:20:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:08][root][INFO] - Training Epoch: 2/2, step 5096/7134 completed (loss: 0.015275995247066021, acc: 0.99190753698349)
[2025-02-13 04:20:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:08][root][INFO] - Training Epoch: 2/2, step 5097/7134 completed (loss: 0.015835991129279137, acc: 0.9926739931106567)
[2025-02-13 04:20:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:09][root][INFO] - Training Epoch: 2/2, step 5098/7134 completed (loss: 0.011654917150735855, acc: 0.9943757057189941)
[2025-02-13 04:20:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:09][root][INFO] - Training Epoch: 2/2, step 5099/7134 completed (loss: 0.017355117946863174, acc: 0.9953810572624207)
[2025-02-13 04:20:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:10][root][INFO] - Training Epoch: 2/2, step 5100/7134 completed (loss: 0.028284532949328423, acc: 0.992290735244751)
[2025-02-13 04:20:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:10][root][INFO] - Training Epoch: 2/2, step 5101/7134 completed (loss: 0.010693310759961605, acc: 0.9953380227088928)
[2025-02-13 04:20:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:11][root][INFO] - Training Epoch: 2/2, step 5102/7134 completed (loss: 0.029517170041799545, acc: 0.9918144345283508)
[2025-02-13 04:20:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:11][root][INFO] - Training Epoch: 2/2, step 5103/7134 completed (loss: 0.026603883132338524, acc: 0.9896907210350037)
[2025-02-13 04:20:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:12][root][INFO] - Training Epoch: 2/2, step 5104/7134 completed (loss: 0.015797415748238564, acc: 0.9966777563095093)
[2025-02-13 04:20:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:12][root][INFO] - Training Epoch: 2/2, step 5105/7134 completed (loss: 0.01852993108332157, acc: 0.9940387606620789)
[2025-02-13 04:20:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:12][root][INFO] - Training Epoch: 2/2, step 5106/7134 completed (loss: 0.014757680706679821, acc: 0.9970370531082153)
[2025-02-13 04:20:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:13][root][INFO] - Training Epoch: 2/2, step 5107/7134 completed (loss: 0.07211384177207947, acc: 0.9857369065284729)
[2025-02-13 04:20:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:13][root][INFO] - Training Epoch: 2/2, step 5108/7134 completed (loss: 0.04070071130990982, acc: 0.9895470142364502)
[2025-02-13 04:20:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:14][root][INFO] - Training Epoch: 2/2, step 5109/7134 completed (loss: 0.04679478704929352, acc: 0.9878318309783936)
[2025-02-13 04:20:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:14][root][INFO] - Training Epoch: 2/2, step 5110/7134 completed (loss: 0.04144871607422829, acc: 0.9884615540504456)
[2025-02-13 04:20:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:15][root][INFO] - Training Epoch: 2/2, step 5111/7134 completed (loss: 0.013242311775684357, acc: 0.9958904385566711)
[2025-02-13 04:20:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:15][root][INFO] - Training Epoch: 2/2, step 5112/7134 completed (loss: 0.04463088512420654, acc: 0.9925280213356018)
[2025-02-13 04:20:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:16][root][INFO] - Training Epoch: 2/2, step 5113/7134 completed (loss: 0.032935529947280884, acc: 0.9895522594451904)
[2025-02-13 04:20:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:16][root][INFO] - Training Epoch: 2/2, step 5114/7134 completed (loss: 0.0173249002546072, acc: 0.9958275556564331)
[2025-02-13 04:20:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:16][root][INFO] - Training Epoch: 2/2, step 5115/7134 completed (loss: 0.018644222989678383, acc: 0.9948387145996094)
[2025-02-13 04:20:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:17][root][INFO] - Training Epoch: 2/2, step 5116/7134 completed (loss: 0.007588433101773262, acc: 0.9974026083946228)
[2025-02-13 04:20:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:17][root][INFO] - Training Epoch: 2/2, step 5117/7134 completed (loss: 0.01775520108640194, acc: 0.9958158731460571)
[2025-02-13 04:20:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:18][root][INFO] - Training Epoch: 2/2, step 5118/7134 completed (loss: 0.03612900152802467, acc: 0.9908952713012695)
[2025-02-13 04:20:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:18][root][INFO] - Training Epoch: 2/2, step 5119/7134 completed (loss: 0.025753045454621315, acc: 0.9972489476203918)
[2025-02-13 04:20:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:19][root][INFO] - Training Epoch: 2/2, step 5120/7134 completed (loss: 0.03968394547700882, acc: 0.9921466112136841)
[2025-02-13 04:20:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:19][root][INFO] - Training Epoch: 2/2, step 5121/7134 completed (loss: 0.02271059714257717, acc: 0.994452178478241)
[2025-02-13 04:20:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:20][root][INFO] - Training Epoch: 2/2, step 5122/7134 completed (loss: 0.03629060089588165, acc: 0.9887482523918152)
[2025-02-13 04:20:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:20][root][INFO] - Training Epoch: 2/2, step 5123/7134 completed (loss: 0.01781417988240719, acc: 0.9951748847961426)
[2025-02-13 04:20:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:20][root][INFO] - Training Epoch: 2/2, step 5124/7134 completed (loss: 0.023621343076229095, acc: 0.991304337978363)
[2025-02-13 04:20:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:21][root][INFO] - Training Epoch: 2/2, step 5125/7134 completed (loss: 0.01695818454027176, acc: 0.9952606558799744)
[2025-02-13 04:20:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:21][root][INFO] - Training Epoch: 2/2, step 5126/7134 completed (loss: 0.011259091086685658, acc: 0.9948717951774597)
[2025-02-13 04:20:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:22][root][INFO] - Training Epoch: 2/2, step 5127/7134 completed (loss: 0.02606087736785412, acc: 0.9930070042610168)
[2025-02-13 04:20:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:22][root][INFO] - Training Epoch: 2/2, step 5128/7134 completed (loss: 0.018372785300016403, acc: 0.9945054650306702)
[2025-02-13 04:20:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:23][root][INFO] - Training Epoch: 2/2, step 5129/7134 completed (loss: 0.03939804434776306, acc: 0.9843137264251709)
[2025-02-13 04:20:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:23][root][INFO] - Training Epoch: 2/2, step 5130/7134 completed (loss: 0.008748723194003105, acc: 0.9968798756599426)
[2025-02-13 04:20:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:23][root][INFO] - Training Epoch: 2/2, step 5131/7134 completed (loss: 0.045736897736787796, acc: 0.9918830990791321)
[2025-02-13 04:20:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:24][root][INFO] - Training Epoch: 2/2, step 5132/7134 completed (loss: 0.008949393406510353, acc: 0.996039628982544)
[2025-02-13 04:20:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:24][root][INFO] - Training Epoch: 2/2, step 5133/7134 completed (loss: 0.03263739496469498, acc: 0.9906832575798035)
[2025-02-13 04:20:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:25][root][INFO] - Training Epoch: 2/2, step 5134/7134 completed (loss: 0.014584091491997242, acc: 0.9962476491928101)
[2025-02-13 04:20:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:25][root][INFO] - Training Epoch: 2/2, step 5135/7134 completed (loss: 0.044455695897340775, acc: 0.9942857027053833)
[2025-02-13 04:20:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:26][root][INFO] - Training Epoch: 2/2, step 5136/7134 completed (loss: 0.016113465651869774, acc: 0.9959016442298889)
[2025-02-13 04:20:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:26][root][INFO] - Training Epoch: 2/2, step 5137/7134 completed (loss: 0.014047782868146896, acc: 0.9981651306152344)
[2025-02-13 04:20:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:27][root][INFO] - Training Epoch: 2/2, step 5138/7134 completed (loss: 0.011244462803006172, acc: 0.9962499737739563)
[2025-02-13 04:20:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:27][root][INFO] - Training Epoch: 2/2, step 5139/7134 completed (loss: 0.010026237927377224, acc: 0.9985693693161011)
[2025-02-13 04:20:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:27][root][INFO] - Training Epoch: 2/2, step 5140/7134 completed (loss: 0.015271630138158798, acc: 0.9968186616897583)
[2025-02-13 04:20:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:28][root][INFO] - Training Epoch: 2/2, step 5141/7134 completed (loss: 0.03586602956056595, acc: 0.992277979850769)
[2025-02-13 04:20:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:28][root][INFO] - Training Epoch: 2/2, step 5142/7134 completed (loss: 0.009045320563018322, acc: 0.9976470470428467)
[2025-02-13 04:20:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:29][root][INFO] - Training Epoch: 2/2, step 5143/7134 completed (loss: 0.015443424694240093, acc: 0.9936548471450806)
[2025-02-13 04:20:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:29][root][INFO] - Training Epoch: 2/2, step 5144/7134 completed (loss: 0.015920337289571762, acc: 0.9953325390815735)
[2025-02-13 04:20:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:30][root][INFO] - Training Epoch: 2/2, step 5145/7134 completed (loss: 0.009275049902498722, acc: 0.9969293475151062)
[2025-02-13 04:20:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:30][root][INFO] - Training Epoch: 2/2, step 5146/7134 completed (loss: 0.059050045907497406, acc: 0.9832776188850403)
[2025-02-13 04:20:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:30][root][INFO] - Training Epoch: 2/2, step 5147/7134 completed (loss: 0.018297748640179634, acc: 0.9918032884597778)
[2025-02-13 04:20:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:31][root][INFO] - Training Epoch: 2/2, step 5148/7134 completed (loss: 0.04651599004864693, acc: 0.9880159497261047)
[2025-02-13 04:20:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:31][root][INFO] - Training Epoch: 2/2, step 5149/7134 completed (loss: 0.019365275278687477, acc: 0.993034839630127)
[2025-02-13 04:20:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:32][root][INFO] - Training Epoch: 2/2, step 5150/7134 completed (loss: 0.03131665289402008, acc: 0.9882659912109375)
[2025-02-13 04:20:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:32][root][INFO] - Training Epoch: 2/2, step 5151/7134 completed (loss: 0.026878230273723602, acc: 0.9913366436958313)
[2025-02-13 04:20:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:33][root][INFO] - Training Epoch: 2/2, step 5152/7134 completed (loss: 0.014324847608804703, acc: 0.9959785342216492)
[2025-02-13 04:20:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:33][root][INFO] - Training Epoch: 2/2, step 5153/7134 completed (loss: 0.018626149743795395, acc: 0.991769552230835)
[2025-02-13 04:20:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:34][root][INFO] - Training Epoch: 2/2, step 5154/7134 completed (loss: 0.027601737529039383, acc: 0.9931740760803223)
[2025-02-13 04:20:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:34][root][INFO] - Training Epoch: 2/2, step 5155/7134 completed (loss: 0.02983159013092518, acc: 0.9939758777618408)
[2025-02-13 04:20:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:35][root][INFO] - Training Epoch: 2/2, step 5156/7134 completed (loss: 0.020700110122561455, acc: 0.9930486679077148)
[2025-02-13 04:20:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:35][root][INFO] - Training Epoch: 2/2, step 5157/7134 completed (loss: 0.06070820987224579, acc: 0.9841075539588928)
[2025-02-13 04:20:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:35][root][INFO] - Training Epoch: 2/2, step 5158/7134 completed (loss: 0.06894609332084656, acc: 0.9821958541870117)
[2025-02-13 04:20:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:36][root][INFO] - Training Epoch: 2/2, step 5159/7134 completed (loss: 0.019800495356321335, acc: 0.9908972978591919)
[2025-02-13 04:20:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:36][root][INFO] - Training Epoch: 2/2, step 5160/7134 completed (loss: 0.04563344642519951, acc: 0.9827089309692383)
[2025-02-13 04:20:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:37][root][INFO] - Training Epoch: 2/2, step 5161/7134 completed (loss: 0.07479729503393173, acc: 0.9790209531784058)
[2025-02-13 04:20:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:37][root][INFO] - Training Epoch: 2/2, step 5162/7134 completed (loss: 0.01162051036953926, acc: 0.9983792304992676)
[2025-02-13 04:20:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:38][root][INFO] - Training Epoch: 2/2, step 5163/7134 completed (loss: 0.018630925565958023, acc: 0.9943342804908752)
[2025-02-13 04:20:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:38][root][INFO] - Training Epoch: 2/2, step 5164/7134 completed (loss: 0.010276418179273605, acc: 0.9966386556625366)
[2025-02-13 04:20:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:38][root][INFO] - Training Epoch: 2/2, step 5165/7134 completed (loss: 0.037384405732154846, acc: 0.9924699068069458)
[2025-02-13 04:20:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:39][root][INFO] - Training Epoch: 2/2, step 5166/7134 completed (loss: 0.008572923019528389, acc: 0.9984779357910156)
[2025-02-13 04:20:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:39][root][INFO] - Training Epoch: 2/2, step 5167/7134 completed (loss: 0.046312421560287476, acc: 0.9855769276618958)
[2025-02-13 04:20:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:40][root][INFO] - Training Epoch: 2/2, step 5168/7134 completed (loss: 0.02761327289044857, acc: 0.989130437374115)
[2025-02-13 04:20:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:40][root][INFO] - Training Epoch: 2/2, step 5169/7134 completed (loss: 0.041256073862314224, acc: 0.985981285572052)
[2025-02-13 04:20:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:41][root][INFO] - Training Epoch: 2/2, step 5170/7134 completed (loss: 0.021826889365911484, acc: 0.9947019815444946)
[2025-02-13 04:20:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:41][root][INFO] - Training Epoch: 2/2, step 5171/7134 completed (loss: 0.014494714327156544, acc: 0.995726466178894)
[2025-02-13 04:20:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:41][root][INFO] - Training Epoch: 2/2, step 5172/7134 completed (loss: 0.03934355452656746, acc: 0.9863713979721069)
[2025-02-13 04:20:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:42][root][INFO] - Training Epoch: 2/2, step 5173/7134 completed (loss: 0.03463078662753105, acc: 0.9858267903327942)
[2025-02-13 04:20:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:42][root][INFO] - Training Epoch: 2/2, step 5174/7134 completed (loss: 0.013119539245963097, acc: 0.9974457025527954)
[2025-02-13 04:20:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:43][root][INFO] - Training Epoch: 2/2, step 5175/7134 completed (loss: 0.01276722364127636, acc: 0.9958563446998596)
[2025-02-13 04:20:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:43][root][INFO] - Training Epoch: 2/2, step 5176/7134 completed (loss: 0.008618022315204144, acc: 1.0)
[2025-02-13 04:20:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:44][root][INFO] - Training Epoch: 2/2, step 5177/7134 completed (loss: 0.04571032524108887, acc: 0.9895651936531067)
[2025-02-13 04:20:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:44][root][INFO] - Training Epoch: 2/2, step 5178/7134 completed (loss: 0.08599752187728882, acc: 0.9843205809593201)
[2025-02-13 04:20:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:44][root][INFO] - Training Epoch: 2/2, step 5179/7134 completed (loss: 0.019616253674030304, acc: 0.9926035404205322)
[2025-02-13 04:20:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:45][root][INFO] - Training Epoch: 2/2, step 5180/7134 completed (loss: 0.037554118782281876, acc: 0.9868667721748352)
[2025-02-13 04:20:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:45][root][INFO] - Training Epoch: 2/2, step 5181/7134 completed (loss: 0.03858568146824837, acc: 0.9893617033958435)
[2025-02-13 04:20:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:46][root][INFO] - Training Epoch: 2/2, step 5182/7134 completed (loss: 0.11101403087377548, acc: 0.9708333611488342)
[2025-02-13 04:20:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:46][root][INFO] - Training Epoch: 2/2, step 5183/7134 completed (loss: 0.04343760386109352, acc: 0.9884488582611084)
[2025-02-13 04:20:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:46][root][INFO] - Training Epoch: 2/2, step 5184/7134 completed (loss: 0.020535364747047424, acc: 0.995110034942627)
[2025-02-13 04:20:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:47][root][INFO] - Training Epoch: 2/2, step 5185/7134 completed (loss: 0.020242836326360703, acc: 0.9951807260513306)
[2025-02-13 04:20:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:47][root][INFO] - Training Epoch: 2/2, step 5186/7134 completed (loss: 0.034781888127326965, acc: 0.9857434034347534)
[2025-02-13 04:20:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:48][root][INFO] - Training Epoch: 2/2, step 5187/7134 completed (loss: 0.01222729030996561, acc: 0.9961904883384705)
[2025-02-13 04:20:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:48][root][INFO] - Training Epoch: 2/2, step 5188/7134 completed (loss: 0.08444990962743759, acc: 0.9745454788208008)
[2025-02-13 04:20:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:48][root][INFO] - Training Epoch: 2/2, step 5189/7134 completed (loss: 0.048986200243234634, acc: 0.9897959232330322)
[2025-02-13 04:20:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:49][root][INFO] - Training Epoch: 2/2, step 5190/7134 completed (loss: 0.02282918244600296, acc: 0.9976246953010559)
[2025-02-13 04:20:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:49][root][INFO] - Training Epoch: 2/2, step 5191/7134 completed (loss: 0.07091181725263596, acc: 0.9780380725860596)
[2025-02-13 04:20:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:50][root][INFO] - Training Epoch: 2/2, step 5192/7134 completed (loss: 0.03535449504852295, acc: 0.9928656220436096)
[2025-02-13 04:20:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:50][root][INFO] - Training Epoch: 2/2, step 5193/7134 completed (loss: 0.011964835226535797, acc: 0.998031497001648)
[2025-02-13 04:20:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:51][root][INFO] - Training Epoch: 2/2, step 5194/7134 completed (loss: 0.03016597032546997, acc: 0.9920791983604431)
[2025-02-13 04:20:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:51][root][INFO] - Training Epoch: 2/2, step 5195/7134 completed (loss: 0.039187684655189514, acc: 0.9912280440330505)
[2025-02-13 04:20:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:51][root][INFO] - Training Epoch: 2/2, step 5196/7134 completed (loss: 0.022174445912241936, acc: 0.9952152967453003)
[2025-02-13 04:20:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:52][root][INFO] - Training Epoch: 2/2, step 5197/7134 completed (loss: 0.020522618666291237, acc: 0.991769552230835)
[2025-02-13 04:20:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:52][root][INFO] - Training Epoch: 2/2, step 5198/7134 completed (loss: 0.018588047474622726, acc: 0.9937629699707031)
[2025-02-13 04:20:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:53][root][INFO] - Training Epoch: 2/2, step 5199/7134 completed (loss: 0.05984242632985115, acc: 0.9840116500854492)
[2025-02-13 04:20:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:53][root][INFO] - Training Epoch: 2/2, step 5200/7134 completed (loss: 0.011591213755309582, acc: 0.9960707426071167)
[2025-02-13 04:20:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:53][root][INFO] - Training Epoch: 2/2, step 5201/7134 completed (loss: 0.04062732309103012, acc: 0.9927623867988586)
[2025-02-13 04:20:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:54][root][INFO] - Training Epoch: 2/2, step 5202/7134 completed (loss: 0.040225885808467865, acc: 0.9903846383094788)
[2025-02-13 04:20:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:54][root][INFO] - Training Epoch: 2/2, step 5203/7134 completed (loss: 0.04459051415324211, acc: 0.9904761910438538)
[2025-02-13 04:20:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:55][root][INFO] - Training Epoch: 2/2, step 5204/7134 completed (loss: 0.014849054627120495, acc: 0.9958763122558594)
[2025-02-13 04:20:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:55][root][INFO] - Training Epoch: 2/2, step 5205/7134 completed (loss: 0.022225577384233475, acc: 0.9962546825408936)
[2025-02-13 04:20:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:56][root][INFO] - Training Epoch: 2/2, step 5206/7134 completed (loss: 0.03249959647655487, acc: 0.995184600353241)
[2025-02-13 04:20:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:56][root][INFO] - Training Epoch: 2/2, step 5207/7134 completed (loss: 0.04635394364595413, acc: 0.98591548204422)
[2025-02-13 04:20:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:56][root][INFO] - Training Epoch: 2/2, step 5208/7134 completed (loss: 0.015744199976325035, acc: 0.9973614811897278)
[2025-02-13 04:20:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:57][root][INFO] - Training Epoch: 2/2, step 5209/7134 completed (loss: 0.03884438797831535, acc: 0.9859437942504883)
[2025-02-13 04:20:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:57][root][INFO] - Training Epoch: 2/2, step 5210/7134 completed (loss: 0.02436862513422966, acc: 0.9946595430374146)
[2025-02-13 04:20:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:58][root][INFO] - Training Epoch: 2/2, step 5211/7134 completed (loss: 0.0243837907910347, acc: 0.9947916865348816)
[2025-02-13 04:20:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:58][root][INFO] - Training Epoch: 2/2, step 5212/7134 completed (loss: 0.039929889142513275, acc: 0.9931740760803223)
[2025-02-13 04:20:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:59][root][INFO] - Training Epoch: 2/2, step 5213/7134 completed (loss: 0.049795351922512054, acc: 0.9892086386680603)
[2025-02-13 04:20:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:20:59][root][INFO] - Training Epoch: 2/2, step 5214/7134 completed (loss: 0.061352260410785675, acc: 0.9785407781600952)
[2025-02-13 04:20:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:00][root][INFO] - Training Epoch: 2/2, step 5215/7134 completed (loss: 0.04346458613872528, acc: 0.9833971858024597)
[2025-02-13 04:21:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:00][root][INFO] - Training Epoch: 2/2, step 5216/7134 completed (loss: 0.0374077633023262, acc: 0.9847908616065979)
[2025-02-13 04:21:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:00][root][INFO] - Training Epoch: 2/2, step 5217/7134 completed (loss: 0.04802240803837776, acc: 0.9847561120986938)
[2025-02-13 04:21:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:01][root][INFO] - Training Epoch: 2/2, step 5218/7134 completed (loss: 0.026085877791047096, acc: 0.9932584166526794)
[2025-02-13 04:21:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:01][root][INFO] - Training Epoch: 2/2, step 5219/7134 completed (loss: 0.03524594381451607, acc: 0.9951573610305786)
[2025-02-13 04:21:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:01][root][INFO] - Training Epoch: 2/2, step 5220/7134 completed (loss: 0.03360307216644287, acc: 0.9874776601791382)
[2025-02-13 04:21:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:02][root][INFO] - Training Epoch: 2/2, step 5221/7134 completed (loss: 0.034627411514520645, acc: 0.9937694668769836)
[2025-02-13 04:21:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:02][root][INFO] - Training Epoch: 2/2, step 5222/7134 completed (loss: 0.01079073641449213, acc: 1.0)
[2025-02-13 04:21:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:03][root][INFO] - Training Epoch: 2/2, step 5223/7134 completed (loss: 0.024965796619653702, acc: 0.9930875301361084)
[2025-02-13 04:21:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:03][root][INFO] - Training Epoch: 2/2, step 5224/7134 completed (loss: 0.03729534149169922, acc: 0.9912126660346985)
[2025-02-13 04:21:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:03][root][INFO] - Training Epoch: 2/2, step 5225/7134 completed (loss: 0.0575210265815258, acc: 0.9903069734573364)
[2025-02-13 04:21:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:04][root][INFO] - Training Epoch: 2/2, step 5226/7134 completed (loss: 0.03365205600857735, acc: 0.9901153445243835)
[2025-02-13 04:21:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:04][root][INFO] - Training Epoch: 2/2, step 5227/7134 completed (loss: 0.03044533170759678, acc: 0.9909090995788574)
[2025-02-13 04:21:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:05][root][INFO] - Training Epoch: 2/2, step 5228/7134 completed (loss: 0.015226693823933601, acc: 0.9941860437393188)
[2025-02-13 04:21:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:05][root][INFO] - Training Epoch: 2/2, step 5229/7134 completed (loss: 0.01300794631242752, acc: 0.9955423474311829)
[2025-02-13 04:21:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:06][root][INFO] - Training Epoch: 2/2, step 5230/7134 completed (loss: 0.014410129748284817, acc: 0.9946091771125793)
[2025-02-13 04:21:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:06][root][INFO] - Training Epoch: 2/2, step 5231/7134 completed (loss: 0.021495405584573746, acc: 0.9936708807945251)
[2025-02-13 04:21:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:06][root][INFO] - Training Epoch: 2/2, step 5232/7134 completed (loss: 0.01868826150894165, acc: 0.9910314083099365)
[2025-02-13 04:21:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:07][root][INFO] - Training Epoch: 2/2, step 5233/7134 completed (loss: 0.021929265931248665, acc: 0.9966216087341309)
[2025-02-13 04:21:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:07][root][INFO] - Training Epoch: 2/2, step 5234/7134 completed (loss: 0.027108242735266685, acc: 0.9951040148735046)
[2025-02-13 04:21:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:08][root][INFO] - Training Epoch: 2/2, step 5235/7134 completed (loss: 0.03763505816459656, acc: 0.9949579834938049)
[2025-02-13 04:21:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:08][root][INFO] - Training Epoch: 2/2, step 5236/7134 completed (loss: 0.005849991925060749, acc: 0.9982638955116272)
[2025-02-13 04:21:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:09][root][INFO] - Training Epoch: 2/2, step 5237/7134 completed (loss: 0.006941716652363539, acc: 0.9982078671455383)
[2025-02-13 04:21:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:09][root][INFO] - Training Epoch: 2/2, step 5238/7134 completed (loss: 0.013614137656986713, acc: 0.9947183132171631)
[2025-02-13 04:21:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:10][root][INFO] - Training Epoch: 2/2, step 5239/7134 completed (loss: 0.015812808647751808, acc: 0.9942085146903992)
[2025-02-13 04:21:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:10][root][INFO] - Training Epoch: 2/2, step 5240/7134 completed (loss: 0.018676191568374634, acc: 0.9958506226539612)
[2025-02-13 04:21:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:10][root][INFO] - Training Epoch: 2/2, step 5241/7134 completed (loss: 0.03704046830534935, acc: 0.9937304258346558)
[2025-02-13 04:21:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:11][root][INFO] - Training Epoch: 2/2, step 5242/7134 completed (loss: 0.005775454919785261, acc: 0.9983471035957336)
[2025-02-13 04:21:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:11][root][INFO] - Training Epoch: 2/2, step 5243/7134 completed (loss: 0.025561971589922905, acc: 0.9948275685310364)
[2025-02-13 04:21:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:12][root][INFO] - Training Epoch: 2/2, step 5244/7134 completed (loss: 0.011055190116167068, acc: 0.9986206889152527)
[2025-02-13 04:21:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:12][root][INFO] - Training Epoch: 2/2, step 5245/7134 completed (loss: 0.016542453318834305, acc: 0.9965096116065979)
[2025-02-13 04:21:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:13][root][INFO] - Training Epoch: 2/2, step 5246/7134 completed (loss: 0.01667395792901516, acc: 0.9938650131225586)
[2025-02-13 04:21:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:13][root][INFO] - Training Epoch: 2/2, step 5247/7134 completed (loss: 0.014118630439043045, acc: 0.9944444298744202)
[2025-02-13 04:21:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:13][root][INFO] - Training Epoch: 2/2, step 5248/7134 completed (loss: 0.032106347382068634, acc: 0.9943820238113403)
[2025-02-13 04:21:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:14][root][INFO] - Training Epoch: 2/2, step 5249/7134 completed (loss: 0.007605262566357851, acc: 0.99842768907547)
[2025-02-13 04:21:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:14][root][INFO] - Training Epoch: 2/2, step 5250/7134 completed (loss: 0.011994213797152042, acc: 0.996216893196106)
[2025-02-13 04:21:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:15][root][INFO] - Training Epoch: 2/2, step 5251/7134 completed (loss: 0.02285388670861721, acc: 0.9915151596069336)
[2025-02-13 04:21:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:15][root][INFO] - Training Epoch: 2/2, step 5252/7134 completed (loss: 0.036039240658283234, acc: 0.9883177280426025)
[2025-02-13 04:21:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:16][root][INFO] - Training Epoch: 2/2, step 5253/7134 completed (loss: 0.008677659556269646, acc: 0.9967319965362549)
[2025-02-13 04:21:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:16][root][INFO] - Training Epoch: 2/2, step 5254/7134 completed (loss: 0.03558218106627464, acc: 0.9881154298782349)
[2025-02-13 04:21:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:16][root][INFO] - Training Epoch: 2/2, step 5255/7134 completed (loss: 0.029779821634292603, acc: 0.9917808175086975)
[2025-02-13 04:21:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:17][root][INFO] - Training Epoch: 2/2, step 5256/7134 completed (loss: 0.04754692688584328, acc: 0.9876352548599243)
[2025-02-13 04:21:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:17][root][INFO] - Training Epoch: 2/2, step 5257/7134 completed (loss: 0.041371043771505356, acc: 0.9878234267234802)
[2025-02-13 04:21:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:18][root][INFO] - Training Epoch: 2/2, step 5258/7134 completed (loss: 0.03001626953482628, acc: 0.9897959232330322)
[2025-02-13 04:21:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:18][root][INFO] - Training Epoch: 2/2, step 5259/7134 completed (loss: 0.038185421377420425, acc: 0.9920886158943176)
[2025-02-13 04:21:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:19][root][INFO] - Training Epoch: 2/2, step 5260/7134 completed (loss: 0.018349923193454742, acc: 0.9963414669036865)
[2025-02-13 04:21:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:19][root][INFO] - Training Epoch: 2/2, step 5261/7134 completed (loss: 0.04200931265950203, acc: 0.9865384697914124)
[2025-02-13 04:21:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:19][root][INFO] - Training Epoch: 2/2, step 5262/7134 completed (loss: 0.012808375060558319, acc: 0.9958620667457581)
[2025-02-13 04:21:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:20][root][INFO] - Training Epoch: 2/2, step 5263/7134 completed (loss: 0.035615112632513046, acc: 0.9894737005233765)
[2025-02-13 04:21:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:20][root][INFO] - Training Epoch: 2/2, step 5264/7134 completed (loss: 0.015139571391046047, acc: 0.9914945363998413)
[2025-02-13 04:21:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:21][root][INFO] - Training Epoch: 2/2, step 5265/7134 completed (loss: 0.021971434354782104, acc: 0.9923760890960693)
[2025-02-13 04:21:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:21][root][INFO] - Training Epoch: 2/2, step 5266/7134 completed (loss: 0.01923767291009426, acc: 0.9917808175086975)
[2025-02-13 04:21:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:22][root][INFO] - Training Epoch: 2/2, step 5267/7134 completed (loss: 0.056064777076244354, acc: 0.9860031008720398)
[2025-02-13 04:21:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:22][root][INFO] - Training Epoch: 2/2, step 5268/7134 completed (loss: 0.025366250425577164, acc: 0.9904030561447144)
[2025-02-13 04:21:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:22][root][INFO] - Training Epoch: 2/2, step 5269/7134 completed (loss: 0.054079603403806686, acc: 0.9785330891609192)
[2025-02-13 04:21:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:23][root][INFO] - Training Epoch: 2/2, step 5270/7134 completed (loss: 0.0167399775236845, acc: 0.9959893226623535)
[2025-02-13 04:21:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:23][root][INFO] - Training Epoch: 2/2, step 5271/7134 completed (loss: 0.04566120728850365, acc: 0.9894179701805115)
[2025-02-13 04:21:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:24][root][INFO] - Training Epoch: 2/2, step 5272/7134 completed (loss: 0.01995731331408024, acc: 0.9956011772155762)
[2025-02-13 04:21:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:24][root][INFO] - Training Epoch: 2/2, step 5273/7134 completed (loss: 0.02212131768465042, acc: 0.9915110468864441)
[2025-02-13 04:21:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:25][root][INFO] - Training Epoch: 2/2, step 5274/7134 completed (loss: 0.03254038840532303, acc: 0.9925373196601868)
[2025-02-13 04:21:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:25][root][INFO] - Training Epoch: 2/2, step 5275/7134 completed (loss: 0.01957637630403042, acc: 0.998171865940094)
[2025-02-13 04:21:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:25][root][INFO] - Training Epoch: 2/2, step 5276/7134 completed (loss: 0.034174103289842606, acc: 0.988041877746582)
[2025-02-13 04:21:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:26][root][INFO] - Training Epoch: 2/2, step 5277/7134 completed (loss: 0.019937565550208092, acc: 0.9942362904548645)
[2025-02-13 04:21:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:26][root][INFO] - Training Epoch: 2/2, step 5278/7134 completed (loss: 0.03260258212685585, acc: 0.9892966151237488)
[2025-02-13 04:21:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:27][root][INFO] - Training Epoch: 2/2, step 5279/7134 completed (loss: 0.00552057009190321, acc: 0.9980545043945312)
[2025-02-13 04:21:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:27][root][INFO] - Training Epoch: 2/2, step 5280/7134 completed (loss: 0.022226564586162567, acc: 0.9966832399368286)
[2025-02-13 04:21:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:27][root][INFO] - Training Epoch: 2/2, step 5281/7134 completed (loss: 0.013967655599117279, acc: 0.9950082898139954)
[2025-02-13 04:21:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:28][root][INFO] - Training Epoch: 2/2, step 5282/7134 completed (loss: 0.02239350788295269, acc: 0.9928673505783081)
[2025-02-13 04:21:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:28][root][INFO] - Training Epoch: 2/2, step 5283/7134 completed (loss: 0.02132299542427063, acc: 0.9917920827865601)
[2025-02-13 04:21:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:29][root][INFO] - Training Epoch: 2/2, step 5284/7134 completed (loss: 0.022444026544690132, acc: 0.9972337484359741)
[2025-02-13 04:21:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:29][root][INFO] - Training Epoch: 2/2, step 5285/7134 completed (loss: 0.020048322156071663, acc: 0.9940828680992126)
[2025-02-13 04:21:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:30][root][INFO] - Training Epoch: 2/2, step 5286/7134 completed (loss: 0.022059181705117226, acc: 0.9970588088035583)
[2025-02-13 04:21:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:30][root][INFO] - Training Epoch: 2/2, step 5287/7134 completed (loss: 0.024799488484859467, acc: 0.9905149340629578)
[2025-02-13 04:21:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:31][root][INFO] - Training Epoch: 2/2, step 5288/7134 completed (loss: 0.010574685409665108, acc: 0.9969325065612793)
[2025-02-13 04:21:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:31][root][INFO] - Training Epoch: 2/2, step 5289/7134 completed (loss: 0.0356430783867836, acc: 0.9900850057601929)
[2025-02-13 04:21:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:31][root][INFO] - Training Epoch: 2/2, step 5290/7134 completed (loss: 0.026074791327118874, acc: 0.9911392331123352)
[2025-02-13 04:21:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:32][root][INFO] - Training Epoch: 2/2, step 5291/7134 completed (loss: 0.027291616424918175, acc: 0.9975757598876953)
[2025-02-13 04:21:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:32][root][INFO] - Training Epoch: 2/2, step 5292/7134 completed (loss: 0.00876931007951498, acc: 0.9974554777145386)
[2025-02-13 04:21:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:33][root][INFO] - Training Epoch: 2/2, step 5293/7134 completed (loss: 0.021656865254044533, acc: 0.9941605925559998)
[2025-02-13 04:21:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:33][root][INFO] - Training Epoch: 2/2, step 5294/7134 completed (loss: 0.021541597321629524, acc: 0.991631805896759)
[2025-02-13 04:21:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:34][root][INFO] - Training Epoch: 2/2, step 5295/7134 completed (loss: 0.011816892772912979, acc: 0.998161792755127)
[2025-02-13 04:21:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:34][root][INFO] - Training Epoch: 2/2, step 5296/7134 completed (loss: 0.01659025251865387, acc: 0.9944367408752441)
[2025-02-13 04:21:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:34][root][INFO] - Training Epoch: 2/2, step 5297/7134 completed (loss: 0.03336106613278389, acc: 0.9913793206214905)
[2025-02-13 04:21:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:35][root][INFO] - Training Epoch: 2/2, step 5298/7134 completed (loss: 0.01441240031272173, acc: 0.9948717951774597)
[2025-02-13 04:21:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:35][root][INFO] - Training Epoch: 2/2, step 5299/7134 completed (loss: 0.021030310541391373, acc: 0.9903692007064819)
[2025-02-13 04:21:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:36][root][INFO] - Training Epoch: 2/2, step 5300/7134 completed (loss: 0.028439274057745934, acc: 0.9926578402519226)
[2025-02-13 04:21:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:36][root][INFO] - Training Epoch: 2/2, step 5301/7134 completed (loss: 0.012260389514267445, acc: 0.9955947399139404)
[2025-02-13 04:21:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:37][root][INFO] - Training Epoch: 2/2, step 5302/7134 completed (loss: 0.044506121426820755, acc: 0.9863221645355225)
[2025-02-13 04:21:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:37][root][INFO] - Training Epoch: 2/2, step 5303/7134 completed (loss: 0.03793146088719368, acc: 0.9946902394294739)
[2025-02-13 04:21:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:37][root][INFO] - Training Epoch: 2/2, step 5304/7134 completed (loss: 0.06977739185094833, acc: 0.9833610653877258)
[2025-02-13 04:21:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:38][root][INFO] - Training Epoch: 2/2, step 5305/7134 completed (loss: 0.053905658423900604, acc: 0.9839416146278381)
[2025-02-13 04:21:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:38][root][INFO] - Training Epoch: 2/2, step 5306/7134 completed (loss: 0.010585597716271877, acc: 0.9986110925674438)
[2025-02-13 04:21:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:39][root][INFO] - Training Epoch: 2/2, step 5307/7134 completed (loss: 0.04076254740357399, acc: 0.9907407164573669)
[2025-02-13 04:21:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:39][root][INFO] - Training Epoch: 2/2, step 5308/7134 completed (loss: 0.031712789088487625, acc: 0.9924471378326416)
[2025-02-13 04:21:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:39][root][INFO] - Training Epoch: 2/2, step 5309/7134 completed (loss: 0.05287227779626846, acc: 0.9821109175682068)
[2025-02-13 04:21:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:40][root][INFO] - Training Epoch: 2/2, step 5310/7134 completed (loss: 0.054867103695869446, acc: 0.9832636117935181)
[2025-02-13 04:21:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:40][root][INFO] - Training Epoch: 2/2, step 5311/7134 completed (loss: 0.04314767196774483, acc: 0.9887640476226807)
[2025-02-13 04:21:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:41][root][INFO] - Training Epoch: 2/2, step 5312/7134 completed (loss: 0.014750003814697266, acc: 0.9943714737892151)
[2025-02-13 04:21:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:41][root][INFO] - Training Epoch: 2/2, step 5313/7134 completed (loss: 0.02147478051483631, acc: 0.994535505771637)
[2025-02-13 04:21:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:42][root][INFO] - Training Epoch: 2/2, step 5314/7134 completed (loss: 0.02643546275794506, acc: 0.9912891983985901)
[2025-02-13 04:21:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:42][root][INFO] - Training Epoch: 2/2, step 5315/7134 completed (loss: 0.021228116005659103, acc: 0.9934533834457397)
[2025-02-13 04:21:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:42][root][INFO] - Training Epoch: 2/2, step 5316/7134 completed (loss: 0.03951121121644974, acc: 0.9932773113250732)
[2025-02-13 04:21:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:43][root][INFO] - Training Epoch: 2/2, step 5317/7134 completed (loss: 0.021287349984049797, acc: 0.9928315281867981)
[2025-02-13 04:21:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:43][root][INFO] - Training Epoch: 2/2, step 5318/7134 completed (loss: 0.018040750175714493, acc: 0.9930915236473083)
[2025-02-13 04:21:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:44][root][INFO] - Training Epoch: 2/2, step 5319/7134 completed (loss: 0.034464795142412186, acc: 0.9945255517959595)
[2025-02-13 04:21:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:44][root][INFO] - Training Epoch: 2/2, step 5320/7134 completed (loss: 0.02700149640440941, acc: 0.9953415989875793)
[2025-02-13 04:21:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:44][root][INFO] - Training Epoch: 2/2, step 5321/7134 completed (loss: 0.01846771314740181, acc: 0.9937984347343445)
[2025-02-13 04:21:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:45][root][INFO] - Training Epoch: 2/2, step 5322/7134 completed (loss: 0.015563644468784332, acc: 0.9954751133918762)
[2025-02-13 04:21:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:45][root][INFO] - Training Epoch: 2/2, step 5323/7134 completed (loss: 0.03309604525566101, acc: 0.989393949508667)
[2025-02-13 04:21:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:46][root][INFO] - Training Epoch: 2/2, step 5324/7134 completed (loss: 0.017621614038944244, acc: 0.9969087839126587)
[2025-02-13 04:21:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:46][root][INFO] - Training Epoch: 2/2, step 5325/7134 completed (loss: 0.012185001745820045, acc: 0.9982964396476746)
[2025-02-13 04:21:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:46][root][INFO] - Training Epoch: 2/2, step 5326/7134 completed (loss: 0.02301201783120632, acc: 0.9919354915618896)
[2025-02-13 04:21:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:47][root][INFO] - Training Epoch: 2/2, step 5327/7134 completed (loss: 0.019252188503742218, acc: 0.9927431344985962)
[2025-02-13 04:21:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:47][root][INFO] - Training Epoch: 2/2, step 5328/7134 completed (loss: 0.014522057957947254, acc: 0.9981651306152344)
[2025-02-13 04:21:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:48][root][INFO] - Training Epoch: 2/2, step 5329/7134 completed (loss: 0.028198692947626114, acc: 0.990275502204895)
[2025-02-13 04:21:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:48][root][INFO] - Training Epoch: 2/2, step 5330/7134 completed (loss: 0.022172383964061737, acc: 0.995312511920929)
[2025-02-13 04:21:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:49][root][INFO] - Training Epoch: 2/2, step 5331/7134 completed (loss: 0.026613393798470497, acc: 0.9912434220314026)
[2025-02-13 04:21:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:49][root][INFO] - Training Epoch: 2/2, step 5332/7134 completed (loss: 0.0315861850976944, acc: 0.995121955871582)
[2025-02-13 04:21:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:49][root][INFO] - Training Epoch: 2/2, step 5333/7134 completed (loss: 0.021547511219978333, acc: 0.9937694668769836)
[2025-02-13 04:21:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:50][root][INFO] - Training Epoch: 2/2, step 5334/7134 completed (loss: 0.04445524141192436, acc: 0.9938744306564331)
[2025-02-13 04:21:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:50][root][INFO] - Training Epoch: 2/2, step 5335/7134 completed (loss: 0.024659132584929466, acc: 0.9936102032661438)
[2025-02-13 04:21:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:51][root][INFO] - Training Epoch: 2/2, step 5336/7134 completed (loss: 0.02648184821009636, acc: 0.995312511920929)
[2025-02-13 04:21:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:51][root][INFO] - Training Epoch: 2/2, step 5337/7134 completed (loss: 0.015787186101078987, acc: 0.9944320917129517)
[2025-02-13 04:21:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:52][root][INFO] - Training Epoch: 2/2, step 5338/7134 completed (loss: 0.009378922171890736, acc: 0.9965753555297852)
[2025-02-13 04:21:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:52][root][INFO] - Training Epoch: 2/2, step 5339/7134 completed (loss: 0.02732253260910511, acc: 0.9896373152732849)
[2025-02-13 04:21:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:53][root][INFO] - Training Epoch: 2/2, step 5340/7134 completed (loss: 0.0500546470284462, acc: 0.984402060508728)
[2025-02-13 04:21:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:53][root][INFO] - Training Epoch: 2/2, step 5341/7134 completed (loss: 0.010289476253092289, acc: 0.9954802393913269)
[2025-02-13 04:21:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:54][root][INFO] - Training Epoch: 2/2, step 5342/7134 completed (loss: 0.03319420665502548, acc: 0.9880159497261047)
[2025-02-13 04:21:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:54][root][INFO] - Training Epoch: 2/2, step 5343/7134 completed (loss: 0.030333420261740685, acc: 0.9894459247589111)
[2025-02-13 04:21:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:54][root][INFO] - Training Epoch: 2/2, step 5344/7134 completed (loss: 0.024177592247724533, acc: 0.9936842322349548)
[2025-02-13 04:21:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:55][root][INFO] - Training Epoch: 2/2, step 5345/7134 completed (loss: 0.023631565272808075, acc: 0.9931034445762634)
[2025-02-13 04:21:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:55][root][INFO] - Training Epoch: 2/2, step 5346/7134 completed (loss: 0.05469198152422905, acc: 0.9884575009346008)
[2025-02-13 04:21:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:21:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:22:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:23:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:24:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:25:59][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.0443, device='cuda:0') eval_epoch_loss=tensor(0.0434, device='cuda:0') eval_epoch_acc=tensor(0.9887, device='cuda:0')
[2025-02-13 04:25:59][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-13 04:25:59][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-13 04:25:59][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme/asr_epoch_2_step_5347_loss_0.043375831097364426/model.pt
[2025-02-13 04:25:59][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme directory
[2025-02-13 04:25:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:00][root][INFO] - Training Epoch: 2/2, step 5347/7134 completed (loss: 0.018562806770205498, acc: 0.9933686852455139)
[2025-02-13 04:26:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:00][root][INFO] - Training Epoch: 2/2, step 5348/7134 completed (loss: 0.03130155801773071, acc: 0.9889196753501892)
[2025-02-13 04:26:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:00][root][INFO] - Training Epoch: 2/2, step 5349/7134 completed (loss: 0.048894863575696945, acc: 0.9870800971984863)
[2025-02-13 04:26:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:01][root][INFO] - Training Epoch: 2/2, step 5350/7134 completed (loss: 0.021514827385544777, acc: 0.9933949708938599)
[2025-02-13 04:26:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:01][root][INFO] - Training Epoch: 2/2, step 5351/7134 completed (loss: 0.032178815454244614, acc: 0.9913151264190674)
[2025-02-13 04:26:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:02][root][INFO] - Training Epoch: 2/2, step 5352/7134 completed (loss: 0.01826159656047821, acc: 0.9935317039489746)
[2025-02-13 04:26:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:02][root][INFO] - Training Epoch: 2/2, step 5353/7134 completed (loss: 0.009533067233860493, acc: 0.9966177940368652)
[2025-02-13 04:26:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:03][root][INFO] - Training Epoch: 2/2, step 5354/7134 completed (loss: 0.03853413462638855, acc: 0.9889867901802063)
[2025-02-13 04:26:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:03][root][INFO] - Training Epoch: 2/2, step 5355/7134 completed (loss: 0.0545131117105484, acc: 0.986997663974762)
[2025-02-13 04:26:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:04][root][INFO] - Training Epoch: 2/2, step 5356/7134 completed (loss: 0.033110033720731735, acc: 0.9900373816490173)
[2025-02-13 04:26:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:04][root][INFO] - Training Epoch: 2/2, step 5357/7134 completed (loss: 0.03185857832431793, acc: 0.9895470142364502)
[2025-02-13 04:26:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:04][root][INFO] - Training Epoch: 2/2, step 5358/7134 completed (loss: 0.038901541382074356, acc: 0.9903448224067688)
[2025-02-13 04:26:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:05][root][INFO] - Training Epoch: 2/2, step 5359/7134 completed (loss: 0.018743354827165604, acc: 0.9953380227088928)
[2025-02-13 04:26:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:05][root][INFO] - Training Epoch: 2/2, step 5360/7134 completed (loss: 0.009749283082783222, acc: 0.9972527623176575)
[2025-02-13 04:26:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:06][root][INFO] - Training Epoch: 2/2, step 5361/7134 completed (loss: 0.019037475809454918, acc: 0.9940119981765747)
[2025-02-13 04:26:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:06][root][INFO] - Training Epoch: 2/2, step 5362/7134 completed (loss: 0.017236964777112007, acc: 0.9940298795700073)
[2025-02-13 04:26:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:07][root][INFO] - Training Epoch: 2/2, step 5363/7134 completed (loss: 0.044296566396951675, acc: 0.9854651093482971)
[2025-02-13 04:26:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:07][root][INFO] - Training Epoch: 2/2, step 5364/7134 completed (loss: 0.015359693206846714, acc: 0.9935064911842346)
[2025-02-13 04:26:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:08][root][INFO] - Training Epoch: 2/2, step 5365/7134 completed (loss: 0.008881392888724804, acc: 0.9964726567268372)
[2025-02-13 04:26:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:08][root][INFO] - Training Epoch: 2/2, step 5366/7134 completed (loss: 0.03792036697268486, acc: 0.9913194179534912)
[2025-02-13 04:26:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:08][root][INFO] - Training Epoch: 2/2, step 5367/7134 completed (loss: 0.02772652544081211, acc: 0.9924623370170593)
[2025-02-13 04:26:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:09][root][INFO] - Training Epoch: 2/2, step 5368/7134 completed (loss: 0.022171566262841225, acc: 0.9933110475540161)
[2025-02-13 04:26:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:09][root][INFO] - Training Epoch: 2/2, step 5369/7134 completed (loss: 0.021054932847619057, acc: 0.993630588054657)
[2025-02-13 04:26:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:10][root][INFO] - Training Epoch: 2/2, step 5370/7134 completed (loss: 0.04762072116136551, acc: 0.9890109896659851)
[2025-02-13 04:26:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:10][root][INFO] - Training Epoch: 2/2, step 5371/7134 completed (loss: 0.0892496109008789, acc: 0.9800664186477661)
[2025-02-13 04:26:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:11][root][INFO] - Training Epoch: 2/2, step 5372/7134 completed (loss: 0.009790311567485332, acc: 0.9971949458122253)
[2025-02-13 04:26:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:11][root][INFO] - Training Epoch: 2/2, step 5373/7134 completed (loss: 0.04546896740794182, acc: 0.9895397424697876)
[2025-02-13 04:26:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:11][root][INFO] - Training Epoch: 2/2, step 5374/7134 completed (loss: 0.02543393149971962, acc: 0.9903537034988403)
[2025-02-13 04:26:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:12][root][INFO] - Training Epoch: 2/2, step 5375/7134 completed (loss: 0.02973363734781742, acc: 0.9865900278091431)
[2025-02-13 04:26:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:12][root][INFO] - Training Epoch: 2/2, step 5376/7134 completed (loss: 0.027211740612983704, acc: 0.9912891983985901)
[2025-02-13 04:26:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:13][root][INFO] - Training Epoch: 2/2, step 5377/7134 completed (loss: 0.028704393655061722, acc: 0.9939302206039429)
[2025-02-13 04:26:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:13][root][INFO] - Training Epoch: 2/2, step 5378/7134 completed (loss: 0.013863210566341877, acc: 0.9963592290878296)
[2025-02-13 04:26:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:13][root][INFO] - Training Epoch: 2/2, step 5379/7134 completed (loss: 0.026860132813453674, acc: 0.9948119521141052)
[2025-02-13 04:26:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:14][root][INFO] - Training Epoch: 2/2, step 5380/7134 completed (loss: 0.03909371793270111, acc: 0.9876543283462524)
[2025-02-13 04:26:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:14][root][INFO] - Training Epoch: 2/2, step 5381/7134 completed (loss: 0.016956033185124397, acc: 0.9958158731460571)
[2025-02-13 04:26:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:15][root][INFO] - Training Epoch: 2/2, step 5382/7134 completed (loss: 0.013263012282550335, acc: 0.9948805570602417)
[2025-02-13 04:26:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:15][root][INFO] - Training Epoch: 2/2, step 5383/7134 completed (loss: 0.014572788961231709, acc: 0.9924242496490479)
[2025-02-13 04:26:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:16][root][INFO] - Training Epoch: 2/2, step 5384/7134 completed (loss: 0.00938358623534441, acc: 0.9964454770088196)
[2025-02-13 04:26:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:16][root][INFO] - Training Epoch: 2/2, step 5385/7134 completed (loss: 0.0252543892711401, acc: 0.9933949708938599)
[2025-02-13 04:26:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:17][root][INFO] - Training Epoch: 2/2, step 5386/7134 completed (loss: 0.006754863075911999, acc: 1.0)
[2025-02-13 04:26:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:17][root][INFO] - Training Epoch: 2/2, step 5387/7134 completed (loss: 0.012633536010980606, acc: 0.9942857027053833)
[2025-02-13 04:26:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:17][root][INFO] - Training Epoch: 2/2, step 5388/7134 completed (loss: 0.015326314605772495, acc: 0.9962121248245239)
[2025-02-13 04:26:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:18][root][INFO] - Training Epoch: 2/2, step 5389/7134 completed (loss: 0.00904969871044159, acc: 0.9984025359153748)
[2025-02-13 04:26:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:18][root][INFO] - Training Epoch: 2/2, step 5390/7134 completed (loss: 0.025287464261054993, acc: 0.9936608672142029)
[2025-02-13 04:26:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:19][root][INFO] - Training Epoch: 2/2, step 5391/7134 completed (loss: 0.047345228493213654, acc: 0.9879931211471558)
[2025-02-13 04:26:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:19][root][INFO] - Training Epoch: 2/2, step 5392/7134 completed (loss: 0.026075351983308792, acc: 0.9862805008888245)
[2025-02-13 04:26:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:19][root][INFO] - Training Epoch: 2/2, step 5393/7134 completed (loss: 0.014970818534493446, acc: 0.994140625)
[2025-02-13 04:26:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:20][root][INFO] - Training Epoch: 2/2, step 5394/7134 completed (loss: 0.026186460629105568, acc: 0.9900793433189392)
[2025-02-13 04:26:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:20][root][INFO] - Training Epoch: 2/2, step 5395/7134 completed (loss: 0.009891970083117485, acc: 1.0)
[2025-02-13 04:26:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:21][root][INFO] - Training Epoch: 2/2, step 5396/7134 completed (loss: 0.029053431004285812, acc: 0.9939849376678467)
[2025-02-13 04:26:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:21][root][INFO] - Training Epoch: 2/2, step 5397/7134 completed (loss: 0.020565632730722427, acc: 0.9932318329811096)
[2025-02-13 04:26:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:22][root][INFO] - Training Epoch: 2/2, step 5398/7134 completed (loss: 0.01736375130712986, acc: 0.9955752491950989)
[2025-02-13 04:26:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:22][root][INFO] - Training Epoch: 2/2, step 5399/7134 completed (loss: 0.015583252534270287, acc: 0.9931600689888)
[2025-02-13 04:26:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:22][root][INFO] - Training Epoch: 2/2, step 5400/7134 completed (loss: 0.08067651838064194, acc: 0.9858934283256531)
[2025-02-13 04:26:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:23][root][INFO] - Training Epoch: 2/2, step 5401/7134 completed (loss: 0.02211417816579342, acc: 0.9919742941856384)
[2025-02-13 04:26:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:23][root][INFO] - Training Epoch: 2/2, step 5402/7134 completed (loss: 0.014088905416429043, acc: 0.9955947399139404)
[2025-02-13 04:26:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:24][root][INFO] - Training Epoch: 2/2, step 5403/7134 completed (loss: 0.03412298858165741, acc: 0.990212082862854)
[2025-02-13 04:26:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:24][root][INFO] - Training Epoch: 2/2, step 5404/7134 completed (loss: 0.04919496923685074, acc: 0.9879194498062134)
[2025-02-13 04:26:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:25][root][INFO] - Training Epoch: 2/2, step 5405/7134 completed (loss: 0.014428000897169113, acc: 0.9947643876075745)
[2025-02-13 04:26:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:25][root][INFO] - Training Epoch: 2/2, step 5406/7134 completed (loss: 0.010261845774948597, acc: 0.9980506896972656)
[2025-02-13 04:26:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:25][root][INFO] - Training Epoch: 2/2, step 5407/7134 completed (loss: 0.019677365198731422, acc: 0.9927536249160767)
[2025-02-13 04:26:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:26][root][INFO] - Training Epoch: 2/2, step 5408/7134 completed (loss: 0.05117499828338623, acc: 0.9921996593475342)
[2025-02-13 04:26:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:26][root][INFO] - Training Epoch: 2/2, step 5409/7134 completed (loss: 0.07153452932834625, acc: 0.9863842725753784)
[2025-02-13 04:26:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:27][root][INFO] - Training Epoch: 2/2, step 5410/7134 completed (loss: 0.036318883299827576, acc: 0.990813672542572)
[2025-02-13 04:26:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:27][root][INFO] - Training Epoch: 2/2, step 5411/7134 completed (loss: 0.04004824906587601, acc: 0.9912917017936707)
[2025-02-13 04:26:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:27][root][INFO] - Training Epoch: 2/2, step 5412/7134 completed (loss: 0.059974852949380875, acc: 0.9884615540504456)
[2025-02-13 04:26:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:28][root][INFO] - Training Epoch: 2/2, step 5413/7134 completed (loss: 0.042550019919872284, acc: 0.9857142567634583)
[2025-02-13 04:26:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:28][root][INFO] - Training Epoch: 2/2, step 5414/7134 completed (loss: 0.09966158121824265, acc: 0.9781491160392761)
[2025-02-13 04:26:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:29][root][INFO] - Training Epoch: 2/2, step 5415/7134 completed (loss: 0.04342915117740631, acc: 0.9871060252189636)
[2025-02-13 04:26:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:29][root][INFO] - Training Epoch: 2/2, step 5416/7134 completed (loss: 0.028870128095149994, acc: 0.9895470142364502)
[2025-02-13 04:26:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:30][root][INFO] - Training Epoch: 2/2, step 5417/7134 completed (loss: 0.029123535379767418, acc: 0.9900124669075012)
[2025-02-13 04:26:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:30][root][INFO] - Training Epoch: 2/2, step 5418/7134 completed (loss: 0.028944890946149826, acc: 0.9896602630615234)
[2025-02-13 04:26:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:31][root][INFO] - Training Epoch: 2/2, step 5419/7134 completed (loss: 0.07134570181369781, acc: 0.9728813767433167)
[2025-02-13 04:26:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:31][root][INFO] - Training Epoch: 2/2, step 5420/7134 completed (loss: 0.048015493899583817, acc: 0.9860140085220337)
[2025-02-13 04:26:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:31][root][INFO] - Training Epoch: 2/2, step 5421/7134 completed (loss: 0.041532304137945175, acc: 0.9906396269798279)
[2025-02-13 04:26:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:32][root][INFO] - Training Epoch: 2/2, step 5422/7134 completed (loss: 0.02490416169166565, acc: 0.9940828680992126)
[2025-02-13 04:26:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:32][root][INFO] - Training Epoch: 2/2, step 5423/7134 completed (loss: 0.020425718277692795, acc: 0.9941349029541016)
[2025-02-13 04:26:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:33][root][INFO] - Training Epoch: 2/2, step 5424/7134 completed (loss: 0.02867271564900875, acc: 0.9911971688270569)
[2025-02-13 04:26:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:33][root][INFO] - Training Epoch: 2/2, step 5425/7134 completed (loss: 0.024189693853259087, acc: 0.9961165189743042)
[2025-02-13 04:26:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:33][root][INFO] - Training Epoch: 2/2, step 5426/7134 completed (loss: 0.06559682637453079, acc: 0.9819276928901672)
[2025-02-13 04:26:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:34][root][INFO] - Training Epoch: 2/2, step 5427/7134 completed (loss: 0.009010112844407558, acc: 0.998420238494873)
[2025-02-13 04:26:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:34][root][INFO] - Training Epoch: 2/2, step 5428/7134 completed (loss: 0.031883202493190765, acc: 0.9846677780151367)
[2025-02-13 04:26:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:35][root][INFO] - Training Epoch: 2/2, step 5429/7134 completed (loss: 0.04938464239239693, acc: 0.9837545156478882)
[2025-02-13 04:26:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:35][root][INFO] - Training Epoch: 2/2, step 5430/7134 completed (loss: 0.027328502386808395, acc: 0.9937304258346558)
[2025-02-13 04:26:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:35][root][INFO] - Training Epoch: 2/2, step 5431/7134 completed (loss: 0.02402869425714016, acc: 0.992438554763794)
[2025-02-13 04:26:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:36][root][INFO] - Training Epoch: 2/2, step 5432/7134 completed (loss: 0.010618560947477818, acc: 0.9981167316436768)
[2025-02-13 04:26:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:36][root][INFO] - Training Epoch: 2/2, step 5433/7134 completed (loss: 0.035655029118061066, acc: 0.9939393997192383)
[2025-02-13 04:26:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:37][root][INFO] - Training Epoch: 2/2, step 5434/7134 completed (loss: 0.03201774135231972, acc: 0.9919517040252686)
[2025-02-13 04:26:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:37][root][INFO] - Training Epoch: 2/2, step 5435/7134 completed (loss: 0.02226274274289608, acc: 0.9937238693237305)
[2025-02-13 04:26:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:37][root][INFO] - Training Epoch: 2/2, step 5436/7134 completed (loss: 0.02456996776163578, acc: 0.9925816059112549)
[2025-02-13 04:26:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:38][root][INFO] - Training Epoch: 2/2, step 5437/7134 completed (loss: 0.0075088622979819775, acc: 1.0)
[2025-02-13 04:26:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:38][root][INFO] - Training Epoch: 2/2, step 5438/7134 completed (loss: 0.015344200655817986, acc: 0.9948320388793945)
[2025-02-13 04:26:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:39][root][INFO] - Training Epoch: 2/2, step 5439/7134 completed (loss: 0.03066933900117874, acc: 0.9898256063461304)
[2025-02-13 04:26:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:39][root][INFO] - Training Epoch: 2/2, step 5440/7134 completed (loss: 0.03593100234866142, acc: 0.9936143159866333)
[2025-02-13 04:26:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:40][root][INFO] - Training Epoch: 2/2, step 5441/7134 completed (loss: 0.016287073493003845, acc: 0.9951040148735046)
[2025-02-13 04:26:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:40][root][INFO] - Training Epoch: 2/2, step 5442/7134 completed (loss: 0.018309738487005234, acc: 0.9932340979576111)
[2025-02-13 04:26:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:40][root][INFO] - Training Epoch: 2/2, step 5443/7134 completed (loss: 0.023286299780011177, acc: 0.9928977489471436)
[2025-02-13 04:26:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:41][root][INFO] - Training Epoch: 2/2, step 5444/7134 completed (loss: 0.009160300716757774, acc: 0.9981752038002014)
[2025-02-13 04:26:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:41][root][INFO] - Training Epoch: 2/2, step 5445/7134 completed (loss: 0.025962959975004196, acc: 0.9945155382156372)
[2025-02-13 04:26:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:42][root][INFO] - Training Epoch: 2/2, step 5446/7134 completed (loss: 0.015792740508913994, acc: 0.9957982897758484)
[2025-02-13 04:26:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:42][root][INFO] - Training Epoch: 2/2, step 5447/7134 completed (loss: 0.02417852357029915, acc: 0.9935587644577026)
[2025-02-13 04:26:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:43][root][INFO] - Training Epoch: 2/2, step 5448/7134 completed (loss: 0.04222146049141884, acc: 0.9874301552772522)
[2025-02-13 04:26:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:43][root][INFO] - Training Epoch: 2/2, step 5449/7134 completed (loss: 0.015660738572478294, acc: 0.9964912533760071)
[2025-02-13 04:26:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:43][root][INFO] - Training Epoch: 2/2, step 5450/7134 completed (loss: 0.01934039779007435, acc: 0.992277979850769)
[2025-02-13 04:26:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:44][root][INFO] - Training Epoch: 2/2, step 5451/7134 completed (loss: 0.024614334106445312, acc: 0.9908257126808167)
[2025-02-13 04:26:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:44][root][INFO] - Training Epoch: 2/2, step 5452/7134 completed (loss: 0.037183478474617004, acc: 0.9909560680389404)
[2025-02-13 04:26:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:45][root][INFO] - Training Epoch: 2/2, step 5453/7134 completed (loss: 0.034310538321733475, acc: 0.9926380515098572)
[2025-02-13 04:26:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:45][root][INFO] - Training Epoch: 2/2, step 5454/7134 completed (loss: 0.01961599476635456, acc: 0.9947848916053772)
[2025-02-13 04:26:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:46][root][INFO] - Training Epoch: 2/2, step 5455/7134 completed (loss: 0.0505884550511837, acc: 0.9911616444587708)
[2025-02-13 04:26:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:46][root][INFO] - Training Epoch: 2/2, step 5456/7134 completed (loss: 0.03072592429816723, acc: 0.992443323135376)
[2025-02-13 04:26:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:47][root][INFO] - Training Epoch: 2/2, step 5457/7134 completed (loss: 0.046023476868867874, acc: 0.991946280002594)
[2025-02-13 04:26:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:47][root][INFO] - Training Epoch: 2/2, step 5458/7134 completed (loss: 0.023804333060979843, acc: 0.993630588054657)
[2025-02-13 04:26:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:48][root][INFO] - Training Epoch: 2/2, step 5459/7134 completed (loss: 0.006515824235975742, acc: 0.997019350528717)
[2025-02-13 04:26:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:48][root][INFO] - Training Epoch: 2/2, step 5460/7134 completed (loss: 0.021976308897137642, acc: 0.9944367408752441)
[2025-02-13 04:26:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:48][root][INFO] - Training Epoch: 2/2, step 5461/7134 completed (loss: 0.036855846643447876, acc: 0.9911699891090393)
[2025-02-13 04:26:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:49][root][INFO] - Training Epoch: 2/2, step 5462/7134 completed (loss: 0.006008912809193134, acc: 0.9985693693161011)
[2025-02-13 04:26:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:49][root][INFO] - Training Epoch: 2/2, step 5463/7134 completed (loss: 0.015292515978217125, acc: 0.9984732866287231)
[2025-02-13 04:26:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:50][root][INFO] - Training Epoch: 2/2, step 5464/7134 completed (loss: 0.023254524916410446, acc: 0.9933949708938599)
[2025-02-13 04:26:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:50][root][INFO] - Training Epoch: 2/2, step 5465/7134 completed (loss: 0.04842900112271309, acc: 0.9925261735916138)
[2025-02-13 04:26:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:51][root][INFO] - Training Epoch: 2/2, step 5466/7134 completed (loss: 0.01552675198763609, acc: 0.994246244430542)
[2025-02-13 04:26:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:51][root][INFO] - Training Epoch: 2/2, step 5467/7134 completed (loss: 0.016260582953691483, acc: 0.9953051805496216)
[2025-02-13 04:26:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:52][root][INFO] - Training Epoch: 2/2, step 5468/7134 completed (loss: 0.02581295743584633, acc: 0.9930394291877747)
[2025-02-13 04:26:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:52][root][INFO] - Training Epoch: 2/2, step 5469/7134 completed (loss: 0.010912490077316761, acc: 0.9950920343399048)
[2025-02-13 04:26:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:53][root][INFO] - Training Epoch: 2/2, step 5470/7134 completed (loss: 0.006878832820802927, acc: 0.9963855147361755)
[2025-02-13 04:26:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:53][root][INFO] - Training Epoch: 2/2, step 5471/7134 completed (loss: 0.008210772648453712, acc: 0.9986486434936523)
[2025-02-13 04:26:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:53][root][INFO] - Training Epoch: 2/2, step 5472/7134 completed (loss: 0.018259624019265175, acc: 0.996259331703186)
[2025-02-13 04:26:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:54][root][INFO] - Training Epoch: 2/2, step 5473/7134 completed (loss: 0.009240509942173958, acc: 0.9975460171699524)
[2025-02-13 04:26:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:54][root][INFO] - Training Epoch: 2/2, step 5474/7134 completed (loss: 0.012426932342350483, acc: 0.9971428513526917)
[2025-02-13 04:26:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:55][root][INFO] - Training Epoch: 2/2, step 5475/7134 completed (loss: 0.017887340858578682, acc: 0.9943100810050964)
[2025-02-13 04:26:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:55][root][INFO] - Training Epoch: 2/2, step 5476/7134 completed (loss: 0.0035053722094744444, acc: 0.9985527992248535)
[2025-02-13 04:26:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:56][root][INFO] - Training Epoch: 2/2, step 5477/7134 completed (loss: 0.008664391934871674, acc: 0.9975369572639465)
[2025-02-13 04:26:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:56][root][INFO] - Training Epoch: 2/2, step 5478/7134 completed (loss: 0.04102115333080292, acc: 0.9845626354217529)
[2025-02-13 04:26:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:57][root][INFO] - Training Epoch: 2/2, step 5479/7134 completed (loss: 0.02717878296971321, acc: 0.99048912525177)
[2025-02-13 04:26:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:57][root][INFO] - Training Epoch: 2/2, step 5480/7134 completed (loss: 0.03899016231298447, acc: 0.99210524559021)
[2025-02-13 04:26:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:58][root][INFO] - Training Epoch: 2/2, step 5481/7134 completed (loss: 0.018104735761880875, acc: 0.9939393997192383)
[2025-02-13 04:26:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:58][root][INFO] - Training Epoch: 2/2, step 5482/7134 completed (loss: 0.026505203917622566, acc: 0.9915356636047363)
[2025-02-13 04:26:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:59][root][INFO] - Training Epoch: 2/2, step 5483/7134 completed (loss: 0.027475014328956604, acc: 0.9916434288024902)
[2025-02-13 04:26:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:26:59][root][INFO] - Training Epoch: 2/2, step 5484/7134 completed (loss: 0.04507198557257652, acc: 0.9833101630210876)
[2025-02-13 04:26:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:00][root][INFO] - Training Epoch: 2/2, step 5485/7134 completed (loss: 0.03431094065308571, acc: 0.9891107082366943)
[2025-02-13 04:27:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:00][root][INFO] - Training Epoch: 2/2, step 5486/7134 completed (loss: 0.03734306991100311, acc: 0.9861591458320618)
[2025-02-13 04:27:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:01][root][INFO] - Training Epoch: 2/2, step 5487/7134 completed (loss: 0.02202216535806656, acc: 0.9934959411621094)
[2025-02-13 04:27:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:01][root][INFO] - Training Epoch: 2/2, step 5488/7134 completed (loss: 0.05541088059544563, acc: 0.9855263233184814)
[2025-02-13 04:27:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:02][root][INFO] - Training Epoch: 2/2, step 5489/7134 completed (loss: 0.03149568289518356, acc: 0.99071204662323)
[2025-02-13 04:27:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:02][root][INFO] - Training Epoch: 2/2, step 5490/7134 completed (loss: 0.01654251664876938, acc: 0.9931662678718567)
[2025-02-13 04:27:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:02][root][INFO] - Training Epoch: 2/2, step 5491/7134 completed (loss: 0.012511464767158031, acc: 0.9956011772155762)
[2025-02-13 04:27:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:03][root][INFO] - Training Epoch: 2/2, step 5492/7134 completed (loss: 0.026607507839798927, acc: 0.9915397763252258)
[2025-02-13 04:27:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:03][root][INFO] - Training Epoch: 2/2, step 5493/7134 completed (loss: 0.02629830315709114, acc: 0.989130437374115)
[2025-02-13 04:27:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:04][root][INFO] - Training Epoch: 2/2, step 5494/7134 completed (loss: 0.015926942229270935, acc: 0.9957325458526611)
[2025-02-13 04:27:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:04][root][INFO] - Training Epoch: 2/2, step 5495/7134 completed (loss: 0.016116272658109665, acc: 0.9964157938957214)
[2025-02-13 04:27:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:04][root][INFO] - Training Epoch: 2/2, step 5496/7134 completed (loss: 0.02929774671792984, acc: 0.9883268475532532)
[2025-02-13 04:27:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:05][root][INFO] - Training Epoch: 2/2, step 5497/7134 completed (loss: 0.05244128033518791, acc: 0.9868228435516357)
[2025-02-13 04:27:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:05][root][INFO] - Training Epoch: 2/2, step 5498/7134 completed (loss: 0.017345262691378593, acc: 0.9933686852455139)
[2025-02-13 04:27:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:06][root][INFO] - Training Epoch: 2/2, step 5499/7134 completed (loss: 0.0332021601498127, acc: 0.9924623370170593)
[2025-02-13 04:27:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:06][root][INFO] - Training Epoch: 2/2, step 5500/7134 completed (loss: 0.03440199792385101, acc: 0.9877216815948486)
[2025-02-13 04:27:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:07][root][INFO] - Training Epoch: 2/2, step 5501/7134 completed (loss: 0.011308081448078156, acc: 0.9975308775901794)
[2025-02-13 04:27:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:07][root][INFO] - Training Epoch: 2/2, step 5502/7134 completed (loss: 0.019207516685128212, acc: 0.9919028282165527)
[2025-02-13 04:27:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:07][root][INFO] - Training Epoch: 2/2, step 5503/7134 completed (loss: 0.03457717224955559, acc: 0.9920477271080017)
[2025-02-13 04:27:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:08][root][INFO] - Training Epoch: 2/2, step 5504/7134 completed (loss: 0.012729114852845669, acc: 1.0)
[2025-02-13 04:27:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:08][root][INFO] - Training Epoch: 2/2, step 5505/7134 completed (loss: 0.023112226277589798, acc: 0.9914893507957458)
[2025-02-13 04:27:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:09][root][INFO] - Training Epoch: 2/2, step 5506/7134 completed (loss: 0.021340386942029, acc: 0.9907727837562561)
[2025-02-13 04:27:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:09][root][INFO] - Training Epoch: 2/2, step 5507/7134 completed (loss: 0.033758047968149185, acc: 0.9887955188751221)
[2025-02-13 04:27:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:10][root][INFO] - Training Epoch: 2/2, step 5508/7134 completed (loss: 0.022818971425294876, acc: 0.9941588640213013)
[2025-02-13 04:27:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:10][root][INFO] - Training Epoch: 2/2, step 5509/7134 completed (loss: 0.026600297540426254, acc: 0.9953216314315796)
[2025-02-13 04:27:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:11][root][INFO] - Training Epoch: 2/2, step 5510/7134 completed (loss: 0.03948675096035004, acc: 0.9871465563774109)
[2025-02-13 04:27:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:11][root][INFO] - Training Epoch: 2/2, step 5511/7134 completed (loss: 0.031168373301625252, acc: 0.9909502267837524)
[2025-02-13 04:27:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:12][root][INFO] - Training Epoch: 2/2, step 5512/7134 completed (loss: 0.010256025940179825, acc: 0.9960474371910095)
[2025-02-13 04:27:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:12][root][INFO] - Training Epoch: 2/2, step 5513/7134 completed (loss: 0.02302374318242073, acc: 0.989130437374115)
[2025-02-13 04:27:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:13][root][INFO] - Training Epoch: 2/2, step 5514/7134 completed (loss: 0.0674133375287056, acc: 0.9788557291030884)
[2025-02-13 04:27:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:13][root][INFO] - Training Epoch: 2/2, step 5515/7134 completed (loss: 0.03362775221467018, acc: 0.9885877370834351)
[2025-02-13 04:27:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:13][root][INFO] - Training Epoch: 2/2, step 5516/7134 completed (loss: 0.0294095017015934, acc: 0.997187077999115)
[2025-02-13 04:27:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:14][root][INFO] - Training Epoch: 2/2, step 5517/7134 completed (loss: 0.032284557819366455, acc: 0.9880319237709045)
[2025-02-13 04:27:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:14][root][INFO] - Training Epoch: 2/2, step 5518/7134 completed (loss: 0.03280371055006981, acc: 0.99048912525177)
[2025-02-13 04:27:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:15][root][INFO] - Training Epoch: 2/2, step 5519/7134 completed (loss: 0.007913295179605484, acc: 0.9958620667457581)
[2025-02-13 04:27:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:15][root][INFO] - Training Epoch: 2/2, step 5520/7134 completed (loss: 0.015300200320780277, acc: 0.9954699873924255)
[2025-02-13 04:27:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:16][root][INFO] - Training Epoch: 2/2, step 5521/7134 completed (loss: 0.036585334688425064, acc: 0.9874371886253357)
[2025-02-13 04:27:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:16][root][INFO] - Training Epoch: 2/2, step 5522/7134 completed (loss: 0.037971191108226776, acc: 0.9882352948188782)
[2025-02-13 04:27:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:16][root][INFO] - Training Epoch: 2/2, step 5523/7134 completed (loss: 0.03439723327755928, acc: 0.9943820238113403)
[2025-02-13 04:27:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:17][root][INFO] - Training Epoch: 2/2, step 5524/7134 completed (loss: 0.0365961529314518, acc: 0.9922600388526917)
[2025-02-13 04:27:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:17][root][INFO] - Training Epoch: 2/2, step 5525/7134 completed (loss: 0.018383989110589027, acc: 0.9941434860229492)
[2025-02-13 04:27:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:18][root][INFO] - Training Epoch: 2/2, step 5526/7134 completed (loss: 0.025005897507071495, acc: 0.9911634922027588)
[2025-02-13 04:27:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:18][root][INFO] - Training Epoch: 2/2, step 5527/7134 completed (loss: 0.024540051817893982, acc: 0.9932773113250732)
[2025-02-13 04:27:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:19][root][INFO] - Training Epoch: 2/2, step 5528/7134 completed (loss: 0.024248024448752403, acc: 0.9928994178771973)
[2025-02-13 04:27:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:19][root][INFO] - Training Epoch: 2/2, step 5529/7134 completed (loss: 0.04142153263092041, acc: 0.9858155846595764)
[2025-02-13 04:27:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:20][root][INFO] - Training Epoch: 2/2, step 5530/7134 completed (loss: 0.019585132598876953, acc: 0.997019350528717)
[2025-02-13 04:27:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:20][root][INFO] - Training Epoch: 2/2, step 5531/7134 completed (loss: 0.03789973631501198, acc: 0.9850560426712036)
[2025-02-13 04:27:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:20][root][INFO] - Training Epoch: 2/2, step 5532/7134 completed (loss: 0.043242983520030975, acc: 0.983930766582489)
[2025-02-13 04:27:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:21][root][INFO] - Training Epoch: 2/2, step 5533/7134 completed (loss: 0.0033971762750297785, acc: 1.0)
[2025-02-13 04:27:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:21][root][INFO] - Training Epoch: 2/2, step 5534/7134 completed (loss: 0.026603825390338898, acc: 0.9929906725883484)
[2025-02-13 04:27:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:22][root][INFO] - Training Epoch: 2/2, step 5535/7134 completed (loss: 0.026532787829637527, acc: 0.9902067184448242)
[2025-02-13 04:27:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:22][root][INFO] - Training Epoch: 2/2, step 5536/7134 completed (loss: 0.016795242205262184, acc: 0.9941860437393188)
[2025-02-13 04:27:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:23][root][INFO] - Training Epoch: 2/2, step 5537/7134 completed (loss: 0.0314653106033802, acc: 0.9918319582939148)
[2025-02-13 04:27:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:23][root][INFO] - Training Epoch: 2/2, step 5538/7134 completed (loss: 0.017169935628771782, acc: 0.9931740760803223)
[2025-02-13 04:27:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:24][root][INFO] - Training Epoch: 2/2, step 5539/7134 completed (loss: 0.026063572615385056, acc: 0.9921171069145203)
[2025-02-13 04:27:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:24][root][INFO] - Training Epoch: 2/2, step 5540/7134 completed (loss: 0.034982968121767044, acc: 0.99210524559021)
[2025-02-13 04:27:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:25][root][INFO] - Training Epoch: 2/2, step 5541/7134 completed (loss: 0.01825883612036705, acc: 0.9946808218955994)
[2025-02-13 04:27:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:25][root][INFO] - Training Epoch: 2/2, step 5542/7134 completed (loss: 0.019208524376153946, acc: 0.995720386505127)
[2025-02-13 04:27:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:26][root][INFO] - Training Epoch: 2/2, step 5543/7134 completed (loss: 0.019054735079407692, acc: 0.9977827072143555)
[2025-02-13 04:27:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:26][root][INFO] - Training Epoch: 2/2, step 5544/7134 completed (loss: 0.016012122854590416, acc: 0.9935483932495117)
[2025-02-13 04:27:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:27][root][INFO] - Training Epoch: 2/2, step 5545/7134 completed (loss: 0.01452564261853695, acc: 0.9978141784667969)
[2025-02-13 04:27:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:27][root][INFO] - Training Epoch: 2/2, step 5546/7134 completed (loss: 0.030832286924123764, acc: 0.9955703020095825)
[2025-02-13 04:27:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:27][root][INFO] - Training Epoch: 2/2, step 5547/7134 completed (loss: 0.01411029975861311, acc: 0.9965437650680542)
[2025-02-13 04:27:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:28][root][INFO] - Training Epoch: 2/2, step 5548/7134 completed (loss: 0.022883988916873932, acc: 0.9930232763290405)
[2025-02-13 04:27:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:28][root][INFO] - Training Epoch: 2/2, step 5549/7134 completed (loss: 0.027822671458125114, acc: 0.9900000095367432)
[2025-02-13 04:27:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:29][root][INFO] - Training Epoch: 2/2, step 5550/7134 completed (loss: 0.0104218740016222, acc: 0.9964747428894043)
[2025-02-13 04:27:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:29][root][INFO] - Training Epoch: 2/2, step 5551/7134 completed (loss: 0.05386878177523613, acc: 0.9891745448112488)
[2025-02-13 04:27:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:30][root][INFO] - Training Epoch: 2/2, step 5552/7134 completed (loss: 0.008424141444265842, acc: 0.9976218938827515)
[2025-02-13 04:27:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:30][root][INFO] - Training Epoch: 2/2, step 5553/7134 completed (loss: 0.03426262363791466, acc: 0.993819534778595)
[2025-02-13 04:27:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:31][root][INFO] - Training Epoch: 2/2, step 5554/7134 completed (loss: 0.01229294203221798, acc: 0.9963325262069702)
[2025-02-13 04:27:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:31][root][INFO] - Training Epoch: 2/2, step 5555/7134 completed (loss: 0.010041660629212856, acc: 0.9973261952400208)
[2025-02-13 04:27:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:31][root][INFO] - Training Epoch: 2/2, step 5556/7134 completed (loss: 0.01912822015583515, acc: 0.990920901298523)
[2025-02-13 04:27:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:32][root][INFO] - Training Epoch: 2/2, step 5557/7134 completed (loss: 0.018932614475488663, acc: 0.9971988797187805)
[2025-02-13 04:27:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:32][root][INFO] - Training Epoch: 2/2, step 5558/7134 completed (loss: 0.019040795043110847, acc: 0.991304337978363)
[2025-02-13 04:27:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:33][root][INFO] - Training Epoch: 2/2, step 5559/7134 completed (loss: 0.02620868571102619, acc: 0.9950310587882996)
[2025-02-13 04:27:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:33][root][INFO] - Training Epoch: 2/2, step 5560/7134 completed (loss: 0.027738427743315697, acc: 0.9922279715538025)
[2025-02-13 04:27:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:34][root][INFO] - Training Epoch: 2/2, step 5561/7134 completed (loss: 0.027792951092123985, acc: 0.9943289160728455)
[2025-02-13 04:27:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:34][root][INFO] - Training Epoch: 2/2, step 5562/7134 completed (loss: 0.029970230534672737, acc: 0.9931740760803223)
[2025-02-13 04:27:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:35][root][INFO] - Training Epoch: 2/2, step 5563/7134 completed (loss: 0.03437303379178047, acc: 0.9969969987869263)
[2025-02-13 04:27:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:35][root][INFO] - Training Epoch: 2/2, step 5564/7134 completed (loss: 0.019615529105067253, acc: 0.9958158731460571)
[2025-02-13 04:27:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:35][root][INFO] - Training Epoch: 2/2, step 5565/7134 completed (loss: 0.00592157244682312, acc: 0.9984639286994934)
[2025-02-13 04:27:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:36][root][INFO] - Training Epoch: 2/2, step 5566/7134 completed (loss: 0.009963486343622208, acc: 0.9988518953323364)
[2025-02-13 04:27:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:36][root][INFO] - Training Epoch: 2/2, step 5567/7134 completed (loss: 0.00661091785877943, acc: 0.9977169036865234)
[2025-02-13 04:27:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:37][root][INFO] - Training Epoch: 2/2, step 5568/7134 completed (loss: 0.017401032149791718, acc: 0.9961340427398682)
[2025-02-13 04:27:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:37][root][INFO] - Training Epoch: 2/2, step 5569/7134 completed (loss: 0.006757301278412342, acc: 0.9977169036865234)
[2025-02-13 04:27:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:38][root][INFO] - Training Epoch: 2/2, step 5570/7134 completed (loss: 0.01670396514236927, acc: 0.9937888383865356)
[2025-02-13 04:27:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:38][root][INFO] - Training Epoch: 2/2, step 5571/7134 completed (loss: 0.029109416529536247, acc: 0.98959881067276)
[2025-02-13 04:27:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:39][root][INFO] - Training Epoch: 2/2, step 5572/7134 completed (loss: 0.04089843109250069, acc: 0.9917582273483276)
[2025-02-13 04:27:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:39][root][INFO] - Training Epoch: 2/2, step 5573/7134 completed (loss: 0.026777613908052444, acc: 0.9941775798797607)
[2025-02-13 04:27:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:39][root][INFO] - Training Epoch: 2/2, step 5574/7134 completed (loss: 0.004891353193670511, acc: 0.9984126687049866)
[2025-02-13 04:27:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:40][root][INFO] - Training Epoch: 2/2, step 5575/7134 completed (loss: 0.016726380214095116, acc: 0.9952152967453003)
[2025-02-13 04:27:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:40][root][INFO] - Training Epoch: 2/2, step 5576/7134 completed (loss: 0.013389513827860355, acc: 0.9957020282745361)
[2025-02-13 04:27:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:41][root][INFO] - Training Epoch: 2/2, step 5577/7134 completed (loss: 0.02129238098859787, acc: 0.9930192232131958)
[2025-02-13 04:27:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:41][root][INFO] - Training Epoch: 2/2, step 5578/7134 completed (loss: 0.009718429297208786, acc: 0.9956958293914795)
[2025-02-13 04:27:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:42][root][INFO] - Training Epoch: 2/2, step 5579/7134 completed (loss: 0.020115746185183525, acc: 0.9917012453079224)
[2025-02-13 04:27:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:42][root][INFO] - Training Epoch: 2/2, step 5580/7134 completed (loss: 0.006817578338086605, acc: 0.9972714781761169)
[2025-02-13 04:27:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:43][root][INFO] - Training Epoch: 2/2, step 5581/7134 completed (loss: 0.01142953708767891, acc: 0.9946808218955994)
[2025-02-13 04:27:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:43][root][INFO] - Training Epoch: 2/2, step 5582/7134 completed (loss: 0.011760269291698933, acc: 0.9973924160003662)
[2025-02-13 04:27:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:43][root][INFO] - Training Epoch: 2/2, step 5583/7134 completed (loss: 0.016739914193749428, acc: 0.996221661567688)
[2025-02-13 04:27:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:44][root][INFO] - Training Epoch: 2/2, step 5584/7134 completed (loss: 0.008793416433036327, acc: 0.9985207319259644)
[2025-02-13 04:27:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:44][root][INFO] - Training Epoch: 2/2, step 5585/7134 completed (loss: 0.026634186506271362, acc: 0.9951515197753906)
[2025-02-13 04:27:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:45][root][INFO] - Training Epoch: 2/2, step 5586/7134 completed (loss: 0.007656414993107319, acc: 0.9982237815856934)
[2025-02-13 04:27:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:45][root][INFO] - Training Epoch: 2/2, step 5587/7134 completed (loss: 0.01638117991387844, acc: 0.9958391189575195)
[2025-02-13 04:27:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:46][root][INFO] - Training Epoch: 2/2, step 5588/7134 completed (loss: 0.005999612621963024, acc: 0.9969742894172668)
[2025-02-13 04:27:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:46][root][INFO] - Training Epoch: 2/2, step 5589/7134 completed (loss: 0.02154647931456566, acc: 0.9952681660652161)
[2025-02-13 04:27:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:47][root][INFO] - Training Epoch: 2/2, step 5590/7134 completed (loss: 0.025002097710967064, acc: 0.9929577708244324)
[2025-02-13 04:27:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:47][root][INFO] - Training Epoch: 2/2, step 5591/7134 completed (loss: 0.007953496649861336, acc: 0.9973822236061096)
[2025-02-13 04:27:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:47][root][INFO] - Training Epoch: 2/2, step 5592/7134 completed (loss: 0.011468839831650257, acc: 0.9972565174102783)
[2025-02-13 04:27:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:48][root][INFO] - Training Epoch: 2/2, step 5593/7134 completed (loss: 0.013066125102341175, acc: 0.9961140155792236)
[2025-02-13 04:27:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:48][root][INFO] - Training Epoch: 2/2, step 5594/7134 completed (loss: 0.005729258991777897, acc: 0.9986522793769836)
[2025-02-13 04:27:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:49][root][INFO] - Training Epoch: 2/2, step 5595/7134 completed (loss: 0.016239367425441742, acc: 0.9946380853652954)
[2025-02-13 04:27:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:49][root][INFO] - Training Epoch: 2/2, step 5596/7134 completed (loss: 0.006799678783863783, acc: 0.9986559152603149)
[2025-02-13 04:27:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:50][root][INFO] - Training Epoch: 2/2, step 5597/7134 completed (loss: 0.015202688053250313, acc: 0.997245192527771)
[2025-02-13 04:27:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:50][root][INFO] - Training Epoch: 2/2, step 5598/7134 completed (loss: 0.011817931197583675, acc: 0.994413435459137)
[2025-02-13 04:27:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:51][root][INFO] - Training Epoch: 2/2, step 5599/7134 completed (loss: 0.05138413980603218, acc: 0.9890710115432739)
[2025-02-13 04:27:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:51][root][INFO] - Training Epoch: 2/2, step 5600/7134 completed (loss: 0.008850229904055595, acc: 0.9971346855163574)
[2025-02-13 04:27:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:51][root][INFO] - Training Epoch: 2/2, step 5601/7134 completed (loss: 0.017173876985907555, acc: 0.991304337978363)
[2025-02-13 04:27:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:52][root][INFO] - Training Epoch: 2/2, step 5602/7134 completed (loss: 0.026873694732785225, acc: 0.988727867603302)
[2025-02-13 04:27:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:52][root][INFO] - Training Epoch: 2/2, step 5603/7134 completed (loss: 0.0041572232730686665, acc: 0.9983633160591125)
[2025-02-13 04:27:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:53][root][INFO] - Training Epoch: 2/2, step 5604/7134 completed (loss: 0.03866439312696457, acc: 0.9903537034988403)
[2025-02-13 04:27:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:53][root][INFO] - Training Epoch: 2/2, step 5605/7134 completed (loss: 0.017244357615709305, acc: 0.9915110468864441)
[2025-02-13 04:27:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:54][root][INFO] - Training Epoch: 2/2, step 5606/7134 completed (loss: 0.0457705557346344, acc: 0.9871794581413269)
[2025-02-13 04:27:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:54][root][INFO] - Training Epoch: 2/2, step 5607/7134 completed (loss: 0.015026462264358997, acc: 0.9939849376678467)
[2025-02-13 04:27:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:54][root][INFO] - Training Epoch: 2/2, step 5608/7134 completed (loss: 0.01816713623702526, acc: 0.9934318661689758)
[2025-02-13 04:27:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:55][root][INFO] - Training Epoch: 2/2, step 5609/7134 completed (loss: 0.014660529792308807, acc: 0.9947826266288757)
[2025-02-13 04:27:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:55][root][INFO] - Training Epoch: 2/2, step 5610/7134 completed (loss: 0.012347179464995861, acc: 0.9939637780189514)
[2025-02-13 04:27:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:56][root][INFO] - Training Epoch: 2/2, step 5611/7134 completed (loss: 0.024264968931674957, acc: 0.9936708807945251)
[2025-02-13 04:27:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:56][root][INFO] - Training Epoch: 2/2, step 5612/7134 completed (loss: 0.03184443712234497, acc: 0.9907975196838379)
[2025-02-13 04:27:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:57][root][INFO] - Training Epoch: 2/2, step 5613/7134 completed (loss: 0.03082115761935711, acc: 0.991055428981781)
[2025-02-13 04:27:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:57][root][INFO] - Training Epoch: 2/2, step 5614/7134 completed (loss: 0.04927000403404236, acc: 0.9842857122421265)
[2025-02-13 04:27:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:58][root][INFO] - Training Epoch: 2/2, step 5615/7134 completed (loss: 0.030884213745594025, acc: 0.984674334526062)
[2025-02-13 04:27:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:58][root][INFO] - Training Epoch: 2/2, step 5616/7134 completed (loss: 0.014904937706887722, acc: 0.9970282316207886)
[2025-02-13 04:27:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:58][root][INFO] - Training Epoch: 2/2, step 5617/7134 completed (loss: 0.008807369507849216, acc: 0.9967532753944397)
[2025-02-13 04:27:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:59][root][INFO] - Training Epoch: 2/2, step 5618/7134 completed (loss: 0.04142745956778526, acc: 0.9886178970336914)
[2025-02-13 04:27:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:27:59][root][INFO] - Training Epoch: 2/2, step 5619/7134 completed (loss: 0.03510678559541702, acc: 0.9878296256065369)
[2025-02-13 04:27:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:00][root][INFO] - Training Epoch: 2/2, step 5620/7134 completed (loss: 0.03009558655321598, acc: 0.9906542301177979)
[2025-02-13 04:28:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:00][root][INFO] - Training Epoch: 2/2, step 5621/7134 completed (loss: 0.020448319613933563, acc: 0.9911054372787476)
[2025-02-13 04:28:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:01][root][INFO] - Training Epoch: 2/2, step 5622/7134 completed (loss: 0.03346242755651474, acc: 0.9931880235671997)
[2025-02-13 04:28:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:01][root][INFO] - Training Epoch: 2/2, step 5623/7134 completed (loss: 0.017711061984300613, acc: 0.99303138256073)
[2025-02-13 04:28:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:01][root][INFO] - Training Epoch: 2/2, step 5624/7134 completed (loss: 0.018441827967762947, acc: 0.9918166995048523)
[2025-02-13 04:28:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:02][root][INFO] - Training Epoch: 2/2, step 5625/7134 completed (loss: 0.006766424048691988, acc: 1.0)
[2025-02-13 04:28:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:02][root][INFO] - Training Epoch: 2/2, step 5626/7134 completed (loss: 0.009892460890114307, acc: 0.9966158866882324)
[2025-02-13 04:28:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:03][root][INFO] - Training Epoch: 2/2, step 5627/7134 completed (loss: 0.07250740379095078, acc: 0.9814471006393433)
[2025-02-13 04:28:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:03][root][INFO] - Training Epoch: 2/2, step 5628/7134 completed (loss: 0.022338565438985825, acc: 0.9908925294876099)
[2025-02-13 04:28:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:04][root][INFO] - Training Epoch: 2/2, step 5629/7134 completed (loss: 0.02142142504453659, acc: 0.9876977205276489)
[2025-02-13 04:28:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:04][root][INFO] - Training Epoch: 2/2, step 5630/7134 completed (loss: 0.04182330146431923, acc: 0.9917864203453064)
[2025-02-13 04:28:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:05][root][INFO] - Training Epoch: 2/2, step 5631/7134 completed (loss: 0.03691738098859787, acc: 0.9917080998420715)
[2025-02-13 04:28:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:05][root][INFO] - Training Epoch: 2/2, step 5632/7134 completed (loss: 0.02417483553290367, acc: 0.996370255947113)
[2025-02-13 04:28:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:05][root][INFO] - Training Epoch: 2/2, step 5633/7134 completed (loss: 0.022082310169935226, acc: 0.9918566942214966)
[2025-02-13 04:28:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:06][root][INFO] - Training Epoch: 2/2, step 5634/7134 completed (loss: 0.025959087535738945, acc: 0.9895287752151489)
[2025-02-13 04:28:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:06][root][INFO] - Training Epoch: 2/2, step 5635/7134 completed (loss: 0.015607668086886406, acc: 0.9949324131011963)
[2025-02-13 04:28:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:07][root][INFO] - Training Epoch: 2/2, step 5636/7134 completed (loss: 0.013688122853636742, acc: 0.995207667350769)
[2025-02-13 04:28:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:07][root][INFO] - Training Epoch: 2/2, step 5637/7134 completed (loss: 0.025359077379107475, acc: 0.994940996170044)
[2025-02-13 04:28:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:08][root][INFO] - Training Epoch: 2/2, step 5638/7134 completed (loss: 0.022516854107379913, acc: 0.994140625)
[2025-02-13 04:28:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:08][root][INFO] - Training Epoch: 2/2, step 5639/7134 completed (loss: 0.020120300352573395, acc: 0.994301974773407)
[2025-02-13 04:28:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:08][root][INFO] - Training Epoch: 2/2, step 5640/7134 completed (loss: 0.027689129114151, acc: 0.9918699264526367)
[2025-02-13 04:28:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:09][root][INFO] - Training Epoch: 2/2, step 5641/7134 completed (loss: 0.030436761677265167, acc: 0.9924242496490479)
[2025-02-13 04:28:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:09][root][INFO] - Training Epoch: 2/2, step 5642/7134 completed (loss: 0.04623448848724365, acc: 0.989393949508667)
[2025-02-13 04:28:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:10][root][INFO] - Training Epoch: 2/2, step 5643/7134 completed (loss: 0.03647496923804283, acc: 0.98893803358078)
[2025-02-13 04:28:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:10][root][INFO] - Training Epoch: 2/2, step 5644/7134 completed (loss: 0.021149124950170517, acc: 0.9939024448394775)
[2025-02-13 04:28:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:11][root][INFO] - Training Epoch: 2/2, step 5645/7134 completed (loss: 0.010433796793222427, acc: 0.9985272288322449)
[2025-02-13 04:28:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:11][root][INFO] - Training Epoch: 2/2, step 5646/7134 completed (loss: 0.01564464531838894, acc: 0.9952229261398315)
[2025-02-13 04:28:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:11][root][INFO] - Training Epoch: 2/2, step 5647/7134 completed (loss: 0.029149796813726425, acc: 0.9922480583190918)
[2025-02-13 04:28:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:12][root][INFO] - Training Epoch: 2/2, step 5648/7134 completed (loss: 0.058525148779153824, acc: 0.983582079410553)
[2025-02-13 04:28:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:12][root][INFO] - Training Epoch: 2/2, step 5649/7134 completed (loss: 0.04332394897937775, acc: 0.9831804037094116)
[2025-02-13 04:28:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:13][root][INFO] - Training Epoch: 2/2, step 5650/7134 completed (loss: 0.0228931475430727, acc: 0.9931389093399048)
[2025-02-13 04:28:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:13][root][INFO] - Training Epoch: 2/2, step 5651/7134 completed (loss: 0.014380600303411484, acc: 0.9955621361732483)
[2025-02-13 04:28:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:14][root][INFO] - Training Epoch: 2/2, step 5652/7134 completed (loss: 0.025827718898653984, acc: 0.9919871687889099)
[2025-02-13 04:28:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:14][root][INFO] - Training Epoch: 2/2, step 5653/7134 completed (loss: 0.05550968647003174, acc: 0.9869791865348816)
[2025-02-13 04:28:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:14][root][INFO] - Training Epoch: 2/2, step 5654/7134 completed (loss: 0.018158724531531334, acc: 0.9939939975738525)
[2025-02-13 04:28:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:15][root][INFO] - Training Epoch: 2/2, step 5655/7134 completed (loss: 0.028795534744858742, acc: 0.9931972622871399)
[2025-02-13 04:28:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:15][root][INFO] - Training Epoch: 2/2, step 5656/7134 completed (loss: 0.017862949520349503, acc: 0.9923809766769409)
[2025-02-13 04:28:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:16][root][INFO] - Training Epoch: 2/2, step 5657/7134 completed (loss: 0.03658493608236313, acc: 0.9912152290344238)
[2025-02-13 04:28:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:16][root][INFO] - Training Epoch: 2/2, step 5658/7134 completed (loss: 0.025044942274689674, acc: 0.9903314709663391)
[2025-02-13 04:28:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:17][root][INFO] - Training Epoch: 2/2, step 5659/7134 completed (loss: 0.028081748634576797, acc: 0.9886524677276611)
[2025-02-13 04:28:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:17][root][INFO] - Training Epoch: 2/2, step 5660/7134 completed (loss: 0.017619645223021507, acc: 0.9950413107872009)
[2025-02-13 04:28:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:17][root][INFO] - Training Epoch: 2/2, step 5661/7134 completed (loss: 0.0063155111856758595, acc: 0.9981752038002014)
[2025-02-13 04:28:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:18][root][INFO] - Training Epoch: 2/2, step 5662/7134 completed (loss: 0.014396783895790577, acc: 0.9967948794364929)
[2025-02-13 04:28:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:18][root][INFO] - Training Epoch: 2/2, step 5663/7134 completed (loss: 0.00840711873024702, acc: 0.9984591603279114)
[2025-02-13 04:28:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:19][root][INFO] - Training Epoch: 2/2, step 5664/7134 completed (loss: 0.045245975255966187, acc: 0.98959881067276)
[2025-02-13 04:28:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:19][root][INFO] - Training Epoch: 2/2, step 5665/7134 completed (loss: 0.015877896919846535, acc: 0.9942938685417175)
[2025-02-13 04:28:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:20][root][INFO] - Training Epoch: 2/2, step 5666/7134 completed (loss: 0.007061470299959183, acc: 0.9986206889152527)
[2025-02-13 04:28:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:20][root][INFO] - Training Epoch: 2/2, step 5667/7134 completed (loss: 0.014673233032226562, acc: 0.9927272796630859)
[2025-02-13 04:28:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:20][root][INFO] - Training Epoch: 2/2, step 5668/7134 completed (loss: 0.019534995779395103, acc: 0.9916943311691284)
[2025-02-13 04:28:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:21][root][INFO] - Training Epoch: 2/2, step 5669/7134 completed (loss: 0.009068520739674568, acc: 0.9956011772155762)
[2025-02-13 04:28:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:21][root][INFO] - Training Epoch: 2/2, step 5670/7134 completed (loss: 0.010444865562021732, acc: 0.996666669845581)
[2025-02-13 04:28:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:22][root][INFO] - Training Epoch: 2/2, step 5671/7134 completed (loss: 0.012599194422364235, acc: 0.9963099360466003)
[2025-02-13 04:28:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:22][root][INFO] - Training Epoch: 2/2, step 5672/7134 completed (loss: 0.008282024413347244, acc: 0.9984227418899536)
[2025-02-13 04:28:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:23][root][INFO] - Training Epoch: 2/2, step 5673/7134 completed (loss: 0.00944424420595169, acc: 0.9970458149909973)
[2025-02-13 04:28:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:23][root][INFO] - Training Epoch: 2/2, step 5674/7134 completed (loss: 0.003920805640518665, acc: 1.0)
[2025-02-13 04:28:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:24][root][INFO] - Training Epoch: 2/2, step 5675/7134 completed (loss: 0.029007555916905403, acc: 0.9917080998420715)
[2025-02-13 04:28:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:24][root][INFO] - Training Epoch: 2/2, step 5676/7134 completed (loss: 0.06887615472078323, acc: 0.9837278127670288)
[2025-02-13 04:28:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:24][root][INFO] - Training Epoch: 2/2, step 5677/7134 completed (loss: 0.04254201054573059, acc: 0.9857954382896423)
[2025-02-13 04:28:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:25][root][INFO] - Training Epoch: 2/2, step 5678/7134 completed (loss: 0.02784726768732071, acc: 0.9911764860153198)
[2025-02-13 04:28:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:25][root][INFO] - Training Epoch: 2/2, step 5679/7134 completed (loss: 0.04616418480873108, acc: 0.9838945865631104)
[2025-02-13 04:28:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:26][root][INFO] - Training Epoch: 2/2, step 5680/7134 completed (loss: 0.052049558609724045, acc: 0.9806259274482727)
[2025-02-13 04:28:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:26][root][INFO] - Training Epoch: 2/2, step 5681/7134 completed (loss: 0.041036978363990784, acc: 0.9879336357116699)
[2025-02-13 04:28:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:27][root][INFO] - Training Epoch: 2/2, step 5682/7134 completed (loss: 0.022056829184293747, acc: 0.9913420081138611)
[2025-02-13 04:28:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:27][root][INFO] - Training Epoch: 2/2, step 5683/7134 completed (loss: 0.04155546426773071, acc: 0.9780380725860596)
[2025-02-13 04:28:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:27][root][INFO] - Training Epoch: 2/2, step 5684/7134 completed (loss: 0.046814922243356705, acc: 0.9915682673454285)
[2025-02-13 04:28:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:28][root][INFO] - Training Epoch: 2/2, step 5685/7134 completed (loss: 0.03800889849662781, acc: 0.9926470518112183)
[2025-02-13 04:28:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:28][root][INFO] - Training Epoch: 2/2, step 5686/7134 completed (loss: 0.042938798666000366, acc: 0.9881556630134583)
[2025-02-13 04:28:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:29][root][INFO] - Training Epoch: 2/2, step 5687/7134 completed (loss: 0.04428781196475029, acc: 0.9890109896659851)
[2025-02-13 04:28:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:29][root][INFO] - Training Epoch: 2/2, step 5688/7134 completed (loss: 0.05132867768406868, acc: 0.9826435446739197)
[2025-02-13 04:28:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:30][root][INFO] - Training Epoch: 2/2, step 5689/7134 completed (loss: 0.04240814223885536, acc: 0.9939117431640625)
[2025-02-13 04:28:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:30][root][INFO] - Training Epoch: 2/2, step 5690/7134 completed (loss: 0.01806015335023403, acc: 0.9952830076217651)
[2025-02-13 04:28:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:30][root][INFO] - Training Epoch: 2/2, step 5691/7134 completed (loss: 0.030870117247104645, acc: 0.9900621175765991)
[2025-02-13 04:28:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:31][root][INFO] - Training Epoch: 2/2, step 5692/7134 completed (loss: 0.04824959114193916, acc: 0.989276111125946)
[2025-02-13 04:28:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:31][root][INFO] - Training Epoch: 2/2, step 5693/7134 completed (loss: 0.00956517644226551, acc: 0.9985007643699646)
[2025-02-13 04:28:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:32][root][INFO] - Training Epoch: 2/2, step 5694/7134 completed (loss: 0.035819489508867264, acc: 0.9927641153335571)
[2025-02-13 04:28:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:32][root][INFO] - Training Epoch: 2/2, step 5695/7134 completed (loss: 0.013590519316494465, acc: 0.9942693114280701)
[2025-02-13 04:28:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:33][root][INFO] - Training Epoch: 2/2, step 5696/7134 completed (loss: 0.015002802014350891, acc: 0.9976470470428467)
[2025-02-13 04:28:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:33][root][INFO] - Training Epoch: 2/2, step 5697/7134 completed (loss: 0.026773307472467422, acc: 0.994358241558075)
[2025-02-13 04:28:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:33][root][INFO] - Training Epoch: 2/2, step 5698/7134 completed (loss: 0.028570929542183876, acc: 0.994557797908783)
[2025-02-13 04:28:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:34][root][INFO] - Training Epoch: 2/2, step 5699/7134 completed (loss: 0.030645407736301422, acc: 0.9923567175865173)
[2025-02-13 04:28:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:34][root][INFO] - Training Epoch: 2/2, step 5700/7134 completed (loss: 0.015812948346138, acc: 0.9957447052001953)
[2025-02-13 04:28:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:35][root][INFO] - Training Epoch: 2/2, step 5701/7134 completed (loss: 0.0322394073009491, acc: 0.9974968433380127)
[2025-02-13 04:28:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:35][root][INFO] - Training Epoch: 2/2, step 5702/7134 completed (loss: 0.027534477412700653, acc: 0.9972936511039734)
[2025-02-13 04:28:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:36][root][INFO] - Training Epoch: 2/2, step 5703/7134 completed (loss: 0.011858580633997917, acc: 0.9957805871963501)
[2025-02-13 04:28:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:36][root][INFO] - Training Epoch: 2/2, step 5704/7134 completed (loss: 0.03416223078966141, acc: 0.9885877370834351)
[2025-02-13 04:28:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:37][root][INFO] - Training Epoch: 2/2, step 5705/7134 completed (loss: 0.003164742374792695, acc: 1.0)
[2025-02-13 04:28:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:37][root][INFO] - Training Epoch: 2/2, step 5706/7134 completed (loss: 0.006042596884071827, acc: 1.0)
[2025-02-13 04:28:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:37][root][INFO] - Training Epoch: 2/2, step 5707/7134 completed (loss: 0.02367752231657505, acc: 0.9929577708244324)
[2025-02-13 04:28:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:38][root][INFO] - Training Epoch: 2/2, step 5708/7134 completed (loss: 0.006780930794775486, acc: 0.9985185265541077)
[2025-02-13 04:28:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:38][root][INFO] - Training Epoch: 2/2, step 5709/7134 completed (loss: 0.03529922664165497, acc: 0.9931600689888)
[2025-02-13 04:28:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:39][root][INFO] - Training Epoch: 2/2, step 5710/7134 completed (loss: 0.011932958848774433, acc: 0.9961832165718079)
[2025-02-13 04:28:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:39][root][INFO] - Training Epoch: 2/2, step 5711/7134 completed (loss: 0.039269622415304184, acc: 0.9895287752151489)
[2025-02-13 04:28:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:40][root][INFO] - Training Epoch: 2/2, step 5712/7134 completed (loss: 0.017747094854712486, acc: 0.9919999837875366)
[2025-02-13 04:28:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:40][root][INFO] - Training Epoch: 2/2, step 5713/7134 completed (loss: 0.013321032747626305, acc: 0.9949173927307129)
[2025-02-13 04:28:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:41][root][INFO] - Training Epoch: 2/2, step 5714/7134 completed (loss: 0.014929601922631264, acc: 0.9962639808654785)
[2025-02-13 04:28:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:41][root][INFO] - Training Epoch: 2/2, step 5715/7134 completed (loss: 0.03614109009504318, acc: 0.9888268113136292)
[2025-02-13 04:28:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:41][root][INFO] - Training Epoch: 2/2, step 5716/7134 completed (loss: 0.046282075345516205, acc: 0.9875665903091431)
[2025-02-13 04:28:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:42][root][INFO] - Training Epoch: 2/2, step 5717/7134 completed (loss: 0.04633935168385506, acc: 0.9860031008720398)
[2025-02-13 04:28:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:42][root][INFO] - Training Epoch: 2/2, step 5718/7134 completed (loss: 0.0070918044075369835, acc: 0.9968454241752625)
[2025-02-13 04:28:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:43][root][INFO] - Training Epoch: 2/2, step 5719/7134 completed (loss: 0.010291765443980694, acc: 0.995502233505249)
[2025-02-13 04:28:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:43][root][INFO] - Training Epoch: 2/2, step 5720/7134 completed (loss: 0.018225278705358505, acc: 0.9930915236473083)
[2025-02-13 04:28:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:43][root][INFO] - Training Epoch: 2/2, step 5721/7134 completed (loss: 0.00962854828685522, acc: 0.996052622795105)
[2025-02-13 04:28:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:44][root][INFO] - Training Epoch: 2/2, step 5722/7134 completed (loss: 0.01023248489946127, acc: 0.9955654144287109)
[2025-02-13 04:28:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:44][root][INFO] - Training Epoch: 2/2, step 5723/7134 completed (loss: 0.031921446323394775, acc: 0.9878970980644226)
[2025-02-13 04:28:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:45][root][INFO] - Training Epoch: 2/2, step 5724/7134 completed (loss: 0.014714987017214298, acc: 0.9936000108718872)
[2025-02-13 04:28:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:45][root][INFO] - Training Epoch: 2/2, step 5725/7134 completed (loss: 0.009174705483019352, acc: 0.9971428513526917)
[2025-02-13 04:28:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:45][root][INFO] - Training Epoch: 2/2, step 5726/7134 completed (loss: 0.04753216728568077, acc: 0.9908257126808167)
[2025-02-13 04:28:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:46][root][INFO] - Training Epoch: 2/2, step 5727/7134 completed (loss: 0.01335200760513544, acc: 0.9973261952400208)
[2025-02-13 04:28:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:46][root][INFO] - Training Epoch: 2/2, step 5728/7134 completed (loss: 0.00778794102370739, acc: 0.9982455968856812)
[2025-02-13 04:28:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:47][root][INFO] - Training Epoch: 2/2, step 5729/7134 completed (loss: 0.008384734392166138, acc: 0.996666669845581)
[2025-02-13 04:28:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:47][root][INFO] - Training Epoch: 2/2, step 5730/7134 completed (loss: 0.041653916239738464, acc: 0.991919219493866)
[2025-02-13 04:28:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:48][root][INFO] - Training Epoch: 2/2, step 5731/7134 completed (loss: 0.0228995680809021, acc: 0.9916943311691284)
[2025-02-13 04:28:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:48][root][INFO] - Training Epoch: 2/2, step 5732/7134 completed (loss: 0.11873401701450348, acc: 0.9728155136108398)
[2025-02-13 04:28:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:48][root][INFO] - Training Epoch: 2/2, step 5733/7134 completed (loss: 0.04643721505999565, acc: 0.9890710115432739)
[2025-02-13 04:28:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:49][root][INFO] - Training Epoch: 2/2, step 5734/7134 completed (loss: 0.006092647556215525, acc: 1.0)
[2025-02-13 04:28:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:49][root][INFO] - Training Epoch: 2/2, step 5735/7134 completed (loss: 0.025960685685276985, acc: 0.9946236610412598)
[2025-02-13 04:28:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:50][root][INFO] - Training Epoch: 2/2, step 5736/7134 completed (loss: 0.02151045762002468, acc: 0.9909774661064148)
[2025-02-13 04:28:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:50][root][INFO] - Training Epoch: 2/2, step 5737/7134 completed (loss: 0.023131825029850006, acc: 0.9939576983451843)
[2025-02-13 04:28:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:50][root][INFO] - Training Epoch: 2/2, step 5738/7134 completed (loss: 0.01639884151518345, acc: 0.996372401714325)
[2025-02-13 04:28:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:51][root][INFO] - Training Epoch: 2/2, step 5739/7134 completed (loss: 0.02351946197450161, acc: 0.9936842322349548)
[2025-02-13 04:28:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:51][root][INFO] - Training Epoch: 2/2, step 5740/7134 completed (loss: 0.02878490462899208, acc: 0.9923273921012878)
[2025-02-13 04:28:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:52][root][INFO] - Training Epoch: 2/2, step 5741/7134 completed (loss: 0.030449390411376953, acc: 0.9910141229629517)
[2025-02-13 04:28:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:52][root][INFO] - Training Epoch: 2/2, step 5742/7134 completed (loss: 0.016294140368700027, acc: 0.9960578083992004)
[2025-02-13 04:28:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:53][root][INFO] - Training Epoch: 2/2, step 5743/7134 completed (loss: 0.006019257474690676, acc: 0.9963235259056091)
[2025-02-13 04:28:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:53][root][INFO] - Training Epoch: 2/2, step 5744/7134 completed (loss: 0.005739356856793165, acc: 0.9979079365730286)
[2025-02-13 04:28:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:53][root][INFO] - Training Epoch: 2/2, step 5745/7134 completed (loss: 0.027792923152446747, acc: 0.9909326434135437)
[2025-02-13 04:28:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:54][root][INFO] - Training Epoch: 2/2, step 5746/7134 completed (loss: 0.03351261839270592, acc: 0.9926470518112183)
[2025-02-13 04:28:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:54][root][INFO] - Training Epoch: 2/2, step 5747/7134 completed (loss: 0.015929056331515312, acc: 0.9944598078727722)
[2025-02-13 04:28:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:55][root][INFO] - Training Epoch: 2/2, step 5748/7134 completed (loss: 0.00686064176261425, acc: 0.998171865940094)
[2025-02-13 04:28:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:55][root][INFO] - Training Epoch: 2/2, step 5749/7134 completed (loss: 0.024499159306287766, acc: 0.9908376932144165)
[2025-02-13 04:28:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:56][root][INFO] - Training Epoch: 2/2, step 5750/7134 completed (loss: 0.026085980236530304, acc: 0.9888613820075989)
[2025-02-13 04:28:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:56][root][INFO] - Training Epoch: 2/2, step 5751/7134 completed (loss: 0.02046162262558937, acc: 0.9910141229629517)
[2025-02-13 04:28:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:56][root][INFO] - Training Epoch: 2/2, step 5752/7134 completed (loss: 0.04343428835272789, acc: 0.9860788583755493)
[2025-02-13 04:28:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:57][root][INFO] - Training Epoch: 2/2, step 5753/7134 completed (loss: 0.04355860874056816, acc: 0.9859374761581421)
[2025-02-13 04:28:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:57][root][INFO] - Training Epoch: 2/2, step 5754/7134 completed (loss: 0.05564279854297638, acc: 0.9847133755683899)
[2025-02-13 04:28:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:58][root][INFO] - Training Epoch: 2/2, step 5755/7134 completed (loss: 0.029784424230456352, acc: 0.9897511005401611)
[2025-02-13 04:28:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:58][root][INFO] - Training Epoch: 2/2, step 5756/7134 completed (loss: 0.02329324372112751, acc: 0.9960106611251831)
[2025-02-13 04:28:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:59][root][INFO] - Training Epoch: 2/2, step 5757/7134 completed (loss: 0.02306612767279148, acc: 0.9948914647102356)
[2025-02-13 04:28:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:59][root][INFO] - Training Epoch: 2/2, step 5758/7134 completed (loss: 0.02552231028676033, acc: 0.9867647290229797)
[2025-02-13 04:28:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:28:59][root][INFO] - Training Epoch: 2/2, step 5759/7134 completed (loss: 0.021344074979424477, acc: 0.9892183542251587)
[2025-02-13 04:28:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:00][root][INFO] - Training Epoch: 2/2, step 5760/7134 completed (loss: 0.023064076900482178, acc: 0.9923664331436157)
[2025-02-13 04:29:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:00][root][INFO] - Training Epoch: 2/2, step 5761/7134 completed (loss: 0.01904030330479145, acc: 0.9945054650306702)
[2025-02-13 04:29:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:01][root][INFO] - Training Epoch: 2/2, step 5762/7134 completed (loss: 0.028723876923322678, acc: 0.9927007555961609)
[2025-02-13 04:29:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:01][root][INFO] - Training Epoch: 2/2, step 5763/7134 completed (loss: 0.024569977074861526, acc: 0.9962359070777893)
[2025-02-13 04:29:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:02][root][INFO] - Training Epoch: 2/2, step 5764/7134 completed (loss: 0.01238249335438013, acc: 0.9965811967849731)
[2025-02-13 04:29:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:02][root][INFO] - Training Epoch: 2/2, step 5765/7134 completed (loss: 0.010411769151687622, acc: 0.9971988797187805)
[2025-02-13 04:29:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:02][root][INFO] - Training Epoch: 2/2, step 5766/7134 completed (loss: 0.015014450065791607, acc: 0.9985052347183228)
[2025-02-13 04:29:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:03][root][INFO] - Training Epoch: 2/2, step 5767/7134 completed (loss: 0.0061623030342161655, acc: 0.9972183704376221)
[2025-02-13 04:29:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:03][root][INFO] - Training Epoch: 2/2, step 5768/7134 completed (loss: 0.006454514805227518, acc: 0.9970059990882874)
[2025-02-13 04:29:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:04][root][INFO] - Training Epoch: 2/2, step 5769/7134 completed (loss: 0.004161621909588575, acc: 0.9983633160591125)
[2025-02-13 04:29:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:04][root][INFO] - Training Epoch: 2/2, step 5770/7134 completed (loss: 0.02199459820985794, acc: 0.993880033493042)
[2025-02-13 04:29:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:05][root][INFO] - Training Epoch: 2/2, step 5771/7134 completed (loss: 0.035708848387002945, acc: 0.9945651888847351)
[2025-02-13 04:29:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:05][root][INFO] - Training Epoch: 2/2, step 5772/7134 completed (loss: 0.019009137526154518, acc: 0.9930434823036194)
[2025-02-13 04:29:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:05][root][INFO] - Training Epoch: 2/2, step 5773/7134 completed (loss: 0.011435149237513542, acc: 0.9949832558631897)
[2025-02-13 04:29:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:06][root][INFO] - Training Epoch: 2/2, step 5774/7134 completed (loss: 0.004179650451987982, acc: 1.0)
[2025-02-13 04:29:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:06][root][INFO] - Training Epoch: 2/2, step 5775/7134 completed (loss: 0.004504920449107885, acc: 1.0)
[2025-02-13 04:29:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:07][root][INFO] - Training Epoch: 2/2, step 5776/7134 completed (loss: 0.010371829383075237, acc: 0.9971910119056702)
[2025-02-13 04:29:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:07][root][INFO] - Training Epoch: 2/2, step 5777/7134 completed (loss: 0.0315970778465271, acc: 0.9938119053840637)
[2025-02-13 04:29:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:08][root][INFO] - Training Epoch: 2/2, step 5778/7134 completed (loss: 0.04137956723570824, acc: 0.9871794581413269)
[2025-02-13 04:29:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:08][root][INFO] - Training Epoch: 2/2, step 5779/7134 completed (loss: 0.033661920577287674, acc: 0.993122398853302)
[2025-02-13 04:29:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:09][root][INFO] - Training Epoch: 2/2, step 5780/7134 completed (loss: 0.01560667622834444, acc: 0.9928401112556458)
[2025-02-13 04:29:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:09][root][INFO] - Training Epoch: 2/2, step 5781/7134 completed (loss: 0.01774861104786396, acc: 0.9944751262664795)
[2025-02-13 04:29:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:09][root][INFO] - Training Epoch: 2/2, step 5782/7134 completed (loss: 0.010939284227788448, acc: 0.9974093437194824)
[2025-02-13 04:29:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:10][root][INFO] - Training Epoch: 2/2, step 5783/7134 completed (loss: 0.013114379718899727, acc: 0.9973439574241638)
[2025-02-13 04:29:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:10][root][INFO] - Training Epoch: 2/2, step 5784/7134 completed (loss: 0.021667759865522385, acc: 0.9919246435165405)
[2025-02-13 04:29:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:11][root][INFO] - Training Epoch: 2/2, step 5785/7134 completed (loss: 0.024935932829976082, acc: 0.9905660152435303)
[2025-02-13 04:29:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:11][root][INFO] - Training Epoch: 2/2, step 5786/7134 completed (loss: 0.010100864805281162, acc: 0.9959016442298889)
[2025-02-13 04:29:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:12][root][INFO] - Training Epoch: 2/2, step 5787/7134 completed (loss: 0.020447930321097374, acc: 0.9937810897827148)
[2025-02-13 04:29:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:12][root][INFO] - Training Epoch: 2/2, step 5788/7134 completed (loss: 0.008304508402943611, acc: 0.9969230890274048)
[2025-02-13 04:29:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:12][root][INFO] - Training Epoch: 2/2, step 5789/7134 completed (loss: 0.008893937803804874, acc: 0.996277928352356)
[2025-02-13 04:29:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:13][root][INFO] - Training Epoch: 2/2, step 5790/7134 completed (loss: 0.012391062453389168, acc: 0.9931318759918213)
[2025-02-13 04:29:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:13][root][INFO] - Training Epoch: 2/2, step 5791/7134 completed (loss: 0.02242615818977356, acc: 0.993127167224884)
[2025-02-13 04:29:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:14][root][INFO] - Training Epoch: 2/2, step 5792/7134 completed (loss: 0.059133656322956085, acc: 0.9881154298782349)
[2025-02-13 04:29:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:14][root][INFO] - Training Epoch: 2/2, step 5793/7134 completed (loss: 0.036089856177568436, acc: 0.9858490824699402)
[2025-02-13 04:29:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:15][root][INFO] - Training Epoch: 2/2, step 5794/7134 completed (loss: 0.02890853025019169, acc: 0.9909090995788574)
[2025-02-13 04:29:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:15][root][INFO] - Training Epoch: 2/2, step 5795/7134 completed (loss: 0.07262006402015686, acc: 0.9810924530029297)
[2025-02-13 04:29:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:15][root][INFO] - Training Epoch: 2/2, step 5796/7134 completed (loss: 0.014262551441788673, acc: 0.9949238300323486)
[2025-02-13 04:29:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:16][root][INFO] - Training Epoch: 2/2, step 5797/7134 completed (loss: 0.02030358277261257, acc: 0.9968000054359436)
[2025-02-13 04:29:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:16][root][INFO] - Training Epoch: 2/2, step 5798/7134 completed (loss: 0.16915476322174072, acc: 0.9652174115180969)
[2025-02-13 04:29:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:17][root][INFO] - Training Epoch: 2/2, step 5799/7134 completed (loss: 0.06033356115221977, acc: 0.9802761077880859)
[2025-02-13 04:29:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:17][root][INFO] - Training Epoch: 2/2, step 5800/7134 completed (loss: 0.05725490301847458, acc: 0.9852941036224365)
[2025-02-13 04:29:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:18][root][INFO] - Training Epoch: 2/2, step 5801/7134 completed (loss: 0.023701855912804604, acc: 0.9923780560493469)
[2025-02-13 04:29:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:18][root][INFO] - Training Epoch: 2/2, step 5802/7134 completed (loss: 0.016453245654702187, acc: 0.9944547414779663)
[2025-02-13 04:29:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:18][root][INFO] - Training Epoch: 2/2, step 5803/7134 completed (loss: 0.018688052892684937, acc: 0.9923469424247742)
[2025-02-13 04:29:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:19][root][INFO] - Training Epoch: 2/2, step 5804/7134 completed (loss: 0.047684356570243835, acc: 0.981055498123169)
[2025-02-13 04:29:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:19][root][INFO] - Training Epoch: 2/2, step 5805/7134 completed (loss: 0.03029530495405197, acc: 0.9899280667304993)
[2025-02-13 04:29:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:20][root][INFO] - Training Epoch: 2/2, step 5806/7134 completed (loss: 0.01651909574866295, acc: 0.994535505771637)
[2025-02-13 04:29:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:20][root][INFO] - Training Epoch: 2/2, step 5807/7134 completed (loss: 0.0491795614361763, acc: 0.9894099831581116)
[2025-02-13 04:29:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:21][root][INFO] - Training Epoch: 2/2, step 5808/7134 completed (loss: 0.017797846347093582, acc: 0.9943740963935852)
[2025-02-13 04:29:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:21][root][INFO] - Training Epoch: 2/2, step 5809/7134 completed (loss: 0.03221047297120094, acc: 0.9934210777282715)
[2025-02-13 04:29:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:21][root][INFO] - Training Epoch: 2/2, step 5810/7134 completed (loss: 0.02383398450911045, acc: 0.9898580312728882)
[2025-02-13 04:29:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:22][root][INFO] - Training Epoch: 2/2, step 5811/7134 completed (loss: 0.0190906785428524, acc: 0.9927710890769958)
[2025-02-13 04:29:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:22][root][INFO] - Training Epoch: 2/2, step 5812/7134 completed (loss: 0.009837158024311066, acc: 0.9966996908187866)
[2025-02-13 04:29:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:23][root][INFO] - Training Epoch: 2/2, step 5813/7134 completed (loss: 0.01064822357147932, acc: 0.99622642993927)
[2025-02-13 04:29:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:23][root][INFO] - Training Epoch: 2/2, step 5814/7134 completed (loss: 0.010000854730606079, acc: 0.9983079433441162)
[2025-02-13 04:29:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:24][root][INFO] - Training Epoch: 2/2, step 5815/7134 completed (loss: 0.02198128029704094, acc: 0.9962335228919983)
[2025-02-13 04:29:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:24][root][INFO] - Training Epoch: 2/2, step 5816/7134 completed (loss: 0.0036580434534698725, acc: 1.0)
[2025-02-13 04:29:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:25][root][INFO] - Training Epoch: 2/2, step 5817/7134 completed (loss: 0.04412238672375679, acc: 0.989924430847168)
[2025-02-13 04:29:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:25][root][INFO] - Training Epoch: 2/2, step 5818/7134 completed (loss: 0.01976725272834301, acc: 0.9925925731658936)
[2025-02-13 04:29:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:25][root][INFO] - Training Epoch: 2/2, step 5819/7134 completed (loss: 0.020458431914448738, acc: 0.9948275685310364)
[2025-02-13 04:29:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:26][root][INFO] - Training Epoch: 2/2, step 5820/7134 completed (loss: 0.03677630051970482, acc: 0.9876033067703247)
[2025-02-13 04:29:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:26][root][INFO] - Training Epoch: 2/2, step 5821/7134 completed (loss: 0.011519888415932655, acc: 0.9953774809837341)
[2025-02-13 04:29:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:27][root][INFO] - Training Epoch: 2/2, step 5822/7134 completed (loss: 0.021761296316981316, acc: 0.9951612949371338)
[2025-02-13 04:29:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:27][root][INFO] - Training Epoch: 2/2, step 5823/7134 completed (loss: 0.021640922874212265, acc: 0.9922239780426025)
[2025-02-13 04:29:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:28][root][INFO] - Training Epoch: 2/2, step 5824/7134 completed (loss: 0.014563317410647869, acc: 0.993779182434082)
[2025-02-13 04:29:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:28][root][INFO] - Training Epoch: 2/2, step 5825/7134 completed (loss: 0.020907940343022346, acc: 0.9965397715568542)
[2025-02-13 04:29:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:28][root][INFO] - Training Epoch: 2/2, step 5826/7134 completed (loss: 0.003673183498904109, acc: 1.0)
[2025-02-13 04:29:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:29][root][INFO] - Training Epoch: 2/2, step 5827/7134 completed (loss: 0.006468579173088074, acc: 0.9961538314819336)
[2025-02-13 04:29:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:29][root][INFO] - Training Epoch: 2/2, step 5828/7134 completed (loss: 0.028996996581554413, acc: 0.98959881067276)
[2025-02-13 04:29:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:30][root][INFO] - Training Epoch: 2/2, step 5829/7134 completed (loss: 0.023055564612150192, acc: 0.992094874382019)
[2025-02-13 04:29:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:30][root][INFO] - Training Epoch: 2/2, step 5830/7134 completed (loss: 0.011828085407614708, acc: 0.9943289160728455)
[2025-02-13 04:29:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:31][root][INFO] - Training Epoch: 2/2, step 5831/7134 completed (loss: 0.00924298632889986, acc: 0.9976744055747986)
[2025-02-13 04:29:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:31][root][INFO] - Training Epoch: 2/2, step 5832/7134 completed (loss: 0.036122847348451614, acc: 0.9915540814399719)
[2025-02-13 04:29:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:32][root][INFO] - Training Epoch: 2/2, step 5833/7134 completed (loss: 0.011888854205608368, acc: 0.9964157938957214)
[2025-02-13 04:29:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:32][root][INFO] - Training Epoch: 2/2, step 5834/7134 completed (loss: 0.012777779251337051, acc: 0.9972222447395325)
[2025-02-13 04:29:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:32][root][INFO] - Training Epoch: 2/2, step 5835/7134 completed (loss: 0.006200017407536507, acc: 0.9985548853874207)
[2025-02-13 04:29:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:33][root][INFO] - Training Epoch: 2/2, step 5836/7134 completed (loss: 0.025654392316937447, acc: 0.9956772327423096)
[2025-02-13 04:29:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:33][root][INFO] - Training Epoch: 2/2, step 5837/7134 completed (loss: 0.051267512142658234, acc: 0.9915540814399719)
[2025-02-13 04:29:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:34][root][INFO] - Training Epoch: 2/2, step 5838/7134 completed (loss: 0.03185199573636055, acc: 0.9950576424598694)
[2025-02-13 04:29:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:34][root][INFO] - Training Epoch: 2/2, step 5839/7134 completed (loss: 0.02265666238963604, acc: 0.9964285492897034)
[2025-02-13 04:29:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:35][root][INFO] - Training Epoch: 2/2, step 5840/7134 completed (loss: 0.00972282700240612, acc: 0.996820330619812)
[2025-02-13 04:29:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:35][root][INFO] - Training Epoch: 2/2, step 5841/7134 completed (loss: 0.01164422370493412, acc: 0.9952152967453003)
[2025-02-13 04:29:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:36][root][INFO] - Training Epoch: 2/2, step 5842/7134 completed (loss: 0.012258093804121017, acc: 0.9981883764266968)
[2025-02-13 04:29:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:36][root][INFO] - Training Epoch: 2/2, step 5843/7134 completed (loss: 0.02214629389345646, acc: 0.9919678568840027)
[2025-02-13 04:29:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:37][root][INFO] - Training Epoch: 2/2, step 5844/7134 completed (loss: 0.007799886632710695, acc: 0.9985775351524353)
[2025-02-13 04:29:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:37][root][INFO] - Training Epoch: 2/2, step 5845/7134 completed (loss: 0.012828676030039787, acc: 0.9971510171890259)
[2025-02-13 04:29:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:37][root][INFO] - Training Epoch: 2/2, step 5846/7134 completed (loss: 0.018621178343892097, acc: 0.99452805519104)
[2025-02-13 04:29:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:38][root][INFO] - Training Epoch: 2/2, step 5847/7134 completed (loss: 0.011929621919989586, acc: 0.9945054650306702)
[2025-02-13 04:29:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:38][root][INFO] - Training Epoch: 2/2, step 5848/7134 completed (loss: 0.014217955991625786, acc: 0.992559552192688)
[2025-02-13 04:29:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:39][root][INFO] - Training Epoch: 2/2, step 5849/7134 completed (loss: 0.033637672662734985, acc: 0.9867256879806519)
[2025-02-13 04:29:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:39][root][INFO] - Training Epoch: 2/2, step 5850/7134 completed (loss: 0.010306626558303833, acc: 0.9954198598861694)
[2025-02-13 04:29:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:40][root][INFO] - Training Epoch: 2/2, step 5851/7134 completed (loss: 0.013460817746818066, acc: 0.9950658082962036)
[2025-02-13 04:29:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:40][root][INFO] - Training Epoch: 2/2, step 5852/7134 completed (loss: 0.0058056083507835865, acc: 0.9985401630401611)
[2025-02-13 04:29:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:41][root][INFO] - Training Epoch: 2/2, step 5853/7134 completed (loss: 0.0347052663564682, acc: 0.9918808937072754)
[2025-02-13 04:29:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:41][root][INFO] - Training Epoch: 2/2, step 5854/7134 completed (loss: 0.005254731979221106, acc: 1.0)
[2025-02-13 04:29:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:41][root][INFO] - Training Epoch: 2/2, step 5855/7134 completed (loss: 0.017905596643686295, acc: 0.9948186278343201)
[2025-02-13 04:29:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:42][root][INFO] - Training Epoch: 2/2, step 5856/7134 completed (loss: 0.02921845205128193, acc: 0.9894259572029114)
[2025-02-13 04:29:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:42][root][INFO] - Training Epoch: 2/2, step 5857/7134 completed (loss: 0.0276575218886137, acc: 0.9921011328697205)
[2025-02-13 04:29:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:43][root][INFO] - Training Epoch: 2/2, step 5858/7134 completed (loss: 0.009336018934845924, acc: 0.9981516003608704)
[2025-02-13 04:29:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:43][root][INFO] - Training Epoch: 2/2, step 5859/7134 completed (loss: 0.02044527605175972, acc: 0.9921875)
[2025-02-13 04:29:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:44][root][INFO] - Training Epoch: 2/2, step 5860/7134 completed (loss: 0.016615426167845726, acc: 0.9917355179786682)
[2025-02-13 04:29:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:44][root][INFO] - Training Epoch: 2/2, step 5861/7134 completed (loss: 0.012295227497816086, acc: 0.9959431886672974)
[2025-02-13 04:29:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:44][root][INFO] - Training Epoch: 2/2, step 5862/7134 completed (loss: 0.015583066269755363, acc: 0.991416335105896)
[2025-02-13 04:29:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:45][root][INFO] - Training Epoch: 2/2, step 5863/7134 completed (loss: 0.019356412813067436, acc: 0.997474730014801)
[2025-02-13 04:29:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:45][root][INFO] - Training Epoch: 2/2, step 5864/7134 completed (loss: 0.04574485495686531, acc: 0.9880239367485046)
[2025-02-13 04:29:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:46][root][INFO] - Training Epoch: 2/2, step 5865/7134 completed (loss: 0.025602353736758232, acc: 0.9920634627342224)
[2025-02-13 04:29:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:46][root][INFO] - Training Epoch: 2/2, step 5866/7134 completed (loss: 0.005914003122597933, acc: 0.9986772537231445)
[2025-02-13 04:29:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:47][root][INFO] - Training Epoch: 2/2, step 5867/7134 completed (loss: 0.005787767004221678, acc: 0.9983471035957336)
[2025-02-13 04:29:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:47][root][INFO] - Training Epoch: 2/2, step 5868/7134 completed (loss: 0.036743901669979095, acc: 0.9913793206214905)
[2025-02-13 04:29:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:48][root][INFO] - Training Epoch: 2/2, step 5869/7134 completed (loss: 0.02965192124247551, acc: 0.9940298795700073)
[2025-02-13 04:29:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:48][root][INFO] - Training Epoch: 2/2, step 5870/7134 completed (loss: 0.032234929502010345, acc: 0.9937238693237305)
[2025-02-13 04:29:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:48][root][INFO] - Training Epoch: 2/2, step 5871/7134 completed (loss: 0.009953298605978489, acc: 0.9957537055015564)
[2025-02-13 04:29:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:49][root][INFO] - Training Epoch: 2/2, step 5872/7134 completed (loss: 0.0158009622246027, acc: 0.9939117431640625)
[2025-02-13 04:29:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:49][root][INFO] - Training Epoch: 2/2, step 5873/7134 completed (loss: 0.017108142375946045, acc: 0.9933333396911621)
[2025-02-13 04:29:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:50][root][INFO] - Training Epoch: 2/2, step 5874/7134 completed (loss: 0.0074121784418821335, acc: 0.9979919791221619)
[2025-02-13 04:29:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:50][root][INFO] - Training Epoch: 2/2, step 5875/7134 completed (loss: 0.032510530203580856, acc: 0.9840707778930664)
[2025-02-13 04:29:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:51][root][INFO] - Training Epoch: 2/2, step 5876/7134 completed (loss: 0.01488141156733036, acc: 0.9951456189155579)
[2025-02-13 04:29:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:51][root][INFO] - Training Epoch: 2/2, step 5877/7134 completed (loss: 0.029355041682720184, acc: 0.9923858046531677)
[2025-02-13 04:29:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:51][root][INFO] - Training Epoch: 2/2, step 5878/7134 completed (loss: 0.010918745771050453, acc: 1.0)
[2025-02-13 04:29:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:52][root][INFO] - Training Epoch: 2/2, step 5879/7134 completed (loss: 0.004922275431454182, acc: 1.0)
[2025-02-13 04:29:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:52][root][INFO] - Training Epoch: 2/2, step 5880/7134 completed (loss: 0.00332279852591455, acc: 1.0)
[2025-02-13 04:29:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:53][root][INFO] - Training Epoch: 2/2, step 5881/7134 completed (loss: 0.006326131988316774, acc: 0.9980806112289429)
[2025-02-13 04:29:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:53][root][INFO] - Training Epoch: 2/2, step 5882/7134 completed (loss: 0.014284474775195122, acc: 0.9929412007331848)
[2025-02-13 04:29:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:53][root][INFO] - Training Epoch: 2/2, step 5883/7134 completed (loss: 0.030437173321843147, acc: 0.9925093650817871)
[2025-02-13 04:29:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:54][root][INFO] - Training Epoch: 2/2, step 5884/7134 completed (loss: 0.03163930028676987, acc: 0.985401451587677)
[2025-02-13 04:29:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:54][root][INFO] - Training Epoch: 2/2, step 5885/7134 completed (loss: 0.05464813485741615, acc: 0.9864130616188049)
[2025-02-13 04:29:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:55][root][INFO] - Training Epoch: 2/2, step 5886/7134 completed (loss: 0.0077699157409369946, acc: 0.9942857027053833)
[2025-02-13 04:29:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:55][root][INFO] - Training Epoch: 2/2, step 5887/7134 completed (loss: 0.016617080196738243, acc: 0.993630588054657)
[2025-02-13 04:29:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:56][root][INFO] - Training Epoch: 2/2, step 5888/7134 completed (loss: 0.03199918940663338, acc: 0.9898648858070374)
[2025-02-13 04:29:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:56][root][INFO] - Training Epoch: 2/2, step 5889/7134 completed (loss: 0.022776087746024132, acc: 0.9955357313156128)
[2025-02-13 04:29:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:56][root][INFO] - Training Epoch: 2/2, step 5890/7134 completed (loss: 0.009420089423656464, acc: 0.998251736164093)
[2025-02-13 04:29:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:57][root][INFO] - Training Epoch: 2/2, step 5891/7134 completed (loss: 0.028507206588983536, acc: 0.990791916847229)
[2025-02-13 04:29:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:57][root][INFO] - Training Epoch: 2/2, step 5892/7134 completed (loss: 0.022792264819145203, acc: 0.991253674030304)
[2025-02-13 04:29:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:58][root][INFO] - Training Epoch: 2/2, step 5893/7134 completed (loss: 0.024702277034521103, acc: 0.9937008023262024)
[2025-02-13 04:29:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:58][root][INFO] - Training Epoch: 2/2, step 5894/7134 completed (loss: 0.005143570713698864, acc: 1.0)
[2025-02-13 04:29:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:58][root][INFO] - Training Epoch: 2/2, step 5895/7134 completed (loss: 0.004267816431820393, acc: 1.0)
[2025-02-13 04:29:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:59][root][INFO] - Training Epoch: 2/2, step 5896/7134 completed (loss: 0.021985048428177834, acc: 0.9931972622871399)
[2025-02-13 04:29:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:29:59][root][INFO] - Training Epoch: 2/2, step 5897/7134 completed (loss: 0.008530214428901672, acc: 0.997245192527771)
[2025-02-13 04:29:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:00][root][INFO] - Training Epoch: 2/2, step 5898/7134 completed (loss: 0.0038209969643503428, acc: 1.0)
[2025-02-13 04:30:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:00][root][INFO] - Training Epoch: 2/2, step 5899/7134 completed (loss: 0.012661004438996315, acc: 0.9940476417541504)
[2025-02-13 04:30:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:00][root][INFO] - Training Epoch: 2/2, step 5900/7134 completed (loss: 0.005737915635108948, acc: 1.0)
[2025-02-13 04:30:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:01][root][INFO] - Training Epoch: 2/2, step 5901/7134 completed (loss: 0.02665983885526657, acc: 0.9900000095367432)
[2025-02-13 04:30:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:01][root][INFO] - Training Epoch: 2/2, step 5902/7134 completed (loss: 0.02319546602666378, acc: 0.9871134161949158)
[2025-02-13 04:30:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:02][root][INFO] - Training Epoch: 2/2, step 5903/7134 completed (loss: 0.02491101250052452, acc: 0.9908758997917175)
[2025-02-13 04:30:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:02][root][INFO] - Training Epoch: 2/2, step 5904/7134 completed (loss: 0.0298042930662632, acc: 0.9931694269180298)
[2025-02-13 04:30:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:03][root][INFO] - Training Epoch: 2/2, step 5905/7134 completed (loss: 0.01918359473347664, acc: 0.9916550517082214)
[2025-02-13 04:30:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:03][root][INFO] - Training Epoch: 2/2, step 5906/7134 completed (loss: 0.03459003567695618, acc: 0.9901719689369202)
[2025-02-13 04:30:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:04][root][INFO] - Training Epoch: 2/2, step 5907/7134 completed (loss: 0.03384887054562569, acc: 0.9909443855285645)
[2025-02-13 04:30:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:04][root][INFO] - Training Epoch: 2/2, step 5908/7134 completed (loss: 0.020339135080575943, acc: 0.9936061501502991)
[2025-02-13 04:30:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:05][root][INFO] - Training Epoch: 2/2, step 5909/7134 completed (loss: 0.023534469306468964, acc: 0.9913544654846191)
[2025-02-13 04:30:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:05][root][INFO] - Training Epoch: 2/2, step 5910/7134 completed (loss: 0.03782620653510094, acc: 0.9861303567886353)
[2025-02-13 04:30:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:06][root][INFO] - Training Epoch: 2/2, step 5911/7134 completed (loss: 0.03252169117331505, acc: 0.9900826215744019)
[2025-02-13 04:30:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:06][root][INFO] - Training Epoch: 2/2, step 5912/7134 completed (loss: 0.015191979706287384, acc: 0.9943740963935852)
[2025-02-13 04:30:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:06][root][INFO] - Training Epoch: 2/2, step 5913/7134 completed (loss: 0.055585384368896484, acc: 0.9864130616188049)
[2025-02-13 04:30:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:07][root][INFO] - Training Epoch: 2/2, step 5914/7134 completed (loss: 0.01953747868537903, acc: 0.9967897534370422)
[2025-02-13 04:30:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:07][root][INFO] - Training Epoch: 2/2, step 5915/7134 completed (loss: 0.04449259117245674, acc: 0.9891473054885864)
[2025-02-13 04:30:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:08][root][INFO] - Training Epoch: 2/2, step 5916/7134 completed (loss: 0.021907106041908264, acc: 0.9921362996101379)
[2025-02-13 04:30:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:08][root][INFO] - Training Epoch: 2/2, step 5917/7134 completed (loss: 0.016449764370918274, acc: 0.9948006868362427)
[2025-02-13 04:30:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:09][root][INFO] - Training Epoch: 2/2, step 5918/7134 completed (loss: 0.06762203574180603, acc: 0.9825834631919861)
[2025-02-13 04:30:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:09][root][INFO] - Training Epoch: 2/2, step 5919/7134 completed (loss: 0.017453862354159355, acc: 0.9954407215118408)
[2025-02-13 04:30:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:10][root][INFO] - Training Epoch: 2/2, step 5920/7134 completed (loss: 0.010646061971783638, acc: 0.9966722130775452)
[2025-02-13 04:30:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:10][root][INFO] - Training Epoch: 2/2, step 5921/7134 completed (loss: 0.011591799557209015, acc: 0.9958246350288391)
[2025-02-13 04:30:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:10][root][INFO] - Training Epoch: 2/2, step 5922/7134 completed (loss: 0.12395598739385605, acc: 0.9716312289237976)
[2025-02-13 04:30:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:11][root][INFO] - Training Epoch: 2/2, step 5923/7134 completed (loss: 0.04732932895421982, acc: 0.9897511005401611)
[2025-02-13 04:30:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:11][root][INFO] - Training Epoch: 2/2, step 5924/7134 completed (loss: 0.020597219467163086, acc: 0.9952267408370972)
[2025-02-13 04:30:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:12][root][INFO] - Training Epoch: 2/2, step 5925/7134 completed (loss: 0.13634875416755676, acc: 0.9677419066429138)
[2025-02-13 04:30:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:12][root][INFO] - Training Epoch: 2/2, step 5926/7134 completed (loss: 0.05139822140336037, acc: 0.9800000190734863)
[2025-02-13 04:30:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:12][root][INFO] - Training Epoch: 2/2, step 5927/7134 completed (loss: 0.04255592077970505, acc: 0.9844098091125488)
[2025-02-13 04:30:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:13][root][INFO] - Training Epoch: 2/2, step 5928/7134 completed (loss: 0.10765685141086578, acc: 0.9623287916183472)
[2025-02-13 04:30:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:13][root][INFO] - Training Epoch: 2/2, step 5929/7134 completed (loss: 0.03279930725693703, acc: 0.9925373196601868)
[2025-02-13 04:30:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:14][root][INFO] - Training Epoch: 2/2, step 5930/7134 completed (loss: 0.04220589995384216, acc: 0.9910913109779358)
[2025-02-13 04:30:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:14][root][INFO] - Training Epoch: 2/2, step 5931/7134 completed (loss: 0.031004225835204124, acc: 0.9913606643676758)
[2025-02-13 04:30:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:15][root][INFO] - Training Epoch: 2/2, step 5932/7134 completed (loss: 0.053630489856004715, acc: 0.9722222089767456)
[2025-02-13 04:30:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:15][root][INFO] - Training Epoch: 2/2, step 5933/7134 completed (loss: 0.08367346227169037, acc: 0.9727463126182556)
[2025-02-13 04:30:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:15][root][INFO] - Training Epoch: 2/2, step 5934/7134 completed (loss: 0.02703157812356949, acc: 0.9911190271377563)
[2025-02-13 04:30:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:16][root][INFO] - Training Epoch: 2/2, step 5935/7134 completed (loss: 0.012585838325321674, acc: 0.9941747784614563)
[2025-02-13 04:30:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:16][root][INFO] - Training Epoch: 2/2, step 5936/7134 completed (loss: 0.051577359437942505, acc: 0.9862174391746521)
[2025-02-13 04:30:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:17][root][INFO] - Training Epoch: 2/2, step 5937/7134 completed (loss: 0.026753462851047516, acc: 0.9920254945755005)
[2025-02-13 04:30:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:17][root][INFO] - Training Epoch: 2/2, step 5938/7134 completed (loss: 0.0341058075428009, acc: 0.990439772605896)
[2025-02-13 04:30:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:17][root][INFO] - Training Epoch: 2/2, step 5939/7134 completed (loss: 0.07618164271116257, acc: 0.9819004535675049)
[2025-02-13 04:30:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:18][root][INFO] - Training Epoch: 2/2, step 5940/7134 completed (loss: 0.03625698387622833, acc: 0.9897959232330322)
[2025-02-13 04:30:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:18][root][INFO] - Training Epoch: 2/2, step 5941/7134 completed (loss: 0.026023397222161293, acc: 0.9937499761581421)
[2025-02-13 04:30:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:19][root][INFO] - Training Epoch: 2/2, step 5942/7134 completed (loss: 0.012443301267921925, acc: 0.9923518300056458)
[2025-02-13 04:30:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:19][root][INFO] - Training Epoch: 2/2, step 5943/7134 completed (loss: 0.01005710568279028, acc: 1.0)
[2025-02-13 04:30:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:20][root][INFO] - Training Epoch: 2/2, step 5944/7134 completed (loss: 0.036493148654699326, acc: 0.9882199168205261)
[2025-02-13 04:30:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:20][root][INFO] - Training Epoch: 2/2, step 5945/7134 completed (loss: 0.03040953539311886, acc: 0.9894894957542419)
[2025-02-13 04:30:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:20][root][INFO] - Training Epoch: 2/2, step 5946/7134 completed (loss: 0.03187614306807518, acc: 0.9873188138008118)
[2025-02-13 04:30:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:21][root][INFO] - Training Epoch: 2/2, step 5947/7134 completed (loss: 0.025769077241420746, acc: 0.9900867342948914)
[2025-02-13 04:30:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:21][root][INFO] - Training Epoch: 2/2, step 5948/7134 completed (loss: 0.050856657326221466, acc: 0.9800000190734863)
[2025-02-13 04:30:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:22][root][INFO] - Training Epoch: 2/2, step 5949/7134 completed (loss: 0.009548293426632881, acc: 0.9962359070777893)
[2025-02-13 04:30:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:22][root][INFO] - Training Epoch: 2/2, step 5950/7134 completed (loss: 0.05085810646414757, acc: 0.9857482314109802)
[2025-02-13 04:30:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:23][root][INFO] - Training Epoch: 2/2, step 5951/7134 completed (loss: 0.031282689422369, acc: 0.9885222315788269)
[2025-02-13 04:30:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:23][root][INFO] - Training Epoch: 2/2, step 5952/7134 completed (loss: 0.12576904892921448, acc: 0.9765625)
[2025-02-13 04:30:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:24][root][INFO] - Training Epoch: 2/2, step 5953/7134 completed (loss: 0.026184305548667908, acc: 0.9889807105064392)
[2025-02-13 04:30:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:24][root][INFO] - Training Epoch: 2/2, step 5954/7134 completed (loss: 0.007578130345791578, acc: 1.0)
[2025-02-13 04:30:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:24][root][INFO] - Training Epoch: 2/2, step 5955/7134 completed (loss: 0.00699784466996789, acc: 0.9952380657196045)
[2025-02-13 04:30:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:25][root][INFO] - Training Epoch: 2/2, step 5956/7134 completed (loss: 0.04341483861207962, acc: 0.9901960492134094)
[2025-02-13 04:30:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:25][root][INFO] - Training Epoch: 2/2, step 5957/7134 completed (loss: 0.03286822885274887, acc: 0.9925037622451782)
[2025-02-13 04:30:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:26][root][INFO] - Training Epoch: 2/2, step 5958/7134 completed (loss: 0.034714240580797195, acc: 0.9879194498062134)
[2025-02-13 04:30:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:26][root][INFO] - Training Epoch: 2/2, step 5959/7134 completed (loss: 0.03355842083692551, acc: 0.9919893145561218)
[2025-02-13 04:30:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:26][root][INFO] - Training Epoch: 2/2, step 5960/7134 completed (loss: 0.009493745863437653, acc: 0.9985954761505127)
[2025-02-13 04:30:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:27][root][INFO] - Training Epoch: 2/2, step 5961/7134 completed (loss: 0.03253909572958946, acc: 0.9897540807723999)
[2025-02-13 04:30:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:27][root][INFO] - Training Epoch: 2/2, step 5962/7134 completed (loss: 0.00927986204624176, acc: 0.995121955871582)
[2025-02-13 04:30:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:28][root][INFO] - Training Epoch: 2/2, step 5963/7134 completed (loss: 0.021402912214398384, acc: 0.994413435459137)
[2025-02-13 04:30:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:28][root][INFO] - Training Epoch: 2/2, step 5964/7134 completed (loss: 0.011577654629945755, acc: 0.9981516003608704)
[2025-02-13 04:30:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:29][root][INFO] - Training Epoch: 2/2, step 5965/7134 completed (loss: 0.028607690706849098, acc: 0.9925037622451782)
[2025-02-13 04:30:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:29][root][INFO] - Training Epoch: 2/2, step 5966/7134 completed (loss: 0.0358487106859684, acc: 0.9861303567886353)
[2025-02-13 04:30:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:29][root][INFO] - Training Epoch: 2/2, step 5967/7134 completed (loss: 0.010675473138689995, acc: 0.9970149397850037)
[2025-02-13 04:30:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:30][root][INFO] - Training Epoch: 2/2, step 5968/7134 completed (loss: 0.0223526693880558, acc: 0.9954338073730469)
[2025-02-13 04:30:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:30][root][INFO] - Training Epoch: 2/2, step 5969/7134 completed (loss: 0.02594093047082424, acc: 0.9938080310821533)
[2025-02-13 04:30:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:31][root][INFO] - Training Epoch: 2/2, step 5970/7134 completed (loss: 0.033621180802583694, acc: 0.993852436542511)
[2025-02-13 04:30:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:31][root][INFO] - Training Epoch: 2/2, step 5971/7134 completed (loss: 0.006597137078642845, acc: 0.9974937438964844)
[2025-02-13 04:30:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:32][root][INFO] - Training Epoch: 2/2, step 5972/7134 completed (loss: 0.040559373795986176, acc: 0.9911894202232361)
[2025-02-13 04:30:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:32][root][INFO] - Training Epoch: 2/2, step 5973/7134 completed (loss: 0.058763135224580765, acc: 0.9841726422309875)
[2025-02-13 04:30:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:33][root][INFO] - Training Epoch: 2/2, step 5974/7134 completed (loss: 0.009541172534227371, acc: 0.9965338110923767)
[2025-02-13 04:30:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:33][root][INFO] - Training Epoch: 2/2, step 5975/7134 completed (loss: 0.0046064648777246475, acc: 1.0)
[2025-02-13 04:30:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:33][root][INFO] - Training Epoch: 2/2, step 5976/7134 completed (loss: 0.009626571089029312, acc: 0.9984662532806396)
[2025-02-13 04:30:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:34][root][INFO] - Training Epoch: 2/2, step 5977/7134 completed (loss: 0.023563586175441742, acc: 0.9938042163848877)
[2025-02-13 04:30:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:34][root][INFO] - Training Epoch: 2/2, step 5978/7134 completed (loss: 0.022501783445477486, acc: 0.9967948794364929)
[2025-02-13 04:30:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:35][root][INFO] - Training Epoch: 2/2, step 5979/7134 completed (loss: 0.02583993226289749, acc: 0.990123450756073)
[2025-02-13 04:30:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:35][root][INFO] - Training Epoch: 2/2, step 5980/7134 completed (loss: 0.016734864562749863, acc: 0.9935979247093201)
[2025-02-13 04:30:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:35][root][INFO] - Training Epoch: 2/2, step 5981/7134 completed (loss: 0.02569909207522869, acc: 0.9965317845344543)
[2025-02-13 04:30:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:36][root][INFO] - Training Epoch: 2/2, step 5982/7134 completed (loss: 0.021841712296009064, acc: 0.9940357804298401)
[2025-02-13 04:30:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:36][root][INFO] - Training Epoch: 2/2, step 5983/7134 completed (loss: 0.031507186591625214, acc: 0.9902777671813965)
[2025-02-13 04:30:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:37][root][INFO] - Training Epoch: 2/2, step 5984/7134 completed (loss: 0.01329083926975727, acc: 0.9957447052001953)
[2025-02-13 04:30:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:37][root][INFO] - Training Epoch: 2/2, step 5985/7134 completed (loss: 0.014738984405994415, acc: 0.9974968433380127)
[2025-02-13 04:30:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:38][root][INFO] - Training Epoch: 2/2, step 5986/7134 completed (loss: 0.023958798497915268, acc: 0.9910827875137329)
[2025-02-13 04:30:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:38][root][INFO] - Training Epoch: 2/2, step 5987/7134 completed (loss: 0.04413503408432007, acc: 0.9883449673652649)
[2025-02-13 04:30:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:39][root][INFO] - Training Epoch: 2/2, step 5988/7134 completed (loss: 0.004140749108046293, acc: 0.9986666440963745)
[2025-02-13 04:30:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:39][root][INFO] - Training Epoch: 2/2, step 5989/7134 completed (loss: 0.02534501440823078, acc: 0.9926199316978455)
[2025-02-13 04:30:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:39][root][INFO] - Training Epoch: 2/2, step 5990/7134 completed (loss: 0.013339009135961533, acc: 0.9958904385566711)
[2025-02-13 04:30:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:40][root][INFO] - Training Epoch: 2/2, step 5991/7134 completed (loss: 0.058387547731399536, acc: 0.9888268113136292)
[2025-02-13 04:30:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:40][root][INFO] - Training Epoch: 2/2, step 5992/7134 completed (loss: 0.018462395295500755, acc: 0.9953542351722717)
[2025-02-13 04:30:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:41][root][INFO] - Training Epoch: 2/2, step 5993/7134 completed (loss: 0.02172682248055935, acc: 0.9930651783943176)
[2025-02-13 04:30:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:41][root][INFO] - Training Epoch: 2/2, step 5994/7134 completed (loss: 0.0062205251306295395, acc: 0.9980988502502441)
[2025-02-13 04:30:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:42][root][INFO] - Training Epoch: 2/2, step 5995/7134 completed (loss: 0.008766704238951206, acc: 0.997183084487915)
[2025-02-13 04:30:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:42][root][INFO] - Training Epoch: 2/2, step 5996/7134 completed (loss: 0.02999979443848133, acc: 0.9906103014945984)
[2025-02-13 04:30:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:42][root][INFO] - Training Epoch: 2/2, step 5997/7134 completed (loss: 0.03396998345851898, acc: 0.9946595430374146)
[2025-02-13 04:30:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:43][root][INFO] - Training Epoch: 2/2, step 5998/7134 completed (loss: 0.019984425976872444, acc: 0.9925187230110168)
[2025-02-13 04:30:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:43][root][INFO] - Training Epoch: 2/2, step 5999/7134 completed (loss: 0.01660623960196972, acc: 0.9938042163848877)
[2025-02-13 04:30:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:44][root][INFO] - Training Epoch: 2/2, step 6000/7134 completed (loss: 0.017882002517580986, acc: 0.9950082898139954)
[2025-02-13 04:30:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:44][root][INFO] - Training Epoch: 2/2, step 6001/7134 completed (loss: 0.009449968114495277, acc: 0.9961636662483215)
[2025-02-13 04:30:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:45][root][INFO] - Training Epoch: 2/2, step 6002/7134 completed (loss: 0.028902586549520493, acc: 0.9922027587890625)
[2025-02-13 04:30:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:45][root][INFO] - Training Epoch: 2/2, step 6003/7134 completed (loss: 0.020794454962015152, acc: 0.9941262602806091)
[2025-02-13 04:30:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:45][root][INFO] - Training Epoch: 2/2, step 6004/7134 completed (loss: 0.02186855487525463, acc: 0.9944367408752441)
[2025-02-13 04:30:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:46][root][INFO] - Training Epoch: 2/2, step 6005/7134 completed (loss: 0.01700456626713276, acc: 0.9923896789550781)
[2025-02-13 04:30:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:46][root][INFO] - Training Epoch: 2/2, step 6006/7134 completed (loss: 0.018342433497309685, acc: 0.9941973090171814)
[2025-02-13 04:30:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:47][root][INFO] - Training Epoch: 2/2, step 6007/7134 completed (loss: 0.010656090453267097, acc: 0.9983305335044861)
[2025-02-13 04:30:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:47][root][INFO] - Training Epoch: 2/2, step 6008/7134 completed (loss: 0.02144956775009632, acc: 0.9910979270935059)
[2025-02-13 04:30:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:48][root][INFO] - Training Epoch: 2/2, step 6009/7134 completed (loss: 0.03678184375166893, acc: 0.9922360181808472)
[2025-02-13 04:30:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:48][root][INFO] - Training Epoch: 2/2, step 6010/7134 completed (loss: 0.03642040491104126, acc: 0.9918699264526367)
[2025-02-13 04:30:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:48][root][INFO] - Training Epoch: 2/2, step 6011/7134 completed (loss: 0.028910592198371887, acc: 0.9896373152732849)
[2025-02-13 04:30:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:49][root][INFO] - Training Epoch: 2/2, step 6012/7134 completed (loss: 0.02194269932806492, acc: 0.9895522594451904)
[2025-02-13 04:30:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:49][root][INFO] - Training Epoch: 2/2, step 6013/7134 completed (loss: 0.0444410964846611, acc: 0.9893048405647278)
[2025-02-13 04:30:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:50][root][INFO] - Training Epoch: 2/2, step 6014/7134 completed (loss: 0.015531458891928196, acc: 0.9945553541183472)
[2025-02-13 04:30:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:50][root][INFO] - Training Epoch: 2/2, step 6015/7134 completed (loss: 0.04641519859433174, acc: 0.9855371713638306)
[2025-02-13 04:30:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:51][root][INFO] - Training Epoch: 2/2, step 6016/7134 completed (loss: 0.025109564885497093, acc: 0.9964157938957214)
[2025-02-13 04:30:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:51][root][INFO] - Training Epoch: 2/2, step 6017/7134 completed (loss: 0.013236787170171738, acc: 1.0)
[2025-02-13 04:30:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:51][root][INFO] - Training Epoch: 2/2, step 6018/7134 completed (loss: 0.011888176202774048, acc: 0.9947368502616882)
[2025-02-13 04:30:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:52][root][INFO] - Training Epoch: 2/2, step 6019/7134 completed (loss: 0.011561165563762188, acc: 1.0)
[2025-02-13 04:30:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:52][root][INFO] - Training Epoch: 2/2, step 6020/7134 completed (loss: 0.0016927048563957214, acc: 1.0)
[2025-02-13 04:30:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:52][root][INFO] - Training Epoch: 2/2, step 6021/7134 completed (loss: 0.006966128945350647, acc: 1.0)
[2025-02-13 04:30:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:53][root][INFO] - Training Epoch: 2/2, step 6022/7134 completed (loss: 0.01842106319963932, acc: 0.9926199316978455)
[2025-02-13 04:30:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:53][root][INFO] - Training Epoch: 2/2, step 6023/7134 completed (loss: 0.0137745076790452, acc: 0.9918699264526367)
[2025-02-13 04:30:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:53][root][INFO] - Training Epoch: 2/2, step 6024/7134 completed (loss: 0.005486874375492334, acc: 1.0)
[2025-02-13 04:30:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:54][root][INFO] - Training Epoch: 2/2, step 6025/7134 completed (loss: 0.007121038623154163, acc: 0.9979079365730286)
[2025-02-13 04:30:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:54][root][INFO] - Training Epoch: 2/2, step 6026/7134 completed (loss: 0.05621284991502762, acc: 0.9942857027053833)
[2025-02-13 04:30:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:54][root][INFO] - Training Epoch: 2/2, step 6027/7134 completed (loss: 0.007365994155406952, acc: 0.9974293112754822)
[2025-02-13 04:30:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:55][root][INFO] - Training Epoch: 2/2, step 6028/7134 completed (loss: 0.006873703561723232, acc: 1.0)
[2025-02-13 04:30:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:55][root][INFO] - Training Epoch: 2/2, step 6029/7134 completed (loss: 0.015446859411895275, acc: 0.9927797913551331)
[2025-02-13 04:30:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:55][root][INFO] - Training Epoch: 2/2, step 6030/7134 completed (loss: 0.006616800092160702, acc: 1.0)
[2025-02-13 04:30:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:56][root][INFO] - Training Epoch: 2/2, step 6031/7134 completed (loss: 0.024980779737234116, acc: 0.9912280440330505)
[2025-02-13 04:30:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:56][root][INFO] - Training Epoch: 2/2, step 6032/7134 completed (loss: 0.012364114634692669, acc: 0.9974683523178101)
[2025-02-13 04:30:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:57][root][INFO] - Training Epoch: 2/2, step 6033/7134 completed (loss: 0.053306326270103455, acc: 0.9795082211494446)
[2025-02-13 04:30:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:57][root][INFO] - Training Epoch: 2/2, step 6034/7134 completed (loss: 0.018593238666653633, acc: 0.9959016442298889)
[2025-02-13 04:30:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:57][root][INFO] - Training Epoch: 2/2, step 6035/7134 completed (loss: 0.061569731682538986, acc: 0.9824561476707458)
[2025-02-13 04:30:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:58][root][INFO] - Training Epoch: 2/2, step 6036/7134 completed (loss: 0.07542800903320312, acc: 0.9832167625427246)
[2025-02-13 04:30:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:58][root][INFO] - Training Epoch: 2/2, step 6037/7134 completed (loss: 0.020685089752078056, acc: 0.991525411605835)
[2025-02-13 04:30:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:59][root][INFO] - Training Epoch: 2/2, step 6038/7134 completed (loss: 0.01959625631570816, acc: 0.9921700358390808)
[2025-02-13 04:30:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:30:59][root][INFO] - Training Epoch: 2/2, step 6039/7134 completed (loss: 0.032285623252391815, acc: 0.9928825497627258)
[2025-02-13 04:30:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:00][root][INFO] - Training Epoch: 2/2, step 6040/7134 completed (loss: 0.04542659595608711, acc: 0.9885655045509338)
[2025-02-13 04:31:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:00][root][INFO] - Training Epoch: 2/2, step 6041/7134 completed (loss: 0.02613070420920849, acc: 0.993446946144104)
[2025-02-13 04:31:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:01][root][INFO] - Training Epoch: 2/2, step 6042/7134 completed (loss: 0.023533614352345467, acc: 0.9919999837875366)
[2025-02-13 04:31:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:01][root][INFO] - Training Epoch: 2/2, step 6043/7134 completed (loss: 0.03146862983703613, acc: 0.9916067123413086)
[2025-02-13 04:31:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:02][root][INFO] - Training Epoch: 2/2, step 6044/7134 completed (loss: 0.02739131450653076, acc: 0.9888888597488403)
[2025-02-13 04:31:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:02][root][INFO] - Training Epoch: 2/2, step 6045/7134 completed (loss: 0.03832466900348663, acc: 0.9869822263717651)
[2025-02-13 04:31:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:02][root][INFO] - Training Epoch: 2/2, step 6046/7134 completed (loss: 0.0329851433634758, acc: 0.9930675625801086)
[2025-02-13 04:31:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:03][root][INFO] - Training Epoch: 2/2, step 6047/7134 completed (loss: 0.07342277467250824, acc: 0.9814502596855164)
[2025-02-13 04:31:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:03][root][INFO] - Training Epoch: 2/2, step 6048/7134 completed (loss: 0.06492602080106735, acc: 0.9799599051475525)
[2025-02-13 04:31:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:04][root][INFO] - Training Epoch: 2/2, step 6049/7134 completed (loss: 0.009197683073580265, acc: 0.9965870380401611)
[2025-02-13 04:31:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:04][root][INFO] - Training Epoch: 2/2, step 6050/7134 completed (loss: 0.0467507429420948, acc: 0.9863201379776001)
[2025-02-13 04:31:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:05][root][INFO] - Training Epoch: 2/2, step 6051/7134 completed (loss: 0.06069701910018921, acc: 0.9793205261230469)
[2025-02-13 04:31:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:05][root][INFO] - Training Epoch: 2/2, step 6052/7134 completed (loss: 0.05700520798563957, acc: 0.9858712553977966)
[2025-02-13 04:31:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:06][root][INFO] - Training Epoch: 2/2, step 6053/7134 completed (loss: 0.045353133231401443, acc: 0.9835526347160339)
[2025-02-13 04:31:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:06][root][INFO] - Training Epoch: 2/2, step 6054/7134 completed (loss: 0.034815575927495956, acc: 0.9927536249160767)
[2025-02-13 04:31:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:06][root][INFO] - Training Epoch: 2/2, step 6055/7134 completed (loss: 0.033345647156238556, acc: 0.9905213117599487)
[2025-02-13 04:31:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:07][root][INFO] - Training Epoch: 2/2, step 6056/7134 completed (loss: 0.025060059502720833, acc: 0.9884792566299438)
[2025-02-13 04:31:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:07][root][INFO] - Training Epoch: 2/2, step 6057/7134 completed (loss: 0.017962748184800148, acc: 0.9934210777282715)
[2025-02-13 04:31:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:08][root][INFO] - Training Epoch: 2/2, step 6058/7134 completed (loss: 0.007023688871413469, acc: 0.9954954981803894)
[2025-02-13 04:31:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:08][root][INFO] - Training Epoch: 2/2, step 6059/7134 completed (loss: 0.027616530656814575, acc: 0.9893993139266968)
[2025-02-13 04:31:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:08][root][INFO] - Training Epoch: 2/2, step 6060/7134 completed (loss: 0.03032919019460678, acc: 0.9894737005233765)
[2025-02-13 04:31:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:09][root][INFO] - Training Epoch: 2/2, step 6061/7134 completed (loss: 0.041475873440504074, acc: 0.990755021572113)
[2025-02-13 04:31:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:09][root][INFO] - Training Epoch: 2/2, step 6062/7134 completed (loss: 0.051963817328214645, acc: 0.9806201457977295)
[2025-02-13 04:31:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:10][root][INFO] - Training Epoch: 2/2, step 6063/7134 completed (loss: 0.048299673944711685, acc: 0.9909583926200867)
[2025-02-13 04:31:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:10][root][INFO] - Training Epoch: 2/2, step 6064/7134 completed (loss: 0.02994009666144848, acc: 0.986975371837616)
[2025-02-13 04:31:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:10][root][INFO] - Training Epoch: 2/2, step 6065/7134 completed (loss: 0.04695833846926689, acc: 0.9866342544555664)
[2025-02-13 04:31:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:11][root][INFO] - Training Epoch: 2/2, step 6066/7134 completed (loss: 0.04173319786787033, acc: 0.9818781018257141)
[2025-02-13 04:31:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:11][root][INFO] - Training Epoch: 2/2, step 6067/7134 completed (loss: 0.034814439713954926, acc: 0.9884560108184814)
[2025-02-13 04:31:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:12][root][INFO] - Training Epoch: 2/2, step 6068/7134 completed (loss: 0.08208055049180984, acc: 0.978723406791687)
[2025-02-13 04:31:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:12][root][INFO] - Training Epoch: 2/2, step 6069/7134 completed (loss: 0.05114848166704178, acc: 0.9824817776679993)
[2025-02-13 04:31:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:13][root][INFO] - Training Epoch: 2/2, step 6070/7134 completed (loss: 0.053614210337400436, acc: 0.9786950945854187)
[2025-02-13 04:31:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:13][root][INFO] - Training Epoch: 2/2, step 6071/7134 completed (loss: 0.03972935304045677, acc: 0.9919246435165405)
[2025-02-13 04:31:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:13][root][INFO] - Training Epoch: 2/2, step 6072/7134 completed (loss: 0.10654256492853165, acc: 0.9747899174690247)
[2025-02-13 04:31:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:14][root][INFO] - Training Epoch: 2/2, step 6073/7134 completed (loss: 0.03924545645713806, acc: 0.988811194896698)
[2025-02-13 04:31:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:14][root][INFO] - Training Epoch: 2/2, step 6074/7134 completed (loss: 0.050506383180618286, acc: 0.9823529124259949)
[2025-02-13 04:31:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:15][root][INFO] - Training Epoch: 2/2, step 6075/7134 completed (loss: 0.04179650545120239, acc: 0.986810564994812)
[2025-02-13 04:31:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:15][root][INFO] - Training Epoch: 2/2, step 6076/7134 completed (loss: 0.044678784906864166, acc: 0.9856321811676025)
[2025-02-13 04:31:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:16][root][INFO] - Training Epoch: 2/2, step 6077/7134 completed (loss: 0.025073731318116188, acc: 0.9904153347015381)
[2025-02-13 04:31:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:16][root][INFO] - Training Epoch: 2/2, step 6078/7134 completed (loss: 0.04673803225159645, acc: 0.9887323975563049)
[2025-02-13 04:31:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:16][root][INFO] - Training Epoch: 2/2, step 6079/7134 completed (loss: 0.01322227530181408, acc: 0.9963503479957581)
[2025-02-13 04:31:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:17][root][INFO] - Training Epoch: 2/2, step 6080/7134 completed (loss: 0.02636697143316269, acc: 0.9943422675132751)
[2025-02-13 04:31:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:17][root][INFO] - Training Epoch: 2/2, step 6081/7134 completed (loss: 0.03376157209277153, acc: 0.9938931465148926)
[2025-02-13 04:31:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:18][root][INFO] - Training Epoch: 2/2, step 6082/7134 completed (loss: 0.012818384915590286, acc: 0.9952229261398315)
[2025-02-13 04:31:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:18][root][INFO] - Training Epoch: 2/2, step 6083/7134 completed (loss: 0.007783029228448868, acc: 0.9975429773330688)
[2025-02-13 04:31:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:19][root][INFO] - Training Epoch: 2/2, step 6084/7134 completed (loss: 0.02873380482196808, acc: 0.9968553185462952)
[2025-02-13 04:31:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:19][root][INFO] - Training Epoch: 2/2, step 6085/7134 completed (loss: 0.016143474727869034, acc: 0.9946332573890686)
[2025-02-13 04:31:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:19][root][INFO] - Training Epoch: 2/2, step 6086/7134 completed (loss: 0.015287667512893677, acc: 0.9957864880561829)
[2025-02-13 04:31:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:20][root][INFO] - Training Epoch: 2/2, step 6087/7134 completed (loss: 0.031109003350138664, acc: 0.9937106966972351)
[2025-02-13 04:31:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:20][root][INFO] - Training Epoch: 2/2, step 6088/7134 completed (loss: 0.016628291457891464, acc: 0.9959785342216492)
[2025-02-13 04:31:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:21][root][INFO] - Training Epoch: 2/2, step 6089/7134 completed (loss: 0.01878558285534382, acc: 0.9931034445762634)
[2025-02-13 04:31:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:21][root][INFO] - Training Epoch: 2/2, step 6090/7134 completed (loss: 0.017872801050543785, acc: 0.9931412935256958)
[2025-02-13 04:31:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:22][root][INFO] - Training Epoch: 2/2, step 6091/7134 completed (loss: 0.016243746504187584, acc: 0.9952229261398315)
[2025-02-13 04:31:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:22][root][INFO] - Training Epoch: 2/2, step 6092/7134 completed (loss: 0.02678278647363186, acc: 0.9936507940292358)
[2025-02-13 04:31:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:22][root][INFO] - Training Epoch: 2/2, step 6093/7134 completed (loss: 0.0331655777990818, acc: 0.9925788640975952)
[2025-02-13 04:31:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:23][root][INFO] - Training Epoch: 2/2, step 6094/7134 completed (loss: 0.019929595291614532, acc: 0.9929971694946289)
[2025-02-13 04:31:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:23][root][INFO] - Training Epoch: 2/2, step 6095/7134 completed (loss: 0.02487776428461075, acc: 0.9942528605461121)
[2025-02-13 04:31:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:24][root][INFO] - Training Epoch: 2/2, step 6096/7134 completed (loss: 0.03468494489789009, acc: 0.9904534816741943)
[2025-02-13 04:31:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:24][root][INFO] - Training Epoch: 2/2, step 6097/7134 completed (loss: 0.026804916560649872, acc: 0.9916897416114807)
[2025-02-13 04:31:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:25][root][INFO] - Training Epoch: 2/2, step 6098/7134 completed (loss: 0.024997875094413757, acc: 0.993565022945404)
[2025-02-13 04:31:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:25][root][INFO] - Training Epoch: 2/2, step 6099/7134 completed (loss: 0.05136163905262947, acc: 0.9847095012664795)
[2025-02-13 04:31:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:25][root][INFO] - Training Epoch: 2/2, step 6100/7134 completed (loss: 0.04860668256878853, acc: 0.9878048896789551)
[2025-02-13 04:31:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:26][root][INFO] - Training Epoch: 2/2, step 6101/7134 completed (loss: 0.011162819340825081, acc: 0.9968051314353943)
[2025-02-13 04:31:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:26][root][INFO] - Training Epoch: 2/2, step 6102/7134 completed (loss: 0.03144404664635658, acc: 0.9947183132171631)
[2025-02-13 04:31:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:27][root][INFO] - Training Epoch: 2/2, step 6103/7134 completed (loss: 0.01985321007668972, acc: 0.9944444298744202)
[2025-02-13 04:31:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:27][root][INFO] - Training Epoch: 2/2, step 6104/7134 completed (loss: 0.03832051530480385, acc: 0.9822221994400024)
[2025-02-13 04:31:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:27][root][INFO] - Training Epoch: 2/2, step 6105/7134 completed (loss: 0.04997353255748749, acc: 0.9866071343421936)
[2025-02-13 04:31:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:28][root][INFO] - Training Epoch: 2/2, step 6106/7134 completed (loss: 0.03856411203742027, acc: 0.9856887459754944)
[2025-02-13 04:31:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:28][root][INFO] - Training Epoch: 2/2, step 6107/7134 completed (loss: 0.036709219217300415, acc: 0.9894958138465881)
[2025-02-13 04:31:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:29][root][INFO] - Training Epoch: 2/2, step 6108/7134 completed (loss: 0.030466260388493538, acc: 0.9927140474319458)
[2025-02-13 04:31:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:29][root][INFO] - Training Epoch: 2/2, step 6109/7134 completed (loss: 0.007288303691893816, acc: 0.9957627058029175)
[2025-02-13 04:31:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:29][root][INFO] - Training Epoch: 2/2, step 6110/7134 completed (loss: 0.05779166519641876, acc: 0.9786324501037598)
[2025-02-13 04:31:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:30][root][INFO] - Training Epoch: 2/2, step 6111/7134 completed (loss: 0.030105900019407272, acc: 0.989276111125946)
[2025-02-13 04:31:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:30][root][INFO] - Training Epoch: 2/2, step 6112/7134 completed (loss: 0.0208150502294302, acc: 0.9933110475540161)
[2025-02-13 04:31:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:31][root][INFO] - Training Epoch: 2/2, step 6113/7134 completed (loss: 0.01263391226530075, acc: 0.9969651103019714)
[2025-02-13 04:31:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:31][root][INFO] - Training Epoch: 2/2, step 6114/7134 completed (loss: 0.04550861194729805, acc: 0.9917355179786682)
[2025-02-13 04:31:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:31][root][INFO] - Training Epoch: 2/2, step 6115/7134 completed (loss: 0.027440043166279793, acc: 0.992337167263031)
[2025-02-13 04:31:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:32][root][INFO] - Training Epoch: 2/2, step 6116/7134 completed (loss: 0.04586875066161156, acc: 0.9901185631752014)
[2025-02-13 04:31:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:32][root][INFO] - Training Epoch: 2/2, step 6117/7134 completed (loss: 0.04958917945623398, acc: 0.9904371500015259)
[2025-02-13 04:31:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:33][root][INFO] - Training Epoch: 2/2, step 6118/7134 completed (loss: 0.03559408336877823, acc: 0.9852941036224365)
[2025-02-13 04:31:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:33][root][INFO] - Training Epoch: 2/2, step 6119/7134 completed (loss: 0.02806219831109047, acc: 0.989195704460144)
[2025-02-13 04:31:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:34][root][INFO] - Training Epoch: 2/2, step 6120/7134 completed (loss: 0.028183219954371452, acc: 0.9964221715927124)
[2025-02-13 04:31:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:34][root][INFO] - Training Epoch: 2/2, step 6121/7134 completed (loss: 0.04569801688194275, acc: 0.9874100685119629)
[2025-02-13 04:31:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:34][root][INFO] - Training Epoch: 2/2, step 6122/7134 completed (loss: 0.01268700510263443, acc: 0.99622642993927)
[2025-02-13 04:31:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:35][root][INFO] - Training Epoch: 2/2, step 6123/7134 completed (loss: 0.06989925354719162, acc: 0.9812889695167542)
[2025-02-13 04:31:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:35][root][INFO] - Training Epoch: 2/2, step 6124/7134 completed (loss: 0.021543918177485466, acc: 0.9923076629638672)
[2025-02-13 04:31:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:36][root][INFO] - Training Epoch: 2/2, step 6125/7134 completed (loss: 0.09854776412248611, acc: 0.9783132672309875)
[2025-02-13 04:31:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:36][root][INFO] - Training Epoch: 2/2, step 6126/7134 completed (loss: 0.04856329783797264, acc: 0.9843260049819946)
[2025-02-13 04:31:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:36][root][INFO] - Training Epoch: 2/2, step 6127/7134 completed (loss: 0.012712504714727402, acc: 0.9952380657196045)
[2025-02-13 04:31:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:37][root][INFO] - Training Epoch: 2/2, step 6128/7134 completed (loss: 0.024970408529043198, acc: 0.9878542423248291)
[2025-02-13 04:31:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:37][root][INFO] - Training Epoch: 2/2, step 6129/7134 completed (loss: 0.03798240050673485, acc: 0.9845288395881653)
[2025-02-13 04:31:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:38][root][INFO] - Training Epoch: 2/2, step 6130/7134 completed (loss: 0.021546615287661552, acc: 0.9928315281867981)
[2025-02-13 04:31:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:38][root][INFO] - Training Epoch: 2/2, step 6131/7134 completed (loss: 0.03997421637177467, acc: 0.9880239367485046)
[2025-02-13 04:31:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:39][root][INFO] - Training Epoch: 2/2, step 6132/7134 completed (loss: 0.027719199657440186, acc: 0.987500011920929)
[2025-02-13 04:31:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:39][root][INFO] - Training Epoch: 2/2, step 6133/7134 completed (loss: 0.04648813605308533, acc: 0.987864077091217)
[2025-02-13 04:31:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:39][root][INFO] - Training Epoch: 2/2, step 6134/7134 completed (loss: 0.05426650494337082, acc: 0.9831546545028687)
[2025-02-13 04:31:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:40][root][INFO] - Training Epoch: 2/2, step 6135/7134 completed (loss: 0.011369201354682446, acc: 0.996515691280365)
[2025-02-13 04:31:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:40][root][INFO] - Training Epoch: 2/2, step 6136/7134 completed (loss: 0.0434996671974659, acc: 0.994397759437561)
[2025-02-13 04:31:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:41][root][INFO] - Training Epoch: 2/2, step 6137/7134 completed (loss: 0.07175539433956146, acc: 0.9859594106674194)
[2025-02-13 04:31:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:41][root][INFO] - Training Epoch: 2/2, step 6138/7134 completed (loss: 0.05374228209257126, acc: 0.9892473220825195)
[2025-02-13 04:31:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:41][root][INFO] - Training Epoch: 2/2, step 6139/7134 completed (loss: 0.050218366086483, acc: 0.9870370626449585)
[2025-02-13 04:31:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:42][root][INFO] - Training Epoch: 2/2, step 6140/7134 completed (loss: 0.03391631320118904, acc: 0.9929328560829163)
[2025-02-13 04:31:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:42][root][INFO] - Training Epoch: 2/2, step 6141/7134 completed (loss: 0.045502059161663055, acc: 0.9938837885856628)
[2025-02-13 04:31:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:43][root][INFO] - Training Epoch: 2/2, step 6142/7134 completed (loss: 0.033365532755851746, acc: 0.9870503544807434)
[2025-02-13 04:31:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:43][root][INFO] - Training Epoch: 2/2, step 6143/7134 completed (loss: 0.024288242682814598, acc: 0.9918919205665588)
[2025-02-13 04:31:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:44][root][INFO] - Training Epoch: 2/2, step 6144/7134 completed (loss: 0.02922866865992546, acc: 0.9899159669876099)
[2025-02-13 04:31:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:44][root][INFO] - Training Epoch: 2/2, step 6145/7134 completed (loss: 0.03984789550304413, acc: 0.9894319772720337)
[2025-02-13 04:31:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:44][root][INFO] - Training Epoch: 2/2, step 6146/7134 completed (loss: 0.051911722868680954, acc: 0.9834087491035461)
[2025-02-13 04:31:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:45][root][INFO] - Training Epoch: 2/2, step 6147/7134 completed (loss: 0.04762512445449829, acc: 0.9900793433189392)
[2025-02-13 04:31:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:45][root][INFO] - Training Epoch: 2/2, step 6148/7134 completed (loss: 0.036533500999212265, acc: 0.981574535369873)
[2025-02-13 04:31:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:46][root][INFO] - Training Epoch: 2/2, step 6149/7134 completed (loss: 0.03315231576561928, acc: 0.9903537034988403)
[2025-02-13 04:31:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:46][root][INFO] - Training Epoch: 2/2, step 6150/7134 completed (loss: 0.01708035357296467, acc: 0.9947368502616882)
[2025-02-13 04:31:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:47][root][INFO] - Training Epoch: 2/2, step 6151/7134 completed (loss: 0.014537203125655651, acc: 0.9937205910682678)
[2025-02-13 04:31:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:47][root][INFO] - Training Epoch: 2/2, step 6152/7134 completed (loss: 0.027223989367485046, acc: 0.9924585223197937)
[2025-02-13 04:31:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:47][root][INFO] - Training Epoch: 2/2, step 6153/7134 completed (loss: 0.01749148592352867, acc: 0.9956584572792053)
[2025-02-13 04:31:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:48][root][INFO] - Training Epoch: 2/2, step 6154/7134 completed (loss: 0.04095146432518959, acc: 0.9888424277305603)
[2025-02-13 04:31:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:48][root][INFO] - Training Epoch: 2/2, step 6155/7134 completed (loss: 0.031091663986444473, acc: 0.9896142482757568)
[2025-02-13 04:31:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:49][root][INFO] - Training Epoch: 2/2, step 6156/7134 completed (loss: 0.012952706776559353, acc: 0.995245635509491)
[2025-02-13 04:31:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:49][root][INFO] - Training Epoch: 2/2, step 6157/7134 completed (loss: 0.02304903417825699, acc: 0.9937304258346558)
[2025-02-13 04:31:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:50][root][INFO] - Training Epoch: 2/2, step 6158/7134 completed (loss: 0.0633397251367569, acc: 0.9855334758758545)
[2025-02-13 04:31:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:50][root][INFO] - Training Epoch: 2/2, step 6159/7134 completed (loss: 0.036074068397283554, acc: 0.9903498291969299)
[2025-02-13 04:31:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:50][root][INFO] - Training Epoch: 2/2, step 6160/7134 completed (loss: 0.0646149069070816, acc: 0.9848484992980957)
[2025-02-13 04:31:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:51][root][INFO] - Training Epoch: 2/2, step 6161/7134 completed (loss: 0.03687712922692299, acc: 0.9912126660346985)
[2025-02-13 04:31:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:51][root][INFO] - Training Epoch: 2/2, step 6162/7134 completed (loss: 0.03807666897773743, acc: 0.9918166995048523)
[2025-02-13 04:31:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:52][root][INFO] - Training Epoch: 2/2, step 6163/7134 completed (loss: 0.08881636708974838, acc: 0.9767857193946838)
[2025-02-13 04:31:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:52][root][INFO] - Training Epoch: 2/2, step 6164/7134 completed (loss: 0.06958327442407608, acc: 0.977337121963501)
[2025-02-13 04:31:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:53][root][INFO] - Training Epoch: 2/2, step 6165/7134 completed (loss: 0.0320785827934742, acc: 0.9889298677444458)
[2025-02-13 04:31:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:53][root][INFO] - Training Epoch: 2/2, step 6166/7134 completed (loss: 0.026616455987095833, acc: 0.9929701089859009)
[2025-02-13 04:31:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:53][root][INFO] - Training Epoch: 2/2, step 6167/7134 completed (loss: 0.01345397625118494, acc: 0.9929278492927551)
[2025-02-13 04:31:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:54][root][INFO] - Training Epoch: 2/2, step 6168/7134 completed (loss: 0.025701528415083885, acc: 0.992277979850769)
[2025-02-13 04:31:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:54][root][INFO] - Training Epoch: 2/2, step 6169/7134 completed (loss: 0.03732311725616455, acc: 0.991253674030304)
[2025-02-13 04:31:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:55][root][INFO] - Training Epoch: 2/2, step 6170/7134 completed (loss: 0.021679291501641273, acc: 0.9943289160728455)
[2025-02-13 04:31:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:55][root][INFO] - Training Epoch: 2/2, step 6171/7134 completed (loss: 0.012399949133396149, acc: 0.9934297204017639)
[2025-02-13 04:31:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:55][root][INFO] - Training Epoch: 2/2, step 6172/7134 completed (loss: 0.013287385925650597, acc: 0.9939879775047302)
[2025-02-13 04:31:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:56][root][INFO] - Training Epoch: 2/2, step 6173/7134 completed (loss: 0.006292513106018305, acc: 0.9985272288322449)
[2025-02-13 04:31:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:56][root][INFO] - Training Epoch: 2/2, step 6174/7134 completed (loss: 0.005340046714991331, acc: 1.0)
[2025-02-13 04:31:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:57][root][INFO] - Training Epoch: 2/2, step 6175/7134 completed (loss: 0.033200886100530624, acc: 0.9911242723464966)
[2025-02-13 04:31:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:57][root][INFO] - Training Epoch: 2/2, step 6176/7134 completed (loss: 0.02159624546766281, acc: 0.9913169145584106)
[2025-02-13 04:31:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:58][root][INFO] - Training Epoch: 2/2, step 6177/7134 completed (loss: 0.011627462692558765, acc: 0.9961636662483215)
[2025-02-13 04:31:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:58][root][INFO] - Training Epoch: 2/2, step 6178/7134 completed (loss: 0.00959350261837244, acc: 0.9970501661300659)
[2025-02-13 04:31:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:58][root][INFO] - Training Epoch: 2/2, step 6179/7134 completed (loss: 0.03938344866037369, acc: 0.9922360181808472)
[2025-02-13 04:31:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:59][root][INFO] - Training Epoch: 2/2, step 6180/7134 completed (loss: 0.09349463135004044, acc: 0.9745222926139832)
[2025-02-13 04:31:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:31:59][root][INFO] - Training Epoch: 2/2, step 6181/7134 completed (loss: 0.12896357476711273, acc: 0.9628099203109741)
[2025-02-13 04:31:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:00][root][INFO] - Training Epoch: 2/2, step 6182/7134 completed (loss: 0.02675141580402851, acc: 0.994434118270874)
[2025-02-13 04:32:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:00][root][INFO] - Training Epoch: 2/2, step 6183/7134 completed (loss: 0.042340993881225586, acc: 0.9877488613128662)
[2025-02-13 04:32:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:00][root][INFO] - Training Epoch: 2/2, step 6184/7134 completed (loss: 0.08036515861749649, acc: 0.9717742204666138)
[2025-02-13 04:32:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:01][root][INFO] - Training Epoch: 2/2, step 6185/7134 completed (loss: 0.026136839762330055, acc: 0.9871323704719543)
[2025-02-13 04:32:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:01][root][INFO] - Training Epoch: 2/2, step 6186/7134 completed (loss: 0.021250618621706963, acc: 0.9912587404251099)
[2025-02-13 04:32:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:02][root][INFO] - Training Epoch: 2/2, step 6187/7134 completed (loss: 0.02083454094827175, acc: 0.9960317611694336)
[2025-02-13 04:32:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:02][root][INFO] - Training Epoch: 2/2, step 6188/7134 completed (loss: 0.036983467638492584, acc: 0.9915373921394348)
[2025-02-13 04:32:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:03][root][INFO] - Training Epoch: 2/2, step 6189/7134 completed (loss: 0.05807012319564819, acc: 0.9841269850730896)
[2025-02-13 04:32:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:03][root][INFO] - Training Epoch: 2/2, step 6190/7134 completed (loss: 0.04302883520722389, acc: 0.9867109656333923)
[2025-02-13 04:32:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:03][root][INFO] - Training Epoch: 2/2, step 6191/7134 completed (loss: 0.044642411172389984, acc: 0.9877622127532959)
[2025-02-13 04:32:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:04][root][INFO] - Training Epoch: 2/2, step 6192/7134 completed (loss: 0.017283182591199875, acc: 0.9935691356658936)
[2025-02-13 04:32:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:04][root][INFO] - Training Epoch: 2/2, step 6193/7134 completed (loss: 0.03611375764012337, acc: 0.9881094098091125)
[2025-02-13 04:32:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:05][root][INFO] - Training Epoch: 2/2, step 6194/7134 completed (loss: 0.013989539816975594, acc: 0.994163453578949)
[2025-02-13 04:32:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:05][root][INFO] - Training Epoch: 2/2, step 6195/7134 completed (loss: 0.019399821758270264, acc: 0.9904305934906006)
[2025-02-13 04:32:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:06][root][INFO] - Training Epoch: 2/2, step 6196/7134 completed (loss: 0.023816274479031563, acc: 0.9917355179786682)
[2025-02-13 04:32:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:06][root][INFO] - Training Epoch: 2/2, step 6197/7134 completed (loss: 0.039399873465299606, acc: 0.9862068891525269)
[2025-02-13 04:32:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:06][root][INFO] - Training Epoch: 2/2, step 6198/7134 completed (loss: 0.025987930595874786, acc: 0.9937759041786194)
[2025-02-13 04:32:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:07][root][INFO] - Training Epoch: 2/2, step 6199/7134 completed (loss: 0.035421550273895264, acc: 0.988399088382721)
[2025-02-13 04:32:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:07][root][INFO] - Training Epoch: 2/2, step 6200/7134 completed (loss: 0.022085901349782944, acc: 0.992682933807373)
[2025-02-13 04:32:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:08][root][INFO] - Training Epoch: 2/2, step 6201/7134 completed (loss: 0.02539094164967537, acc: 0.9921962022781372)
[2025-02-13 04:32:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:08][root][INFO] - Training Epoch: 2/2, step 6202/7134 completed (loss: 0.024845264852046967, acc: 0.9913232326507568)
[2025-02-13 04:32:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:09][root][INFO] - Training Epoch: 2/2, step 6203/7134 completed (loss: 0.00984786543995142, acc: 0.9974489808082581)
[2025-02-13 04:32:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:09][root][INFO] - Training Epoch: 2/2, step 6204/7134 completed (loss: 0.0071990713477134705, acc: 0.9980353713035583)
[2025-02-13 04:32:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:09][root][INFO] - Training Epoch: 2/2, step 6205/7134 completed (loss: 0.018140258267521858, acc: 0.9965870380401611)
[2025-02-13 04:32:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:10][root][INFO] - Training Epoch: 2/2, step 6206/7134 completed (loss: 0.020604168996214867, acc: 0.9942113161087036)
[2025-02-13 04:32:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:10][root][INFO] - Training Epoch: 2/2, step 6207/7134 completed (loss: 0.102487713098526, acc: 0.9746328592300415)
[2025-02-13 04:32:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:11][root][INFO] - Training Epoch: 2/2, step 6208/7134 completed (loss: 0.02335505560040474, acc: 0.9918981194496155)
[2025-02-13 04:32:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:11][root][INFO] - Training Epoch: 2/2, step 6209/7134 completed (loss: 0.04169231653213501, acc: 0.9883570671081543)
[2025-02-13 04:32:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:12][root][INFO] - Training Epoch: 2/2, step 6210/7134 completed (loss: 0.03307555243372917, acc: 0.9901098608970642)
[2025-02-13 04:32:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:12][root][INFO] - Training Epoch: 2/2, step 6211/7134 completed (loss: 0.02690337784588337, acc: 0.9932340979576111)
[2025-02-13 04:32:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:13][root][INFO] - Training Epoch: 2/2, step 6212/7134 completed (loss: 0.009370631538331509, acc: 0.9964912533760071)
[2025-02-13 04:32:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:13][root][INFO] - Training Epoch: 2/2, step 6213/7134 completed (loss: 0.012819611467421055, acc: 0.9979209899902344)
[2025-02-13 04:32:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:13][root][INFO] - Training Epoch: 2/2, step 6214/7134 completed (loss: 0.007897892035543919, acc: 1.0)
[2025-02-13 04:32:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:14][root][INFO] - Training Epoch: 2/2, step 6215/7134 completed (loss: 0.02260165847837925, acc: 0.9953917264938354)
[2025-02-13 04:32:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:14][root][INFO] - Training Epoch: 2/2, step 6216/7134 completed (loss: 0.009983069263398647, acc: 0.9957447052001953)
[2025-02-13 04:32:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:15][root][INFO] - Training Epoch: 2/2, step 6217/7134 completed (loss: 0.013229007832705975, acc: 0.9946308732032776)
[2025-02-13 04:32:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:15][root][INFO] - Training Epoch: 2/2, step 6218/7134 completed (loss: 0.031029466539621353, acc: 0.9893292784690857)
[2025-02-13 04:32:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:15][root][INFO] - Training Epoch: 2/2, step 6219/7134 completed (loss: 0.024477912113070488, acc: 0.9929676651954651)
[2025-02-13 04:32:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:16][root][INFO] - Training Epoch: 2/2, step 6220/7134 completed (loss: 0.03366978466510773, acc: 0.9904761910438538)
[2025-02-13 04:32:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:16][root][INFO] - Training Epoch: 2/2, step 6221/7134 completed (loss: 0.014138022437691689, acc: 0.9947368502616882)
[2025-02-13 04:32:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:17][root][INFO] - Training Epoch: 2/2, step 6222/7134 completed (loss: 0.04974076524376869, acc: 0.9863387942314148)
[2025-02-13 04:32:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:17][root][INFO] - Training Epoch: 2/2, step 6223/7134 completed (loss: 0.02779175527393818, acc: 0.9926739931106567)
[2025-02-13 04:32:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:18][root][INFO] - Training Epoch: 2/2, step 6224/7134 completed (loss: 0.00677289254963398, acc: 0.9970972537994385)
[2025-02-13 04:32:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:18][root][INFO] - Training Epoch: 2/2, step 6225/7134 completed (loss: 0.017960112541913986, acc: 0.9936224222183228)
[2025-02-13 04:32:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:18][root][INFO] - Training Epoch: 2/2, step 6226/7134 completed (loss: 0.02957170642912388, acc: 0.9908257126808167)
[2025-02-13 04:32:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:19][root][INFO] - Training Epoch: 2/2, step 6227/7134 completed (loss: 0.004370453301817179, acc: 1.0)
[2025-02-13 04:32:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:19][root][INFO] - Training Epoch: 2/2, step 6228/7134 completed (loss: 0.022457946091890335, acc: 0.9889435172080994)
[2025-02-13 04:32:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:20][root][INFO] - Training Epoch: 2/2, step 6229/7134 completed (loss: 0.04859325289726257, acc: 0.988399088382721)
[2025-02-13 04:32:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:20][root][INFO] - Training Epoch: 2/2, step 6230/7134 completed (loss: 0.04742524400353432, acc: 0.9900249242782593)
[2025-02-13 04:32:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:21][root][INFO] - Training Epoch: 2/2, step 6231/7134 completed (loss: 0.01328916847705841, acc: 0.9961240291595459)
[2025-02-13 04:32:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:21][root][INFO] - Training Epoch: 2/2, step 6232/7134 completed (loss: 0.020736124366521835, acc: 0.9924242496490479)
[2025-02-13 04:32:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:21][root][INFO] - Training Epoch: 2/2, step 6233/7134 completed (loss: 0.023297708481550217, acc: 0.9919571280479431)
[2025-02-13 04:32:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:22][root][INFO] - Training Epoch: 2/2, step 6234/7134 completed (loss: 0.058490343391895294, acc: 0.9799465537071228)
[2025-02-13 04:32:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:22][root][INFO] - Training Epoch: 2/2, step 6235/7134 completed (loss: 0.013750187121331692, acc: 0.9973924160003662)
[2025-02-13 04:32:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:23][root][INFO] - Training Epoch: 2/2, step 6236/7134 completed (loss: 0.021845035254955292, acc: 0.9959072470664978)
[2025-02-13 04:32:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:23][root][INFO] - Training Epoch: 2/2, step 6237/7134 completed (loss: 0.020692948251962662, acc: 0.9923547506332397)
[2025-02-13 04:32:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:24][root][INFO] - Training Epoch: 2/2, step 6238/7134 completed (loss: 0.0063184890896081924, acc: 0.9983844757080078)
[2025-02-13 04:32:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:24][root][INFO] - Training Epoch: 2/2, step 6239/7134 completed (loss: 0.029269155114889145, acc: 0.9940263032913208)
[2025-02-13 04:32:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:25][root][INFO] - Training Epoch: 2/2, step 6240/7134 completed (loss: 0.007766101974993944, acc: 0.995768666267395)
[2025-02-13 04:32:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:25][root][INFO] - Training Epoch: 2/2, step 6241/7134 completed (loss: 0.006275154184550047, acc: 0.9984177350997925)
[2025-02-13 04:32:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:25][root][INFO] - Training Epoch: 2/2, step 6242/7134 completed (loss: 0.009308316744863987, acc: 0.9965986609458923)
[2025-02-13 04:32:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:26][root][INFO] - Training Epoch: 2/2, step 6243/7134 completed (loss: 0.018649956211447716, acc: 0.993630588054657)
[2025-02-13 04:32:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:26][root][INFO] - Training Epoch: 2/2, step 6244/7134 completed (loss: 0.017155209556221962, acc: 0.9931412935256958)
[2025-02-13 04:32:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:27][root][INFO] - Training Epoch: 2/2, step 6245/7134 completed (loss: 0.02258210815489292, acc: 0.99726402759552)
[2025-02-13 04:32:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:27][root][INFO] - Training Epoch: 2/2, step 6246/7134 completed (loss: 0.020931553095579147, acc: 0.990777313709259)
[2025-02-13 04:32:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:28][root][INFO] - Training Epoch: 2/2, step 6247/7134 completed (loss: 0.030972277745604515, acc: 0.9867469668388367)
[2025-02-13 04:32:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:28][root][INFO] - Training Epoch: 2/2, step 6248/7134 completed (loss: 0.04989761859178543, acc: 0.985981285572052)
[2025-02-13 04:32:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:28][root][INFO] - Training Epoch: 2/2, step 6249/7134 completed (loss: 0.006084021646529436, acc: 0.9968553185462952)
[2025-02-13 04:32:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:29][root][INFO] - Training Epoch: 2/2, step 6250/7134 completed (loss: 0.0021150384563952684, acc: 1.0)
[2025-02-13 04:32:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:29][root][INFO] - Training Epoch: 2/2, step 6251/7134 completed (loss: 0.018554028123617172, acc: 0.995398759841919)
[2025-02-13 04:32:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:30][root][INFO] - Training Epoch: 2/2, step 6252/7134 completed (loss: 0.005375150591135025, acc: 0.9983818531036377)
[2025-02-13 04:32:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:30][root][INFO] - Training Epoch: 2/2, step 6253/7134 completed (loss: 0.009888687171041965, acc: 0.9965517520904541)
[2025-02-13 04:32:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:30][root][INFO] - Training Epoch: 2/2, step 6254/7134 completed (loss: 0.01618855819106102, acc: 0.9948096871376038)
[2025-02-13 04:32:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:31][root][INFO] - Training Epoch: 2/2, step 6255/7134 completed (loss: 0.019637703895568848, acc: 0.9932249188423157)
[2025-02-13 04:32:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:31][root][INFO] - Training Epoch: 2/2, step 6256/7134 completed (loss: 0.017333723604679108, acc: 0.9929078221321106)
[2025-02-13 04:32:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:32][root][INFO] - Training Epoch: 2/2, step 6257/7134 completed (loss: 0.012803668156266212, acc: 0.9928057789802551)
[2025-02-13 04:32:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:32][root][INFO] - Training Epoch: 2/2, step 6258/7134 completed (loss: 0.009196850471198559, acc: 0.9936000108718872)
[2025-02-13 04:32:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:32][root][INFO] - Training Epoch: 2/2, step 6259/7134 completed (loss: 0.011839806102216244, acc: 0.9954954981803894)
[2025-02-13 04:32:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:33][root][INFO] - Training Epoch: 2/2, step 6260/7134 completed (loss: 0.016161346808075905, acc: 0.994966447353363)
[2025-02-13 04:32:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:33][root][INFO] - Training Epoch: 2/2, step 6261/7134 completed (loss: 0.014057902619242668, acc: 0.9969512224197388)
[2025-02-13 04:32:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:34][root][INFO] - Training Epoch: 2/2, step 6262/7134 completed (loss: 0.008525394834578037, acc: 0.9971305727958679)
[2025-02-13 04:32:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:34][root][INFO] - Training Epoch: 2/2, step 6263/7134 completed (loss: 0.0026193191297352314, acc: 1.0)
[2025-02-13 04:32:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:35][root][INFO] - Training Epoch: 2/2, step 6264/7134 completed (loss: 0.007299949415028095, acc: 0.9950739145278931)
[2025-02-13 04:32:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:35][root][INFO] - Training Epoch: 2/2, step 6265/7134 completed (loss: 0.009571925736963749, acc: 0.9970414042472839)
[2025-02-13 04:32:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:35][root][INFO] - Training Epoch: 2/2, step 6266/7134 completed (loss: 0.006769243162125349, acc: 0.998251736164093)
[2025-02-13 04:32:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:36][root][INFO] - Training Epoch: 2/2, step 6267/7134 completed (loss: 0.012364055030047894, acc: 0.9983713626861572)
[2025-02-13 04:32:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:36][root][INFO] - Training Epoch: 2/2, step 6268/7134 completed (loss: 0.02178654447197914, acc: 0.9970238208770752)
[2025-02-13 04:32:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:37][root][INFO] - Training Epoch: 2/2, step 6269/7134 completed (loss: 0.012284005992114544, acc: 0.9978678226470947)
[2025-02-13 04:32:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:37][root][INFO] - Training Epoch: 2/2, step 6270/7134 completed (loss: 0.002864518668502569, acc: 1.0)
[2025-02-13 04:32:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:37][root][INFO] - Training Epoch: 2/2, step 6271/7134 completed (loss: 0.016172317788004875, acc: 0.9986631274223328)
[2025-02-13 04:32:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:38][root][INFO] - Training Epoch: 2/2, step 6272/7134 completed (loss: 0.0028667920269072056, acc: 0.9985315799713135)
[2025-02-13 04:32:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:38][root][INFO] - Training Epoch: 2/2, step 6273/7134 completed (loss: 0.006695652846246958, acc: 0.99863201379776)
[2025-02-13 04:32:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:39][root][INFO] - Training Epoch: 2/2, step 6274/7134 completed (loss: 0.007538941223174334, acc: 0.9985734820365906)
[2025-02-13 04:32:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:39][root][INFO] - Training Epoch: 2/2, step 6275/7134 completed (loss: 0.018837371841073036, acc: 0.9939117431640625)
[2025-02-13 04:32:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:40][root][INFO] - Training Epoch: 2/2, step 6276/7134 completed (loss: 0.007350133266299963, acc: 0.996497392654419)
[2025-02-13 04:32:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:40][root][INFO] - Training Epoch: 2/2, step 6277/7134 completed (loss: 0.03437498211860657, acc: 0.9886877536773682)
[2025-02-13 04:32:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:40][root][INFO] - Training Epoch: 2/2, step 6278/7134 completed (loss: 0.005127505864948034, acc: 0.9983552694320679)
[2025-02-13 04:32:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:41][root][INFO] - Training Epoch: 2/2, step 6279/7134 completed (loss: 0.003137400606647134, acc: 1.0)
[2025-02-13 04:32:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:41][root][INFO] - Training Epoch: 2/2, step 6280/7134 completed (loss: 0.011148008517920971, acc: 0.9947643876075745)
[2025-02-13 04:32:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:42][root][INFO] - Training Epoch: 2/2, step 6281/7134 completed (loss: 0.015270804986357689, acc: 0.9948006868362427)
[2025-02-13 04:32:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:42][root][INFO] - Training Epoch: 2/2, step 6282/7134 completed (loss: 0.0038847592659294605, acc: 0.99842768907547)
[2025-02-13 04:32:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:42][root][INFO] - Training Epoch: 2/2, step 6283/7134 completed (loss: 0.017024792730808258, acc: 0.9956331849098206)
[2025-02-13 04:32:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:43][root][INFO] - Training Epoch: 2/2, step 6284/7134 completed (loss: 0.025496458634734154, acc: 0.9890590906143188)
[2025-02-13 04:32:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:43][root][INFO] - Training Epoch: 2/2, step 6285/7134 completed (loss: 0.013282833620905876, acc: 0.9947506785392761)
[2025-02-13 04:32:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:44][root][INFO] - Training Epoch: 2/2, step 6286/7134 completed (loss: 0.02706284634768963, acc: 0.9950658082962036)
[2025-02-13 04:32:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:44][root][INFO] - Training Epoch: 2/2, step 6287/7134 completed (loss: 0.02123289555311203, acc: 0.9944853186607361)
[2025-02-13 04:32:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:44][root][INFO] - Training Epoch: 2/2, step 6288/7134 completed (loss: 0.0063134729862213135, acc: 0.9983659982681274)
[2025-02-13 04:32:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:45][root][INFO] - Training Epoch: 2/2, step 6289/7134 completed (loss: 0.01769818551838398, acc: 0.9927927851676941)
[2025-02-13 04:32:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:45][root][INFO] - Training Epoch: 2/2, step 6290/7134 completed (loss: 0.012811158783733845, acc: 0.9983999729156494)
[2025-02-13 04:32:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:46][root][INFO] - Training Epoch: 2/2, step 6291/7134 completed (loss: 0.0038620675913989544, acc: 1.0)
[2025-02-13 04:32:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:46][root][INFO] - Training Epoch: 2/2, step 6292/7134 completed (loss: 0.0235622338950634, acc: 0.9886105060577393)
[2025-02-13 04:32:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:46][root][INFO] - Training Epoch: 2/2, step 6293/7134 completed (loss: 0.03767605125904083, acc: 0.988054633140564)
[2025-02-13 04:32:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:47][root][INFO] - Training Epoch: 2/2, step 6294/7134 completed (loss: 0.016927318647503853, acc: 0.9951377511024475)
[2025-02-13 04:32:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:47][root][INFO] - Training Epoch: 2/2, step 6295/7134 completed (loss: 0.015081110410392284, acc: 0.994575023651123)
[2025-02-13 04:32:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:48][root][INFO] - Training Epoch: 2/2, step 6296/7134 completed (loss: 0.047097720205783844, acc: 0.9881889820098877)
[2025-02-13 04:32:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:48][root][INFO] - Training Epoch: 2/2, step 6297/7134 completed (loss: 0.02450563758611679, acc: 0.9885246157646179)
[2025-02-13 04:32:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:49][root][INFO] - Training Epoch: 2/2, step 6298/7134 completed (loss: 0.027716385200619698, acc: 0.9914089441299438)
[2025-02-13 04:32:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:49][root][INFO] - Training Epoch: 2/2, step 6299/7134 completed (loss: 0.03980659320950508, acc: 0.9872204661369324)
[2025-02-13 04:32:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:49][root][INFO] - Training Epoch: 2/2, step 6300/7134 completed (loss: 0.03129343315958977, acc: 0.9899497628211975)
[2025-02-13 04:32:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:50][root][INFO] - Training Epoch: 2/2, step 6301/7134 completed (loss: 0.006501437164843082, acc: 0.9988505840301514)
[2025-02-13 04:32:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:50][root][INFO] - Training Epoch: 2/2, step 6302/7134 completed (loss: 0.005690505262464285, acc: 1.0)
[2025-02-13 04:32:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:51][root][INFO] - Training Epoch: 2/2, step 6303/7134 completed (loss: 0.010498846881091595, acc: 0.9957401752471924)
[2025-02-13 04:32:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:51][root][INFO] - Training Epoch: 2/2, step 6304/7134 completed (loss: 0.013266597874462605, acc: 0.9945945739746094)
[2025-02-13 04:32:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:52][root][INFO] - Training Epoch: 2/2, step 6305/7134 completed (loss: 0.005082296207547188, acc: 1.0)
[2025-02-13 04:32:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:52][root][INFO] - Training Epoch: 2/2, step 6306/7134 completed (loss: 0.02287227474153042, acc: 0.9939975738525391)
[2025-02-13 04:32:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:53][root][INFO] - Training Epoch: 2/2, step 6307/7134 completed (loss: 0.04667065292596817, acc: 0.9929453134536743)
[2025-02-13 04:32:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:53][root][INFO] - Training Epoch: 2/2, step 6308/7134 completed (loss: 0.01784941367805004, acc: 0.996314525604248)
[2025-02-13 04:32:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:53][root][INFO] - Training Epoch: 2/2, step 6309/7134 completed (loss: 0.01281774789094925, acc: 0.995199978351593)
[2025-02-13 04:32:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:54][root][INFO] - Training Epoch: 2/2, step 6310/7134 completed (loss: 0.035929497331380844, acc: 0.9890109896659851)
[2025-02-13 04:32:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:54][root][INFO] - Training Epoch: 2/2, step 6311/7134 completed (loss: 0.0269808117300272, acc: 0.9938499331474304)
[2025-02-13 04:32:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:55][root][INFO] - Training Epoch: 2/2, step 6312/7134 completed (loss: 0.0179903544485569, acc: 0.9968553185462952)
[2025-02-13 04:32:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:55][root][INFO] - Training Epoch: 2/2, step 6313/7134 completed (loss: 0.008327532559633255, acc: 0.9963189959526062)
[2025-02-13 04:32:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:56][root][INFO] - Training Epoch: 2/2, step 6314/7134 completed (loss: 0.011263432912528515, acc: 0.9987389445304871)
[2025-02-13 04:32:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:56][root][INFO] - Training Epoch: 2/2, step 6315/7134 completed (loss: 0.0473150797188282, acc: 0.9868852496147156)
[2025-02-13 04:32:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:57][root][INFO] - Training Epoch: 2/2, step 6316/7134 completed (loss: 0.016474392265081406, acc: 0.9951377511024475)
[2025-02-13 04:32:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:57][root][INFO] - Training Epoch: 2/2, step 6317/7134 completed (loss: 0.005905817728489637, acc: 1.0)
[2025-02-13 04:32:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:57][root][INFO] - Training Epoch: 2/2, step 6318/7134 completed (loss: 0.005536019802093506, acc: 0.998410165309906)
[2025-02-13 04:32:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:58][root][INFO] - Training Epoch: 2/2, step 6319/7134 completed (loss: 0.007154231425374746, acc: 0.9979423880577087)
[2025-02-13 04:32:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:58][root][INFO] - Training Epoch: 2/2, step 6320/7134 completed (loss: 0.009643079712986946, acc: 0.9968404173851013)
[2025-02-13 04:32:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:59][root][INFO] - Training Epoch: 2/2, step 6321/7134 completed (loss: 0.006003961432725191, acc: 0.9977678656578064)
[2025-02-13 04:32:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:32:59][root][INFO] - Training Epoch: 2/2, step 6322/7134 completed (loss: 0.020276999101042747, acc: 0.992732584476471)
[2025-02-13 04:32:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:00][root][INFO] - Training Epoch: 2/2, step 6323/7134 completed (loss: 0.015850599855184555, acc: 0.9909909963607788)
[2025-02-13 04:33:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:00][root][INFO] - Training Epoch: 2/2, step 6324/7134 completed (loss: 0.015787579119205475, acc: 0.9960052967071533)
[2025-02-13 04:33:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:01][root][INFO] - Training Epoch: 2/2, step 6325/7134 completed (loss: 0.013553246855735779, acc: 0.994339644908905)
[2025-02-13 04:33:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:01][root][INFO] - Training Epoch: 2/2, step 6326/7134 completed (loss: 0.025145096704363823, acc: 0.9871588945388794)
[2025-02-13 04:33:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:01][root][INFO] - Training Epoch: 2/2, step 6327/7134 completed (loss: 0.011442072689533234, acc: 0.9973614811897278)
[2025-02-13 04:33:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:02][root][INFO] - Training Epoch: 2/2, step 6328/7134 completed (loss: 0.01133433822542429, acc: 0.9984050989151001)
[2025-02-13 04:33:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:02][root][INFO] - Training Epoch: 2/2, step 6329/7134 completed (loss: 0.016582239419221878, acc: 0.9954545497894287)
[2025-02-13 04:33:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:03][root][INFO] - Training Epoch: 2/2, step 6330/7134 completed (loss: 0.01693372055888176, acc: 0.9950371980667114)
[2025-02-13 04:33:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:03][root][INFO] - Training Epoch: 2/2, step 6331/7134 completed (loss: 0.0060259695164859295, acc: 1.0)
[2025-02-13 04:33:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:04][root][INFO] - Training Epoch: 2/2, step 6332/7134 completed (loss: 0.032667551189661026, acc: 0.9869888424873352)
[2025-02-13 04:33:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:04][root][INFO] - Training Epoch: 2/2, step 6333/7134 completed (loss: 0.026410382241010666, acc: 0.9904761910438538)
[2025-02-13 04:33:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:04][root][INFO] - Training Epoch: 2/2, step 6334/7134 completed (loss: 0.033126309514045715, acc: 0.9943181872367859)
[2025-02-13 04:33:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:05][root][INFO] - Training Epoch: 2/2, step 6335/7134 completed (loss: 0.008723425678908825, acc: 0.994966447353363)
[2025-02-13 04:33:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:05][root][INFO] - Training Epoch: 2/2, step 6336/7134 completed (loss: 0.0219820998609066, acc: 0.9885321259498596)
[2025-02-13 04:33:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:06][root][INFO] - Training Epoch: 2/2, step 6337/7134 completed (loss: 0.0038751920219510794, acc: 0.9981752038002014)
[2025-02-13 04:33:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:06][root][INFO] - Training Epoch: 2/2, step 6338/7134 completed (loss: 0.01692519523203373, acc: 0.9921011328697205)
[2025-02-13 04:33:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:07][root][INFO] - Training Epoch: 2/2, step 6339/7134 completed (loss: 0.017671648412942886, acc: 0.9930796027183533)
[2025-02-13 04:33:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:07][root][INFO] - Training Epoch: 2/2, step 6340/7134 completed (loss: 0.007839275524020195, acc: 0.9985337257385254)
[2025-02-13 04:33:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:07][root][INFO] - Training Epoch: 2/2, step 6341/7134 completed (loss: 0.015785209834575653, acc: 0.9943073987960815)
[2025-02-13 04:33:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:08][root][INFO] - Training Epoch: 2/2, step 6342/7134 completed (loss: 0.019728947430849075, acc: 0.9932705163955688)
[2025-02-13 04:33:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:08][root][INFO] - Training Epoch: 2/2, step 6343/7134 completed (loss: 0.01223698165267706, acc: 0.9948365092277527)
[2025-02-13 04:33:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:09][root][INFO] - Training Epoch: 2/2, step 6344/7134 completed (loss: 0.009652001783251762, acc: 0.99589604139328)
[2025-02-13 04:33:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:09][root][INFO] - Training Epoch: 2/2, step 6345/7134 completed (loss: 0.03318760544061661, acc: 0.9931694269180298)
[2025-02-13 04:33:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:10][root][INFO] - Training Epoch: 2/2, step 6346/7134 completed (loss: 0.0545334592461586, acc: 0.9860681295394897)
[2025-02-13 04:33:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:10][root][INFO] - Training Epoch: 2/2, step 6347/7134 completed (loss: 0.016941023990511894, acc: 0.9978355169296265)
[2025-02-13 04:33:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:10][root][INFO] - Training Epoch: 2/2, step 6348/7134 completed (loss: 0.031111976131796837, acc: 0.9947916865348816)
[2025-02-13 04:33:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:11][root][INFO] - Training Epoch: 2/2, step 6349/7134 completed (loss: 0.01789303682744503, acc: 0.9890710115432739)
[2025-02-13 04:33:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:11][root][INFO] - Training Epoch: 2/2, step 6350/7134 completed (loss: 0.025076013058423996, acc: 0.9931972622871399)
[2025-02-13 04:33:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:12][root][INFO] - Training Epoch: 2/2, step 6351/7134 completed (loss: 0.0323491208255291, acc: 0.9895522594451904)
[2025-02-13 04:33:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:12][root][INFO] - Training Epoch: 2/2, step 6352/7134 completed (loss: 0.08733241260051727, acc: 0.9821693897247314)
[2025-02-13 04:33:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:13][root][INFO] - Training Epoch: 2/2, step 6353/7134 completed (loss: 0.011278804391622543, acc: 0.9970717430114746)
[2025-02-13 04:33:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:13][root][INFO] - Training Epoch: 2/2, step 6354/7134 completed (loss: 0.009476146660745144, acc: 0.9972299337387085)
[2025-02-13 04:33:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:13][root][INFO] - Training Epoch: 2/2, step 6355/7134 completed (loss: 0.005570691078901291, acc: 0.9983818531036377)
[2025-02-13 04:33:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:14][root][INFO] - Training Epoch: 2/2, step 6356/7134 completed (loss: 0.017946317791938782, acc: 0.994020938873291)
[2025-02-13 04:33:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:14][root][INFO] - Training Epoch: 2/2, step 6357/7134 completed (loss: 0.008314735256135464, acc: 0.9972222447395325)
[2025-02-13 04:33:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:15][root][INFO] - Training Epoch: 2/2, step 6358/7134 completed (loss: 0.02123824506998062, acc: 0.9957746267318726)
[2025-02-13 04:33:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:15][root][INFO] - Training Epoch: 2/2, step 6359/7134 completed (loss: 0.046859011054039, acc: 0.9929971694946289)
[2025-02-13 04:33:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:16][root][INFO] - Training Epoch: 2/2, step 6360/7134 completed (loss: 0.01768343895673752, acc: 0.9952229261398315)
[2025-02-13 04:33:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:16][root][INFO] - Training Epoch: 2/2, step 6361/7134 completed (loss: 0.029992086812853813, acc: 0.989051103591919)
[2025-02-13 04:33:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:16][root][INFO] - Training Epoch: 2/2, step 6362/7134 completed (loss: 0.01717749983072281, acc: 0.9954268336296082)
[2025-02-13 04:33:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:17][root][INFO] - Training Epoch: 2/2, step 6363/7134 completed (loss: 0.01961834356188774, acc: 0.9941349029541016)
[2025-02-13 04:33:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:17][root][INFO] - Training Epoch: 2/2, step 6364/7134 completed (loss: 0.012885868549346924, acc: 0.9985693693161011)
[2025-02-13 04:33:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:18][root][INFO] - Training Epoch: 2/2, step 6365/7134 completed (loss: 0.008577542379498482, acc: 0.9959349632263184)
[2025-02-13 04:33:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:18][root][INFO] - Training Epoch: 2/2, step 6366/7134 completed (loss: 0.025144290179014206, acc: 0.9920739531517029)
[2025-02-13 04:33:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:19][root][INFO] - Training Epoch: 2/2, step 6367/7134 completed (loss: 0.0230748038738966, acc: 0.992438554763794)
[2025-02-13 04:33:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:19][root][INFO] - Training Epoch: 2/2, step 6368/7134 completed (loss: 0.048284273594617844, acc: 0.9901130199432373)
[2025-02-13 04:33:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:19][root][INFO] - Training Epoch: 2/2, step 6369/7134 completed (loss: 0.042227741330862045, acc: 0.9881734848022461)
[2025-02-13 04:33:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:20][root][INFO] - Training Epoch: 2/2, step 6370/7134 completed (loss: 0.07414601743221283, acc: 0.9762532711029053)
[2025-02-13 04:33:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:20][root][INFO] - Training Epoch: 2/2, step 6371/7134 completed (loss: 0.03477112576365471, acc: 0.990304708480835)
[2025-02-13 04:33:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:21][root][INFO] - Training Epoch: 2/2, step 6372/7134 completed (loss: 0.08272910118103027, acc: 0.9788867831230164)
[2025-02-13 04:33:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:21][root][INFO] - Training Epoch: 2/2, step 6373/7134 completed (loss: 0.03322106599807739, acc: 0.9908925294876099)
[2025-02-13 04:33:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:22][root][INFO] - Training Epoch: 2/2, step 6374/7134 completed (loss: 0.013520007953047752, acc: 0.9979252815246582)
[2025-02-13 04:33:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:22][root][INFO] - Training Epoch: 2/2, step 6375/7134 completed (loss: 0.05301399901509285, acc: 0.9858793616294861)
[2025-02-13 04:33:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:22][root][INFO] - Training Epoch: 2/2, step 6376/7134 completed (loss: 0.018842065706849098, acc: 0.9889298677444458)
[2025-02-13 04:33:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:23][root][INFO] - Training Epoch: 2/2, step 6377/7134 completed (loss: 0.01583249680697918, acc: 0.9945054650306702)
[2025-02-13 04:33:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:23][root][INFO] - Training Epoch: 2/2, step 6378/7134 completed (loss: 0.037798862904310226, acc: 0.9882100820541382)
[2025-02-13 04:33:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:24][root][INFO] - Training Epoch: 2/2, step 6379/7134 completed (loss: 0.01825510896742344, acc: 0.9949238300323486)
[2025-02-13 04:33:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:24][root][INFO] - Training Epoch: 2/2, step 6380/7134 completed (loss: 0.0472056120634079, acc: 0.9910179376602173)
[2025-02-13 04:33:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:25][root][INFO] - Training Epoch: 2/2, step 6381/7134 completed (loss: 0.009077583439648151, acc: 0.9969465732574463)
[2025-02-13 04:33:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:25][root][INFO] - Training Epoch: 2/2, step 6382/7134 completed (loss: 0.004512154497206211, acc: 0.9986859560012817)
[2025-02-13 04:33:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:25][root][INFO] - Training Epoch: 2/2, step 6383/7134 completed (loss: 0.042880479246377945, acc: 0.9932157397270203)
[2025-02-13 04:33:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:26][root][INFO] - Training Epoch: 2/2, step 6384/7134 completed (loss: 0.014396465383470058, acc: 0.9955157041549683)
[2025-02-13 04:33:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:26][root][INFO] - Training Epoch: 2/2, step 6385/7134 completed (loss: 0.058336008340120316, acc: 0.9868263602256775)
[2025-02-13 04:33:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:27][root][INFO] - Training Epoch: 2/2, step 6386/7134 completed (loss: 0.020092856138944626, acc: 0.9938949942588806)
[2025-02-13 04:33:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:27][root][INFO] - Training Epoch: 2/2, step 6387/7134 completed (loss: 0.02432921528816223, acc: 0.9935483932495117)
[2025-02-13 04:33:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:28][root][INFO] - Training Epoch: 2/2, step 6388/7134 completed (loss: 0.020191622897982597, acc: 0.992514967918396)
[2025-02-13 04:33:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:28][root][INFO] - Training Epoch: 2/2, step 6389/7134 completed (loss: 0.03523864969611168, acc: 0.990231990814209)
[2025-02-13 04:33:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:29][root][INFO] - Training Epoch: 2/2, step 6390/7134 completed (loss: 0.048111941665410995, acc: 0.9878542423248291)
[2025-02-13 04:33:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:29][root][INFO] - Training Epoch: 2/2, step 6391/7134 completed (loss: 0.04255206510424614, acc: 0.991037130355835)
[2025-02-13 04:33:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:30][root][INFO] - Training Epoch: 2/2, step 6392/7134 completed (loss: 0.03773628920316696, acc: 0.9876922965049744)
[2025-02-13 04:33:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:30][root][INFO] - Training Epoch: 2/2, step 6393/7134 completed (loss: 0.01825535297393799, acc: 0.9943116903305054)
[2025-02-13 04:33:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:30][root][INFO] - Training Epoch: 2/2, step 6394/7134 completed (loss: 0.05394198000431061, acc: 0.9805285334587097)
[2025-02-13 04:33:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:31][root][INFO] - Training Epoch: 2/2, step 6395/7134 completed (loss: 0.025307735428214073, acc: 0.9938499331474304)
[2025-02-13 04:33:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:31][root][INFO] - Training Epoch: 2/2, step 6396/7134 completed (loss: 0.021091971546411514, acc: 0.9954128265380859)
[2025-02-13 04:33:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:32][root][INFO] - Training Epoch: 2/2, step 6397/7134 completed (loss: 0.013303392566740513, acc: 0.9950371980667114)
[2025-02-13 04:33:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:32][root][INFO] - Training Epoch: 2/2, step 6398/7134 completed (loss: 0.010614873841404915, acc: 0.9957805871963501)
[2025-02-13 04:33:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:33][root][INFO] - Training Epoch: 2/2, step 6399/7134 completed (loss: 0.031077345833182335, acc: 0.9899749159812927)
[2025-02-13 04:33:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:33][root][INFO] - Training Epoch: 2/2, step 6400/7134 completed (loss: 0.013300648890435696, acc: 0.9971510171890259)
[2025-02-13 04:33:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:34][root][INFO] - Training Epoch: 2/2, step 6401/7134 completed (loss: 0.028178267180919647, acc: 0.9923995733261108)
[2025-02-13 04:33:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:34][root][INFO] - Training Epoch: 2/2, step 6402/7134 completed (loss: 0.031174028292298317, acc: 0.9954596757888794)
[2025-02-13 04:33:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:35][root][INFO] - Training Epoch: 2/2, step 6403/7134 completed (loss: 0.013691325671970844, acc: 0.9961734414100647)
[2025-02-13 04:33:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:35][root][INFO] - Training Epoch: 2/2, step 6404/7134 completed (loss: 0.01044213492423296, acc: 0.998609185218811)
[2025-02-13 04:33:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:35][root][INFO] - Training Epoch: 2/2, step 6405/7134 completed (loss: 0.010987821035087109, acc: 0.9956989288330078)
[2025-02-13 04:33:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:36][root][INFO] - Training Epoch: 2/2, step 6406/7134 completed (loss: 0.03116633929312229, acc: 0.9885975122451782)
[2025-02-13 04:33:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:36][root][INFO] - Training Epoch: 2/2, step 6407/7134 completed (loss: 0.01192531269043684, acc: 0.9973718523979187)
[2025-02-13 04:33:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:37][root][INFO] - Training Epoch: 2/2, step 6408/7134 completed (loss: 0.027960792183876038, acc: 0.9911373853683472)
[2025-02-13 04:33:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:37][root][INFO] - Training Epoch: 2/2, step 6409/7134 completed (loss: 0.031758762896060944, acc: 0.996052622795105)
[2025-02-13 04:33:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:38][root][INFO] - Training Epoch: 2/2, step 6410/7134 completed (loss: 0.015379714779555798, acc: 0.9926470518112183)
[2025-02-13 04:33:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:38][root][INFO] - Training Epoch: 2/2, step 6411/7134 completed (loss: 0.031998034566640854, acc: 0.9932998418807983)
[2025-02-13 04:33:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:39][root][INFO] - Training Epoch: 2/2, step 6412/7134 completed (loss: 0.010999097488820553, acc: 0.9973333477973938)
[2025-02-13 04:33:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:39][root][INFO] - Training Epoch: 2/2, step 6413/7134 completed (loss: 0.030153773725032806, acc: 0.9897260069847107)
[2025-02-13 04:33:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:39][root][INFO] - Training Epoch: 2/2, step 6414/7134 completed (loss: 0.008029001764953136, acc: 0.9984894394874573)
[2025-02-13 04:33:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:40][root][INFO] - Training Epoch: 2/2, step 6415/7134 completed (loss: 0.008547334000468254, acc: 0.9974325895309448)
[2025-02-13 04:33:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:40][root][INFO] - Training Epoch: 2/2, step 6416/7134 completed (loss: 0.05859284847974777, acc: 0.9848197102546692)
[2025-02-13 04:33:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:41][root][INFO] - Training Epoch: 2/2, step 6417/7134 completed (loss: 0.057572342455387115, acc: 0.9912126660346985)
[2025-02-13 04:33:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:41][root][INFO] - Training Epoch: 2/2, step 6418/7134 completed (loss: 0.0596897155046463, acc: 0.9845505356788635)
[2025-02-13 04:33:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:41][root][INFO] - Training Epoch: 2/2, step 6419/7134 completed (loss: 0.03684065863490105, acc: 0.985318124294281)
[2025-02-13 04:33:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:42][root][INFO] - Training Epoch: 2/2, step 6420/7134 completed (loss: 0.019577663391828537, acc: 0.9927954077720642)
[2025-02-13 04:33:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:42][root][INFO] - Training Epoch: 2/2, step 6421/7134 completed (loss: 0.036267127841711044, acc: 0.9910846948623657)
[2025-02-13 04:33:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:43][root][INFO] - Training Epoch: 2/2, step 6422/7134 completed (loss: 0.026333434507250786, acc: 0.9917920827865601)
[2025-02-13 04:33:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:43][root][INFO] - Training Epoch: 2/2, step 6423/7134 completed (loss: 0.04930828884243965, acc: 0.992668628692627)
[2025-02-13 04:33:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:44][root][INFO] - Training Epoch: 2/2, step 6424/7134 completed (loss: 0.03666471317410469, acc: 0.9886506795883179)
[2025-02-13 04:33:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:44][root][INFO] - Training Epoch: 2/2, step 6425/7134 completed (loss: 0.02137475647032261, acc: 0.9934810996055603)
[2025-02-13 04:33:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:45][root][INFO] - Training Epoch: 2/2, step 6426/7134 completed (loss: 0.031373485922813416, acc: 0.9907651543617249)
[2025-02-13 04:33:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:45][root][INFO] - Training Epoch: 2/2, step 6427/7134 completed (loss: 0.03367500379681587, acc: 0.9900568127632141)
[2025-02-13 04:33:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:46][root][INFO] - Training Epoch: 2/2, step 6428/7134 completed (loss: 0.012684783898293972, acc: 0.9958100318908691)
[2025-02-13 04:33:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:46][root][INFO] - Training Epoch: 2/2, step 6429/7134 completed (loss: 0.015578513033688068, acc: 0.9939393997192383)
[2025-02-13 04:33:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:46][root][INFO] - Training Epoch: 2/2, step 6430/7134 completed (loss: 0.04345918446779251, acc: 0.9884892106056213)
[2025-02-13 04:33:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:47][root][INFO] - Training Epoch: 2/2, step 6431/7134 completed (loss: 0.04654613882303238, acc: 0.9881578683853149)
[2025-02-13 04:33:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:47][root][INFO] - Training Epoch: 2/2, step 6432/7134 completed (loss: 0.1029643565416336, acc: 0.9747474789619446)
[2025-02-13 04:33:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:48][root][INFO] - Training Epoch: 2/2, step 6433/7134 completed (loss: 0.028257446363568306, acc: 0.9904371500015259)
[2025-02-13 04:33:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:48][root][INFO] - Training Epoch: 2/2, step 6434/7134 completed (loss: 0.006316015031188726, acc: 0.9973368644714355)
[2025-02-13 04:33:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:49][root][INFO] - Training Epoch: 2/2, step 6435/7134 completed (loss: 0.03450452536344528, acc: 0.9930675625801086)
[2025-02-13 04:33:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:49][root][INFO] - Training Epoch: 2/2, step 6436/7134 completed (loss: 0.007421519141644239, acc: 1.0)
[2025-02-13 04:33:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:49][root][INFO] - Training Epoch: 2/2, step 6437/7134 completed (loss: 0.03479738533496857, acc: 0.9933333396911621)
[2025-02-13 04:33:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:50][root][INFO] - Training Epoch: 2/2, step 6438/7134 completed (loss: 0.0324598029255867, acc: 0.9913473129272461)
[2025-02-13 04:33:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:50][root][INFO] - Training Epoch: 2/2, step 6439/7134 completed (loss: 0.02407689206302166, acc: 0.9921259880065918)
[2025-02-13 04:33:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:51][root][INFO] - Training Epoch: 2/2, step 6440/7134 completed (loss: 0.018410565331578255, acc: 0.9940263032913208)
[2025-02-13 04:33:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:51][root][INFO] - Training Epoch: 2/2, step 6441/7134 completed (loss: 0.016479486599564552, acc: 0.9970458149909973)
[2025-02-13 04:33:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:52][root][INFO] - Training Epoch: 2/2, step 6442/7134 completed (loss: 0.019923187792301178, acc: 0.9954596757888794)
[2025-02-13 04:33:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:52][root][INFO] - Training Epoch: 2/2, step 6443/7134 completed (loss: 0.018500916659832, acc: 0.9962871074676514)
[2025-02-13 04:33:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:53][root][INFO] - Training Epoch: 2/2, step 6444/7134 completed (loss: 0.028837135061621666, acc: 0.9916201233863831)
[2025-02-13 04:33:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:53][root][INFO] - Training Epoch: 2/2, step 6445/7134 completed (loss: 0.041640765964984894, acc: 0.9876695275306702)
[2025-02-13 04:33:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:54][root][INFO] - Training Epoch: 2/2, step 6446/7134 completed (loss: 0.016242176294326782, acc: 0.9952324032783508)
[2025-02-13 04:33:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:54][root][INFO] - Training Epoch: 2/2, step 6447/7134 completed (loss: 0.03409635275602341, acc: 0.9919447898864746)
[2025-02-13 04:33:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:54][root][INFO] - Training Epoch: 2/2, step 6448/7134 completed (loss: 0.05767960101366043, acc: 0.9875389337539673)
[2025-02-13 04:33:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:55][root][INFO] - Training Epoch: 2/2, step 6449/7134 completed (loss: 0.015116602182388306, acc: 0.9985507130622864)
[2025-02-13 04:33:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:55][root][INFO] - Training Epoch: 2/2, step 6450/7134 completed (loss: 0.018876606598496437, acc: 0.9955357313156128)
[2025-02-13 04:33:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:56][root][INFO] - Training Epoch: 2/2, step 6451/7134 completed (loss: 0.04378863796591759, acc: 0.9872673749923706)
[2025-02-13 04:33:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:56][root][INFO] - Training Epoch: 2/2, step 6452/7134 completed (loss: 0.014630040153861046, acc: 0.994425892829895)
[2025-02-13 04:33:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:57][root][INFO] - Training Epoch: 2/2, step 6453/7134 completed (loss: 0.01605779305100441, acc: 0.9932795763015747)
[2025-02-13 04:33:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:57][root][INFO] - Training Epoch: 2/2, step 6454/7134 completed (loss: 0.024362964555621147, acc: 0.9917550086975098)
[2025-02-13 04:33:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:58][root][INFO] - Training Epoch: 2/2, step 6455/7134 completed (loss: 0.009599670767784119, acc: 0.9966044425964355)
[2025-02-13 04:33:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:58][root][INFO] - Training Epoch: 2/2, step 6456/7134 completed (loss: 0.018020300194621086, acc: 0.995398759841919)
[2025-02-13 04:33:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:59][root][INFO] - Training Epoch: 2/2, step 6457/7134 completed (loss: 0.007277526892721653, acc: 0.9977973699569702)
[2025-02-13 04:33:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:59][root][INFO] - Training Epoch: 2/2, step 6458/7134 completed (loss: 0.019374558702111244, acc: 0.9932773113250732)
[2025-02-13 04:33:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:33:59][root][INFO] - Training Epoch: 2/2, step 6459/7134 completed (loss: 0.0016679508844390512, acc: 1.0)
[2025-02-13 04:34:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:00][root][INFO] - Training Epoch: 2/2, step 6460/7134 completed (loss: 0.007124286610633135, acc: 0.996458113193512)
[2025-02-13 04:34:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:00][root][INFO] - Training Epoch: 2/2, step 6461/7134 completed (loss: 0.01652831956744194, acc: 0.9947437644004822)
[2025-02-13 04:34:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:01][root][INFO] - Training Epoch: 2/2, step 6462/7134 completed (loss: 0.009910850785672665, acc: 0.9950799345970154)
[2025-02-13 04:34:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:01][root][INFO] - Training Epoch: 2/2, step 6463/7134 completed (loss: 0.01095102820545435, acc: 0.9954441785812378)
[2025-02-13 04:34:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:02][root][INFO] - Training Epoch: 2/2, step 6464/7134 completed (loss: 0.0219899769872427, acc: 0.9960988163948059)
[2025-02-13 04:34:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:02][root][INFO] - Training Epoch: 2/2, step 6465/7134 completed (loss: 0.012246220372617245, acc: 0.9935732483863831)
[2025-02-13 04:34:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:03][root][INFO] - Training Epoch: 2/2, step 6466/7134 completed (loss: 0.010470351204276085, acc: 0.997357964515686)
[2025-02-13 04:34:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:03][root][INFO] - Training Epoch: 2/2, step 6467/7134 completed (loss: 0.018662039190530777, acc: 0.9938931465148926)
[2025-02-13 04:34:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:04][root][INFO] - Training Epoch: 2/2, step 6468/7134 completed (loss: 0.011566037312150002, acc: 0.9962476491928101)
[2025-02-13 04:34:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:04][root][INFO] - Training Epoch: 2/2, step 6469/7134 completed (loss: 0.031219687312841415, acc: 0.9914737939834595)
[2025-02-13 04:34:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:04][root][INFO] - Training Epoch: 2/2, step 6470/7134 completed (loss: 0.026859987527132034, acc: 0.9926650524139404)
[2025-02-13 04:34:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:05][root][INFO] - Training Epoch: 2/2, step 6471/7134 completed (loss: 0.0039376383647322655, acc: 1.0)
[2025-02-13 04:34:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:05][root][INFO] - Training Epoch: 2/2, step 6472/7134 completed (loss: 0.037144068628549576, acc: 0.9929078221321106)
[2025-02-13 04:34:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:06][root][INFO] - Training Epoch: 2/2, step 6473/7134 completed (loss: 0.005142558831721544, acc: 0.9986594915390015)
[2025-02-13 04:34:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:06][root][INFO] - Training Epoch: 2/2, step 6474/7134 completed (loss: 0.019969472661614418, acc: 0.9959623217582703)
[2025-02-13 04:34:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:07][root][INFO] - Training Epoch: 2/2, step 6475/7134 completed (loss: 0.015735076740384102, acc: 0.9950000047683716)
[2025-02-13 04:34:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:07][root][INFO] - Training Epoch: 2/2, step 6476/7134 completed (loss: 0.04215630888938904, acc: 0.9884488582611084)
[2025-02-13 04:34:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:07][root][INFO] - Training Epoch: 2/2, step 6477/7134 completed (loss: 0.008066271431744099, acc: 0.9957627058029175)
[2025-02-13 04:34:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:08][root][INFO] - Training Epoch: 2/2, step 6478/7134 completed (loss: 0.023699916899204254, acc: 0.9963099360466003)
[2025-02-13 04:34:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:08][root][INFO] - Training Epoch: 2/2, step 6479/7134 completed (loss: 0.006328396499156952, acc: 0.9983249306678772)
[2025-02-13 04:34:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:09][root][INFO] - Training Epoch: 2/2, step 6480/7134 completed (loss: 0.026456793770194054, acc: 0.9914089441299438)
[2025-02-13 04:34:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:09][root][INFO] - Training Epoch: 2/2, step 6481/7134 completed (loss: 0.0029159437399357557, acc: 1.0)
[2025-02-13 04:34:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:10][root][INFO] - Training Epoch: 2/2, step 6482/7134 completed (loss: 0.008502933196723461, acc: 0.9950000047683716)
[2025-02-13 04:34:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:10][root][INFO] - Training Epoch: 2/2, step 6483/7134 completed (loss: 0.04346701502799988, acc: 0.994584858417511)
[2025-02-13 04:34:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:10][root][INFO] - Training Epoch: 2/2, step 6484/7134 completed (loss: 0.0177387073636055, acc: 0.9941349029541016)
[2025-02-13 04:34:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:11][root][INFO] - Training Epoch: 2/2, step 6485/7134 completed (loss: 0.010030610486865044, acc: 0.9960474371910095)
[2025-02-13 04:34:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:11][root][INFO] - Training Epoch: 2/2, step 6486/7134 completed (loss: 0.0970335602760315, acc: 0.9836363792419434)
[2025-02-13 04:34:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:12][root][INFO] - Training Epoch: 2/2, step 6487/7134 completed (loss: 0.0823524072766304, acc: 0.9736841917037964)
[2025-02-13 04:34:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:12][root][INFO] - Training Epoch: 2/2, step 6488/7134 completed (loss: 0.04664138704538345, acc: 0.9778671860694885)
[2025-02-13 04:34:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:13][root][INFO] - Training Epoch: 2/2, step 6489/7134 completed (loss: 0.06209920719265938, acc: 0.986940324306488)
[2025-02-13 04:34:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:13][root][INFO] - Training Epoch: 2/2, step 6490/7134 completed (loss: 0.05155591294169426, acc: 0.9914236664772034)
[2025-02-13 04:34:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:13][root][INFO] - Training Epoch: 2/2, step 6491/7134 completed (loss: 0.04332668334245682, acc: 0.9932523369789124)
[2025-02-13 04:34:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:14][root][INFO] - Training Epoch: 2/2, step 6492/7134 completed (loss: 0.012857153080403805, acc: 0.9948253631591797)
[2025-02-13 04:34:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:14][root][INFO] - Training Epoch: 2/2, step 6493/7134 completed (loss: 0.04700442776083946, acc: 0.9860050678253174)
[2025-02-13 04:34:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:15][root][INFO] - Training Epoch: 2/2, step 6494/7134 completed (loss: 0.022555403411388397, acc: 0.9944649338722229)
[2025-02-13 04:34:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:15][root][INFO] - Training Epoch: 2/2, step 6495/7134 completed (loss: 0.03345239534974098, acc: 0.9887797832489014)
[2025-02-13 04:34:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:16][root][INFO] - Training Epoch: 2/2, step 6496/7134 completed (loss: 0.02485004998743534, acc: 0.991183876991272)
[2025-02-13 04:34:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:16][root][INFO] - Training Epoch: 2/2, step 6497/7134 completed (loss: 0.06383122503757477, acc: 0.9797822833061218)
[2025-02-13 04:34:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:16][root][INFO] - Training Epoch: 2/2, step 6498/7134 completed (loss: 0.01611868478357792, acc: 0.997560977935791)
[2025-02-13 04:34:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:17][root][INFO] - Training Epoch: 2/2, step 6499/7134 completed (loss: 0.02027910202741623, acc: 0.993773341178894)
[2025-02-13 04:34:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:17][root][INFO] - Training Epoch: 2/2, step 6500/7134 completed (loss: 0.056130021810531616, acc: 0.9831932783126831)
[2025-02-13 04:34:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:18][root][INFO] - Training Epoch: 2/2, step 6501/7134 completed (loss: 0.09798141568899155, acc: 0.9757032990455627)
[2025-02-13 04:34:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:18][root][INFO] - Training Epoch: 2/2, step 6502/7134 completed (loss: 0.018595602363348007, acc: 0.9952493906021118)
[2025-02-13 04:34:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:19][root][INFO] - Training Epoch: 2/2, step 6503/7134 completed (loss: 0.027265671640634537, acc: 0.992668628692627)
[2025-02-13 04:34:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:19][root][INFO] - Training Epoch: 2/2, step 6504/7134 completed (loss: 0.029384339228272438, acc: 0.9905213117599487)
[2025-02-13 04:34:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:20][root][INFO] - Training Epoch: 2/2, step 6505/7134 completed (loss: 0.061963461339473724, acc: 0.9855072498321533)
[2025-02-13 04:34:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:20][root][INFO] - Training Epoch: 2/2, step 6506/7134 completed (loss: 0.03994610533118248, acc: 0.9869706630706787)
[2025-02-13 04:34:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:20][root][INFO] - Training Epoch: 2/2, step 6507/7134 completed (loss: 0.05740739032626152, acc: 0.9848713874816895)
[2025-02-13 04:34:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:21][root][INFO] - Training Epoch: 2/2, step 6508/7134 completed (loss: 0.047456204891204834, acc: 0.9929577708244324)
[2025-02-13 04:34:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:21][root][INFO] - Training Epoch: 2/2, step 6509/7134 completed (loss: 0.015018886886537075, acc: 0.9955703020095825)
[2025-02-13 04:34:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:22][root][INFO] - Training Epoch: 2/2, step 6510/7134 completed (loss: 0.029534492641687393, acc: 0.9894291758537292)
[2025-02-13 04:34:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:22][root][INFO] - Training Epoch: 2/2, step 6511/7134 completed (loss: 0.04155675321817398, acc: 0.9894737005233765)
[2025-02-13 04:34:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:23][root][INFO] - Training Epoch: 2/2, step 6512/7134 completed (loss: 0.006445176433771849, acc: 1.0)
[2025-02-13 04:34:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:23][root][INFO] - Training Epoch: 2/2, step 6513/7134 completed (loss: 0.026490256190299988, acc: 0.9938271641731262)
[2025-02-13 04:34:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:24][root][INFO] - Training Epoch: 2/2, step 6514/7134 completed (loss: 0.037421997636556625, acc: 0.9900568127632141)
[2025-02-13 04:34:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:24][root][INFO] - Training Epoch: 2/2, step 6515/7134 completed (loss: 0.02157391794025898, acc: 0.9936548471450806)
[2025-02-13 04:34:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:24][root][INFO] - Training Epoch: 2/2, step 6516/7134 completed (loss: 0.053026750683784485, acc: 0.9867374300956726)
[2025-02-13 04:34:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:25][root][INFO] - Training Epoch: 2/2, step 6517/7134 completed (loss: 0.016416292637586594, acc: 0.9973683953285217)
[2025-02-13 04:34:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:25][root][INFO] - Training Epoch: 2/2, step 6518/7134 completed (loss: 0.011465304531157017, acc: 0.9986206889152527)
[2025-02-13 04:34:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:26][root][INFO] - Training Epoch: 2/2, step 6519/7134 completed (loss: 0.024155650287866592, acc: 0.9964538812637329)
[2025-02-13 04:34:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:26][root][INFO] - Training Epoch: 2/2, step 6520/7134 completed (loss: 0.016282860189676285, acc: 0.996889591217041)
[2025-02-13 04:34:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:27][root][INFO] - Training Epoch: 2/2, step 6521/7134 completed (loss: 0.007259346544742584, acc: 0.9973958134651184)
[2025-02-13 04:34:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:27][root][INFO] - Training Epoch: 2/2, step 6522/7134 completed (loss: 0.029238123446702957, acc: 0.9914236664772034)
[2025-02-13 04:34:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:27][root][INFO] - Training Epoch: 2/2, step 6523/7134 completed (loss: 0.062272973358631134, acc: 0.9818511605262756)
[2025-02-13 04:34:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:28][root][INFO] - Training Epoch: 2/2, step 6524/7134 completed (loss: 0.055567674338817596, acc: 0.9910025596618652)
[2025-02-13 04:34:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:28][root][INFO] - Training Epoch: 2/2, step 6525/7134 completed (loss: 0.0070455437526106834, acc: 0.9979919791221619)
[2025-02-13 04:34:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:29][root][INFO] - Training Epoch: 2/2, step 6526/7134 completed (loss: 0.03594408184289932, acc: 0.9935979247093201)
[2025-02-13 04:34:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:29][root][INFO] - Training Epoch: 2/2, step 6527/7134 completed (loss: 0.021360158920288086, acc: 0.9940652847290039)
[2025-02-13 04:34:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:30][root][INFO] - Training Epoch: 2/2, step 6528/7134 completed (loss: 0.05654335767030716, acc: 0.9909747242927551)
[2025-02-13 04:34:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:30][root][INFO] - Training Epoch: 2/2, step 6529/7134 completed (loss: 0.045819252729415894, acc: 0.9887217879295349)
[2025-02-13 04:34:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:30][root][INFO] - Training Epoch: 2/2, step 6530/7134 completed (loss: 0.0269473809748888, acc: 0.9900142550468445)
[2025-02-13 04:34:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:31][root][INFO] - Training Epoch: 2/2, step 6531/7134 completed (loss: 0.006143407896161079, acc: 0.9985548853874207)
[2025-02-13 04:34:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:31][root][INFO] - Training Epoch: 2/2, step 6532/7134 completed (loss: 0.03343821316957474, acc: 0.9887359142303467)
[2025-02-13 04:34:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:32][root][INFO] - Training Epoch: 2/2, step 6533/7134 completed (loss: 0.007496390491724014, acc: 0.9970149397850037)
[2025-02-13 04:34:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:32][root][INFO] - Training Epoch: 2/2, step 6534/7134 completed (loss: 0.042542606592178345, acc: 0.9932975769042969)
[2025-02-13 04:34:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:33][root][INFO] - Training Epoch: 2/2, step 6535/7134 completed (loss: 0.015811799094080925, acc: 0.9953161478042603)
[2025-02-13 04:34:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:33][root][INFO] - Training Epoch: 2/2, step 6536/7134 completed (loss: 0.011613855138421059, acc: 0.9980158805847168)
[2025-02-13 04:34:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:33][root][INFO] - Training Epoch: 2/2, step 6537/7134 completed (loss: 0.04296856001019478, acc: 0.982300877571106)
[2025-02-13 04:34:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:34][root][INFO] - Training Epoch: 2/2, step 6538/7134 completed (loss: 0.05923601984977722, acc: 0.9857904314994812)
[2025-02-13 04:34:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:34][root][INFO] - Training Epoch: 2/2, step 6539/7134 completed (loss: 0.003559096483513713, acc: 1.0)
[2025-02-13 04:34:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:35][root][INFO] - Training Epoch: 2/2, step 6540/7134 completed (loss: 0.17731884121894836, acc: 0.9672130942344666)
[2025-02-13 04:34:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:35][root][INFO] - Training Epoch: 2/2, step 6541/7134 completed (loss: 0.027541829273104668, acc: 0.9889705777168274)
[2025-02-13 04:34:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:36][root][INFO] - Training Epoch: 2/2, step 6542/7134 completed (loss: 0.016628820449113846, acc: 0.998305082321167)
[2025-02-13 04:34:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:36][root][INFO] - Training Epoch: 2/2, step 6543/7134 completed (loss: 0.008049918338656425, acc: 1.0)
[2025-02-13 04:34:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:36][root][INFO] - Training Epoch: 2/2, step 6544/7134 completed (loss: 0.03445297107100487, acc: 0.9920477271080017)
[2025-02-13 04:34:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:37][root][INFO] - Training Epoch: 2/2, step 6545/7134 completed (loss: 0.015508378855884075, acc: 0.9962825179100037)
[2025-02-13 04:34:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:37][root][INFO] - Training Epoch: 2/2, step 6546/7134 completed (loss: 0.028674088418483734, acc: 0.9918566942214966)
[2025-02-13 04:34:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:38][root][INFO] - Training Epoch: 2/2, step 6547/7134 completed (loss: 0.08231114596128464, acc: 0.9829642176628113)
[2025-02-13 04:34:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:38][root][INFO] - Training Epoch: 2/2, step 6548/7134 completed (loss: 0.010106230154633522, acc: 0.9960317611694336)
[2025-02-13 04:34:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:38][root][INFO] - Training Epoch: 2/2, step 6549/7134 completed (loss: 0.06162673234939575, acc: 0.9864077568054199)
[2025-02-13 04:34:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:39][root][INFO] - Training Epoch: 2/2, step 6550/7134 completed (loss: 0.058380674570798874, acc: 0.9846153855323792)
[2025-02-13 04:34:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:39][root][INFO] - Training Epoch: 2/2, step 6551/7134 completed (loss: 0.009378360584378242, acc: 0.9977220892906189)
[2025-02-13 04:34:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:40][root][INFO] - Training Epoch: 2/2, step 6552/7134 completed (loss: 0.021477827802300453, acc: 0.9970119595527649)
[2025-02-13 04:34:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:40][root][INFO] - Training Epoch: 2/2, step 6553/7134 completed (loss: 0.020505569875240326, acc: 0.991094172000885)
[2025-02-13 04:34:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:41][root][INFO] - Training Epoch: 2/2, step 6554/7134 completed (loss: 0.034787099808454514, acc: 0.9935135245323181)
[2025-02-13 04:34:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:41][root][INFO] - Training Epoch: 2/2, step 6555/7134 completed (loss: 0.043960876762866974, acc: 0.9879275560379028)
[2025-02-13 04:34:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:42][root][INFO] - Training Epoch: 2/2, step 6556/7134 completed (loss: 0.034179024398326874, acc: 0.9894737005233765)
[2025-02-13 04:34:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:42][root][INFO] - Training Epoch: 2/2, step 6557/7134 completed (loss: 0.02740807831287384, acc: 0.9939516186714172)
[2025-02-13 04:34:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:43][root][INFO] - Training Epoch: 2/2, step 6558/7134 completed (loss: 0.029315227642655373, acc: 0.989130437374115)
[2025-02-13 04:34:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:43][root][INFO] - Training Epoch: 2/2, step 6559/7134 completed (loss: 0.01783384382724762, acc: 0.9930651783943176)
[2025-02-13 04:34:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:44][root][INFO] - Training Epoch: 2/2, step 6560/7134 completed (loss: 0.015892868861556053, acc: 0.9940546751022339)
[2025-02-13 04:34:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:44][root][INFO] - Training Epoch: 2/2, step 6561/7134 completed (loss: 0.01330114807933569, acc: 0.9953216314315796)
[2025-02-13 04:34:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:45][root][INFO] - Training Epoch: 2/2, step 6562/7134 completed (loss: 0.0207124724984169, acc: 0.9925261735916138)
[2025-02-13 04:34:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:45][root][INFO] - Training Epoch: 2/2, step 6563/7134 completed (loss: 0.02716216631233692, acc: 0.9941792488098145)
[2025-02-13 04:34:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:46][root][INFO] - Training Epoch: 2/2, step 6564/7134 completed (loss: 0.03794560581445694, acc: 0.9892037510871887)
[2025-02-13 04:34:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:46][root][INFO] - Training Epoch: 2/2, step 6565/7134 completed (loss: 0.02868923917412758, acc: 0.9936575293540955)
[2025-02-13 04:34:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:46][root][INFO] - Training Epoch: 2/2, step 6566/7134 completed (loss: 0.01148676872253418, acc: 0.9962013363838196)
[2025-02-13 04:34:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:47][root][INFO] - Training Epoch: 2/2, step 6567/7134 completed (loss: 0.011405407451093197, acc: 0.996960461139679)
[2025-02-13 04:34:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:47][root][INFO] - Training Epoch: 2/2, step 6568/7134 completed (loss: 0.010990775190293789, acc: 0.9987389445304871)
[2025-02-13 04:34:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:48][root][INFO] - Training Epoch: 2/2, step 6569/7134 completed (loss: 0.0064132241532206535, acc: 0.9967032670974731)
[2025-02-13 04:34:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:48][root][INFO] - Training Epoch: 2/2, step 6570/7134 completed (loss: 0.019401302561163902, acc: 0.9953542351722717)
[2025-02-13 04:34:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:49][root][INFO] - Training Epoch: 2/2, step 6571/7134 completed (loss: 0.011438176035881042, acc: 0.996073305606842)
[2025-02-13 04:34:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:49][root][INFO] - Training Epoch: 2/2, step 6572/7134 completed (loss: 0.03559456393122673, acc: 0.9911308288574219)
[2025-02-13 04:34:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:50][root][INFO] - Training Epoch: 2/2, step 6573/7134 completed (loss: 0.01250460371375084, acc: 0.9954853057861328)
[2025-02-13 04:34:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:50][root][INFO] - Training Epoch: 2/2, step 6574/7134 completed (loss: 0.016088612377643585, acc: 0.9918200373649597)
[2025-02-13 04:34:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:51][root][INFO] - Training Epoch: 2/2, step 6575/7134 completed (loss: 0.02189653366804123, acc: 0.9922651648521423)
[2025-02-13 04:34:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:51][root][INFO] - Training Epoch: 2/2, step 6576/7134 completed (loss: 0.007752227131277323, acc: 0.9978991746902466)
[2025-02-13 04:34:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:52][root][INFO] - Training Epoch: 2/2, step 6577/7134 completed (loss: 0.03423616662621498, acc: 0.9887955188751221)
[2025-02-13 04:34:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:52][root][INFO] - Training Epoch: 2/2, step 6578/7134 completed (loss: 0.010410238988697529, acc: 0.9973309636116028)
[2025-02-13 04:34:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:53][root][INFO] - Training Epoch: 2/2, step 6579/7134 completed (loss: 0.028663117438554764, acc: 0.9927404522895813)
[2025-02-13 04:34:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:53][root][INFO] - Training Epoch: 2/2, step 6580/7134 completed (loss: 0.007071805652230978, acc: 0.9988235235214233)
[2025-02-13 04:34:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:54][root][INFO] - Training Epoch: 2/2, step 6581/7134 completed (loss: 0.018850920721888542, acc: 0.9950799345970154)
[2025-02-13 04:34:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:54][root][INFO] - Training Epoch: 2/2, step 6582/7134 completed (loss: 0.02928706631064415, acc: 0.9928057789802551)
[2025-02-13 04:34:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:54][root][INFO] - Training Epoch: 2/2, step 6583/7134 completed (loss: 0.013797296211123466, acc: 0.9931350350379944)
[2025-02-13 04:34:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:55][root][INFO] - Training Epoch: 2/2, step 6584/7134 completed (loss: 0.03572669252753258, acc: 0.9865360856056213)
[2025-02-13 04:34:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:55][root][INFO] - Training Epoch: 2/2, step 6585/7134 completed (loss: 0.016659211367368698, acc: 0.9950658082962036)
[2025-02-13 04:34:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:56][root][INFO] - Training Epoch: 2/2, step 6586/7134 completed (loss: 0.014202947728335857, acc: 0.9974093437194824)
[2025-02-13 04:34:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:56][root][INFO] - Training Epoch: 2/2, step 6587/7134 completed (loss: 0.06312844157218933, acc: 0.9870283007621765)
[2025-02-13 04:34:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:57][root][INFO] - Training Epoch: 2/2, step 6588/7134 completed (loss: 0.03509112447500229, acc: 0.9937106966972351)
[2025-02-13 04:34:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:57][root][INFO] - Training Epoch: 2/2, step 6589/7134 completed (loss: 0.028406986966729164, acc: 0.9889349937438965)
[2025-02-13 04:34:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:57][root][INFO] - Training Epoch: 2/2, step 6590/7134 completed (loss: 0.03201603516936302, acc: 0.9941792488098145)
[2025-02-13 04:34:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:58][root][INFO] - Training Epoch: 2/2, step 6591/7134 completed (loss: 0.029380669817328453, acc: 0.9938042163848877)
[2025-02-13 04:34:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:58][root][INFO] - Training Epoch: 2/2, step 6592/7134 completed (loss: 0.028180623427033424, acc: 0.9931895732879639)
[2025-02-13 04:34:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:59][root][INFO] - Training Epoch: 2/2, step 6593/7134 completed (loss: 0.029139578342437744, acc: 0.990867555141449)
[2025-02-13 04:34:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:34:59][root][INFO] - Training Epoch: 2/2, step 6594/7134 completed (loss: 0.02590014971792698, acc: 0.9941314458847046)
[2025-02-13 04:34:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:00][root][INFO] - Training Epoch: 2/2, step 6595/7134 completed (loss: 0.02912752516567707, acc: 0.9925816059112549)
[2025-02-13 04:35:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:00][root][INFO] - Training Epoch: 2/2, step 6596/7134 completed (loss: 0.015122065320611, acc: 0.9954476356506348)
[2025-02-13 04:35:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:01][root][INFO] - Training Epoch: 2/2, step 6597/7134 completed (loss: 0.020100412890315056, acc: 0.9929245114326477)
[2025-02-13 04:35:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:01][root][INFO] - Training Epoch: 2/2, step 6598/7134 completed (loss: 0.016511481255292892, acc: 0.991946280002594)
[2025-02-13 04:35:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:02][root][INFO] - Training Epoch: 2/2, step 6599/7134 completed (loss: 0.03884454071521759, acc: 0.990338146686554)
[2025-02-13 04:35:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:02][root][INFO] - Training Epoch: 2/2, step 6600/7134 completed (loss: 0.022530678659677505, acc: 0.9904912710189819)
[2025-02-13 04:35:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:02][root][INFO] - Training Epoch: 2/2, step 6601/7134 completed (loss: 0.037050988525152206, acc: 0.9917840361595154)
[2025-02-13 04:35:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:03][root][INFO] - Training Epoch: 2/2, step 6602/7134 completed (loss: 0.01475164107978344, acc: 0.9975932836532593)
[2025-02-13 04:35:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:03][root][INFO] - Training Epoch: 2/2, step 6603/7134 completed (loss: 0.030071858316659927, acc: 0.9901823401451111)
[2025-02-13 04:35:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:04][root][INFO] - Training Epoch: 2/2, step 6604/7134 completed (loss: 0.029081089422106743, acc: 0.9913793206214905)
[2025-02-13 04:35:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:04][root][INFO] - Training Epoch: 2/2, step 6605/7134 completed (loss: 0.0647660493850708, acc: 0.9737206101417542)
[2025-02-13 04:35:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:05][root][INFO] - Training Epoch: 2/2, step 6606/7134 completed (loss: 0.029909003525972366, acc: 0.9924242496490479)
[2025-02-13 04:35:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:05][root][INFO] - Training Epoch: 2/2, step 6607/7134 completed (loss: 0.05760170519351959, acc: 0.9853723645210266)
[2025-02-13 04:35:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:05][root][INFO] - Training Epoch: 2/2, step 6608/7134 completed (loss: 0.014317909255623817, acc: 0.9973439574241638)
[2025-02-13 04:35:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:06][root][INFO] - Training Epoch: 2/2, step 6609/7134 completed (loss: 0.03332865238189697, acc: 0.9841075539588928)
[2025-02-13 04:35:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:06][root][INFO] - Training Epoch: 2/2, step 6610/7134 completed (loss: 0.05574340000748634, acc: 0.9851751923561096)
[2025-02-13 04:35:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:07][root][INFO] - Training Epoch: 2/2, step 6611/7134 completed (loss: 0.06436429172754288, acc: 0.9830769300460815)
[2025-02-13 04:35:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:07][root][INFO] - Training Epoch: 2/2, step 6612/7134 completed (loss: 0.04150744527578354, acc: 0.9883313775062561)
[2025-02-13 04:35:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:08][root][INFO] - Training Epoch: 2/2, step 6613/7134 completed (loss: 0.029740646481513977, acc: 0.9910714030265808)
[2025-02-13 04:35:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:08][root][INFO] - Training Epoch: 2/2, step 6614/7134 completed (loss: 0.05232970789074898, acc: 0.9900990128517151)
[2025-02-13 04:35:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:09][root][INFO] - Training Epoch: 2/2, step 6615/7134 completed (loss: 0.026467081159353256, acc: 0.9915134310722351)
[2025-02-13 04:35:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:09][root][INFO] - Training Epoch: 2/2, step 6616/7134 completed (loss: 0.02717188559472561, acc: 0.9941314458847046)
[2025-02-13 04:35:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:09][root][INFO] - Training Epoch: 2/2, step 6617/7134 completed (loss: 0.023333989083766937, acc: 0.9913544654846191)
[2025-02-13 04:35:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:10][root][INFO] - Training Epoch: 2/2, step 6618/7134 completed (loss: 0.018006453290581703, acc: 0.9940564632415771)
[2025-02-13 04:35:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:10][root][INFO] - Training Epoch: 2/2, step 6619/7134 completed (loss: 0.026832889765501022, acc: 0.9932735562324524)
[2025-02-13 04:35:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:11][root][INFO] - Training Epoch: 2/2, step 6620/7134 completed (loss: 0.021993307396769524, acc: 0.9917469024658203)
[2025-02-13 04:35:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:11][root][INFO] - Training Epoch: 2/2, step 6621/7134 completed (loss: 0.028091153129935265, acc: 0.99190753698349)
[2025-02-13 04:35:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:12][root][INFO] - Training Epoch: 2/2, step 6622/7134 completed (loss: 0.026456370949745178, acc: 0.994535505771637)
[2025-02-13 04:35:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:12][root][INFO] - Training Epoch: 2/2, step 6623/7134 completed (loss: 0.013106399215757847, acc: 0.9949685335159302)
[2025-02-13 04:35:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:13][root][INFO] - Training Epoch: 2/2, step 6624/7134 completed (loss: 0.008599823340773582, acc: 0.9986979365348816)
[2025-02-13 04:35:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:13][root][INFO] - Training Epoch: 2/2, step 6625/7134 completed (loss: 0.02966652624309063, acc: 0.993122398853302)
[2025-02-13 04:35:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:13][root][INFO] - Training Epoch: 2/2, step 6626/7134 completed (loss: 0.012215590104460716, acc: 0.9987878799438477)
[2025-02-13 04:35:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:14][root][INFO] - Training Epoch: 2/2, step 6627/7134 completed (loss: 0.0183673407882452, acc: 0.9945945739746094)
[2025-02-13 04:35:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:14][root][INFO] - Training Epoch: 2/2, step 6628/7134 completed (loss: 0.027362054213881493, acc: 0.9905325174331665)
[2025-02-13 04:35:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:15][root][INFO] - Training Epoch: 2/2, step 6629/7134 completed (loss: 0.04549436271190643, acc: 0.9910141229629517)
[2025-02-13 04:35:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:15][root][INFO] - Training Epoch: 2/2, step 6630/7134 completed (loss: 0.03575696423649788, acc: 0.9899665713310242)
[2025-02-13 04:35:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:16][root][INFO] - Training Epoch: 2/2, step 6631/7134 completed (loss: 0.026470141485333443, acc: 0.9908257126808167)
[2025-02-13 04:35:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:16][root][INFO] - Training Epoch: 2/2, step 6632/7134 completed (loss: 0.024196989834308624, acc: 0.9955005645751953)
[2025-02-13 04:35:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:17][root][INFO] - Training Epoch: 2/2, step 6633/7134 completed (loss: 0.026004167273640633, acc: 0.9963414669036865)
[2025-02-13 04:35:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:17][root][INFO] - Training Epoch: 2/2, step 6634/7134 completed (loss: 0.0368625745177269, acc: 0.9856887459754944)
[2025-02-13 04:35:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:18][root][INFO] - Training Epoch: 2/2, step 6635/7134 completed (loss: 0.07544274628162384, acc: 0.9808183908462524)
[2025-02-13 04:35:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:18][root][INFO] - Training Epoch: 2/2, step 6636/7134 completed (loss: 0.02292630635201931, acc: 0.9897260069847107)
[2025-02-13 04:35:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:18][root][INFO] - Training Epoch: 2/2, step 6637/7134 completed (loss: 0.02532249130308628, acc: 0.9941775798797607)
[2025-02-13 04:35:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:19][root][INFO] - Training Epoch: 2/2, step 6638/7134 completed (loss: 0.01736520417034626, acc: 0.9949173927307129)
[2025-02-13 04:35:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:19][root][INFO] - Training Epoch: 2/2, step 6639/7134 completed (loss: 0.016217811033129692, acc: 0.992548406124115)
[2025-02-13 04:35:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:20][root][INFO] - Training Epoch: 2/2, step 6640/7134 completed (loss: 0.009240787476301193, acc: 0.9969372153282166)
[2025-02-13 04:35:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:20][root][INFO] - Training Epoch: 2/2, step 6641/7134 completed (loss: 0.054341915994882584, acc: 0.9859319925308228)
[2025-02-13 04:35:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:21][root][INFO] - Training Epoch: 2/2, step 6642/7134 completed (loss: 0.023099320009350777, acc: 0.9939758777618408)
[2025-02-13 04:35:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:21][root][INFO] - Training Epoch: 2/2, step 6643/7134 completed (loss: 0.013879365287721157, acc: 0.9960370063781738)
[2025-02-13 04:35:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:22][root][INFO] - Training Epoch: 2/2, step 6644/7134 completed (loss: 0.03908476233482361, acc: 0.989827036857605)
[2025-02-13 04:35:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:22][root][INFO] - Training Epoch: 2/2, step 6645/7134 completed (loss: 0.020161310210824013, acc: 0.9926289916038513)
[2025-02-13 04:35:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:23][root][INFO] - Training Epoch: 2/2, step 6646/7134 completed (loss: 0.027058517560362816, acc: 0.992290735244751)
[2025-02-13 04:35:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:23][root][INFO] - Training Epoch: 2/2, step 6647/7134 completed (loss: 0.012388450093567371, acc: 0.9959239363670349)
[2025-02-13 04:35:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:24][root][INFO] - Training Epoch: 2/2, step 6648/7134 completed (loss: 0.05915147811174393, acc: 0.9760000109672546)
[2025-02-13 04:35:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:24][root][INFO] - Training Epoch: 2/2, step 6649/7134 completed (loss: 0.030503738671541214, acc: 0.9893742799758911)
[2025-02-13 04:35:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:25][root][INFO] - Training Epoch: 2/2, step 6650/7134 completed (loss: 0.00653728935867548, acc: 0.9988839030265808)
[2025-02-13 04:35:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:25][root][INFO] - Training Epoch: 2/2, step 6651/7134 completed (loss: 0.023081718012690544, acc: 0.9972714781761169)
[2025-02-13 04:35:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:25][root][INFO] - Training Epoch: 2/2, step 6652/7134 completed (loss: 0.024336539208889008, acc: 0.9939613342285156)
[2025-02-13 04:35:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:26][root][INFO] - Training Epoch: 2/2, step 6653/7134 completed (loss: 0.0339818075299263, acc: 0.9879518151283264)
[2025-02-13 04:35:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:26][root][INFO] - Training Epoch: 2/2, step 6654/7134 completed (loss: 0.06285777688026428, acc: 0.9829457402229309)
[2025-02-13 04:35:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:27][root][INFO] - Training Epoch: 2/2, step 6655/7134 completed (loss: 0.023743776604533195, acc: 0.9875930547714233)
[2025-02-13 04:35:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:27][root][INFO] - Training Epoch: 2/2, step 6656/7134 completed (loss: 0.03705562651157379, acc: 0.991094172000885)
[2025-02-13 04:35:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:28][root][INFO] - Training Epoch: 2/2, step 6657/7134 completed (loss: 0.023500703275203705, acc: 0.9945799708366394)
[2025-02-13 04:35:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:28][root][INFO] - Training Epoch: 2/2, step 6658/7134 completed (loss: 0.011906973086297512, acc: 0.9974193572998047)
[2025-02-13 04:35:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:29][root][INFO] - Training Epoch: 2/2, step 6659/7134 completed (loss: 0.02011936902999878, acc: 0.99609375)
[2025-02-13 04:35:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:29][root][INFO] - Training Epoch: 2/2, step 6660/7134 completed (loss: 0.009243011474609375, acc: 0.9980158805847168)
[2025-02-13 04:35:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:30][root][INFO] - Training Epoch: 2/2, step 6661/7134 completed (loss: 0.019638527184724808, acc: 0.9925742745399475)
[2025-02-13 04:35:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:30][root][INFO] - Training Epoch: 2/2, step 6662/7134 completed (loss: 0.03992027789354324, acc: 0.9828326106071472)
[2025-02-13 04:35:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:30][root][INFO] - Training Epoch: 2/2, step 6663/7134 completed (loss: 0.02867455594241619, acc: 0.9934425950050354)
[2025-02-13 04:35:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:31][root][INFO] - Training Epoch: 2/2, step 6664/7134 completed (loss: 0.03564829379320145, acc: 0.9833948612213135)
[2025-02-13 04:35:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:31][root][INFO] - Training Epoch: 2/2, step 6665/7134 completed (loss: 0.04361720010638237, acc: 0.9788960814476013)
[2025-02-13 04:35:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:32][root][INFO] - Training Epoch: 2/2, step 6666/7134 completed (loss: 0.017399953678250313, acc: 0.9936548471450806)
[2025-02-13 04:35:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:32][root][INFO] - Training Epoch: 2/2, step 6667/7134 completed (loss: 0.031944263726472855, acc: 0.9876325130462646)
[2025-02-13 04:35:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:33][root][INFO] - Training Epoch: 2/2, step 6668/7134 completed (loss: 0.005923778750002384, acc: 1.0)
[2025-02-13 04:35:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:33][root][INFO] - Training Epoch: 2/2, step 6669/7134 completed (loss: 0.018722906708717346, acc: 0.9966443181037903)
[2025-02-13 04:35:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:33][root][INFO] - Training Epoch: 2/2, step 6670/7134 completed (loss: 0.011242919601500034, acc: 0.9949832558631897)
[2025-02-13 04:35:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:34][root][INFO] - Training Epoch: 2/2, step 6671/7134 completed (loss: 0.011443817056715488, acc: 0.9941349029541016)
[2025-02-13 04:35:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:34][root][INFO] - Training Epoch: 2/2, step 6672/7134 completed (loss: 0.012257379479706287, acc: 0.9962476491928101)
[2025-02-13 04:35:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:35][root][INFO] - Training Epoch: 2/2, step 6673/7134 completed (loss: 0.009360048919916153, acc: 0.997063159942627)
[2025-02-13 04:35:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:35][root][INFO] - Training Epoch: 2/2, step 6674/7134 completed (loss: 0.005948808044195175, acc: 1.0)
[2025-02-13 04:35:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:36][root][INFO] - Training Epoch: 2/2, step 6675/7134 completed (loss: 0.01298883929848671, acc: 0.9925925731658936)
[2025-02-13 04:35:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:36][root][INFO] - Training Epoch: 2/2, step 6676/7134 completed (loss: 0.02428795024752617, acc: 0.9928401112556458)
[2025-02-13 04:35:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:36][root][INFO] - Training Epoch: 2/2, step 6677/7134 completed (loss: 0.03979228064417839, acc: 0.9885495901107788)
[2025-02-13 04:35:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:37][root][INFO] - Training Epoch: 2/2, step 6678/7134 completed (loss: 0.00955916941165924, acc: 0.9946428537368774)
[2025-02-13 04:35:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:37][root][INFO] - Training Epoch: 2/2, step 6679/7134 completed (loss: 0.004584037698805332, acc: 1.0)
[2025-02-13 04:35:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:37][root][INFO] - Training Epoch: 2/2, step 6680/7134 completed (loss: 0.01860938034951687, acc: 0.9972972869873047)
[2025-02-13 04:35:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:38][root][INFO] - Training Epoch: 2/2, step 6681/7134 completed (loss: 0.0038242076989263296, acc: 0.9979209899902344)
[2025-02-13 04:35:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:38][root][INFO] - Training Epoch: 2/2, step 6682/7134 completed (loss: 0.04728781804442406, acc: 0.9929906725883484)
[2025-02-13 04:35:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:39][root][INFO] - Training Epoch: 2/2, step 6683/7134 completed (loss: 0.014216271229088306, acc: 0.9945054650306702)
[2025-02-13 04:35:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:39][root][INFO] - Training Epoch: 2/2, step 6684/7134 completed (loss: 0.036851365119218826, acc: 0.9910714030265808)
[2025-02-13 04:35:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:40][root][INFO] - Training Epoch: 2/2, step 6685/7134 completed (loss: 0.023075491189956665, acc: 0.9942747950553894)
[2025-02-13 04:35:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:40][root][INFO] - Training Epoch: 2/2, step 6686/7134 completed (loss: 0.06767681986093521, acc: 0.9876033067703247)
[2025-02-13 04:35:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:40][root][INFO] - Training Epoch: 2/2, step 6687/7134 completed (loss: 0.032240234315395355, acc: 0.9876543283462524)
[2025-02-13 04:35:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:41][root][INFO] - Training Epoch: 2/2, step 6688/7134 completed (loss: 0.029013535007834435, acc: 0.9879724979400635)
[2025-02-13 04:35:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:41][root][INFO] - Training Epoch: 2/2, step 6689/7134 completed (loss: 0.02949729561805725, acc: 0.986138641834259)
[2025-02-13 04:35:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:42][root][INFO] - Training Epoch: 2/2, step 6690/7134 completed (loss: 0.043456658720970154, acc: 0.9830508232116699)
[2025-02-13 04:35:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:42][root][INFO] - Training Epoch: 2/2, step 6691/7134 completed (loss: 0.02115827426314354, acc: 0.9934533834457397)
[2025-02-13 04:35:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:42][root][INFO] - Training Epoch: 2/2, step 6692/7134 completed (loss: 0.017014155164361, acc: 0.9936440587043762)
[2025-02-13 04:35:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:43][root][INFO] - Training Epoch: 2/2, step 6693/7134 completed (loss: 0.022787103429436684, acc: 0.9966996908187866)
[2025-02-13 04:35:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:43][root][INFO] - Training Epoch: 2/2, step 6694/7134 completed (loss: 0.013157852925360203, acc: 0.9984709620475769)
[2025-02-13 04:35:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:44][root][INFO] - Training Epoch: 2/2, step 6695/7134 completed (loss: 0.020523501560091972, acc: 0.99589604139328)
[2025-02-13 04:35:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:44][root][INFO] - Training Epoch: 2/2, step 6696/7134 completed (loss: 0.05560792237520218, acc: 0.9829721450805664)
[2025-02-13 04:35:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:44][root][INFO] - Training Epoch: 2/2, step 6697/7134 completed (loss: 0.05592429265379906, acc: 0.9819672107696533)
[2025-02-13 04:35:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:45][root][INFO] - Training Epoch: 2/2, step 6698/7134 completed (loss: 0.010382270440459251, acc: 0.9955489635467529)
[2025-02-13 04:35:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:45][root][INFO] - Training Epoch: 2/2, step 6699/7134 completed (loss: 0.012431681156158447, acc: 0.9954441785812378)
[2025-02-13 04:35:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:46][root][INFO] - Training Epoch: 2/2, step 6700/7134 completed (loss: 0.06821732968091965, acc: 0.9852941036224365)
[2025-02-13 04:35:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:46][root][INFO] - Training Epoch: 2/2, step 6701/7134 completed (loss: 0.07469364255666733, acc: 0.9801587462425232)
[2025-02-13 04:35:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:47][root][INFO] - Training Epoch: 2/2, step 6702/7134 completed (loss: 0.030591795220971107, acc: 0.990176796913147)
[2025-02-13 04:35:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:47][root][INFO] - Training Epoch: 2/2, step 6703/7134 completed (loss: 0.037508297711610794, acc: 0.9917762875556946)
[2025-02-13 04:35:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:47][root][INFO] - Training Epoch: 2/2, step 6704/7134 completed (loss: 0.0795973390340805, acc: 0.9857697486877441)
[2025-02-13 04:35:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:48][root][INFO] - Training Epoch: 2/2, step 6705/7134 completed (loss: 0.02855537086725235, acc: 0.9908972978591919)
[2025-02-13 04:35:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:48][root][INFO] - Training Epoch: 2/2, step 6706/7134 completed (loss: 0.030867159366607666, acc: 0.9876033067703247)
[2025-02-13 04:35:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:49][root][INFO] - Training Epoch: 2/2, step 6707/7134 completed (loss: 0.033505432307720184, acc: 0.9921104311943054)
[2025-02-13 04:35:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:49][root][INFO] - Training Epoch: 2/2, step 6708/7134 completed (loss: 0.02392956055700779, acc: 0.9966996908187866)
[2025-02-13 04:35:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:50][root][INFO] - Training Epoch: 2/2, step 6709/7134 completed (loss: 0.030172841623425484, acc: 0.992514967918396)
[2025-02-13 04:35:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:50][root][INFO] - Training Epoch: 2/2, step 6710/7134 completed (loss: 0.04283847287297249, acc: 0.9872029423713684)
[2025-02-13 04:35:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:50][root][INFO] - Training Epoch: 2/2, step 6711/7134 completed (loss: 0.010677711106836796, acc: 0.9968253970146179)
[2025-02-13 04:35:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:51][root][INFO] - Training Epoch: 2/2, step 6712/7134 completed (loss: 0.03951714560389519, acc: 0.9909228682518005)
[2025-02-13 04:35:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:51][root][INFO] - Training Epoch: 2/2, step 6713/7134 completed (loss: 0.04291445389389992, acc: 0.986975371837616)
[2025-02-13 04:35:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:52][root][INFO] - Training Epoch: 2/2, step 6714/7134 completed (loss: 0.024324269965291023, acc: 0.98959881067276)
[2025-02-13 04:35:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:52][root][INFO] - Training Epoch: 2/2, step 6715/7134 completed (loss: 0.06343544274568558, acc: 0.9780219793319702)
[2025-02-13 04:35:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:53][root][INFO] - Training Epoch: 2/2, step 6716/7134 completed (loss: 0.010140943340957165, acc: 0.9970059990882874)
[2025-02-13 04:35:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:53][root][INFO] - Training Epoch: 2/2, step 6717/7134 completed (loss: 0.024274935945868492, acc: 0.9919484853744507)
[2025-02-13 04:35:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:54][root][INFO] - Training Epoch: 2/2, step 6718/7134 completed (loss: 0.03900567814707756, acc: 0.9833333492279053)
[2025-02-13 04:35:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:54][root][INFO] - Training Epoch: 2/2, step 6719/7134 completed (loss: 0.020955169573426247, acc: 0.9885621070861816)
[2025-02-13 04:35:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:54][root][INFO] - Training Epoch: 2/2, step 6720/7134 completed (loss: 0.04234030470252037, acc: 0.9902234673500061)
[2025-02-13 04:35:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:55][root][INFO] - Training Epoch: 2/2, step 6721/7134 completed (loss: 0.05225154384970665, acc: 0.9859594106674194)
[2025-02-13 04:35:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:55][root][INFO] - Training Epoch: 2/2, step 6722/7134 completed (loss: 0.054045889526605606, acc: 0.9929701089859009)
[2025-02-13 04:35:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:56][root][INFO] - Training Epoch: 2/2, step 6723/7134 completed (loss: 0.021608084440231323, acc: 0.9958333373069763)
[2025-02-13 04:35:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:56][root][INFO] - Training Epoch: 2/2, step 6724/7134 completed (loss: 0.0362214669585228, acc: 0.9926793575286865)
[2025-02-13 04:35:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:56][root][INFO] - Training Epoch: 2/2, step 6725/7134 completed (loss: 0.01746176742017269, acc: 0.9931740760803223)
[2025-02-13 04:35:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:57][root][INFO] - Training Epoch: 2/2, step 6726/7134 completed (loss: 0.056490037590265274, acc: 0.9899598360061646)
[2025-02-13 04:35:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:57][root][INFO] - Training Epoch: 2/2, step 6727/7134 completed (loss: 0.03250639885663986, acc: 0.9929577708244324)
[2025-02-13 04:35:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:58][root][INFO] - Training Epoch: 2/2, step 6728/7134 completed (loss: 0.043892763555049896, acc: 0.9907833933830261)
[2025-02-13 04:35:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:58][root][INFO] - Training Epoch: 2/2, step 6729/7134 completed (loss: 0.015618211589753628, acc: 0.9959239363670349)
[2025-02-13 04:35:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:58][root][INFO] - Training Epoch: 2/2, step 6730/7134 completed (loss: 0.025393139570951462, acc: 0.9952903985977173)
[2025-02-13 04:35:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:59][root][INFO] - Training Epoch: 2/2, step 6731/7134 completed (loss: 0.02186526358127594, acc: 0.993966817855835)
[2025-02-13 04:35:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:35:59][root][INFO] - Training Epoch: 2/2, step 6732/7134 completed (loss: 0.022260203957557678, acc: 0.9937106966972351)
[2025-02-13 04:35:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:00][root][INFO] - Training Epoch: 2/2, step 6733/7134 completed (loss: 0.03537489473819733, acc: 0.9867172837257385)
[2025-02-13 04:36:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:00][root][INFO] - Training Epoch: 2/2, step 6734/7134 completed (loss: 0.030972201377153397, acc: 0.9925233721733093)
[2025-02-13 04:36:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:01][root][INFO] - Training Epoch: 2/2, step 6735/7134 completed (loss: 0.03315303847193718, acc: 0.9907833933830261)
[2025-02-13 04:36:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:01][root][INFO] - Training Epoch: 2/2, step 6736/7134 completed (loss: 0.04413945972919464, acc: 0.9874652028083801)
[2025-02-13 04:36:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:01][root][INFO] - Training Epoch: 2/2, step 6737/7134 completed (loss: 0.036518849432468414, acc: 0.9821717739105225)
[2025-02-13 04:36:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:02][root][INFO] - Training Epoch: 2/2, step 6738/7134 completed (loss: 0.030704854056239128, acc: 0.9925373196601868)
[2025-02-13 04:36:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:02][root][INFO] - Training Epoch: 2/2, step 6739/7134 completed (loss: 0.033854879438877106, acc: 0.9871323704719543)
[2025-02-13 04:36:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:03][root][INFO] - Training Epoch: 2/2, step 6740/7134 completed (loss: 0.04162852093577385, acc: 0.9915110468864441)
[2025-02-13 04:36:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:03][root][INFO] - Training Epoch: 2/2, step 6741/7134 completed (loss: 0.015762783586978912, acc: 0.9927431344985962)
[2025-02-13 04:36:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:03][root][INFO] - Training Epoch: 2/2, step 6742/7134 completed (loss: 0.012827616184949875, acc: 0.9942280054092407)
[2025-02-13 04:36:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:04][root][INFO] - Training Epoch: 2/2, step 6743/7134 completed (loss: 0.012029837816953659, acc: 0.9986376166343689)
[2025-02-13 04:36:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:04][root][INFO] - Training Epoch: 2/2, step 6744/7134 completed (loss: 0.016281576827168465, acc: 0.9935897588729858)
[2025-02-13 04:36:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:05][root][INFO] - Training Epoch: 2/2, step 6745/7134 completed (loss: 0.007248337846249342, acc: 1.0)
[2025-02-13 04:36:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:05][root][INFO] - Training Epoch: 2/2, step 6746/7134 completed (loss: 0.006598586682230234, acc: 0.998236358165741)
[2025-02-13 04:36:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:06][root][INFO] - Training Epoch: 2/2, step 6747/7134 completed (loss: 0.020252950489521027, acc: 0.9975154995918274)
[2025-02-13 04:36:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:06][root][INFO] - Training Epoch: 2/2, step 6748/7134 completed (loss: 0.010233605280518532, acc: 0.9988725781440735)
[2025-02-13 04:36:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:06][root][INFO] - Training Epoch: 2/2, step 6749/7134 completed (loss: 0.01722703129053116, acc: 0.9934640526771545)
[2025-02-13 04:36:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:07][root][INFO] - Training Epoch: 2/2, step 6750/7134 completed (loss: 0.01168950367718935, acc: 0.9988465905189514)
[2025-02-13 04:36:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:07][root][INFO] - Training Epoch: 2/2, step 6751/7134 completed (loss: 0.014557876624166965, acc: 0.9965753555297852)
[2025-02-13 04:36:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:08][root][INFO] - Training Epoch: 2/2, step 6752/7134 completed (loss: 0.008407427929341793, acc: 0.9985486268997192)
[2025-02-13 04:36:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:08][root][INFO] - Training Epoch: 2/2, step 6753/7134 completed (loss: 0.017489412799477577, acc: 0.997706413269043)
[2025-02-13 04:36:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:09][root][INFO] - Training Epoch: 2/2, step 6754/7134 completed (loss: 0.00747464457526803, acc: 0.9987951517105103)
[2025-02-13 04:36:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:09][root][INFO] - Training Epoch: 2/2, step 6755/7134 completed (loss: 0.01098650973290205, acc: 0.9963054060935974)
[2025-02-13 04:36:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:10][root][INFO] - Training Epoch: 2/2, step 6756/7134 completed (loss: 0.008374989032745361, acc: 0.9946380853652954)
[2025-02-13 04:36:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:10][root][INFO] - Training Epoch: 2/2, step 6757/7134 completed (loss: 0.006123165600001812, acc: 0.9984756112098694)
[2025-02-13 04:36:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:11][root][INFO] - Training Epoch: 2/2, step 6758/7134 completed (loss: 0.007132896687835455, acc: 0.9974126815795898)
[2025-02-13 04:36:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:11][root][INFO] - Training Epoch: 2/2, step 6759/7134 completed (loss: 0.016952626407146454, acc: 0.9944071769714355)
[2025-02-13 04:36:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:11][root][INFO] - Training Epoch: 2/2, step 6760/7134 completed (loss: 0.01623627543449402, acc: 0.9960629940032959)
[2025-02-13 04:36:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:12][root][INFO] - Training Epoch: 2/2, step 6761/7134 completed (loss: 0.015962574630975723, acc: 0.9927710890769958)
[2025-02-13 04:36:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:12][root][INFO] - Training Epoch: 2/2, step 6762/7134 completed (loss: 0.024924039840698242, acc: 0.9889705777168274)
[2025-02-13 04:36:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:13][root][INFO] - Training Epoch: 2/2, step 6763/7134 completed (loss: 0.006142090540379286, acc: 0.9974905848503113)
[2025-02-13 04:36:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:13][root][INFO] - Training Epoch: 2/2, step 6764/7134 completed (loss: 0.01445780135691166, acc: 0.9956896305084229)
[2025-02-13 04:36:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:14][root][INFO] - Training Epoch: 2/2, step 6765/7134 completed (loss: 0.018173104152083397, acc: 0.9946737885475159)
[2025-02-13 04:36:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:14][root][INFO] - Training Epoch: 2/2, step 6766/7134 completed (loss: 0.005780945997685194, acc: 0.9972413778305054)
[2025-02-13 04:36:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:14][root][INFO] - Training Epoch: 2/2, step 6767/7134 completed (loss: 0.04615122824907303, acc: 0.9932432174682617)
[2025-02-13 04:36:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:15][root][INFO] - Training Epoch: 2/2, step 6768/7134 completed (loss: 0.005800084210932255, acc: 0.9980988502502441)
[2025-02-13 04:36:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:15][root][INFO] - Training Epoch: 2/2, step 6769/7134 completed (loss: 0.00930526852607727, acc: 0.9969135522842407)
[2025-02-13 04:36:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:16][root][INFO] - Training Epoch: 2/2, step 6770/7134 completed (loss: 0.019065838307142258, acc: 0.991946280002594)
[2025-02-13 04:36:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:16][root][INFO] - Training Epoch: 2/2, step 6771/7134 completed (loss: 0.02731035277247429, acc: 0.9936908483505249)
[2025-02-13 04:36:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:17][root][INFO] - Training Epoch: 2/2, step 6772/7134 completed (loss: 0.03864569589495659, acc: 0.9928698539733887)
[2025-02-13 04:36:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:17][root][INFO] - Training Epoch: 2/2, step 6773/7134 completed (loss: 0.028423022478818893, acc: 0.9905063509941101)
[2025-02-13 04:36:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:18][root][INFO] - Training Epoch: 2/2, step 6774/7134 completed (loss: 0.025444824248552322, acc: 0.993630588054657)
[2025-02-13 04:36:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:18][root][INFO] - Training Epoch: 2/2, step 6775/7134 completed (loss: 0.01281172875314951, acc: 0.996219277381897)
[2025-02-13 04:36:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:18][root][INFO] - Training Epoch: 2/2, step 6776/7134 completed (loss: 0.024599621072411537, acc: 0.9893617033958435)
[2025-02-13 04:36:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:19][root][INFO] - Training Epoch: 2/2, step 6777/7134 completed (loss: 0.017717592418193817, acc: 0.9946308732032776)
[2025-02-13 04:36:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:19][root][INFO] - Training Epoch: 2/2, step 6778/7134 completed (loss: 0.021317079663276672, acc: 0.9925705790519714)
[2025-02-13 04:36:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:20][root][INFO] - Training Epoch: 2/2, step 6779/7134 completed (loss: 0.012706169858574867, acc: 0.9955223798751831)
[2025-02-13 04:36:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:20][root][INFO] - Training Epoch: 2/2, step 6780/7134 completed (loss: 0.021100619807839394, acc: 0.9955157041549683)
[2025-02-13 04:36:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:21][root][INFO] - Training Epoch: 2/2, step 6781/7134 completed (loss: 0.013574107550084591, acc: 0.9955089688301086)
[2025-02-13 04:36:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:21][root][INFO] - Training Epoch: 2/2, step 6782/7134 completed (loss: 0.012427914887666702, acc: 0.9965277910232544)
[2025-02-13 04:36:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:21][root][INFO] - Training Epoch: 2/2, step 6783/7134 completed (loss: 0.026438549160957336, acc: 0.9923224449157715)
[2025-02-13 04:36:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:22][root][INFO] - Training Epoch: 2/2, step 6784/7134 completed (loss: 0.013803331181406975, acc: 0.9966722130775452)
[2025-02-13 04:36:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:22][root][INFO] - Training Epoch: 2/2, step 6785/7134 completed (loss: 0.009763709269464016, acc: 0.9952830076217651)
[2025-02-13 04:36:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:23][root][INFO] - Training Epoch: 2/2, step 6786/7134 completed (loss: 0.006163796875625849, acc: 0.9985611438751221)
[2025-02-13 04:36:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:23][root][INFO] - Training Epoch: 2/2, step 6787/7134 completed (loss: 0.02409203350543976, acc: 0.99210524559021)
[2025-02-13 04:36:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:23][root][INFO] - Training Epoch: 2/2, step 6788/7134 completed (loss: 0.01141746249049902, acc: 0.9964028596878052)
[2025-02-13 04:36:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:24][root][INFO] - Training Epoch: 2/2, step 6789/7134 completed (loss: 0.013690604828298092, acc: 0.9961089491844177)
[2025-02-13 04:36:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:24][root][INFO] - Training Epoch: 2/2, step 6790/7134 completed (loss: 0.029705924913287163, acc: 0.9928160905838013)
[2025-02-13 04:36:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:25][root][INFO] - Training Epoch: 2/2, step 6791/7134 completed (loss: 0.020979681983590126, acc: 0.991482138633728)
[2025-02-13 04:36:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:25][root][INFO] - Training Epoch: 2/2, step 6792/7134 completed (loss: 0.005055127665400505, acc: 1.0)
[2025-02-13 04:36:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:26][root][INFO] - Training Epoch: 2/2, step 6793/7134 completed (loss: 0.007252547424286604, acc: 0.9957805871963501)
[2025-02-13 04:36:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:26][root][INFO] - Training Epoch: 2/2, step 6794/7134 completed (loss: 0.007370030973106623, acc: 0.9983579516410828)
[2025-02-13 04:36:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:26][root][INFO] - Training Epoch: 2/2, step 6795/7134 completed (loss: 0.0035737319849431515, acc: 0.998701274394989)
[2025-02-13 04:36:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:27][root][INFO] - Training Epoch: 2/2, step 6796/7134 completed (loss: 0.006486851256340742, acc: 0.9985337257385254)
[2025-02-13 04:36:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:27][root][INFO] - Training Epoch: 2/2, step 6797/7134 completed (loss: 0.02765537239611149, acc: 0.9898107647895813)
[2025-02-13 04:36:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:28][root][INFO] - Training Epoch: 2/2, step 6798/7134 completed (loss: 0.013341403566300869, acc: 0.997245192527771)
[2025-02-13 04:36:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:28][root][INFO] - Training Epoch: 2/2, step 6799/7134 completed (loss: 0.009295901283621788, acc: 0.99726402759552)
[2025-02-13 04:36:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:29][root][INFO] - Training Epoch: 2/2, step 6800/7134 completed (loss: 0.01640518195927143, acc: 0.9944751262664795)
[2025-02-13 04:36:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:29][root][INFO] - Training Epoch: 2/2, step 6801/7134 completed (loss: 0.024831868708133698, acc: 0.9937008023262024)
[2025-02-13 04:36:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:29][root][INFO] - Training Epoch: 2/2, step 6802/7134 completed (loss: 0.011930807493627071, acc: 0.9985358715057373)
[2025-02-13 04:36:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:30][root][INFO] - Training Epoch: 2/2, step 6803/7134 completed (loss: 0.005575683433562517, acc: 0.9987421631813049)
[2025-02-13 04:36:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:30][root][INFO] - Training Epoch: 2/2, step 6804/7134 completed (loss: 0.03262089937925339, acc: 0.995312511920929)
[2025-02-13 04:36:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:31][root][INFO] - Training Epoch: 2/2, step 6805/7134 completed (loss: 0.013673675246536732, acc: 0.995192289352417)
[2025-02-13 04:36:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:31][root][INFO] - Training Epoch: 2/2, step 6806/7134 completed (loss: 0.02332468330860138, acc: 0.9936708807945251)
[2025-02-13 04:36:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:32][root][INFO] - Training Epoch: 2/2, step 6807/7134 completed (loss: 0.012712948955595493, acc: 0.9942528605461121)
[2025-02-13 04:36:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:32][root][INFO] - Training Epoch: 2/2, step 6808/7134 completed (loss: 0.02598763443529606, acc: 0.9920254945755005)
[2025-02-13 04:36:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:32][root][INFO] - Training Epoch: 2/2, step 6809/7134 completed (loss: 0.035279061645269394, acc: 0.9905149340629578)
[2025-02-13 04:36:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:33][root][INFO] - Training Epoch: 2/2, step 6810/7134 completed (loss: 0.01024415623396635, acc: 0.99210524559021)
[2025-02-13 04:36:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:33][root][INFO] - Training Epoch: 2/2, step 6811/7134 completed (loss: 0.03378560394048691, acc: 0.9921362996101379)
[2025-02-13 04:36:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:34][root][INFO] - Training Epoch: 2/2, step 6812/7134 completed (loss: 0.008443830534815788, acc: 0.9951377511024475)
[2025-02-13 04:36:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:34][root][INFO] - Training Epoch: 2/2, step 6813/7134 completed (loss: 0.010425812564790249, acc: 0.9943661689758301)
[2025-02-13 04:36:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:35][root][INFO] - Training Epoch: 2/2, step 6814/7134 completed (loss: 0.009829435497522354, acc: 0.9973118305206299)
[2025-02-13 04:36:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:35][root][INFO] - Training Epoch: 2/2, step 6815/7134 completed (loss: 0.028068266808986664, acc: 0.9921156167984009)
[2025-02-13 04:36:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:35][root][INFO] - Training Epoch: 2/2, step 6816/7134 completed (loss: 0.01000396627932787, acc: 0.9966555237770081)
[2025-02-13 04:36:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:36][root][INFO] - Training Epoch: 2/2, step 6817/7134 completed (loss: 0.03961766138672829, acc: 0.9931623935699463)
[2025-02-13 04:36:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:36][root][INFO] - Training Epoch: 2/2, step 6818/7134 completed (loss: 0.011858508922159672, acc: 0.9940029978752136)
[2025-02-13 04:36:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:37][root][INFO] - Training Epoch: 2/2, step 6819/7134 completed (loss: 0.03142128512263298, acc: 0.9910072088241577)
[2025-02-13 04:36:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:37][root][INFO] - Training Epoch: 2/2, step 6820/7134 completed (loss: 0.015125343576073647, acc: 0.9963964223861694)
[2025-02-13 04:36:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:38][root][INFO] - Training Epoch: 2/2, step 6821/7134 completed (loss: 0.014527480117976665, acc: 0.991919219493866)
[2025-02-13 04:36:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:38][root][INFO] - Training Epoch: 2/2, step 6822/7134 completed (loss: 0.03654837608337402, acc: 0.9895287752151489)
[2025-02-13 04:36:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:38][root][INFO] - Training Epoch: 2/2, step 6823/7134 completed (loss: 0.05050624907016754, acc: 0.9885550737380981)
[2025-02-13 04:36:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:39][root][INFO] - Training Epoch: 2/2, step 6824/7134 completed (loss: 0.047707054764032364, acc: 0.9818548560142517)
[2025-02-13 04:36:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:39][root][INFO] - Training Epoch: 2/2, step 6825/7134 completed (loss: 0.012395924888551235, acc: 0.995121955871582)
[2025-02-13 04:36:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:40][root][INFO] - Training Epoch: 2/2, step 6826/7134 completed (loss: 0.03176259249448776, acc: 0.9882155060768127)
[2025-02-13 04:36:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:40][root][INFO] - Training Epoch: 2/2, step 6827/7134 completed (loss: 0.08820902556180954, acc: 0.9780927896499634)
[2025-02-13 04:36:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:40][root][INFO] - Training Epoch: 2/2, step 6828/7134 completed (loss: 0.03666079044342041, acc: 0.989051103591919)
[2025-02-13 04:36:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:41][root][INFO] - Training Epoch: 2/2, step 6829/7134 completed (loss: 0.03720257058739662, acc: 0.9896142482757568)
[2025-02-13 04:36:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:41][root][INFO] - Training Epoch: 2/2, step 6830/7134 completed (loss: 0.01766839250922203, acc: 0.9950576424598694)
[2025-02-13 04:36:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:42][root][INFO] - Training Epoch: 2/2, step 6831/7134 completed (loss: 0.028898293152451515, acc: 0.9905914068222046)
[2025-02-13 04:36:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:42][root][INFO] - Training Epoch: 2/2, step 6832/7134 completed (loss: 0.024454161524772644, acc: 0.9944547414779663)
[2025-02-13 04:36:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:42][root][INFO] - Training Epoch: 2/2, step 6833/7134 completed (loss: 0.026934335008263588, acc: 0.9919614195823669)
[2025-02-13 04:36:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:43][root][INFO] - Training Epoch: 2/2, step 6834/7134 completed (loss: 0.027963843196630478, acc: 0.991482138633728)
[2025-02-13 04:36:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:43][root][INFO] - Training Epoch: 2/2, step 6835/7134 completed (loss: 0.009406602010130882, acc: 0.9967532753944397)
[2025-02-13 04:36:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:44][root][INFO] - Training Epoch: 2/2, step 6836/7134 completed (loss: 0.02882789634168148, acc: 0.9899497628211975)
[2025-02-13 04:36:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:44][root][INFO] - Training Epoch: 2/2, step 6837/7134 completed (loss: 0.022710219025611877, acc: 0.9901840686798096)
[2025-02-13 04:36:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:44][root][INFO] - Training Epoch: 2/2, step 6838/7134 completed (loss: 0.0567677803337574, acc: 0.9857549667358398)
[2025-02-13 04:36:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:45][root][INFO] - Training Epoch: 2/2, step 6839/7134 completed (loss: 0.0719098225235939, acc: 0.9801980257034302)
[2025-02-13 04:36:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:45][root][INFO] - Training Epoch: 2/2, step 6840/7134 completed (loss: 0.04581788182258606, acc: 0.9813753366470337)
[2025-02-13 04:36:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:46][root][INFO] - Training Epoch: 2/2, step 6841/7134 completed (loss: 0.030841534957289696, acc: 0.9884560108184814)
[2025-02-13 04:36:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:46][root][INFO] - Training Epoch: 2/2, step 6842/7134 completed (loss: 0.017110617831349373, acc: 0.9949874877929688)
[2025-02-13 04:36:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:47][root][INFO] - Training Epoch: 2/2, step 6843/7134 completed (loss: 0.039548035711050034, acc: 0.9864364862442017)
[2025-02-13 04:36:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:47][root][INFO] - Training Epoch: 2/2, step 6844/7134 completed (loss: 0.016301782801747322, acc: 0.9930915236473083)
[2025-02-13 04:36:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:47][root][INFO] - Training Epoch: 2/2, step 6845/7134 completed (loss: 0.01837301440536976, acc: 0.9931972622871399)
[2025-02-13 04:36:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:48][root][INFO] - Training Epoch: 2/2, step 6846/7134 completed (loss: 0.01462723407894373, acc: 0.9985673427581787)
[2025-02-13 04:36:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:48][root][INFO] - Training Epoch: 2/2, step 6847/7134 completed (loss: 0.022802194580435753, acc: 0.9954545497894287)
[2025-02-13 04:36:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:49][root][INFO] - Training Epoch: 2/2, step 6848/7134 completed (loss: 0.00623804796487093, acc: 1.0)
[2025-02-13 04:36:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:49][root][INFO] - Training Epoch: 2/2, step 6849/7134 completed (loss: 0.0229964517056942, acc: 0.9948453903198242)
[2025-02-13 04:36:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:50][root][INFO] - Training Epoch: 2/2, step 6850/7134 completed (loss: 0.012723371386528015, acc: 0.9931350350379944)
[2025-02-13 04:36:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:50][root][INFO] - Training Epoch: 2/2, step 6851/7134 completed (loss: 0.037403445690870285, acc: 0.9901269674301147)
[2025-02-13 04:36:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:50][root][INFO] - Training Epoch: 2/2, step 6852/7134 completed (loss: 0.0149791045114398, acc: 0.9930070042610168)
[2025-02-13 04:36:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:51][root][INFO] - Training Epoch: 2/2, step 6853/7134 completed (loss: 0.02575230598449707, acc: 0.9903181195259094)
[2025-02-13 04:36:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:51][root][INFO] - Training Epoch: 2/2, step 6854/7134 completed (loss: 0.024945970624685287, acc: 0.9934924244880676)
[2025-02-13 04:36:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:52][root][INFO] - Training Epoch: 2/2, step 6855/7134 completed (loss: 0.0048746513202786446, acc: 1.0)
[2025-02-13 04:36:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:52][root][INFO] - Training Epoch: 2/2, step 6856/7134 completed (loss: 0.0020942646078765392, acc: 1.0)
[2025-02-13 04:36:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:53][root][INFO] - Training Epoch: 2/2, step 6857/7134 completed (loss: 0.01228369027376175, acc: 0.9970674514770508)
[2025-02-13 04:36:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:53][root][INFO] - Training Epoch: 2/2, step 6858/7134 completed (loss: 0.0006723468541167676, acc: 1.0)
[2025-02-13 04:36:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:53][root][INFO] - Training Epoch: 2/2, step 6859/7134 completed (loss: 0.003160261083394289, acc: 0.9987195730209351)
[2025-02-13 04:36:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:54][root][INFO] - Training Epoch: 2/2, step 6860/7134 completed (loss: 0.01316196471452713, acc: 0.9964538812637329)
[2025-02-13 04:36:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:54][root][INFO] - Training Epoch: 2/2, step 6861/7134 completed (loss: 0.009796218946576118, acc: 0.9982699155807495)
[2025-02-13 04:36:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:55][root][INFO] - Training Epoch: 2/2, step 6862/7134 completed (loss: 0.026598971337080002, acc: 0.9942857027053833)
[2025-02-13 04:36:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:55][root][INFO] - Training Epoch: 2/2, step 6863/7134 completed (loss: 0.007666958961635828, acc: 0.9985875487327576)
[2025-02-13 04:36:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:55][root][INFO] - Training Epoch: 2/2, step 6864/7134 completed (loss: 0.011872278526425362, acc: 0.9966722130775452)
[2025-02-13 04:36:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:56][root][INFO] - Training Epoch: 2/2, step 6865/7134 completed (loss: 0.043907277286052704, acc: 0.985567033290863)
[2025-02-13 04:36:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:56][root][INFO] - Training Epoch: 2/2, step 6866/7134 completed (loss: 0.008681455627083778, acc: 0.9984802603721619)
[2025-02-13 04:36:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:57][root][INFO] - Training Epoch: 2/2, step 6867/7134 completed (loss: 0.0017760309856384993, acc: 1.0)
[2025-02-13 04:36:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:57][root][INFO] - Training Epoch: 2/2, step 6868/7134 completed (loss: 0.0012093938421458006, acc: 1.0)
[2025-02-13 04:36:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:57][root][INFO] - Training Epoch: 2/2, step 6869/7134 completed (loss: 0.013176795095205307, acc: 0.9978586435317993)
[2025-02-13 04:36:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:58][root][INFO] - Training Epoch: 2/2, step 6870/7134 completed (loss: 0.018756406381726265, acc: 0.9967426657676697)
[2025-02-13 04:36:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:58][root][INFO] - Training Epoch: 2/2, step 6871/7134 completed (loss: 0.002085719723254442, acc: 1.0)
[2025-02-13 04:36:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:59][root][INFO] - Training Epoch: 2/2, step 6872/7134 completed (loss: 0.0057394374161958694, acc: 0.9986149668693542)
[2025-02-13 04:36:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:36:59][root][INFO] - Training Epoch: 2/2, step 6873/7134 completed (loss: 0.006663365289568901, acc: 0.9957982897758484)
[2025-02-13 04:36:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:00][root][INFO] - Training Epoch: 2/2, step 6874/7134 completed (loss: 0.009639838710427284, acc: 0.9982269406318665)
[2025-02-13 04:37:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:00][root][INFO] - Training Epoch: 2/2, step 6875/7134 completed (loss: 0.011766927316784859, acc: 0.9953632354736328)
[2025-02-13 04:37:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:00][root][INFO] - Training Epoch: 2/2, step 6876/7134 completed (loss: 0.007111539598554373, acc: 0.9983079433441162)
[2025-02-13 04:37:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:01][root][INFO] - Training Epoch: 2/2, step 6877/7134 completed (loss: 0.015650568529963493, acc: 0.9971910119056702)
[2025-02-13 04:37:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:01][root][INFO] - Training Epoch: 2/2, step 6878/7134 completed (loss: 0.010624956339597702, acc: 0.9970015287399292)
[2025-02-13 04:37:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:02][root][INFO] - Training Epoch: 2/2, step 6879/7134 completed (loss: 0.0028217234648764133, acc: 1.0)
[2025-02-13 04:37:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:02][root][INFO] - Training Epoch: 2/2, step 6880/7134 completed (loss: 0.008561138063669205, acc: 0.9953434467315674)
[2025-02-13 04:37:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:03][root][INFO] - Training Epoch: 2/2, step 6881/7134 completed (loss: 0.03972315043210983, acc: 0.9939246773719788)
[2025-02-13 04:37:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:03][root][INFO] - Training Epoch: 2/2, step 6882/7134 completed (loss: 0.026534179225564003, acc: 0.9906014800071716)
[2025-02-13 04:37:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:03][root][INFO] - Training Epoch: 2/2, step 6883/7134 completed (loss: 0.007198346313089132, acc: 0.9983498454093933)
[2025-02-13 04:37:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:04][root][INFO] - Training Epoch: 2/2, step 6884/7134 completed (loss: 0.03828718140721321, acc: 0.99370276927948)
[2025-02-13 04:37:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:04][root][INFO] - Training Epoch: 2/2, step 6885/7134 completed (loss: 0.03386484831571579, acc: 0.9910714030265808)
[2025-02-13 04:37:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:05][root][INFO] - Training Epoch: 2/2, step 6886/7134 completed (loss: 0.016896309331059456, acc: 0.995245635509491)
[2025-02-13 04:37:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:05][root][INFO] - Training Epoch: 2/2, step 6887/7134 completed (loss: 0.019478222355246544, acc: 0.9938398599624634)
[2025-02-13 04:37:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:06][root][INFO] - Training Epoch: 2/2, step 6888/7134 completed (loss: 0.021244775503873825, acc: 0.9950166344642639)
[2025-02-13 04:37:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:06][root][INFO] - Training Epoch: 2/2, step 6889/7134 completed (loss: 0.0033596667926758528, acc: 1.0)
[2025-02-13 04:37:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:06][root][INFO] - Training Epoch: 2/2, step 6890/7134 completed (loss: 0.020685886964201927, acc: 0.9920739531517029)
[2025-02-13 04:37:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:07][root][INFO] - Training Epoch: 2/2, step 6891/7134 completed (loss: 0.016662705689668655, acc: 0.995555579662323)
[2025-02-13 04:37:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:07][root][INFO] - Training Epoch: 2/2, step 6892/7134 completed (loss: 0.019363360479474068, acc: 0.99609375)
[2025-02-13 04:37:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:08][root][INFO] - Training Epoch: 2/2, step 6893/7134 completed (loss: 0.032405950129032135, acc: 0.985981285572052)
[2025-02-13 04:37:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:08][root][INFO] - Training Epoch: 2/2, step 6894/7134 completed (loss: 0.02427629940211773, acc: 0.9934210777282715)
[2025-02-13 04:37:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:09][root][INFO] - Training Epoch: 2/2, step 6895/7134 completed (loss: 0.026287898421287537, acc: 0.9880095720291138)
[2025-02-13 04:37:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:09][root][INFO] - Training Epoch: 2/2, step 6896/7134 completed (loss: 0.01693226397037506, acc: 0.9971014261245728)
[2025-02-13 04:37:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:10][root][INFO] - Training Epoch: 2/2, step 6897/7134 completed (loss: 0.014931495301425457, acc: 0.9963189959526062)
[2025-02-13 04:37:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:10][root][INFO] - Training Epoch: 2/2, step 6898/7134 completed (loss: 0.009771477431058884, acc: 0.9958791136741638)
[2025-02-13 04:37:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:10][root][INFO] - Training Epoch: 2/2, step 6899/7134 completed (loss: 0.01703018881380558, acc: 0.9937185645103455)
[2025-02-13 04:37:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:11][root][INFO] - Training Epoch: 2/2, step 6900/7134 completed (loss: 0.02183712273836136, acc: 0.9957567453384399)
[2025-02-13 04:37:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:11][root][INFO] - Training Epoch: 2/2, step 6901/7134 completed (loss: 0.03586186096072197, acc: 0.9947159886360168)
[2025-02-13 04:37:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:12][root][INFO] - Training Epoch: 2/2, step 6902/7134 completed (loss: 0.020068997517228127, acc: 0.9916782379150391)
[2025-02-13 04:37:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:12][root][INFO] - Training Epoch: 2/2, step 6903/7134 completed (loss: 0.012442488223314285, acc: 0.9974651336669922)
[2025-02-13 04:37:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:13][root][INFO] - Training Epoch: 2/2, step 6904/7134 completed (loss: 0.009069972671568394, acc: 0.996268630027771)
[2025-02-13 04:37:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:13][root][INFO] - Training Epoch: 2/2, step 6905/7134 completed (loss: 0.019677218049764633, acc: 0.9961636662483215)
[2025-02-13 04:37:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:14][root][INFO] - Training Epoch: 2/2, step 6906/7134 completed (loss: 0.03295255824923515, acc: 0.9940119981765747)
[2025-02-13 04:37:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:14][root][INFO] - Training Epoch: 2/2, step 6907/7134 completed (loss: 0.02357795275747776, acc: 0.996277928352356)
[2025-02-13 04:37:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:14][root][INFO] - Training Epoch: 2/2, step 6908/7134 completed (loss: 0.018171144649386406, acc: 0.9922279715538025)
[2025-02-13 04:37:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:15][root][INFO] - Training Epoch: 2/2, step 6909/7134 completed (loss: 0.012917266227304935, acc: 0.9947368502616882)
[2025-02-13 04:37:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:15][root][INFO] - Training Epoch: 2/2, step 6910/7134 completed (loss: 0.014777252450585365, acc: 0.9949044585227966)
[2025-02-13 04:37:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:16][root][INFO] - Training Epoch: 2/2, step 6911/7134 completed (loss: 0.025781188160181046, acc: 0.9941860437393188)
[2025-02-13 04:37:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:16][root][INFO] - Training Epoch: 2/2, step 6912/7134 completed (loss: 0.017847945913672447, acc: 0.9962546825408936)
[2025-02-13 04:37:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:17][root][INFO] - Training Epoch: 2/2, step 6913/7134 completed (loss: 0.024525638669729233, acc: 0.9951279163360596)
[2025-02-13 04:37:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:17][root][INFO] - Training Epoch: 2/2, step 6914/7134 completed (loss: 0.06267257034778595, acc: 0.9898989796638489)
[2025-02-13 04:37:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:18][root][INFO] - Training Epoch: 2/2, step 6915/7134 completed (loss: 0.015279168263077736, acc: 0.9948186278343201)
[2025-02-13 04:37:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:18][root][INFO] - Training Epoch: 2/2, step 6916/7134 completed (loss: 0.02020309306681156, acc: 0.9975460171699524)
[2025-02-13 04:37:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:19][root][INFO] - Training Epoch: 2/2, step 6917/7134 completed (loss: 0.03260394558310509, acc: 0.9949495196342468)
[2025-02-13 04:37:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:19][root][INFO] - Training Epoch: 2/2, step 6918/7134 completed (loss: 0.016282647848129272, acc: 0.9953161478042603)
[2025-02-13 04:37:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:19][root][INFO] - Training Epoch: 2/2, step 6919/7134 completed (loss: 0.018850205466151237, acc: 0.9953863620758057)
[2025-02-13 04:37:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:20][root][INFO] - Training Epoch: 2/2, step 6920/7134 completed (loss: 0.008962573483586311, acc: 0.9971510171890259)
[2025-02-13 04:37:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:20][root][INFO] - Training Epoch: 2/2, step 6921/7134 completed (loss: 0.027981368824839592, acc: 0.9930716156959534)
[2025-02-13 04:37:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:21][root][INFO] - Training Epoch: 2/2, step 6922/7134 completed (loss: 0.04267170652747154, acc: 0.9890795350074768)
[2025-02-13 04:37:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:21][root][INFO] - Training Epoch: 2/2, step 6923/7134 completed (loss: 0.04129575565457344, acc: 0.9922178983688354)
[2025-02-13 04:37:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:22][root][INFO] - Training Epoch: 2/2, step 6924/7134 completed (loss: 0.02060704305768013, acc: 0.9931880235671997)
[2025-02-13 04:37:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:22][root][INFO] - Training Epoch: 2/2, step 6925/7134 completed (loss: 0.004581271205097437, acc: 1.0)
[2025-02-13 04:37:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:22][root][INFO] - Training Epoch: 2/2, step 6926/7134 completed (loss: 0.014660745859146118, acc: 0.994575023651123)
[2025-02-13 04:37:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:23][root][INFO] - Training Epoch: 2/2, step 6927/7134 completed (loss: 0.024650245904922485, acc: 0.9964028596878052)
[2025-02-13 04:37:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:23][root][INFO] - Training Epoch: 2/2, step 6928/7134 completed (loss: 0.031750455498695374, acc: 0.9921721816062927)
[2025-02-13 04:37:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:24][root][INFO] - Training Epoch: 2/2, step 6929/7134 completed (loss: 0.02893492951989174, acc: 0.9929378628730774)
[2025-02-13 04:37:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:24][root][INFO] - Training Epoch: 2/2, step 6930/7134 completed (loss: 0.01280179899185896, acc: 0.9950371980667114)
[2025-02-13 04:37:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:25][root][INFO] - Training Epoch: 2/2, step 6931/7134 completed (loss: 0.020374460145831108, acc: 0.995121955871582)
[2025-02-13 04:37:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:25][root][INFO] - Training Epoch: 2/2, step 6932/7134 completed (loss: 0.030377749353647232, acc: 0.9882352948188782)
[2025-02-13 04:37:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:26][root][INFO] - Training Epoch: 2/2, step 6933/7134 completed (loss: 0.04951109364628792, acc: 0.9896507263183594)
[2025-02-13 04:37:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:26][root][INFO] - Training Epoch: 2/2, step 6934/7134 completed (loss: 0.02560633420944214, acc: 0.9919742941856384)
[2025-02-13 04:37:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:26][root][INFO] - Training Epoch: 2/2, step 6935/7134 completed (loss: 0.031122418120503426, acc: 0.9934554696083069)
[2025-02-13 04:37:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:27][root][INFO] - Training Epoch: 2/2, step 6936/7134 completed (loss: 0.008240638300776482, acc: 0.9959999918937683)
[2025-02-13 04:37:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:27][root][INFO] - Training Epoch: 2/2, step 6937/7134 completed (loss: 0.09288312494754791, acc: 0.9789842367172241)
[2025-02-13 04:37:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:28][root][INFO] - Training Epoch: 2/2, step 6938/7134 completed (loss: 0.0851893499493599, acc: 0.9765625)
[2025-02-13 04:37:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:28][root][INFO] - Training Epoch: 2/2, step 6939/7134 completed (loss: 0.049743279814720154, acc: 0.9860917925834656)
[2025-02-13 04:37:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:29][root][INFO] - Training Epoch: 2/2, step 6940/7134 completed (loss: 0.055082518607378006, acc: 0.9845890402793884)
[2025-02-13 04:37:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:29][root][INFO] - Training Epoch: 2/2, step 6941/7134 completed (loss: 0.02344539761543274, acc: 0.9965277910232544)
[2025-02-13 04:37:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:30][root][INFO] - Training Epoch: 2/2, step 6942/7134 completed (loss: 0.031012559309601784, acc: 0.9931507110595703)
[2025-02-13 04:37:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:30][root][INFO] - Training Epoch: 2/2, step 6943/7134 completed (loss: 0.024363074451684952, acc: 0.991631805896759)
[2025-02-13 04:37:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:30][root][INFO] - Training Epoch: 2/2, step 6944/7134 completed (loss: 0.02362947165966034, acc: 0.9968152642250061)
[2025-02-13 04:37:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:31][root][INFO] - Training Epoch: 2/2, step 6945/7134 completed (loss: 0.05135694891214371, acc: 0.9860383868217468)
[2025-02-13 04:37:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:31][root][INFO] - Training Epoch: 2/2, step 6946/7134 completed (loss: 0.023744918406009674, acc: 0.9914039969444275)
[2025-02-13 04:37:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:32][root][INFO] - Training Epoch: 2/2, step 6947/7134 completed (loss: 0.018705355003476143, acc: 0.9956521987915039)
[2025-02-13 04:37:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:32][root][INFO] - Training Epoch: 2/2, step 6948/7134 completed (loss: 0.02419142611324787, acc: 0.9949748516082764)
[2025-02-13 04:37:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:32][root][INFO] - Training Epoch: 2/2, step 6949/7134 completed (loss: 0.026841916143894196, acc: 0.9914529919624329)
[2025-02-13 04:37:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:33][root][INFO] - Training Epoch: 2/2, step 6950/7134 completed (loss: 0.023467285558581352, acc: 0.9948979616165161)
[2025-02-13 04:37:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:33][root][INFO] - Training Epoch: 2/2, step 6951/7134 completed (loss: 0.01809818670153618, acc: 0.991525411605835)
[2025-02-13 04:37:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:34][root][INFO] - Training Epoch: 2/2, step 6952/7134 completed (loss: 0.011733464896678925, acc: 0.9953271150588989)
[2025-02-13 04:37:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:34][root][INFO] - Training Epoch: 2/2, step 6953/7134 completed (loss: 0.009376388043165207, acc: 1.0)
[2025-02-13 04:37:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:34][root][INFO] - Training Epoch: 2/2, step 6954/7134 completed (loss: 0.05302508920431137, acc: 0.9893389940261841)
[2025-02-13 04:37:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:35][root][INFO] - Training Epoch: 2/2, step 6955/7134 completed (loss: 0.02801213413476944, acc: 0.9878048896789551)
[2025-02-13 04:37:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:35][root][INFO] - Training Epoch: 2/2, step 6956/7134 completed (loss: 0.015132451429963112, acc: 0.9964476227760315)
[2025-02-13 04:37:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:36][root][INFO] - Training Epoch: 2/2, step 6957/7134 completed (loss: 0.009595909155905247, acc: 0.9980430603027344)
[2025-02-13 04:37:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:36][root][INFO] - Training Epoch: 2/2, step 6958/7134 completed (loss: 0.0452510230243206, acc: 0.9844054579734802)
[2025-02-13 04:37:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:36][root][INFO] - Training Epoch: 2/2, step 6959/7134 completed (loss: 0.03522908315062523, acc: 0.9866962432861328)
[2025-02-13 04:37:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:37][root][INFO] - Training Epoch: 2/2, step 6960/7134 completed (loss: 0.0111555065959692, acc: 0.9967479705810547)
[2025-02-13 04:37:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:37][root][INFO] - Training Epoch: 2/2, step 6961/7134 completed (loss: 0.03133181482553482, acc: 0.9939393997192383)
[2025-02-13 04:37:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:38][root][INFO] - Training Epoch: 2/2, step 6962/7134 completed (loss: 0.026653779670596123, acc: 0.9947826266288757)
[2025-02-13 04:37:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:38][root][INFO] - Training Epoch: 2/2, step 6963/7134 completed (loss: 0.01914120838046074, acc: 0.9949495196342468)
[2025-02-13 04:37:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:39][root][INFO] - Training Epoch: 2/2, step 6964/7134 completed (loss: 0.01176478061825037, acc: 0.9981949329376221)
[2025-02-13 04:37:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:39][root][INFO] - Training Epoch: 2/2, step 6965/7134 completed (loss: 0.023431794717907906, acc: 0.9908424615859985)
[2025-02-13 04:37:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:39][root][INFO] - Training Epoch: 2/2, step 6966/7134 completed (loss: 0.029309570789337158, acc: 0.9900249242782593)
[2025-02-13 04:37:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:40][root][INFO] - Training Epoch: 2/2, step 6967/7134 completed (loss: 0.01397745218127966, acc: 0.9954751133918762)
[2025-02-13 04:37:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:40][root][INFO] - Training Epoch: 2/2, step 6968/7134 completed (loss: 0.015397117473185062, acc: 0.9965517520904541)
[2025-02-13 04:37:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:41][root][INFO] - Training Epoch: 2/2, step 6969/7134 completed (loss: 0.08365210890769958, acc: 0.9771309494972229)
[2025-02-13 04:37:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:41][root][INFO] - Training Epoch: 2/2, step 6970/7134 completed (loss: 0.04297536984086037, acc: 0.9894737005233765)
[2025-02-13 04:37:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:41][root][INFO] - Training Epoch: 2/2, step 6971/7134 completed (loss: 0.0463220588862896, acc: 0.9861830472946167)
[2025-02-13 04:37:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:42][root][INFO] - Training Epoch: 2/2, step 6972/7134 completed (loss: 0.009633789770305157, acc: 0.9964349269866943)
[2025-02-13 04:37:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:42][root][INFO] - Training Epoch: 2/2, step 6973/7134 completed (loss: 0.026489997282624245, acc: 0.9933920502662659)
[2025-02-13 04:37:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:43][root][INFO] - Training Epoch: 2/2, step 6974/7134 completed (loss: 0.027975155040621758, acc: 0.9896551966667175)
[2025-02-13 04:37:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:43][root][INFO] - Training Epoch: 2/2, step 6975/7134 completed (loss: 0.031400348991155624, acc: 0.9934924244880676)
[2025-02-13 04:37:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:43][root][INFO] - Training Epoch: 2/2, step 6976/7134 completed (loss: 0.040795885026454926, acc: 0.9896013736724854)
[2025-02-13 04:37:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:44][root][INFO] - Training Epoch: 2/2, step 6977/7134 completed (loss: 0.025000467896461487, acc: 0.9924812316894531)
[2025-02-13 04:37:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:44][root][INFO] - Training Epoch: 2/2, step 6978/7134 completed (loss: 0.0171564482152462, acc: 0.9934959411621094)
[2025-02-13 04:37:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:45][root][INFO] - Training Epoch: 2/2, step 6979/7134 completed (loss: 0.02585720643401146, acc: 0.9939024448394775)
[2025-02-13 04:37:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:45][root][INFO] - Training Epoch: 2/2, step 6980/7134 completed (loss: 0.06873613595962524, acc: 0.9735848903656006)
[2025-02-13 04:37:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:46][root][INFO] - Training Epoch: 2/2, step 6981/7134 completed (loss: 0.05926104262471199, acc: 0.9872286319732666)
[2025-02-13 04:37:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:46][root][INFO] - Training Epoch: 2/2, step 6982/7134 completed (loss: 0.05538968741893768, acc: 0.986601710319519)
[2025-02-13 04:37:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:47][root][INFO] - Training Epoch: 2/2, step 6983/7134 completed (loss: 0.07468464970588684, acc: 0.9821958541870117)
[2025-02-13 04:37:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:47][root][INFO] - Training Epoch: 2/2, step 6984/7134 completed (loss: 0.023014521226286888, acc: 0.9946409463882446)
[2025-02-13 04:37:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:48][root][INFO] - Training Epoch: 2/2, step 6985/7134 completed (loss: 0.09491346031427383, acc: 0.9774358868598938)
[2025-02-13 04:37:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:48][root][INFO] - Training Epoch: 2/2, step 6986/7134 completed (loss: 0.021316884085536003, acc: 0.9910846948623657)
[2025-02-13 04:37:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:48][root][INFO] - Training Epoch: 2/2, step 6987/7134 completed (loss: 0.03510162979364395, acc: 0.9898989796638489)
[2025-02-13 04:37:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:49][root][INFO] - Training Epoch: 2/2, step 6988/7134 completed (loss: 0.012337313033640385, acc: 0.9980119466781616)
[2025-02-13 04:37:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:49][root][INFO] - Training Epoch: 2/2, step 6989/7134 completed (loss: 0.04538945108652115, acc: 0.9927272796630859)
[2025-02-13 04:37:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:50][root][INFO] - Training Epoch: 2/2, step 6990/7134 completed (loss: 0.007856114767491817, acc: 0.9955357313156128)
[2025-02-13 04:37:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:50][root][INFO] - Training Epoch: 2/2, step 6991/7134 completed (loss: 0.04017810896039009, acc: 0.9899749159812927)
[2025-02-13 04:37:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:50][root][INFO] - Training Epoch: 2/2, step 6992/7134 completed (loss: 0.03532939776778221, acc: 0.9915789365768433)
[2025-02-13 04:37:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:51][root][INFO] - Training Epoch: 2/2, step 6993/7134 completed (loss: 0.06646555662155151, acc: 0.9864253401756287)
[2025-02-13 04:37:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:51][root][INFO] - Training Epoch: 2/2, step 6994/7134 completed (loss: 0.07911980897188187, acc: 0.9823434948921204)
[2025-02-13 04:37:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:52][root][INFO] - Training Epoch: 2/2, step 6995/7134 completed (loss: 0.023444605991244316, acc: 0.9908376932144165)
[2025-02-13 04:37:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:52][root][INFO] - Training Epoch: 2/2, step 6996/7134 completed (loss: 0.01703423261642456, acc: 0.9951298832893372)
[2025-02-13 04:37:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:53][root][INFO] - Training Epoch: 2/2, step 6997/7134 completed (loss: 0.0237432811409235, acc: 0.9922118186950684)
[2025-02-13 04:37:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:53][root][INFO] - Training Epoch: 2/2, step 6998/7134 completed (loss: 0.05759358033537865, acc: 0.9828431606292725)
[2025-02-13 04:37:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:54][root][INFO] - Training Epoch: 2/2, step 6999/7134 completed (loss: 0.017244836315512657, acc: 0.9934640526771545)
[2025-02-13 04:37:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:54][root][INFO] - Training Epoch: 2/2, step 7000/7134 completed (loss: 0.010562269017100334, acc: 0.9957627058029175)
[2025-02-13 04:37:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:54][root][INFO] - Training Epoch: 2/2, step 7001/7134 completed (loss: 0.04425540193915367, acc: 0.9894578456878662)
[2025-02-13 04:37:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:55][root][INFO] - Training Epoch: 2/2, step 7002/7134 completed (loss: 0.004751375876367092, acc: 1.0)
[2025-02-13 04:37:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:55][root][INFO] - Training Epoch: 2/2, step 7003/7134 completed (loss: 0.005556157324463129, acc: 0.9989047050476074)
[2025-02-13 04:37:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:56][root][INFO] - Training Epoch: 2/2, step 7004/7134 completed (loss: 0.07021056115627289, acc: 0.9803149700164795)
[2025-02-13 04:37:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:56][root][INFO] - Training Epoch: 2/2, step 7005/7134 completed (loss: 0.028812939301133156, acc: 0.9878542423248291)
[2025-02-13 04:37:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:57][root][INFO] - Training Epoch: 2/2, step 7006/7134 completed (loss: 0.01234421692788601, acc: 0.99452805519104)
[2025-02-13 04:37:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:57][root][INFO] - Training Epoch: 2/2, step 7007/7134 completed (loss: 0.02320782281458378, acc: 0.992514967918396)
[2025-02-13 04:37:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:58][root][INFO] - Training Epoch: 2/2, step 7008/7134 completed (loss: 0.020507769659161568, acc: 0.992601752281189)
[2025-02-13 04:37:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:58][root][INFO] - Training Epoch: 2/2, step 7009/7134 completed (loss: 0.027379300445318222, acc: 0.9942129850387573)
[2025-02-13 04:37:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:58][root][INFO] - Training Epoch: 2/2, step 7010/7134 completed (loss: 0.011642555706202984, acc: 0.9953774809837341)
[2025-02-13 04:37:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:59][root][INFO] - Training Epoch: 2/2, step 7011/7134 completed (loss: 0.030298400670289993, acc: 0.989182710647583)
[2025-02-13 04:37:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:37:59][root][INFO] - Training Epoch: 2/2, step 7012/7134 completed (loss: 0.019471999257802963, acc: 0.991253674030304)
[2025-02-13 04:37:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:00][root][INFO] - Training Epoch: 2/2, step 7013/7134 completed (loss: 0.005718563217669725, acc: 0.9985590577125549)
[2025-02-13 04:38:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:00][root][INFO] - Training Epoch: 2/2, step 7014/7134 completed (loss: 0.018180450424551964, acc: 0.9935587644577026)
[2025-02-13 04:38:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:01][root][INFO] - Training Epoch: 2/2, step 7015/7134 completed (loss: 0.017954478040337563, acc: 0.9928443431854248)
[2025-02-13 04:38:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:01][root][INFO] - Training Epoch: 2/2, step 7016/7134 completed (loss: 0.02860126830637455, acc: 0.9927745461463928)
[2025-02-13 04:38:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:01][root][INFO] - Training Epoch: 2/2, step 7017/7134 completed (loss: 0.03622177243232727, acc: 0.9896265268325806)
[2025-02-13 04:38:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:02][root][INFO] - Training Epoch: 2/2, step 7018/7134 completed (loss: 0.032584283500909805, acc: 0.9906976819038391)
[2025-02-13 04:38:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:02][root][INFO] - Training Epoch: 2/2, step 7019/7134 completed (loss: 0.07811423391103745, acc: 0.9824561476707458)
[2025-02-13 04:38:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:03][root][INFO] - Training Epoch: 2/2, step 7020/7134 completed (loss: 0.019716233015060425, acc: 0.995768666267395)
[2025-02-13 04:38:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:03][root][INFO] - Training Epoch: 2/2, step 7021/7134 completed (loss: 0.021042143926024437, acc: 0.9948892593383789)
[2025-02-13 04:38:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:04][root][INFO] - Training Epoch: 2/2, step 7022/7134 completed (loss: 0.05305372178554535, acc: 0.9876543283462524)
[2025-02-13 04:38:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:04][root][INFO] - Training Epoch: 2/2, step 7023/7134 completed (loss: 0.05860043317079544, acc: 0.9880668520927429)
[2025-02-13 04:38:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:04][root][INFO] - Training Epoch: 2/2, step 7024/7134 completed (loss: 0.019274737685918808, acc: 0.993630588054657)
[2025-02-13 04:38:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:05][root][INFO] - Training Epoch: 2/2, step 7025/7134 completed (loss: 0.062450964003801346, acc: 0.9793103337287903)
[2025-02-13 04:38:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:05][root][INFO] - Training Epoch: 2/2, step 7026/7134 completed (loss: 0.055757075548172, acc: 0.9806950092315674)
[2025-02-13 04:38:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:06][root][INFO] - Training Epoch: 2/2, step 7027/7134 completed (loss: 0.03131423518061638, acc: 0.9878048896789551)
[2025-02-13 04:38:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:06][root][INFO] - Training Epoch: 2/2, step 7028/7134 completed (loss: 0.01036389172077179, acc: 0.9975786805152893)
[2025-02-13 04:38:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:06][root][INFO] - Training Epoch: 2/2, step 7029/7134 completed (loss: 0.014986542984843254, acc: 0.9953595995903015)
[2025-02-13 04:38:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:07][root][INFO] - Training Epoch: 2/2, step 7030/7134 completed (loss: 0.041892874985933304, acc: 0.9857819676399231)
[2025-02-13 04:38:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:07][root][INFO] - Training Epoch: 2/2, step 7031/7134 completed (loss: 0.04510711133480072, acc: 0.9873096346855164)
[2025-02-13 04:38:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:08][root][INFO] - Training Epoch: 2/2, step 7032/7134 completed (loss: 0.043364740908145905, acc: 0.989159882068634)
[2025-02-13 04:38:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:08][root][INFO] - Training Epoch: 2/2, step 7033/7134 completed (loss: 0.03950042277574539, acc: 0.9824047088623047)
[2025-02-13 04:38:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:08][root][INFO] - Training Epoch: 2/2, step 7034/7134 completed (loss: 0.06964908540248871, acc: 0.9787985682487488)
[2025-02-13 04:38:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:09][root][INFO] - Training Epoch: 2/2, step 7035/7134 completed (loss: 0.06544341146945953, acc: 0.9841269850730896)
[2025-02-13 04:38:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:09][root][INFO] - Training Epoch: 2/2, step 7036/7134 completed (loss: 0.013027258217334747, acc: 0.9958847761154175)
[2025-02-13 04:38:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:09][root][INFO] - Training Epoch: 2/2, step 7037/7134 completed (loss: 0.017621638253331184, acc: 0.99609375)
[2025-02-13 04:38:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:10][root][INFO] - Training Epoch: 2/2, step 7038/7134 completed (loss: 0.08236707746982574, acc: 0.9762532711029053)
[2025-02-13 04:38:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:10][root][INFO] - Training Epoch: 2/2, step 7039/7134 completed (loss: 0.02295624651014805, acc: 0.990338146686554)
[2025-02-13 04:38:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:10][root][INFO] - Training Epoch: 2/2, step 7040/7134 completed (loss: 0.06263864785432816, acc: 0.9798850417137146)
[2025-02-13 04:38:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:11][root][INFO] - Training Epoch: 2/2, step 7041/7134 completed (loss: 0.06042544171214104, acc: 0.9748427867889404)
[2025-02-13 04:38:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:11][root][INFO] - Training Epoch: 2/2, step 7042/7134 completed (loss: 0.028865259140729904, acc: 0.9907692074775696)
[2025-02-13 04:38:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:12][root][INFO] - Training Epoch: 2/2, step 7043/7134 completed (loss: 0.054025039076805115, acc: 0.9786666631698608)
[2025-02-13 04:38:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:12][root][INFO] - Training Epoch: 2/2, step 7044/7134 completed (loss: 0.06245104968547821, acc: 0.9755351543426514)
[2025-02-13 04:38:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:12][root][INFO] - Training Epoch: 2/2, step 7045/7134 completed (loss: 0.030466806143522263, acc: 0.9914966225624084)
[2025-02-13 04:38:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:13][root][INFO] - Training Epoch: 2/2, step 7046/7134 completed (loss: 0.05638324096798897, acc: 0.9795082211494446)
[2025-02-13 04:38:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:13][root][INFO] - Training Epoch: 2/2, step 7047/7134 completed (loss: 0.05254384130239487, acc: 0.9857142567634583)
[2025-02-13 04:38:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:14][root][INFO] - Training Epoch: 2/2, step 7048/7134 completed (loss: 0.00935550220310688, acc: 0.9976498484611511)
[2025-02-13 04:38:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:14][root][INFO] - Training Epoch: 2/2, step 7049/7134 completed (loss: 0.014806495048105717, acc: 0.9973261952400208)
[2025-02-13 04:38:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:15][root][INFO] - Training Epoch: 2/2, step 7050/7134 completed (loss: 0.026220716536045074, acc: 0.9935232996940613)
[2025-02-13 04:38:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:15][root][INFO] - Training Epoch: 2/2, step 7051/7134 completed (loss: 0.027848221361637115, acc: 0.993678867816925)
[2025-02-13 04:38:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:15][root][INFO] - Training Epoch: 2/2, step 7052/7134 completed (loss: 0.030318662524223328, acc: 0.9920106530189514)
[2025-02-13 04:38:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:16][root][INFO] - Training Epoch: 2/2, step 7053/7134 completed (loss: 0.00754955131560564, acc: 0.9975639581680298)
[2025-02-13 04:38:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:16][root][INFO] - Training Epoch: 2/2, step 7054/7134 completed (loss: 0.04785635322332382, acc: 0.9873417615890503)
[2025-02-13 04:38:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:17][root][INFO] - Training Epoch: 2/2, step 7055/7134 completed (loss: 0.03433356434106827, acc: 0.9910714030265808)
[2025-02-13 04:38:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:17][root][INFO] - Training Epoch: 2/2, step 7056/7134 completed (loss: 0.03534584864974022, acc: 0.9915966391563416)
[2025-02-13 04:38:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:18][root][INFO] - Training Epoch: 2/2, step 7057/7134 completed (loss: 0.08440478891134262, acc: 0.983561635017395)
[2025-02-13 04:38:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:18][root][INFO] - Training Epoch: 2/2, step 7058/7134 completed (loss: 0.04911332204937935, acc: 0.9822834730148315)
[2025-02-13 04:38:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:19][root][INFO] - Training Epoch: 2/2, step 7059/7134 completed (loss: 0.004672979470342398, acc: 0.998603343963623)
[2025-02-13 04:38:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:19][root][INFO] - Training Epoch: 2/2, step 7060/7134 completed (loss: 0.05846657231450081, acc: 0.9870967864990234)
[2025-02-13 04:38:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:19][root][INFO] - Training Epoch: 2/2, step 7061/7134 completed (loss: 0.02528526820242405, acc: 0.9934895634651184)
[2025-02-13 04:38:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:20][root][INFO] - Training Epoch: 2/2, step 7062/7134 completed (loss: 0.06291930377483368, acc: 0.9844444394111633)
[2025-02-13 04:38:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:20][root][INFO] - Training Epoch: 2/2, step 7063/7134 completed (loss: 0.06642664968967438, acc: 0.9895424842834473)
[2025-02-13 04:38:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:21][root][INFO] - Training Epoch: 2/2, step 7064/7134 completed (loss: 0.017961081117391586, acc: 0.9955489635467529)
[2025-02-13 04:38:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:21][root][INFO] - Training Epoch: 2/2, step 7065/7134 completed (loss: 0.012466381303966045, acc: 0.997682511806488)
[2025-02-13 04:38:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:22][root][INFO] - Training Epoch: 2/2, step 7066/7134 completed (loss: 0.03747916966676712, acc: 0.9904875159263611)
[2025-02-13 04:38:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:22][root][INFO] - Training Epoch: 2/2, step 7067/7134 completed (loss: 0.014461089856922626, acc: 0.9949044585227966)
[2025-02-13 04:38:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:23][root][INFO] - Training Epoch: 2/2, step 7068/7134 completed (loss: 0.027463866397738457, acc: 0.9915561079978943)
[2025-02-13 04:38:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:23][root][INFO] - Training Epoch: 2/2, step 7069/7134 completed (loss: 0.016301851719617844, acc: 0.9970414042472839)
[2025-02-13 04:38:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:24][root][INFO] - Training Epoch: 2/2, step 7070/7134 completed (loss: 0.01589593105018139, acc: 0.9972565174102783)
[2025-02-13 04:38:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:24][root][INFO] - Training Epoch: 2/2, step 7071/7134 completed (loss: 0.02420782297849655, acc: 0.9943609237670898)
[2025-02-13 04:38:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:24][root][INFO] - Training Epoch: 2/2, step 7072/7134 completed (loss: 0.013627436943352222, acc: 0.996363639831543)
[2025-02-13 04:38:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:25][root][INFO] - Training Epoch: 2/2, step 7073/7134 completed (loss: 0.029046205803751945, acc: 0.9920364022254944)
[2025-02-13 04:38:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:25][root][INFO] - Training Epoch: 2/2, step 7074/7134 completed (loss: 0.02556811273097992, acc: 0.9961758852005005)
[2025-02-13 04:38:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:26][root][INFO] - Training Epoch: 2/2, step 7075/7134 completed (loss: 0.019410349428653717, acc: 0.9955157041549683)
[2025-02-13 04:38:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:26][root][INFO] - Training Epoch: 2/2, step 7076/7134 completed (loss: 0.02791527844965458, acc: 0.9900285005569458)
[2025-02-13 04:38:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:27][root][INFO] - Training Epoch: 2/2, step 7077/7134 completed (loss: 0.03610915318131447, acc: 0.9901639223098755)
[2025-02-13 04:38:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:27][root][INFO] - Training Epoch: 2/2, step 7078/7134 completed (loss: 0.016554564237594604, acc: 0.995192289352417)
[2025-02-13 04:38:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:27][root][INFO] - Training Epoch: 2/2, step 7079/7134 completed (loss: 0.021489256992936134, acc: 0.9950739145278931)
[2025-02-13 04:38:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:28][root][INFO] - Training Epoch: 2/2, step 7080/7134 completed (loss: 0.013880092650651932, acc: 0.9983388781547546)
[2025-02-13 04:38:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:28][root][INFO] - Training Epoch: 2/2, step 7081/7134 completed (loss: 0.019850175827741623, acc: 0.9943898916244507)
[2025-02-13 04:38:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:29][root][INFO] - Training Epoch: 2/2, step 7082/7134 completed (loss: 0.03388475626707077, acc: 0.9903069734573364)
[2025-02-13 04:38:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:29][root][INFO] - Training Epoch: 2/2, step 7083/7134 completed (loss: 0.012817945331335068, acc: 0.9949832558631897)
[2025-02-13 04:38:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:29][root][INFO] - Training Epoch: 2/2, step 7084/7134 completed (loss: 0.021307796239852905, acc: 0.9898403286933899)
[2025-02-13 04:38:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:30][root][INFO] - Training Epoch: 2/2, step 7085/7134 completed (loss: 0.03280900791287422, acc: 0.9922839403152466)
[2025-02-13 04:38:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:30][root][INFO] - Training Epoch: 2/2, step 7086/7134 completed (loss: 0.013698730617761612, acc: 0.99452805519104)
[2025-02-13 04:38:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:31][root][INFO] - Training Epoch: 2/2, step 7087/7134 completed (loss: 0.05014049634337425, acc: 0.9907578825950623)
[2025-02-13 04:38:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:31][root][INFO] - Training Epoch: 2/2, step 7088/7134 completed (loss: 0.03031555376946926, acc: 0.9933775067329407)
[2025-02-13 04:38:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:32][root][INFO] - Training Epoch: 2/2, step 7089/7134 completed (loss: 0.08993091434240341, acc: 0.9808917045593262)
[2025-02-13 04:38:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:32][root][INFO] - Training Epoch: 2/2, step 7090/7134 completed (loss: 0.026144802570343018, acc: 0.9945799708366394)
[2025-02-13 04:38:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:33][root][INFO] - Training Epoch: 2/2, step 7091/7134 completed (loss: 0.0025636511854827404, acc: 1.0)
[2025-02-13 04:38:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:33][root][INFO] - Training Epoch: 2/2, step 7092/7134 completed (loss: 0.01935034617781639, acc: 0.994854211807251)
[2025-02-13 04:38:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:33][root][INFO] - Training Epoch: 2/2, step 7093/7134 completed (loss: 0.021888835355639458, acc: 0.9960317611694336)
[2025-02-13 04:38:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:34][root][INFO] - Training Epoch: 2/2, step 7094/7134 completed (loss: 0.005872176494449377, acc: 0.9953917264938354)
[2025-02-13 04:38:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:34][root][INFO] - Training Epoch: 2/2, step 7095/7134 completed (loss: 0.021126076579093933, acc: 0.9958677887916565)
[2025-02-13 04:38:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:35][root][INFO] - Training Epoch: 2/2, step 7096/7134 completed (loss: 0.04947063699364662, acc: 0.9848739504814148)
[2025-02-13 04:38:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:35][root][INFO] - Training Epoch: 2/2, step 7097/7134 completed (loss: 0.025543050840497017, acc: 0.9942362904548645)
[2025-02-13 04:38:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:35][root][INFO] - Training Epoch: 2/2, step 7098/7134 completed (loss: 0.02156399004161358, acc: 0.9887640476226807)
[2025-02-13 04:38:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:36][root][INFO] - Training Epoch: 2/2, step 7099/7134 completed (loss: 0.027789775282144547, acc: 0.9914529919624329)
[2025-02-13 04:38:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:36][root][INFO] - Training Epoch: 2/2, step 7100/7134 completed (loss: 0.03845415636897087, acc: 0.9829457402229309)
[2025-02-13 04:38:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:37][root][INFO] - Training Epoch: 2/2, step 7101/7134 completed (loss: 0.038718331605196, acc: 0.980322003364563)
[2025-02-13 04:38:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:37][root][INFO] - Training Epoch: 2/2, step 7102/7134 completed (loss: 0.0112070357427001, acc: 0.9922480583190918)
[2025-02-13 04:38:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:37][root][INFO] - Training Epoch: 2/2, step 7103/7134 completed (loss: 0.022258099168539047, acc: 0.9919354915618896)
[2025-02-13 04:38:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:38][root][INFO] - Training Epoch: 2/2, step 7104/7134 completed (loss: 0.01707001030445099, acc: 0.9979423880577087)
[2025-02-13 04:38:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:38][root][INFO] - Training Epoch: 2/2, step 7105/7134 completed (loss: 0.03537421673536301, acc: 0.9900000095367432)
[2025-02-13 04:38:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:39][root][INFO] - Training Epoch: 2/2, step 7106/7134 completed (loss: 0.0040649170987308025, acc: 1.0)
[2025-02-13 04:38:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:39][root][INFO] - Training Epoch: 2/2, step 7107/7134 completed (loss: 0.01886705681681633, acc: 0.9906832575798035)
[2025-02-13 04:38:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:39][root][INFO] - Training Epoch: 2/2, step 7108/7134 completed (loss: 0.024290656670928, acc: 0.9916666746139526)
[2025-02-13 04:38:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:40][root][INFO] - Training Epoch: 2/2, step 7109/7134 completed (loss: 0.03192012384533882, acc: 0.9887429475784302)
[2025-02-13 04:38:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:40][root][INFO] - Training Epoch: 2/2, step 7110/7134 completed (loss: 0.009001847356557846, acc: 0.996610164642334)
[2025-02-13 04:38:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:41][root][INFO] - Training Epoch: 2/2, step 7111/7134 completed (loss: 0.026674719527363777, acc: 0.9911308288574219)
[2025-02-13 04:38:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:41][root][INFO] - Training Epoch: 2/2, step 7112/7134 completed (loss: 0.016939273104071617, acc: 0.991769552230835)
[2025-02-13 04:38:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:42][root][INFO] - Training Epoch: 2/2, step 7113/7134 completed (loss: 0.04353100433945656, acc: 0.9858044385910034)
[2025-02-13 04:38:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:42][root][INFO] - Training Epoch: 2/2, step 7114/7134 completed (loss: 0.02310684137046337, acc: 0.9942638874053955)
[2025-02-13 04:38:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:42][root][INFO] - Training Epoch: 2/2, step 7115/7134 completed (loss: 0.027369525283575058, acc: 0.9894179701805115)
[2025-02-13 04:38:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:43][root][INFO] - Training Epoch: 2/2, step 7116/7134 completed (loss: 0.012271314859390259, acc: 0.9982425570487976)
[2025-02-13 04:38:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:43][root][INFO] - Training Epoch: 2/2, step 7117/7134 completed (loss: 0.014832954853773117, acc: 0.996666669845581)
[2025-02-13 04:38:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:44][root][INFO] - Training Epoch: 2/2, step 7118/7134 completed (loss: 0.03836359456181526, acc: 0.9912790656089783)
[2025-02-13 04:38:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:44][root][INFO] - Training Epoch: 2/2, step 7119/7134 completed (loss: 0.012994363903999329, acc: 0.9950900077819824)
[2025-02-13 04:38:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:44][root][INFO] - Training Epoch: 2/2, step 7120/7134 completed (loss: 0.03346525505185127, acc: 0.9883177280426025)
[2025-02-13 04:38:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:45][root][INFO] - Training Epoch: 2/2, step 7121/7134 completed (loss: 0.008308440446853638, acc: 0.9964788556098938)
[2025-02-13 04:38:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:45][root][INFO] - Training Epoch: 2/2, step 7122/7134 completed (loss: 0.0217168927192688, acc: 0.9866310358047485)
[2025-02-13 04:38:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:46][root][INFO] - Training Epoch: 2/2, step 7123/7134 completed (loss: 0.02176131308078766, acc: 0.992682933807373)
[2025-02-13 04:38:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:46][root][INFO] - Training Epoch: 2/2, step 7124/7134 completed (loss: 0.017384950071573257, acc: 0.9924952983856201)
[2025-02-13 04:38:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:46][root][INFO] - Training Epoch: 2/2, step 7125/7134 completed (loss: 0.02000381052494049, acc: 0.9933664798736572)
[2025-02-13 04:38:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:47][root][INFO] - Training Epoch: 2/2, step 7126/7134 completed (loss: 0.045862238854169846, acc: 0.9831365942955017)
[2025-02-13 04:38:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:47][root][INFO] - Training Epoch: 2/2, step 7127/7134 completed (loss: 0.015801703557372093, acc: 0.993630588054657)
[2025-02-13 04:38:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:48][root][INFO] - Training Epoch: 2/2, step 7128/7134 completed (loss: 0.02937459573149681, acc: 0.9869918823242188)
[2025-02-13 04:38:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:48][root][INFO] - Training Epoch: 2/2, step 7129/7134 completed (loss: 0.044149789959192276, acc: 0.9843400716781616)
[2025-02-13 04:38:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:38:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:39:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:40:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:41:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:50][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.0459, device='cuda:0') eval_epoch_loss=tensor(0.0449, device='cuda:0') eval_epoch_acc=tensor(0.9884, device='cuda:0')
[2025-02-13 04:42:50][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-13 04:42:50][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-13 04:42:50][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme/asr_epoch_2_step_7130_loss_0.044864483177661896/model.pt
[2025-02-13 04:42:50][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme directory
[2025-02-13 04:42:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:51][root][INFO] - Training Epoch: 2/2, step 7130/7134 completed (loss: 0.07230217754840851, acc: 0.9767857193946838)
[2025-02-13 04:42:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:51][root][INFO] - Training Epoch: 2/2, step 7131/7134 completed (loss: 0.01116393692791462, acc: 0.9953595995903015)
[2025-02-13 04:42:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:52][root][INFO] - Training Epoch: 2/2, step 7132/7134 completed (loss: 0.01717902161180973, acc: 0.9932659864425659)
[2025-02-13 04:42:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:42:52][root][INFO] - Training Epoch: 2/2, step 7133/7134 completed (loss: 0.005255572497844696, acc: 0.9981516003608704)
[2025-02-13 04:42:52][slam_llm.utils.train_utils][INFO] - Epoch 2: train_perplexity=1.0313, train_epoch_loss=0.0308, epoch time 4116.836296599358s
[2025-02-13 04:42:52][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 14 GB
[2025-02-13 04:42:52][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 30 GB
[2025-02-13 04:42:52][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 14 GB
[2025-02-13 04:42:52][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 12
[2025-02-13 04:42:52][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 8 GB
[2025-02-13 04:42:52][root][INFO] - Key: avg_train_prep, Value: 1.0532505512237549
[2025-02-13 04:42:52][root][INFO] - Key: avg_train_loss, Value: 0.05166344717144966
[2025-02-13 04:42:52][root][INFO] - Key: avg_train_acc, Value: 0.986373782157898
[2025-02-13 04:42:52][root][INFO] - Key: avg_eval_prep, Value: 1.0491071939468384
[2025-02-13 04:42:52][root][INFO] - Key: avg_eval_loss, Value: 0.047915924340486526
[2025-02-13 04:42:52][root][INFO] - Key: avg_eval_acc, Value: 0.9870713353157043
[2025-02-13 04:42:52][root][INFO] - Key: avg_epoch_time, Value: 4120.985209038481
[2025-02-13 04:42:52][root][INFO] - Key: avg_checkpoint_time, Value: 0.306930263992399
Selected lowest loss checkpoint: asr_epoch_2_step_3564_loss_0.04128989577293396
Checkpoint file: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme/asr_epoch_2_step_3564_loss_0.04128989577293396/model.pt
ckpt_folder /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme/asr_epoch_2_step_3564_loss_0.04128989577293396
[2025-02-13 04:43:31][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'resume_step': 0, 'resume_epoch': 0, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True, 'freeze_encoder2': False, 'save_embedding': False}
[2025-02-13 04:43:31][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': <ShardingStrategy.NO_SHARD: 3>, 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2025-02-13 04:43:31][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'llm_inference_config': 'repetition_penalty', 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'qformer_layers': 8, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': '', 'identifier': 'aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme'}
[2025-02-13 04:43:34][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2025-02-13 04:43:39][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-02-13 04:43:39][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2025-02-13 04:43:39][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-02-13 04:43:39][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2025-02-13 04:43:45][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-02-13 04:43:45][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2025-02-13 04:43:45][slam_llm.models.slam_model][INFO] - setup peft...
trainable params: 5,636,096 || all params: 1,241,450,496 || trainable%: 0.4539928106807088
[2025-02-13 04:43:45][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-02-13 04:43:45][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2025-02-13 04:43:46][slam_llm.utils.train_utils][INFO] - --> Module linear
[2025-02-13 04:43:46][slam_llm.utils.train_utils][INFO] - --> linear has 14.68416 Million params

[2025-02-13 04:43:46][slam_model_asr.py][INFO] - loading other parts from: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme/asr_epoch_2_step_3564_loss_0.04128989577293396/model.pt
[2025-02-13 04:43:46][slam_llm.utils.train_utils][INFO] - --> Model asr
[2025-02-13 04:43:46][slam_llm.utils.train_utils][INFO] - --> asr has 20.320256 Million params

[2025-02-13 04:43:47][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': None, 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/librispeech-100_phoneme/test.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2025-02-13 04:43:49][root][INFO] - --> Training Set Length = 2620
[2025-02-13 04:43:49][root][INFO] - =====================================
Loaded LLM Config Path: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/examples/asr_librispeech/scripts/llm_config/repetition_penalty.json
Loaded LLM Config: {'max_new_tokens': 200, 'num_beams': 4, 'do_sample': False, 'min_length': 1, 'top_p': 1.0, 'repetition_penalty': 2.0, 'length_penalty': 1.0, 'temperature': 1.0, 'no_repeat_ngram_size': 1}
[2025-02-13 04:43:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:43:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:44:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:45:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:45:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:45:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:45:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:45:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:45:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:45:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:45:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:45:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:45:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:45:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:45:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:45:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:46:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:46:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:46:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:46:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:46:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:46:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:46:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:46:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:46:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:46:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:46:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:47:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:47:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:47:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:47:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:47:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:47:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:47:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:47:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:47:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:47:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:47:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:48:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:48:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:48:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:48:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:48:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:48:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:48:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:48:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:48:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:48:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:48:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:48:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:48:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:48:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:48:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:49:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:49:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:49:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:49:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:49:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:49:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:49:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:49:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:49:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:49:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:50:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:50:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:50:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:50:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:50:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:50:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:50:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:50:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:50:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:50:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:50:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:51:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:51:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:51:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:51:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:51:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:51:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:51:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:51:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:51:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:51:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:51:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:51:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:51:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:52:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:52:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:52:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:52:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:52:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:52:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:52:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:52:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:52:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:52:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:52:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:53:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:53:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:53:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:53:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:53:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:53:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:53:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:53:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:53:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:53:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:53:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:53:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:53:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:53:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:54:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:54:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:54:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:54:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:54:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:54:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:54:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:54:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:54:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:54:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:55:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:55:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:55:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:55:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:55:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:55:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:55:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:55:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:55:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:55:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:55:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:55:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:56:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:56:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:56:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:56:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:56:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:56:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:56:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:56:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:56:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:56:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:56:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:56:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:57:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:57:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:57:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:57:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:57:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:57:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:57:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:57:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:57:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:57:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:57:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:58:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:58:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:58:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:58:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:58:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:58:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:58:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:58:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:58:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:58:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:58:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:59:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:59:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:59:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:59:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:59:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:59:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:59:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:59:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:59:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:59:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:59:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:59:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 04:59:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:00:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:00:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:00:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:00:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:00:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:00:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:00:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:00:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:00:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:00:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:00:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:00:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:00:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:01:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:01:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:01:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:01:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:01:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:01:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:01:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:01:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:01:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:01:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:01:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:02:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:02:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:02:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:02:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:02:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:02:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:02:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:02:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:02:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:02:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:02:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:02:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:02:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:03:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:03:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:03:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:03:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:03:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:03:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:03:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:03:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:03:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:03:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:03:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:03:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:03:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:03:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:03:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:03:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:03:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:03:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:04:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:04:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:04:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:04:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:04:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:04:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:04:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:04:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:04:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:04:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:04:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:05:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:05:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:05:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:05:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:05:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:05:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:05:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:05:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:05:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:05:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:05:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:05:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:05:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:05:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:05:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:05:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:06:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:06:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:06:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:06:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:06:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:06:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:06:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:06:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:06:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:06:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:06:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:06:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:06:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:07:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:07:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:07:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:07:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:07:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:07:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:07:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:07:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:07:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:07:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:07:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:07:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:07:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:07:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:07:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:07:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:08:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:08:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:08:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:08:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:08:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:08:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:08:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:08:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:08:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:08:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:08:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:08:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:08:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:09:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:09:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:09:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:09:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:09:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:09:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:09:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:09:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:09:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:10:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:10:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:10:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:10:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:10:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:10:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:10:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:10:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:10:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:10:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:11:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:11:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:11:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:11:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:11:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:11:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:11:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:11:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:11:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:11:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:11:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:11:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:11:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:12:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:12:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:12:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:12:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:12:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:12:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:12:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:12:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:12:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:12:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:12:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:12:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:13:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:13:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:13:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:13:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:13:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:13:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:13:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:13:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:13:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:14:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:14:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:14:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:14:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:14:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:14:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:14:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:14:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:14:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:14:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:14:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:14:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:15:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:15:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:15:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:15:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:15:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:15:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:15:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:15:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:15:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:15:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:15:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:15:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:15:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:15:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:15:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:15:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:16:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:16:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:16:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:16:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:16:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:16:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:16:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:16:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:16:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:16:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:17:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:17:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:17:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:17:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:17:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:17:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:17:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:17:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:17:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:17:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:17:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:17:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:18:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:18:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:18:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:18:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:18:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:18:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:18:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:18:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:18:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:18:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:19:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:19:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:19:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:19:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:19:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:19:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:19:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:19:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:19:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:19:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:19:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:20:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:20:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:20:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:20:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:20:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:20:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:20:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:20:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:20:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:20:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:20:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:21:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:21:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:21:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:21:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:21:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:21:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:21:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:21:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:21:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:22:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:22:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:22:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:22:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:22:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:22:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:22:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:22:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:22:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:22:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:22:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:22:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:22:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:22:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:22:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:22:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:23:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:23:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:23:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:23:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:23:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:23:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:23:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:23:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:23:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:23:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:23:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:23:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:23:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:24:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:24:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:24:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:24:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:24:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:24:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:24:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:24:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:24:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:24:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:24:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:24:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:24:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:24:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:24:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:24:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:24:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:24:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:25:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:25:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:25:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:25:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:25:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:25:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:25:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:25:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:25:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:25:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:25:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:25:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:25:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:25:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:26:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:26:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:26:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:26:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:26:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:26:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:26:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:26:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:26:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:26:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:26:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:26:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:27:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:27:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:27:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:27:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:27:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:27:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:27:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:27:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:27:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:27:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:27:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:27:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:28:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:28:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:28:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:28:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:28:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:28:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:28:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:28:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:28:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:28:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:28:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:28:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:28:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:28:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:28:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:28:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:28:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:28:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:28:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:29:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:29:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:29:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:29:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:29:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:29:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:29:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:29:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:29:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:29:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:29:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:29:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:29:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:30:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:30:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:30:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:30:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:30:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:30:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:30:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:30:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:30:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:30:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:30:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:30:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:30:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:31:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:31:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:31:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:31:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:31:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:31:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:31:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:31:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:31:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:31:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:31:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:31:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:31:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:31:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:31:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:32:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:32:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:32:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:32:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:32:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:32:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:32:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:32:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:32:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:32:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:33:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:33:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:33:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:33:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:33:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:33:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:33:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:33:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:33:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:33:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:33:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:33:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:34:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:34:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:34:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:34:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:34:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:34:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:34:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:34:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:34:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:34:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:34:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:34:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:34:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:34:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:34:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:35:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:35:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:35:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:35:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:35:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:35:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:35:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:35:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:35:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:35:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:35:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:36:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 05:36:11][root][INFO] - Predictions written to: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme/decode_test_beam4_pred_20250213_044349
[2025-02-13 05:36:11][root][INFO] - Ground truth written to: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme/decode_test_beam4_gt_20250213_044349
Using folder: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme
Using GT file: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme/decode_test_beam4_gt_20250213_044349
Using PRED file: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/aphasia_phoneme_wavlm_llama32_1b_linear_peft_transfer_librispeech-100_phoneme/decode_test_beam4_pred_20250213_044349
Combined WER: 0.09333469308886361

Filtering repeated words...

Found 0 repeated lines in total.
Filtered Combined WER: 0.09333469308886361
