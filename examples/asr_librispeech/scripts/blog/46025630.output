/work/van-speech-nlp/jindaznb/slamenv/bin/python
task_flag: all
encoder_config: wavlm-mono
num_epochs: 10
batch_size_training: 4
train_data_folder: ami_phoneme
test_data_folder: ami_phoneme
use_peft: true
seed: 
llm_name: llama32_1b
debug: 
test_small: 
freeze_encoder: true
eval_ckpt: best
encoder_projector: linear
encoder_projector_ds_rate: 5
save_embedding: false
projector_transfer_learning: true
transfer_data_folder: psst_phoneme
Final identifier: ami_phoneme_wavlm_llama32_1b_linear_peft
ckpt_folder: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_2_step_26970_loss_0.22486534714698792



----- Transfer Learning Information -----
Resume Epoch: 1
Resume Step: 0
Train Data Path: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/train.jsonl
Validation Data Path: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/validation.jsonl
Test Data Path: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/test.jsonl
Identifier: ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme
Output Directory: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme
----------------------------------------
Resume epoch: 1
Resume step: 0
[2025-01-06 00:59:04][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 10, 'resume_step': 0, 'resume_epoch': 1, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 3000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': True, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': False, 'freeze_encoder': True, 'freeze_encoder2': True, 'save_embedding': False}
[2025-01-06 00:59:04][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': True, 'sharding_strategy': <ShardingStrategy.NO_SHARD: 3>, 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2025-01-06 00:59:04][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'llm_inference_config': 'test_config.json', 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'qformer_layers': 8, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': '', 'identifier': 'ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme'}
[2025-01-06 00:59:04][root][INFO] - log_config: {'use_wandb': True, 'wandb_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/wandb_log', 'wandb_entity_name': 'jindaz-work', 'wandb_project_name': 'SLAM-LLM', 'wandb_exp_name': 'ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme', 'log_file': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/2025-01-06_00-59-03.txt', 'log_interval': 5}
[2025-01-06 00:59:32][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2025-01-06 00:59:37][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-01-06 00:59:37][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2025-01-06 00:59:37][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-01-06 00:59:37][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2025-01-06 00:59:47][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-01-06 00:59:47][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2025-01-06 00:59:47][slam_llm.models.slam_model][INFO] - setup peft...
trainable params: 5,636,096 || all params: 1,241,450,496 || trainable%: 0.4539928106807088
[2025-01-06 00:59:47][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-01-06 00:59:47][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2025-01-06 00:59:47][slam_llm.utils.train_utils][INFO] - --> Module linear
[2025-01-06 00:59:47][slam_llm.utils.train_utils][INFO] - --> linear has 14.68416 Million params

[2025-01-06 00:59:47][slam_model_asr.py][INFO] - loading other parts from: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft/asr_epoch_2_step_26970_loss_0.22486534714698792/model.pt
[2025-01-06 00:59:48][slam_llm.utils.train_utils][INFO] - --> Model asr
[2025-01-06 00:59:48][slam_llm.utils.train_utils][INFO] - --> asr has 20.320256 Million params

[2025-01-06 00:59:49][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/train.jsonl', 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/validation.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': False, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2025-01-06 00:59:53][root][INFO] - --> Training Set Length = 2298
[2025-01-06 00:59:53][root][INFO] - --> Validation Set Length = 341
[2025-01-06 00:59:53][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2025-01-06 00:59:53][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2025-01-06 00:59:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 00:59:56][root][INFO] - Training Epoch: 1/10, step 0/574 completed (loss: 2.0960772037506104, acc: 0.48148149251937866)
[2025-01-06 00:59:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 00:59:57][root][INFO] - Training Epoch: 1/10, step 1/574 completed (loss: 0.9558073282241821, acc: 0.800000011920929)
[2025-01-06 00:59:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 00:59:57][root][INFO] - Training Epoch: 1/10, step 2/574 completed (loss: 2.3097050189971924, acc: 0.5135135054588318)
[2025-01-06 00:59:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 00:59:58][root][INFO] - Training Epoch: 1/10, step 3/574 completed (loss: 2.1763036251068115, acc: 0.5789473652839661)
[2025-01-06 00:59:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 00:59:58][root][INFO] - Training Epoch: 1/10, step 4/574 completed (loss: 1.6998047828674316, acc: 0.5945945978164673)
[2025-01-06 00:59:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 00:59:59][root][INFO] - Training Epoch: 1/10, step 5/574 completed (loss: 0.8240347504615784, acc: 0.8214285969734192)
[2025-01-06 00:59:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 00:59:59][root][INFO] - Training Epoch: 1/10, step 6/574 completed (loss: 2.6075844764709473, acc: 0.4693877696990967)
[2025-01-06 00:59:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 00:59:59][root][INFO] - Training Epoch: 1/10, step 7/574 completed (loss: 1.3130346536636353, acc: 0.7666666507720947)
[2025-01-06 00:59:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:00][root][INFO] - Training Epoch: 1/10, step 8/574 completed (loss: 1.1516512632369995, acc: 0.7272727489471436)
[2025-01-06 01:00:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:00][root][INFO] - Training Epoch: 1/10, step 9/574 completed (loss: 0.9723425507545471, acc: 0.692307710647583)
[2025-01-06 01:00:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:00][root][INFO] - Training Epoch: 1/10, step 10/574 completed (loss: 1.304939866065979, acc: 0.5555555820465088)
[2025-01-06 01:00:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:01][root][INFO] - Training Epoch: 1/10, step 11/574 completed (loss: 2.1893601417541504, acc: 0.5897436141967773)
[2025-01-06 01:00:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:01][root][INFO] - Training Epoch: 1/10, step 12/574 completed (loss: 1.3780752420425415, acc: 0.7272727489471436)
[2025-01-06 01:00:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:02][root][INFO] - Training Epoch: 1/10, step 13/574 completed (loss: 1.3336013555526733, acc: 0.5652173757553101)
[2025-01-06 01:00:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:02][root][INFO] - Training Epoch: 1/10, step 14/574 completed (loss: 1.912348747253418, acc: 0.5882353186607361)
[2025-01-06 01:00:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:02][root][INFO] - Training Epoch: 1/10, step 15/574 completed (loss: 0.8959335088729858, acc: 0.7142857313156128)
[2025-01-06 01:00:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:03][root][INFO] - Training Epoch: 1/10, step 16/574 completed (loss: 1.910796880722046, acc: 0.6315789222717285)
[2025-01-06 01:00:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:03][root][INFO] - Training Epoch: 1/10, step 17/574 completed (loss: 2.4606688022613525, acc: 0.5)
[2025-01-06 01:00:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:03][root][INFO] - Training Epoch: 1/10, step 18/574 completed (loss: 1.4996267557144165, acc: 0.5833333134651184)
[2025-01-06 01:00:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:04][root][INFO] - Training Epoch: 1/10, step 19/574 completed (loss: 1.468733310699463, acc: 0.7368420958518982)
[2025-01-06 01:00:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:04][root][INFO] - Training Epoch: 1/10, step 20/574 completed (loss: 1.2159637212753296, acc: 0.7307692170143127)
[2025-01-06 01:00:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:05][root][INFO] - Training Epoch: 1/10, step 21/574 completed (loss: 1.7433260679244995, acc: 0.6896551847457886)
[2025-01-06 01:00:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:05][root][INFO] - Training Epoch: 1/10, step 22/574 completed (loss: 2.2058966159820557, acc: 0.47999998927116394)
[2025-01-06 01:00:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:05][root][INFO] - Training Epoch: 1/10, step 23/574 completed (loss: 1.8753631114959717, acc: 0.7142857313156128)
[2025-01-06 01:00:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:06][root][INFO] - Training Epoch: 1/10, step 24/574 completed (loss: 0.8815869688987732, acc: 0.75)
[2025-01-06 01:00:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:06][root][INFO] - Training Epoch: 1/10, step 25/574 completed (loss: 1.5402711629867554, acc: 0.6226415038108826)
[2025-01-06 01:00:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:07][root][INFO] - Training Epoch: 1/10, step 26/574 completed (loss: 1.955437421798706, acc: 0.5205479264259338)
[2025-01-06 01:00:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:08][root][INFO] - Training Epoch: 1/10, step 27/574 completed (loss: 1.7114523649215698, acc: 0.5889328122138977)
[2025-01-06 01:00:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:08][root][INFO] - Training Epoch: 1/10, step 28/574 completed (loss: 1.3423949480056763, acc: 0.7209302186965942)
[2025-01-06 01:00:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:09][root][INFO] - Training Epoch: 1/10, step 29/574 completed (loss: 1.500946283340454, acc: 0.6385542154312134)
[2025-01-06 01:00:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:09][root][INFO] - Training Epoch: 1/10, step 30/574 completed (loss: 1.382470965385437, acc: 0.7283950448036194)
[2025-01-06 01:00:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:10][root][INFO] - Training Epoch: 1/10, step 31/574 completed (loss: 2.0714735984802246, acc: 0.4642857015132904)
[2025-01-06 01:00:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:10][root][INFO] - Training Epoch: 1/10, step 32/574 completed (loss: 0.9791938662528992, acc: 0.7037037014961243)
[2025-01-06 01:00:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:10][root][INFO] - Training Epoch: 1/10, step 33/574 completed (loss: 1.1704047918319702, acc: 0.8260869383811951)
[2025-01-06 01:00:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:11][root][INFO] - Training Epoch: 1/10, step 34/574 completed (loss: 1.2558906078338623, acc: 0.7310924530029297)
[2025-01-06 01:00:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:11][root][INFO] - Training Epoch: 1/10, step 35/574 completed (loss: 1.3465101718902588, acc: 0.7704917788505554)
[2025-01-06 01:00:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:12][root][INFO] - Training Epoch: 1/10, step 36/574 completed (loss: 1.5427273511886597, acc: 0.6984127163887024)
[2025-01-06 01:00:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:12][root][INFO] - Training Epoch: 1/10, step 37/574 completed (loss: 1.3849573135375977, acc: 0.6610169410705566)
[2025-01-06 01:00:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:12][root][INFO] - Training Epoch: 1/10, step 38/574 completed (loss: 1.877794623374939, acc: 0.6896551847457886)
[2025-01-06 01:00:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:13][root][INFO] - Training Epoch: 1/10, step 39/574 completed (loss: 3.1021664142608643, acc: 0.3333333432674408)
[2025-01-06 01:00:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:13][root][INFO] - Training Epoch: 1/10, step 40/574 completed (loss: 0.8725772500038147, acc: 0.6538461446762085)
[2025-01-06 01:00:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:14][root][INFO] - Training Epoch: 1/10, step 41/574 completed (loss: 0.8198295831680298, acc: 0.7567567825317383)
[2025-01-06 01:00:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:14][root][INFO] - Training Epoch: 1/10, step 42/574 completed (loss: 2.6914615631103516, acc: 0.5384615659713745)
[2025-01-06 01:00:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:15][root][INFO] - Training Epoch: 1/10, step 43/574 completed (loss: 1.6860326528549194, acc: 0.6969696879386902)
[2025-01-06 01:00:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:15][root][INFO] - Training Epoch: 1/10, step 44/574 completed (loss: 2.789707660675049, acc: 0.6701030731201172)
[2025-01-06 01:00:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:15][root][INFO] - Training Epoch: 1/10, step 45/574 completed (loss: 3.0055675506591797, acc: 0.6617646813392639)
[2025-01-06 01:00:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:16][root][INFO] - Training Epoch: 1/10, step 46/574 completed (loss: 1.5173671245574951, acc: 0.5)
[2025-01-06 01:00:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:16][root][INFO] - Training Epoch: 1/10, step 47/574 completed (loss: 1.3630419969558716, acc: 0.6666666865348816)
[2025-01-06 01:00:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:17][root][INFO] - Training Epoch: 1/10, step 48/574 completed (loss: 1.3895180225372314, acc: 0.6785714030265808)
[2025-01-06 01:00:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:17][root][INFO] - Training Epoch: 1/10, step 49/574 completed (loss: 1.0181450843811035, acc: 0.8055555820465088)
[2025-01-06 01:00:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:17][root][INFO] - Training Epoch: 1/10, step 50/574 completed (loss: 3.2852566242218018, acc: 0.5438596606254578)
[2025-01-06 01:00:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:18][root][INFO] - Training Epoch: 1/10, step 51/574 completed (loss: 2.5162854194641113, acc: 0.5555555820465088)
[2025-01-06 01:00:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:18][root][INFO] - Training Epoch: 1/10, step 52/574 completed (loss: 4.431297779083252, acc: 0.49295774102211)
[2025-01-06 01:00:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:18][root][INFO] - Training Epoch: 1/10, step 53/574 completed (loss: 3.6485249996185303, acc: 0.4000000059604645)
[2025-01-06 01:00:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:19][root][INFO] - Training Epoch: 1/10, step 54/574 completed (loss: 3.78129243850708, acc: 0.4054054021835327)
[2025-01-06 01:00:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:19][root][INFO] - Training Epoch: 1/10, step 55/574 completed (loss: 0.9954131245613098, acc: 0.692307710647583)
[2025-01-06 01:00:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:22][root][INFO] - Training Epoch: 1/10, step 56/574 completed (loss: 2.2599079608917236, acc: 0.4846416413784027)
[2025-01-06 01:00:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:24][root][INFO] - Training Epoch: 1/10, step 57/574 completed (loss: 2.0813755989074707, acc: 0.5838779807090759)
[2025-01-06 01:00:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:24][root][INFO] - Training Epoch: 1/10, step 58/574 completed (loss: 2.736443042755127, acc: 0.5852272510528564)
[2025-01-06 01:00:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:25][root][INFO] - Training Epoch: 1/10, step 59/574 completed (loss: 1.3058348894119263, acc: 0.7426470518112183)
[2025-01-06 01:00:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:26][root][INFO] - Training Epoch: 1/10, step 60/574 completed (loss: 2.3382418155670166, acc: 0.6014492511749268)
[2025-01-06 01:00:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:26][root][INFO] - Training Epoch: 1/10, step 61/574 completed (loss: 2.1934895515441895, acc: 0.6000000238418579)
[2025-01-06 01:00:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:26][root][INFO] - Training Epoch: 1/10, step 62/574 completed (loss: 1.6479579210281372, acc: 0.6176470518112183)
[2025-01-06 01:00:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:27][root][INFO] - Training Epoch: 1/10, step 63/574 completed (loss: 3.2112069129943848, acc: 0.5277777910232544)
[2025-01-06 01:00:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:27][root][INFO] - Training Epoch: 1/10, step 64/574 completed (loss: 1.2382947206497192, acc: 0.796875)
[2025-01-06 01:00:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:28][root][INFO] - Training Epoch: 1/10, step 65/574 completed (loss: 0.6247630715370178, acc: 0.931034505367279)
[2025-01-06 01:00:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:28][root][INFO] - Training Epoch: 1/10, step 66/574 completed (loss: 3.242856502532959, acc: 0.5178571343421936)
[2025-01-06 01:00:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:28][root][INFO] - Training Epoch: 1/10, step 67/574 completed (loss: 1.932536244392395, acc: 0.699999988079071)
[2025-01-06 01:00:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:29][root][INFO] - Training Epoch: 1/10, step 68/574 completed (loss: 0.9694967865943909, acc: 0.7200000286102295)
[2025-01-06 01:00:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:29][root][INFO] - Training Epoch: 1/10, step 69/574 completed (loss: 2.0032596588134766, acc: 0.3888888955116272)
[2025-01-06 01:00:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:29][root][INFO] - Training Epoch: 1/10, step 70/574 completed (loss: 3.6090400218963623, acc: 0.39393940567970276)
[2025-01-06 01:00:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:30][root][INFO] - Training Epoch: 1/10, step 71/574 completed (loss: 2.2645766735076904, acc: 0.5073529481887817)
[2025-01-06 01:00:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:30][root][INFO] - Training Epoch: 1/10, step 72/574 completed (loss: 1.4376345872879028, acc: 0.6349206566810608)
[2025-01-06 01:00:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:31][root][INFO] - Training Epoch: 1/10, step 73/574 completed (loss: 1.9589128494262695, acc: 0.5384615659713745)
[2025-01-06 01:00:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:31][root][INFO] - Training Epoch: 1/10, step 74/574 completed (loss: 3.4755287170410156, acc: 0.4693877696990967)
[2025-01-06 01:00:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:31][root][INFO] - Training Epoch: 1/10, step 75/574 completed (loss: 2.1028525829315186, acc: 0.49253731966018677)
[2025-01-06 01:00:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:32][root][INFO] - Training Epoch: 1/10, step 76/574 completed (loss: 2.5429904460906982, acc: 0.4781021773815155)
[2025-01-06 01:00:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:32][root][INFO] - Training Epoch: 1/10, step 77/574 completed (loss: 0.9221081733703613, acc: 0.6666666865348816)
[2025-01-06 01:00:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:32][root][INFO] - Training Epoch: 1/10, step 78/574 completed (loss: 0.9714252352714539, acc: 0.6666666865348816)
[2025-01-06 01:00:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:33][root][INFO] - Training Epoch: 1/10, step 79/574 completed (loss: 1.3005354404449463, acc: 0.6666666865348816)
[2025-01-06 01:00:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:33][root][INFO] - Training Epoch: 1/10, step 80/574 completed (loss: 1.6556757688522339, acc: 0.807692289352417)
[2025-01-06 01:00:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:34][root][INFO] - Training Epoch: 1/10, step 81/574 completed (loss: 2.1469831466674805, acc: 0.6538461446762085)
[2025-01-06 01:00:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:34][root][INFO] - Training Epoch: 1/10, step 82/574 completed (loss: 2.2033472061157227, acc: 0.557692289352417)
[2025-01-06 01:00:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:34][root][INFO] - Training Epoch: 1/10, step 83/574 completed (loss: 0.7546709775924683, acc: 0.78125)
[2025-01-06 01:00:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:35][root][INFO] - Training Epoch: 1/10, step 84/574 completed (loss: 2.1145565509796143, acc: 0.6231883764266968)
[2025-01-06 01:00:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:35][root][INFO] - Training Epoch: 1/10, step 85/574 completed (loss: 2.4699556827545166, acc: 0.6200000047683716)
[2025-01-06 01:00:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:35][root][INFO] - Training Epoch: 1/10, step 86/574 completed (loss: 0.9845025539398193, acc: 0.739130437374115)
[2025-01-06 01:00:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:36][root][INFO] - Training Epoch: 1/10, step 87/574 completed (loss: 2.959163188934326, acc: 0.4000000059604645)
[2025-01-06 01:00:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:36][root][INFO] - Training Epoch: 1/10, step 88/574 completed (loss: 3.0210726261138916, acc: 0.5048543810844421)
[2025-01-06 01:00:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:38][root][INFO] - Training Epoch: 1/10, step 89/574 completed (loss: 2.64070200920105, acc: 0.5485436916351318)
[2025-01-06 01:00:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:38][root][INFO] - Training Epoch: 1/10, step 90/574 completed (loss: 3.2235021591186523, acc: 0.42473119497299194)
[2025-01-06 01:00:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:39][root][INFO] - Training Epoch: 1/10, step 91/574 completed (loss: 2.356602430343628, acc: 0.5387930870056152)
[2025-01-06 01:00:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:40][root][INFO] - Training Epoch: 1/10, step 92/574 completed (loss: 2.27778697013855, acc: 0.6000000238418579)
[2025-01-06 01:00:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:41][root][INFO] - Training Epoch: 1/10, step 93/574 completed (loss: 2.8838136196136475, acc: 0.3762376308441162)
[2025-01-06 01:00:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:41][root][INFO] - Training Epoch: 1/10, step 94/574 completed (loss: 2.0652685165405273, acc: 0.5645161271095276)
[2025-01-06 01:00:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:42][root][INFO] - Training Epoch: 1/10, step 95/574 completed (loss: 1.7059935331344604, acc: 0.6521739363670349)
[2025-01-06 01:00:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:42][root][INFO] - Training Epoch: 1/10, step 96/574 completed (loss: 2.907857894897461, acc: 0.462184876203537)
[2025-01-06 01:00:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:42][root][INFO] - Training Epoch: 1/10, step 97/574 completed (loss: 2.4340083599090576, acc: 0.5288461446762085)
[2025-01-06 01:00:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:43][root][INFO] - Training Epoch: 1/10, step 98/574 completed (loss: 2.447782516479492, acc: 0.5182482004165649)
[2025-01-06 01:00:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:43][root][INFO] - Training Epoch: 1/10, step 99/574 completed (loss: 3.2687087059020996, acc: 0.34328359365463257)
[2025-01-06 01:00:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:44][root][INFO] - Training Epoch: 1/10, step 100/574 completed (loss: 1.685002326965332, acc: 0.6000000238418579)
[2025-01-06 01:00:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:44][root][INFO] - Training Epoch: 1/10, step 101/574 completed (loss: 1.5151630640029907, acc: 0.7727272510528564)
[2025-01-06 01:00:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:44][root][INFO] - Training Epoch: 1/10, step 102/574 completed (loss: 0.4701274335384369, acc: 0.8260869383811951)
[2025-01-06 01:00:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:45][root][INFO] - Training Epoch: 1/10, step 103/574 completed (loss: 0.49450138211250305, acc: 0.8636363744735718)
[2025-01-06 01:00:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:45][root][INFO] - Training Epoch: 1/10, step 104/574 completed (loss: 1.2553813457489014, acc: 0.7586206793785095)
[2025-01-06 01:00:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:45][root][INFO] - Training Epoch: 1/10, step 105/574 completed (loss: 0.6483151912689209, acc: 0.7441860437393188)
[2025-01-06 01:00:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:46][root][INFO] - Training Epoch: 1/10, step 106/574 completed (loss: 0.6564112305641174, acc: 0.7599999904632568)
[2025-01-06 01:00:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:46][root][INFO] - Training Epoch: 1/10, step 107/574 completed (loss: 1.098104476928711, acc: 0.7647058963775635)
[2025-01-06 01:00:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:46][root][INFO] - Training Epoch: 1/10, step 108/574 completed (loss: 0.6119911670684814, acc: 0.8461538553237915)
[2025-01-06 01:00:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:47][root][INFO] - Training Epoch: 1/10, step 109/574 completed (loss: 0.5733845829963684, acc: 0.9047619104385376)
[2025-01-06 01:00:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:47][root][INFO] - Training Epoch: 1/10, step 110/574 completed (loss: 1.4297422170639038, acc: 0.7692307829856873)
[2025-01-06 01:00:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:48][root][INFO] - Training Epoch: 1/10, step 111/574 completed (loss: 1.242030143737793, acc: 0.7017543911933899)
[2025-01-06 01:00:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:48][root][INFO] - Training Epoch: 1/10, step 112/574 completed (loss: 2.030719757080078, acc: 0.5789473652839661)
[2025-01-06 01:00:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:49][root][INFO] - Training Epoch: 1/10, step 113/574 completed (loss: 1.3355225324630737, acc: 0.6666666865348816)
[2025-01-06 01:00:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:49][root][INFO] - Training Epoch: 1/10, step 114/574 completed (loss: 1.4531559944152832, acc: 0.7142857313156128)
[2025-01-06 01:00:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:49][root][INFO] - Training Epoch: 1/10, step 115/574 completed (loss: 0.9336803555488586, acc: 0.7727272510528564)
[2025-01-06 01:00:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:50][root][INFO] - Training Epoch: 1/10, step 116/574 completed (loss: 0.9202200174331665, acc: 0.7936508059501648)
[2025-01-06 01:00:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:50][root][INFO] - Training Epoch: 1/10, step 117/574 completed (loss: 0.9818724989891052, acc: 0.7804877758026123)
[2025-01-06 01:00:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:50][root][INFO] - Training Epoch: 1/10, step 118/574 completed (loss: 1.192895531654358, acc: 0.8064516186714172)
[2025-01-06 01:00:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:51][root][INFO] - Training Epoch: 1/10, step 119/574 completed (loss: 1.144753098487854, acc: 0.7186312079429626)
[2025-01-06 01:00:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:52][root][INFO] - Training Epoch: 1/10, step 120/574 completed (loss: 1.305894136428833, acc: 0.7200000286102295)
[2025-01-06 01:00:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:52][root][INFO] - Training Epoch: 1/10, step 121/574 completed (loss: 2.1191201210021973, acc: 0.6538461446762085)
[2025-01-06 01:00:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:52][root][INFO] - Training Epoch: 1/10, step 122/574 completed (loss: 1.2040207386016846, acc: 0.625)
[2025-01-06 01:00:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:53][root][INFO] - Training Epoch: 1/10, step 123/574 completed (loss: 1.5368326902389526, acc: 0.6315789222717285)
[2025-01-06 01:00:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:53][root][INFO] - Training Epoch: 1/10, step 124/574 completed (loss: 1.5712474584579468, acc: 0.6625766754150391)
[2025-01-06 01:00:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:54][root][INFO] - Training Epoch: 1/10, step 125/574 completed (loss: 1.4695100784301758, acc: 0.6388888955116272)
[2025-01-06 01:00:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:54][root][INFO] - Training Epoch: 1/10, step 126/574 completed (loss: 1.814400315284729, acc: 0.5416666865348816)
[2025-01-06 01:00:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:55][root][INFO] - Training Epoch: 1/10, step 127/574 completed (loss: 1.1863170862197876, acc: 0.6845238208770752)
[2025-01-06 01:00:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:55][root][INFO] - Training Epoch: 1/10, step 128/574 completed (loss: 1.4157365560531616, acc: 0.620512843132019)
[2025-01-06 01:00:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:55][root][INFO] - Training Epoch: 1/10, step 129/574 completed (loss: 1.4988969564437866, acc: 0.654411792755127)
[2025-01-06 01:00:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:56][root][INFO] - Training Epoch: 1/10, step 130/574 completed (loss: 2.1824846267700195, acc: 0.4615384638309479)
[2025-01-06 01:00:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:56][root][INFO] - Training Epoch: 1/10, step 131/574 completed (loss: 2.4251701831817627, acc: 0.47826087474823)
[2025-01-06 01:00:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:57][root][INFO] - Training Epoch: 1/10, step 132/574 completed (loss: 2.391698122024536, acc: 0.375)
[2025-01-06 01:00:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:57][root][INFO] - Training Epoch: 1/10, step 133/574 completed (loss: 2.359903335571289, acc: 0.3913043439388275)
[2025-01-06 01:00:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:57][root][INFO] - Training Epoch: 1/10, step 134/574 completed (loss: 2.0656535625457764, acc: 0.5428571701049805)
[2025-01-06 01:00:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:58][root][INFO] - Training Epoch: 1/10, step 135/574 completed (loss: 2.0671348571777344, acc: 0.5)
[2025-01-06 01:00:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:58][root][INFO] - Training Epoch: 1/10, step 136/574 completed (loss: 1.7794500589370728, acc: 0.5)
[2025-01-06 01:00:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:58][root][INFO] - Training Epoch: 1/10, step 137/574 completed (loss: 2.2228643894195557, acc: 0.3333333432674408)
[2025-01-06 01:00:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:59][root][INFO] - Training Epoch: 1/10, step 138/574 completed (loss: 1.490444540977478, acc: 0.5652173757553101)
[2025-01-06 01:00:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:00:59][root][INFO] - Training Epoch: 1/10, step 139/574 completed (loss: 0.7352169752120972, acc: 0.761904776096344)
[2025-01-06 01:00:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:00][root][INFO] - Training Epoch: 1/10, step 140/574 completed (loss: 1.3862265348434448, acc: 0.5769230723381042)
[2025-01-06 01:01:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:00][root][INFO] - Training Epoch: 1/10, step 141/574 completed (loss: 1.2803184986114502, acc: 0.6129032373428345)
[2025-01-06 01:01:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:00][root][INFO] - Training Epoch: 1/10, step 142/574 completed (loss: 1.2730916738510132, acc: 0.6216216087341309)
[2025-01-06 01:01:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:33][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(3.1857, device='cuda:0') eval_epoch_loss=tensor(1.1587, device='cuda:0') eval_epoch_acc=tensor(0.7317, device='cuda:0')
[2025-01-06 01:01:33][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:01:33][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:01:34][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_1_step_143_loss_1.1586825847625732/model.pt
[2025-01-06 01:01:34][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:01:34][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 1.1586825847625732
[2025-01-06 01:01:34][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.7316946983337402
[2025-01-06 01:01:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:35][root][INFO] - Training Epoch: 1/10, step 143/574 completed (loss: 1.8133007287979126, acc: 0.6140350699424744)
[2025-01-06 01:01:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:35][root][INFO] - Training Epoch: 1/10, step 144/574 completed (loss: 1.498198390007019, acc: 0.6492537260055542)
[2025-01-06 01:01:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:35][root][INFO] - Training Epoch: 1/10, step 145/574 completed (loss: 1.383817434310913, acc: 0.6734693646430969)
[2025-01-06 01:01:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:36][root][INFO] - Training Epoch: 1/10, step 146/574 completed (loss: 1.9412192106246948, acc: 0.478723406791687)
[2025-01-06 01:01:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:36][root][INFO] - Training Epoch: 1/10, step 147/574 completed (loss: 2.0077733993530273, acc: 0.6142857074737549)
[2025-01-06 01:01:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:36][root][INFO] - Training Epoch: 1/10, step 148/574 completed (loss: 2.2853736877441406, acc: 0.3928571343421936)
[2025-01-06 01:01:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:37][root][INFO] - Training Epoch: 1/10, step 149/574 completed (loss: 1.6805156469345093, acc: 0.6521739363670349)
[2025-01-06 01:01:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:37][root][INFO] - Training Epoch: 1/10, step 150/574 completed (loss: 1.7355554103851318, acc: 0.5517241358757019)
[2025-01-06 01:01:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:38][root][INFO] - Training Epoch: 1/10, step 151/574 completed (loss: 1.9950246810913086, acc: 0.52173912525177)
[2025-01-06 01:01:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:38][root][INFO] - Training Epoch: 1/10, step 152/574 completed (loss: 1.4595916271209717, acc: 0.6610169410705566)
[2025-01-06 01:01:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:38][root][INFO] - Training Epoch: 1/10, step 153/574 completed (loss: 1.740472435951233, acc: 0.5438596606254578)
[2025-01-06 01:01:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:39][root][INFO] - Training Epoch: 1/10, step 154/574 completed (loss: 1.7371227741241455, acc: 0.5810810923576355)
[2025-01-06 01:01:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:39][root][INFO] - Training Epoch: 1/10, step 155/574 completed (loss: 1.4880610704421997, acc: 0.7857142686843872)
[2025-01-06 01:01:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:40][root][INFO] - Training Epoch: 1/10, step 156/574 completed (loss: 1.0979986190795898, acc: 0.695652186870575)
[2025-01-06 01:01:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:40][root][INFO] - Training Epoch: 1/10, step 157/574 completed (loss: 3.6154866218566895, acc: 0.21052631735801697)
[2025-01-06 01:01:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:42][root][INFO] - Training Epoch: 1/10, step 158/574 completed (loss: 3.11930775642395, acc: 0.3918918967247009)
[2025-01-06 01:01:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:42][root][INFO] - Training Epoch: 1/10, step 159/574 completed (loss: 2.2521164417266846, acc: 0.46296295523643494)
[2025-01-06 01:01:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:42][root][INFO] - Training Epoch: 1/10, step 160/574 completed (loss: 2.8270204067230225, acc: 0.40697672963142395)
[2025-01-06 01:01:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:43][root][INFO] - Training Epoch: 1/10, step 161/574 completed (loss: 2.9823381900787354, acc: 0.3176470696926117)
[2025-01-06 01:01:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:44][root][INFO] - Training Epoch: 1/10, step 162/574 completed (loss: 2.8240461349487305, acc: 0.42696627974510193)
[2025-01-06 01:01:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:44][root][INFO] - Training Epoch: 1/10, step 163/574 completed (loss: 1.4406166076660156, acc: 0.7272727489471436)
[2025-01-06 01:01:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:44][root][INFO] - Training Epoch: 1/10, step 164/574 completed (loss: 0.9069750905036926, acc: 0.761904776096344)
[2025-01-06 01:01:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:45][root][INFO] - Training Epoch: 1/10, step 165/574 completed (loss: 1.2419666051864624, acc: 0.6206896305084229)
[2025-01-06 01:01:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:45][root][INFO] - Training Epoch: 1/10, step 166/574 completed (loss: 0.5901293754577637, acc: 0.8571428656578064)
[2025-01-06 01:01:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:46][root][INFO] - Training Epoch: 1/10, step 167/574 completed (loss: 0.3323826491832733, acc: 0.8999999761581421)
[2025-01-06 01:01:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:46][root][INFO] - Training Epoch: 1/10, step 168/574 completed (loss: 1.3737258911132812, acc: 0.7222222089767456)
[2025-01-06 01:01:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:46][root][INFO] - Training Epoch: 1/10, step 169/574 completed (loss: 1.2656753063201904, acc: 0.686274528503418)
[2025-01-06 01:01:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:47][root][INFO] - Training Epoch: 1/10, step 170/574 completed (loss: 1.7622241973876953, acc: 0.6095890402793884)
[2025-01-06 01:01:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:48][root][INFO] - Training Epoch: 1/10, step 171/574 completed (loss: 1.0892925262451172, acc: 0.6666666865348816)
[2025-01-06 01:01:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:48][root][INFO] - Training Epoch: 1/10, step 172/574 completed (loss: 1.9727182388305664, acc: 0.4444444477558136)
[2025-01-06 01:01:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:48][root][INFO] - Training Epoch: 1/10, step 173/574 completed (loss: 1.8523234128952026, acc: 0.5357142686843872)
[2025-01-06 01:01:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:49][root][INFO] - Training Epoch: 1/10, step 174/574 completed (loss: 1.400709867477417, acc: 0.7079645991325378)
[2025-01-06 01:01:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:49][root][INFO] - Training Epoch: 1/10, step 175/574 completed (loss: 1.6509157419204712, acc: 0.5652173757553101)
[2025-01-06 01:01:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:50][root][INFO] - Training Epoch: 1/10, step 176/574 completed (loss: 1.069796085357666, acc: 0.75)
[2025-01-06 01:01:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:51][root][INFO] - Training Epoch: 1/10, step 177/574 completed (loss: 1.7988947629928589, acc: 0.5572519302368164)
[2025-01-06 01:01:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:51][root][INFO] - Training Epoch: 1/10, step 178/574 completed (loss: 1.934402346611023, acc: 0.5111111402511597)
[2025-01-06 01:01:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:52][root][INFO] - Training Epoch: 1/10, step 179/574 completed (loss: 1.2353763580322266, acc: 0.7213114500045776)
[2025-01-06 01:01:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:52][root][INFO] - Training Epoch: 1/10, step 180/574 completed (loss: 0.3638155460357666, acc: 0.9583333134651184)
[2025-01-06 01:01:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:52][root][INFO] - Training Epoch: 1/10, step 181/574 completed (loss: 0.3809497356414795, acc: 0.8799999952316284)
[2025-01-06 01:01:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:53][root][INFO] - Training Epoch: 1/10, step 182/574 completed (loss: 0.8016209006309509, acc: 0.8214285969734192)
[2025-01-06 01:01:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:53][root][INFO] - Training Epoch: 1/10, step 183/574 completed (loss: 0.7147901654243469, acc: 0.8048780560493469)
[2025-01-06 01:01:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:54][root][INFO] - Training Epoch: 1/10, step 184/574 completed (loss: 1.2877061367034912, acc: 0.773413896560669)
[2025-01-06 01:01:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:54][root][INFO] - Training Epoch: 1/10, step 185/574 completed (loss: 0.8187381625175476, acc: 0.8097983002662659)
[2025-01-06 01:01:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:55][root][INFO] - Training Epoch: 1/10, step 186/574 completed (loss: 0.8777696490287781, acc: 0.784375011920929)
[2025-01-06 01:01:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:55][root][INFO] - Training Epoch: 1/10, step 187/574 completed (loss: 0.8265831470489502, acc: 0.7879924774169922)
[2025-01-06 01:01:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:56][root][INFO] - Training Epoch: 1/10, step 188/574 completed (loss: 1.0721641778945923, acc: 0.725978672504425)
[2025-01-06 01:01:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:56][root][INFO] - Training Epoch: 1/10, step 189/574 completed (loss: 1.1922845840454102, acc: 0.6399999856948853)
[2025-01-06 01:01:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:57][root][INFO] - Training Epoch: 1/10, step 190/574 completed (loss: 1.5684839487075806, acc: 0.6395348906517029)
[2025-01-06 01:01:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:57][root][INFO] - Training Epoch: 1/10, step 191/574 completed (loss: 2.316472291946411, acc: 0.4841269850730896)
[2025-01-06 01:01:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:58][root][INFO] - Training Epoch: 1/10, step 192/574 completed (loss: 1.714808702468872, acc: 0.560606062412262)
[2025-01-06 01:01:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:01:59][root][INFO] - Training Epoch: 1/10, step 193/574 completed (loss: 1.7674024105072021, acc: 0.5764706134796143)
[2025-01-06 01:01:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:00][root][INFO] - Training Epoch: 1/10, step 194/574 completed (loss: 1.9359149932861328, acc: 0.5432098507881165)
[2025-01-06 01:02:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:01][root][INFO] - Training Epoch: 1/10, step 195/574 completed (loss: 1.951974630355835, acc: 0.5322580933570862)
[2025-01-06 01:02:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:01][root][INFO] - Training Epoch: 1/10, step 196/574 completed (loss: 0.8484710454940796, acc: 0.7857142686843872)
[2025-01-06 01:02:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:02][root][INFO] - Training Epoch: 1/10, step 197/574 completed (loss: 2.130281925201416, acc: 0.5)
[2025-01-06 01:02:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:02][root][INFO] - Training Epoch: 1/10, step 198/574 completed (loss: 1.6386348009109497, acc: 0.6470588445663452)
[2025-01-06 01:02:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:03][root][INFO] - Training Epoch: 1/10, step 199/574 completed (loss: 1.5695605278015137, acc: 0.6985294222831726)
[2025-01-06 01:02:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:03][root][INFO] - Training Epoch: 1/10, step 200/574 completed (loss: 0.9777063131332397, acc: 0.7118644118309021)
[2025-01-06 01:02:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:03][root][INFO] - Training Epoch: 1/10, step 201/574 completed (loss: 1.6179800033569336, acc: 0.6194030046463013)
[2025-01-06 01:02:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:04][root][INFO] - Training Epoch: 1/10, step 202/574 completed (loss: 1.7721519470214844, acc: 0.6213592290878296)
[2025-01-06 01:02:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:04][root][INFO] - Training Epoch: 1/10, step 203/574 completed (loss: 1.4674711227416992, acc: 0.6507936716079712)
[2025-01-06 01:02:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:05][root][INFO] - Training Epoch: 1/10, step 204/574 completed (loss: 0.6210256814956665, acc: 0.8901098966598511)
[2025-01-06 01:02:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:05][root][INFO] - Training Epoch: 1/10, step 205/574 completed (loss: 0.8537670373916626, acc: 0.8206278085708618)
[2025-01-06 01:02:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:05][root][INFO] - Training Epoch: 1/10, step 206/574 completed (loss: 0.8675282001495361, acc: 0.7795275449752808)
[2025-01-06 01:02:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:06][root][INFO] - Training Epoch: 1/10, step 207/574 completed (loss: 1.023289442062378, acc: 0.7715517282485962)
[2025-01-06 01:02:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:06][root][INFO] - Training Epoch: 1/10, step 208/574 completed (loss: 0.7584750652313232, acc: 0.8079710006713867)
[2025-01-06 01:02:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:07][root][INFO] - Training Epoch: 1/10, step 209/574 completed (loss: 0.881391704082489, acc: 0.801556408405304)
[2025-01-06 01:02:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:07][root][INFO] - Training Epoch: 1/10, step 210/574 completed (loss: 0.6284053921699524, acc: 0.8586956262588501)
[2025-01-06 01:02:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:07][root][INFO] - Training Epoch: 1/10, step 211/574 completed (loss: 1.1885472536087036, acc: 0.739130437374115)
[2025-01-06 01:02:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:08][root][INFO] - Training Epoch: 1/10, step 212/574 completed (loss: 0.4852323830127716, acc: 0.8214285969734192)
[2025-01-06 01:02:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:08][root][INFO] - Training Epoch: 1/10, step 213/574 completed (loss: 0.8277787566184998, acc: 0.8297872543334961)
[2025-01-06 01:02:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:09][root][INFO] - Training Epoch: 1/10, step 214/574 completed (loss: 0.9558125734329224, acc: 0.800000011920929)
[2025-01-06 01:02:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:09][root][INFO] - Training Epoch: 1/10, step 215/574 completed (loss: 0.34890785813331604, acc: 0.9054054021835327)
[2025-01-06 01:02:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:09][root][INFO] - Training Epoch: 1/10, step 216/574 completed (loss: 0.7036503553390503, acc: 0.8720930218696594)
[2025-01-06 01:02:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:10][root][INFO] - Training Epoch: 1/10, step 217/574 completed (loss: 0.9447864890098572, acc: 0.7747747898101807)
[2025-01-06 01:02:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:10][root][INFO] - Training Epoch: 1/10, step 218/574 completed (loss: 0.6039091944694519, acc: 0.8444444537162781)
[2025-01-06 01:02:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:11][root][INFO] - Training Epoch: 1/10, step 219/574 completed (loss: 0.704095721244812, acc: 0.8181818127632141)
[2025-01-06 01:02:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:11][root][INFO] - Training Epoch: 1/10, step 220/574 completed (loss: 0.7219918370246887, acc: 0.7407407164573669)
[2025-01-06 01:02:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:12][root][INFO] - Training Epoch: 1/10, step 221/574 completed (loss: 0.37386173009872437, acc: 0.8799999952316284)
[2025-01-06 01:02:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:12][root][INFO] - Training Epoch: 1/10, step 222/574 completed (loss: 1.1299134492874146, acc: 0.5961538553237915)
[2025-01-06 01:02:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:13][root][INFO] - Training Epoch: 1/10, step 223/574 completed (loss: 0.9880853891372681, acc: 0.7771739363670349)
[2025-01-06 01:02:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:13][root][INFO] - Training Epoch: 1/10, step 224/574 completed (loss: 1.0224581956863403, acc: 0.7727272510528564)
[2025-01-06 01:02:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:14][root][INFO] - Training Epoch: 1/10, step 225/574 completed (loss: 1.157251238822937, acc: 0.7021276354789734)
[2025-01-06 01:02:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:14][root][INFO] - Training Epoch: 1/10, step 226/574 completed (loss: 1.3819166421890259, acc: 0.6415094137191772)
[2025-01-06 01:02:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:15][root][INFO] - Training Epoch: 1/10, step 227/574 completed (loss: 0.9733750224113464, acc: 0.7166666388511658)
[2025-01-06 01:02:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:15][root][INFO] - Training Epoch: 1/10, step 228/574 completed (loss: 1.2306827306747437, acc: 0.7209302186965942)
[2025-01-06 01:02:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:15][root][INFO] - Training Epoch: 1/10, step 229/574 completed (loss: 2.3621790409088135, acc: 0.4333333373069763)
[2025-01-06 01:02:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:16][root][INFO] - Training Epoch: 1/10, step 230/574 completed (loss: 2.3176932334899902, acc: 0.4000000059604645)
[2025-01-06 01:02:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:16][root][INFO] - Training Epoch: 1/10, step 231/574 completed (loss: 2.104884386062622, acc: 0.4888888895511627)
[2025-01-06 01:02:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:17][root][INFO] - Training Epoch: 1/10, step 232/574 completed (loss: 2.078317880630493, acc: 0.4444444477558136)
[2025-01-06 01:02:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:17][root][INFO] - Training Epoch: 1/10, step 233/574 completed (loss: 2.3752667903900146, acc: 0.4678899049758911)
[2025-01-06 01:02:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:18][root][INFO] - Training Epoch: 1/10, step 234/574 completed (loss: 2.1621525287628174, acc: 0.4692307710647583)
[2025-01-06 01:02:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:18][root][INFO] - Training Epoch: 1/10, step 235/574 completed (loss: 1.0092352628707886, acc: 0.6842105388641357)
[2025-01-06 01:02:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:18][root][INFO] - Training Epoch: 1/10, step 236/574 completed (loss: 0.9620973467826843, acc: 0.7083333134651184)
[2025-01-06 01:02:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:19][root][INFO] - Training Epoch: 1/10, step 237/574 completed (loss: 1.7593655586242676, acc: 0.5909090638160706)
[2025-01-06 01:02:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:19][root][INFO] - Training Epoch: 1/10, step 238/574 completed (loss: 1.333633303642273, acc: 0.7407407164573669)
[2025-01-06 01:02:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:19][root][INFO] - Training Epoch: 1/10, step 239/574 completed (loss: 1.4449703693389893, acc: 0.5714285969734192)
[2025-01-06 01:02:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:20][root][INFO] - Training Epoch: 1/10, step 240/574 completed (loss: 1.8470721244812012, acc: 0.5454545617103577)
[2025-01-06 01:02:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:20][root][INFO] - Training Epoch: 1/10, step 241/574 completed (loss: 1.1872295141220093, acc: 0.7045454382896423)
[2025-01-06 01:02:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:21][root][INFO] - Training Epoch: 1/10, step 242/574 completed (loss: 2.138962984085083, acc: 0.4354838728904724)
[2025-01-06 01:02:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:21][root][INFO] - Training Epoch: 1/10, step 243/574 completed (loss: 1.941954493522644, acc: 0.4545454680919647)
[2025-01-06 01:02:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:22][root][INFO] - Training Epoch: 1/10, step 244/574 completed (loss: 0.14991600811481476, acc: 1.0)
[2025-01-06 01:02:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:22][root][INFO] - Training Epoch: 1/10, step 245/574 completed (loss: 0.7288596630096436, acc: 0.7692307829856873)
[2025-01-06 01:02:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:22][root][INFO] - Training Epoch: 1/10, step 246/574 completed (loss: 0.23707932233810425, acc: 0.8709677457809448)
[2025-01-06 01:02:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:23][root][INFO] - Training Epoch: 1/10, step 247/574 completed (loss: 0.49087876081466675, acc: 0.8500000238418579)
[2025-01-06 01:02:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:23][root][INFO] - Training Epoch: 1/10, step 248/574 completed (loss: 1.2458029985427856, acc: 0.7567567825317383)
[2025-01-06 01:02:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:23][root][INFO] - Training Epoch: 1/10, step 249/574 completed (loss: 0.7291963696479797, acc: 0.7837837934494019)
[2025-01-06 01:02:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:24][root][INFO] - Training Epoch: 1/10, step 250/574 completed (loss: 0.6870085597038269, acc: 0.8918918967247009)
[2025-01-06 01:02:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:24][root][INFO] - Training Epoch: 1/10, step 251/574 completed (loss: 0.8631909489631653, acc: 0.779411792755127)
[2025-01-06 01:02:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:25][root][INFO] - Training Epoch: 1/10, step 252/574 completed (loss: 0.7271607518196106, acc: 0.8780487775802612)
[2025-01-06 01:02:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:25][root][INFO] - Training Epoch: 1/10, step 253/574 completed (loss: 0.7405747771263123, acc: 0.800000011920929)
[2025-01-06 01:02:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:25][root][INFO] - Training Epoch: 1/10, step 254/574 completed (loss: 0.12232787907123566, acc: 0.9599999785423279)
[2025-01-06 01:02:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:26][root][INFO] - Training Epoch: 1/10, step 255/574 completed (loss: 0.8839313983917236, acc: 0.7096773982048035)
[2025-01-06 01:02:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:26][root][INFO] - Training Epoch: 1/10, step 256/574 completed (loss: 0.8938177824020386, acc: 0.859649121761322)
[2025-01-06 01:02:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:26][root][INFO] - Training Epoch: 1/10, step 257/574 completed (loss: 0.3828529417514801, acc: 0.8999999761581421)
[2025-01-06 01:02:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:27][root][INFO] - Training Epoch: 1/10, step 258/574 completed (loss: 0.266270250082016, acc: 0.9210526347160339)
[2025-01-06 01:02:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:27][root][INFO] - Training Epoch: 1/10, step 259/574 completed (loss: 0.6527619361877441, acc: 0.8113207817077637)
[2025-01-06 01:02:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:28][root][INFO] - Training Epoch: 1/10, step 260/574 completed (loss: 0.7154539227485657, acc: 0.8166666626930237)
[2025-01-06 01:02:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:28][root][INFO] - Training Epoch: 1/10, step 261/574 completed (loss: 0.48788580298423767, acc: 0.8888888955116272)
[2025-01-06 01:02:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:29][root][INFO] - Training Epoch: 1/10, step 262/574 completed (loss: 1.4611833095550537, acc: 0.5806451439857483)
[2025-01-06 01:02:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:29][root][INFO] - Training Epoch: 1/10, step 263/574 completed (loss: 1.6506986618041992, acc: 0.5733333230018616)
[2025-01-06 01:02:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:30][root][INFO] - Training Epoch: 1/10, step 264/574 completed (loss: 0.9303960204124451, acc: 0.7291666865348816)
[2025-01-06 01:02:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:30][root][INFO] - Training Epoch: 1/10, step 265/574 completed (loss: 1.9742937088012695, acc: 0.527999997138977)
[2025-01-06 01:02:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:31][root][INFO] - Training Epoch: 1/10, step 266/574 completed (loss: 1.7131458520889282, acc: 0.5393258333206177)
[2025-01-06 01:02:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:31][root][INFO] - Training Epoch: 1/10, step 267/574 completed (loss: 1.6324859857559204, acc: 0.6486486196517944)
[2025-01-06 01:02:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:32][root][INFO] - Training Epoch: 1/10, step 268/574 completed (loss: 1.23255455493927, acc: 0.6896551847457886)
[2025-01-06 01:02:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:32][root][INFO] - Training Epoch: 1/10, step 269/574 completed (loss: 0.8253955245018005, acc: 0.7727272510528564)
[2025-01-06 01:02:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:32][root][INFO] - Training Epoch: 1/10, step 270/574 completed (loss: 0.4990929663181305, acc: 0.8636363744735718)
[2025-01-06 01:02:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:33][root][INFO] - Training Epoch: 1/10, step 271/574 completed (loss: 0.3829019069671631, acc: 0.90625)
[2025-01-06 01:02:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:33][root][INFO] - Training Epoch: 1/10, step 272/574 completed (loss: 0.19782447814941406, acc: 0.9333333373069763)
[2025-01-06 01:02:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:34][root][INFO] - Training Epoch: 1/10, step 273/574 completed (loss: 0.5849542021751404, acc: 0.8666666746139526)
[2025-01-06 01:02:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:34][root][INFO] - Training Epoch: 1/10, step 274/574 completed (loss: 0.3207775950431824, acc: 0.875)
[2025-01-06 01:02:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:34][root][INFO] - Training Epoch: 1/10, step 275/574 completed (loss: 0.510931134223938, acc: 0.8999999761581421)
[2025-01-06 01:02:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:35][root][INFO] - Training Epoch: 1/10, step 276/574 completed (loss: 0.5002748370170593, acc: 0.7931034564971924)
[2025-01-06 01:02:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:35][root][INFO] - Training Epoch: 1/10, step 277/574 completed (loss: 0.4310816526412964, acc: 0.9200000166893005)
[2025-01-06 01:02:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:35][root][INFO] - Training Epoch: 1/10, step 278/574 completed (loss: 0.7206261157989502, acc: 0.8297872543334961)
[2025-01-06 01:02:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:36][root][INFO] - Training Epoch: 1/10, step 279/574 completed (loss: 0.5919608473777771, acc: 0.875)
[2025-01-06 01:02:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:36][root][INFO] - Training Epoch: 1/10, step 280/574 completed (loss: 0.3500055968761444, acc: 0.9090909361839294)
[2025-01-06 01:02:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:37][root][INFO] - Training Epoch: 1/10, step 281/574 completed (loss: 1.015984058380127, acc: 0.7108433842658997)
[2025-01-06 01:02:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:37][root][INFO] - Training Epoch: 1/10, step 282/574 completed (loss: 1.1961537599563599, acc: 0.6666666865348816)
[2025-01-06 01:02:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:37][root][INFO] - Training Epoch: 1/10, step 283/574 completed (loss: 0.42980632185935974, acc: 0.8157894611358643)
[2025-01-06 01:02:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:38][root][INFO] - Training Epoch: 1/10, step 284/574 completed (loss: 1.1846035718917847, acc: 0.7058823704719543)
[2025-01-06 01:02:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:38][root][INFO] - Training Epoch: 1/10, step 285/574 completed (loss: 0.389043927192688, acc: 0.925000011920929)
[2025-01-06 01:02:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:02:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:08][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.2298, device='cuda:0') eval_epoch_loss=tensor(0.8019, device='cuda:0') eval_epoch_acc=tensor(0.7935, device='cuda:0')
[2025-01-06 01:03:08][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:03:08][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:03:09][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_1_step_286_loss_0.8019208908081055/model.pt
[2025-01-06 01:03:09][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:03:09][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.8019208908081055
[2025-01-06 01:03:09][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.7934848666191101
[2025-01-06 01:03:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:09][root][INFO] - Training Epoch: 1/10, step 286/574 completed (loss: 0.7133325338363647, acc: 0.8046875)
[2025-01-06 01:03:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:10][root][INFO] - Training Epoch: 1/10, step 287/574 completed (loss: 0.9387662410736084, acc: 0.7440000176429749)
[2025-01-06 01:03:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:10][root][INFO] - Training Epoch: 1/10, step 288/574 completed (loss: 0.6216768622398376, acc: 0.8791208863258362)
[2025-01-06 01:03:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:10][root][INFO] - Training Epoch: 1/10, step 289/574 completed (loss: 0.9460833072662354, acc: 0.7888198494911194)
[2025-01-06 01:03:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:11][root][INFO] - Training Epoch: 1/10, step 290/574 completed (loss: 0.8468931317329407, acc: 0.8144329786300659)
[2025-01-06 01:03:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:11][root][INFO] - Training Epoch: 1/10, step 291/574 completed (loss: 0.3929460346698761, acc: 0.9090909361839294)
[2025-01-06 01:03:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:11][root][INFO] - Training Epoch: 1/10, step 292/574 completed (loss: 0.9273978471755981, acc: 0.738095223903656)
[2025-01-06 01:03:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:12][root][INFO] - Training Epoch: 1/10, step 293/574 completed (loss: 0.7628114223480225, acc: 0.8275862336158752)
[2025-01-06 01:03:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:12][root][INFO] - Training Epoch: 1/10, step 294/574 completed (loss: 0.7498511672019958, acc: 0.8181818127632141)
[2025-01-06 01:03:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:13][root][INFO] - Training Epoch: 1/10, step 295/574 completed (loss: 0.7499509453773499, acc: 0.8144329786300659)
[2025-01-06 01:03:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:13][root][INFO] - Training Epoch: 1/10, step 296/574 completed (loss: 0.7951481342315674, acc: 0.7931034564971924)
[2025-01-06 01:03:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:14][root][INFO] - Training Epoch: 1/10, step 297/574 completed (loss: 0.7010436654090881, acc: 0.7777777910232544)
[2025-01-06 01:03:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:14][root][INFO] - Training Epoch: 1/10, step 298/574 completed (loss: 1.152280569076538, acc: 0.6842105388641357)
[2025-01-06 01:03:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:14][root][INFO] - Training Epoch: 1/10, step 299/574 completed (loss: 0.2008243054151535, acc: 0.9642857313156128)
[2025-01-06 01:03:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:15][root][INFO] - Training Epoch: 1/10, step 300/574 completed (loss: 0.3846021294593811, acc: 0.90625)
[2025-01-06 01:03:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:15][root][INFO] - Training Epoch: 1/10, step 301/574 completed (loss: 0.741578221321106, acc: 0.849056601524353)
[2025-01-06 01:03:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:15][root][INFO] - Training Epoch: 1/10, step 302/574 completed (loss: 0.5088059306144714, acc: 0.8867924809455872)
[2025-01-06 01:03:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:16][root][INFO] - Training Epoch: 1/10, step 303/574 completed (loss: 0.24371017515659332, acc: 0.9411764740943909)
[2025-01-06 01:03:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:16][root][INFO] - Training Epoch: 1/10, step 304/574 completed (loss: 0.6731700301170349, acc: 0.78125)
[2025-01-06 01:03:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:16][root][INFO] - Training Epoch: 1/10, step 305/574 completed (loss: 0.819186270236969, acc: 0.7377049326896667)
[2025-01-06 01:03:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:17][root][INFO] - Training Epoch: 1/10, step 306/574 completed (loss: 0.6796799302101135, acc: 0.8666666746139526)
[2025-01-06 01:03:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:17][root][INFO] - Training Epoch: 1/10, step 307/574 completed (loss: 0.3412132263183594, acc: 0.9473684430122375)
[2025-01-06 01:03:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:17][root][INFO] - Training Epoch: 1/10, step 308/574 completed (loss: 0.5922541618347168, acc: 0.8550724387168884)
[2025-01-06 01:03:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:18][root][INFO] - Training Epoch: 1/10, step 309/574 completed (loss: 0.3839297294616699, acc: 0.9305555820465088)
[2025-01-06 01:03:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:18][root][INFO] - Training Epoch: 1/10, step 310/574 completed (loss: 0.501461386680603, acc: 0.8192771077156067)
[2025-01-06 01:03:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:19][root][INFO] - Training Epoch: 1/10, step 311/574 completed (loss: 0.6396678686141968, acc: 0.807692289352417)
[2025-01-06 01:03:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:19][root][INFO] - Training Epoch: 1/10, step 312/574 completed (loss: 0.31354498863220215, acc: 0.9489796161651611)
[2025-01-06 01:03:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:19][root][INFO] - Training Epoch: 1/10, step 313/574 completed (loss: 0.34681394696235657, acc: 0.875)
[2025-01-06 01:03:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:20][root][INFO] - Training Epoch: 1/10, step 314/574 completed (loss: 0.27741196751594543, acc: 0.9166666865348816)
[2025-01-06 01:03:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:20][root][INFO] - Training Epoch: 1/10, step 315/574 completed (loss: 0.5003311634063721, acc: 0.8709677457809448)
[2025-01-06 01:03:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:21][root][INFO] - Training Epoch: 1/10, step 316/574 completed (loss: 1.0043245553970337, acc: 0.774193525314331)
[2025-01-06 01:03:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:21][root][INFO] - Training Epoch: 1/10, step 317/574 completed (loss: 0.6104412078857422, acc: 0.89552241563797)
[2025-01-06 01:03:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:21][root][INFO] - Training Epoch: 1/10, step 318/574 completed (loss: 0.252848744392395, acc: 0.9134615659713745)
[2025-01-06 01:03:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:22][root][INFO] - Training Epoch: 1/10, step 319/574 completed (loss: 0.5686008930206299, acc: 0.8222222328186035)
[2025-01-06 01:03:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:22][root][INFO] - Training Epoch: 1/10, step 320/574 completed (loss: 0.20443867146968842, acc: 0.9354838728904724)
[2025-01-06 01:03:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:22][root][INFO] - Training Epoch: 1/10, step 321/574 completed (loss: 0.26219359040260315, acc: 0.9399999976158142)
[2025-01-06 01:03:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:23][root][INFO] - Training Epoch: 1/10, step 322/574 completed (loss: 1.0115891695022583, acc: 0.6666666865348816)
[2025-01-06 01:03:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:23][root][INFO] - Training Epoch: 1/10, step 323/574 completed (loss: 2.0313608646392822, acc: 0.4285714328289032)
[2025-01-06 01:03:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:23][root][INFO] - Training Epoch: 1/10, step 324/574 completed (loss: 2.13932466506958, acc: 0.5128205418586731)
[2025-01-06 01:03:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:24][root][INFO] - Training Epoch: 1/10, step 325/574 completed (loss: 1.9493122100830078, acc: 0.5609756112098694)
[2025-01-06 01:03:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:24][root][INFO] - Training Epoch: 1/10, step 326/574 completed (loss: 1.3116586208343506, acc: 0.6315789222717285)
[2025-01-06 01:03:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:25][root][INFO] - Training Epoch: 1/10, step 327/574 completed (loss: 0.9057068824768066, acc: 0.8421052694320679)
[2025-01-06 01:03:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:25][root][INFO] - Training Epoch: 1/10, step 328/574 completed (loss: 0.42213064432144165, acc: 0.8928571343421936)
[2025-01-06 01:03:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:25][root][INFO] - Training Epoch: 1/10, step 329/574 completed (loss: 0.5507743954658508, acc: 0.8518518805503845)
[2025-01-06 01:03:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:26][root][INFO] - Training Epoch: 1/10, step 330/574 completed (loss: 0.3359340727329254, acc: 0.9375)
[2025-01-06 01:03:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:26][root][INFO] - Training Epoch: 1/10, step 331/574 completed (loss: 0.6429754495620728, acc: 0.8387096524238586)
[2025-01-06 01:03:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:26][root][INFO] - Training Epoch: 1/10, step 332/574 completed (loss: 0.37479129433631897, acc: 0.9122806787490845)
[2025-01-06 01:03:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:27][root][INFO] - Training Epoch: 1/10, step 333/574 completed (loss: 0.28325536847114563, acc: 0.9375)
[2025-01-06 01:03:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:27][root][INFO] - Training Epoch: 1/10, step 334/574 completed (loss: 0.3177732825279236, acc: 0.8999999761581421)
[2025-01-06 01:03:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:27][root][INFO] - Training Epoch: 1/10, step 335/574 completed (loss: 1.0092910528182983, acc: 0.6315789222717285)
[2025-01-06 01:03:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:28][root][INFO] - Training Epoch: 1/10, step 336/574 completed (loss: 1.2960195541381836, acc: 0.6399999856948853)
[2025-01-06 01:03:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:28][root][INFO] - Training Epoch: 1/10, step 337/574 completed (loss: 1.6848795413970947, acc: 0.6321839094161987)
[2025-01-06 01:03:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:29][root][INFO] - Training Epoch: 1/10, step 338/574 completed (loss: 1.6567370891571045, acc: 0.5531914830207825)
[2025-01-06 01:03:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:29][root][INFO] - Training Epoch: 1/10, step 339/574 completed (loss: 1.5951972007751465, acc: 0.5783132314682007)
[2025-01-06 01:03:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:29][root][INFO] - Training Epoch: 1/10, step 340/574 completed (loss: 0.6147688031196594, acc: 0.8260869383811951)
[2025-01-06 01:03:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:29][root][INFO] - Training Epoch: 1/10, step 341/574 completed (loss: 0.9457850456237793, acc: 0.7948718070983887)
[2025-01-06 01:03:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:30][root][INFO] - Training Epoch: 1/10, step 342/574 completed (loss: 0.7894880771636963, acc: 0.8313252925872803)
[2025-01-06 01:03:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:30][root][INFO] - Training Epoch: 1/10, step 343/574 completed (loss: 0.7989235520362854, acc: 0.7924528121948242)
[2025-01-06 01:03:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:31][root][INFO] - Training Epoch: 1/10, step 344/574 completed (loss: 0.36584559082984924, acc: 0.8987341523170471)
[2025-01-06 01:03:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:31][root][INFO] - Training Epoch: 1/10, step 345/574 completed (loss: 0.1923258900642395, acc: 0.9215686321258545)
[2025-01-06 01:03:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:31][root][INFO] - Training Epoch: 1/10, step 346/574 completed (loss: 0.7073538303375244, acc: 0.8208954930305481)
[2025-01-06 01:03:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:32][root][INFO] - Training Epoch: 1/10, step 347/574 completed (loss: 0.3340352773666382, acc: 0.8999999761581421)
[2025-01-06 01:03:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:32][root][INFO] - Training Epoch: 1/10, step 348/574 completed (loss: 0.9714672565460205, acc: 0.7599999904632568)
[2025-01-06 01:03:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:32][root][INFO] - Training Epoch: 1/10, step 349/574 completed (loss: 1.1837072372436523, acc: 0.6944444179534912)
[2025-01-06 01:03:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:33][root][INFO] - Training Epoch: 1/10, step 350/574 completed (loss: 1.1957793235778809, acc: 0.604651153087616)
[2025-01-06 01:03:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:33][root][INFO] - Training Epoch: 1/10, step 351/574 completed (loss: 0.7172326445579529, acc: 0.7692307829856873)
[2025-01-06 01:03:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:34][root][INFO] - Training Epoch: 1/10, step 352/574 completed (loss: 1.3927744626998901, acc: 0.5777778029441833)
[2025-01-06 01:03:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:34][root][INFO] - Training Epoch: 1/10, step 353/574 completed (loss: 0.2453639805316925, acc: 0.8695651888847351)
[2025-01-06 01:03:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:34][root][INFO] - Training Epoch: 1/10, step 354/574 completed (loss: 0.9511269927024841, acc: 0.692307710647583)
[2025-01-06 01:03:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:35][root][INFO] - Training Epoch: 1/10, step 355/574 completed (loss: 1.1009798049926758, acc: 0.7032967209815979)
[2025-01-06 01:03:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:35][root][INFO] - Training Epoch: 1/10, step 356/574 completed (loss: 1.0857340097427368, acc: 0.6521739363670349)
[2025-01-06 01:03:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:36][root][INFO] - Training Epoch: 1/10, step 357/574 completed (loss: 1.0368748903274536, acc: 0.6739130616188049)
[2025-01-06 01:03:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:36][root][INFO] - Training Epoch: 1/10, step 358/574 completed (loss: 0.9623553156852722, acc: 0.7142857313156128)
[2025-01-06 01:03:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:36][root][INFO] - Training Epoch: 1/10, step 359/574 completed (loss: 0.162997767329216, acc: 0.9583333134651184)
[2025-01-06 01:03:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:37][root][INFO] - Training Epoch: 1/10, step 360/574 completed (loss: 0.8441877961158752, acc: 0.7307692170143127)
[2025-01-06 01:03:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:37][root][INFO] - Training Epoch: 1/10, step 361/574 completed (loss: 0.8637880682945251, acc: 0.8048780560493469)
[2025-01-06 01:03:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:37][root][INFO] - Training Epoch: 1/10, step 362/574 completed (loss: 0.4070178270339966, acc: 0.8888888955116272)
[2025-01-06 01:03:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:38][root][INFO] - Training Epoch: 1/10, step 363/574 completed (loss: 0.43989071249961853, acc: 0.8947368264198303)
[2025-01-06 01:03:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:38][root][INFO] - Training Epoch: 1/10, step 364/574 completed (loss: 0.502916157245636, acc: 0.8292682766914368)
[2025-01-06 01:03:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:38][root][INFO] - Training Epoch: 1/10, step 365/574 completed (loss: 0.634061336517334, acc: 0.8484848737716675)
[2025-01-06 01:03:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:39][root][INFO] - Training Epoch: 1/10, step 366/574 completed (loss: 0.10641727596521378, acc: 0.9583333134651184)
[2025-01-06 01:03:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:39][root][INFO] - Training Epoch: 1/10, step 367/574 completed (loss: 0.580049991607666, acc: 0.8260869383811951)
[2025-01-06 01:03:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:40][root][INFO] - Training Epoch: 1/10, step 368/574 completed (loss: 0.4057846963405609, acc: 0.9285714030265808)
[2025-01-06 01:03:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:40][root][INFO] - Training Epoch: 1/10, step 369/574 completed (loss: 0.7013149857521057, acc: 0.8125)
[2025-01-06 01:03:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:41][root][INFO] - Training Epoch: 1/10, step 370/574 completed (loss: 0.8213034868240356, acc: 0.7515151500701904)
[2025-01-06 01:03:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:41][root][INFO] - Training Epoch: 1/10, step 371/574 completed (loss: 0.6460599899291992, acc: 0.8207547068595886)
[2025-01-06 01:03:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:42][root][INFO] - Training Epoch: 1/10, step 372/574 completed (loss: 0.2965221703052521, acc: 0.9111111164093018)
[2025-01-06 01:03:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:42][root][INFO] - Training Epoch: 1/10, step 373/574 completed (loss: 0.5364047884941101, acc: 0.9285714030265808)
[2025-01-06 01:03:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:42][root][INFO] - Training Epoch: 1/10, step 374/574 completed (loss: 0.36913201212882996, acc: 0.8857142925262451)
[2025-01-06 01:03:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:43][root][INFO] - Training Epoch: 1/10, step 375/574 completed (loss: 0.051501356065273285, acc: 0.9599999785423279)
[2025-01-06 01:03:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:43][root][INFO] - Training Epoch: 1/10, step 376/574 completed (loss: 0.37806397676467896, acc: 0.8695651888847351)
[2025-01-06 01:03:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:43][root][INFO] - Training Epoch: 1/10, step 377/574 completed (loss: 0.2991867661476135, acc: 0.9166666865348816)
[2025-01-06 01:03:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:44][root][INFO] - Training Epoch: 1/10, step 378/574 completed (loss: 0.1597858965396881, acc: 0.9368420839309692)
[2025-01-06 01:03:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:44][root][INFO] - Training Epoch: 1/10, step 379/574 completed (loss: 0.38388875126838684, acc: 0.8982036113739014)
[2025-01-06 01:03:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:45][root][INFO] - Training Epoch: 1/10, step 380/574 completed (loss: 0.46795400977134705, acc: 0.8646616339683533)
[2025-01-06 01:03:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:46][root][INFO] - Training Epoch: 1/10, step 381/574 completed (loss: 0.9217604398727417, acc: 0.7433155179023743)
[2025-01-06 01:03:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:47][root][INFO] - Training Epoch: 1/10, step 382/574 completed (loss: 0.3108471930027008, acc: 0.8828828930854797)
[2025-01-06 01:03:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:47][root][INFO] - Training Epoch: 1/10, step 383/574 completed (loss: 0.8907738327980042, acc: 0.7857142686843872)
[2025-01-06 01:03:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:47][root][INFO] - Training Epoch: 1/10, step 384/574 completed (loss: 0.14951595664024353, acc: 0.9642857313156128)
[2025-01-06 01:03:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:48][root][INFO] - Training Epoch: 1/10, step 385/574 completed (loss: 0.40960273146629333, acc: 0.9375)
[2025-01-06 01:03:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:48][root][INFO] - Training Epoch: 1/10, step 386/574 completed (loss: 0.1412639021873474, acc: 0.9722222089767456)
[2025-01-06 01:03:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:48][root][INFO] - Training Epoch: 1/10, step 387/574 completed (loss: 0.17698317766189575, acc: 0.9736841917037964)
[2025-01-06 01:03:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:49][root][INFO] - Training Epoch: 1/10, step 388/574 completed (loss: 0.21772700548171997, acc: 0.9545454382896423)
[2025-01-06 01:03:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:49][root][INFO] - Training Epoch: 1/10, step 389/574 completed (loss: 0.10819413512945175, acc: 0.949999988079071)
[2025-01-06 01:03:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:50][root][INFO] - Training Epoch: 1/10, step 390/574 completed (loss: 0.7510315179824829, acc: 0.761904776096344)
[2025-01-06 01:03:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:50][root][INFO] - Training Epoch: 1/10, step 391/574 completed (loss: 1.4274235963821411, acc: 0.5925925970077515)
[2025-01-06 01:03:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:50][root][INFO] - Training Epoch: 1/10, step 392/574 completed (loss: 1.1736772060394287, acc: 0.6893203854560852)
[2025-01-06 01:03:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:51][root][INFO] - Training Epoch: 1/10, step 393/574 completed (loss: 1.2658659219741821, acc: 0.7279411554336548)
[2025-01-06 01:03:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:51][root][INFO] - Training Epoch: 1/10, step 394/574 completed (loss: 1.101735234260559, acc: 0.6666666865348816)
[2025-01-06 01:03:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:52][root][INFO] - Training Epoch: 1/10, step 395/574 completed (loss: 1.218258023262024, acc: 0.6736111044883728)
[2025-01-06 01:03:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:52][root][INFO] - Training Epoch: 1/10, step 396/574 completed (loss: 1.085007667541504, acc: 0.6744186282157898)
[2025-01-06 01:03:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:52][root][INFO] - Training Epoch: 1/10, step 397/574 completed (loss: 0.4723799228668213, acc: 0.875)
[2025-01-06 01:03:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:53][root][INFO] - Training Epoch: 1/10, step 398/574 completed (loss: 0.6331213712692261, acc: 0.8139534592628479)
[2025-01-06 01:03:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:53][root][INFO] - Training Epoch: 1/10, step 399/574 completed (loss: 0.2770223319530487, acc: 0.8799999952316284)
[2025-01-06 01:03:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:54][root][INFO] - Training Epoch: 1/10, step 400/574 completed (loss: 0.6974082589149475, acc: 0.8088235259056091)
[2025-01-06 01:03:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:54][root][INFO] - Training Epoch: 1/10, step 401/574 completed (loss: 0.6118826866149902, acc: 0.8399999737739563)
[2025-01-06 01:03:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:54][root][INFO] - Training Epoch: 1/10, step 402/574 completed (loss: 0.6457494497299194, acc: 0.8484848737716675)
[2025-01-06 01:03:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:55][root][INFO] - Training Epoch: 1/10, step 403/574 completed (loss: 0.7245611548423767, acc: 0.8484848737716675)
[2025-01-06 01:03:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:55][root][INFO] - Training Epoch: 1/10, step 404/574 completed (loss: 0.7485826015472412, acc: 0.8064516186714172)
[2025-01-06 01:03:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:56][root][INFO] - Training Epoch: 1/10, step 405/574 completed (loss: 0.30111196637153625, acc: 0.8888888955116272)
[2025-01-06 01:03:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:56][root][INFO] - Training Epoch: 1/10, step 406/574 completed (loss: 0.3989086151123047, acc: 0.9200000166893005)
[2025-01-06 01:03:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:56][root][INFO] - Training Epoch: 1/10, step 407/574 completed (loss: 0.40391266345977783, acc: 0.8888888955116272)
[2025-01-06 01:03:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:57][root][INFO] - Training Epoch: 1/10, step 408/574 completed (loss: 0.4643043279647827, acc: 0.8518518805503845)
[2025-01-06 01:03:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:57][root][INFO] - Training Epoch: 1/10, step 409/574 completed (loss: 0.22214621305465698, acc: 0.9230769276618958)
[2025-01-06 01:03:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:58][root][INFO] - Training Epoch: 1/10, step 410/574 completed (loss: 0.3456108570098877, acc: 0.9137930870056152)
[2025-01-06 01:03:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:58][root][INFO] - Training Epoch: 1/10, step 411/574 completed (loss: 0.23725877702236176, acc: 0.9285714030265808)
[2025-01-06 01:03:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:58][root][INFO] - Training Epoch: 1/10, step 412/574 completed (loss: 0.29249563813209534, acc: 0.9333333373069763)
[2025-01-06 01:03:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:59][root][INFO] - Training Epoch: 1/10, step 413/574 completed (loss: 0.5479106903076172, acc: 0.8484848737716675)
[2025-01-06 01:03:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:59][root][INFO] - Training Epoch: 1/10, step 414/574 completed (loss: 0.3262665867805481, acc: 0.9090909361839294)
[2025-01-06 01:03:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:03:59][root][INFO] - Training Epoch: 1/10, step 415/574 completed (loss: 0.583116352558136, acc: 0.843137264251709)
[2025-01-06 01:04:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:00][root][INFO] - Training Epoch: 1/10, step 416/574 completed (loss: 0.47532960772514343, acc: 0.8461538553237915)
[2025-01-06 01:04:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:00][root][INFO] - Training Epoch: 1/10, step 417/574 completed (loss: 0.4862183928489685, acc: 0.9444444179534912)
[2025-01-06 01:04:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:01][root][INFO] - Training Epoch: 1/10, step 418/574 completed (loss: 0.47190576791763306, acc: 0.8999999761581421)
[2025-01-06 01:04:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:01][root][INFO] - Training Epoch: 1/10, step 419/574 completed (loss: 0.6672390699386597, acc: 0.800000011920929)
[2025-01-06 01:04:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:01][root][INFO] - Training Epoch: 1/10, step 420/574 completed (loss: 0.37585124373435974, acc: 0.9047619104385376)
[2025-01-06 01:04:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:02][root][INFO] - Training Epoch: 1/10, step 421/574 completed (loss: 0.5132392048835754, acc: 0.8666666746139526)
[2025-01-06 01:04:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:02][root][INFO] - Training Epoch: 1/10, step 422/574 completed (loss: 0.8660603165626526, acc: 0.78125)
[2025-01-06 01:04:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:02][root][INFO] - Training Epoch: 1/10, step 423/574 completed (loss: 1.2172640562057495, acc: 0.6666666865348816)
[2025-01-06 01:04:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:03][root][INFO] - Training Epoch: 1/10, step 424/574 completed (loss: 0.6477375626564026, acc: 0.8518518805503845)
[2025-01-06 01:04:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:03][root][INFO] - Training Epoch: 1/10, step 425/574 completed (loss: 0.39961814880371094, acc: 0.939393937587738)
[2025-01-06 01:04:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:04][root][INFO] - Training Epoch: 1/10, step 426/574 completed (loss: 0.3247663080692291, acc: 0.9130434989929199)
[2025-01-06 01:04:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:04][root][INFO] - Training Epoch: 1/10, step 427/574 completed (loss: 0.49659693241119385, acc: 0.837837815284729)
[2025-01-06 01:04:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:04][root][INFO] - Training Epoch: 1/10, step 428/574 completed (loss: 0.40429338812828064, acc: 0.9629629850387573)
[2025-01-06 01:04:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:35][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.9671, device='cuda:0') eval_epoch_loss=tensor(0.6766, device='cuda:0') eval_epoch_acc=tensor(0.8174, device='cuda:0')
[2025-01-06 01:04:35][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:04:35][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:04:35][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_1_step_429_loss_0.6765642166137695/model.pt
[2025-01-06 01:04:35][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:04:35][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.6765642166137695
[2025-01-06 01:04:35][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.8173673748970032
[2025-01-06 01:04:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:36][root][INFO] - Training Epoch: 1/10, step 429/574 completed (loss: 0.3658028542995453, acc: 0.9130434989929199)
[2025-01-06 01:04:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:36][root][INFO] - Training Epoch: 1/10, step 430/574 completed (loss: 0.06116054579615593, acc: 1.0)
[2025-01-06 01:04:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:37][root][INFO] - Training Epoch: 1/10, step 431/574 completed (loss: 0.11992242932319641, acc: 0.9629629850387573)
[2025-01-06 01:04:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:37][root][INFO] - Training Epoch: 1/10, step 432/574 completed (loss: 0.4546505808830261, acc: 0.8260869383811951)
[2025-01-06 01:04:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:37][root][INFO] - Training Epoch: 1/10, step 433/574 completed (loss: 0.47723668813705444, acc: 0.8888888955116272)
[2025-01-06 01:04:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:38][root][INFO] - Training Epoch: 1/10, step 434/574 completed (loss: 0.021968500688672066, acc: 1.0)
[2025-01-06 01:04:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:38][root][INFO] - Training Epoch: 1/10, step 435/574 completed (loss: 0.05711772292852402, acc: 0.9696969985961914)
[2025-01-06 01:04:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:38][root][INFO] - Training Epoch: 1/10, step 436/574 completed (loss: 0.41889750957489014, acc: 0.8611111044883728)
[2025-01-06 01:04:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:39][root][INFO] - Training Epoch: 1/10, step 437/574 completed (loss: 0.10279800742864609, acc: 0.9545454382896423)
[2025-01-06 01:04:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:39][root][INFO] - Training Epoch: 1/10, step 438/574 completed (loss: 0.22673489153385162, acc: 0.9523809552192688)
[2025-01-06 01:04:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:39][root][INFO] - Training Epoch: 1/10, step 439/574 completed (loss: 0.8678648471832275, acc: 0.8205128312110901)
[2025-01-06 01:04:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:40][root][INFO] - Training Epoch: 1/10, step 440/574 completed (loss: 0.6144784092903137, acc: 0.8636363744735718)
[2025-01-06 01:04:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:41][root][INFO] - Training Epoch: 1/10, step 441/574 completed (loss: 0.9506895542144775, acc: 0.7440000176429749)
[2025-01-06 01:04:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:41][root][INFO] - Training Epoch: 1/10, step 442/574 completed (loss: 0.9117076396942139, acc: 0.7419354915618896)
[2025-01-06 01:04:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:42][root][INFO] - Training Epoch: 1/10, step 443/574 completed (loss: 0.5459526181221008, acc: 0.8606964945793152)
[2025-01-06 01:04:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:42][root][INFO] - Training Epoch: 1/10, step 444/574 completed (loss: 0.24805817008018494, acc: 0.8867924809455872)
[2025-01-06 01:04:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:43][root][INFO] - Training Epoch: 1/10, step 445/574 completed (loss: 0.4075601398944855, acc: 0.8636363744735718)
[2025-01-06 01:04:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:43][root][INFO] - Training Epoch: 1/10, step 446/574 completed (loss: 0.9767727255821228, acc: 0.739130437374115)
[2025-01-06 01:04:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:43][root][INFO] - Training Epoch: 1/10, step 447/574 completed (loss: 0.7544057369232178, acc: 0.807692289352417)
[2025-01-06 01:04:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:44][root][INFO] - Training Epoch: 1/10, step 448/574 completed (loss: 0.3137170374393463, acc: 0.8928571343421936)
[2025-01-06 01:04:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:44][root][INFO] - Training Epoch: 1/10, step 449/574 completed (loss: 0.24006490409374237, acc: 0.9701492786407471)
[2025-01-06 01:04:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:44][root][INFO] - Training Epoch: 1/10, step 450/574 completed (loss: 0.17883466184139252, acc: 0.9722222089767456)
[2025-01-06 01:04:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:45][root][INFO] - Training Epoch: 1/10, step 451/574 completed (loss: 0.13756370544433594, acc: 0.945652186870575)
[2025-01-06 01:04:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:45][root][INFO] - Training Epoch: 1/10, step 452/574 completed (loss: 0.38889241218566895, acc: 0.8589743375778198)
[2025-01-06 01:04:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:46][root][INFO] - Training Epoch: 1/10, step 453/574 completed (loss: 0.5080274939537048, acc: 0.8552631735801697)
[2025-01-06 01:04:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:46][root][INFO] - Training Epoch: 1/10, step 454/574 completed (loss: 0.3115530014038086, acc: 0.918367326259613)
[2025-01-06 01:04:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:47][root][INFO] - Training Epoch: 1/10, step 455/574 completed (loss: 0.505463182926178, acc: 0.8484848737716675)
[2025-01-06 01:04:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:47][root][INFO] - Training Epoch: 1/10, step 456/574 completed (loss: 0.8159522414207458, acc: 0.8350515365600586)
[2025-01-06 01:04:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:47][root][INFO] - Training Epoch: 1/10, step 457/574 completed (loss: 0.09703460335731506, acc: 0.9714285731315613)
[2025-01-06 01:04:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:48][root][INFO] - Training Epoch: 1/10, step 458/574 completed (loss: 0.5424477458000183, acc: 0.8779069781303406)
[2025-01-06 01:04:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:48][root][INFO] - Training Epoch: 1/10, step 459/574 completed (loss: 0.12435363233089447, acc: 0.9285714030265808)
[2025-01-06 01:04:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:48][root][INFO] - Training Epoch: 1/10, step 460/574 completed (loss: 0.4258556663990021, acc: 0.9012345671653748)
[2025-01-06 01:04:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:49][root][INFO] - Training Epoch: 1/10, step 461/574 completed (loss: 0.644440770149231, acc: 0.8611111044883728)
[2025-01-06 01:04:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:49][root][INFO] - Training Epoch: 1/10, step 462/574 completed (loss: 0.33825036883354187, acc: 0.875)
[2025-01-06 01:04:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:49][root][INFO] - Training Epoch: 1/10, step 463/574 completed (loss: 0.7706355452537537, acc: 0.8846153616905212)
[2025-01-06 01:04:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:50][root][INFO] - Training Epoch: 1/10, step 464/574 completed (loss: 0.48913922905921936, acc: 0.8260869383811951)
[2025-01-06 01:04:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:50][root][INFO] - Training Epoch: 1/10, step 465/574 completed (loss: 0.5132101774215698, acc: 0.8333333134651184)
[2025-01-06 01:04:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:50][root][INFO] - Training Epoch: 1/10, step 466/574 completed (loss: 0.5932947993278503, acc: 0.8674699068069458)
[2025-01-06 01:04:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:51][root][INFO] - Training Epoch: 1/10, step 467/574 completed (loss: 0.44416871666908264, acc: 0.8828828930854797)
[2025-01-06 01:04:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:51][root][INFO] - Training Epoch: 1/10, step 468/574 completed (loss: 0.927232027053833, acc: 0.7766990065574646)
[2025-01-06 01:04:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:52][root][INFO] - Training Epoch: 1/10, step 469/574 completed (loss: 0.7204468250274658, acc: 0.8130081295967102)
[2025-01-06 01:04:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:52][root][INFO] - Training Epoch: 1/10, step 470/574 completed (loss: 0.40125688910484314, acc: 0.875)
[2025-01-06 01:04:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:52][root][INFO] - Training Epoch: 1/10, step 471/574 completed (loss: 0.8564974665641785, acc: 0.7142857313156128)
[2025-01-06 01:04:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:53][root][INFO] - Training Epoch: 1/10, step 472/574 completed (loss: 1.038540005683899, acc: 0.6764705777168274)
[2025-01-06 01:04:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:53][root][INFO] - Training Epoch: 1/10, step 473/574 completed (loss: 0.883495032787323, acc: 0.7641921639442444)
[2025-01-06 01:04:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:53][root][INFO] - Training Epoch: 1/10, step 474/574 completed (loss: 0.8723722100257874, acc: 0.78125)
[2025-01-06 01:04:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:54][root][INFO] - Training Epoch: 1/10, step 475/574 completed (loss: 0.5445380806922913, acc: 0.8527607321739197)
[2025-01-06 01:04:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:54][root][INFO] - Training Epoch: 1/10, step 476/574 completed (loss: 0.554931640625, acc: 0.8417266011238098)
[2025-01-06 01:04:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:54][root][INFO] - Training Epoch: 1/10, step 477/574 completed (loss: 1.0867276191711426, acc: 0.6783919334411621)
[2025-01-06 01:04:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:55][root][INFO] - Training Epoch: 1/10, step 478/574 completed (loss: 0.9756627678871155, acc: 0.6666666865348816)
[2025-01-06 01:04:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:55][root][INFO] - Training Epoch: 1/10, step 479/574 completed (loss: 0.9773929715156555, acc: 0.7878788113594055)
[2025-01-06 01:04:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:56][root][INFO] - Training Epoch: 1/10, step 480/574 completed (loss: 0.9156703352928162, acc: 0.8148148059844971)
[2025-01-06 01:04:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:56][root][INFO] - Training Epoch: 1/10, step 481/574 completed (loss: 0.7452800869941711, acc: 0.75)
[2025-01-06 01:04:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:56][root][INFO] - Training Epoch: 1/10, step 482/574 completed (loss: 1.654907464981079, acc: 0.5)
[2025-01-06 01:04:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:57][root][INFO] - Training Epoch: 1/10, step 483/574 completed (loss: 1.1997735500335693, acc: 0.6206896305084229)
[2025-01-06 01:04:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:57][root][INFO] - Training Epoch: 1/10, step 484/574 completed (loss: 0.4169554114341736, acc: 0.9032257795333862)
[2025-01-06 01:04:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:57][root][INFO] - Training Epoch: 1/10, step 485/574 completed (loss: 1.0760122537612915, acc: 0.7368420958518982)
[2025-01-06 01:04:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:58][root][INFO] - Training Epoch: 1/10, step 486/574 completed (loss: 1.6697596311569214, acc: 0.5925925970077515)
[2025-01-06 01:04:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:58][root][INFO] - Training Epoch: 1/10, step 487/574 completed (loss: 0.8864482045173645, acc: 0.761904776096344)
[2025-01-06 01:04:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:58][root][INFO] - Training Epoch: 1/10, step 488/574 completed (loss: 1.0888912677764893, acc: 0.8181818127632141)
[2025-01-06 01:04:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:59][root][INFO] - Training Epoch: 1/10, step 489/574 completed (loss: 1.2934883832931519, acc: 0.6615384817123413)
[2025-01-06 01:04:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:04:59][root][INFO] - Training Epoch: 1/10, step 490/574 completed (loss: 0.7877187132835388, acc: 0.8333333134651184)
[2025-01-06 01:04:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:00][root][INFO] - Training Epoch: 1/10, step 491/574 completed (loss: 1.1029647588729858, acc: 0.7241379022598267)
[2025-01-06 01:05:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:00][root][INFO] - Training Epoch: 1/10, step 492/574 completed (loss: 0.6794244647026062, acc: 0.7843137383460999)
[2025-01-06 01:05:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:00][root][INFO] - Training Epoch: 1/10, step 493/574 completed (loss: 0.6318362951278687, acc: 0.7241379022598267)
[2025-01-06 01:05:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:01][root][INFO] - Training Epoch: 1/10, step 494/574 completed (loss: 0.8278921842575073, acc: 0.7368420958518982)
[2025-01-06 01:05:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:01][root][INFO] - Training Epoch: 1/10, step 495/574 completed (loss: 1.1137114763259888, acc: 0.7368420958518982)
[2025-01-06 01:05:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:01][root][INFO] - Training Epoch: 1/10, step 496/574 completed (loss: 0.7488706707954407, acc: 0.8035714030265808)
[2025-01-06 01:05:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:02][root][INFO] - Training Epoch: 1/10, step 497/574 completed (loss: 0.5937235951423645, acc: 0.8426966071128845)
[2025-01-06 01:05:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:02][root][INFO] - Training Epoch: 1/10, step 498/574 completed (loss: 0.9788896441459656, acc: 0.7191011309623718)
[2025-01-06 01:05:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:03][root][INFO] - Training Epoch: 1/10, step 499/574 completed (loss: 1.3724110126495361, acc: 0.5957446694374084)
[2025-01-06 01:05:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:03][root][INFO] - Training Epoch: 1/10, step 500/574 completed (loss: 1.0241119861602783, acc: 0.695652186870575)
[2025-01-06 01:05:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:03][root][INFO] - Training Epoch: 1/10, step 501/574 completed (loss: 0.09170521795749664, acc: 1.0)
[2025-01-06 01:05:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:04][root][INFO] - Training Epoch: 1/10, step 502/574 completed (loss: 0.3901512920856476, acc: 0.9230769276618958)
[2025-01-06 01:05:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:04][root][INFO] - Training Epoch: 1/10, step 503/574 completed (loss: 0.5045039057731628, acc: 0.8518518805503845)
[2025-01-06 01:05:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:04][root][INFO] - Training Epoch: 1/10, step 504/574 completed (loss: 0.5534527897834778, acc: 0.7777777910232544)
[2025-01-06 01:05:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:05][root][INFO] - Training Epoch: 1/10, step 505/574 completed (loss: 0.8159187436103821, acc: 0.8301886916160583)
[2025-01-06 01:05:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:05][root][INFO] - Training Epoch: 1/10, step 506/574 completed (loss: 0.730451226234436, acc: 0.7931034564971924)
[2025-01-06 01:05:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:06][root][INFO] - Training Epoch: 1/10, step 507/574 completed (loss: 1.268875241279602, acc: 0.6576576828956604)
[2025-01-06 01:05:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:06][root][INFO] - Training Epoch: 1/10, step 508/574 completed (loss: 0.9963908791542053, acc: 0.7464788556098938)
[2025-01-06 01:05:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:07][root][INFO] - Training Epoch: 1/10, step 509/574 completed (loss: 0.32577764987945557, acc: 0.8500000238418579)
[2025-01-06 01:05:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:07][root][INFO] - Training Epoch: 1/10, step 510/574 completed (loss: 0.578191876411438, acc: 0.800000011920929)
[2025-01-06 01:05:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:07][root][INFO] - Training Epoch: 1/10, step 511/574 completed (loss: 0.9120505452156067, acc: 0.7692307829856873)
[2025-01-06 01:05:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:10][root][INFO] - Training Epoch: 1/10, step 512/574 completed (loss: 1.4155980348587036, acc: 0.6428571343421936)
[2025-01-06 01:05:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:11][root][INFO] - Training Epoch: 1/10, step 513/574 completed (loss: 0.5361999273300171, acc: 0.841269850730896)
[2025-01-06 01:05:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:11][root][INFO] - Training Epoch: 1/10, step 514/574 completed (loss: 0.7895970940589905, acc: 0.75)
[2025-01-06 01:05:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:12][root][INFO] - Training Epoch: 1/10, step 515/574 completed (loss: 0.24980181455612183, acc: 0.9333333373069763)
[2025-01-06 01:05:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:12][root][INFO] - Training Epoch: 1/10, step 516/574 completed (loss: 0.8449305295944214, acc: 0.7638888955116272)
[2025-01-06 01:05:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:13][root][INFO] - Training Epoch: 1/10, step 517/574 completed (loss: 0.020985933020710945, acc: 1.0)
[2025-01-06 01:05:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:13][root][INFO] - Training Epoch: 1/10, step 518/574 completed (loss: 0.285753458738327, acc: 0.9032257795333862)
[2025-01-06 01:05:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:13][root][INFO] - Training Epoch: 1/10, step 519/574 completed (loss: 0.720779538154602, acc: 0.75)
[2025-01-06 01:05:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:14][root][INFO] - Training Epoch: 1/10, step 520/574 completed (loss: 0.6868394017219543, acc: 0.7407407164573669)
[2025-01-06 01:05:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:15][root][INFO] - Training Epoch: 1/10, step 521/574 completed (loss: 0.7544456720352173, acc: 0.7711864113807678)
[2025-01-06 01:05:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:15][root][INFO] - Training Epoch: 1/10, step 522/574 completed (loss: 0.3670450448989868, acc: 0.888059675693512)
[2025-01-06 01:05:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:16][root][INFO] - Training Epoch: 1/10, step 523/574 completed (loss: 0.4992264211177826, acc: 0.8540145754814148)
[2025-01-06 01:05:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:16][root][INFO] - Training Epoch: 1/10, step 524/574 completed (loss: 0.7697305083274841, acc: 0.800000011920929)
[2025-01-06 01:05:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:16][root][INFO] - Training Epoch: 1/10, step 525/574 completed (loss: 0.1681254357099533, acc: 0.9629629850387573)
[2025-01-06 01:05:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:17][root][INFO] - Training Epoch: 1/10, step 526/574 completed (loss: 0.3108213245868683, acc: 0.9230769276618958)
[2025-01-06 01:05:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:17][root][INFO] - Training Epoch: 1/10, step 527/574 completed (loss: 1.0260882377624512, acc: 0.7142857313156128)
[2025-01-06 01:05:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:17][root][INFO] - Training Epoch: 1/10, step 528/574 completed (loss: 2.161731004714966, acc: 0.4590163826942444)
[2025-01-06 01:05:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:18][root][INFO] - Training Epoch: 1/10, step 529/574 completed (loss: 0.5015478730201721, acc: 0.8305084705352783)
[2025-01-06 01:05:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:18][root][INFO] - Training Epoch: 1/10, step 530/574 completed (loss: 1.4623866081237793, acc: 0.6279069781303406)
[2025-01-06 01:05:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:19][root][INFO] - Training Epoch: 1/10, step 531/574 completed (loss: 1.2591017484664917, acc: 0.6818181872367859)
[2025-01-06 01:05:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:19][root][INFO] - Training Epoch: 1/10, step 532/574 completed (loss: 1.2514292001724243, acc: 0.6226415038108826)
[2025-01-06 01:05:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:19][root][INFO] - Training Epoch: 1/10, step 533/574 completed (loss: 1.0817354917526245, acc: 0.6818181872367859)
[2025-01-06 01:05:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:20][root][INFO] - Training Epoch: 1/10, step 534/574 completed (loss: 0.8553323149681091, acc: 0.6399999856948853)
[2025-01-06 01:05:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:20][root][INFO] - Training Epoch: 1/10, step 535/574 completed (loss: 0.8430142402648926, acc: 0.800000011920929)
[2025-01-06 01:05:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:20][root][INFO] - Training Epoch: 1/10, step 536/574 completed (loss: 0.4091193377971649, acc: 0.9090909361839294)
[2025-01-06 01:05:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:21][root][INFO] - Training Epoch: 1/10, step 537/574 completed (loss: 0.8808736205101013, acc: 0.7538461685180664)
[2025-01-06 01:05:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:21][root][INFO] - Training Epoch: 1/10, step 538/574 completed (loss: 0.8204123973846436, acc: 0.703125)
[2025-01-06 01:05:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:22][root][INFO] - Training Epoch: 1/10, step 539/574 completed (loss: 0.7529906630516052, acc: 0.75)
[2025-01-06 01:05:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:22][root][INFO] - Training Epoch: 1/10, step 540/574 completed (loss: 0.9976338744163513, acc: 0.6666666865348816)
[2025-01-06 01:05:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:22][root][INFO] - Training Epoch: 1/10, step 541/574 completed (loss: 0.7829928994178772, acc: 0.6875)
[2025-01-06 01:05:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:23][root][INFO] - Training Epoch: 1/10, step 542/574 completed (loss: 0.22586704790592194, acc: 0.9032257795333862)
[2025-01-06 01:05:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:23][root][INFO] - Training Epoch: 1/10, step 543/574 completed (loss: 0.13436445593833923, acc: 0.95652174949646)
[2025-01-06 01:05:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:23][root][INFO] - Training Epoch: 1/10, step 544/574 completed (loss: 0.40904441475868225, acc: 0.8666666746139526)
[2025-01-06 01:05:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:24][root][INFO] - Training Epoch: 1/10, step 545/574 completed (loss: 0.1952754259109497, acc: 0.9756097793579102)
[2025-01-06 01:05:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:24][root][INFO] - Training Epoch: 1/10, step 546/574 completed (loss: 0.04242555797100067, acc: 1.0)
[2025-01-06 01:05:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:25][root][INFO] - Training Epoch: 1/10, step 547/574 completed (loss: 0.18120919167995453, acc: 0.9210526347160339)
[2025-01-06 01:05:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:25][root][INFO] - Training Epoch: 1/10, step 548/574 completed (loss: 0.4153086841106415, acc: 0.9354838728904724)
[2025-01-06 01:05:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:25][root][INFO] - Training Epoch: 1/10, step 549/574 completed (loss: 0.029889078810811043, acc: 1.0)
[2025-01-06 01:05:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:25][root][INFO] - Training Epoch: 1/10, step 550/574 completed (loss: 0.5840049982070923, acc: 0.8181818127632141)
[2025-01-06 01:05:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:26][root][INFO] - Training Epoch: 1/10, step 551/574 completed (loss: 0.23057611286640167, acc: 0.8999999761581421)
[2025-01-06 01:05:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:26][root][INFO] - Training Epoch: 1/10, step 552/574 completed (loss: 0.32205086946487427, acc: 0.9285714030265808)
[2025-01-06 01:05:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:27][root][INFO] - Training Epoch: 1/10, step 553/574 completed (loss: 0.5501851439476013, acc: 0.8540145754814148)
[2025-01-06 01:05:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:27][root][INFO] - Training Epoch: 1/10, step 554/574 completed (loss: 0.3995909094810486, acc: 0.8896551728248596)
[2025-01-06 01:05:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:27][root][INFO] - Training Epoch: 1/10, step 555/574 completed (loss: 0.4819357693195343, acc: 0.8785714507102966)
[2025-01-06 01:05:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:28][root][INFO] - Training Epoch: 1/10, step 556/574 completed (loss: 0.5657161474227905, acc: 0.860927164554596)
[2025-01-06 01:05:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:28][root][INFO] - Training Epoch: 1/10, step 557/574 completed (loss: 0.4444589912891388, acc: 0.8803418874740601)
[2025-01-06 01:05:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:28][root][INFO] - Training Epoch: 1/10, step 558/574 completed (loss: 0.37140074372291565, acc: 0.9200000166893005)
[2025-01-06 01:05:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:29][root][INFO] - Training Epoch: 1/10, step 559/574 completed (loss: 0.5242212414741516, acc: 0.9230769276618958)
[2025-01-06 01:05:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:29][root][INFO] - Training Epoch: 1/10, step 560/574 completed (loss: 0.19299966096878052, acc: 0.9230769276618958)
[2025-01-06 01:05:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:29][root][INFO] - Training Epoch: 1/10, step 561/574 completed (loss: 0.22862288355827332, acc: 0.9487179517745972)
[2025-01-06 01:05:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:30][root][INFO] - Training Epoch: 1/10, step 562/574 completed (loss: 0.5631411075592041, acc: 0.8666666746139526)
[2025-01-06 01:05:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:30][root][INFO] - Training Epoch: 1/10, step 563/574 completed (loss: 0.4802827835083008, acc: 0.8831169009208679)
[2025-01-06 01:05:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:31][root][INFO] - Training Epoch: 1/10, step 564/574 completed (loss: 0.6044090986251831, acc: 0.7708333134651184)
[2025-01-06 01:05:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:31][root][INFO] - Training Epoch: 1/10, step 565/574 completed (loss: 0.32538947463035583, acc: 0.8793103694915771)
[2025-01-06 01:05:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:31][root][INFO] - Training Epoch: 1/10, step 566/574 completed (loss: 0.46807920932769775, acc: 0.9047619104385376)
[2025-01-06 01:05:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:32][root][INFO] - Training Epoch: 1/10, step 567/574 completed (loss: 0.08282721042633057, acc: 0.9736841917037964)
[2025-01-06 01:05:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:32][root][INFO] - Training Epoch: 1/10, step 568/574 completed (loss: 0.14750856161117554, acc: 0.9629629850387573)
[2025-01-06 01:05:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:32][root][INFO] - Training Epoch: 1/10, step 569/574 completed (loss: 0.2587588429450989, acc: 0.9358288645744324)
[2025-01-06 01:05:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:33][root][INFO] - Training Epoch: 1/10, step 570/574 completed (loss: 0.022803906351327896, acc: 1.0)
[2025-01-06 01:05:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:33][root][INFO] - Training Epoch: 1/10, step 571/574 completed (loss: 0.2815634608268738, acc: 0.9145299196243286)
[2025-01-06 01:05:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:05:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:03][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.8495, device='cuda:0') eval_epoch_loss=tensor(0.6149, device='cuda:0') eval_epoch_acc=tensor(0.8259, device='cuda:0')
[2025-01-06 01:06:03][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:06:03][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:06:04][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_1_step_572_loss_0.6149211525917053/model.pt
[2025-01-06 01:06:04][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:06:04][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.6149211525917053
[2025-01-06 01:06:04][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.8259221911430359
[2025-01-06 01:06:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:04][root][INFO] - Training Epoch: 1/10, step 572/574 completed (loss: 0.4742846190929413, acc: 0.8571428656578064)
[2025-01-06 01:06:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:04][root][INFO] - Training Epoch: 1/10, step 573/574 completed (loss: 0.5229641199111938, acc: 0.8742138147354126)
[2025-01-06 01:06:05][slam_llm.utils.train_utils][INFO] - Epoch 1: train_perplexity=2.9198, train_epoch_loss=1.0715, epoch time 371.9666225835681s
[2025-01-06 01:06:05][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 12 GB
[2025-01-06 01:06:05][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 15 GB
[2025-01-06 01:06:05][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 12 GB
[2025-01-06 01:06:05][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 3
[2025-01-06 01:06:05][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2025-01-06 01:06:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:06][root][INFO] - Training Epoch: 2/10, step 0/574 completed (loss: 0.7208008766174316, acc: 0.8148148059844971)
[2025-01-06 01:06:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:06][root][INFO] - Training Epoch: 2/10, step 1/574 completed (loss: 0.6486573815345764, acc: 0.800000011920929)
[2025-01-06 01:06:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:06][root][INFO] - Training Epoch: 2/10, step 2/574 completed (loss: 1.1583443880081177, acc: 0.7297297120094299)
[2025-01-06 01:06:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:07][root][INFO] - Training Epoch: 2/10, step 3/574 completed (loss: 0.7375479340553284, acc: 0.8157894611358643)
[2025-01-06 01:06:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:07][root][INFO] - Training Epoch: 2/10, step 4/574 completed (loss: 0.9777830839157104, acc: 0.7567567825317383)
[2025-01-06 01:06:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:07][root][INFO] - Training Epoch: 2/10, step 5/574 completed (loss: 0.42351865768432617, acc: 0.7857142686843872)
[2025-01-06 01:06:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:08][root][INFO] - Training Epoch: 2/10, step 6/574 completed (loss: 1.0758931636810303, acc: 0.6326530575752258)
[2025-01-06 01:06:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:08][root][INFO] - Training Epoch: 2/10, step 7/574 completed (loss: 0.6642690300941467, acc: 0.8333333134651184)
[2025-01-06 01:06:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:08][root][INFO] - Training Epoch: 2/10, step 8/574 completed (loss: 0.2705538272857666, acc: 0.8636363744735718)
[2025-01-06 01:06:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:09][root][INFO] - Training Epoch: 2/10, step 9/574 completed (loss: 0.21263140439987183, acc: 0.9230769276618958)
[2025-01-06 01:06:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:09][root][INFO] - Training Epoch: 2/10, step 10/574 completed (loss: 0.39379459619522095, acc: 0.8888888955116272)
[2025-01-06 01:06:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:09][root][INFO] - Training Epoch: 2/10, step 11/574 completed (loss: 0.5820255875587463, acc: 0.7948718070983887)
[2025-01-06 01:06:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:10][root][INFO] - Training Epoch: 2/10, step 12/574 completed (loss: 0.21597599983215332, acc: 0.9090909361839294)
[2025-01-06 01:06:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:10][root][INFO] - Training Epoch: 2/10, step 13/574 completed (loss: 0.46526774764060974, acc: 0.8695651888847351)
[2025-01-06 01:06:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:11][root][INFO] - Training Epoch: 2/10, step 14/574 completed (loss: 0.2789430022239685, acc: 0.9411764740943909)
[2025-01-06 01:06:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:11][root][INFO] - Training Epoch: 2/10, step 15/574 completed (loss: 0.4552028775215149, acc: 0.8979591727256775)
[2025-01-06 01:06:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:11][root][INFO] - Training Epoch: 2/10, step 16/574 completed (loss: 0.38318929076194763, acc: 0.8947368264198303)
[2025-01-06 01:06:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:12][root][INFO] - Training Epoch: 2/10, step 17/574 completed (loss: 0.7053587436676025, acc: 0.7916666865348816)
[2025-01-06 01:06:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:12][root][INFO] - Training Epoch: 2/10, step 18/574 completed (loss: 0.7032404541969299, acc: 0.75)
[2025-01-06 01:06:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:12][root][INFO] - Training Epoch: 2/10, step 19/574 completed (loss: 0.6624778509140015, acc: 0.7894737124443054)
[2025-01-06 01:06:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:13][root][INFO] - Training Epoch: 2/10, step 20/574 completed (loss: 0.4520837068557739, acc: 0.807692289352417)
[2025-01-06 01:06:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:13][root][INFO] - Training Epoch: 2/10, step 21/574 completed (loss: 0.931245744228363, acc: 0.8275862336158752)
[2025-01-06 01:06:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:13][root][INFO] - Training Epoch: 2/10, step 22/574 completed (loss: 1.107893705368042, acc: 0.6399999856948853)
[2025-01-06 01:06:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:14][root][INFO] - Training Epoch: 2/10, step 23/574 completed (loss: 0.9914445281028748, acc: 0.8095238208770752)
[2025-01-06 01:06:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:14][root][INFO] - Training Epoch: 2/10, step 24/574 completed (loss: 0.22374595701694489, acc: 1.0)
[2025-01-06 01:06:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:14][root][INFO] - Training Epoch: 2/10, step 25/574 completed (loss: 0.7920220494270325, acc: 0.7735849022865295)
[2025-01-06 01:06:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:15][root][INFO] - Training Epoch: 2/10, step 26/574 completed (loss: 0.9703331589698792, acc: 0.7260273694992065)
[2025-01-06 01:06:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:16][root][INFO] - Training Epoch: 2/10, step 27/574 completed (loss: 1.2191190719604492, acc: 0.6798418760299683)
[2025-01-06 01:06:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:16][root][INFO] - Training Epoch: 2/10, step 28/574 completed (loss: 0.5640444159507751, acc: 0.7906976938247681)
[2025-01-06 01:06:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:17][root][INFO] - Training Epoch: 2/10, step 29/574 completed (loss: 0.7830657958984375, acc: 0.7951807379722595)
[2025-01-06 01:06:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:17][root][INFO] - Training Epoch: 2/10, step 30/574 completed (loss: 0.7488556504249573, acc: 0.7777777910232544)
[2025-01-06 01:06:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:17][root][INFO] - Training Epoch: 2/10, step 31/574 completed (loss: 0.7940849661827087, acc: 0.75)
[2025-01-06 01:06:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:18][root][INFO] - Training Epoch: 2/10, step 32/574 completed (loss: 0.5730765461921692, acc: 0.8148148059844971)
[2025-01-06 01:06:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:18][root][INFO] - Training Epoch: 2/10, step 33/574 completed (loss: 0.2982548773288727, acc: 0.9130434989929199)
[2025-01-06 01:06:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:18][root][INFO] - Training Epoch: 2/10, step 34/574 completed (loss: 0.7088443040847778, acc: 0.7731092572212219)
[2025-01-06 01:06:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:19][root][INFO] - Training Epoch: 2/10, step 35/574 completed (loss: 0.4227296710014343, acc: 0.868852436542511)
[2025-01-06 01:06:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:19][root][INFO] - Training Epoch: 2/10, step 36/574 completed (loss: 0.6198167204856873, acc: 0.8253968358039856)
[2025-01-06 01:06:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:20][root][INFO] - Training Epoch: 2/10, step 37/574 completed (loss: 0.6703565716743469, acc: 0.8305084705352783)
[2025-01-06 01:06:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:20][root][INFO] - Training Epoch: 2/10, step 38/574 completed (loss: 0.5116943120956421, acc: 0.8735632300376892)
[2025-01-06 01:06:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:20][root][INFO] - Training Epoch: 2/10, step 39/574 completed (loss: 0.6663711071014404, acc: 0.761904776096344)
[2025-01-06 01:06:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:21][root][INFO] - Training Epoch: 2/10, step 40/574 completed (loss: 0.6128401160240173, acc: 0.8461538553237915)
[2025-01-06 01:06:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:21][root][INFO] - Training Epoch: 2/10, step 41/574 completed (loss: 0.4262574315071106, acc: 0.8918918967247009)
[2025-01-06 01:06:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:21][root][INFO] - Training Epoch: 2/10, step 42/574 completed (loss: 0.6774780750274658, acc: 0.8153846263885498)
[2025-01-06 01:06:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:22][root][INFO] - Training Epoch: 2/10, step 43/574 completed (loss: 0.7154717445373535, acc: 0.8787878751754761)
[2025-01-06 01:06:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:22][root][INFO] - Training Epoch: 2/10, step 44/574 completed (loss: 0.5029227137565613, acc: 0.8556700944900513)
[2025-01-06 01:06:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:23][root][INFO] - Training Epoch: 2/10, step 45/574 completed (loss: 0.5411576628684998, acc: 0.8308823704719543)
[2025-01-06 01:06:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:23][root][INFO] - Training Epoch: 2/10, step 46/574 completed (loss: 0.47124770283699036, acc: 0.7307692170143127)
[2025-01-06 01:06:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:23][root][INFO] - Training Epoch: 2/10, step 47/574 completed (loss: 0.45721790194511414, acc: 0.9259259104728699)
[2025-01-06 01:06:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:24][root][INFO] - Training Epoch: 2/10, step 48/574 completed (loss: 0.46174582839012146, acc: 0.8928571343421936)
[2025-01-06 01:06:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:24][root][INFO] - Training Epoch: 2/10, step 49/574 completed (loss: 0.11038201302289963, acc: 1.0)
[2025-01-06 01:06:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:24][root][INFO] - Training Epoch: 2/10, step 50/574 completed (loss: 0.9210165143013, acc: 0.7543859481811523)
[2025-01-06 01:06:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:25][root][INFO] - Training Epoch: 2/10, step 51/574 completed (loss: 0.8519516587257385, acc: 0.7460317611694336)
[2025-01-06 01:06:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:25][root][INFO] - Training Epoch: 2/10, step 52/574 completed (loss: 1.2543967962265015, acc: 0.7183098793029785)
[2025-01-06 01:06:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:25][root][INFO] - Training Epoch: 2/10, step 53/574 completed (loss: 1.5136117935180664, acc: 0.5400000214576721)
[2025-01-06 01:06:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:26][root][INFO] - Training Epoch: 2/10, step 54/574 completed (loss: 1.2429218292236328, acc: 0.5945945978164673)
[2025-01-06 01:06:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:26][root][INFO] - Training Epoch: 2/10, step 55/574 completed (loss: 0.171199768781662, acc: 0.9615384340286255)
[2025-01-06 01:06:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:29][root][INFO] - Training Epoch: 2/10, step 56/574 completed (loss: 1.333075761795044, acc: 0.658703088760376)
[2025-01-06 01:06:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:30][root][INFO] - Training Epoch: 2/10, step 57/574 completed (loss: 1.3744534254074097, acc: 0.6296296119689941)
[2025-01-06 01:06:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:31][root][INFO] - Training Epoch: 2/10, step 58/574 completed (loss: 0.958892822265625, acc: 0.6931818127632141)
[2025-01-06 01:06:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:32][root][INFO] - Training Epoch: 2/10, step 59/574 completed (loss: 0.3630378544330597, acc: 0.8897058963775635)
[2025-01-06 01:06:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:32][root][INFO] - Training Epoch: 2/10, step 60/574 completed (loss: 0.9544416666030884, acc: 0.7028985619544983)
[2025-01-06 01:06:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:33][root][INFO] - Training Epoch: 2/10, step 61/574 completed (loss: 0.6780011653900146, acc: 0.8125)
[2025-01-06 01:06:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:33][root][INFO] - Training Epoch: 2/10, step 62/574 completed (loss: 0.6941309571266174, acc: 0.7941176295280457)
[2025-01-06 01:06:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:33][root][INFO] - Training Epoch: 2/10, step 63/574 completed (loss: 0.5019770264625549, acc: 0.8611111044883728)
[2025-01-06 01:06:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:34][root][INFO] - Training Epoch: 2/10, step 64/574 completed (loss: 0.3379836678504944, acc: 0.890625)
[2025-01-06 01:06:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:34][root][INFO] - Training Epoch: 2/10, step 65/574 completed (loss: 0.19241288304328918, acc: 0.8965517282485962)
[2025-01-06 01:06:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:34][root][INFO] - Training Epoch: 2/10, step 66/574 completed (loss: 1.0396617650985718, acc: 0.8035714030265808)
[2025-01-06 01:06:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:35][root][INFO] - Training Epoch: 2/10, step 67/574 completed (loss: 0.7483240962028503, acc: 0.7666666507720947)
[2025-01-06 01:06:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:35][root][INFO] - Training Epoch: 2/10, step 68/574 completed (loss: 0.1256123185157776, acc: 0.9599999785423279)
[2025-01-06 01:06:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:36][root][INFO] - Training Epoch: 2/10, step 69/574 completed (loss: 0.8254468441009521, acc: 0.8333333134651184)
[2025-01-06 01:06:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:36][root][INFO] - Training Epoch: 2/10, step 70/574 completed (loss: 1.1265501976013184, acc: 0.7272727489471436)
[2025-01-06 01:06:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:36][root][INFO] - Training Epoch: 2/10, step 71/574 completed (loss: 1.0208508968353271, acc: 0.6764705777168274)
[2025-01-06 01:06:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:37][root][INFO] - Training Epoch: 2/10, step 72/574 completed (loss: 0.8443939089775085, acc: 0.761904776096344)
[2025-01-06 01:06:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:37][root][INFO] - Training Epoch: 2/10, step 73/574 completed (loss: 1.2488926649093628, acc: 0.656410276889801)
[2025-01-06 01:06:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:37][root][INFO] - Training Epoch: 2/10, step 74/574 completed (loss: 1.2419986724853516, acc: 0.6836734414100647)
[2025-01-06 01:06:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:38][root][INFO] - Training Epoch: 2/10, step 75/574 completed (loss: 1.3080780506134033, acc: 0.6492537260055542)
[2025-01-06 01:06:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:38][root][INFO] - Training Epoch: 2/10, step 76/574 completed (loss: 1.4568694829940796, acc: 0.5948905348777771)
[2025-01-06 01:06:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:38][root][INFO] - Training Epoch: 2/10, step 77/574 completed (loss: 0.12921856343746185, acc: 0.9523809552192688)
[2025-01-06 01:06:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:39][root][INFO] - Training Epoch: 2/10, step 78/574 completed (loss: 0.3656540811061859, acc: 0.9166666865348816)
[2025-01-06 01:06:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:39][root][INFO] - Training Epoch: 2/10, step 79/574 completed (loss: 0.15819032490253448, acc: 1.0)
[2025-01-06 01:06:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:39][root][INFO] - Training Epoch: 2/10, step 80/574 completed (loss: 0.519710898399353, acc: 0.8461538553237915)
[2025-01-06 01:06:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:40][root][INFO] - Training Epoch: 2/10, step 81/574 completed (loss: 0.8100433945655823, acc: 0.7692307829856873)
[2025-01-06 01:06:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:40][root][INFO] - Training Epoch: 2/10, step 82/574 completed (loss: 0.9017245173454285, acc: 0.7884615659713745)
[2025-01-06 01:06:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:41][root][INFO] - Training Epoch: 2/10, step 83/574 completed (loss: 0.40031886100769043, acc: 0.9375)
[2025-01-06 01:06:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:41][root][INFO] - Training Epoch: 2/10, step 84/574 completed (loss: 0.5862709879875183, acc: 0.8405796885490417)
[2025-01-06 01:06:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:41][root][INFO] - Training Epoch: 2/10, step 85/574 completed (loss: 0.7837892770767212, acc: 0.7799999713897705)
[2025-01-06 01:06:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:42][root][INFO] - Training Epoch: 2/10, step 86/574 completed (loss: 0.6702028512954712, acc: 0.8695651888847351)
[2025-01-06 01:06:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:42][root][INFO] - Training Epoch: 2/10, step 87/574 completed (loss: 0.9467266798019409, acc: 0.6800000071525574)
[2025-01-06 01:06:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:43][root][INFO] - Training Epoch: 2/10, step 88/574 completed (loss: 0.792012095451355, acc: 0.7864077687263489)
[2025-01-06 01:06:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:44][root][INFO] - Training Epoch: 2/10, step 89/574 completed (loss: 0.9849845170974731, acc: 0.762135922908783)
[2025-01-06 01:06:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:44][root][INFO] - Training Epoch: 2/10, step 90/574 completed (loss: 1.1861587762832642, acc: 0.6881720423698425)
[2025-01-06 01:06:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:45][root][INFO] - Training Epoch: 2/10, step 91/574 completed (loss: 1.124176025390625, acc: 0.6896551847457886)
[2025-01-06 01:06:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:46][root][INFO] - Training Epoch: 2/10, step 92/574 completed (loss: 0.7182541489601135, acc: 0.7684210538864136)
[2025-01-06 01:06:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:47][root][INFO] - Training Epoch: 2/10, step 93/574 completed (loss: 1.6024290323257446, acc: 0.5247524976730347)
[2025-01-06 01:06:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:47][root][INFO] - Training Epoch: 2/10, step 94/574 completed (loss: 1.2008962631225586, acc: 0.6612903475761414)
[2025-01-06 01:06:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:48][root][INFO] - Training Epoch: 2/10, step 95/574 completed (loss: 0.8567084670066833, acc: 0.739130437374115)
[2025-01-06 01:06:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:48][root][INFO] - Training Epoch: 2/10, step 96/574 completed (loss: 1.226864218711853, acc: 0.6638655662536621)
[2025-01-06 01:06:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:48][root][INFO] - Training Epoch: 2/10, step 97/574 completed (loss: 1.2365658283233643, acc: 0.6730769276618958)
[2025-01-06 01:06:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:49][root][INFO] - Training Epoch: 2/10, step 98/574 completed (loss: 1.2611770629882812, acc: 0.6496350169181824)
[2025-01-06 01:06:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:49][root][INFO] - Training Epoch: 2/10, step 99/574 completed (loss: 1.7294172048568726, acc: 0.5373134613037109)
[2025-01-06 01:06:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:49][root][INFO] - Training Epoch: 2/10, step 100/574 completed (loss: 0.6444767117500305, acc: 0.8500000238418579)
[2025-01-06 01:06:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:50][root][INFO] - Training Epoch: 2/10, step 101/574 completed (loss: 0.04562711343169212, acc: 1.0)
[2025-01-06 01:06:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:50][root][INFO] - Training Epoch: 2/10, step 102/574 completed (loss: 0.19107136130332947, acc: 0.95652174949646)
[2025-01-06 01:06:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:50][root][INFO] - Training Epoch: 2/10, step 103/574 completed (loss: 0.15379564464092255, acc: 0.9545454382896423)
[2025-01-06 01:06:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:51][root][INFO] - Training Epoch: 2/10, step 104/574 completed (loss: 0.586373507976532, acc: 0.8620689511299133)
[2025-01-06 01:06:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:51][root][INFO] - Training Epoch: 2/10, step 105/574 completed (loss: 0.41103410720825195, acc: 0.8604651093482971)
[2025-01-06 01:06:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:51][root][INFO] - Training Epoch: 2/10, step 106/574 completed (loss: 0.3832707107067108, acc: 0.800000011920929)
[2025-01-06 01:06:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:52][root][INFO] - Training Epoch: 2/10, step 107/574 completed (loss: 0.047014642506837845, acc: 1.0)
[2025-01-06 01:06:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:52][root][INFO] - Training Epoch: 2/10, step 108/574 completed (loss: 0.08774011582136154, acc: 0.9615384340286255)
[2025-01-06 01:06:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:52][root][INFO] - Training Epoch: 2/10, step 109/574 completed (loss: 0.1532578021287918, acc: 0.9047619104385376)
[2025-01-06 01:06:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:53][root][INFO] - Training Epoch: 2/10, step 110/574 completed (loss: 0.19310109317302704, acc: 0.9230769276618958)
[2025-01-06 01:06:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:53][root][INFO] - Training Epoch: 2/10, step 111/574 completed (loss: 0.45355817675590515, acc: 0.8070175647735596)
[2025-01-06 01:06:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:54][root][INFO] - Training Epoch: 2/10, step 112/574 completed (loss: 0.8395028114318848, acc: 0.7894737124443054)
[2025-01-06 01:06:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:54][root][INFO] - Training Epoch: 2/10, step 113/574 completed (loss: 0.5369480848312378, acc: 0.8461538553237915)
[2025-01-06 01:06:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:54][root][INFO] - Training Epoch: 2/10, step 114/574 completed (loss: 0.43218475580215454, acc: 0.8979591727256775)
[2025-01-06 01:06:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:55][root][INFO] - Training Epoch: 2/10, step 115/574 completed (loss: 0.07857315987348557, acc: 1.0)
[2025-01-06 01:06:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:55][root][INFO] - Training Epoch: 2/10, step 116/574 completed (loss: 0.7473755478858948, acc: 0.8253968358039856)
[2025-01-06 01:06:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:56][root][INFO] - Training Epoch: 2/10, step 117/574 completed (loss: 0.4034920930862427, acc: 0.9024389982223511)
[2025-01-06 01:06:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:56][root][INFO] - Training Epoch: 2/10, step 118/574 completed (loss: 0.2575814723968506, acc: 0.9354838728904724)
[2025-01-06 01:06:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:57][root][INFO] - Training Epoch: 2/10, step 119/574 completed (loss: 0.6026501655578613, acc: 0.8441064357757568)
[2025-01-06 01:06:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:57][root][INFO] - Training Epoch: 2/10, step 120/574 completed (loss: 0.4445089101791382, acc: 0.8399999737739563)
[2025-01-06 01:06:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:58][root][INFO] - Training Epoch: 2/10, step 121/574 completed (loss: 0.4735879898071289, acc: 0.8846153616905212)
[2025-01-06 01:06:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:58][root][INFO] - Training Epoch: 2/10, step 122/574 completed (loss: 0.39043399691581726, acc: 0.7916666865348816)
[2025-01-06 01:06:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:58][root][INFO] - Training Epoch: 2/10, step 123/574 completed (loss: 0.569234311580658, acc: 0.8421052694320679)
[2025-01-06 01:06:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:59][root][INFO] - Training Epoch: 2/10, step 124/574 completed (loss: 1.0754709243774414, acc: 0.6932515501976013)
[2025-01-06 01:06:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:59][root][INFO] - Training Epoch: 2/10, step 125/574 completed (loss: 0.9913610816001892, acc: 0.7152777910232544)
[2025-01-06 01:06:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:06:59][root][INFO] - Training Epoch: 2/10, step 126/574 completed (loss: 1.2290102243423462, acc: 0.6333333253860474)
[2025-01-06 01:07:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:00][root][INFO] - Training Epoch: 2/10, step 127/574 completed (loss: 0.706498384475708, acc: 0.7916666865348816)
[2025-01-06 01:07:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:00][root][INFO] - Training Epoch: 2/10, step 128/574 completed (loss: 0.9105314612388611, acc: 0.7333333492279053)
[2025-01-06 01:07:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:01][root][INFO] - Training Epoch: 2/10, step 129/574 completed (loss: 1.0406720638275146, acc: 0.6764705777168274)
[2025-01-06 01:07:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:01][root][INFO] - Training Epoch: 2/10, step 130/574 completed (loss: 0.6227989196777344, acc: 0.7692307829856873)
[2025-01-06 01:07:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:01][root][INFO] - Training Epoch: 2/10, step 131/574 completed (loss: 0.7247493863105774, acc: 0.8695651888847351)
[2025-01-06 01:07:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:02][root][INFO] - Training Epoch: 2/10, step 132/574 completed (loss: 1.062666416168213, acc: 0.78125)
[2025-01-06 01:07:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:02][root][INFO] - Training Epoch: 2/10, step 133/574 completed (loss: 1.4002079963684082, acc: 0.5652173757553101)
[2025-01-06 01:07:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:02][root][INFO] - Training Epoch: 2/10, step 134/574 completed (loss: 1.096394658088684, acc: 0.6571428775787354)
[2025-01-06 01:07:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:03][root][INFO] - Training Epoch: 2/10, step 135/574 completed (loss: 1.2522703409194946, acc: 0.7307692170143127)
[2025-01-06 01:07:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:03][root][INFO] - Training Epoch: 2/10, step 136/574 completed (loss: 0.8345078229904175, acc: 0.738095223903656)
[2025-01-06 01:07:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:03][root][INFO] - Training Epoch: 2/10, step 137/574 completed (loss: 1.3781911134719849, acc: 0.5333333611488342)
[2025-01-06 01:07:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:04][root][INFO] - Training Epoch: 2/10, step 138/574 completed (loss: 1.002128005027771, acc: 0.739130437374115)
[2025-01-06 01:07:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:04][root][INFO] - Training Epoch: 2/10, step 139/574 completed (loss: 0.4716627597808838, acc: 0.8571428656578064)
[2025-01-06 01:07:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:04][root][INFO] - Training Epoch: 2/10, step 140/574 completed (loss: 0.6792459487915039, acc: 0.7692307829856873)
[2025-01-06 01:07:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:35][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.8467, device='cuda:0') eval_epoch_loss=tensor(0.6134, device='cuda:0') eval_epoch_acc=tensor(0.8283, device='cuda:0')
[2025-01-06 01:07:35][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:07:35][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:07:35][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_2_step_141_loss_0.6133849024772644/model.pt
[2025-01-06 01:07:35][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:07:35][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 2 is 0.6133849024772644
[2025-01-06 01:07:35][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.8282821774482727
[2025-01-06 01:07:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:36][root][INFO] - Training Epoch: 2/10, step 141/574 completed (loss: 0.873360812664032, acc: 0.7096773982048035)
[2025-01-06 01:07:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:36][root][INFO] - Training Epoch: 2/10, step 142/574 completed (loss: 0.8661066293716431, acc: 0.7027027010917664)
[2025-01-06 01:07:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:37][root][INFO] - Training Epoch: 2/10, step 143/574 completed (loss: 0.8386752605438232, acc: 0.7280701994895935)
[2025-01-06 01:07:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:37][root][INFO] - Training Epoch: 2/10, step 144/574 completed (loss: 1.0202281475067139, acc: 0.7014925479888916)
[2025-01-06 01:07:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:37][root][INFO] - Training Epoch: 2/10, step 145/574 completed (loss: 0.9254411458969116, acc: 0.704081654548645)
[2025-01-06 01:07:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:38][root][INFO] - Training Epoch: 2/10, step 146/574 completed (loss: 1.4014099836349487, acc: 0.563829779624939)
[2025-01-06 01:07:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:38][root][INFO] - Training Epoch: 2/10, step 147/574 completed (loss: 1.0264511108398438, acc: 0.699999988079071)
[2025-01-06 01:07:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:38][root][INFO] - Training Epoch: 2/10, step 148/574 completed (loss: 1.5003079175949097, acc: 0.5357142686843872)
[2025-01-06 01:07:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:39][root][INFO] - Training Epoch: 2/10, step 149/574 completed (loss: 1.1794655323028564, acc: 0.695652186870575)
[2025-01-06 01:07:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:39][root][INFO] - Training Epoch: 2/10, step 150/574 completed (loss: 0.9401271939277649, acc: 0.6896551847457886)
[2025-01-06 01:07:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:40][root][INFO] - Training Epoch: 2/10, step 151/574 completed (loss: 1.2284879684448242, acc: 0.695652186870575)
[2025-01-06 01:07:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:40][root][INFO] - Training Epoch: 2/10, step 152/574 completed (loss: 0.939081609249115, acc: 0.7118644118309021)
[2025-01-06 01:07:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:40][root][INFO] - Training Epoch: 2/10, step 153/574 completed (loss: 1.2069635391235352, acc: 0.7017543911933899)
[2025-01-06 01:07:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:41][root][INFO] - Training Epoch: 2/10, step 154/574 completed (loss: 0.9406977295875549, acc: 0.7162162065505981)
[2025-01-06 01:07:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:41][root][INFO] - Training Epoch: 2/10, step 155/574 completed (loss: 0.6051913499832153, acc: 0.7857142686843872)
[2025-01-06 01:07:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:41][root][INFO] - Training Epoch: 2/10, step 156/574 completed (loss: 0.8038878440856934, acc: 0.739130437374115)
[2025-01-06 01:07:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:42][root][INFO] - Training Epoch: 2/10, step 157/574 completed (loss: 2.4875731468200684, acc: 0.31578946113586426)
[2025-01-06 01:07:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:43][root][INFO] - Training Epoch: 2/10, step 158/574 completed (loss: 1.408858060836792, acc: 0.6351351141929626)
[2025-01-06 01:07:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:44][root][INFO] - Training Epoch: 2/10, step 159/574 completed (loss: 1.5831395387649536, acc: 0.5185185074806213)
[2025-01-06 01:07:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:44][root][INFO] - Training Epoch: 2/10, step 160/574 completed (loss: 1.6026121377944946, acc: 0.5581395626068115)
[2025-01-06 01:07:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:45][root][INFO] - Training Epoch: 2/10, step 161/574 completed (loss: 1.7080305814743042, acc: 0.47058823704719543)
[2025-01-06 01:07:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:45][root][INFO] - Training Epoch: 2/10, step 162/574 completed (loss: 1.8001832962036133, acc: 0.5730336904525757)
[2025-01-06 01:07:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:46][root][INFO] - Training Epoch: 2/10, step 163/574 completed (loss: 0.5737327337265015, acc: 0.8863636255264282)
[2025-01-06 01:07:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:46][root][INFO] - Training Epoch: 2/10, step 164/574 completed (loss: 0.6101858615875244, acc: 0.8095238208770752)
[2025-01-06 01:07:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:46][root][INFO] - Training Epoch: 2/10, step 165/574 completed (loss: 1.050488829612732, acc: 0.6206896305084229)
[2025-01-06 01:07:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:47][root][INFO] - Training Epoch: 2/10, step 166/574 completed (loss: 0.24955490231513977, acc: 0.918367326259613)
[2025-01-06 01:07:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:47][root][INFO] - Training Epoch: 2/10, step 167/574 completed (loss: 0.17228718101978302, acc: 0.9800000190734863)
[2025-01-06 01:07:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:47][root][INFO] - Training Epoch: 2/10, step 168/574 completed (loss: 0.5935750007629395, acc: 0.8472222089767456)
[2025-01-06 01:07:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:48][root][INFO] - Training Epoch: 2/10, step 169/574 completed (loss: 1.0493675470352173, acc: 0.7745097875595093)
[2025-01-06 01:07:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:49][root][INFO] - Training Epoch: 2/10, step 170/574 completed (loss: 0.9713513255119324, acc: 0.732876718044281)
[2025-01-06 01:07:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:49][root][INFO] - Training Epoch: 2/10, step 171/574 completed (loss: 0.2291577011346817, acc: 0.9583333134651184)
[2025-01-06 01:07:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:49][root][INFO] - Training Epoch: 2/10, step 172/574 completed (loss: 0.7591484785079956, acc: 0.7777777910232544)
[2025-01-06 01:07:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:50][root][INFO] - Training Epoch: 2/10, step 173/574 completed (loss: 0.7749648094177246, acc: 0.7857142686843872)
[2025-01-06 01:07:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:50][root][INFO] - Training Epoch: 2/10, step 174/574 completed (loss: 1.2081079483032227, acc: 0.7168141603469849)
[2025-01-06 01:07:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:51][root][INFO] - Training Epoch: 2/10, step 175/574 completed (loss: 0.7791454195976257, acc: 0.8115941882133484)
[2025-01-06 01:07:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:51][root][INFO] - Training Epoch: 2/10, step 176/574 completed (loss: 0.619605302810669, acc: 0.7954545617103577)
[2025-01-06 01:07:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:52][root][INFO] - Training Epoch: 2/10, step 177/574 completed (loss: 1.2161242961883545, acc: 0.6488549709320068)
[2025-01-06 01:07:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:53][root][INFO] - Training Epoch: 2/10, step 178/574 completed (loss: 1.1498712301254272, acc: 0.644444465637207)
[2025-01-06 01:07:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:53][root][INFO] - Training Epoch: 2/10, step 179/574 completed (loss: 0.7274362444877625, acc: 0.7868852615356445)
[2025-01-06 01:07:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:53][root][INFO] - Training Epoch: 2/10, step 180/574 completed (loss: 0.03494692221283913, acc: 1.0)
[2025-01-06 01:07:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:54][root][INFO] - Training Epoch: 2/10, step 181/574 completed (loss: 0.3803783655166626, acc: 0.9200000166893005)
[2025-01-06 01:07:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:54][root][INFO] - Training Epoch: 2/10, step 182/574 completed (loss: 0.2723160982131958, acc: 0.9285714030265808)
[2025-01-06 01:07:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:54][root][INFO] - Training Epoch: 2/10, step 183/574 completed (loss: 0.27729323506355286, acc: 0.9024389982223511)
[2025-01-06 01:07:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:55][root][INFO] - Training Epoch: 2/10, step 184/574 completed (loss: 0.5812690258026123, acc: 0.861027181148529)
[2025-01-06 01:07:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:55][root][INFO] - Training Epoch: 2/10, step 185/574 completed (loss: 0.48728063702583313, acc: 0.8703169822692871)
[2025-01-06 01:07:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:55][root][INFO] - Training Epoch: 2/10, step 186/574 completed (loss: 0.4478732645511627, acc: 0.859375)
[2025-01-06 01:07:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:56][root][INFO] - Training Epoch: 2/10, step 187/574 completed (loss: 0.5145640969276428, acc: 0.8649155497550964)
[2025-01-06 01:07:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:56][root][INFO] - Training Epoch: 2/10, step 188/574 completed (loss: 0.5792753100395203, acc: 0.8398576378822327)
[2025-01-06 01:07:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:57][root][INFO] - Training Epoch: 2/10, step 189/574 completed (loss: 0.6577731966972351, acc: 0.800000011920929)
[2025-01-06 01:07:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:57][root][INFO] - Training Epoch: 2/10, step 190/574 completed (loss: 0.871265172958374, acc: 0.7325581312179565)
[2025-01-06 01:07:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:58][root][INFO] - Training Epoch: 2/10, step 191/574 completed (loss: 1.3563264608383179, acc: 0.6190476417541504)
[2025-01-06 01:07:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:07:59][root][INFO] - Training Epoch: 2/10, step 192/574 completed (loss: 0.9383601546287537, acc: 0.7272727489471436)
[2025-01-06 01:07:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:00][root][INFO] - Training Epoch: 2/10, step 193/574 completed (loss: 0.8866698145866394, acc: 0.7764706015586853)
[2025-01-06 01:08:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:01][root][INFO] - Training Epoch: 2/10, step 194/574 completed (loss: 1.0334964990615845, acc: 0.6913580298423767)
[2025-01-06 01:08:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:02][root][INFO] - Training Epoch: 2/10, step 195/574 completed (loss: 0.6327799558639526, acc: 0.7903226017951965)
[2025-01-06 01:08:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:02][root][INFO] - Training Epoch: 2/10, step 196/574 completed (loss: 0.3205872178077698, acc: 0.9285714030265808)
[2025-01-06 01:08:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:02][root][INFO] - Training Epoch: 2/10, step 197/574 completed (loss: 1.225093126296997, acc: 0.699999988079071)
[2025-01-06 01:08:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:03][root][INFO] - Training Epoch: 2/10, step 198/574 completed (loss: 1.0174331665039062, acc: 0.720588207244873)
[2025-01-06 01:08:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:03][root][INFO] - Training Epoch: 2/10, step 199/574 completed (loss: 0.9789033532142639, acc: 0.7352941036224365)
[2025-01-06 01:08:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:03][root][INFO] - Training Epoch: 2/10, step 200/574 completed (loss: 0.7351197004318237, acc: 0.805084764957428)
[2025-01-06 01:08:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:04][root][INFO] - Training Epoch: 2/10, step 201/574 completed (loss: 1.0197926759719849, acc: 0.7388059496879578)
[2025-01-06 01:08:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:04][root][INFO] - Training Epoch: 2/10, step 202/574 completed (loss: 1.0243254899978638, acc: 0.7281553149223328)
[2025-01-06 01:08:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:05][root][INFO] - Training Epoch: 2/10, step 203/574 completed (loss: 0.8751444220542908, acc: 0.7301587462425232)
[2025-01-06 01:08:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:05][root][INFO] - Training Epoch: 2/10, step 204/574 completed (loss: 0.2478695958852768, acc: 0.9230769276618958)
[2025-01-06 01:08:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:05][root][INFO] - Training Epoch: 2/10, step 205/574 completed (loss: 0.36324235796928406, acc: 0.9058296084403992)
[2025-01-06 01:08:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:06][root][INFO] - Training Epoch: 2/10, step 206/574 completed (loss: 0.5173825621604919, acc: 0.8267716765403748)
[2025-01-06 01:08:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:06][root][INFO] - Training Epoch: 2/10, step 207/574 completed (loss: 0.4304673969745636, acc: 0.8793103694915771)
[2025-01-06 01:08:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:07][root][INFO] - Training Epoch: 2/10, step 208/574 completed (loss: 0.4933858811855316, acc: 0.8768116235733032)
[2025-01-06 01:08:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:07][root][INFO] - Training Epoch: 2/10, step 209/574 completed (loss: 0.46488139033317566, acc: 0.8599221706390381)
[2025-01-06 01:08:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:07][root][INFO] - Training Epoch: 2/10, step 210/574 completed (loss: 0.4092496931552887, acc: 0.8586956262588501)
[2025-01-06 01:08:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:08][root][INFO] - Training Epoch: 2/10, step 211/574 completed (loss: 0.37518373131752014, acc: 0.8260869383811951)
[2025-01-06 01:08:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:08][root][INFO] - Training Epoch: 2/10, step 212/574 completed (loss: 0.1353784054517746, acc: 1.0)
[2025-01-06 01:08:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:08][root][INFO] - Training Epoch: 2/10, step 213/574 completed (loss: 0.18662075698375702, acc: 0.914893627166748)
[2025-01-06 01:08:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:09][root][INFO] - Training Epoch: 2/10, step 214/574 completed (loss: 0.2191350907087326, acc: 0.9538461565971375)
[2025-01-06 01:08:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:09][root][INFO] - Training Epoch: 2/10, step 215/574 completed (loss: 0.2050870656967163, acc: 0.9459459185600281)
[2025-01-06 01:08:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:10][root][INFO] - Training Epoch: 2/10, step 216/574 completed (loss: 0.19047954678535461, acc: 0.930232584476471)
[2025-01-06 01:08:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:10][root][INFO] - Training Epoch: 2/10, step 217/574 completed (loss: 0.30463090538978577, acc: 0.8918918967247009)
[2025-01-06 01:08:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:11][root][INFO] - Training Epoch: 2/10, step 218/574 completed (loss: 0.181721493601799, acc: 0.9444444179534912)
[2025-01-06 01:08:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:11][root][INFO] - Training Epoch: 2/10, step 219/574 completed (loss: 0.25818267464637756, acc: 0.9696969985961914)
[2025-01-06 01:08:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:11][root][INFO] - Training Epoch: 2/10, step 220/574 completed (loss: 0.17193907499313354, acc: 0.9259259104728699)
[2025-01-06 01:08:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:12][root][INFO] - Training Epoch: 2/10, step 221/574 completed (loss: 0.1478959321975708, acc: 0.9599999785423279)
[2025-01-06 01:08:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:12][root][INFO] - Training Epoch: 2/10, step 222/574 completed (loss: 0.7627344727516174, acc: 0.7884615659713745)
[2025-01-06 01:08:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:13][root][INFO] - Training Epoch: 2/10, step 223/574 completed (loss: 0.4612210690975189, acc: 0.8695651888847351)
[2025-01-06 01:08:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:13][root][INFO] - Training Epoch: 2/10, step 224/574 completed (loss: 0.6039080619812012, acc: 0.8068181872367859)
[2025-01-06 01:08:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:14][root][INFO] - Training Epoch: 2/10, step 225/574 completed (loss: 0.8780367970466614, acc: 0.7765957713127136)
[2025-01-06 01:08:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:14][root][INFO] - Training Epoch: 2/10, step 226/574 completed (loss: 0.7256584763526917, acc: 0.8113207817077637)
[2025-01-06 01:08:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:15][root][INFO] - Training Epoch: 2/10, step 227/574 completed (loss: 0.5215114951133728, acc: 0.8166666626930237)
[2025-01-06 01:08:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:15][root][INFO] - Training Epoch: 2/10, step 228/574 completed (loss: 0.36300671100616455, acc: 0.8837209343910217)
[2025-01-06 01:08:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:15][root][INFO] - Training Epoch: 2/10, step 229/574 completed (loss: 1.3852877616882324, acc: 0.6000000238418579)
[2025-01-06 01:08:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:16][root][INFO] - Training Epoch: 2/10, step 230/574 completed (loss: 1.8614717721939087, acc: 0.5263158082962036)
[2025-01-06 01:08:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:16][root][INFO] - Training Epoch: 2/10, step 231/574 completed (loss: 1.4663372039794922, acc: 0.6222222447395325)
[2025-01-06 01:08:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:17][root][INFO] - Training Epoch: 2/10, step 232/574 completed (loss: 1.5620490312576294, acc: 0.5888888835906982)
[2025-01-06 01:08:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:17][root][INFO] - Training Epoch: 2/10, step 233/574 completed (loss: 1.897567868232727, acc: 0.5229358077049255)
[2025-01-06 01:08:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:18][root][INFO] - Training Epoch: 2/10, step 234/574 completed (loss: 1.663816213607788, acc: 0.5307692289352417)
[2025-01-06 01:08:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:18][root][INFO] - Training Epoch: 2/10, step 235/574 completed (loss: 0.39869412779808044, acc: 0.8421052694320679)
[2025-01-06 01:08:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:18][root][INFO] - Training Epoch: 2/10, step 236/574 completed (loss: 0.5083500146865845, acc: 0.875)
[2025-01-06 01:08:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:19][root][INFO] - Training Epoch: 2/10, step 237/574 completed (loss: 1.2531882524490356, acc: 0.6818181872367859)
[2025-01-06 01:08:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:19][root][INFO] - Training Epoch: 2/10, step 238/574 completed (loss: 0.5937809944152832, acc: 0.8148148059844971)
[2025-01-06 01:08:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:19][root][INFO] - Training Epoch: 2/10, step 239/574 completed (loss: 1.0262434482574463, acc: 0.6571428775787354)
[2025-01-06 01:08:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:20][root][INFO] - Training Epoch: 2/10, step 240/574 completed (loss: 1.2184929847717285, acc: 0.7272727489471436)
[2025-01-06 01:08:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:20][root][INFO] - Training Epoch: 2/10, step 241/574 completed (loss: 0.8291811347007751, acc: 0.7045454382896423)
[2025-01-06 01:08:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:21][root][INFO] - Training Epoch: 2/10, step 242/574 completed (loss: 1.214719533920288, acc: 0.5322580933570862)
[2025-01-06 01:08:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:21][root][INFO] - Training Epoch: 2/10, step 243/574 completed (loss: 1.3463047742843628, acc: 0.6136363744735718)
[2025-01-06 01:08:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:21][root][INFO] - Training Epoch: 2/10, step 244/574 completed (loss: 0.009452697820961475, acc: 1.0)
[2025-01-06 01:08:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:22][root][INFO] - Training Epoch: 2/10, step 245/574 completed (loss: 0.7142009139060974, acc: 0.7692307829856873)
[2025-01-06 01:08:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:22][root][INFO] - Training Epoch: 2/10, step 246/574 completed (loss: 0.15961939096450806, acc: 0.9354838728904724)
[2025-01-06 01:08:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:22][root][INFO] - Training Epoch: 2/10, step 247/574 completed (loss: 0.191090926527977, acc: 0.949999988079071)
[2025-01-06 01:08:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:23][root][INFO] - Training Epoch: 2/10, step 248/574 completed (loss: 0.2324734777212143, acc: 0.9189189076423645)
[2025-01-06 01:08:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:23][root][INFO] - Training Epoch: 2/10, step 249/574 completed (loss: 0.4039711654186249, acc: 0.9189189076423645)
[2025-01-06 01:08:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:23][root][INFO] - Training Epoch: 2/10, step 250/574 completed (loss: 0.0574905164539814, acc: 1.0)
[2025-01-06 01:08:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:24][root][INFO] - Training Epoch: 2/10, step 251/574 completed (loss: 0.35766148567199707, acc: 0.8676470518112183)
[2025-01-06 01:08:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:24][root][INFO] - Training Epoch: 2/10, step 252/574 completed (loss: 0.19536042213439941, acc: 0.9268292784690857)
[2025-01-06 01:08:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:25][root][INFO] - Training Epoch: 2/10, step 253/574 completed (loss: 0.06356854736804962, acc: 1.0)
[2025-01-06 01:08:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:25][root][INFO] - Training Epoch: 2/10, step 254/574 completed (loss: 0.028839340433478355, acc: 1.0)
[2025-01-06 01:08:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:25][root][INFO] - Training Epoch: 2/10, step 255/574 completed (loss: 0.21576686203479767, acc: 0.9354838728904724)
[2025-01-06 01:08:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:26][root][INFO] - Training Epoch: 2/10, step 256/574 completed (loss: 0.3628368079662323, acc: 0.8947368264198303)
[2025-01-06 01:08:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:26][root][INFO] - Training Epoch: 2/10, step 257/574 completed (loss: 0.16504628956317902, acc: 0.9571428298950195)
[2025-01-06 01:08:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:26][root][INFO] - Training Epoch: 2/10, step 258/574 completed (loss: 0.21180780231952667, acc: 0.9342105388641357)
[2025-01-06 01:08:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:27][root][INFO] - Training Epoch: 2/10, step 259/574 completed (loss: 0.5231960415840149, acc: 0.8773584961891174)
[2025-01-06 01:08:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:27][root][INFO] - Training Epoch: 2/10, step 260/574 completed (loss: 0.5335209369659424, acc: 0.8666666746139526)
[2025-01-06 01:08:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:28][root][INFO] - Training Epoch: 2/10, step 261/574 completed (loss: 0.24339696764945984, acc: 0.9444444179534912)
[2025-01-06 01:08:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:28][root][INFO] - Training Epoch: 2/10, step 262/574 completed (loss: 0.7238325476646423, acc: 0.8064516186714172)
[2025-01-06 01:08:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:28][root][INFO] - Training Epoch: 2/10, step 263/574 completed (loss: 1.3118358850479126, acc: 0.7200000286102295)
[2025-01-06 01:08:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:29][root][INFO] - Training Epoch: 2/10, step 264/574 completed (loss: 0.6788750290870667, acc: 0.7708333134651184)
[2025-01-06 01:08:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:30][root][INFO] - Training Epoch: 2/10, step 265/574 completed (loss: 1.4595978260040283, acc: 0.6240000128746033)
[2025-01-06 01:08:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:30][root][INFO] - Training Epoch: 2/10, step 266/574 completed (loss: 1.4433612823486328, acc: 0.584269642829895)
[2025-01-06 01:08:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:30][root][INFO] - Training Epoch: 2/10, step 267/574 completed (loss: 1.0934873819351196, acc: 0.6891891956329346)
[2025-01-06 01:08:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:31][root][INFO] - Training Epoch: 2/10, step 268/574 completed (loss: 0.8022980690002441, acc: 0.7931034564971924)
[2025-01-06 01:08:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:31][root][INFO] - Training Epoch: 2/10, step 269/574 completed (loss: 0.3029016852378845, acc: 0.9545454382896423)
[2025-01-06 01:08:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:32][root][INFO] - Training Epoch: 2/10, step 270/574 completed (loss: 0.2582816183567047, acc: 0.9090909361839294)
[2025-01-06 01:08:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:32][root][INFO] - Training Epoch: 2/10, step 271/574 completed (loss: 0.17005567252635956, acc: 0.9375)
[2025-01-06 01:08:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:32][root][INFO] - Training Epoch: 2/10, step 272/574 completed (loss: 0.10974802076816559, acc: 0.9666666388511658)
[2025-01-06 01:08:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:33][root][INFO] - Training Epoch: 2/10, step 273/574 completed (loss: 0.440248966217041, acc: 0.9166666865348816)
[2025-01-06 01:08:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:33][root][INFO] - Training Epoch: 2/10, step 274/574 completed (loss: 0.1566820591688156, acc: 0.96875)
[2025-01-06 01:08:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:33][root][INFO] - Training Epoch: 2/10, step 275/574 completed (loss: 0.4042738378047943, acc: 0.8999999761581421)
[2025-01-06 01:08:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:34][root][INFO] - Training Epoch: 2/10, step 276/574 completed (loss: 0.4323141276836395, acc: 0.931034505367279)
[2025-01-06 01:08:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:34][root][INFO] - Training Epoch: 2/10, step 277/574 completed (loss: 0.32182666659355164, acc: 0.9200000166893005)
[2025-01-06 01:08:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:35][root][INFO] - Training Epoch: 2/10, step 278/574 completed (loss: 0.37258604168891907, acc: 0.8936170339584351)
[2025-01-06 01:08:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:35][root][INFO] - Training Epoch: 2/10, step 279/574 completed (loss: 0.5757883191108704, acc: 0.8958333134651184)
[2025-01-06 01:08:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:35][root][INFO] - Training Epoch: 2/10, step 280/574 completed (loss: 0.1899528056383133, acc: 0.9545454382896423)
[2025-01-06 01:08:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:36][root][INFO] - Training Epoch: 2/10, step 281/574 completed (loss: 0.9998740553855896, acc: 0.6626505851745605)
[2025-01-06 01:08:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:36][root][INFO] - Training Epoch: 2/10, step 282/574 completed (loss: 0.9177838563919067, acc: 0.7592592835426331)
[2025-01-06 01:08:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:36][root][INFO] - Training Epoch: 2/10, step 283/574 completed (loss: 0.12658433616161346, acc: 0.9736841917037964)
[2025-01-06 01:08:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:08:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:07][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.7486, device='cuda:0') eval_epoch_loss=tensor(0.5588, device='cuda:0') eval_epoch_acc=tensor(0.8429, device='cuda:0')
[2025-01-06 01:09:07][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:09:07][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:09:08][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_2_step_284_loss_0.5588149428367615/model.pt
[2025-01-06 01:09:08][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:09:08][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 2 is 0.5588149428367615
[2025-01-06 01:09:08][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.8429339528083801
[2025-01-06 01:09:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:08][root][INFO] - Training Epoch: 2/10, step 284/574 completed (loss: 0.5410014986991882, acc: 0.7647058963775635)
[2025-01-06 01:09:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:08][root][INFO] - Training Epoch: 2/10, step 285/574 completed (loss: 0.3317835330963135, acc: 0.949999988079071)
[2025-01-06 01:09:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:09][root][INFO] - Training Epoch: 2/10, step 286/574 completed (loss: 0.4919784665107727, acc: 0.8359375)
[2025-01-06 01:09:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:09][root][INFO] - Training Epoch: 2/10, step 287/574 completed (loss: 0.5918710231781006, acc: 0.8560000061988831)
[2025-01-06 01:09:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:09][root][INFO] - Training Epoch: 2/10, step 288/574 completed (loss: 0.38669222593307495, acc: 0.8791208863258362)
[2025-01-06 01:09:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:10][root][INFO] - Training Epoch: 2/10, step 289/574 completed (loss: 0.5131129622459412, acc: 0.8198757767677307)
[2025-01-06 01:09:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:10][root][INFO] - Training Epoch: 2/10, step 290/574 completed (loss: 0.5449120998382568, acc: 0.8711340427398682)
[2025-01-06 01:09:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:10][root][INFO] - Training Epoch: 2/10, step 291/574 completed (loss: 0.031286004930734634, acc: 1.0)
[2025-01-06 01:09:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:11][root][INFO] - Training Epoch: 2/10, step 292/574 completed (loss: 0.5264348983764648, acc: 0.7857142686843872)
[2025-01-06 01:09:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:11][root][INFO] - Training Epoch: 2/10, step 293/574 completed (loss: 0.1554819494485855, acc: 0.982758641242981)
[2025-01-06 01:09:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:12][root][INFO] - Training Epoch: 2/10, step 294/574 completed (loss: 0.6471142768859863, acc: 0.8363636136054993)
[2025-01-06 01:09:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:12][root][INFO] - Training Epoch: 2/10, step 295/574 completed (loss: 0.5110794901847839, acc: 0.8659793734550476)
[2025-01-06 01:09:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:12][root][INFO] - Training Epoch: 2/10, step 296/574 completed (loss: 0.4683077037334442, acc: 0.8103448152542114)
[2025-01-06 01:09:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:13][root][INFO] - Training Epoch: 2/10, step 297/574 completed (loss: 0.12846320867538452, acc: 0.9629629850387573)
[2025-01-06 01:09:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:13][root][INFO] - Training Epoch: 2/10, step 298/574 completed (loss: 0.6405021548271179, acc: 0.8421052694320679)
[2025-01-06 01:09:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:13][root][INFO] - Training Epoch: 2/10, step 299/574 completed (loss: 0.07647217810153961, acc: 1.0)
[2025-01-06 01:09:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:14][root][INFO] - Training Epoch: 2/10, step 300/574 completed (loss: 0.08431078493595123, acc: 0.96875)
[2025-01-06 01:09:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:14][root][INFO] - Training Epoch: 2/10, step 301/574 completed (loss: 0.45516812801361084, acc: 0.9056603908538818)
[2025-01-06 01:09:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:14][root][INFO] - Training Epoch: 2/10, step 302/574 completed (loss: 0.058179959654808044, acc: 0.9622641801834106)
[2025-01-06 01:09:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:15][root][INFO] - Training Epoch: 2/10, step 303/574 completed (loss: 0.12783603370189667, acc: 0.970588207244873)
[2025-01-06 01:09:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:15][root][INFO] - Training Epoch: 2/10, step 304/574 completed (loss: 0.3417429029941559, acc: 0.90625)
[2025-01-06 01:09:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:16][root][INFO] - Training Epoch: 2/10, step 305/574 completed (loss: 0.49240830540657043, acc: 0.868852436542511)
[2025-01-06 01:09:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:16][root][INFO] - Training Epoch: 2/10, step 306/574 completed (loss: 0.14319531619548798, acc: 0.9333333373069763)
[2025-01-06 01:09:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:16][root][INFO] - Training Epoch: 2/10, step 307/574 completed (loss: 0.11584921926259995, acc: 0.9473684430122375)
[2025-01-06 01:09:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:17][root][INFO] - Training Epoch: 2/10, step 308/574 completed (loss: 0.3329738676548004, acc: 0.8985507488250732)
[2025-01-06 01:09:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:17][root][INFO] - Training Epoch: 2/10, step 309/574 completed (loss: 0.19488213956356049, acc: 0.9583333134651184)
[2025-01-06 01:09:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:17][root][INFO] - Training Epoch: 2/10, step 310/574 completed (loss: 0.1911911964416504, acc: 0.9759036302566528)
[2025-01-06 01:09:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:18][root][INFO] - Training Epoch: 2/10, step 311/574 completed (loss: 0.40208226442337036, acc: 0.8333333134651184)
[2025-01-06 01:09:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:18][root][INFO] - Training Epoch: 2/10, step 312/574 completed (loss: 0.1609107404947281, acc: 0.9489796161651611)
[2025-01-06 01:09:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:19][root][INFO] - Training Epoch: 2/10, step 313/574 completed (loss: 0.02529752254486084, acc: 1.0)
[2025-01-06 01:09:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:19][root][INFO] - Training Epoch: 2/10, step 314/574 completed (loss: 0.08305060118436813, acc: 0.9583333134651184)
[2025-01-06 01:09:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:19][root][INFO] - Training Epoch: 2/10, step 315/574 completed (loss: 0.399039089679718, acc: 0.9032257795333862)
[2025-01-06 01:09:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:20][root][INFO] - Training Epoch: 2/10, step 316/574 completed (loss: 0.5309193134307861, acc: 0.8709677457809448)
[2025-01-06 01:09:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:20][root][INFO] - Training Epoch: 2/10, step 317/574 completed (loss: 0.2862647473812103, acc: 0.9253731369972229)
[2025-01-06 01:09:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:20][root][INFO] - Training Epoch: 2/10, step 318/574 completed (loss: 0.123493991792202, acc: 0.9711538553237915)
[2025-01-06 01:09:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:21][root][INFO] - Training Epoch: 2/10, step 319/574 completed (loss: 0.2532704174518585, acc: 0.9333333373069763)
[2025-01-06 01:09:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:21][root][INFO] - Training Epoch: 2/10, step 320/574 completed (loss: 0.1433107554912567, acc: 0.9516128897666931)
[2025-01-06 01:09:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:21][root][INFO] - Training Epoch: 2/10, step 321/574 completed (loss: 0.058755069971084595, acc: 1.0)
[2025-01-06 01:09:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:22][root][INFO] - Training Epoch: 2/10, step 322/574 completed (loss: 0.8285386562347412, acc: 0.7037037014961243)
[2025-01-06 01:09:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:22][root][INFO] - Training Epoch: 2/10, step 323/574 completed (loss: 1.652667760848999, acc: 0.5714285969734192)
[2025-01-06 01:09:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:22][root][INFO] - Training Epoch: 2/10, step 324/574 completed (loss: 1.2668801546096802, acc: 0.7435897588729858)
[2025-01-06 01:09:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:23][root][INFO] - Training Epoch: 2/10, step 325/574 completed (loss: 1.634549617767334, acc: 0.5121951103210449)
[2025-01-06 01:09:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:23][root][INFO] - Training Epoch: 2/10, step 326/574 completed (loss: 1.1904208660125732, acc: 0.6578947305679321)
[2025-01-06 01:09:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:23][root][INFO] - Training Epoch: 2/10, step 327/574 completed (loss: 0.5665881633758545, acc: 0.8421052694320679)
[2025-01-06 01:09:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:24][root][INFO] - Training Epoch: 2/10, step 328/574 completed (loss: 0.2439710795879364, acc: 0.9285714030265808)
[2025-01-06 01:09:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:24][root][INFO] - Training Epoch: 2/10, step 329/574 completed (loss: 0.38667964935302734, acc: 0.8888888955116272)
[2025-01-06 01:09:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:25][root][INFO] - Training Epoch: 2/10, step 330/574 completed (loss: 0.1381019502878189, acc: 0.96875)
[2025-01-06 01:09:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:25][root][INFO] - Training Epoch: 2/10, step 331/574 completed (loss: 0.29083874821662903, acc: 0.9516128897666931)
[2025-01-06 01:09:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:25][root][INFO] - Training Epoch: 2/10, step 332/574 completed (loss: 0.17583853006362915, acc: 0.9824561476707458)
[2025-01-06 01:09:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:26][root][INFO] - Training Epoch: 2/10, step 333/574 completed (loss: 0.1814458668231964, acc: 0.90625)
[2025-01-06 01:09:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:26][root][INFO] - Training Epoch: 2/10, step 334/574 completed (loss: 0.15867792069911957, acc: 0.9666666388511658)
[2025-01-06 01:09:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:26][root][INFO] - Training Epoch: 2/10, step 335/574 completed (loss: 0.4876401722431183, acc: 0.7894737124443054)
[2025-01-06 01:09:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:27][root][INFO] - Training Epoch: 2/10, step 336/574 completed (loss: 1.0329313278198242, acc: 0.7400000095367432)
[2025-01-06 01:09:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:27][root][INFO] - Training Epoch: 2/10, step 337/574 completed (loss: 1.3398667573928833, acc: 0.6436781883239746)
[2025-01-06 01:09:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:27][root][INFO] - Training Epoch: 2/10, step 338/574 completed (loss: 1.4483225345611572, acc: 0.5531914830207825)
[2025-01-06 01:09:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:28][root][INFO] - Training Epoch: 2/10, step 339/574 completed (loss: 1.3676869869232178, acc: 0.6385542154312134)
[2025-01-06 01:09:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:28][root][INFO] - Training Epoch: 2/10, step 340/574 completed (loss: 0.12341728061437607, acc: 0.95652174949646)
[2025-01-06 01:09:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:29][root][INFO] - Training Epoch: 2/10, step 341/574 completed (loss: 0.5471484661102295, acc: 0.8461538553237915)
[2025-01-06 01:09:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:29][root][INFO] - Training Epoch: 2/10, step 342/574 completed (loss: 0.4893673360347748, acc: 0.8795180916786194)
[2025-01-06 01:09:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:29][root][INFO] - Training Epoch: 2/10, step 343/574 completed (loss: 0.6121851801872253, acc: 0.8113207817077637)
[2025-01-06 01:09:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:30][root][INFO] - Training Epoch: 2/10, step 344/574 completed (loss: 0.22377699613571167, acc: 0.9240506291389465)
[2025-01-06 01:09:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:30][root][INFO] - Training Epoch: 2/10, step 345/574 completed (loss: 0.11987923830747604, acc: 0.9607843160629272)
[2025-01-06 01:09:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:30][root][INFO] - Training Epoch: 2/10, step 346/574 completed (loss: 0.42870548367500305, acc: 0.8805969953536987)
[2025-01-06 01:09:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:31][root][INFO] - Training Epoch: 2/10, step 347/574 completed (loss: 0.020947087556123734, acc: 1.0)
[2025-01-06 01:09:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:31][root][INFO] - Training Epoch: 2/10, step 348/574 completed (loss: 0.28168216347694397, acc: 0.8799999952316284)
[2025-01-06 01:09:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:32][root][INFO] - Training Epoch: 2/10, step 349/574 completed (loss: 0.8372288346290588, acc: 0.7777777910232544)
[2025-01-06 01:09:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:32][root][INFO] - Training Epoch: 2/10, step 350/574 completed (loss: 0.7765555381774902, acc: 0.6744186282157898)
[2025-01-06 01:09:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:32][root][INFO] - Training Epoch: 2/10, step 351/574 completed (loss: 0.2054610550403595, acc: 0.9743589758872986)
[2025-01-06 01:09:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:33][root][INFO] - Training Epoch: 2/10, step 352/574 completed (loss: 0.9189299941062927, acc: 0.7111111283302307)
[2025-01-06 01:09:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:33][root][INFO] - Training Epoch: 2/10, step 353/574 completed (loss: 0.03860524669289589, acc: 1.0)
[2025-01-06 01:09:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:33][root][INFO] - Training Epoch: 2/10, step 354/574 completed (loss: 0.5844686627388, acc: 0.807692289352417)
[2025-01-06 01:09:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:34][root][INFO] - Training Epoch: 2/10, step 355/574 completed (loss: 0.8578941226005554, acc: 0.7692307829856873)
[2025-01-06 01:09:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:34][root][INFO] - Training Epoch: 2/10, step 356/574 completed (loss: 0.7219026684761047, acc: 0.791304349899292)
[2025-01-06 01:09:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:34][root][INFO] - Training Epoch: 2/10, step 357/574 completed (loss: 0.6226112246513367, acc: 0.804347813129425)
[2025-01-06 01:09:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:35][root][INFO] - Training Epoch: 2/10, step 358/574 completed (loss: 0.765417218208313, acc: 0.7551020383834839)
[2025-01-06 01:09:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:35][root][INFO] - Training Epoch: 2/10, step 359/574 completed (loss: 0.009050541557371616, acc: 1.0)
[2025-01-06 01:09:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:35][root][INFO] - Training Epoch: 2/10, step 360/574 completed (loss: 0.30224791169166565, acc: 0.9230769276618958)
[2025-01-06 01:09:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:36][root][INFO] - Training Epoch: 2/10, step 361/574 completed (loss: 0.4699779152870178, acc: 0.8536585569381714)
[2025-01-06 01:09:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:36][root][INFO] - Training Epoch: 2/10, step 362/574 completed (loss: 0.2768059968948364, acc: 0.9333333373069763)
[2025-01-06 01:09:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:36][root][INFO] - Training Epoch: 2/10, step 363/574 completed (loss: 0.1592438519001007, acc: 0.9736841917037964)
[2025-01-06 01:09:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:37][root][INFO] - Training Epoch: 2/10, step 364/574 completed (loss: 0.24650222063064575, acc: 0.8780487775802612)
[2025-01-06 01:09:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:37][root][INFO] - Training Epoch: 2/10, step 365/574 completed (loss: 0.18801447749137878, acc: 0.939393937587738)
[2025-01-06 01:09:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:38][root][INFO] - Training Epoch: 2/10, step 366/574 completed (loss: 0.021567748859524727, acc: 1.0)
[2025-01-06 01:09:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:38][root][INFO] - Training Epoch: 2/10, step 367/574 completed (loss: 0.05651022121310234, acc: 1.0)
[2025-01-06 01:09:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:38][root][INFO] - Training Epoch: 2/10, step 368/574 completed (loss: 0.22789618372917175, acc: 0.9642857313156128)
[2025-01-06 01:09:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:39][root][INFO] - Training Epoch: 2/10, step 369/574 completed (loss: 0.21620382368564606, acc: 0.90625)
[2025-01-06 01:09:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:39][root][INFO] - Training Epoch: 2/10, step 370/574 completed (loss: 0.45272234082221985, acc: 0.8545454740524292)
[2025-01-06 01:09:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:40][root][INFO] - Training Epoch: 2/10, step 371/574 completed (loss: 0.34736377000808716, acc: 0.8867924809455872)
[2025-01-06 01:09:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:40][root][INFO] - Training Epoch: 2/10, step 372/574 completed (loss: 0.212608203291893, acc: 0.9222221970558167)
[2025-01-06 01:09:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:41][root][INFO] - Training Epoch: 2/10, step 373/574 completed (loss: 0.31225988268852234, acc: 0.9642857313156128)
[2025-01-06 01:09:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:41][root][INFO] - Training Epoch: 2/10, step 374/574 completed (loss: 0.1646706759929657, acc: 0.9142857193946838)
[2025-01-06 01:09:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:41][root][INFO] - Training Epoch: 2/10, step 375/574 completed (loss: 0.003307010279968381, acc: 1.0)
[2025-01-06 01:09:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:42][root][INFO] - Training Epoch: 2/10, step 376/574 completed (loss: 0.01455992367118597, acc: 1.0)
[2025-01-06 01:09:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:42][root][INFO] - Training Epoch: 2/10, step 377/574 completed (loss: 0.21936948597431183, acc: 0.9166666865348816)
[2025-01-06 01:09:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:43][root][INFO] - Training Epoch: 2/10, step 378/574 completed (loss: 0.04033759608864784, acc: 0.9789473414421082)
[2025-01-06 01:09:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:43][root][INFO] - Training Epoch: 2/10, step 379/574 completed (loss: 0.2843441665172577, acc: 0.916167676448822)
[2025-01-06 01:09:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:44][root][INFO] - Training Epoch: 2/10, step 380/574 completed (loss: 0.3678136169910431, acc: 0.9097744226455688)
[2025-01-06 01:09:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:45][root][INFO] - Training Epoch: 2/10, step 381/574 completed (loss: 0.6576180458068848, acc: 0.8342245817184448)
[2025-01-06 01:09:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:45][root][INFO] - Training Epoch: 2/10, step 382/574 completed (loss: 0.11896272748708725, acc: 0.954954981803894)
[2025-01-06 01:09:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:46][root][INFO] - Training Epoch: 2/10, step 383/574 completed (loss: 0.495131254196167, acc: 0.8928571343421936)
[2025-01-06 01:09:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:46][root][INFO] - Training Epoch: 2/10, step 384/574 completed (loss: 0.05110403150320053, acc: 0.9642857313156128)
[2025-01-06 01:09:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:46][root][INFO] - Training Epoch: 2/10, step 385/574 completed (loss: 0.19042661786079407, acc: 0.96875)
[2025-01-06 01:09:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:47][root][INFO] - Training Epoch: 2/10, step 386/574 completed (loss: 0.03064574860036373, acc: 0.9722222089767456)
[2025-01-06 01:09:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:47][root][INFO] - Training Epoch: 2/10, step 387/574 completed (loss: 0.032798610627651215, acc: 0.9736841917037964)
[2025-01-06 01:09:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:47][root][INFO] - Training Epoch: 2/10, step 388/574 completed (loss: 0.0324217863380909, acc: 1.0)
[2025-01-06 01:09:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:48][root][INFO] - Training Epoch: 2/10, step 389/574 completed (loss: 0.009395003318786621, acc: 1.0)
[2025-01-06 01:09:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:48][root][INFO] - Training Epoch: 2/10, step 390/574 completed (loss: 0.2922971248626709, acc: 0.9047619104385376)
[2025-01-06 01:09:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:49][root][INFO] - Training Epoch: 2/10, step 391/574 completed (loss: 1.148522973060608, acc: 0.6666666865348816)
[2025-01-06 01:09:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:49][root][INFO] - Training Epoch: 2/10, step 392/574 completed (loss: 0.9660611748695374, acc: 0.7572815418243408)
[2025-01-06 01:09:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:49][root][INFO] - Training Epoch: 2/10, step 393/574 completed (loss: 1.188594937324524, acc: 0.7720588445663452)
[2025-01-06 01:09:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:50][root][INFO] - Training Epoch: 2/10, step 394/574 completed (loss: 0.8501960635185242, acc: 0.753333330154419)
[2025-01-06 01:09:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:50][root][INFO] - Training Epoch: 2/10, step 395/574 completed (loss: 0.9026190638542175, acc: 0.7430555820465088)
[2025-01-06 01:09:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:51][root][INFO] - Training Epoch: 2/10, step 396/574 completed (loss: 0.6254448294639587, acc: 0.8139534592628479)
[2025-01-06 01:09:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:51][root][INFO] - Training Epoch: 2/10, step 397/574 completed (loss: 0.22602415084838867, acc: 0.875)
[2025-01-06 01:09:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:51][root][INFO] - Training Epoch: 2/10, step 398/574 completed (loss: 0.3454936146736145, acc: 0.8837209343910217)
[2025-01-06 01:09:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:52][root][INFO] - Training Epoch: 2/10, step 399/574 completed (loss: 0.08710908144712448, acc: 1.0)
[2025-01-06 01:09:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:52][root][INFO] - Training Epoch: 2/10, step 400/574 completed (loss: 0.3659229278564453, acc: 0.8823529481887817)
[2025-01-06 01:09:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:53][root][INFO] - Training Epoch: 2/10, step 401/574 completed (loss: 0.712188184261322, acc: 0.8266666531562805)
[2025-01-06 01:09:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:53][root][INFO] - Training Epoch: 2/10, step 402/574 completed (loss: 0.4309138357639313, acc: 0.8787878751754761)
[2025-01-06 01:09:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:53][root][INFO] - Training Epoch: 2/10, step 403/574 completed (loss: 0.40064895153045654, acc: 0.8484848737716675)
[2025-01-06 01:09:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:54][root][INFO] - Training Epoch: 2/10, step 404/574 completed (loss: 0.1417180895805359, acc: 0.9677419066429138)
[2025-01-06 01:09:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:54][root][INFO] - Training Epoch: 2/10, step 405/574 completed (loss: 0.2110268473625183, acc: 0.9259259104728699)
[2025-01-06 01:09:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:54][root][INFO] - Training Epoch: 2/10, step 406/574 completed (loss: 0.17241860926151276, acc: 0.9200000166893005)
[2025-01-06 01:09:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:55][root][INFO] - Training Epoch: 2/10, step 407/574 completed (loss: 0.11225351691246033, acc: 0.9722222089767456)
[2025-01-06 01:09:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:55][root][INFO] - Training Epoch: 2/10, step 408/574 completed (loss: 0.21549008786678314, acc: 0.8888888955116272)
[2025-01-06 01:09:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:55][root][INFO] - Training Epoch: 2/10, step 409/574 completed (loss: 0.10385574400424957, acc: 1.0)
[2025-01-06 01:09:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:56][root][INFO] - Training Epoch: 2/10, step 410/574 completed (loss: 0.11354184150695801, acc: 0.982758641242981)
[2025-01-06 01:09:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:56][root][INFO] - Training Epoch: 2/10, step 411/574 completed (loss: 0.034977514296770096, acc: 1.0)
[2025-01-06 01:09:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:56][root][INFO] - Training Epoch: 2/10, step 412/574 completed (loss: 0.06902731955051422, acc: 1.0)
[2025-01-06 01:09:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:57][root][INFO] - Training Epoch: 2/10, step 413/574 completed (loss: 0.1992989331483841, acc: 0.9696969985961914)
[2025-01-06 01:09:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:57][root][INFO] - Training Epoch: 2/10, step 414/574 completed (loss: 0.04320191219449043, acc: 1.0)
[2025-01-06 01:09:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:57][root][INFO] - Training Epoch: 2/10, step 415/574 completed (loss: 0.41117358207702637, acc: 0.8823529481887817)
[2025-01-06 01:09:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:58][root][INFO] - Training Epoch: 2/10, step 416/574 completed (loss: 0.3048931658267975, acc: 0.8846153616905212)
[2025-01-06 01:09:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:58][root][INFO] - Training Epoch: 2/10, step 417/574 completed (loss: 0.2140517234802246, acc: 0.8888888955116272)
[2025-01-06 01:09:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:58][root][INFO] - Training Epoch: 2/10, step 418/574 completed (loss: 0.29394200444221497, acc: 0.925000011920929)
[2025-01-06 01:09:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:59][root][INFO] - Training Epoch: 2/10, step 419/574 completed (loss: 0.15621694922447205, acc: 0.949999988079071)
[2025-01-06 01:09:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:09:59][root][INFO] - Training Epoch: 2/10, step 420/574 completed (loss: 0.12796294689178467, acc: 1.0)
[2025-01-06 01:09:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:00][root][INFO] - Training Epoch: 2/10, step 421/574 completed (loss: 0.20657728612422943, acc: 0.9666666388511658)
[2025-01-06 01:10:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:00][root][INFO] - Training Epoch: 2/10, step 422/574 completed (loss: 0.30431848764419556, acc: 0.875)
[2025-01-06 01:10:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:00][root][INFO] - Training Epoch: 2/10, step 423/574 completed (loss: 0.4275369942188263, acc: 0.8611111044883728)
[2025-01-06 01:10:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:01][root][INFO] - Training Epoch: 2/10, step 424/574 completed (loss: 0.2572050392627716, acc: 0.9629629850387573)
[2025-01-06 01:10:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:01][root][INFO] - Training Epoch: 2/10, step 425/574 completed (loss: 0.11367017030715942, acc: 0.9696969985961914)
[2025-01-06 01:10:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:01][root][INFO] - Training Epoch: 2/10, step 426/574 completed (loss: 0.011768035590648651, acc: 1.0)
[2025-01-06 01:10:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:32][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.7696, device='cuda:0') eval_epoch_loss=tensor(0.5708, device='cuda:0') eval_epoch_acc=tensor(0.8469, device='cuda:0')
[2025-01-06 01:10:32][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:10:32][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:10:32][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_2_step_427_loss_0.5707646608352661/model.pt
[2025-01-06 01:10:32][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:10:32][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.8469486832618713
[2025-01-06 01:10:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:32][root][INFO] - Training Epoch: 2/10, step 427/574 completed (loss: 0.16360192000865936, acc: 0.9189189076423645)
[2025-01-06 01:10:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:33][root][INFO] - Training Epoch: 2/10, step 428/574 completed (loss: 0.018785851076245308, acc: 1.0)
[2025-01-06 01:10:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:33][root][INFO] - Training Epoch: 2/10, step 429/574 completed (loss: 0.15976406633853912, acc: 0.95652174949646)
[2025-01-06 01:10:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:34][root][INFO] - Training Epoch: 2/10, step 430/574 completed (loss: 0.004332300275564194, acc: 1.0)
[2025-01-06 01:10:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:34][root][INFO] - Training Epoch: 2/10, step 431/574 completed (loss: 0.007757706567645073, acc: 1.0)
[2025-01-06 01:10:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:34][root][INFO] - Training Epoch: 2/10, step 432/574 completed (loss: 0.0825561061501503, acc: 0.95652174949646)
[2025-01-06 01:10:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:35][root][INFO] - Training Epoch: 2/10, step 433/574 completed (loss: 0.28720852732658386, acc: 0.8888888955116272)
[2025-01-06 01:10:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:35][root][INFO] - Training Epoch: 2/10, step 434/574 completed (loss: 0.0033494671806693077, acc: 1.0)
[2025-01-06 01:10:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:35][root][INFO] - Training Epoch: 2/10, step 435/574 completed (loss: 0.0494907908141613, acc: 0.9696969985961914)
[2025-01-06 01:10:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:36][root][INFO] - Training Epoch: 2/10, step 436/574 completed (loss: 0.25067150592803955, acc: 0.9166666865348816)
[2025-01-06 01:10:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:36][root][INFO] - Training Epoch: 2/10, step 437/574 completed (loss: 0.02527969889342785, acc: 1.0)
[2025-01-06 01:10:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:36][root][INFO] - Training Epoch: 2/10, step 438/574 completed (loss: 0.006398588418960571, acc: 1.0)
[2025-01-06 01:10:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:37][root][INFO] - Training Epoch: 2/10, step 439/574 completed (loss: 0.441477507352829, acc: 0.8974359035491943)
[2025-01-06 01:10:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:37][root][INFO] - Training Epoch: 2/10, step 440/574 completed (loss: 0.5450254678726196, acc: 0.8636363744735718)
[2025-01-06 01:10:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:38][root][INFO] - Training Epoch: 2/10, step 441/574 completed (loss: 0.6531332731246948, acc: 0.8080000281333923)
[2025-01-06 01:10:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:38][root][INFO] - Training Epoch: 2/10, step 442/574 completed (loss: 0.80190509557724, acc: 0.7822580933570862)
[2025-01-06 01:10:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:39][root][INFO] - Training Epoch: 2/10, step 443/574 completed (loss: 0.45512062311172485, acc: 0.8706467747688293)
[2025-01-06 01:10:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:39][root][INFO] - Training Epoch: 2/10, step 444/574 completed (loss: 0.13647815585136414, acc: 0.9433962106704712)
[2025-01-06 01:10:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:40][root][INFO] - Training Epoch: 2/10, step 445/574 completed (loss: 0.24489599466323853, acc: 0.9090909361839294)
[2025-01-06 01:10:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:40][root][INFO] - Training Epoch: 2/10, step 446/574 completed (loss: 0.4259825646877289, acc: 0.95652174949646)
[2025-01-06 01:10:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:40][root][INFO] - Training Epoch: 2/10, step 447/574 completed (loss: 0.42165735363960266, acc: 0.9230769276618958)
[2025-01-06 01:10:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:41][root][INFO] - Training Epoch: 2/10, step 448/574 completed (loss: 0.1739359050989151, acc: 0.9642857313156128)
[2025-01-06 01:10:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:41][root][INFO] - Training Epoch: 2/10, step 449/574 completed (loss: 0.135550394654274, acc: 0.9850746393203735)
[2025-01-06 01:10:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:41][root][INFO] - Training Epoch: 2/10, step 450/574 completed (loss: 0.09972956776618958, acc: 0.9861111044883728)
[2025-01-06 01:10:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:42][root][INFO] - Training Epoch: 2/10, step 451/574 completed (loss: 0.08823937177658081, acc: 0.95652174949646)
[2025-01-06 01:10:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:42][root][INFO] - Training Epoch: 2/10, step 452/574 completed (loss: 0.17861062288284302, acc: 0.9358974099159241)
[2025-01-06 01:10:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:42][root][INFO] - Training Epoch: 2/10, step 453/574 completed (loss: 0.2765738070011139, acc: 0.9078947305679321)
[2025-01-06 01:10:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:43][root][INFO] - Training Epoch: 2/10, step 454/574 completed (loss: 0.1136411651968956, acc: 0.9591836929321289)
[2025-01-06 01:10:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:43][root][INFO] - Training Epoch: 2/10, step 455/574 completed (loss: 0.18896369636058807, acc: 0.939393937587738)
[2025-01-06 01:10:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:44][root][INFO] - Training Epoch: 2/10, step 456/574 completed (loss: 0.6199153065681458, acc: 0.8453608155250549)
[2025-01-06 01:10:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:44][root][INFO] - Training Epoch: 2/10, step 457/574 completed (loss: 0.028197135776281357, acc: 0.9857142567634583)
[2025-01-06 01:10:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:44][root][INFO] - Training Epoch: 2/10, step 458/574 completed (loss: 0.32925671339035034, acc: 0.9069767594337463)
[2025-01-06 01:10:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:45][root][INFO] - Training Epoch: 2/10, step 459/574 completed (loss: 0.04699961096048355, acc: 1.0)
[2025-01-06 01:10:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:45][root][INFO] - Training Epoch: 2/10, step 460/574 completed (loss: 0.22589589655399323, acc: 0.9382715821266174)
[2025-01-06 01:10:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:45][root][INFO] - Training Epoch: 2/10, step 461/574 completed (loss: 0.3075620234012604, acc: 0.8611111044883728)
[2025-01-06 01:10:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:46][root][INFO] - Training Epoch: 2/10, step 462/574 completed (loss: 0.07157162576913834, acc: 1.0)
[2025-01-06 01:10:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:46][root][INFO] - Training Epoch: 2/10, step 463/574 completed (loss: 0.5584648847579956, acc: 0.8846153616905212)
[2025-01-06 01:10:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:46][root][INFO] - Training Epoch: 2/10, step 464/574 completed (loss: 0.24493679404258728, acc: 0.9347826242446899)
[2025-01-06 01:10:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:47][root][INFO] - Training Epoch: 2/10, step 465/574 completed (loss: 0.3388485312461853, acc: 0.8928571343421936)
[2025-01-06 01:10:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:47][root][INFO] - Training Epoch: 2/10, step 466/574 completed (loss: 0.4714629054069519, acc: 0.8554216623306274)
[2025-01-06 01:10:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:47][root][INFO] - Training Epoch: 2/10, step 467/574 completed (loss: 0.2064892202615738, acc: 0.954954981803894)
[2025-01-06 01:10:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:48][root][INFO] - Training Epoch: 2/10, step 468/574 completed (loss: 0.6410931944847107, acc: 0.8543689250946045)
[2025-01-06 01:10:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:48][root][INFO] - Training Epoch: 2/10, step 469/574 completed (loss: 0.44647130370140076, acc: 0.8943089246749878)
[2025-01-06 01:10:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:48][root][INFO] - Training Epoch: 2/10, step 470/574 completed (loss: 0.14008691906929016, acc: 0.9583333134651184)
[2025-01-06 01:10:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:49][root][INFO] - Training Epoch: 2/10, step 471/574 completed (loss: 0.34323838353157043, acc: 0.8571428656578064)
[2025-01-06 01:10:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:49][root][INFO] - Training Epoch: 2/10, step 472/574 completed (loss: 0.6213769316673279, acc: 0.813725471496582)
[2025-01-06 01:10:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:50][root][INFO] - Training Epoch: 2/10, step 473/574 completed (loss: 0.74852454662323, acc: 0.7903929948806763)
[2025-01-06 01:10:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:50][root][INFO] - Training Epoch: 2/10, step 474/574 completed (loss: 0.6829695701599121, acc: 0.8333333134651184)
[2025-01-06 01:10:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:50][root][INFO] - Training Epoch: 2/10, step 475/574 completed (loss: 0.45539167523384094, acc: 0.8527607321739197)
[2025-01-06 01:10:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:51][root][INFO] - Training Epoch: 2/10, step 476/574 completed (loss: 0.3955043852329254, acc: 0.8920863270759583)
[2025-01-06 01:10:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:51][root][INFO] - Training Epoch: 2/10, step 477/574 completed (loss: 0.8844289183616638, acc: 0.713567852973938)
[2025-01-06 01:10:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:51][root][INFO] - Training Epoch: 2/10, step 478/574 completed (loss: 0.548839807510376, acc: 0.8055555820465088)
[2025-01-06 01:10:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:52][root][INFO] - Training Epoch: 2/10, step 479/574 completed (loss: 0.5780535936355591, acc: 0.8484848737716675)
[2025-01-06 01:10:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:52][root][INFO] - Training Epoch: 2/10, step 480/574 completed (loss: 0.2392561137676239, acc: 0.8518518805503845)
[2025-01-06 01:10:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:52][root][INFO] - Training Epoch: 2/10, step 481/574 completed (loss: 0.36581939458847046, acc: 0.8999999761581421)
[2025-01-06 01:10:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:53][root][INFO] - Training Epoch: 2/10, step 482/574 completed (loss: 0.5579794645309448, acc: 0.800000011920929)
[2025-01-06 01:10:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:53][root][INFO] - Training Epoch: 2/10, step 483/574 completed (loss: 0.6921526193618774, acc: 0.7931034564971924)
[2025-01-06 01:10:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:53][root][INFO] - Training Epoch: 2/10, step 484/574 completed (loss: 0.30977025628089905, acc: 0.9354838728904724)
[2025-01-06 01:10:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:54][root][INFO] - Training Epoch: 2/10, step 485/574 completed (loss: 0.34942832589149475, acc: 0.8947368264198303)
[2025-01-06 01:10:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:54][root][INFO] - Training Epoch: 2/10, step 486/574 completed (loss: 0.644447386264801, acc: 0.7407407164573669)
[2025-01-06 01:10:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:54][root][INFO] - Training Epoch: 2/10, step 487/574 completed (loss: 0.43753406405448914, acc: 0.9047619104385376)
[2025-01-06 01:10:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:55][root][INFO] - Training Epoch: 2/10, step 488/574 completed (loss: 0.634667158126831, acc: 0.7727272510528564)
[2025-01-06 01:10:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:55][root][INFO] - Training Epoch: 2/10, step 489/574 completed (loss: 0.9891632199287415, acc: 0.7384615540504456)
[2025-01-06 01:10:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:56][root][INFO] - Training Epoch: 2/10, step 490/574 completed (loss: 0.2034786343574524, acc: 0.9666666388511658)
[2025-01-06 01:10:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:56][root][INFO] - Training Epoch: 2/10, step 491/574 completed (loss: 0.5632674694061279, acc: 0.7931034564971924)
[2025-01-06 01:10:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:56][root][INFO] - Training Epoch: 2/10, step 492/574 completed (loss: 0.5830202698707581, acc: 0.8039215803146362)
[2025-01-06 01:10:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:57][root][INFO] - Training Epoch: 2/10, step 493/574 completed (loss: 0.35877758264541626, acc: 0.8620689511299133)
[2025-01-06 01:10:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:57][root][INFO] - Training Epoch: 2/10, step 494/574 completed (loss: 0.35375332832336426, acc: 0.9473684430122375)
[2025-01-06 01:10:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:57][root][INFO] - Training Epoch: 2/10, step 495/574 completed (loss: 0.971849262714386, acc: 0.7894737124443054)
[2025-01-06 01:10:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:58][root][INFO] - Training Epoch: 2/10, step 496/574 completed (loss: 0.607971727848053, acc: 0.8125)
[2025-01-06 01:10:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:58][root][INFO] - Training Epoch: 2/10, step 497/574 completed (loss: 0.36669304966926575, acc: 0.8876404762268066)
[2025-01-06 01:10:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:58][root][INFO] - Training Epoch: 2/10, step 498/574 completed (loss: 0.817624032497406, acc: 0.7528089880943298)
[2025-01-06 01:10:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:59][root][INFO] - Training Epoch: 2/10, step 499/574 completed (loss: 1.1368296146392822, acc: 0.6382978558540344)
[2025-01-06 01:10:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:59][root][INFO] - Training Epoch: 2/10, step 500/574 completed (loss: 0.6681164503097534, acc: 0.8369565010070801)
[2025-01-06 01:10:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:10:59][root][INFO] - Training Epoch: 2/10, step 501/574 completed (loss: 0.013707300648093224, acc: 1.0)
[2025-01-06 01:11:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:00][root][INFO] - Training Epoch: 2/10, step 502/574 completed (loss: 0.08924949914216995, acc: 0.9615384340286255)
[2025-01-06 01:11:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:00][root][INFO] - Training Epoch: 2/10, step 503/574 completed (loss: 0.13567867875099182, acc: 0.9259259104728699)
[2025-01-06 01:11:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:00][root][INFO] - Training Epoch: 2/10, step 504/574 completed (loss: 0.10835915803909302, acc: 1.0)
[2025-01-06 01:11:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:01][root][INFO] - Training Epoch: 2/10, step 505/574 completed (loss: 0.5379522442817688, acc: 0.8867924809455872)
[2025-01-06 01:11:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:01][root][INFO] - Training Epoch: 2/10, step 506/574 completed (loss: 0.6210573315620422, acc: 0.7931034564971924)
[2025-01-06 01:11:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:02][root][INFO] - Training Epoch: 2/10, step 507/574 completed (loss: 0.9512720108032227, acc: 0.7567567825317383)
[2025-01-06 01:11:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:02][root][INFO] - Training Epoch: 2/10, step 508/574 completed (loss: 0.7340131402015686, acc: 0.7605633735656738)
[2025-01-06 01:11:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:02][root][INFO] - Training Epoch: 2/10, step 509/574 completed (loss: 0.1591266691684723, acc: 0.949999988079071)
[2025-01-06 01:11:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:03][root][INFO] - Training Epoch: 2/10, step 510/574 completed (loss: 0.29675599932670593, acc: 0.8999999761581421)
[2025-01-06 01:11:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:03][root][INFO] - Training Epoch: 2/10, step 511/574 completed (loss: 0.5315117835998535, acc: 0.807692289352417)
[2025-01-06 01:11:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:06][root][INFO] - Training Epoch: 2/10, step 512/574 completed (loss: 0.990842342376709, acc: 0.699999988079071)
[2025-01-06 01:11:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:07][root][INFO] - Training Epoch: 2/10, step 513/574 completed (loss: 0.22422708570957184, acc: 0.9047619104385376)
[2025-01-06 01:11:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:07][root][INFO] - Training Epoch: 2/10, step 514/574 completed (loss: 0.5630789995193481, acc: 0.8214285969734192)
[2025-01-06 01:11:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:07][root][INFO] - Training Epoch: 2/10, step 515/574 completed (loss: 0.05975300073623657, acc: 1.0)
[2025-01-06 01:11:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:08][root][INFO] - Training Epoch: 2/10, step 516/574 completed (loss: 0.5119724273681641, acc: 0.8611111044883728)
[2025-01-06 01:11:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:09][root][INFO] - Training Epoch: 2/10, step 517/574 completed (loss: 0.01990935392677784, acc: 1.0)
[2025-01-06 01:11:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:09][root][INFO] - Training Epoch: 2/10, step 518/574 completed (loss: 0.046315424144268036, acc: 1.0)
[2025-01-06 01:11:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:09][root][INFO] - Training Epoch: 2/10, step 519/574 completed (loss: 0.2906058430671692, acc: 0.949999988079071)
[2025-01-06 01:11:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:10][root][INFO] - Training Epoch: 2/10, step 520/574 completed (loss: 0.5301843285560608, acc: 0.8148148059844971)
[2025-01-06 01:11:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:11][root][INFO] - Training Epoch: 2/10, step 521/574 completed (loss: 0.6161277294158936, acc: 0.805084764957428)
[2025-01-06 01:11:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:11][root][INFO] - Training Epoch: 2/10, step 522/574 completed (loss: 0.223541259765625, acc: 0.9477611780166626)
[2025-01-06 01:11:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:11][root][INFO] - Training Epoch: 2/10, step 523/574 completed (loss: 0.3892118036746979, acc: 0.9051094651222229)
[2025-01-06 01:11:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:12][root][INFO] - Training Epoch: 2/10, step 524/574 completed (loss: 0.631954550743103, acc: 0.8550000190734863)
[2025-01-06 01:11:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:12][root][INFO] - Training Epoch: 2/10, step 525/574 completed (loss: 0.07950883358716965, acc: 0.9629629850387573)
[2025-01-06 01:11:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:13][root][INFO] - Training Epoch: 2/10, step 526/574 completed (loss: 0.17438122630119324, acc: 0.942307710647583)
[2025-01-06 01:11:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:13][root][INFO] - Training Epoch: 2/10, step 527/574 completed (loss: 0.3513564467430115, acc: 0.9047619104385376)
[2025-01-06 01:11:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:13][root][INFO] - Training Epoch: 2/10, step 528/574 completed (loss: 1.5616508722305298, acc: 0.5737704634666443)
[2025-01-06 01:11:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:14][root][INFO] - Training Epoch: 2/10, step 529/574 completed (loss: 0.3881109952926636, acc: 0.8644067645072937)
[2025-01-06 01:11:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:14][root][INFO] - Training Epoch: 2/10, step 530/574 completed (loss: 1.0549105405807495, acc: 0.6744186282157898)
[2025-01-06 01:11:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:14][root][INFO] - Training Epoch: 2/10, step 531/574 completed (loss: 0.6448480486869812, acc: 0.7954545617103577)
[2025-01-06 01:11:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:15][root][INFO] - Training Epoch: 2/10, step 532/574 completed (loss: 0.8574753999710083, acc: 0.7735849022865295)
[2025-01-06 01:11:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:15][root][INFO] - Training Epoch: 2/10, step 533/574 completed (loss: 0.680187463760376, acc: 0.8409090638160706)
[2025-01-06 01:11:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:16][root][INFO] - Training Epoch: 2/10, step 534/574 completed (loss: 0.34320107102394104, acc: 0.8799999952316284)
[2025-01-06 01:11:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:16][root][INFO] - Training Epoch: 2/10, step 535/574 completed (loss: 0.472426176071167, acc: 0.8999999761581421)
[2025-01-06 01:11:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:16][root][INFO] - Training Epoch: 2/10, step 536/574 completed (loss: 0.14588388800621033, acc: 0.9545454382896423)
[2025-01-06 01:11:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:17][root][INFO] - Training Epoch: 2/10, step 537/574 completed (loss: 0.6489800810813904, acc: 0.8307692408561707)
[2025-01-06 01:11:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:17][root][INFO] - Training Epoch: 2/10, step 538/574 completed (loss: 0.40987372398376465, acc: 0.875)
[2025-01-06 01:11:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:17][root][INFO] - Training Epoch: 2/10, step 539/574 completed (loss: 0.4687007963657379, acc: 0.78125)
[2025-01-06 01:11:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:18][root][INFO] - Training Epoch: 2/10, step 540/574 completed (loss: 0.5609972476959229, acc: 0.7878788113594055)
[2025-01-06 01:11:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:18][root][INFO] - Training Epoch: 2/10, step 541/574 completed (loss: 0.2510058581829071, acc: 0.9375)
[2025-01-06 01:11:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:18][root][INFO] - Training Epoch: 2/10, step 542/574 completed (loss: 0.06286849081516266, acc: 0.9677419066429138)
[2025-01-06 01:11:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:19][root][INFO] - Training Epoch: 2/10, step 543/574 completed (loss: 0.021792108193039894, acc: 1.0)
[2025-01-06 01:11:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:19][root][INFO] - Training Epoch: 2/10, step 544/574 completed (loss: 0.07390976697206497, acc: 0.9666666388511658)
[2025-01-06 01:11:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:20][root][INFO] - Training Epoch: 2/10, step 545/574 completed (loss: 0.1422467976808548, acc: 0.9512194991111755)
[2025-01-06 01:11:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:20][root][INFO] - Training Epoch: 2/10, step 546/574 completed (loss: 0.012496347539126873, acc: 1.0)
[2025-01-06 01:11:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:20][root][INFO] - Training Epoch: 2/10, step 547/574 completed (loss: 0.04954984039068222, acc: 0.9736841917037964)
[2025-01-06 01:11:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:21][root][INFO] - Training Epoch: 2/10, step 548/574 completed (loss: 0.17968522012233734, acc: 0.9677419066429138)
[2025-01-06 01:11:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:21][root][INFO] - Training Epoch: 2/10, step 549/574 completed (loss: 0.0020025044213980436, acc: 1.0)
[2025-01-06 01:11:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:21][root][INFO] - Training Epoch: 2/10, step 550/574 completed (loss: 0.27830201387405396, acc: 0.9090909361839294)
[2025-01-06 01:11:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:22][root][INFO] - Training Epoch: 2/10, step 551/574 completed (loss: 0.1304420381784439, acc: 0.925000011920929)
[2025-01-06 01:11:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:22][root][INFO] - Training Epoch: 2/10, step 552/574 completed (loss: 0.16317354142665863, acc: 0.9571428298950195)
[2025-01-06 01:11:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:22][root][INFO] - Training Epoch: 2/10, step 553/574 completed (loss: 0.41737455129623413, acc: 0.8759124279022217)
[2025-01-06 01:11:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:23][root][INFO] - Training Epoch: 2/10, step 554/574 completed (loss: 0.17063908278942108, acc: 0.9448275566101074)
[2025-01-06 01:11:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:23][root][INFO] - Training Epoch: 2/10, step 555/574 completed (loss: 0.2852111756801605, acc: 0.9428571462631226)
[2025-01-06 01:11:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:23][root][INFO] - Training Epoch: 2/10, step 556/574 completed (loss: 0.39311403036117554, acc: 0.9072847962379456)
[2025-01-06 01:11:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:24][root][INFO] - Training Epoch: 2/10, step 557/574 completed (loss: 0.19363617897033691, acc: 0.9316239356994629)
[2025-01-06 01:11:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:24][root][INFO] - Training Epoch: 2/10, step 558/574 completed (loss: 0.14645683765411377, acc: 0.9599999785423279)
[2025-01-06 01:11:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:24][root][INFO] - Training Epoch: 2/10, step 559/574 completed (loss: 0.12222567200660706, acc: 0.9615384340286255)
[2025-01-06 01:11:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:25][root][INFO] - Training Epoch: 2/10, step 560/574 completed (loss: 0.02235976979136467, acc: 1.0)
[2025-01-06 01:11:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:25][root][INFO] - Training Epoch: 2/10, step 561/574 completed (loss: 0.16384169459342957, acc: 0.9487179517745972)
[2025-01-06 01:11:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:25][root][INFO] - Training Epoch: 2/10, step 562/574 completed (loss: 0.49335387349128723, acc: 0.8888888955116272)
[2025-01-06 01:11:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:26][root][INFO] - Training Epoch: 2/10, step 563/574 completed (loss: 0.42917200922966003, acc: 0.8961039185523987)
[2025-01-06 01:11:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:26][root][INFO] - Training Epoch: 2/10, step 564/574 completed (loss: 0.20779003202915192, acc: 0.9375)
[2025-01-06 01:11:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:26][root][INFO] - Training Epoch: 2/10, step 565/574 completed (loss: 0.24889878928661346, acc: 0.931034505367279)
[2025-01-06 01:11:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:27][root][INFO] - Training Epoch: 2/10, step 566/574 completed (loss: 0.2689167559146881, acc: 0.9285714030265808)
[2025-01-06 01:11:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:27][root][INFO] - Training Epoch: 2/10, step 567/574 completed (loss: 0.02881295420229435, acc: 1.0)
[2025-01-06 01:11:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:27][root][INFO] - Training Epoch: 2/10, step 568/574 completed (loss: 0.04308350384235382, acc: 1.0)
[2025-01-06 01:11:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:28][root][INFO] - Training Epoch: 2/10, step 569/574 completed (loss: 0.15379218757152557, acc: 0.9625668525695801)
[2025-01-06 01:11:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:58][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.8453, device='cuda:0') eval_epoch_loss=tensor(0.6126, device='cuda:0') eval_epoch_acc=tensor(0.8393, device='cuda:0')
[2025-01-06 01:11:58][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:11:58][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:11:58][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_2_step_570_loss_0.6126416325569153/model.pt
[2025-01-06 01:11:58][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:11:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:59][root][INFO] - Training Epoch: 2/10, step 570/574 completed (loss: 0.012046636082231998, acc: 1.0)
[2025-01-06 01:11:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:59][root][INFO] - Training Epoch: 2/10, step 571/574 completed (loss: 0.14596819877624512, acc: 0.9316239356994629)
[2025-01-06 01:11:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:11:59][root][INFO] - Training Epoch: 2/10, step 572/574 completed (loss: 0.4620581567287445, acc: 0.8571428656578064)
[2025-01-06 01:11:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:00][root][INFO] - Training Epoch: 2/10, step 573/574 completed (loss: 0.37414711713790894, acc: 0.9182389974594116)
[2025-01-06 01:12:00][slam_llm.utils.train_utils][INFO] - Epoch 2: train_perplexity=1.7165, train_epoch_loss=0.5403, epoch time 355.22854625433683s
[2025-01-06 01:12:00][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 12 GB
[2025-01-06 01:12:00][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 15 GB
[2025-01-06 01:12:00][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 12 GB
[2025-01-06 01:12:00][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 6
[2025-01-06 01:12:00][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2025-01-06 01:12:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:01][root][INFO] - Training Epoch: 3/10, step 0/574 completed (loss: 0.2060568481683731, acc: 0.8888888955116272)
[2025-01-06 01:12:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:01][root][INFO] - Training Epoch: 3/10, step 1/574 completed (loss: 0.3444031774997711, acc: 0.9200000166893005)
[2025-01-06 01:12:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:01][root][INFO] - Training Epoch: 3/10, step 2/574 completed (loss: 0.893118679523468, acc: 0.7837837934494019)
[2025-01-06 01:12:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:02][root][INFO] - Training Epoch: 3/10, step 3/574 completed (loss: 0.36575546860694885, acc: 0.9210526347160339)
[2025-01-06 01:12:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:02][root][INFO] - Training Epoch: 3/10, step 4/574 completed (loss: 0.44873690605163574, acc: 0.8918918967247009)
[2025-01-06 01:12:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:02][root][INFO] - Training Epoch: 3/10, step 5/574 completed (loss: 0.14844803512096405, acc: 0.9642857313156128)
[2025-01-06 01:12:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:03][root][INFO] - Training Epoch: 3/10, step 6/574 completed (loss: 0.7344887256622314, acc: 0.7755101919174194)
[2025-01-06 01:12:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:03][root][INFO] - Training Epoch: 3/10, step 7/574 completed (loss: 0.2743644416332245, acc: 0.8999999761581421)
[2025-01-06 01:12:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:04][root][INFO] - Training Epoch: 3/10, step 8/574 completed (loss: 0.09698138386011124, acc: 0.9545454382896423)
[2025-01-06 01:12:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:04][root][INFO] - Training Epoch: 3/10, step 9/574 completed (loss: 0.02853897400200367, acc: 1.0)
[2025-01-06 01:12:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:04][root][INFO] - Training Epoch: 3/10, step 10/574 completed (loss: 0.16746623814105988, acc: 0.9259259104728699)
[2025-01-06 01:12:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:05][root][INFO] - Training Epoch: 3/10, step 11/574 completed (loss: 0.27293771505355835, acc: 0.8974359035491943)
[2025-01-06 01:12:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:05][root][INFO] - Training Epoch: 3/10, step 12/574 completed (loss: 0.02517767809331417, acc: 1.0)
[2025-01-06 01:12:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:05][root][INFO] - Training Epoch: 3/10, step 13/574 completed (loss: 0.19653913378715515, acc: 0.95652174949646)
[2025-01-06 01:12:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:06][root][INFO] - Training Epoch: 3/10, step 14/574 completed (loss: 0.1556161791086197, acc: 0.9607843160629272)
[2025-01-06 01:12:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:06][root][INFO] - Training Epoch: 3/10, step 15/574 completed (loss: 0.42290857434272766, acc: 0.918367326259613)
[2025-01-06 01:12:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:06][root][INFO] - Training Epoch: 3/10, step 16/574 completed (loss: 0.23793257772922516, acc: 0.8947368264198303)
[2025-01-06 01:12:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:07][root][INFO] - Training Epoch: 3/10, step 17/574 completed (loss: 0.1575743854045868, acc: 0.9166666865348816)
[2025-01-06 01:12:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:07][root][INFO] - Training Epoch: 3/10, step 18/574 completed (loss: 0.20022082328796387, acc: 0.9166666865348816)
[2025-01-06 01:12:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:07][root][INFO] - Training Epoch: 3/10, step 19/574 completed (loss: 0.12335749715566635, acc: 0.9473684430122375)
[2025-01-06 01:12:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:08][root][INFO] - Training Epoch: 3/10, step 20/574 completed (loss: 0.08519307523965836, acc: 0.9615384340286255)
[2025-01-06 01:12:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:08][root][INFO] - Training Epoch: 3/10, step 21/574 completed (loss: 0.18633291125297546, acc: 0.931034505367279)
[2025-01-06 01:12:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:08][root][INFO] - Training Epoch: 3/10, step 22/574 completed (loss: 0.46786361932754517, acc: 0.8799999952316284)
[2025-01-06 01:12:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:09][root][INFO] - Training Epoch: 3/10, step 23/574 completed (loss: 0.9079558849334717, acc: 0.8095238208770752)
[2025-01-06 01:12:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:09][root][INFO] - Training Epoch: 3/10, step 24/574 completed (loss: 0.22600293159484863, acc: 0.875)
[2025-01-06 01:12:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:10][root][INFO] - Training Epoch: 3/10, step 25/574 completed (loss: 0.46493202447891235, acc: 0.849056601524353)
[2025-01-06 01:12:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:10][root][INFO] - Training Epoch: 3/10, step 26/574 completed (loss: 0.6114639639854431, acc: 0.8219178318977356)
[2025-01-06 01:12:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:11][root][INFO] - Training Epoch: 3/10, step 27/574 completed (loss: 0.7543997764587402, acc: 0.7865612506866455)
[2025-01-06 01:12:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:12][root][INFO] - Training Epoch: 3/10, step 28/574 completed (loss: 0.3116782605648041, acc: 0.930232584476471)
[2025-01-06 01:12:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:12][root][INFO] - Training Epoch: 3/10, step 29/574 completed (loss: 0.481818825006485, acc: 0.8313252925872803)
[2025-01-06 01:12:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:12][root][INFO] - Training Epoch: 3/10, step 30/574 completed (loss: 0.4632972478866577, acc: 0.8641975522041321)
[2025-01-06 01:12:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:13][root][INFO] - Training Epoch: 3/10, step 31/574 completed (loss: 0.3388075828552246, acc: 0.8571428656578064)
[2025-01-06 01:12:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:13][root][INFO] - Training Epoch: 3/10, step 32/574 completed (loss: 0.36489996314048767, acc: 0.8888888955116272)
[2025-01-06 01:12:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:13][root][INFO] - Training Epoch: 3/10, step 33/574 completed (loss: 0.03720909357070923, acc: 1.0)
[2025-01-06 01:12:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:14][root][INFO] - Training Epoch: 3/10, step 34/574 completed (loss: 0.5407080054283142, acc: 0.831932783126831)
[2025-01-06 01:12:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:14][root][INFO] - Training Epoch: 3/10, step 35/574 completed (loss: 0.2605383098125458, acc: 0.9016393423080444)
[2025-01-06 01:12:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:15][root][INFO] - Training Epoch: 3/10, step 36/574 completed (loss: 0.34652838110923767, acc: 0.8888888955116272)
[2025-01-06 01:12:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:15][root][INFO] - Training Epoch: 3/10, step 37/574 completed (loss: 0.4994460940361023, acc: 0.8983050584793091)
[2025-01-06 01:12:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:15][root][INFO] - Training Epoch: 3/10, step 38/574 completed (loss: 0.3342307507991791, acc: 0.8735632300376892)
[2025-01-06 01:12:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:16][root][INFO] - Training Epoch: 3/10, step 39/574 completed (loss: 0.17400896549224854, acc: 0.9047619104385376)
[2025-01-06 01:12:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:16][root][INFO] - Training Epoch: 3/10, step 40/574 completed (loss: 0.2716866433620453, acc: 0.8846153616905212)
[2025-01-06 01:12:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:16][root][INFO] - Training Epoch: 3/10, step 41/574 completed (loss: 0.23428712785243988, acc: 0.9189189076423645)
[2025-01-06 01:12:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:17][root][INFO] - Training Epoch: 3/10, step 42/574 completed (loss: 0.37503549456596375, acc: 0.8615384697914124)
[2025-01-06 01:12:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:17][root][INFO] - Training Epoch: 3/10, step 43/574 completed (loss: 0.5182538032531738, acc: 0.8787878751754761)
[2025-01-06 01:12:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:18][root][INFO] - Training Epoch: 3/10, step 44/574 completed (loss: 0.36680150032043457, acc: 0.907216489315033)
[2025-01-06 01:12:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:18][root][INFO] - Training Epoch: 3/10, step 45/574 completed (loss: 0.33621150255203247, acc: 0.904411792755127)
[2025-01-06 01:12:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:18][root][INFO] - Training Epoch: 3/10, step 46/574 completed (loss: 0.2225184589624405, acc: 0.9230769276618958)
[2025-01-06 01:12:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:19][root][INFO] - Training Epoch: 3/10, step 47/574 completed (loss: 0.32145389914512634, acc: 0.9259259104728699)
[2025-01-06 01:12:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:19][root][INFO] - Training Epoch: 3/10, step 48/574 completed (loss: 0.18705371022224426, acc: 0.9642857313156128)
[2025-01-06 01:12:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:19][root][INFO] - Training Epoch: 3/10, step 49/574 completed (loss: 0.024326032027602196, acc: 1.0)
[2025-01-06 01:12:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:20][root][INFO] - Training Epoch: 3/10, step 50/574 completed (loss: 0.43142732977867126, acc: 0.859649121761322)
[2025-01-06 01:12:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:20][root][INFO] - Training Epoch: 3/10, step 51/574 completed (loss: 0.6148939728736877, acc: 0.8253968358039856)
[2025-01-06 01:12:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:20][root][INFO] - Training Epoch: 3/10, step 52/574 completed (loss: 0.7232410311698914, acc: 0.7746478915214539)
[2025-01-06 01:12:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:21][root][INFO] - Training Epoch: 3/10, step 53/574 completed (loss: 1.3271900415420532, acc: 0.5666666626930237)
[2025-01-06 01:12:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:21][root][INFO] - Training Epoch: 3/10, step 54/574 completed (loss: 0.5265825390815735, acc: 0.8648648858070374)
[2025-01-06 01:12:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:22][root][INFO] - Training Epoch: 3/10, step 55/574 completed (loss: 0.040178172290325165, acc: 1.0)
[2025-01-06 01:12:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:25][root][INFO] - Training Epoch: 3/10, step 56/574 completed (loss: 1.2532460689544678, acc: 0.6484641432762146)
[2025-01-06 01:12:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:26][root][INFO] - Training Epoch: 3/10, step 57/574 completed (loss: 1.0612987279891968, acc: 0.6971677541732788)
[2025-01-06 01:12:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:26][root][INFO] - Training Epoch: 3/10, step 58/574 completed (loss: 0.6549302935600281, acc: 0.8068181872367859)
[2025-01-06 01:12:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:27][root][INFO] - Training Epoch: 3/10, step 59/574 completed (loss: 0.20511168241500854, acc: 0.9558823704719543)
[2025-01-06 01:12:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:28][root][INFO] - Training Epoch: 3/10, step 60/574 completed (loss: 0.7426593899726868, acc: 0.804347813129425)
[2025-01-06 01:12:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:28][root][INFO] - Training Epoch: 3/10, step 61/574 completed (loss: 0.4510098397731781, acc: 0.862500011920929)
[2025-01-06 01:12:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:28][root][INFO] - Training Epoch: 3/10, step 62/574 completed (loss: 0.3077095150947571, acc: 0.9411764740943909)
[2025-01-06 01:12:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:29][root][INFO] - Training Epoch: 3/10, step 63/574 completed (loss: 0.24751795828342438, acc: 0.9444444179534912)
[2025-01-06 01:12:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:29][root][INFO] - Training Epoch: 3/10, step 64/574 completed (loss: 0.08065885305404663, acc: 0.96875)
[2025-01-06 01:12:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:30][root][INFO] - Training Epoch: 3/10, step 65/574 completed (loss: 0.020372670143842697, acc: 1.0)
[2025-01-06 01:12:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:30][root][INFO] - Training Epoch: 3/10, step 66/574 completed (loss: 0.6039611101150513, acc: 0.8214285969734192)
[2025-01-06 01:12:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:30][root][INFO] - Training Epoch: 3/10, step 67/574 completed (loss: 0.4417625069618225, acc: 0.8500000238418579)
[2025-01-06 01:12:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:31][root][INFO] - Training Epoch: 3/10, step 68/574 completed (loss: 0.05322854965925217, acc: 0.9599999785423279)
[2025-01-06 01:12:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:31][root][INFO] - Training Epoch: 3/10, step 69/574 completed (loss: 0.3476681113243103, acc: 0.8888888955116272)
[2025-01-06 01:12:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:31][root][INFO] - Training Epoch: 3/10, step 70/574 completed (loss: 0.43166399002075195, acc: 0.8787878751754761)
[2025-01-06 01:12:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:32][root][INFO] - Training Epoch: 3/10, step 71/574 completed (loss: 0.819855272769928, acc: 0.7720588445663452)
[2025-01-06 01:12:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:32][root][INFO] - Training Epoch: 3/10, step 72/574 completed (loss: 0.7244407534599304, acc: 0.7857142686843872)
[2025-01-06 01:12:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:32][root][INFO] - Training Epoch: 3/10, step 73/574 completed (loss: 1.1067181825637817, acc: 0.6666666865348816)
[2025-01-06 01:12:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:33][root][INFO] - Training Epoch: 3/10, step 74/574 completed (loss: 0.9553011059761047, acc: 0.7653061151504517)
[2025-01-06 01:12:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:33][root][INFO] - Training Epoch: 3/10, step 75/574 completed (loss: 1.0444265604019165, acc: 0.6940298676490784)
[2025-01-06 01:12:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:33][root][INFO] - Training Epoch: 3/10, step 76/574 completed (loss: 1.2782835960388184, acc: 0.6569343209266663)
[2025-01-06 01:12:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:34][root][INFO] - Training Epoch: 3/10, step 77/574 completed (loss: 0.017382508143782616, acc: 1.0)
[2025-01-06 01:12:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:34][root][INFO] - Training Epoch: 3/10, step 78/574 completed (loss: 0.14397187530994415, acc: 0.9166666865348816)
[2025-01-06 01:12:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:35][root][INFO] - Training Epoch: 3/10, step 79/574 completed (loss: 0.02289547771215439, acc: 1.0)
[2025-01-06 01:12:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:35][root][INFO] - Training Epoch: 3/10, step 80/574 completed (loss: 0.2280210703611374, acc: 0.9230769276618958)
[2025-01-06 01:12:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:35][root][INFO] - Training Epoch: 3/10, step 81/574 completed (loss: 0.5149118900299072, acc: 0.8461538553237915)
[2025-01-06 01:12:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:36][root][INFO] - Training Epoch: 3/10, step 82/574 completed (loss: 0.47302940487861633, acc: 0.8653846383094788)
[2025-01-06 01:12:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:36][root][INFO] - Training Epoch: 3/10, step 83/574 completed (loss: 0.2532682418823242, acc: 0.9375)
[2025-01-06 01:12:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:36][root][INFO] - Training Epoch: 3/10, step 84/574 completed (loss: 0.34420958161354065, acc: 0.8985507488250732)
[2025-01-06 01:12:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:37][root][INFO] - Training Epoch: 3/10, step 85/574 completed (loss: 0.3971780836582184, acc: 0.8799999952316284)
[2025-01-06 01:12:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:37][root][INFO] - Training Epoch: 3/10, step 86/574 completed (loss: 0.18041092157363892, acc: 0.9130434989929199)
[2025-01-06 01:12:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:38][root][INFO] - Training Epoch: 3/10, step 87/574 completed (loss: 0.5913477540016174, acc: 0.8199999928474426)
[2025-01-06 01:12:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:38][root][INFO] - Training Epoch: 3/10, step 88/574 completed (loss: 0.6253039836883545, acc: 0.8155339956283569)
[2025-01-06 01:12:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:39][root][INFO] - Training Epoch: 3/10, step 89/574 completed (loss: 0.7909851670265198, acc: 0.7864077687263489)
[2025-01-06 01:12:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:40][root][INFO] - Training Epoch: 3/10, step 90/574 completed (loss: 0.9507529735565186, acc: 0.7688171863555908)
[2025-01-06 01:12:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:41][root][INFO] - Training Epoch: 3/10, step 91/574 completed (loss: 0.929197371006012, acc: 0.7715517282485962)
[2025-01-06 01:12:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:41][root][INFO] - Training Epoch: 3/10, step 92/574 completed (loss: 0.5900800824165344, acc: 0.8631578683853149)
[2025-01-06 01:12:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:42][root][INFO] - Training Epoch: 3/10, step 93/574 completed (loss: 1.0732629299163818, acc: 0.7128713130950928)
[2025-01-06 01:12:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:43][root][INFO] - Training Epoch: 3/10, step 94/574 completed (loss: 0.8157100081443787, acc: 0.774193525314331)
[2025-01-06 01:12:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:43][root][INFO] - Training Epoch: 3/10, step 95/574 completed (loss: 0.7720286846160889, acc: 0.7681159377098083)
[2025-01-06 01:12:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:44][root][INFO] - Training Epoch: 3/10, step 96/574 completed (loss: 0.988433301448822, acc: 0.7142857313156128)
[2025-01-06 01:12:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:44][root][INFO] - Training Epoch: 3/10, step 97/574 completed (loss: 0.8809146881103516, acc: 0.7403846383094788)
[2025-01-06 01:12:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:44][root][INFO] - Training Epoch: 3/10, step 98/574 completed (loss: 0.9640091061592102, acc: 0.7153284549713135)
[2025-01-06 01:12:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:45][root][INFO] - Training Epoch: 3/10, step 99/574 completed (loss: 1.1693707704544067, acc: 0.6865671873092651)
[2025-01-06 01:12:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:45][root][INFO] - Training Epoch: 3/10, step 100/574 completed (loss: 0.2821902334690094, acc: 0.8999999761581421)
[2025-01-06 01:12:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:45][root][INFO] - Training Epoch: 3/10, step 101/574 completed (loss: 0.010163858532905579, acc: 1.0)
[2025-01-06 01:12:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:46][root][INFO] - Training Epoch: 3/10, step 102/574 completed (loss: 0.05774132162332535, acc: 1.0)
[2025-01-06 01:12:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:46][root][INFO] - Training Epoch: 3/10, step 103/574 completed (loss: 0.02178250625729561, acc: 1.0)
[2025-01-06 01:12:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:46][root][INFO] - Training Epoch: 3/10, step 104/574 completed (loss: 0.3920137584209442, acc: 0.9137930870056152)
[2025-01-06 01:12:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:47][root][INFO] - Training Epoch: 3/10, step 105/574 completed (loss: 0.1100425273180008, acc: 0.9534883499145508)
[2025-01-06 01:12:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:47][root][INFO] - Training Epoch: 3/10, step 106/574 completed (loss: 0.1302783340215683, acc: 0.9599999785423279)
[2025-01-06 01:12:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:47][root][INFO] - Training Epoch: 3/10, step 107/574 completed (loss: 0.007310142740607262, acc: 1.0)
[2025-01-06 01:12:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:48][root][INFO] - Training Epoch: 3/10, step 108/574 completed (loss: 0.015192924998700619, acc: 1.0)
[2025-01-06 01:12:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:48][root][INFO] - Training Epoch: 3/10, step 109/574 completed (loss: 0.021089615300297737, acc: 1.0)
[2025-01-06 01:12:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:49][root][INFO] - Training Epoch: 3/10, step 110/574 completed (loss: 0.10911314934492111, acc: 0.9538461565971375)
[2025-01-06 01:12:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:49][root][INFO] - Training Epoch: 3/10, step 111/574 completed (loss: 0.3213804066181183, acc: 0.8771929740905762)
[2025-01-06 01:12:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:49][root][INFO] - Training Epoch: 3/10, step 112/574 completed (loss: 0.37880903482437134, acc: 0.859649121761322)
[2025-01-06 01:12:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:50][root][INFO] - Training Epoch: 3/10, step 113/574 completed (loss: 0.21338778734207153, acc: 0.9230769276618958)
[2025-01-06 01:12:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:50][root][INFO] - Training Epoch: 3/10, step 114/574 completed (loss: 0.2835119366645813, acc: 0.918367326259613)
[2025-01-06 01:12:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:50][root][INFO] - Training Epoch: 3/10, step 115/574 completed (loss: 0.003633946180343628, acc: 1.0)
[2025-01-06 01:12:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:51][root][INFO] - Training Epoch: 3/10, step 116/574 completed (loss: 0.4722462296485901, acc: 0.8730158805847168)
[2025-01-06 01:12:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:51][root][INFO] - Training Epoch: 3/10, step 117/574 completed (loss: 0.33070749044418335, acc: 0.9024389982223511)
[2025-01-06 01:12:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:51][root][INFO] - Training Epoch: 3/10, step 118/574 completed (loss: 0.13621112704277039, acc: 0.9677419066429138)
[2025-01-06 01:12:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:52][root][INFO] - Training Epoch: 3/10, step 119/574 completed (loss: 0.4486697018146515, acc: 0.8821292519569397)
[2025-01-06 01:12:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:53][root][INFO] - Training Epoch: 3/10, step 120/574 completed (loss: 0.285171777009964, acc: 0.9066666960716248)
[2025-01-06 01:12:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:53][root][INFO] - Training Epoch: 3/10, step 121/574 completed (loss: 0.4452970325946808, acc: 0.8846153616905212)
[2025-01-06 01:12:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:53][root][INFO] - Training Epoch: 3/10, step 122/574 completed (loss: 0.11450314521789551, acc: 0.9583333134651184)
[2025-01-06 01:12:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:54][root][INFO] - Training Epoch: 3/10, step 123/574 completed (loss: 0.2541395127773285, acc: 0.8947368264198303)
[2025-01-06 01:12:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:54][root][INFO] - Training Epoch: 3/10, step 124/574 completed (loss: 0.8013123273849487, acc: 0.7791411280632019)
[2025-01-06 01:12:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:55][root][INFO] - Training Epoch: 3/10, step 125/574 completed (loss: 0.8168716430664062, acc: 0.8125)
[2025-01-06 01:12:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:55][root][INFO] - Training Epoch: 3/10, step 126/574 completed (loss: 1.024716854095459, acc: 0.7166666388511658)
[2025-01-06 01:12:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:55][root][INFO] - Training Epoch: 3/10, step 127/574 completed (loss: 0.5629020929336548, acc: 0.8273809552192688)
[2025-01-06 01:12:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:56][root][INFO] - Training Epoch: 3/10, step 128/574 completed (loss: 0.6331472992897034, acc: 0.8205128312110901)
[2025-01-06 01:12:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:56][root][INFO] - Training Epoch: 3/10, step 129/574 completed (loss: 0.6862581372261047, acc: 0.779411792755127)
[2025-01-06 01:12:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:56][root][INFO] - Training Epoch: 3/10, step 130/574 completed (loss: 0.688994288444519, acc: 0.8461538553237915)
[2025-01-06 01:12:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:57][root][INFO] - Training Epoch: 3/10, step 131/574 completed (loss: 0.26796260476112366, acc: 0.9130434989929199)
[2025-01-06 01:12:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:57][root][INFO] - Training Epoch: 3/10, step 132/574 completed (loss: 0.31386759877204895, acc: 0.90625)
[2025-01-06 01:12:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:57][root][INFO] - Training Epoch: 3/10, step 133/574 completed (loss: 0.3105770945549011, acc: 0.9130434989929199)
[2025-01-06 01:12:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:58][root][INFO] - Training Epoch: 3/10, step 134/574 completed (loss: 0.3560868799686432, acc: 0.8285714387893677)
[2025-01-06 01:12:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:58][root][INFO] - Training Epoch: 3/10, step 135/574 completed (loss: 0.3365468680858612, acc: 0.8461538553237915)
[2025-01-06 01:12:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:58][root][INFO] - Training Epoch: 3/10, step 136/574 completed (loss: 0.4032745957374573, acc: 0.8809523582458496)
[2025-01-06 01:12:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:59][root][INFO] - Training Epoch: 3/10, step 137/574 completed (loss: 0.6619200110435486, acc: 0.7666666507720947)
[2025-01-06 01:12:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:12:59][root][INFO] - Training Epoch: 3/10, step 138/574 completed (loss: 0.11411780118942261, acc: 1.0)
[2025-01-06 01:13:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:30][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.8039, device='cuda:0') eval_epoch_loss=tensor(0.5900, device='cuda:0') eval_epoch_acc=tensor(0.8447, device='cuda:0')
[2025-01-06 01:13:30][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:13:30][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:13:30][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_3_step_139_loss_0.5899622440338135/model.pt
[2025-01-06 01:13:30][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:13:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:30][root][INFO] - Training Epoch: 3/10, step 139/574 completed (loss: 0.027257319539785385, acc: 1.0)
[2025-01-06 01:13:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:31][root][INFO] - Training Epoch: 3/10, step 140/574 completed (loss: 0.32478487491607666, acc: 0.8461538553237915)
[2025-01-06 01:13:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:31][root][INFO] - Training Epoch: 3/10, step 141/574 completed (loss: 0.22934529185295105, acc: 0.9354838728904724)
[2025-01-06 01:13:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:31][root][INFO] - Training Epoch: 3/10, step 142/574 completed (loss: 0.3634032607078552, acc: 0.8918918967247009)
[2025-01-06 01:13:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:32][root][INFO] - Training Epoch: 3/10, step 143/574 completed (loss: 0.5743769407272339, acc: 0.7719298005104065)
[2025-01-06 01:13:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:32][root][INFO] - Training Epoch: 3/10, step 144/574 completed (loss: 0.7099626660346985, acc: 0.7910447716712952)
[2025-01-06 01:13:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:33][root][INFO] - Training Epoch: 3/10, step 145/574 completed (loss: 0.5044105648994446, acc: 0.8571428656578064)
[2025-01-06 01:13:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:33][root][INFO] - Training Epoch: 3/10, step 146/574 completed (loss: 1.1381199359893799, acc: 0.6276595592498779)
[2025-01-06 01:13:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:33][root][INFO] - Training Epoch: 3/10, step 147/574 completed (loss: 0.37314170598983765, acc: 0.8714285492897034)
[2025-01-06 01:13:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:34][root][INFO] - Training Epoch: 3/10, step 148/574 completed (loss: 0.4448532164096832, acc: 0.8214285969734192)
[2025-01-06 01:13:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:34][root][INFO] - Training Epoch: 3/10, step 149/574 completed (loss: 0.43880534172058105, acc: 0.95652174949646)
[2025-01-06 01:13:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:34][root][INFO] - Training Epoch: 3/10, step 150/574 completed (loss: 0.2008540779352188, acc: 0.9655172228813171)
[2025-01-06 01:13:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:35][root][INFO] - Training Epoch: 3/10, step 151/574 completed (loss: 0.6929510235786438, acc: 0.804347813129425)
[2025-01-06 01:13:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:35][root][INFO] - Training Epoch: 3/10, step 152/574 completed (loss: 0.6946059465408325, acc: 0.7966101765632629)
[2025-01-06 01:13:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:35][root][INFO] - Training Epoch: 3/10, step 153/574 completed (loss: 0.5261437296867371, acc: 0.7894737124443054)
[2025-01-06 01:13:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:36][root][INFO] - Training Epoch: 3/10, step 154/574 completed (loss: 0.6511924862861633, acc: 0.7972972989082336)
[2025-01-06 01:13:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:36][root][INFO] - Training Epoch: 3/10, step 155/574 completed (loss: 0.11981165409088135, acc: 0.9285714030265808)
[2025-01-06 01:13:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:36][root][INFO] - Training Epoch: 3/10, step 156/574 completed (loss: 0.4225808382034302, acc: 0.8695651888847351)
[2025-01-06 01:13:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:37][root][INFO] - Training Epoch: 3/10, step 157/574 completed (loss: 1.7317616939544678, acc: 0.42105263471603394)
[2025-01-06 01:13:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:38][root][INFO] - Training Epoch: 3/10, step 158/574 completed (loss: 0.8824605941772461, acc: 0.7432432174682617)
[2025-01-06 01:13:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:38][root][INFO] - Training Epoch: 3/10, step 159/574 completed (loss: 0.9866980910301208, acc: 0.6481481194496155)
[2025-01-06 01:13:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:39][root][INFO] - Training Epoch: 3/10, step 160/574 completed (loss: 1.100540041923523, acc: 0.6860465407371521)
[2025-01-06 01:13:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:39][root][INFO] - Training Epoch: 3/10, step 161/574 completed (loss: 1.1729393005371094, acc: 0.6352941393852234)
[2025-01-06 01:13:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:40][root][INFO] - Training Epoch: 3/10, step 162/574 completed (loss: 1.386168360710144, acc: 0.6516854166984558)
[2025-01-06 01:13:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:40][root][INFO] - Training Epoch: 3/10, step 163/574 completed (loss: 0.31256037950515747, acc: 0.9090909361839294)
[2025-01-06 01:13:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:41][root][INFO] - Training Epoch: 3/10, step 164/574 completed (loss: 0.40952378511428833, acc: 0.9047619104385376)
[2025-01-06 01:13:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:41][root][INFO] - Training Epoch: 3/10, step 165/574 completed (loss: 0.5419397950172424, acc: 0.8620689511299133)
[2025-01-06 01:13:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:41][root][INFO] - Training Epoch: 3/10, step 166/574 completed (loss: 0.10251650214195251, acc: 0.9795918464660645)
[2025-01-06 01:13:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:42][root][INFO] - Training Epoch: 3/10, step 167/574 completed (loss: 0.13694219291210175, acc: 0.9399999976158142)
[2025-01-06 01:13:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:42][root][INFO] - Training Epoch: 3/10, step 168/574 completed (loss: 0.31216728687286377, acc: 0.8888888955116272)
[2025-01-06 01:13:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:42][root][INFO] - Training Epoch: 3/10, step 169/574 completed (loss: 0.9257858991622925, acc: 0.813725471496582)
[2025-01-06 01:13:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:43][root][INFO] - Training Epoch: 3/10, step 170/574 completed (loss: 0.6130539774894714, acc: 0.8219178318977356)
[2025-01-06 01:13:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:44][root][INFO] - Training Epoch: 3/10, step 171/574 completed (loss: 0.09388357400894165, acc: 0.9583333134651184)
[2025-01-06 01:13:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:44][root][INFO] - Training Epoch: 3/10, step 172/574 completed (loss: 0.5607706308364868, acc: 0.8518518805503845)
[2025-01-06 01:13:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:44][root][INFO] - Training Epoch: 3/10, step 173/574 completed (loss: 0.2686103284358978, acc: 0.9642857313156128)
[2025-01-06 01:13:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:45][root][INFO] - Training Epoch: 3/10, step 174/574 completed (loss: 0.9218355417251587, acc: 0.76106196641922)
[2025-01-06 01:13:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:45][root][INFO] - Training Epoch: 3/10, step 175/574 completed (loss: 0.5038160681724548, acc: 0.8985507488250732)
[2025-01-06 01:13:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:46][root][INFO] - Training Epoch: 3/10, step 176/574 completed (loss: 0.5445501804351807, acc: 0.8295454382896423)
[2025-01-06 01:13:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:46][root][INFO] - Training Epoch: 3/10, step 177/574 completed (loss: 1.009447693824768, acc: 0.7404580116271973)
[2025-01-06 01:13:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:47][root][INFO] - Training Epoch: 3/10, step 178/574 completed (loss: 0.7909722924232483, acc: 0.7851851582527161)
[2025-01-06 01:13:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:47][root][INFO] - Training Epoch: 3/10, step 179/574 completed (loss: 0.3483680784702301, acc: 0.868852436542511)
[2025-01-06 01:13:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:48][root][INFO] - Training Epoch: 3/10, step 180/574 completed (loss: 0.008543473668396473, acc: 1.0)
[2025-01-06 01:13:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:48][root][INFO] - Training Epoch: 3/10, step 181/574 completed (loss: 0.09686777740716934, acc: 0.9599999785423279)
[2025-01-06 01:13:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:48][root][INFO] - Training Epoch: 3/10, step 182/574 completed (loss: 0.08640497177839279, acc: 1.0)
[2025-01-06 01:13:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:49][root][INFO] - Training Epoch: 3/10, step 183/574 completed (loss: 0.19964727759361267, acc: 0.9146341681480408)
[2025-01-06 01:13:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:49][root][INFO] - Training Epoch: 3/10, step 184/574 completed (loss: 0.3667992949485779, acc: 0.9093655347824097)
[2025-01-06 01:13:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:49][root][INFO] - Training Epoch: 3/10, step 185/574 completed (loss: 0.4055154621601105, acc: 0.8962535858154297)
[2025-01-06 01:13:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:50][root][INFO] - Training Epoch: 3/10, step 186/574 completed (loss: 0.34985870122909546, acc: 0.887499988079071)
[2025-01-06 01:13:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:50][root][INFO] - Training Epoch: 3/10, step 187/574 completed (loss: 0.4359094202518463, acc: 0.8724202513694763)
[2025-01-06 01:13:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:51][root][INFO] - Training Epoch: 3/10, step 188/574 completed (loss: 0.4626369774341583, acc: 0.8612099885940552)
[2025-01-06 01:13:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:51][root][INFO] - Training Epoch: 3/10, step 189/574 completed (loss: 0.11685898900032043, acc: 1.0)
[2025-01-06 01:13:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:52][root][INFO] - Training Epoch: 3/10, step 190/574 completed (loss: 0.6150932908058167, acc: 0.7790697813034058)
[2025-01-06 01:13:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:53][root][INFO] - Training Epoch: 3/10, step 191/574 completed (loss: 0.9673848748207092, acc: 0.6984127163887024)
[2025-01-06 01:13:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:53][root][INFO] - Training Epoch: 3/10, step 192/574 completed (loss: 0.6975104808807373, acc: 0.7727272510528564)
[2025-01-06 01:13:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:54][root][INFO] - Training Epoch: 3/10, step 193/574 completed (loss: 0.5505044460296631, acc: 0.8705882430076599)
[2025-01-06 01:13:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:55][root][INFO] - Training Epoch: 3/10, step 194/574 completed (loss: 0.8471108078956604, acc: 0.7716049551963806)
[2025-01-06 01:13:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:56][root][INFO] - Training Epoch: 3/10, step 195/574 completed (loss: 0.43001142144203186, acc: 0.8548387289047241)
[2025-01-06 01:13:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:57][root][INFO] - Training Epoch: 3/10, step 196/574 completed (loss: 0.11924320459365845, acc: 0.9642857313156128)
[2025-01-06 01:13:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:57][root][INFO] - Training Epoch: 3/10, step 197/574 completed (loss: 0.6108386516571045, acc: 0.8500000238418579)
[2025-01-06 01:13:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:57][root][INFO] - Training Epoch: 3/10, step 198/574 completed (loss: 0.7595453262329102, acc: 0.7941176295280457)
[2025-01-06 01:13:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:58][root][INFO] - Training Epoch: 3/10, step 199/574 completed (loss: 0.7141152620315552, acc: 0.7867646813392639)
[2025-01-06 01:13:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:58][root][INFO] - Training Epoch: 3/10, step 200/574 completed (loss: 0.5076078772544861, acc: 0.8474576473236084)
[2025-01-06 01:13:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:58][root][INFO] - Training Epoch: 3/10, step 201/574 completed (loss: 0.710196852684021, acc: 0.8059701323509216)
[2025-01-06 01:13:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:59][root][INFO] - Training Epoch: 3/10, step 202/574 completed (loss: 0.7993957996368408, acc: 0.7864077687263489)
[2025-01-06 01:13:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:13:59][root][INFO] - Training Epoch: 3/10, step 203/574 completed (loss: 0.6024582386016846, acc: 0.8253968358039856)
[2025-01-06 01:13:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:00][root][INFO] - Training Epoch: 3/10, step 204/574 completed (loss: 0.17515261471271515, acc: 0.9450549483299255)
[2025-01-06 01:14:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:00][root][INFO] - Training Epoch: 3/10, step 205/574 completed (loss: 0.29426896572113037, acc: 0.9282511472702026)
[2025-01-06 01:14:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:00][root][INFO] - Training Epoch: 3/10, step 206/574 completed (loss: 0.42326661944389343, acc: 0.8582677245140076)
[2025-01-06 01:14:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:01][root][INFO] - Training Epoch: 3/10, step 207/574 completed (loss: 0.2513430118560791, acc: 0.9181034564971924)
[2025-01-06 01:14:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:01][root][INFO] - Training Epoch: 3/10, step 208/574 completed (loss: 0.3680625259876251, acc: 0.8804348111152649)
[2025-01-06 01:14:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:01][root][INFO] - Training Epoch: 3/10, step 209/574 completed (loss: 0.30802080035209656, acc: 0.9066147804260254)
[2025-01-06 01:14:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:02][root][INFO] - Training Epoch: 3/10, step 210/574 completed (loss: 0.1704394370317459, acc: 0.9239130616188049)
[2025-01-06 01:14:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:02][root][INFO] - Training Epoch: 3/10, step 211/574 completed (loss: 0.08848095685243607, acc: 1.0)
[2025-01-06 01:14:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:03][root][INFO] - Training Epoch: 3/10, step 212/574 completed (loss: 0.02174227498471737, acc: 1.0)
[2025-01-06 01:14:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:03][root][INFO] - Training Epoch: 3/10, step 213/574 completed (loss: 0.07079651206731796, acc: 0.978723406791687)
[2025-01-06 01:14:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:04][root][INFO] - Training Epoch: 3/10, step 214/574 completed (loss: 0.16536641120910645, acc: 0.9538461565971375)
[2025-01-06 01:14:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:04][root][INFO] - Training Epoch: 3/10, step 215/574 completed (loss: 0.09391031414270401, acc: 0.9594594836235046)
[2025-01-06 01:14:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:04][root][INFO] - Training Epoch: 3/10, step 216/574 completed (loss: 0.12265164405107498, acc: 0.9534883499145508)
[2025-01-06 01:14:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:05][root][INFO] - Training Epoch: 3/10, step 217/574 completed (loss: 0.16672012209892273, acc: 0.954954981803894)
[2025-01-06 01:14:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:05][root][INFO] - Training Epoch: 3/10, step 218/574 completed (loss: 0.11750569939613342, acc: 0.9555555582046509)
[2025-01-06 01:14:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:06][root][INFO] - Training Epoch: 3/10, step 219/574 completed (loss: 0.20543494820594788, acc: 0.9696969985961914)
[2025-01-06 01:14:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:06][root][INFO] - Training Epoch: 3/10, step 220/574 completed (loss: 0.06275846064090729, acc: 1.0)
[2025-01-06 01:14:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:06][root][INFO] - Training Epoch: 3/10, step 221/574 completed (loss: 0.10802090913057327, acc: 0.9599999785423279)
[2025-01-06 01:14:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:07][root][INFO] - Training Epoch: 3/10, step 222/574 completed (loss: 0.4598197340965271, acc: 0.8846153616905212)
[2025-01-06 01:14:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:07][root][INFO] - Training Epoch: 3/10, step 223/574 completed (loss: 0.3086632490158081, acc: 0.91847825050354)
[2025-01-06 01:14:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:08][root][INFO] - Training Epoch: 3/10, step 224/574 completed (loss: 0.4752352833747864, acc: 0.875)
[2025-01-06 01:14:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:08][root][INFO] - Training Epoch: 3/10, step 225/574 completed (loss: 0.760499119758606, acc: 0.7765957713127136)
[2025-01-06 01:14:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:09][root][INFO] - Training Epoch: 3/10, step 226/574 completed (loss: 0.3762703239917755, acc: 0.9056603908538818)
[2025-01-06 01:14:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:09][root][INFO] - Training Epoch: 3/10, step 227/574 completed (loss: 0.2634456753730774, acc: 0.9166666865348816)
[2025-01-06 01:14:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:09][root][INFO] - Training Epoch: 3/10, step 228/574 completed (loss: 0.19673588871955872, acc: 0.9534883499145508)
[2025-01-06 01:14:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:10][root][INFO] - Training Epoch: 3/10, step 229/574 completed (loss: 0.5875488519668579, acc: 0.8333333134651184)
[2025-01-06 01:14:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:10][root][INFO] - Training Epoch: 3/10, step 230/574 completed (loss: 1.6497446298599243, acc: 0.5684210658073425)
[2025-01-06 01:14:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:11][root][INFO] - Training Epoch: 3/10, step 231/574 completed (loss: 1.1404857635498047, acc: 0.6777777671813965)
[2025-01-06 01:14:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:11][root][INFO] - Training Epoch: 3/10, step 232/574 completed (loss: 1.2224854230880737, acc: 0.6777777671813965)
[2025-01-06 01:14:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:11][root][INFO] - Training Epoch: 3/10, step 233/574 completed (loss: 1.4972894191741943, acc: 0.5825688242912292)
[2025-01-06 01:14:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:12][root][INFO] - Training Epoch: 3/10, step 234/574 completed (loss: 1.3277275562286377, acc: 0.607692301273346)
[2025-01-06 01:14:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:12][root][INFO] - Training Epoch: 3/10, step 235/574 completed (loss: 0.049027081578969955, acc: 1.0)
[2025-01-06 01:14:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:13][root][INFO] - Training Epoch: 3/10, step 236/574 completed (loss: 0.19450069963932037, acc: 0.9583333134651184)
[2025-01-06 01:14:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:13][root][INFO] - Training Epoch: 3/10, step 237/574 completed (loss: 0.60166335105896, acc: 0.7727272510528564)
[2025-01-06 01:14:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:13][root][INFO] - Training Epoch: 3/10, step 238/574 completed (loss: 0.3898281455039978, acc: 0.8888888955116272)
[2025-01-06 01:14:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:14][root][INFO] - Training Epoch: 3/10, step 239/574 completed (loss: 0.23786330223083496, acc: 0.9428571462631226)
[2025-01-06 01:14:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:14][root][INFO] - Training Epoch: 3/10, step 240/574 completed (loss: 0.5951701998710632, acc: 0.8863636255264282)
[2025-01-06 01:14:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:14][root][INFO] - Training Epoch: 3/10, step 241/574 completed (loss: 0.39266616106033325, acc: 0.8636363744735718)
[2025-01-06 01:14:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:15][root][INFO] - Training Epoch: 3/10, step 242/574 completed (loss: 0.8482591509819031, acc: 0.7903226017951965)
[2025-01-06 01:14:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:16][root][INFO] - Training Epoch: 3/10, step 243/574 completed (loss: 0.6510757207870483, acc: 0.8409090638160706)
[2025-01-06 01:14:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:16][root][INFO] - Training Epoch: 3/10, step 244/574 completed (loss: 0.06620605289936066, acc: 0.9523809552192688)
[2025-01-06 01:14:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:16][root][INFO] - Training Epoch: 3/10, step 245/574 completed (loss: 0.37400150299072266, acc: 0.8461538553237915)
[2025-01-06 01:14:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:17][root][INFO] - Training Epoch: 3/10, step 246/574 completed (loss: 0.006816304754465818, acc: 1.0)
[2025-01-06 01:14:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:17][root][INFO] - Training Epoch: 3/10, step 247/574 completed (loss: 0.040624819695949554, acc: 1.0)
[2025-01-06 01:14:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:17][root][INFO] - Training Epoch: 3/10, step 248/574 completed (loss: 0.10346191376447678, acc: 0.9729729890823364)
[2025-01-06 01:14:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:18][root][INFO] - Training Epoch: 3/10, step 249/574 completed (loss: 0.2221061885356903, acc: 0.9189189076423645)
[2025-01-06 01:14:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:18][root][INFO] - Training Epoch: 3/10, step 250/574 completed (loss: 0.011697323061525822, acc: 1.0)
[2025-01-06 01:14:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:18][root][INFO] - Training Epoch: 3/10, step 251/574 completed (loss: 0.12795889377593994, acc: 0.9411764740943909)
[2025-01-06 01:14:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:19][root][INFO] - Training Epoch: 3/10, step 252/574 completed (loss: 0.011366854421794415, acc: 1.0)
[2025-01-06 01:14:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:19][root][INFO] - Training Epoch: 3/10, step 253/574 completed (loss: 0.01878800429403782, acc: 1.0)
[2025-01-06 01:14:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:19][root][INFO] - Training Epoch: 3/10, step 254/574 completed (loss: 0.057332128286361694, acc: 0.9599999785423279)
[2025-01-06 01:14:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:20][root][INFO] - Training Epoch: 3/10, step 255/574 completed (loss: 0.09630005806684494, acc: 0.9677419066429138)
[2025-01-06 01:14:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:20][root][INFO] - Training Epoch: 3/10, step 256/574 completed (loss: 0.07546228170394897, acc: 0.9824561476707458)
[2025-01-06 01:14:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:20][root][INFO] - Training Epoch: 3/10, step 257/574 completed (loss: 0.10582731664180756, acc: 0.9428571462631226)
[2025-01-06 01:14:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:21][root][INFO] - Training Epoch: 3/10, step 258/574 completed (loss: 0.08280593901872635, acc: 0.9605262875556946)
[2025-01-06 01:14:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:21][root][INFO] - Training Epoch: 3/10, step 259/574 completed (loss: 0.31203410029411316, acc: 0.9245283007621765)
[2025-01-06 01:14:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:22][root][INFO] - Training Epoch: 3/10, step 260/574 completed (loss: 0.4355441629886627, acc: 0.875)
[2025-01-06 01:14:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:22][root][INFO] - Training Epoch: 3/10, step 261/574 completed (loss: 0.13076752424240112, acc: 0.9444444179534912)
[2025-01-06 01:14:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:22][root][INFO] - Training Epoch: 3/10, step 262/574 completed (loss: 0.23131881654262543, acc: 0.9032257795333862)
[2025-01-06 01:14:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:23][root][INFO] - Training Epoch: 3/10, step 263/574 completed (loss: 0.8845037817955017, acc: 0.800000011920929)
[2025-01-06 01:14:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:23][root][INFO] - Training Epoch: 3/10, step 264/574 completed (loss: 0.5214489102363586, acc: 0.7916666865348816)
[2025-01-06 01:14:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:24][root][INFO] - Training Epoch: 3/10, step 265/574 completed (loss: 1.2572312355041504, acc: 0.5759999752044678)
[2025-01-06 01:14:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:24][root][INFO] - Training Epoch: 3/10, step 266/574 completed (loss: 1.3367042541503906, acc: 0.6292135119438171)
[2025-01-06 01:14:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:25][root][INFO] - Training Epoch: 3/10, step 267/574 completed (loss: 0.7945707440376282, acc: 0.7702702879905701)
[2025-01-06 01:14:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:25][root][INFO] - Training Epoch: 3/10, step 268/574 completed (loss: 0.4944681227207184, acc: 0.8793103694915771)
[2025-01-06 01:14:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:25][root][INFO] - Training Epoch: 3/10, step 269/574 completed (loss: 0.16606615483760834, acc: 0.9545454382896423)
[2025-01-06 01:14:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:26][root][INFO] - Training Epoch: 3/10, step 270/574 completed (loss: 0.017755301669239998, acc: 1.0)
[2025-01-06 01:14:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:26][root][INFO] - Training Epoch: 3/10, step 271/574 completed (loss: 0.028288867324590683, acc: 1.0)
[2025-01-06 01:14:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:27][root][INFO] - Training Epoch: 3/10, step 272/574 completed (loss: 0.05830370634794235, acc: 0.9666666388511658)
[2025-01-06 01:14:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:27][root][INFO] - Training Epoch: 3/10, step 273/574 completed (loss: 0.20270411670207977, acc: 0.9833333492279053)
[2025-01-06 01:14:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:27][root][INFO] - Training Epoch: 3/10, step 274/574 completed (loss: 0.10046447813510895, acc: 0.96875)
[2025-01-06 01:14:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:28][root][INFO] - Training Epoch: 3/10, step 275/574 completed (loss: 0.11326465010643005, acc: 0.9666666388511658)
[2025-01-06 01:14:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:28][root][INFO] - Training Epoch: 3/10, step 276/574 completed (loss: 0.26937195658683777, acc: 0.9655172228813171)
[2025-01-06 01:14:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:28][root][INFO] - Training Epoch: 3/10, step 277/574 completed (loss: 0.20277535915374756, acc: 0.9599999785423279)
[2025-01-06 01:14:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:29][root][INFO] - Training Epoch: 3/10, step 278/574 completed (loss: 0.16478031873703003, acc: 0.957446813583374)
[2025-01-06 01:14:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:29][root][INFO] - Training Epoch: 3/10, step 279/574 completed (loss: 0.20071886479854584, acc: 0.9375)
[2025-01-06 01:14:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:29][root][INFO] - Training Epoch: 3/10, step 280/574 completed (loss: 0.054474592208862305, acc: 1.0)
[2025-01-06 01:14:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:30][root][INFO] - Training Epoch: 3/10, step 281/574 completed (loss: 0.40017589926719666, acc: 0.8674699068069458)
[2025-01-06 01:14:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:14:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:01][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.8198, device='cuda:0') eval_epoch_loss=tensor(0.5987, device='cuda:0') eval_epoch_acc=tensor(0.8369, device='cuda:0')
[2025-01-06 01:15:01][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:15:01][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:15:01][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_3_step_282_loss_0.5987038612365723/model.pt
[2025-01-06 01:15:01][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:15:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:01][root][INFO] - Training Epoch: 3/10, step 282/574 completed (loss: 0.5849902033805847, acc: 0.8333333134651184)
[2025-01-06 01:15:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:02][root][INFO] - Training Epoch: 3/10, step 283/574 completed (loss: 0.04763147979974747, acc: 1.0)
[2025-01-06 01:15:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:02][root][INFO] - Training Epoch: 3/10, step 284/574 completed (loss: 0.10913043469190598, acc: 0.970588207244873)
[2025-01-06 01:15:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:03][root][INFO] - Training Epoch: 3/10, step 285/574 completed (loss: 0.12542232871055603, acc: 0.949999988079071)
[2025-01-06 01:15:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:03][root][INFO] - Training Epoch: 3/10, step 286/574 completed (loss: 0.3992087244987488, acc: 0.859375)
[2025-01-06 01:15:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:03][root][INFO] - Training Epoch: 3/10, step 287/574 completed (loss: 0.4904775619506836, acc: 0.8799999952316284)
[2025-01-06 01:15:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:04][root][INFO] - Training Epoch: 3/10, step 288/574 completed (loss: 0.23344992101192474, acc: 0.9120879173278809)
[2025-01-06 01:15:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:04][root][INFO] - Training Epoch: 3/10, step 289/574 completed (loss: 0.31586623191833496, acc: 0.8819875717163086)
[2025-01-06 01:15:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:04][root][INFO] - Training Epoch: 3/10, step 290/574 completed (loss: 0.4198976755142212, acc: 0.907216489315033)
[2025-01-06 01:15:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:05][root][INFO] - Training Epoch: 3/10, step 291/574 completed (loss: 0.015250337310135365, acc: 1.0)
[2025-01-06 01:15:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:05][root][INFO] - Training Epoch: 3/10, step 292/574 completed (loss: 0.16720923781394958, acc: 0.9523809552192688)
[2025-01-06 01:15:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:05][root][INFO] - Training Epoch: 3/10, step 293/574 completed (loss: 0.09556786715984344, acc: 0.982758641242981)
[2025-01-06 01:15:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:06][root][INFO] - Training Epoch: 3/10, step 294/574 completed (loss: 0.3919914960861206, acc: 0.8727272748947144)
[2025-01-06 01:15:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:06][root][INFO] - Training Epoch: 3/10, step 295/574 completed (loss: 0.44978663325309753, acc: 0.8865979313850403)
[2025-01-06 01:15:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:07][root][INFO] - Training Epoch: 3/10, step 296/574 completed (loss: 0.1893802136182785, acc: 0.9482758641242981)
[2025-01-06 01:15:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:07][root][INFO] - Training Epoch: 3/10, step 297/574 completed (loss: 0.24321988224983215, acc: 0.9259259104728699)
[2025-01-06 01:15:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:07][root][INFO] - Training Epoch: 3/10, step 298/574 completed (loss: 0.2299431562423706, acc: 0.9210526347160339)
[2025-01-06 01:15:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:08][root][INFO] - Training Epoch: 3/10, step 299/574 completed (loss: 0.04034150391817093, acc: 1.0)
[2025-01-06 01:15:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:08][root][INFO] - Training Epoch: 3/10, step 300/574 completed (loss: 0.051214706152677536, acc: 1.0)
[2025-01-06 01:15:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:08][root][INFO] - Training Epoch: 3/10, step 301/574 completed (loss: 0.171427920460701, acc: 0.9622641801834106)
[2025-01-06 01:15:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:09][root][INFO] - Training Epoch: 3/10, step 302/574 completed (loss: 0.039233483374118805, acc: 0.9811320900917053)
[2025-01-06 01:15:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:09][root][INFO] - Training Epoch: 3/10, step 303/574 completed (loss: 0.02715236507356167, acc: 1.0)
[2025-01-06 01:15:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:10][root][INFO] - Training Epoch: 3/10, step 304/574 completed (loss: 0.07465199381113052, acc: 0.96875)
[2025-01-06 01:15:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:10][root][INFO] - Training Epoch: 3/10, step 305/574 completed (loss: 0.15426598489284515, acc: 0.9836065769195557)
[2025-01-06 01:15:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:10][root][INFO] - Training Epoch: 3/10, step 306/574 completed (loss: 0.042684003710746765, acc: 1.0)
[2025-01-06 01:15:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:11][root][INFO] - Training Epoch: 3/10, step 307/574 completed (loss: 0.005813079420477152, acc: 1.0)
[2025-01-06 01:15:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:11][root][INFO] - Training Epoch: 3/10, step 308/574 completed (loss: 0.16638267040252686, acc: 0.9710144996643066)
[2025-01-06 01:15:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:11][root][INFO] - Training Epoch: 3/10, step 309/574 completed (loss: 0.09019270539283752, acc: 0.9861111044883728)
[2025-01-06 01:15:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:12][root][INFO] - Training Epoch: 3/10, step 310/574 completed (loss: 0.16188284754753113, acc: 0.9638554453849792)
[2025-01-06 01:15:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:12][root][INFO] - Training Epoch: 3/10, step 311/574 completed (loss: 0.22862762212753296, acc: 0.9230769276618958)
[2025-01-06 01:15:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:12][root][INFO] - Training Epoch: 3/10, step 312/574 completed (loss: 0.09686516970396042, acc: 0.9795918464660645)
[2025-01-06 01:15:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:13][root][INFO] - Training Epoch: 3/10, step 313/574 completed (loss: 0.0039032783824950457, acc: 1.0)
[2025-01-06 01:15:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:13][root][INFO] - Training Epoch: 3/10, step 314/574 completed (loss: 0.00613338453695178, acc: 1.0)
[2025-01-06 01:15:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:13][root][INFO] - Training Epoch: 3/10, step 315/574 completed (loss: 0.09487301856279373, acc: 0.9677419066429138)
[2025-01-06 01:15:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:14][root][INFO] - Training Epoch: 3/10, step 316/574 completed (loss: 0.13754500448703766, acc: 0.9354838728904724)
[2025-01-06 01:15:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:14][root][INFO] - Training Epoch: 3/10, step 317/574 completed (loss: 0.12086532264947891, acc: 0.9552238583564758)
[2025-01-06 01:15:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:15][root][INFO] - Training Epoch: 3/10, step 318/574 completed (loss: 0.06251540035009384, acc: 0.9711538553237915)
[2025-01-06 01:15:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:15][root][INFO] - Training Epoch: 3/10, step 319/574 completed (loss: 0.08839233219623566, acc: 0.9555555582046509)
[2025-01-06 01:15:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:15][root][INFO] - Training Epoch: 3/10, step 320/574 completed (loss: 0.04729226231575012, acc: 0.9838709831237793)
[2025-01-06 01:15:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:15][root][INFO] - Training Epoch: 3/10, step 321/574 completed (loss: 0.023461876437067986, acc: 1.0)
[2025-01-06 01:15:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:16][root][INFO] - Training Epoch: 3/10, step 322/574 completed (loss: 0.5382065773010254, acc: 0.8518518805503845)
[2025-01-06 01:15:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:16][root][INFO] - Training Epoch: 3/10, step 323/574 completed (loss: 0.8216984272003174, acc: 0.7142857313156128)
[2025-01-06 01:15:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:17][root][INFO] - Training Epoch: 3/10, step 324/574 completed (loss: 0.8082858920097351, acc: 0.7692307829856873)
[2025-01-06 01:15:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:17][root][INFO] - Training Epoch: 3/10, step 325/574 completed (loss: 0.968281090259552, acc: 0.7560975551605225)
[2025-01-06 01:15:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:17][root][INFO] - Training Epoch: 3/10, step 326/574 completed (loss: 0.5891940593719482, acc: 0.7631579041481018)
[2025-01-06 01:15:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:18][root][INFO] - Training Epoch: 3/10, step 327/574 completed (loss: 0.3267359137535095, acc: 0.8947368264198303)
[2025-01-06 01:15:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:18][root][INFO] - Training Epoch: 3/10, step 328/574 completed (loss: 0.09440203756093979, acc: 0.9642857313156128)
[2025-01-06 01:15:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:18][root][INFO] - Training Epoch: 3/10, step 329/574 completed (loss: 0.012528556399047375, acc: 1.0)
[2025-01-06 01:15:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:19][root][INFO] - Training Epoch: 3/10, step 330/574 completed (loss: 0.007333712186664343, acc: 1.0)
[2025-01-06 01:15:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:19][root][INFO] - Training Epoch: 3/10, step 331/574 completed (loss: 0.2224707156419754, acc: 0.9677419066429138)
[2025-01-06 01:15:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:20][root][INFO] - Training Epoch: 3/10, step 332/574 completed (loss: 0.08615348488092422, acc: 0.9473684430122375)
[2025-01-06 01:15:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:20][root][INFO] - Training Epoch: 3/10, step 333/574 completed (loss: 0.06883963942527771, acc: 1.0)
[2025-01-06 01:15:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:20][root][INFO] - Training Epoch: 3/10, step 334/574 completed (loss: 0.03371613845229149, acc: 1.0)
[2025-01-06 01:15:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:21][root][INFO] - Training Epoch: 3/10, step 335/574 completed (loss: 0.033646367490291595, acc: 1.0)
[2025-01-06 01:15:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:21][root][INFO] - Training Epoch: 3/10, step 336/574 completed (loss: 0.7020953297615051, acc: 0.7799999713897705)
[2025-01-06 01:15:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:21][root][INFO] - Training Epoch: 3/10, step 337/574 completed (loss: 0.8780990839004517, acc: 0.6896551847457886)
[2025-01-06 01:15:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:22][root][INFO] - Training Epoch: 3/10, step 338/574 completed (loss: 1.0753487348556519, acc: 0.6489361524581909)
[2025-01-06 01:15:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:22][root][INFO] - Training Epoch: 3/10, step 339/574 completed (loss: 0.9792063236236572, acc: 0.7108433842658997)
[2025-01-06 01:15:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:22][root][INFO] - Training Epoch: 3/10, step 340/574 completed (loss: 0.006094671320170164, acc: 1.0)
[2025-01-06 01:15:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:23][root][INFO] - Training Epoch: 3/10, step 341/574 completed (loss: 0.07162179797887802, acc: 0.9743589758872986)
[2025-01-06 01:15:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:23][root][INFO] - Training Epoch: 3/10, step 342/574 completed (loss: 0.2895849943161011, acc: 0.9277108311653137)
[2025-01-06 01:15:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:24][root][INFO] - Training Epoch: 3/10, step 343/574 completed (loss: 0.19607162475585938, acc: 0.9622641801834106)
[2025-01-06 01:15:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:24][root][INFO] - Training Epoch: 3/10, step 344/574 completed (loss: 0.09779047220945358, acc: 0.9746835231781006)
[2025-01-06 01:15:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:24][root][INFO] - Training Epoch: 3/10, step 345/574 completed (loss: 0.04851669445633888, acc: 0.9803921580314636)
[2025-01-06 01:15:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:25][root][INFO] - Training Epoch: 3/10, step 346/574 completed (loss: 0.2803933620452881, acc: 0.9253731369972229)
[2025-01-06 01:15:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:25][root][INFO] - Training Epoch: 3/10, step 347/574 completed (loss: 0.0027965169865638018, acc: 1.0)
[2025-01-06 01:15:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:25][root][INFO] - Training Epoch: 3/10, step 348/574 completed (loss: 0.038399018347263336, acc: 1.0)
[2025-01-06 01:15:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:26][root][INFO] - Training Epoch: 3/10, step 349/574 completed (loss: 0.5752474665641785, acc: 0.8055555820465088)
[2025-01-06 01:15:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:26][root][INFO] - Training Epoch: 3/10, step 350/574 completed (loss: 0.34700241684913635, acc: 0.8604651093482971)
[2025-01-06 01:15:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:26][root][INFO] - Training Epoch: 3/10, step 351/574 completed (loss: 0.052772462368011475, acc: 1.0)
[2025-01-06 01:15:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:27][root][INFO] - Training Epoch: 3/10, step 352/574 completed (loss: 0.43719354271888733, acc: 0.8888888955116272)
[2025-01-06 01:15:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:27][root][INFO] - Training Epoch: 3/10, step 353/574 completed (loss: 0.048340097069740295, acc: 1.0)
[2025-01-06 01:15:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:27][root][INFO] - Training Epoch: 3/10, step 354/574 completed (loss: 0.0581304207444191, acc: 1.0)
[2025-01-06 01:15:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:28][root][INFO] - Training Epoch: 3/10, step 355/574 completed (loss: 0.5963963270187378, acc: 0.8571428656578064)
[2025-01-06 01:15:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:28][root][INFO] - Training Epoch: 3/10, step 356/574 completed (loss: 0.5707334280014038, acc: 0.834782600402832)
[2025-01-06 01:15:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:29][root][INFO] - Training Epoch: 3/10, step 357/574 completed (loss: 0.4176997244358063, acc: 0.8804348111152649)
[2025-01-06 01:15:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:29][root][INFO] - Training Epoch: 3/10, step 358/574 completed (loss: 0.4184977412223816, acc: 0.9387755393981934)
[2025-01-06 01:15:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:29][root][INFO] - Training Epoch: 3/10, step 359/574 completed (loss: 0.005628162994980812, acc: 1.0)
[2025-01-06 01:15:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:30][root][INFO] - Training Epoch: 3/10, step 360/574 completed (loss: 0.12599259614944458, acc: 0.9230769276618958)
[2025-01-06 01:15:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:30][root][INFO] - Training Epoch: 3/10, step 361/574 completed (loss: 0.16923558712005615, acc: 0.9756097793579102)
[2025-01-06 01:15:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:30][root][INFO] - Training Epoch: 3/10, step 362/574 completed (loss: 0.2530866861343384, acc: 0.9111111164093018)
[2025-01-06 01:15:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:31][root][INFO] - Training Epoch: 3/10, step 363/574 completed (loss: 0.0558510459959507, acc: 0.9868420958518982)
[2025-01-06 01:15:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:31][root][INFO] - Training Epoch: 3/10, step 364/574 completed (loss: 0.04268299788236618, acc: 1.0)
[2025-01-06 01:15:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:32][root][INFO] - Training Epoch: 3/10, step 365/574 completed (loss: 0.04027954861521721, acc: 1.0)
[2025-01-06 01:15:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:32][root][INFO] - Training Epoch: 3/10, step 366/574 completed (loss: 0.013070132583379745, acc: 1.0)
[2025-01-06 01:15:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:32][root][INFO] - Training Epoch: 3/10, step 367/574 completed (loss: 0.01821347326040268, acc: 1.0)
[2025-01-06 01:15:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:33][root][INFO] - Training Epoch: 3/10, step 368/574 completed (loss: 0.012743547558784485, acc: 1.0)
[2025-01-06 01:15:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:33][root][INFO] - Training Epoch: 3/10, step 369/574 completed (loss: 0.009015592746436596, acc: 1.0)
[2025-01-06 01:15:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:34][root][INFO] - Training Epoch: 3/10, step 370/574 completed (loss: 0.33470478653907776, acc: 0.8848484754562378)
[2025-01-06 01:15:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:34][root][INFO] - Training Epoch: 3/10, step 371/574 completed (loss: 0.14850929379463196, acc: 0.9528301954269409)
[2025-01-06 01:15:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:35][root][INFO] - Training Epoch: 3/10, step 372/574 completed (loss: 0.1376684457063675, acc: 0.9444444179534912)
[2025-01-06 01:15:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:35][root][INFO] - Training Epoch: 3/10, step 373/574 completed (loss: 0.26374509930610657, acc: 0.9464285969734192)
[2025-01-06 01:15:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:35][root][INFO] - Training Epoch: 3/10, step 374/574 completed (loss: 0.0360136404633522, acc: 1.0)
[2025-01-06 01:15:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:36][root][INFO] - Training Epoch: 3/10, step 375/574 completed (loss: 0.0007284539169631898, acc: 1.0)
[2025-01-06 01:15:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:36][root][INFO] - Training Epoch: 3/10, step 376/574 completed (loss: 0.004668225534260273, acc: 1.0)
[2025-01-06 01:15:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:36][root][INFO] - Training Epoch: 3/10, step 377/574 completed (loss: 0.00719635421410203, acc: 1.0)
[2025-01-06 01:15:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:37][root][INFO] - Training Epoch: 3/10, step 378/574 completed (loss: 0.022464020177721977, acc: 0.9789473414421082)
[2025-01-06 01:15:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:37][root][INFO] - Training Epoch: 3/10, step 379/574 completed (loss: 0.1925467997789383, acc: 0.940119743347168)
[2025-01-06 01:15:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:38][root][INFO] - Training Epoch: 3/10, step 380/574 completed (loss: 0.2844800055027008, acc: 0.9473684430122375)
[2025-01-06 01:15:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:39][root][INFO] - Training Epoch: 3/10, step 381/574 completed (loss: 0.4615322947502136, acc: 0.8502673506736755)
[2025-01-06 01:15:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:40][root][INFO] - Training Epoch: 3/10, step 382/574 completed (loss: 0.1298730969429016, acc: 0.9369369149208069)
[2025-01-06 01:15:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:40][root][INFO] - Training Epoch: 3/10, step 383/574 completed (loss: 0.12133944779634476, acc: 0.9642857313156128)
[2025-01-06 01:15:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:40][root][INFO] - Training Epoch: 3/10, step 384/574 completed (loss: 0.01266528107225895, acc: 1.0)
[2025-01-06 01:15:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:41][root][INFO] - Training Epoch: 3/10, step 385/574 completed (loss: 0.02664697729051113, acc: 0.96875)
[2025-01-06 01:15:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:41][root][INFO] - Training Epoch: 3/10, step 386/574 completed (loss: 0.0046162353828549385, acc: 1.0)
[2025-01-06 01:15:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:41][root][INFO] - Training Epoch: 3/10, step 387/574 completed (loss: 0.10675887018442154, acc: 0.9736841917037964)
[2025-01-06 01:15:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:42][root][INFO] - Training Epoch: 3/10, step 388/574 completed (loss: 0.0005668714875355363, acc: 1.0)
[2025-01-06 01:15:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:42][root][INFO] - Training Epoch: 3/10, step 389/574 completed (loss: 0.0020937523804605007, acc: 1.0)
[2025-01-06 01:15:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:42][root][INFO] - Training Epoch: 3/10, step 390/574 completed (loss: 0.09144970774650574, acc: 0.9523809552192688)
[2025-01-06 01:15:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:43][root][INFO] - Training Epoch: 3/10, step 391/574 completed (loss: 0.5027034282684326, acc: 0.8148148059844971)
[2025-01-06 01:15:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:43][root][INFO] - Training Epoch: 3/10, step 392/574 completed (loss: 0.604831337928772, acc: 0.8155339956283569)
[2025-01-06 01:15:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:44][root][INFO] - Training Epoch: 3/10, step 393/574 completed (loss: 0.8104667663574219, acc: 0.8382353186607361)
[2025-01-06 01:15:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:44][root][INFO] - Training Epoch: 3/10, step 394/574 completed (loss: 0.4777505397796631, acc: 0.8533333539962769)
[2025-01-06 01:15:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:44][root][INFO] - Training Epoch: 3/10, step 395/574 completed (loss: 0.5876733660697937, acc: 0.8055555820465088)
[2025-01-06 01:15:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:45][root][INFO] - Training Epoch: 3/10, step 396/574 completed (loss: 0.3278249502182007, acc: 0.9069767594337463)
[2025-01-06 01:15:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:45][root][INFO] - Training Epoch: 3/10, step 397/574 completed (loss: 0.028165945783257484, acc: 1.0)
[2025-01-06 01:15:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:45][root][INFO] - Training Epoch: 3/10, step 398/574 completed (loss: 0.09219244122505188, acc: 0.9767441749572754)
[2025-01-06 01:15:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:46][root][INFO] - Training Epoch: 3/10, step 399/574 completed (loss: 0.05913819000124931, acc: 0.9599999785423279)
[2025-01-06 01:15:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:46][root][INFO] - Training Epoch: 3/10, step 400/574 completed (loss: 0.22549943625926971, acc: 0.9264705777168274)
[2025-01-06 01:15:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:46][root][INFO] - Training Epoch: 3/10, step 401/574 completed (loss: 0.21402989327907562, acc: 0.9466666579246521)
[2025-01-06 01:15:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:47][root][INFO] - Training Epoch: 3/10, step 402/574 completed (loss: 0.11554232984781265, acc: 0.9696969985961914)
[2025-01-06 01:15:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:47][root][INFO] - Training Epoch: 3/10, step 403/574 completed (loss: 0.16815119981765747, acc: 0.9696969985961914)
[2025-01-06 01:15:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:48][root][INFO] - Training Epoch: 3/10, step 404/574 completed (loss: 0.031780313700437546, acc: 1.0)
[2025-01-06 01:15:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:48][root][INFO] - Training Epoch: 3/10, step 405/574 completed (loss: 0.007712688762694597, acc: 1.0)
[2025-01-06 01:15:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:48][root][INFO] - Training Epoch: 3/10, step 406/574 completed (loss: 0.01619972474873066, acc: 1.0)
[2025-01-06 01:15:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:49][root][INFO] - Training Epoch: 3/10, step 407/574 completed (loss: 0.0115602295845747, acc: 1.0)
[2025-01-06 01:15:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:49][root][INFO] - Training Epoch: 3/10, step 408/574 completed (loss: 0.052723340690135956, acc: 1.0)
[2025-01-06 01:15:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:49][root][INFO] - Training Epoch: 3/10, step 409/574 completed (loss: 0.016912950202822685, acc: 1.0)
[2025-01-06 01:15:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:50][root][INFO] - Training Epoch: 3/10, step 410/574 completed (loss: 0.07566409558057785, acc: 0.9655172228813171)
[2025-01-06 01:15:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:50][root][INFO] - Training Epoch: 3/10, step 411/574 completed (loss: 0.007563020568341017, acc: 1.0)
[2025-01-06 01:15:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:50][root][INFO] - Training Epoch: 3/10, step 412/574 completed (loss: 0.013133048079907894, acc: 1.0)
[2025-01-06 01:15:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:51][root][INFO] - Training Epoch: 3/10, step 413/574 completed (loss: 0.021012507379055023, acc: 1.0)
[2025-01-06 01:15:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:51][root][INFO] - Training Epoch: 3/10, step 414/574 completed (loss: 0.003304531332105398, acc: 1.0)
[2025-01-06 01:15:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:51][root][INFO] - Training Epoch: 3/10, step 415/574 completed (loss: 0.21534912288188934, acc: 0.9607843160629272)
[2025-01-06 01:15:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:52][root][INFO] - Training Epoch: 3/10, step 416/574 completed (loss: 0.099913589656353, acc: 0.9230769276618958)
[2025-01-06 01:15:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:52][root][INFO] - Training Epoch: 3/10, step 417/574 completed (loss: 0.05598089471459389, acc: 1.0)
[2025-01-06 01:15:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:52][root][INFO] - Training Epoch: 3/10, step 418/574 completed (loss: 0.05433148890733719, acc: 0.9750000238418579)
[2025-01-06 01:15:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:53][root][INFO] - Training Epoch: 3/10, step 419/574 completed (loss: 0.024803346022963524, acc: 1.0)
[2025-01-06 01:15:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:53][root][INFO] - Training Epoch: 3/10, step 420/574 completed (loss: 0.013218384236097336, acc: 1.0)
[2025-01-06 01:15:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:53][root][INFO] - Training Epoch: 3/10, step 421/574 completed (loss: 0.028104634955525398, acc: 1.0)
[2025-01-06 01:15:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:54][root][INFO] - Training Epoch: 3/10, step 422/574 completed (loss: 0.06928477436304092, acc: 1.0)
[2025-01-06 01:15:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:54][root][INFO] - Training Epoch: 3/10, step 423/574 completed (loss: 0.11897345632314682, acc: 0.9722222089767456)
[2025-01-06 01:15:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:54][root][INFO] - Training Epoch: 3/10, step 424/574 completed (loss: 0.031509000808000565, acc: 1.0)
[2025-01-06 01:15:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:15:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:25][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.9466, device='cuda:0') eval_epoch_loss=tensor(0.6661, device='cuda:0') eval_epoch_acc=tensor(0.8424, device='cuda:0')
[2025-01-06 01:16:25][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:16:25][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:16:25][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_3_step_425_loss_0.6660631895065308/model.pt
[2025-01-06 01:16:25][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:16:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:25][root][INFO] - Training Epoch: 3/10, step 425/574 completed (loss: 0.07569825649261475, acc: 0.939393937587738)
[2025-01-06 01:16:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:26][root][INFO] - Training Epoch: 3/10, step 426/574 completed (loss: 0.0076217325404286385, acc: 1.0)
[2025-01-06 01:16:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:26][root][INFO] - Training Epoch: 3/10, step 427/574 completed (loss: 0.12389195710420609, acc: 0.9729729890823364)
[2025-01-06 01:16:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:26][root][INFO] - Training Epoch: 3/10, step 428/574 completed (loss: 0.007342509459704161, acc: 1.0)
[2025-01-06 01:16:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:27][root][INFO] - Training Epoch: 3/10, step 429/574 completed (loss: 0.0039525991305708885, acc: 1.0)
[2025-01-06 01:16:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:27][root][INFO] - Training Epoch: 3/10, step 430/574 completed (loss: 0.0013561664381995797, acc: 1.0)
[2025-01-06 01:16:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:27][root][INFO] - Training Epoch: 3/10, step 431/574 completed (loss: 0.00633352342993021, acc: 1.0)
[2025-01-06 01:16:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:28][root][INFO] - Training Epoch: 3/10, step 432/574 completed (loss: 0.003419470740482211, acc: 1.0)
[2025-01-06 01:16:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:28][root][INFO] - Training Epoch: 3/10, step 433/574 completed (loss: 0.09371353685855865, acc: 0.9722222089767456)
[2025-01-06 01:16:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:29][root][INFO] - Training Epoch: 3/10, step 434/574 completed (loss: 0.008732862770557404, acc: 1.0)
[2025-01-06 01:16:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:29][root][INFO] - Training Epoch: 3/10, step 435/574 completed (loss: 0.0013128425925970078, acc: 1.0)
[2025-01-06 01:16:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:29][root][INFO] - Training Epoch: 3/10, step 436/574 completed (loss: 0.0949074849486351, acc: 0.9722222089767456)
[2025-01-06 01:16:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:30][root][INFO] - Training Epoch: 3/10, step 437/574 completed (loss: 0.003747012699022889, acc: 1.0)
[2025-01-06 01:16:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:30][root][INFO] - Training Epoch: 3/10, step 438/574 completed (loss: 0.002026722999289632, acc: 1.0)
[2025-01-06 01:16:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:30][root][INFO] - Training Epoch: 3/10, step 439/574 completed (loss: 0.13658878207206726, acc: 0.9487179517745972)
[2025-01-06 01:16:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:31][root][INFO] - Training Epoch: 3/10, step 440/574 completed (loss: 0.2189101129770279, acc: 0.9242424368858337)
[2025-01-06 01:16:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:31][root][INFO] - Training Epoch: 3/10, step 441/574 completed (loss: 0.4764975607395172, acc: 0.8159999847412109)
[2025-01-06 01:16:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:32][root][INFO] - Training Epoch: 3/10, step 442/574 completed (loss: 0.6398841738700867, acc: 0.8306451439857483)
[2025-01-06 01:16:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:32][root][INFO] - Training Epoch: 3/10, step 443/574 completed (loss: 0.35821935534477234, acc: 0.89552241563797)
[2025-01-06 01:16:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:33][root][INFO] - Training Epoch: 3/10, step 444/574 completed (loss: 0.08620786666870117, acc: 0.9622641801834106)
[2025-01-06 01:16:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:33][root][INFO] - Training Epoch: 3/10, step 445/574 completed (loss: 0.07584700733423233, acc: 0.9772727489471436)
[2025-01-06 01:16:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:34][root][INFO] - Training Epoch: 3/10, step 446/574 completed (loss: 0.1096968874335289, acc: 0.95652174949646)
[2025-01-06 01:16:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:34][root][INFO] - Training Epoch: 3/10, step 447/574 completed (loss: 0.11995996534824371, acc: 0.9230769276618958)
[2025-01-06 01:16:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:34][root][INFO] - Training Epoch: 3/10, step 448/574 completed (loss: 0.009600162506103516, acc: 1.0)
[2025-01-06 01:16:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:35][root][INFO] - Training Epoch: 3/10, step 449/574 completed (loss: 0.06574354320764542, acc: 0.9850746393203735)
[2025-01-06 01:16:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:35][root][INFO] - Training Epoch: 3/10, step 450/574 completed (loss: 0.04015098139643669, acc: 0.9861111044883728)
[2025-01-06 01:16:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:35][root][INFO] - Training Epoch: 3/10, step 451/574 completed (loss: 0.017359687015414238, acc: 0.989130437374115)
[2025-01-06 01:16:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:36][root][INFO] - Training Epoch: 3/10, step 452/574 completed (loss: 0.051048509776592255, acc: 0.9871794581413269)
[2025-01-06 01:16:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:36][root][INFO] - Training Epoch: 3/10, step 453/574 completed (loss: 0.10220616310834885, acc: 0.9473684430122375)
[2025-01-06 01:16:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:36][root][INFO] - Training Epoch: 3/10, step 454/574 completed (loss: 0.03332175686955452, acc: 0.9795918464660645)
[2025-01-06 01:16:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:37][root][INFO] - Training Epoch: 3/10, step 455/574 completed (loss: 0.022471468895673752, acc: 1.0)
[2025-01-06 01:16:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:37][root][INFO] - Training Epoch: 3/10, step 456/574 completed (loss: 0.44448399543762207, acc: 0.876288652420044)
[2025-01-06 01:16:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:38][root][INFO] - Training Epoch: 3/10, step 457/574 completed (loss: 0.010251367464661598, acc: 1.0)
[2025-01-06 01:16:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:38][root][INFO] - Training Epoch: 3/10, step 458/574 completed (loss: 0.13272781670093536, acc: 0.9534883499145508)
[2025-01-06 01:16:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:38][root][INFO] - Training Epoch: 3/10, step 459/574 completed (loss: 0.018666090443730354, acc: 0.9821428656578064)
[2025-01-06 01:16:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:39][root][INFO] - Training Epoch: 3/10, step 460/574 completed (loss: 0.09585079550743103, acc: 0.9629629850387573)
[2025-01-06 01:16:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:39][root][INFO] - Training Epoch: 3/10, step 461/574 completed (loss: 0.028732262551784515, acc: 1.0)
[2025-01-06 01:16:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:39][root][INFO] - Training Epoch: 3/10, step 462/574 completed (loss: 0.036920640617609024, acc: 1.0)
[2025-01-06 01:16:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:40][root][INFO] - Training Epoch: 3/10, step 463/574 completed (loss: 0.23201298713684082, acc: 0.9230769276618958)
[2025-01-06 01:16:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:40][root][INFO] - Training Epoch: 3/10, step 464/574 completed (loss: 0.07237564772367477, acc: 0.97826087474823)
[2025-01-06 01:16:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:40][root][INFO] - Training Epoch: 3/10, step 465/574 completed (loss: 0.1616465151309967, acc: 0.9523809552192688)
[2025-01-06 01:16:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:41][root][INFO] - Training Epoch: 3/10, step 466/574 completed (loss: 0.3195815980434418, acc: 0.9277108311653137)
[2025-01-06 01:16:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:41][root][INFO] - Training Epoch: 3/10, step 467/574 completed (loss: 0.0939689353108406, acc: 0.9639639854431152)
[2025-01-06 01:16:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:41][root][INFO] - Training Epoch: 3/10, step 468/574 completed (loss: 0.4457226097583771, acc: 0.8737863898277283)
[2025-01-06 01:16:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:42][root][INFO] - Training Epoch: 3/10, step 469/574 completed (loss: 0.2890144884586334, acc: 0.9268292784690857)
[2025-01-06 01:16:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:42][root][INFO] - Training Epoch: 3/10, step 470/574 completed (loss: 0.012133773416280746, acc: 1.0)
[2025-01-06 01:16:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:42][root][INFO] - Training Epoch: 3/10, step 471/574 completed (loss: 0.0694989487528801, acc: 0.9642857313156128)
[2025-01-06 01:16:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:43][root][INFO] - Training Epoch: 3/10, step 472/574 completed (loss: 0.3320537209510803, acc: 0.8921568393707275)
[2025-01-06 01:16:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:43][root][INFO] - Training Epoch: 3/10, step 473/574 completed (loss: 0.616849958896637, acc: 0.8296943306922913)
[2025-01-06 01:16:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:44][root][INFO] - Training Epoch: 3/10, step 474/574 completed (loss: 0.3281414806842804, acc: 0.8958333134651184)
[2025-01-06 01:16:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:44][root][INFO] - Training Epoch: 3/10, step 475/574 completed (loss: 0.34238582849502563, acc: 0.89570552110672)
[2025-01-06 01:16:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:44][root][INFO] - Training Epoch: 3/10, step 476/574 completed (loss: 0.23639130592346191, acc: 0.9208633303642273)
[2025-01-06 01:16:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:45][root][INFO] - Training Epoch: 3/10, step 477/574 completed (loss: 0.6522883176803589, acc: 0.7989949584007263)
[2025-01-06 01:16:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:45][root][INFO] - Training Epoch: 3/10, step 478/574 completed (loss: 0.11717290431261063, acc: 0.9722222089767456)
[2025-01-06 01:16:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:45][root][INFO] - Training Epoch: 3/10, step 479/574 completed (loss: 0.4867539405822754, acc: 0.8787878751754761)
[2025-01-06 01:16:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:46][root][INFO] - Training Epoch: 3/10, step 480/574 completed (loss: 0.030331678688526154, acc: 1.0)
[2025-01-06 01:16:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:46][root][INFO] - Training Epoch: 3/10, step 481/574 completed (loss: 0.025777077302336693, acc: 1.0)
[2025-01-06 01:16:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:47][root][INFO] - Training Epoch: 3/10, step 482/574 completed (loss: 0.35787537693977356, acc: 0.8999999761581421)
[2025-01-06 01:16:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:47][root][INFO] - Training Epoch: 3/10, step 483/574 completed (loss: 0.40501829981803894, acc: 0.8793103694915771)
[2025-01-06 01:16:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:47][root][INFO] - Training Epoch: 3/10, step 484/574 completed (loss: 0.013036269694566727, acc: 1.0)
[2025-01-06 01:16:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:48][root][INFO] - Training Epoch: 3/10, step 485/574 completed (loss: 0.060687121003866196, acc: 1.0)
[2025-01-06 01:16:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:48][root][INFO] - Training Epoch: 3/10, step 486/574 completed (loss: 0.10860970616340637, acc: 1.0)
[2025-01-06 01:16:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:48][root][INFO] - Training Epoch: 3/10, step 487/574 completed (loss: 0.18295781314373016, acc: 0.9047619104385376)
[2025-01-06 01:16:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:49][root][INFO] - Training Epoch: 3/10, step 488/574 completed (loss: 0.051038432866334915, acc: 1.0)
[2025-01-06 01:16:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:49][root][INFO] - Training Epoch: 3/10, step 489/574 completed (loss: 0.33749786019325256, acc: 0.892307698726654)
[2025-01-06 01:16:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:49][root][INFO] - Training Epoch: 3/10, step 490/574 completed (loss: 0.04661447927355766, acc: 1.0)
[2025-01-06 01:16:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:50][root][INFO] - Training Epoch: 3/10, step 491/574 completed (loss: 0.04991982877254486, acc: 1.0)
[2025-01-06 01:16:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:50][root][INFO] - Training Epoch: 3/10, step 492/574 completed (loss: 0.17734521627426147, acc: 0.9215686321258545)
[2025-01-06 01:16:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:51][root][INFO] - Training Epoch: 3/10, step 493/574 completed (loss: 0.05872908979654312, acc: 1.0)
[2025-01-06 01:16:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:51][root][INFO] - Training Epoch: 3/10, step 494/574 completed (loss: 0.05529043450951576, acc: 1.0)
[2025-01-06 01:16:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:51][root][INFO] - Training Epoch: 3/10, step 495/574 completed (loss: 0.5533989667892456, acc: 0.8947368264198303)
[2025-01-06 01:16:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:52][root][INFO] - Training Epoch: 3/10, step 496/574 completed (loss: 0.5225176215171814, acc: 0.8303571343421936)
[2025-01-06 01:16:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:52][root][INFO] - Training Epoch: 3/10, step 497/574 completed (loss: 0.17838214337825775, acc: 0.9213483333587646)
[2025-01-06 01:16:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:52][root][INFO] - Training Epoch: 3/10, step 498/574 completed (loss: 0.4714684784412384, acc: 0.8426966071128845)
[2025-01-06 01:16:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:53][root][INFO] - Training Epoch: 3/10, step 499/574 completed (loss: 0.7639073729515076, acc: 0.758865237236023)
[2025-01-06 01:16:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:53][root][INFO] - Training Epoch: 3/10, step 500/574 completed (loss: 0.4224555492401123, acc: 0.8913043737411499)
[2025-01-06 01:16:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:53][root][INFO] - Training Epoch: 3/10, step 501/574 completed (loss: 0.0024008143227547407, acc: 1.0)
[2025-01-06 01:16:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:54][root][INFO] - Training Epoch: 3/10, step 502/574 completed (loss: 0.0017218241700902581, acc: 1.0)
[2025-01-06 01:16:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:54][root][INFO] - Training Epoch: 3/10, step 503/574 completed (loss: 0.01653648167848587, acc: 1.0)
[2025-01-06 01:16:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:54][root][INFO] - Training Epoch: 3/10, step 504/574 completed (loss: 0.018637215718626976, acc: 1.0)
[2025-01-06 01:16:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:55][root][INFO] - Training Epoch: 3/10, step 505/574 completed (loss: 0.24954751133918762, acc: 0.9622641801834106)
[2025-01-06 01:16:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:55][root][INFO] - Training Epoch: 3/10, step 506/574 completed (loss: 0.36385083198547363, acc: 0.8965517282485962)
[2025-01-06 01:16:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:56][root][INFO] - Training Epoch: 3/10, step 507/574 completed (loss: 0.6063101291656494, acc: 0.8558558821678162)
[2025-01-06 01:16:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:56][root][INFO] - Training Epoch: 3/10, step 508/574 completed (loss: 0.46468621492385864, acc: 0.8169013857841492)
[2025-01-06 01:16:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:56][root][INFO] - Training Epoch: 3/10, step 509/574 completed (loss: 0.021513337269425392, acc: 1.0)
[2025-01-06 01:16:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:57][root][INFO] - Training Epoch: 3/10, step 510/574 completed (loss: 0.029847504571080208, acc: 1.0)
[2025-01-06 01:16:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:16:57][root][INFO] - Training Epoch: 3/10, step 511/574 completed (loss: 0.2634037137031555, acc: 0.9230769276618958)
[2025-01-06 01:16:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:00][root][INFO] - Training Epoch: 3/10, step 512/574 completed (loss: 0.6315717697143555, acc: 0.8214285969734192)
[2025-01-06 01:17:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:01][root][INFO] - Training Epoch: 3/10, step 513/574 completed (loss: 0.14609883725643158, acc: 0.9365079402923584)
[2025-01-06 01:17:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:01][root][INFO] - Training Epoch: 3/10, step 514/574 completed (loss: 0.04361632093787193, acc: 1.0)
[2025-01-06 01:17:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:01][root][INFO] - Training Epoch: 3/10, step 515/574 completed (loss: 0.05141688510775566, acc: 0.9833333492279053)
[2025-01-06 01:17:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:02][root][INFO] - Training Epoch: 3/10, step 516/574 completed (loss: 0.42028889060020447, acc: 0.8888888955116272)
[2025-01-06 01:17:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:02][root][INFO] - Training Epoch: 3/10, step 517/574 completed (loss: 0.08164071291685104, acc: 0.9615384340286255)
[2025-01-06 01:17:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:03][root][INFO] - Training Epoch: 3/10, step 518/574 completed (loss: 0.019474603235721588, acc: 1.0)
[2025-01-06 01:17:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:03][root][INFO] - Training Epoch: 3/10, step 519/574 completed (loss: 0.039830051362514496, acc: 1.0)
[2025-01-06 01:17:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:03][root][INFO] - Training Epoch: 3/10, step 520/574 completed (loss: 0.04793901368975639, acc: 1.0)
[2025-01-06 01:17:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:04][root][INFO] - Training Epoch: 3/10, step 521/574 completed (loss: 0.5193353295326233, acc: 0.8389830589294434)
[2025-01-06 01:17:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:05][root][INFO] - Training Epoch: 3/10, step 522/574 completed (loss: 0.15952762961387634, acc: 0.9477611780166626)
[2025-01-06 01:17:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:05][root][INFO] - Training Epoch: 3/10, step 523/574 completed (loss: 0.23301583528518677, acc: 0.9270073175430298)
[2025-01-06 01:17:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:06][root][INFO] - Training Epoch: 3/10, step 524/574 completed (loss: 0.4866219460964203, acc: 0.8849999904632568)
[2025-01-06 01:17:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:06][root][INFO] - Training Epoch: 3/10, step 525/574 completed (loss: 0.027770554646849632, acc: 1.0)
[2025-01-06 01:17:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:06][root][INFO] - Training Epoch: 3/10, step 526/574 completed (loss: 0.1354628950357437, acc: 0.942307710647583)
[2025-01-06 01:17:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:07][root][INFO] - Training Epoch: 3/10, step 527/574 completed (loss: 0.046461690217256546, acc: 1.0)
[2025-01-06 01:17:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:07][root][INFO] - Training Epoch: 3/10, step 528/574 completed (loss: 0.7796592116355896, acc: 0.7540983557701111)
[2025-01-06 01:17:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:07][root][INFO] - Training Epoch: 3/10, step 529/574 completed (loss: 0.15041625499725342, acc: 0.9491525292396545)
[2025-01-06 01:17:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:08][root][INFO] - Training Epoch: 3/10, step 530/574 completed (loss: 0.9894471168518066, acc: 0.7906976938247681)
[2025-01-06 01:17:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:08][root][INFO] - Training Epoch: 3/10, step 531/574 completed (loss: 0.23217949271202087, acc: 0.9090909361839294)
[2025-01-06 01:17:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:08][root][INFO] - Training Epoch: 3/10, step 532/574 completed (loss: 0.36796846985816956, acc: 0.9056603908538818)
[2025-01-06 01:17:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:09][root][INFO] - Training Epoch: 3/10, step 533/574 completed (loss: 0.21628357470035553, acc: 0.9090909361839294)
[2025-01-06 01:17:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:09][root][INFO] - Training Epoch: 3/10, step 534/574 completed (loss: 0.11870325356721878, acc: 1.0)
[2025-01-06 01:17:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:10][root][INFO] - Training Epoch: 3/10, step 535/574 completed (loss: 0.0984092578291893, acc: 0.949999988079071)
[2025-01-06 01:17:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:10][root][INFO] - Training Epoch: 3/10, step 536/574 completed (loss: 0.03860512003302574, acc: 1.0)
[2025-01-06 01:17:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:10][root][INFO] - Training Epoch: 3/10, step 537/574 completed (loss: 0.3141135573387146, acc: 0.892307698726654)
[2025-01-06 01:17:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:11][root][INFO] - Training Epoch: 3/10, step 538/574 completed (loss: 0.3139643669128418, acc: 0.921875)
[2025-01-06 01:17:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:11][root][INFO] - Training Epoch: 3/10, step 539/574 completed (loss: 0.28092285990715027, acc: 0.875)
[2025-01-06 01:17:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:11][root][INFO] - Training Epoch: 3/10, step 540/574 completed (loss: 0.2838160991668701, acc: 0.9090909361839294)
[2025-01-06 01:17:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:12][root][INFO] - Training Epoch: 3/10, step 541/574 completed (loss: 0.14916396141052246, acc: 0.9375)
[2025-01-06 01:17:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:12][root][INFO] - Training Epoch: 3/10, step 542/574 completed (loss: 0.01344336662441492, acc: 1.0)
[2025-01-06 01:17:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:12][root][INFO] - Training Epoch: 3/10, step 543/574 completed (loss: 0.003782743588089943, acc: 1.0)
[2025-01-06 01:17:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:13][root][INFO] - Training Epoch: 3/10, step 544/574 completed (loss: 0.011140932328999043, acc: 1.0)
[2025-01-06 01:17:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:13][root][INFO] - Training Epoch: 3/10, step 545/574 completed (loss: 0.008075417019426823, acc: 1.0)
[2025-01-06 01:17:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:13][root][INFO] - Training Epoch: 3/10, step 546/574 completed (loss: 0.005549981724470854, acc: 1.0)
[2025-01-06 01:17:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:14][root][INFO] - Training Epoch: 3/10, step 547/574 completed (loss: 0.12973278760910034, acc: 0.9736841917037964)
[2025-01-06 01:17:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:14][root][INFO] - Training Epoch: 3/10, step 548/574 completed (loss: 0.013838226906955242, acc: 1.0)
[2025-01-06 01:17:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:14][root][INFO] - Training Epoch: 3/10, step 549/574 completed (loss: 0.000684328842908144, acc: 1.0)
[2025-01-06 01:17:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:15][root][INFO] - Training Epoch: 3/10, step 550/574 completed (loss: 0.07687634229660034, acc: 0.9696969985961914)
[2025-01-06 01:17:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:15][root][INFO] - Training Epoch: 3/10, step 551/574 completed (loss: 0.012788532301783562, acc: 1.0)
[2025-01-06 01:17:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:15][root][INFO] - Training Epoch: 3/10, step 552/574 completed (loss: 0.12413334101438522, acc: 0.9428571462631226)
[2025-01-06 01:17:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:16][root][INFO] - Training Epoch: 3/10, step 553/574 completed (loss: 0.31994009017944336, acc: 0.9270073175430298)
[2025-01-06 01:17:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:16][root][INFO] - Training Epoch: 3/10, step 554/574 completed (loss: 0.12431198358535767, acc: 0.9586206674575806)
[2025-01-06 01:17:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:17][root][INFO] - Training Epoch: 3/10, step 555/574 completed (loss: 0.24419236183166504, acc: 0.9357143044471741)
[2025-01-06 01:17:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:17][root][INFO] - Training Epoch: 3/10, step 556/574 completed (loss: 0.3908594250679016, acc: 0.9139072895050049)
[2025-01-06 01:17:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:17][root][INFO] - Training Epoch: 3/10, step 557/574 completed (loss: 0.10983113944530487, acc: 0.94017094373703)
[2025-01-06 01:17:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:18][root][INFO] - Training Epoch: 3/10, step 558/574 completed (loss: 0.031055428087711334, acc: 1.0)
[2025-01-06 01:17:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:18][root][INFO] - Training Epoch: 3/10, step 559/574 completed (loss: 0.007181466091424227, acc: 1.0)
[2025-01-06 01:17:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:18][root][INFO] - Training Epoch: 3/10, step 560/574 completed (loss: 0.018485071137547493, acc: 1.0)
[2025-01-06 01:17:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:19][root][INFO] - Training Epoch: 3/10, step 561/574 completed (loss: 0.0052594575099647045, acc: 1.0)
[2025-01-06 01:17:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:19][root][INFO] - Training Epoch: 3/10, step 562/574 completed (loss: 0.18675340712070465, acc: 0.9444444179534912)
[2025-01-06 01:17:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:19][root][INFO] - Training Epoch: 3/10, step 563/574 completed (loss: 0.3075474798679352, acc: 0.9220778942108154)
[2025-01-06 01:17:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:20][root][INFO] - Training Epoch: 3/10, step 564/574 completed (loss: 0.1598668247461319, acc: 0.9375)
[2025-01-06 01:17:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:20][root][INFO] - Training Epoch: 3/10, step 565/574 completed (loss: 0.15796233713626862, acc: 0.931034505367279)
[2025-01-06 01:17:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:21][root][INFO] - Training Epoch: 3/10, step 566/574 completed (loss: 0.18491461873054504, acc: 0.9523809552192688)
[2025-01-06 01:17:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:21][root][INFO] - Training Epoch: 3/10, step 567/574 completed (loss: 0.004706013947725296, acc: 1.0)
[2025-01-06 01:17:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:52][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.9934, device='cuda:0') eval_epoch_loss=tensor(0.6898, device='cuda:0') eval_epoch_acc=tensor(0.8399, device='cuda:0')
[2025-01-06 01:17:52][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:17:52][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:17:52][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_3_step_568_loss_0.6898293495178223/model.pt
[2025-01-06 01:17:52][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:17:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:52][root][INFO] - Training Epoch: 3/10, step 568/574 completed (loss: 0.0033794583287090063, acc: 1.0)
[2025-01-06 01:17:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:53][root][INFO] - Training Epoch: 3/10, step 569/574 completed (loss: 0.16301298141479492, acc: 0.9679144620895386)
[2025-01-06 01:17:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:53][root][INFO] - Training Epoch: 3/10, step 570/574 completed (loss: 0.006282580550760031, acc: 1.0)
[2025-01-06 01:17:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:53][root][INFO] - Training Epoch: 3/10, step 571/574 completed (loss: 0.015188757330179214, acc: 1.0)
[2025-01-06 01:17:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:54][root][INFO] - Training Epoch: 3/10, step 572/574 completed (loss: 0.27378612756729126, acc: 0.9285714030265808)
[2025-01-06 01:17:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:54][root][INFO] - Training Epoch: 3/10, step 573/574 completed (loss: 0.21220849454402924, acc: 0.9371069073677063)
[2025-01-06 01:17:54][slam_llm.utils.train_utils][INFO] - Epoch 3: train_perplexity=1.3584, train_epoch_loss=0.3063, epoch time 354.4841407351196s
[2025-01-06 01:17:54][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 13 GB
[2025-01-06 01:17:54][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 16 GB
[2025-01-06 01:17:54][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 13 GB
[2025-01-06 01:17:54][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 9
[2025-01-06 01:17:54][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2025-01-06 01:17:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:55][root][INFO] - Training Epoch: 4/10, step 0/574 completed (loss: 0.1996583789587021, acc: 0.9629629850387573)
[2025-01-06 01:17:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:56][root][INFO] - Training Epoch: 4/10, step 1/574 completed (loss: 0.031966567039489746, acc: 1.0)
[2025-01-06 01:17:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:56][root][INFO] - Training Epoch: 4/10, step 2/574 completed (loss: 0.46868789196014404, acc: 0.8648648858070374)
[2025-01-06 01:17:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:56][root][INFO] - Training Epoch: 4/10, step 3/574 completed (loss: 0.19846117496490479, acc: 0.9473684430122375)
[2025-01-06 01:17:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:57][root][INFO] - Training Epoch: 4/10, step 4/574 completed (loss: 0.128665491938591, acc: 0.9729729890823364)
[2025-01-06 01:17:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:57][root][INFO] - Training Epoch: 4/10, step 5/574 completed (loss: 0.040931787341833115, acc: 1.0)
[2025-01-06 01:17:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:58][root][INFO] - Training Epoch: 4/10, step 6/574 completed (loss: 0.17619813978672028, acc: 0.9387755393981934)
[2025-01-06 01:17:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:58][root][INFO] - Training Epoch: 4/10, step 7/574 completed (loss: 0.028211809694767, acc: 1.0)
[2025-01-06 01:17:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:58][root][INFO] - Training Epoch: 4/10, step 8/574 completed (loss: 0.020906299352645874, acc: 1.0)
[2025-01-06 01:17:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:59][root][INFO] - Training Epoch: 4/10, step 9/574 completed (loss: 0.02819697931408882, acc: 1.0)
[2025-01-06 01:17:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:59][root][INFO] - Training Epoch: 4/10, step 10/574 completed (loss: 0.007154356222599745, acc: 1.0)
[2025-01-06 01:17:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:17:59][root][INFO] - Training Epoch: 4/10, step 11/574 completed (loss: 0.08257442712783813, acc: 0.9743589758872986)
[2025-01-06 01:17:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:00][root][INFO] - Training Epoch: 4/10, step 12/574 completed (loss: 0.015089014545083046, acc: 1.0)
[2025-01-06 01:18:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:00][root][INFO] - Training Epoch: 4/10, step 13/574 completed (loss: 0.12039637565612793, acc: 0.9347826242446899)
[2025-01-06 01:18:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:00][root][INFO] - Training Epoch: 4/10, step 14/574 completed (loss: 0.023094967007637024, acc: 1.0)
[2025-01-06 01:18:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:01][root][INFO] - Training Epoch: 4/10, step 15/574 completed (loss: 0.15798325836658478, acc: 0.9591836929321289)
[2025-01-06 01:18:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:01][root][INFO] - Training Epoch: 4/10, step 16/574 completed (loss: 0.08776254206895828, acc: 0.9473684430122375)
[2025-01-06 01:18:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:02][root][INFO] - Training Epoch: 4/10, step 17/574 completed (loss: 0.12109509855508804, acc: 0.9583333134651184)
[2025-01-06 01:18:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:02][root][INFO] - Training Epoch: 4/10, step 18/574 completed (loss: 0.11636222898960114, acc: 0.9444444179534912)
[2025-01-06 01:18:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:02][root][INFO] - Training Epoch: 4/10, step 19/574 completed (loss: 0.014693553559482098, acc: 1.0)
[2025-01-06 01:18:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:03][root][INFO] - Training Epoch: 4/10, step 20/574 completed (loss: 0.15888606011867523, acc: 0.9615384340286255)
[2025-01-06 01:18:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:03][root][INFO] - Training Epoch: 4/10, step 21/574 completed (loss: 0.011280067265033722, acc: 1.0)
[2025-01-06 01:18:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:03][root][INFO] - Training Epoch: 4/10, step 22/574 completed (loss: 0.10566629469394684, acc: 0.9200000166893005)
[2025-01-06 01:18:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:04][root][INFO] - Training Epoch: 4/10, step 23/574 completed (loss: 0.1673641949892044, acc: 0.9523809552192688)
[2025-01-06 01:18:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:04][root][INFO] - Training Epoch: 4/10, step 24/574 completed (loss: 0.10730976611375809, acc: 0.9375)
[2025-01-06 01:18:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:04][root][INFO] - Training Epoch: 4/10, step 25/574 completed (loss: 0.19530561566352844, acc: 0.9433962106704712)
[2025-01-06 01:18:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:05][root][INFO] - Training Epoch: 4/10, step 26/574 completed (loss: 0.27266740798950195, acc: 0.9041095972061157)
[2025-01-06 01:18:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:06][root][INFO] - Training Epoch: 4/10, step 27/574 completed (loss: 0.6642839908599854, acc: 0.8023715615272522)
[2025-01-06 01:18:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:06][root][INFO] - Training Epoch: 4/10, step 28/574 completed (loss: 0.1770678162574768, acc: 0.9534883499145508)
[2025-01-06 01:18:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:07][root][INFO] - Training Epoch: 4/10, step 29/574 completed (loss: 0.2717043459415436, acc: 0.9036144614219666)
[2025-01-06 01:18:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:07][root][INFO] - Training Epoch: 4/10, step 30/574 completed (loss: 0.23775623738765717, acc: 0.9259259104728699)
[2025-01-06 01:18:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:07][root][INFO] - Training Epoch: 4/10, step 31/574 completed (loss: 0.10505043715238571, acc: 0.9642857313156128)
[2025-01-06 01:18:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:08][root][INFO] - Training Epoch: 4/10, step 32/574 completed (loss: 0.05852092057466507, acc: 0.9629629850387573)
[2025-01-06 01:18:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:08][root][INFO] - Training Epoch: 4/10, step 33/574 completed (loss: 0.0024667850229889154, acc: 1.0)
[2025-01-06 01:18:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:09][root][INFO] - Training Epoch: 4/10, step 34/574 completed (loss: 0.45038875937461853, acc: 0.8739495873451233)
[2025-01-06 01:18:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:09][root][INFO] - Training Epoch: 4/10, step 35/574 completed (loss: 0.09871965646743774, acc: 0.9672130942344666)
[2025-01-06 01:18:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:09][root][INFO] - Training Epoch: 4/10, step 36/574 completed (loss: 0.23373009264469147, acc: 0.9047619104385376)
[2025-01-06 01:18:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:10][root][INFO] - Training Epoch: 4/10, step 37/574 completed (loss: 0.2649860382080078, acc: 0.9322034120559692)
[2025-01-06 01:18:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:10][root][INFO] - Training Epoch: 4/10, step 38/574 completed (loss: 0.21694745123386383, acc: 0.931034505367279)
[2025-01-06 01:18:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:10][root][INFO] - Training Epoch: 4/10, step 39/574 completed (loss: 0.3777976930141449, acc: 0.9523809552192688)
[2025-01-06 01:18:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:11][root][INFO] - Training Epoch: 4/10, step 40/574 completed (loss: 0.18918849527835846, acc: 0.9615384340286255)
[2025-01-06 01:18:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:11][root][INFO] - Training Epoch: 4/10, step 41/574 completed (loss: 0.16448856890201569, acc: 0.9324324131011963)
[2025-01-06 01:18:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:11][root][INFO] - Training Epoch: 4/10, step 42/574 completed (loss: 0.2602345943450928, acc: 0.9076923131942749)
[2025-01-06 01:18:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:12][root][INFO] - Training Epoch: 4/10, step 43/574 completed (loss: 0.4288994371891022, acc: 0.8787878751754761)
[2025-01-06 01:18:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:12][root][INFO] - Training Epoch: 4/10, step 44/574 completed (loss: 0.22599273920059204, acc: 0.9278350472450256)
[2025-01-06 01:18:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:13][root][INFO] - Training Epoch: 4/10, step 45/574 completed (loss: 0.26924586296081543, acc: 0.9264705777168274)
[2025-01-06 01:18:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:13][root][INFO] - Training Epoch: 4/10, step 46/574 completed (loss: 0.046013299375772476, acc: 1.0)
[2025-01-06 01:18:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:13][root][INFO] - Training Epoch: 4/10, step 47/574 completed (loss: 0.06798674166202545, acc: 1.0)
[2025-01-06 01:18:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:14][root][INFO] - Training Epoch: 4/10, step 48/574 completed (loss: 0.09822166711091995, acc: 0.9642857313156128)
[2025-01-06 01:18:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:14][root][INFO] - Training Epoch: 4/10, step 49/574 completed (loss: 0.009844095446169376, acc: 1.0)
[2025-01-06 01:18:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:14][root][INFO] - Training Epoch: 4/10, step 50/574 completed (loss: 0.2993212938308716, acc: 0.9122806787490845)
[2025-01-06 01:18:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:15][root][INFO] - Training Epoch: 4/10, step 51/574 completed (loss: 0.28032490611076355, acc: 0.9047619104385376)
[2025-01-06 01:18:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:15][root][INFO] - Training Epoch: 4/10, step 52/574 completed (loss: 0.3943198025226593, acc: 0.8591549396514893)
[2025-01-06 01:18:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:16][root][INFO] - Training Epoch: 4/10, step 53/574 completed (loss: 0.9674098491668701, acc: 0.7133333086967468)
[2025-01-06 01:18:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:16][root][INFO] - Training Epoch: 4/10, step 54/574 completed (loss: 0.1560957133769989, acc: 0.9729729890823364)
[2025-01-06 01:18:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:16][root][INFO] - Training Epoch: 4/10, step 55/574 completed (loss: 0.055411938577890396, acc: 0.9615384340286255)
[2025-01-06 01:18:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:19][root][INFO] - Training Epoch: 4/10, step 56/574 completed (loss: 0.769967257976532, acc: 0.7849829196929932)
[2025-01-06 01:18:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:20][root][INFO] - Training Epoch: 4/10, step 57/574 completed (loss: 0.9412669539451599, acc: 0.7298474907875061)
[2025-01-06 01:18:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:21][root][INFO] - Training Epoch: 4/10, step 58/574 completed (loss: 0.5850971937179565, acc: 0.8068181872367859)
[2025-01-06 01:18:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:22][root][INFO] - Training Epoch: 4/10, step 59/574 completed (loss: 0.2051413655281067, acc: 0.9485294222831726)
[2025-01-06 01:18:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:22][root][INFO] - Training Epoch: 4/10, step 60/574 completed (loss: 0.5459245443344116, acc: 0.8260869383811951)
[2025-01-06 01:18:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:23][root][INFO] - Training Epoch: 4/10, step 61/574 completed (loss: 0.46979865431785583, acc: 0.875)
[2025-01-06 01:18:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:23][root][INFO] - Training Epoch: 4/10, step 62/574 completed (loss: 0.06686180084943771, acc: 1.0)
[2025-01-06 01:18:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:23][root][INFO] - Training Epoch: 4/10, step 63/574 completed (loss: 0.3438034653663635, acc: 0.9444444179534912)
[2025-01-06 01:18:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:24][root][INFO] - Training Epoch: 4/10, step 64/574 completed (loss: 0.03671806678175926, acc: 1.0)
[2025-01-06 01:18:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:24][root][INFO] - Training Epoch: 4/10, step 65/574 completed (loss: 0.031099168583750725, acc: 1.0)
[2025-01-06 01:18:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:24][root][INFO] - Training Epoch: 4/10, step 66/574 completed (loss: 0.32624199986457825, acc: 0.9107142686843872)
[2025-01-06 01:18:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:25][root][INFO] - Training Epoch: 4/10, step 67/574 completed (loss: 0.11142483353614807, acc: 0.949999988079071)
[2025-01-06 01:18:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:25][root][INFO] - Training Epoch: 4/10, step 68/574 completed (loss: 0.0027449852786958218, acc: 1.0)
[2025-01-06 01:18:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:26][root][INFO] - Training Epoch: 4/10, step 69/574 completed (loss: 0.1115742102265358, acc: 1.0)
[2025-01-06 01:18:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:26][root][INFO] - Training Epoch: 4/10, step 70/574 completed (loss: 0.1482153683900833, acc: 1.0)
[2025-01-06 01:18:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:26][root][INFO] - Training Epoch: 4/10, step 71/574 completed (loss: 0.6315156817436218, acc: 0.8088235259056091)
[2025-01-06 01:18:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:27][root][INFO] - Training Epoch: 4/10, step 72/574 completed (loss: 0.5054418444633484, acc: 0.8809523582458496)
[2025-01-06 01:18:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:27][root][INFO] - Training Epoch: 4/10, step 73/574 completed (loss: 0.8679264187812805, acc: 0.7384615540504456)
[2025-01-06 01:18:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:27][root][INFO] - Training Epoch: 4/10, step 74/574 completed (loss: 0.5858290791511536, acc: 0.8571428656578064)
[2025-01-06 01:18:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:28][root][INFO] - Training Epoch: 4/10, step 75/574 completed (loss: 0.8181937336921692, acc: 0.7761194109916687)
[2025-01-06 01:18:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:28][root][INFO] - Training Epoch: 4/10, step 76/574 completed (loss: 1.1479378938674927, acc: 0.6751824617385864)
[2025-01-06 01:18:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:29][root][INFO] - Training Epoch: 4/10, step 77/574 completed (loss: 0.007191786542534828, acc: 1.0)
[2025-01-06 01:18:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:29][root][INFO] - Training Epoch: 4/10, step 78/574 completed (loss: 0.03326563164591789, acc: 1.0)
[2025-01-06 01:18:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:29][root][INFO] - Training Epoch: 4/10, step 79/574 completed (loss: 0.010837143287062645, acc: 1.0)
[2025-01-06 01:18:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:30][root][INFO] - Training Epoch: 4/10, step 80/574 completed (loss: 0.007790414150804281, acc: 1.0)
[2025-01-06 01:18:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:30][root][INFO] - Training Epoch: 4/10, step 81/574 completed (loss: 0.23555408418178558, acc: 0.9230769276618958)
[2025-01-06 01:18:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:30][root][INFO] - Training Epoch: 4/10, step 82/574 completed (loss: 0.28636425733566284, acc: 0.9230769276618958)
[2025-01-06 01:18:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:31][root][INFO] - Training Epoch: 4/10, step 83/574 completed (loss: 0.09524978697299957, acc: 0.96875)
[2025-01-06 01:18:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:31][root][INFO] - Training Epoch: 4/10, step 84/574 completed (loss: 0.2277638167142868, acc: 0.9275362491607666)
[2025-01-06 01:18:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:31][root][INFO] - Training Epoch: 4/10, step 85/574 completed (loss: 0.1274494230747223, acc: 0.9399999976158142)
[2025-01-06 01:18:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:32][root][INFO] - Training Epoch: 4/10, step 86/574 completed (loss: 0.040653493255376816, acc: 1.0)
[2025-01-06 01:18:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:32][root][INFO] - Training Epoch: 4/10, step 87/574 completed (loss: 0.24080118536949158, acc: 0.9399999976158142)
[2025-01-06 01:18:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:33][root][INFO] - Training Epoch: 4/10, step 88/574 completed (loss: 0.494332879781723, acc: 0.8640776872634888)
[2025-01-06 01:18:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:34][root][INFO] - Training Epoch: 4/10, step 89/574 completed (loss: 0.6845685839653015, acc: 0.8155339956283569)
[2025-01-06 01:18:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:35][root][INFO] - Training Epoch: 4/10, step 90/574 completed (loss: 0.6752414107322693, acc: 0.8225806355476379)
[2025-01-06 01:18:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:35][root][INFO] - Training Epoch: 4/10, step 91/574 completed (loss: 0.6852740049362183, acc: 0.8275862336158752)
[2025-01-06 01:18:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:36][root][INFO] - Training Epoch: 4/10, step 92/574 completed (loss: 0.43559303879737854, acc: 0.8842105269432068)
[2025-01-06 01:18:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:37][root][INFO] - Training Epoch: 4/10, step 93/574 completed (loss: 0.7083930373191833, acc: 0.8118811845779419)
[2025-01-06 01:18:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:37][root][INFO] - Training Epoch: 4/10, step 94/574 completed (loss: 0.5048126578330994, acc: 0.8548387289047241)
[2025-01-06 01:18:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:38][root][INFO] - Training Epoch: 4/10, step 95/574 completed (loss: 0.45513883233070374, acc: 0.8550724387168884)
[2025-01-06 01:18:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:38][root][INFO] - Training Epoch: 4/10, step 96/574 completed (loss: 0.7449186444282532, acc: 0.7478991746902466)
[2025-01-06 01:18:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:39][root][INFO] - Training Epoch: 4/10, step 97/574 completed (loss: 0.4880842864513397, acc: 0.8942307829856873)
[2025-01-06 01:18:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:39][root][INFO] - Training Epoch: 4/10, step 98/574 completed (loss: 0.7362006902694702, acc: 0.7664233446121216)
[2025-01-06 01:18:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:39][root][INFO] - Training Epoch: 4/10, step 99/574 completed (loss: 0.5741999745368958, acc: 0.7761194109916687)
[2025-01-06 01:18:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:40][root][INFO] - Training Epoch: 4/10, step 100/574 completed (loss: 0.06283441185951233, acc: 1.0)
[2025-01-06 01:18:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:40][root][INFO] - Training Epoch: 4/10, step 101/574 completed (loss: 0.005626875441521406, acc: 1.0)
[2025-01-06 01:18:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:40][root][INFO] - Training Epoch: 4/10, step 102/574 completed (loss: 0.010245045647025108, acc: 1.0)
[2025-01-06 01:18:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:41][root][INFO] - Training Epoch: 4/10, step 103/574 completed (loss: 0.03339057043194771, acc: 0.9772727489471436)
[2025-01-06 01:18:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:41][root][INFO] - Training Epoch: 4/10, step 104/574 completed (loss: 0.2992328405380249, acc: 0.9137930870056152)
[2025-01-06 01:18:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:41][root][INFO] - Training Epoch: 4/10, step 105/574 completed (loss: 0.03414946794509888, acc: 1.0)
[2025-01-06 01:18:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:42][root][INFO] - Training Epoch: 4/10, step 106/574 completed (loss: 0.03786788880825043, acc: 1.0)
[2025-01-06 01:18:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:42][root][INFO] - Training Epoch: 4/10, step 107/574 completed (loss: 0.09966220706701279, acc: 0.9411764740943909)
[2025-01-06 01:18:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:43][root][INFO] - Training Epoch: 4/10, step 108/574 completed (loss: 0.00731790903955698, acc: 1.0)
[2025-01-06 01:18:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:43][root][INFO] - Training Epoch: 4/10, step 109/574 completed (loss: 0.012675918638706207, acc: 1.0)
[2025-01-06 01:18:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:43][root][INFO] - Training Epoch: 4/10, step 110/574 completed (loss: 0.04630538076162338, acc: 0.9846153855323792)
[2025-01-06 01:18:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:44][root][INFO] - Training Epoch: 4/10, step 111/574 completed (loss: 0.21631698310375214, acc: 0.9473684430122375)
[2025-01-06 01:18:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:44][root][INFO] - Training Epoch: 4/10, step 112/574 completed (loss: 0.18383556604385376, acc: 0.9122806787490845)
[2025-01-06 01:18:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:44][root][INFO] - Training Epoch: 4/10, step 113/574 completed (loss: 0.11149438470602036, acc: 0.9230769276618958)
[2025-01-06 01:18:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:45][root][INFO] - Training Epoch: 4/10, step 114/574 completed (loss: 0.06753245741128922, acc: 1.0)
[2025-01-06 01:18:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:45][root][INFO] - Training Epoch: 4/10, step 115/574 completed (loss: 0.002489113248884678, acc: 1.0)
[2025-01-06 01:18:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:46][root][INFO] - Training Epoch: 4/10, step 116/574 completed (loss: 0.3020956516265869, acc: 0.920634925365448)
[2025-01-06 01:18:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:46][root][INFO] - Training Epoch: 4/10, step 117/574 completed (loss: 0.2549746334552765, acc: 0.9186992049217224)
[2025-01-06 01:18:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:46][root][INFO] - Training Epoch: 4/10, step 118/574 completed (loss: 0.044575080275535583, acc: 1.0)
[2025-01-06 01:18:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:47][root][INFO] - Training Epoch: 4/10, step 119/574 completed (loss: 0.341438889503479, acc: 0.9201520681381226)
[2025-01-06 01:18:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:47][root][INFO] - Training Epoch: 4/10, step 120/574 completed (loss: 0.16173511743545532, acc: 0.9466666579246521)
[2025-01-06 01:18:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:48][root][INFO] - Training Epoch: 4/10, step 121/574 completed (loss: 0.18625925481319427, acc: 0.942307710647583)
[2025-01-06 01:18:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:48][root][INFO] - Training Epoch: 4/10, step 122/574 completed (loss: 0.10568661242723465, acc: 0.9583333134651184)
[2025-01-06 01:18:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:49][root][INFO] - Training Epoch: 4/10, step 123/574 completed (loss: 0.4254663288593292, acc: 0.8947368264198303)
[2025-01-06 01:18:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:49][root][INFO] - Training Epoch: 4/10, step 124/574 completed (loss: 0.6176135540008545, acc: 0.8404908180236816)
[2025-01-06 01:18:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:49][root][INFO] - Training Epoch: 4/10, step 125/574 completed (loss: 0.738135576248169, acc: 0.7916666865348816)
[2025-01-06 01:18:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:50][root][INFO] - Training Epoch: 4/10, step 126/574 completed (loss: 0.7560954689979553, acc: 0.7666666507720947)
[2025-01-06 01:18:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:50][root][INFO] - Training Epoch: 4/10, step 127/574 completed (loss: 0.36501097679138184, acc: 0.8571428656578064)
[2025-01-06 01:18:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:51][root][INFO] - Training Epoch: 4/10, step 128/574 completed (loss: 0.5052440762519836, acc: 0.8512820601463318)
[2025-01-06 01:18:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:51][root][INFO] - Training Epoch: 4/10, step 129/574 completed (loss: 0.506032407283783, acc: 0.8602941036224365)
[2025-01-06 01:18:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:51][root][INFO] - Training Epoch: 4/10, step 130/574 completed (loss: 0.11641691625118256, acc: 0.9615384340286255)
[2025-01-06 01:18:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:52][root][INFO] - Training Epoch: 4/10, step 131/574 completed (loss: 0.0747826099395752, acc: 1.0)
[2025-01-06 01:18:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:52][root][INFO] - Training Epoch: 4/10, step 132/574 completed (loss: 0.05964542552828789, acc: 1.0)
[2025-01-06 01:18:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:52][root][INFO] - Training Epoch: 4/10, step 133/574 completed (loss: 0.34560146927833557, acc: 0.8260869383811951)
[2025-01-06 01:18:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:53][root][INFO] - Training Epoch: 4/10, step 134/574 completed (loss: 0.07007449120283127, acc: 1.0)
[2025-01-06 01:18:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:53][root][INFO] - Training Epoch: 4/10, step 135/574 completed (loss: 0.07223968952894211, acc: 0.9615384340286255)
[2025-01-06 01:18:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:53][root][INFO] - Training Epoch: 4/10, step 136/574 completed (loss: 0.11767131835222244, acc: 0.9523809552192688)
[2025-01-06 01:18:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:18:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:24][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.9666, device='cuda:0') eval_epoch_loss=tensor(0.6763, device='cuda:0') eval_epoch_acc=tensor(0.8433, device='cuda:0')
[2025-01-06 01:19:24][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:19:24][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:19:25][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_4_step_137_loss_0.6762973666191101/model.pt
[2025-01-06 01:19:25][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:19:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:25][root][INFO] - Training Epoch: 4/10, step 137/574 completed (loss: 0.19335833191871643, acc: 0.9666666388511658)
[2025-01-06 01:19:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:25][root][INFO] - Training Epoch: 4/10, step 138/574 completed (loss: 0.013754788786172867, acc: 1.0)
[2025-01-06 01:19:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:26][root][INFO] - Training Epoch: 4/10, step 139/574 completed (loss: 0.010448718443512917, acc: 1.0)
[2025-01-06 01:19:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:26][root][INFO] - Training Epoch: 4/10, step 140/574 completed (loss: 0.03812042251229286, acc: 1.0)
[2025-01-06 01:19:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:26][root][INFO] - Training Epoch: 4/10, step 141/574 completed (loss: 0.032908838242292404, acc: 1.0)
[2025-01-06 01:19:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:27][root][INFO] - Training Epoch: 4/10, step 142/574 completed (loss: 0.3475523889064789, acc: 0.9729729890823364)
[2025-01-06 01:19:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:27][root][INFO] - Training Epoch: 4/10, step 143/574 completed (loss: 0.48300522565841675, acc: 0.8157894611358643)
[2025-01-06 01:19:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:28][root][INFO] - Training Epoch: 4/10, step 144/574 completed (loss: 0.5282967686653137, acc: 0.8507462739944458)
[2025-01-06 01:19:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:28][root][INFO] - Training Epoch: 4/10, step 145/574 completed (loss: 0.2805593013763428, acc: 0.9081632494926453)
[2025-01-06 01:19:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:28][root][INFO] - Training Epoch: 4/10, step 146/574 completed (loss: 0.6448691487312317, acc: 0.7659574747085571)
[2025-01-06 01:19:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:29][root][INFO] - Training Epoch: 4/10, step 147/574 completed (loss: 0.23081493377685547, acc: 0.9142857193946838)
[2025-01-06 01:19:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:29][root][INFO] - Training Epoch: 4/10, step 148/574 completed (loss: 0.23006601631641388, acc: 0.9285714030265808)
[2025-01-06 01:19:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:29][root][INFO] - Training Epoch: 4/10, step 149/574 completed (loss: 0.12275685369968414, acc: 1.0)
[2025-01-06 01:19:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:30][root][INFO] - Training Epoch: 4/10, step 150/574 completed (loss: 0.0590311698615551, acc: 1.0)
[2025-01-06 01:19:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:30][root][INFO] - Training Epoch: 4/10, step 151/574 completed (loss: 0.3261245787143707, acc: 0.9130434989929199)
[2025-01-06 01:19:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:30][root][INFO] - Training Epoch: 4/10, step 152/574 completed (loss: 0.35226866602897644, acc: 0.9152542352676392)
[2025-01-06 01:19:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:31][root][INFO] - Training Epoch: 4/10, step 153/574 completed (loss: 0.16860798001289368, acc: 0.9298245906829834)
[2025-01-06 01:19:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:31][root][INFO] - Training Epoch: 4/10, step 154/574 completed (loss: 0.46785885095596313, acc: 0.8513513803482056)
[2025-01-06 01:19:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:32][root][INFO] - Training Epoch: 4/10, step 155/574 completed (loss: 0.015291711315512657, acc: 1.0)
[2025-01-06 01:19:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:32][root][INFO] - Training Epoch: 4/10, step 156/574 completed (loss: 0.11447949707508087, acc: 0.95652174949646)
[2025-01-06 01:19:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:32][root][INFO] - Training Epoch: 4/10, step 157/574 completed (loss: 0.6473756432533264, acc: 0.7894737124443054)
[2025-01-06 01:19:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:34][root][INFO] - Training Epoch: 4/10, step 158/574 completed (loss: 0.5220425128936768, acc: 0.8918918967247009)
[2025-01-06 01:19:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:34][root][INFO] - Training Epoch: 4/10, step 159/574 completed (loss: 0.5962398052215576, acc: 0.8333333134651184)
[2025-01-06 01:19:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:35][root][INFO] - Training Epoch: 4/10, step 160/574 completed (loss: 0.7051142454147339, acc: 0.8139534592628479)
[2025-01-06 01:19:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:35][root][INFO] - Training Epoch: 4/10, step 161/574 completed (loss: 0.5430803894996643, acc: 0.8352941274642944)
[2025-01-06 01:19:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:36][root][INFO] - Training Epoch: 4/10, step 162/574 completed (loss: 0.7601079940795898, acc: 0.7977527976036072)
[2025-01-06 01:19:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:36][root][INFO] - Training Epoch: 4/10, step 163/574 completed (loss: 0.17807914316654205, acc: 0.9545454382896423)
[2025-01-06 01:19:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:37][root][INFO] - Training Epoch: 4/10, step 164/574 completed (loss: 0.08603433519601822, acc: 0.9047619104385376)
[2025-01-06 01:19:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:37][root][INFO] - Training Epoch: 4/10, step 165/574 completed (loss: 0.15653887391090393, acc: 0.931034505367279)
[2025-01-06 01:19:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:37][root][INFO] - Training Epoch: 4/10, step 166/574 completed (loss: 0.03910769522190094, acc: 0.9795918464660645)
[2025-01-06 01:19:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:38][root][INFO] - Training Epoch: 4/10, step 167/574 completed (loss: 0.09027586132287979, acc: 1.0)
[2025-01-06 01:19:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:38][root][INFO] - Training Epoch: 4/10, step 168/574 completed (loss: 0.2134813666343689, acc: 0.9166666865348816)
[2025-01-06 01:19:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:38][root][INFO] - Training Epoch: 4/10, step 169/574 completed (loss: 0.5606650710105896, acc: 0.8333333134651184)
[2025-01-06 01:19:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:39][root][INFO] - Training Epoch: 4/10, step 170/574 completed (loss: 0.3983875811100006, acc: 0.8561643958091736)
[2025-01-06 01:19:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:40][root][INFO] - Training Epoch: 4/10, step 171/574 completed (loss: 0.03010927326977253, acc: 1.0)
[2025-01-06 01:19:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:40][root][INFO] - Training Epoch: 4/10, step 172/574 completed (loss: 0.6197677850723267, acc: 0.8518518805503845)
[2025-01-06 01:19:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:41][root][INFO] - Training Epoch: 4/10, step 173/574 completed (loss: 0.1473601758480072, acc: 0.8928571343421936)
[2025-01-06 01:19:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:41][root][INFO] - Training Epoch: 4/10, step 174/574 completed (loss: 0.790982186794281, acc: 0.7876105904579163)
[2025-01-06 01:19:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:41][root][INFO] - Training Epoch: 4/10, step 175/574 completed (loss: 0.5092169642448425, acc: 0.8695651888847351)
[2025-01-06 01:19:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:42][root][INFO] - Training Epoch: 4/10, step 176/574 completed (loss: 0.2964051067829132, acc: 0.8863636255264282)
[2025-01-06 01:19:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:43][root][INFO] - Training Epoch: 4/10, step 177/574 completed (loss: 0.695491373538971, acc: 0.8320610523223877)
[2025-01-06 01:19:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:43][root][INFO] - Training Epoch: 4/10, step 178/574 completed (loss: 0.6156279444694519, acc: 0.8296296000480652)
[2025-01-06 01:19:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:44][root][INFO] - Training Epoch: 4/10, step 179/574 completed (loss: 0.13614118099212646, acc: 0.9508196711540222)
[2025-01-06 01:19:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:44][root][INFO] - Training Epoch: 4/10, step 180/574 completed (loss: 0.0213368758559227, acc: 1.0)
[2025-01-06 01:19:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:44][root][INFO] - Training Epoch: 4/10, step 181/574 completed (loss: 0.0078887315467, acc: 1.0)
[2025-01-06 01:19:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:45][root][INFO] - Training Epoch: 4/10, step 182/574 completed (loss: 0.009907637722790241, acc: 1.0)
[2025-01-06 01:19:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:45][root][INFO] - Training Epoch: 4/10, step 183/574 completed (loss: 0.06267630308866501, acc: 0.9634146094322205)
[2025-01-06 01:19:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:46][root][INFO] - Training Epoch: 4/10, step 184/574 completed (loss: 0.3253243863582611, acc: 0.9244713187217712)
[2025-01-06 01:19:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:46][root][INFO] - Training Epoch: 4/10, step 185/574 completed (loss: 0.35275405645370483, acc: 0.9135446548461914)
[2025-01-06 01:19:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:46][root][INFO] - Training Epoch: 4/10, step 186/574 completed (loss: 0.3491693139076233, acc: 0.890625)
[2025-01-06 01:19:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:47][root][INFO] - Training Epoch: 4/10, step 187/574 completed (loss: 0.41729649901390076, acc: 0.8874296545982361)
[2025-01-06 01:19:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:47][root][INFO] - Training Epoch: 4/10, step 188/574 completed (loss: 0.37732914090156555, acc: 0.8896797299385071)
[2025-01-06 01:19:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:48][root][INFO] - Training Epoch: 4/10, step 189/574 completed (loss: 0.029762396588921547, acc: 1.0)
[2025-01-06 01:19:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:48][root][INFO] - Training Epoch: 4/10, step 190/574 completed (loss: 0.5553146004676819, acc: 0.8372092843055725)
[2025-01-06 01:19:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:49][root][INFO] - Training Epoch: 4/10, step 191/574 completed (loss: 0.9134002327919006, acc: 0.7301587462425232)
[2025-01-06 01:19:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:50][root][INFO] - Training Epoch: 4/10, step 192/574 completed (loss: 0.5309584736824036, acc: 0.8560606241226196)
[2025-01-06 01:19:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:51][root][INFO] - Training Epoch: 4/10, step 193/574 completed (loss: 0.37287989258766174, acc: 0.8588235378265381)
[2025-01-06 01:19:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:52][root][INFO] - Training Epoch: 4/10, step 194/574 completed (loss: 0.6767848134040833, acc: 0.7962962985038757)
[2025-01-06 01:19:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:53][root][INFO] - Training Epoch: 4/10, step 195/574 completed (loss: 0.22115448117256165, acc: 0.9193548560142517)
[2025-01-06 01:19:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:53][root][INFO] - Training Epoch: 4/10, step 196/574 completed (loss: 0.14103302359580994, acc: 0.9642857313156128)
[2025-01-06 01:19:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:53][root][INFO] - Training Epoch: 4/10, step 197/574 completed (loss: 0.28404727578163147, acc: 0.8999999761581421)
[2025-01-06 01:19:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:54][root][INFO] - Training Epoch: 4/10, step 198/574 completed (loss: 0.29965922236442566, acc: 0.8970588445663452)
[2025-01-06 01:19:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:54][root][INFO] - Training Epoch: 4/10, step 199/574 completed (loss: 0.5650362372398376, acc: 0.8382353186607361)
[2025-01-06 01:19:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:54][root][INFO] - Training Epoch: 4/10, step 200/574 completed (loss: 0.3718961179256439, acc: 0.8813559412956238)
[2025-01-06 01:19:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:55][root][INFO] - Training Epoch: 4/10, step 201/574 completed (loss: 0.5114176869392395, acc: 0.8507462739944458)
[2025-01-06 01:19:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:55][root][INFO] - Training Epoch: 4/10, step 202/574 completed (loss: 0.5312240123748779, acc: 0.844660222530365)
[2025-01-06 01:19:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:56][root][INFO] - Training Epoch: 4/10, step 203/574 completed (loss: 0.24105329811573029, acc: 0.9047619104385376)
[2025-01-06 01:19:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:56][root][INFO] - Training Epoch: 4/10, step 204/574 completed (loss: 0.07672405987977982, acc: 0.9670329689979553)
[2025-01-06 01:19:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:56][root][INFO] - Training Epoch: 4/10, step 205/574 completed (loss: 0.17883118987083435, acc: 0.9551569223403931)
[2025-01-06 01:19:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:57][root][INFO] - Training Epoch: 4/10, step 206/574 completed (loss: 0.2879548966884613, acc: 0.8976377844810486)
[2025-01-06 01:19:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:57][root][INFO] - Training Epoch: 4/10, step 207/574 completed (loss: 0.19498759508132935, acc: 0.943965494632721)
[2025-01-06 01:19:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:57][root][INFO] - Training Epoch: 4/10, step 208/574 completed (loss: 0.30975306034088135, acc: 0.9202898740768433)
[2025-01-06 01:19:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:58][root][INFO] - Training Epoch: 4/10, step 209/574 completed (loss: 0.2773122489452362, acc: 0.9105058312416077)
[2025-01-06 01:19:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:58][root][INFO] - Training Epoch: 4/10, step 210/574 completed (loss: 0.057757433503866196, acc: 0.97826087474823)
[2025-01-06 01:19:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:58][root][INFO] - Training Epoch: 4/10, step 211/574 completed (loss: 0.018127724528312683, acc: 1.0)
[2025-01-06 01:19:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:59][root][INFO] - Training Epoch: 4/10, step 212/574 completed (loss: 0.023736190050840378, acc: 1.0)
[2025-01-06 01:19:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:19:59][root][INFO] - Training Epoch: 4/10, step 213/574 completed (loss: 0.13652071356773376, acc: 0.936170220375061)
[2025-01-06 01:19:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:00][root][INFO] - Training Epoch: 4/10, step 214/574 completed (loss: 0.10573767125606537, acc: 0.9538461565971375)
[2025-01-06 01:20:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:00][root][INFO] - Training Epoch: 4/10, step 215/574 completed (loss: 0.023275073617696762, acc: 1.0)
[2025-01-06 01:20:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:01][root][INFO] - Training Epoch: 4/10, step 216/574 completed (loss: 0.0729002133011818, acc: 0.9883720874786377)
[2025-01-06 01:20:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:01][root][INFO] - Training Epoch: 4/10, step 217/574 completed (loss: 0.10350189357995987, acc: 0.954954981803894)
[2025-01-06 01:20:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:02][root][INFO] - Training Epoch: 4/10, step 218/574 completed (loss: 0.08804280310869217, acc: 0.9777777791023254)
[2025-01-06 01:20:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:02][root][INFO] - Training Epoch: 4/10, step 219/574 completed (loss: 0.10539007186889648, acc: 0.9696969985961914)
[2025-01-06 01:20:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:02][root][INFO] - Training Epoch: 4/10, step 220/574 completed (loss: 0.009984738193452358, acc: 1.0)
[2025-01-06 01:20:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:03][root][INFO] - Training Epoch: 4/10, step 221/574 completed (loss: 0.006580662913620472, acc: 1.0)
[2025-01-06 01:20:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:03][root][INFO] - Training Epoch: 4/10, step 222/574 completed (loss: 0.1589205414056778, acc: 0.942307710647583)
[2025-01-06 01:20:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:04][root][INFO] - Training Epoch: 4/10, step 223/574 completed (loss: 0.21787035465240479, acc: 0.9347826242446899)
[2025-01-06 01:20:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:04][root][INFO] - Training Epoch: 4/10, step 224/574 completed (loss: 0.36841753125190735, acc: 0.8977272510528564)
[2025-01-06 01:20:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:05][root][INFO] - Training Epoch: 4/10, step 225/574 completed (loss: 0.4484587013721466, acc: 0.8723404407501221)
[2025-01-06 01:20:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:05][root][INFO] - Training Epoch: 4/10, step 226/574 completed (loss: 0.11999712884426117, acc: 0.9622641801834106)
[2025-01-06 01:20:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:05][root][INFO] - Training Epoch: 4/10, step 227/574 completed (loss: 0.09331166744232178, acc: 0.9666666388511658)
[2025-01-06 01:20:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:06][root][INFO] - Training Epoch: 4/10, step 228/574 completed (loss: 0.33474934101104736, acc: 0.8604651093482971)
[2025-01-06 01:20:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:06][root][INFO] - Training Epoch: 4/10, step 229/574 completed (loss: 0.3631497621536255, acc: 0.8999999761581421)
[2025-01-06 01:20:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:06][root][INFO] - Training Epoch: 4/10, step 230/574 completed (loss: 1.0786508321762085, acc: 0.6736842393875122)
[2025-01-06 01:20:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:07][root][INFO] - Training Epoch: 4/10, step 231/574 completed (loss: 0.9280809760093689, acc: 0.7888888716697693)
[2025-01-06 01:20:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:07][root][INFO] - Training Epoch: 4/10, step 232/574 completed (loss: 0.9008857607841492, acc: 0.7333333492279053)
[2025-01-06 01:20:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:08][root][INFO] - Training Epoch: 4/10, step 233/574 completed (loss: 1.1860235929489136, acc: 0.6651375889778137)
[2025-01-06 01:20:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:08][root][INFO] - Training Epoch: 4/10, step 234/574 completed (loss: 1.0217955112457275, acc: 0.699999988079071)
[2025-01-06 01:20:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:08][root][INFO] - Training Epoch: 4/10, step 235/574 completed (loss: 0.014772162772715092, acc: 1.0)
[2025-01-06 01:20:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:09][root][INFO] - Training Epoch: 4/10, step 236/574 completed (loss: 0.044786859303712845, acc: 1.0)
[2025-01-06 01:20:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:09][root][INFO] - Training Epoch: 4/10, step 237/574 completed (loss: 0.31855860352516174, acc: 0.9545454382896423)
[2025-01-06 01:20:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:09][root][INFO] - Training Epoch: 4/10, step 238/574 completed (loss: 0.05018668621778488, acc: 1.0)
[2025-01-06 01:20:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:10][root][INFO] - Training Epoch: 4/10, step 239/574 completed (loss: 0.1604418307542801, acc: 0.9428571462631226)
[2025-01-06 01:20:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:10][root][INFO] - Training Epoch: 4/10, step 240/574 completed (loss: 0.488224595785141, acc: 0.8636363744735718)
[2025-01-06 01:20:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:10][root][INFO] - Training Epoch: 4/10, step 241/574 completed (loss: 0.2150600403547287, acc: 0.9318181872367859)
[2025-01-06 01:20:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:11][root][INFO] - Training Epoch: 4/10, step 242/574 completed (loss: 0.5343244075775146, acc: 0.8387096524238586)
[2025-01-06 01:20:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:11][root][INFO] - Training Epoch: 4/10, step 243/574 completed (loss: 0.4476442337036133, acc: 0.8636363744735718)
[2025-01-06 01:20:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:12][root][INFO] - Training Epoch: 4/10, step 244/574 completed (loss: 0.000706761609762907, acc: 1.0)
[2025-01-06 01:20:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:12][root][INFO] - Training Epoch: 4/10, step 245/574 completed (loss: 0.28415000438690186, acc: 0.9615384340286255)
[2025-01-06 01:20:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:12][root][INFO] - Training Epoch: 4/10, step 246/574 completed (loss: 0.01701405830681324, acc: 1.0)
[2025-01-06 01:20:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:13][root][INFO] - Training Epoch: 4/10, step 247/574 completed (loss: 0.06622256338596344, acc: 1.0)
[2025-01-06 01:20:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:13][root][INFO] - Training Epoch: 4/10, step 248/574 completed (loss: 0.025388378649950027, acc: 1.0)
[2025-01-06 01:20:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:14][root][INFO] - Training Epoch: 4/10, step 249/574 completed (loss: 0.12222645431756973, acc: 0.9459459185600281)
[2025-01-06 01:20:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:14][root][INFO] - Training Epoch: 4/10, step 250/574 completed (loss: 0.011724747717380524, acc: 1.0)
[2025-01-06 01:20:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:14][root][INFO] - Training Epoch: 4/10, step 251/574 completed (loss: 0.07344089448451996, acc: 0.9558823704719543)
[2025-01-06 01:20:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:15][root][INFO] - Training Epoch: 4/10, step 252/574 completed (loss: 0.0928940698504448, acc: 0.9756097793579102)
[2025-01-06 01:20:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:15][root][INFO] - Training Epoch: 4/10, step 253/574 completed (loss: 0.0029715292621403933, acc: 1.0)
[2025-01-06 01:20:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:15][root][INFO] - Training Epoch: 4/10, step 254/574 completed (loss: 0.0005471071926876903, acc: 1.0)
[2025-01-06 01:20:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:16][root][INFO] - Training Epoch: 4/10, step 255/574 completed (loss: 0.004114776849746704, acc: 1.0)
[2025-01-06 01:20:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:16][root][INFO] - Training Epoch: 4/10, step 256/574 completed (loss: 0.055701468139886856, acc: 0.9824561476707458)
[2025-01-06 01:20:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:16][root][INFO] - Training Epoch: 4/10, step 257/574 completed (loss: 0.049240998923778534, acc: 0.9714285731315613)
[2025-01-06 01:20:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:17][root][INFO] - Training Epoch: 4/10, step 258/574 completed (loss: 0.010690310038626194, acc: 1.0)
[2025-01-06 01:20:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:17][root][INFO] - Training Epoch: 4/10, step 259/574 completed (loss: 0.15397939085960388, acc: 0.9528301954269409)
[2025-01-06 01:20:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:18][root][INFO] - Training Epoch: 4/10, step 260/574 completed (loss: 0.2562921345233917, acc: 0.9166666865348816)
[2025-01-06 01:20:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:18][root][INFO] - Training Epoch: 4/10, step 261/574 completed (loss: 0.022193659096956253, acc: 1.0)
[2025-01-06 01:20:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:18][root][INFO] - Training Epoch: 4/10, step 262/574 completed (loss: 0.06701496243476868, acc: 1.0)
[2025-01-06 01:20:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:19][root][INFO] - Training Epoch: 4/10, step 263/574 completed (loss: 0.415061354637146, acc: 0.8933333158493042)
[2025-01-06 01:20:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:19][root][INFO] - Training Epoch: 4/10, step 264/574 completed (loss: 0.2678440809249878, acc: 0.9375)
[2025-01-06 01:20:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:20][root][INFO] - Training Epoch: 4/10, step 265/574 completed (loss: 0.9478247165679932, acc: 0.7519999742507935)
[2025-01-06 01:20:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:20][root][INFO] - Training Epoch: 4/10, step 266/574 completed (loss: 0.7138039469718933, acc: 0.7977527976036072)
[2025-01-06 01:20:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:21][root][INFO] - Training Epoch: 4/10, step 267/574 completed (loss: 0.27815160155296326, acc: 0.8918918967247009)
[2025-01-06 01:20:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:21][root][INFO] - Training Epoch: 4/10, step 268/574 completed (loss: 0.25760889053344727, acc: 0.9137930870056152)
[2025-01-06 01:20:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:21][root][INFO] - Training Epoch: 4/10, step 269/574 completed (loss: 0.001274994807317853, acc: 1.0)
[2025-01-06 01:20:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:22][root][INFO] - Training Epoch: 4/10, step 270/574 completed (loss: 0.01111900806427002, acc: 1.0)
[2025-01-06 01:20:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:22][root][INFO] - Training Epoch: 4/10, step 271/574 completed (loss: 0.016288835555315018, acc: 1.0)
[2025-01-06 01:20:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:22][root][INFO] - Training Epoch: 4/10, step 272/574 completed (loss: 0.027740539982914925, acc: 1.0)
[2025-01-06 01:20:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:23][root][INFO] - Training Epoch: 4/10, step 273/574 completed (loss: 0.1466224491596222, acc: 0.9833333492279053)
[2025-01-06 01:20:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:23][root][INFO] - Training Epoch: 4/10, step 274/574 completed (loss: 0.12989088892936707, acc: 0.96875)
[2025-01-06 01:20:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:24][root][INFO] - Training Epoch: 4/10, step 275/574 completed (loss: 0.07105603069067001, acc: 0.9666666388511658)
[2025-01-06 01:20:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:24][root][INFO] - Training Epoch: 4/10, step 276/574 completed (loss: 0.13538137078285217, acc: 0.9655172228813171)
[2025-01-06 01:20:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:24][root][INFO] - Training Epoch: 4/10, step 277/574 completed (loss: 0.15672433376312256, acc: 0.9599999785423279)
[2025-01-06 01:20:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:25][root][INFO] - Training Epoch: 4/10, step 278/574 completed (loss: 0.14721083641052246, acc: 0.936170220375061)
[2025-01-06 01:20:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:25][root][INFO] - Training Epoch: 4/10, step 279/574 completed (loss: 0.11829423904418945, acc: 0.9583333134651184)
[2025-01-06 01:20:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:56][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.0572, device='cuda:0') eval_epoch_loss=tensor(0.7213, device='cuda:0') eval_epoch_acc=tensor(0.8390, device='cuda:0')
[2025-01-06 01:20:56][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:20:56][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:20:56][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_4_step_280_loss_0.721332848072052/model.pt
[2025-01-06 01:20:56][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:20:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:56][root][INFO] - Training Epoch: 4/10, step 280/574 completed (loss: 0.00740846386179328, acc: 1.0)
[2025-01-06 01:20:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:57][root][INFO] - Training Epoch: 4/10, step 281/574 completed (loss: 0.29194176197052, acc: 0.891566276550293)
[2025-01-06 01:20:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:57][root][INFO] - Training Epoch: 4/10, step 282/574 completed (loss: 0.3892766535282135, acc: 0.8703703880310059)
[2025-01-06 01:20:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:58][root][INFO] - Training Epoch: 4/10, step 283/574 completed (loss: 0.0700598731637001, acc: 0.9736841917037964)
[2025-01-06 01:20:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:58][root][INFO] - Training Epoch: 4/10, step 284/574 completed (loss: 0.007130909711122513, acc: 1.0)
[2025-01-06 01:20:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:58][root][INFO] - Training Epoch: 4/10, step 285/574 completed (loss: 0.0738244578242302, acc: 0.9750000238418579)
[2025-01-06 01:20:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:59][root][INFO] - Training Epoch: 4/10, step 286/574 completed (loss: 0.2058815211057663, acc: 0.9296875)
[2025-01-06 01:20:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:59][root][INFO] - Training Epoch: 4/10, step 287/574 completed (loss: 0.3682396113872528, acc: 0.9120000004768372)
[2025-01-06 01:20:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:20:59][root][INFO] - Training Epoch: 4/10, step 288/574 completed (loss: 0.16879306733608246, acc: 0.9230769276618958)
[2025-01-06 01:21:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:00][root][INFO] - Training Epoch: 4/10, step 289/574 completed (loss: 0.15577231347560883, acc: 0.9503105878829956)
[2025-01-06 01:21:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:00][root][INFO] - Training Epoch: 4/10, step 290/574 completed (loss: 0.3222205638885498, acc: 0.9278350472450256)
[2025-01-06 01:21:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:00][root][INFO] - Training Epoch: 4/10, step 291/574 completed (loss: 0.013152099214494228, acc: 1.0)
[2025-01-06 01:21:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:01][root][INFO] - Training Epoch: 4/10, step 292/574 completed (loss: 0.08908611536026001, acc: 0.976190447807312)
[2025-01-06 01:21:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:01][root][INFO] - Training Epoch: 4/10, step 293/574 completed (loss: 0.07614148408174515, acc: 0.982758641242981)
[2025-01-06 01:21:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:02][root][INFO] - Training Epoch: 4/10, step 294/574 completed (loss: 0.24034729599952698, acc: 0.8727272748947144)
[2025-01-06 01:21:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:02][root][INFO] - Training Epoch: 4/10, step 295/574 completed (loss: 0.30892425775527954, acc: 0.9123711585998535)
[2025-01-06 01:21:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:02][root][INFO] - Training Epoch: 4/10, step 296/574 completed (loss: 0.1340673416852951, acc: 0.9482758641242981)
[2025-01-06 01:21:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:03][root][INFO] - Training Epoch: 4/10, step 297/574 completed (loss: 0.03114534169435501, acc: 1.0)
[2025-01-06 01:21:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:03][root][INFO] - Training Epoch: 4/10, step 298/574 completed (loss: 0.06865998357534409, acc: 1.0)
[2025-01-06 01:21:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:04][root][INFO] - Training Epoch: 4/10, step 299/574 completed (loss: 0.020669149234890938, acc: 1.0)
[2025-01-06 01:21:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:04][root][INFO] - Training Epoch: 4/10, step 300/574 completed (loss: 0.005711985751986504, acc: 1.0)
[2025-01-06 01:21:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:04][root][INFO] - Training Epoch: 4/10, step 301/574 completed (loss: 0.018021542578935623, acc: 1.0)
[2025-01-06 01:21:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:05][root][INFO] - Training Epoch: 4/10, step 302/574 completed (loss: 0.013624560087919235, acc: 1.0)
[2025-01-06 01:21:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:05][root][INFO] - Training Epoch: 4/10, step 303/574 completed (loss: 0.010862561874091625, acc: 1.0)
[2025-01-06 01:21:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:05][root][INFO] - Training Epoch: 4/10, step 304/574 completed (loss: 0.07621946930885315, acc: 0.96875)
[2025-01-06 01:21:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:06][root][INFO] - Training Epoch: 4/10, step 305/574 completed (loss: 0.21732056140899658, acc: 0.9508196711540222)
[2025-01-06 01:21:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:06][root][INFO] - Training Epoch: 4/10, step 306/574 completed (loss: 0.4243510663509369, acc: 0.9333333373069763)
[2025-01-06 01:21:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:06][root][INFO] - Training Epoch: 4/10, step 307/574 completed (loss: 0.004817747510969639, acc: 1.0)
[2025-01-06 01:21:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:07][root][INFO] - Training Epoch: 4/10, step 308/574 completed (loss: 0.10888669639825821, acc: 0.9710144996643066)
[2025-01-06 01:21:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:07][root][INFO] - Training Epoch: 4/10, step 309/574 completed (loss: 0.05746915563941002, acc: 0.9722222089767456)
[2025-01-06 01:21:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:08][root][INFO] - Training Epoch: 4/10, step 310/574 completed (loss: 0.11900890618562698, acc: 0.9518072009086609)
[2025-01-06 01:21:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:08][root][INFO] - Training Epoch: 4/10, step 311/574 completed (loss: 0.06638161092996597, acc: 1.0)
[2025-01-06 01:21:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:08][root][INFO] - Training Epoch: 4/10, step 312/574 completed (loss: 0.04855719953775406, acc: 0.9897959232330322)
[2025-01-06 01:21:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:09][root][INFO] - Training Epoch: 4/10, step 313/574 completed (loss: 0.007906812243163586, acc: 1.0)
[2025-01-06 01:21:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:09][root][INFO] - Training Epoch: 4/10, step 314/574 completed (loss: 0.005065165460109711, acc: 1.0)
[2025-01-06 01:21:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:09][root][INFO] - Training Epoch: 4/10, step 315/574 completed (loss: 0.010358543135225773, acc: 1.0)
[2025-01-06 01:21:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:10][root][INFO] - Training Epoch: 4/10, step 316/574 completed (loss: 0.299386590719223, acc: 0.9354838728904724)
[2025-01-06 01:21:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:10][root][INFO] - Training Epoch: 4/10, step 317/574 completed (loss: 0.020042071118950844, acc: 1.0)
[2025-01-06 01:21:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:10][root][INFO] - Training Epoch: 4/10, step 318/574 completed (loss: 0.0516270250082016, acc: 0.9807692170143127)
[2025-01-06 01:21:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:11][root][INFO] - Training Epoch: 4/10, step 319/574 completed (loss: 0.0417313352227211, acc: 0.9777777791023254)
[2025-01-06 01:21:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:11][root][INFO] - Training Epoch: 4/10, step 320/574 completed (loss: 0.06623106449842453, acc: 0.9677419066429138)
[2025-01-06 01:21:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:11][root][INFO] - Training Epoch: 4/10, step 321/574 completed (loss: 0.03208789601922035, acc: 1.0)
[2025-01-06 01:21:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:12][root][INFO] - Training Epoch: 4/10, step 322/574 completed (loss: 0.15202274918556213, acc: 0.9629629850387573)
[2025-01-06 01:21:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:12][root][INFO] - Training Epoch: 4/10, step 323/574 completed (loss: 0.36114487051963806, acc: 0.9142857193946838)
[2025-01-06 01:21:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:12][root][INFO] - Training Epoch: 4/10, step 324/574 completed (loss: 0.35305142402648926, acc: 0.8974359035491943)
[2025-01-06 01:21:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:13][root][INFO] - Training Epoch: 4/10, step 325/574 completed (loss: 0.4209345877170563, acc: 0.8536585569381714)
[2025-01-06 01:21:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:13][root][INFO] - Training Epoch: 4/10, step 326/574 completed (loss: 0.19474835693836212, acc: 0.9210526347160339)
[2025-01-06 01:21:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:13][root][INFO] - Training Epoch: 4/10, step 327/574 completed (loss: 0.08072039484977722, acc: 1.0)
[2025-01-06 01:21:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:14][root][INFO] - Training Epoch: 4/10, step 328/574 completed (loss: 0.02496255561709404, acc: 1.0)
[2025-01-06 01:21:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:14][root][INFO] - Training Epoch: 4/10, step 329/574 completed (loss: 0.0037943569477647543, acc: 1.0)
[2025-01-06 01:21:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:14][root][INFO] - Training Epoch: 4/10, step 330/574 completed (loss: 0.0033474972005933523, acc: 1.0)
[2025-01-06 01:21:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:15][root][INFO] - Training Epoch: 4/10, step 331/574 completed (loss: 0.16280174255371094, acc: 0.9516128897666931)
[2025-01-06 01:21:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:15][root][INFO] - Training Epoch: 4/10, step 332/574 completed (loss: 0.0107421251013875, acc: 1.0)
[2025-01-06 01:21:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:16][root][INFO] - Training Epoch: 4/10, step 333/574 completed (loss: 0.013906477950513363, acc: 1.0)
[2025-01-06 01:21:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:16][root][INFO] - Training Epoch: 4/10, step 334/574 completed (loss: 0.277995765209198, acc: 0.9666666388511658)
[2025-01-06 01:21:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:16][root][INFO] - Training Epoch: 4/10, step 335/574 completed (loss: 0.021290406584739685, acc: 1.0)
[2025-01-06 01:21:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:17][root][INFO] - Training Epoch: 4/10, step 336/574 completed (loss: 0.1993684619665146, acc: 0.9599999785423279)
[2025-01-06 01:21:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:17][root][INFO] - Training Epoch: 4/10, step 337/574 completed (loss: 0.3363301157951355, acc: 0.9080459475517273)
[2025-01-06 01:21:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:17][root][INFO] - Training Epoch: 4/10, step 338/574 completed (loss: 0.6794771552085876, acc: 0.8085106611251831)
[2025-01-06 01:21:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:18][root][INFO] - Training Epoch: 4/10, step 339/574 completed (loss: 0.5225037932395935, acc: 0.8554216623306274)
[2025-01-06 01:21:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:18][root][INFO] - Training Epoch: 4/10, step 340/574 completed (loss: 0.01412777416408062, acc: 1.0)
[2025-01-06 01:21:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:18][root][INFO] - Training Epoch: 4/10, step 341/574 completed (loss: 0.005343560129404068, acc: 1.0)
[2025-01-06 01:21:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:19][root][INFO] - Training Epoch: 4/10, step 342/574 completed (loss: 0.1399042308330536, acc: 0.9518072009086609)
[2025-01-06 01:21:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:19][root][INFO] - Training Epoch: 4/10, step 343/574 completed (loss: 0.21000999212265015, acc: 0.9811320900917053)
[2025-01-06 01:21:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:19][root][INFO] - Training Epoch: 4/10, step 344/574 completed (loss: 0.07330196350812912, acc: 0.9746835231781006)
[2025-01-06 01:21:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:20][root][INFO] - Training Epoch: 4/10, step 345/574 completed (loss: 0.04051848128437996, acc: 0.9803921580314636)
[2025-01-06 01:21:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:20][root][INFO] - Training Epoch: 4/10, step 346/574 completed (loss: 0.3600466251373291, acc: 0.9253731369972229)
[2025-01-06 01:21:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:20][root][INFO] - Training Epoch: 4/10, step 347/574 completed (loss: 0.0029655699618160725, acc: 1.0)
[2025-01-06 01:21:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:21][root][INFO] - Training Epoch: 4/10, step 348/574 completed (loss: 0.013696527108550072, acc: 1.0)
[2025-01-06 01:21:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:21][root][INFO] - Training Epoch: 4/10, step 349/574 completed (loss: 0.29312586784362793, acc: 0.9444444179534912)
[2025-01-06 01:21:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:21][root][INFO] - Training Epoch: 4/10, step 350/574 completed (loss: 0.17770496010780334, acc: 0.9534883499145508)
[2025-01-06 01:21:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:22][root][INFO] - Training Epoch: 4/10, step 351/574 completed (loss: 0.03986469656229019, acc: 1.0)
[2025-01-06 01:21:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:22][root][INFO] - Training Epoch: 4/10, step 352/574 completed (loss: 0.3593731224536896, acc: 0.8888888955116272)
[2025-01-06 01:21:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:22][root][INFO] - Training Epoch: 4/10, step 353/574 completed (loss: 0.00811754073947668, acc: 1.0)
[2025-01-06 01:21:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:23][root][INFO] - Training Epoch: 4/10, step 354/574 completed (loss: 0.012227805331349373, acc: 1.0)
[2025-01-06 01:21:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:23][root][INFO] - Training Epoch: 4/10, step 355/574 completed (loss: 0.3306786119937897, acc: 0.8901098966598511)
[2025-01-06 01:21:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:24][root][INFO] - Training Epoch: 4/10, step 356/574 completed (loss: 0.3244589865207672, acc: 0.886956512928009)
[2025-01-06 01:21:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:24][root][INFO] - Training Epoch: 4/10, step 357/574 completed (loss: 0.3191465139389038, acc: 0.9239130616188049)
[2025-01-06 01:21:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:25][root][INFO] - Training Epoch: 4/10, step 358/574 completed (loss: 0.10449858754873276, acc: 0.9795918464660645)
[2025-01-06 01:21:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:25][root][INFO] - Training Epoch: 4/10, step 359/574 completed (loss: 0.0005740172346122563, acc: 1.0)
[2025-01-06 01:21:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:25][root][INFO] - Training Epoch: 4/10, step 360/574 completed (loss: 0.016085252165794373, acc: 1.0)
[2025-01-06 01:21:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:26][root][INFO] - Training Epoch: 4/10, step 361/574 completed (loss: 0.18652649223804474, acc: 0.9512194991111755)
[2025-01-06 01:21:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:26][root][INFO] - Training Epoch: 4/10, step 362/574 completed (loss: 0.03771911934018135, acc: 0.9777777791023254)
[2025-01-06 01:21:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:26][root][INFO] - Training Epoch: 4/10, step 363/574 completed (loss: 0.05526840686798096, acc: 0.9868420958518982)
[2025-01-06 01:21:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:27][root][INFO] - Training Epoch: 4/10, step 364/574 completed (loss: 0.046126268804073334, acc: 0.9756097793579102)
[2025-01-06 01:21:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:27][root][INFO] - Training Epoch: 4/10, step 365/574 completed (loss: 0.009784293361008167, acc: 1.0)
[2025-01-06 01:21:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:27][root][INFO] - Training Epoch: 4/10, step 366/574 completed (loss: 0.0003980745968874544, acc: 1.0)
[2025-01-06 01:21:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:27][root][INFO] - Training Epoch: 4/10, step 367/574 completed (loss: 0.012680765241384506, acc: 1.0)
[2025-01-06 01:21:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:28][root][INFO] - Training Epoch: 4/10, step 368/574 completed (loss: 0.06115783005952835, acc: 0.9642857313156128)
[2025-01-06 01:21:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:28][root][INFO] - Training Epoch: 4/10, step 369/574 completed (loss: 0.012596307322382927, acc: 1.0)
[2025-01-06 01:21:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:29][root][INFO] - Training Epoch: 4/10, step 370/574 completed (loss: 0.28408458828926086, acc: 0.9151515364646912)
[2025-01-06 01:21:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:30][root][INFO] - Training Epoch: 4/10, step 371/574 completed (loss: 0.1402890682220459, acc: 0.9433962106704712)
[2025-01-06 01:21:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:30][root][INFO] - Training Epoch: 4/10, step 372/574 completed (loss: 0.061340633779764175, acc: 0.9777777791023254)
[2025-01-06 01:21:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:30][root][INFO] - Training Epoch: 4/10, step 373/574 completed (loss: 0.06429436802864075, acc: 0.9821428656578064)
[2025-01-06 01:21:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:31][root][INFO] - Training Epoch: 4/10, step 374/574 completed (loss: 0.010478234849870205, acc: 1.0)
[2025-01-06 01:21:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:31][root][INFO] - Training Epoch: 4/10, step 375/574 completed (loss: 0.00047074840404093266, acc: 1.0)
[2025-01-06 01:21:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:32][root][INFO] - Training Epoch: 4/10, step 376/574 completed (loss: 0.027451099827885628, acc: 1.0)
[2025-01-06 01:21:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:32][root][INFO] - Training Epoch: 4/10, step 377/574 completed (loss: 0.010986440815031528, acc: 1.0)
[2025-01-06 01:21:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:32][root][INFO] - Training Epoch: 4/10, step 378/574 completed (loss: 0.11530527472496033, acc: 0.9789473414421082)
[2025-01-06 01:21:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:33][root][INFO] - Training Epoch: 4/10, step 379/574 completed (loss: 0.1476617008447647, acc: 0.9580838084220886)
[2025-01-06 01:21:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:33][root][INFO] - Training Epoch: 4/10, step 380/574 completed (loss: 0.20943285524845123, acc: 0.9473684430122375)
[2025-01-06 01:21:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:34][root][INFO] - Training Epoch: 4/10, step 381/574 completed (loss: 0.3453192114830017, acc: 0.8823529481887817)
[2025-01-06 01:21:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:35][root][INFO] - Training Epoch: 4/10, step 382/574 completed (loss: 0.054483845829963684, acc: 0.9909909963607788)
[2025-01-06 01:21:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:35][root][INFO] - Training Epoch: 4/10, step 383/574 completed (loss: 0.009421895258128643, acc: 1.0)
[2025-01-06 01:21:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:36][root][INFO] - Training Epoch: 4/10, step 384/574 completed (loss: 0.010200655087828636, acc: 1.0)
[2025-01-06 01:21:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:36][root][INFO] - Training Epoch: 4/10, step 385/574 completed (loss: 0.0034471654798835516, acc: 1.0)
[2025-01-06 01:21:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:36][root][INFO] - Training Epoch: 4/10, step 386/574 completed (loss: 0.000851326622068882, acc: 1.0)
[2025-01-06 01:21:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:37][root][INFO] - Training Epoch: 4/10, step 387/574 completed (loss: 0.001087688491679728, acc: 1.0)
[2025-01-06 01:21:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:37][root][INFO] - Training Epoch: 4/10, step 388/574 completed (loss: 0.0017992559587582946, acc: 1.0)
[2025-01-06 01:21:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:37][root][INFO] - Training Epoch: 4/10, step 389/574 completed (loss: 0.0007218344835564494, acc: 1.0)
[2025-01-06 01:21:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:38][root][INFO] - Training Epoch: 4/10, step 390/574 completed (loss: 0.41191309690475464, acc: 0.9523809552192688)
[2025-01-06 01:21:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:38][root][INFO] - Training Epoch: 4/10, step 391/574 completed (loss: 0.2660942077636719, acc: 0.9259259104728699)
[2025-01-06 01:21:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:38][root][INFO] - Training Epoch: 4/10, step 392/574 completed (loss: 0.2842894494533539, acc: 0.9029126167297363)
[2025-01-06 01:21:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:39][root][INFO] - Training Epoch: 4/10, step 393/574 completed (loss: 0.610426127910614, acc: 0.875)
[2025-01-06 01:21:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:39][root][INFO] - Training Epoch: 4/10, step 394/574 completed (loss: 0.3259909152984619, acc: 0.9066666960716248)
[2025-01-06 01:21:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:40][root][INFO] - Training Epoch: 4/10, step 395/574 completed (loss: 0.4688858389854431, acc: 0.8680555820465088)
[2025-01-06 01:21:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:40][root][INFO] - Training Epoch: 4/10, step 396/574 completed (loss: 0.20748834311962128, acc: 0.9767441749572754)
[2025-01-06 01:21:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:40][root][INFO] - Training Epoch: 4/10, step 397/574 completed (loss: 0.17515723407268524, acc: 0.9583333134651184)
[2025-01-06 01:21:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:41][root][INFO] - Training Epoch: 4/10, step 398/574 completed (loss: 0.14571231603622437, acc: 0.9534883499145508)
[2025-01-06 01:21:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:41][root][INFO] - Training Epoch: 4/10, step 399/574 completed (loss: 0.003676481544971466, acc: 1.0)
[2025-01-06 01:21:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:41][root][INFO] - Training Epoch: 4/10, step 400/574 completed (loss: 0.11514461785554886, acc: 0.9852941036224365)
[2025-01-06 01:21:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:42][root][INFO] - Training Epoch: 4/10, step 401/574 completed (loss: 0.1949332356452942, acc: 0.9066666960716248)
[2025-01-06 01:21:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:42][root][INFO] - Training Epoch: 4/10, step 402/574 completed (loss: 0.14203685522079468, acc: 0.9696969985961914)
[2025-01-06 01:21:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:43][root][INFO] - Training Epoch: 4/10, step 403/574 completed (loss: 0.05036935955286026, acc: 0.9696969985961914)
[2025-01-06 01:21:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:43][root][INFO] - Training Epoch: 4/10, step 404/574 completed (loss: 0.091546930372715, acc: 0.9354838728904724)
[2025-01-06 01:21:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:43][root][INFO] - Training Epoch: 4/10, step 405/574 completed (loss: 0.0026808450929820538, acc: 1.0)
[2025-01-06 01:21:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:44][root][INFO] - Training Epoch: 4/10, step 406/574 completed (loss: 0.04564937576651573, acc: 1.0)
[2025-01-06 01:21:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:44][root][INFO] - Training Epoch: 4/10, step 407/574 completed (loss: 0.005246406886726618, acc: 1.0)
[2025-01-06 01:21:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:44][root][INFO] - Training Epoch: 4/10, step 408/574 completed (loss: 0.01153505314141512, acc: 1.0)
[2025-01-06 01:21:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:45][root][INFO] - Training Epoch: 4/10, step 409/574 completed (loss: 0.016666311770677567, acc: 1.0)
[2025-01-06 01:21:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:45][root][INFO] - Training Epoch: 4/10, step 410/574 completed (loss: 0.009132477454841137, acc: 1.0)
[2025-01-06 01:21:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:45][root][INFO] - Training Epoch: 4/10, step 411/574 completed (loss: 0.015530929900705814, acc: 1.0)
[2025-01-06 01:21:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:46][root][INFO] - Training Epoch: 4/10, step 412/574 completed (loss: 0.0019625327549874783, acc: 1.0)
[2025-01-06 01:21:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:46][root][INFO] - Training Epoch: 4/10, step 413/574 completed (loss: 0.019039439037442207, acc: 1.0)
[2025-01-06 01:21:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:46][root][INFO] - Training Epoch: 4/10, step 414/574 completed (loss: 0.005133335944265127, acc: 1.0)
[2025-01-06 01:21:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:47][root][INFO] - Training Epoch: 4/10, step 415/574 completed (loss: 0.10333807021379471, acc: 0.9607843160629272)
[2025-01-06 01:21:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:47][root][INFO] - Training Epoch: 4/10, step 416/574 completed (loss: 0.012249200604856014, acc: 1.0)
[2025-01-06 01:21:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:47][root][INFO] - Training Epoch: 4/10, step 417/574 completed (loss: 0.09022817760705948, acc: 0.9444444179534912)
[2025-01-06 01:21:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:48][root][INFO] - Training Epoch: 4/10, step 418/574 completed (loss: 0.10309545695781708, acc: 0.9750000238418579)
[2025-01-06 01:21:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:48][root][INFO] - Training Epoch: 4/10, step 419/574 completed (loss: 0.011480451561510563, acc: 1.0)
[2025-01-06 01:21:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:48][root][INFO] - Training Epoch: 4/10, step 420/574 completed (loss: 0.008644884452223778, acc: 1.0)
[2025-01-06 01:21:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:49][root][INFO] - Training Epoch: 4/10, step 421/574 completed (loss: 0.016911858692765236, acc: 1.0)
[2025-01-06 01:21:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:49][root][INFO] - Training Epoch: 4/10, step 422/574 completed (loss: 0.005110962316393852, acc: 1.0)
[2025-01-06 01:21:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:21:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:18][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.0576, device='cuda:0') eval_epoch_loss=tensor(0.7215, device='cuda:0') eval_epoch_acc=tensor(0.8381, device='cuda:0')
[2025-01-06 01:22:18][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:22:18][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:22:19][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_4_step_423_loss_0.7215169072151184/model.pt
[2025-01-06 01:22:19][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:22:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:19][root][INFO] - Training Epoch: 4/10, step 423/574 completed (loss: 0.16329821944236755, acc: 0.9722222089767456)
[2025-01-06 01:22:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:20][root][INFO] - Training Epoch: 4/10, step 424/574 completed (loss: 0.1979086846113205, acc: 0.9259259104728699)
[2025-01-06 01:22:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:20][root][INFO] - Training Epoch: 4/10, step 425/574 completed (loss: 0.034730784595012665, acc: 0.9696969985961914)
[2025-01-06 01:22:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:20][root][INFO] - Training Epoch: 4/10, step 426/574 completed (loss: 0.004117846023291349, acc: 1.0)
[2025-01-06 01:22:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:21][root][INFO] - Training Epoch: 4/10, step 427/574 completed (loss: 0.0820070430636406, acc: 0.9729729890823364)
[2025-01-06 01:22:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:21][root][INFO] - Training Epoch: 4/10, step 428/574 completed (loss: 0.04452384263277054, acc: 1.0)
[2025-01-06 01:22:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:21][root][INFO] - Training Epoch: 4/10, step 429/574 completed (loss: 0.0012545526260510087, acc: 1.0)
[2025-01-06 01:22:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:22][root][INFO] - Training Epoch: 4/10, step 430/574 completed (loss: 0.0007624722202308476, acc: 1.0)
[2025-01-06 01:22:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:22][root][INFO] - Training Epoch: 4/10, step 431/574 completed (loss: 0.008278435096144676, acc: 1.0)
[2025-01-06 01:22:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:22][root][INFO] - Training Epoch: 4/10, step 432/574 completed (loss: 0.0016370725352317095, acc: 1.0)
[2025-01-06 01:22:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:23][root][INFO] - Training Epoch: 4/10, step 433/574 completed (loss: 0.12210524082183838, acc: 0.9444444179534912)
[2025-01-06 01:22:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:23][root][INFO] - Training Epoch: 4/10, step 434/574 completed (loss: 0.03471769019961357, acc: 0.9599999785423279)
[2025-01-06 01:22:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:24][root][INFO] - Training Epoch: 4/10, step 435/574 completed (loss: 0.001189547241665423, acc: 1.0)
[2025-01-06 01:22:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:24][root][INFO] - Training Epoch: 4/10, step 436/574 completed (loss: 0.03693551570177078, acc: 1.0)
[2025-01-06 01:22:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:24][root][INFO] - Training Epoch: 4/10, step 437/574 completed (loss: 0.009694485925137997, acc: 1.0)
[2025-01-06 01:22:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:25][root][INFO] - Training Epoch: 4/10, step 438/574 completed (loss: 0.003900888841599226, acc: 1.0)
[2025-01-06 01:22:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:25][root][INFO] - Training Epoch: 4/10, step 439/574 completed (loss: 0.004400054924190044, acc: 1.0)
[2025-01-06 01:22:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:25][root][INFO] - Training Epoch: 4/10, step 440/574 completed (loss: 0.10509684681892395, acc: 0.9545454382896423)
[2025-01-06 01:22:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:26][root][INFO] - Training Epoch: 4/10, step 441/574 completed (loss: 0.31373170018196106, acc: 0.9039999842643738)
[2025-01-06 01:22:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:27][root][INFO] - Training Epoch: 4/10, step 442/574 completed (loss: 0.4243440330028534, acc: 0.8870967626571655)
[2025-01-06 01:22:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:27][root][INFO] - Training Epoch: 4/10, step 443/574 completed (loss: 0.30083709955215454, acc: 0.9104477763175964)
[2025-01-06 01:22:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:28][root][INFO] - Training Epoch: 4/10, step 444/574 completed (loss: 0.0617084875702858, acc: 0.9811320900917053)
[2025-01-06 01:22:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:28][root][INFO] - Training Epoch: 4/10, step 445/574 completed (loss: 0.033575043082237244, acc: 1.0)
[2025-01-06 01:22:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:28][root][INFO] - Training Epoch: 4/10, step 446/574 completed (loss: 0.01429785043001175, acc: 1.0)
[2025-01-06 01:22:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:29][root][INFO] - Training Epoch: 4/10, step 447/574 completed (loss: 0.16796912252902985, acc: 0.9615384340286255)
[2025-01-06 01:22:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:29][root][INFO] - Training Epoch: 4/10, step 448/574 completed (loss: 0.012341344729065895, acc: 1.0)
[2025-01-06 01:22:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:29][root][INFO] - Training Epoch: 4/10, step 449/574 completed (loss: 0.02284196950495243, acc: 0.9850746393203735)
[2025-01-06 01:22:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:30][root][INFO] - Training Epoch: 4/10, step 450/574 completed (loss: 0.008604801259934902, acc: 1.0)
[2025-01-06 01:22:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:30][root][INFO] - Training Epoch: 4/10, step 451/574 completed (loss: 0.017460795119404793, acc: 1.0)
[2025-01-06 01:22:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:30][root][INFO] - Training Epoch: 4/10, step 452/574 completed (loss: 0.05789008364081383, acc: 0.9871794581413269)
[2025-01-06 01:22:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:31][root][INFO] - Training Epoch: 4/10, step 453/574 completed (loss: 0.06603879481554031, acc: 0.9736841917037964)
[2025-01-06 01:22:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:31][root][INFO] - Training Epoch: 4/10, step 454/574 completed (loss: 0.006815850734710693, acc: 1.0)
[2025-01-06 01:22:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:32][root][INFO] - Training Epoch: 4/10, step 455/574 completed (loss: 0.018000002950429916, acc: 1.0)
[2025-01-06 01:22:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:32][root][INFO] - Training Epoch: 4/10, step 456/574 completed (loss: 0.3220456540584564, acc: 0.9278350472450256)
[2025-01-06 01:22:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:32][root][INFO] - Training Epoch: 4/10, step 457/574 completed (loss: 0.007041491102427244, acc: 1.0)
[2025-01-06 01:22:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:33][root][INFO] - Training Epoch: 4/10, step 458/574 completed (loss: 0.13796277344226837, acc: 0.9534883499145508)
[2025-01-06 01:22:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:33][root][INFO] - Training Epoch: 4/10, step 459/574 completed (loss: 0.005752589087933302, acc: 1.0)
[2025-01-06 01:22:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:33][root][INFO] - Training Epoch: 4/10, step 460/574 completed (loss: 0.03381248563528061, acc: 1.0)
[2025-01-06 01:22:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:34][root][INFO] - Training Epoch: 4/10, step 461/574 completed (loss: 0.01969578117132187, acc: 1.0)
[2025-01-06 01:22:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:34][root][INFO] - Training Epoch: 4/10, step 462/574 completed (loss: 0.00931653380393982, acc: 1.0)
[2025-01-06 01:22:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:34][root][INFO] - Training Epoch: 4/10, step 463/574 completed (loss: 0.05200057476758957, acc: 1.0)
[2025-01-06 01:22:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:35][root][INFO] - Training Epoch: 4/10, step 464/574 completed (loss: 0.02119695581495762, acc: 1.0)
[2025-01-06 01:22:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:35][root][INFO] - Training Epoch: 4/10, step 465/574 completed (loss: 0.08805594593286514, acc: 0.9642857313156128)
[2025-01-06 01:22:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:35][root][INFO] - Training Epoch: 4/10, step 466/574 completed (loss: 0.1953592449426651, acc: 0.9397590160369873)
[2025-01-06 01:22:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:36][root][INFO] - Training Epoch: 4/10, step 467/574 completed (loss: 0.031378500163555145, acc: 0.9909909963607788)
[2025-01-06 01:22:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:36][root][INFO] - Training Epoch: 4/10, step 468/574 completed (loss: 0.29467377066612244, acc: 0.9320388436317444)
[2025-01-06 01:22:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:37][root][INFO] - Training Epoch: 4/10, step 469/574 completed (loss: 0.13373172283172607, acc: 0.9674796462059021)
[2025-01-06 01:22:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:37][root][INFO] - Training Epoch: 4/10, step 470/574 completed (loss: 0.006791144143790007, acc: 1.0)
[2025-01-06 01:22:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:37][root][INFO] - Training Epoch: 4/10, step 471/574 completed (loss: 0.008584840223193169, acc: 1.0)
[2025-01-06 01:22:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:38][root][INFO] - Training Epoch: 4/10, step 472/574 completed (loss: 0.3062222898006439, acc: 0.9313725233078003)
[2025-01-06 01:22:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:38][root][INFO] - Training Epoch: 4/10, step 473/574 completed (loss: 0.5566161274909973, acc: 0.8515284061431885)
[2025-01-06 01:22:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:38][root][INFO] - Training Epoch: 4/10, step 474/574 completed (loss: 0.14783868193626404, acc: 0.9270833134651184)
[2025-01-06 01:22:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:39][root][INFO] - Training Epoch: 4/10, step 475/574 completed (loss: 0.18544448912143707, acc: 0.9447852969169617)
[2025-01-06 01:22:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:39][root][INFO] - Training Epoch: 4/10, step 476/574 completed (loss: 0.14111366868019104, acc: 0.9424460530281067)
[2025-01-06 01:22:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:39][root][INFO] - Training Epoch: 4/10, step 477/574 completed (loss: 0.45057782530784607, acc: 0.8743718862533569)
[2025-01-06 01:22:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:40][root][INFO] - Training Epoch: 4/10, step 478/574 completed (loss: 0.032938163727521896, acc: 1.0)
[2025-01-06 01:22:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:40][root][INFO] - Training Epoch: 4/10, step 479/574 completed (loss: 0.037901587784290314, acc: 1.0)
[2025-01-06 01:22:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:41][root][INFO] - Training Epoch: 4/10, step 480/574 completed (loss: 0.014518028125166893, acc: 1.0)
[2025-01-06 01:22:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:41][root][INFO] - Training Epoch: 4/10, step 481/574 completed (loss: 0.14020796120166779, acc: 0.949999988079071)
[2025-01-06 01:22:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:41][root][INFO] - Training Epoch: 4/10, step 482/574 completed (loss: 0.2719058692455292, acc: 0.8500000238418579)
[2025-01-06 01:22:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:42][root][INFO] - Training Epoch: 4/10, step 483/574 completed (loss: 0.4504775404930115, acc: 0.9137930870056152)
[2025-01-06 01:22:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:42][root][INFO] - Training Epoch: 4/10, step 484/574 completed (loss: 0.005185466259717941, acc: 1.0)
[2025-01-06 01:22:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:42][root][INFO] - Training Epoch: 4/10, step 485/574 completed (loss: 0.02675282023847103, acc: 1.0)
[2025-01-06 01:22:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:43][root][INFO] - Training Epoch: 4/10, step 486/574 completed (loss: 0.12263811379671097, acc: 0.9259259104728699)
[2025-01-06 01:22:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:43][root][INFO] - Training Epoch: 4/10, step 487/574 completed (loss: 0.023165490478277206, acc: 1.0)
[2025-01-06 01:22:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:43][root][INFO] - Training Epoch: 4/10, step 488/574 completed (loss: 0.03776389732956886, acc: 1.0)
[2025-01-06 01:22:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:44][root][INFO] - Training Epoch: 4/10, step 489/574 completed (loss: 0.269597589969635, acc: 0.9076923131942749)
[2025-01-06 01:22:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:44][root][INFO] - Training Epoch: 4/10, step 490/574 completed (loss: 0.017697079107165337, acc: 1.0)
[2025-01-06 01:22:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:44][root][INFO] - Training Epoch: 4/10, step 491/574 completed (loss: 0.04707453399896622, acc: 1.0)
[2025-01-06 01:22:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:45][root][INFO] - Training Epoch: 4/10, step 492/574 completed (loss: 0.10991669446229935, acc: 0.9607843160629272)
[2025-01-06 01:22:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:45][root][INFO] - Training Epoch: 4/10, step 493/574 completed (loss: 0.011859224177896976, acc: 1.0)
[2025-01-06 01:22:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:46][root][INFO] - Training Epoch: 4/10, step 494/574 completed (loss: 0.1416277438402176, acc: 0.9473684430122375)
[2025-01-06 01:22:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:46][root][INFO] - Training Epoch: 4/10, step 495/574 completed (loss: 0.015942543745040894, acc: 1.0)
[2025-01-06 01:22:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:46][root][INFO] - Training Epoch: 4/10, step 496/574 completed (loss: 0.3009811043739319, acc: 0.9017857313156128)
[2025-01-06 01:22:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:47][root][INFO] - Training Epoch: 4/10, step 497/574 completed (loss: 0.10648035258054733, acc: 0.966292142868042)
[2025-01-06 01:22:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:47][root][INFO] - Training Epoch: 4/10, step 498/574 completed (loss: 0.24879655241966248, acc: 0.8876404762268066)
[2025-01-06 01:22:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:48][root][INFO] - Training Epoch: 4/10, step 499/574 completed (loss: 0.4990527033805847, acc: 0.8226950168609619)
[2025-01-06 01:22:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:48][root][INFO] - Training Epoch: 4/10, step 500/574 completed (loss: 0.360302597284317, acc: 0.9021739363670349)
[2025-01-06 01:22:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:48][root][INFO] - Training Epoch: 4/10, step 501/574 completed (loss: 0.0364045612514019, acc: 0.9599999785423279)
[2025-01-06 01:22:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:49][root][INFO] - Training Epoch: 4/10, step 502/574 completed (loss: 0.000955507974140346, acc: 1.0)
[2025-01-06 01:22:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:49][root][INFO] - Training Epoch: 4/10, step 503/574 completed (loss: 0.007397472392767668, acc: 1.0)
[2025-01-06 01:22:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:49][root][INFO] - Training Epoch: 4/10, step 504/574 completed (loss: 0.009489822201430798, acc: 1.0)
[2025-01-06 01:22:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:50][root][INFO] - Training Epoch: 4/10, step 505/574 completed (loss: 0.20408767461776733, acc: 0.9245283007621765)
[2025-01-06 01:22:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:50][root][INFO] - Training Epoch: 4/10, step 506/574 completed (loss: 0.8078685998916626, acc: 0.8275862336158752)
[2025-01-06 01:22:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:51][root][INFO] - Training Epoch: 4/10, step 507/574 completed (loss: 0.6194598078727722, acc: 0.8468468189239502)
[2025-01-06 01:22:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:51][root][INFO] - Training Epoch: 4/10, step 508/574 completed (loss: 0.22152899205684662, acc: 0.9436619877815247)
[2025-01-06 01:22:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:51][root][INFO] - Training Epoch: 4/10, step 509/574 completed (loss: 0.008567312732338905, acc: 1.0)
[2025-01-06 01:22:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:52][root][INFO] - Training Epoch: 4/10, step 510/574 completed (loss: 0.007783496752381325, acc: 1.0)
[2025-01-06 01:22:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:52][root][INFO] - Training Epoch: 4/10, step 511/574 completed (loss: 0.047158755362033844, acc: 0.9615384340286255)
[2025-01-06 01:22:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:55][root][INFO] - Training Epoch: 4/10, step 512/574 completed (loss: 0.4341450035572052, acc: 0.8785714507102966)
[2025-01-06 01:22:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:56][root][INFO] - Training Epoch: 4/10, step 513/574 completed (loss: 0.06299154460430145, acc: 0.976190447807312)
[2025-01-06 01:22:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:56][root][INFO] - Training Epoch: 4/10, step 514/574 completed (loss: 0.16584445536136627, acc: 0.9285714030265808)
[2025-01-06 01:22:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:56][root][INFO] - Training Epoch: 4/10, step 515/574 completed (loss: 0.016427787020802498, acc: 1.0)
[2025-01-06 01:22:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:57][root][INFO] - Training Epoch: 4/10, step 516/574 completed (loss: 0.19140392541885376, acc: 0.9444444179534912)
[2025-01-06 01:22:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:57][root][INFO] - Training Epoch: 4/10, step 517/574 completed (loss: 0.000823112903162837, acc: 1.0)
[2025-01-06 01:22:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:57][root][INFO] - Training Epoch: 4/10, step 518/574 completed (loss: 0.009221833199262619, acc: 1.0)
[2025-01-06 01:22:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:58][root][INFO] - Training Epoch: 4/10, step 519/574 completed (loss: 0.05177469179034233, acc: 1.0)
[2025-01-06 01:22:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:58][root][INFO] - Training Epoch: 4/10, step 520/574 completed (loss: 0.018502771854400635, acc: 1.0)
[2025-01-06 01:22:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:59][root][INFO] - Training Epoch: 4/10, step 521/574 completed (loss: 0.4555322825908661, acc: 0.8516949415206909)
[2025-01-06 01:22:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:22:59][root][INFO] - Training Epoch: 4/10, step 522/574 completed (loss: 0.049714453518390656, acc: 0.9925373196601868)
[2025-01-06 01:23:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:00][root][INFO] - Training Epoch: 4/10, step 523/574 completed (loss: 0.1651802361011505, acc: 0.9343065619468689)
[2025-01-06 01:23:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:00][root][INFO] - Training Epoch: 4/10, step 524/574 completed (loss: 0.39213359355926514, acc: 0.8949999809265137)
[2025-01-06 01:23:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:01][root][INFO] - Training Epoch: 4/10, step 525/574 completed (loss: 0.005283591337502003, acc: 1.0)
[2025-01-06 01:23:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:01][root][INFO] - Training Epoch: 4/10, step 526/574 completed (loss: 0.038947947323322296, acc: 1.0)
[2025-01-06 01:23:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:01][root][INFO] - Training Epoch: 4/10, step 527/574 completed (loss: 0.02767491340637207, acc: 1.0)
[2025-01-06 01:23:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:02][root][INFO] - Training Epoch: 4/10, step 528/574 completed (loss: 0.4452095031738281, acc: 0.8524590134620667)
[2025-01-06 01:23:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:02][root][INFO] - Training Epoch: 4/10, step 529/574 completed (loss: 0.08805210143327713, acc: 0.9830508232116699)
[2025-01-06 01:23:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:03][root][INFO] - Training Epoch: 4/10, step 530/574 completed (loss: 0.4729470908641815, acc: 0.8837209343910217)
[2025-01-06 01:23:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:03][root][INFO] - Training Epoch: 4/10, step 531/574 completed (loss: 0.15396995842456818, acc: 0.9772727489471436)
[2025-01-06 01:23:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:03][root][INFO] - Training Epoch: 4/10, step 532/574 completed (loss: 0.19061918556690216, acc: 0.9056603908538818)
[2025-01-06 01:23:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:04][root][INFO] - Training Epoch: 4/10, step 533/574 completed (loss: 0.33842456340789795, acc: 0.9545454382896423)
[2025-01-06 01:23:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:04][root][INFO] - Training Epoch: 4/10, step 534/574 completed (loss: 0.04975563660264015, acc: 1.0)
[2025-01-06 01:23:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:04][root][INFO] - Training Epoch: 4/10, step 535/574 completed (loss: 0.028610264882445335, acc: 1.0)
[2025-01-06 01:23:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:05][root][INFO] - Training Epoch: 4/10, step 536/574 completed (loss: 0.1006564199924469, acc: 0.9545454382896423)
[2025-01-06 01:23:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:05][root][INFO] - Training Epoch: 4/10, step 537/574 completed (loss: 0.16098365187644958, acc: 0.9692307710647583)
[2025-01-06 01:23:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:05][root][INFO] - Training Epoch: 4/10, step 538/574 completed (loss: 0.23977245390415192, acc: 0.90625)
[2025-01-06 01:23:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:06][root][INFO] - Training Epoch: 4/10, step 539/574 completed (loss: 0.03805811330676079, acc: 1.0)
[2025-01-06 01:23:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:06][root][INFO] - Training Epoch: 4/10, step 540/574 completed (loss: 0.05822901800274849, acc: 0.9696969985961914)
[2025-01-06 01:23:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:07][root][INFO] - Training Epoch: 4/10, step 541/574 completed (loss: 0.003570006461814046, acc: 1.0)
[2025-01-06 01:23:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:07][root][INFO] - Training Epoch: 4/10, step 542/574 completed (loss: 0.0018334167543798685, acc: 1.0)
[2025-01-06 01:23:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:07][root][INFO] - Training Epoch: 4/10, step 543/574 completed (loss: 0.0015173867577686906, acc: 1.0)
[2025-01-06 01:23:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:08][root][INFO] - Training Epoch: 4/10, step 544/574 completed (loss: 0.018886912614107132, acc: 1.0)
[2025-01-06 01:23:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:08][root][INFO] - Training Epoch: 4/10, step 545/574 completed (loss: 0.011263348162174225, acc: 1.0)
[2025-01-06 01:23:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:08][root][INFO] - Training Epoch: 4/10, step 546/574 completed (loss: 0.013193645514547825, acc: 1.0)
[2025-01-06 01:23:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:09][root][INFO] - Training Epoch: 4/10, step 547/574 completed (loss: 0.01037314347922802, acc: 1.0)
[2025-01-06 01:23:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:09][root][INFO] - Training Epoch: 4/10, step 548/574 completed (loss: 0.045091498643159866, acc: 0.9677419066429138)
[2025-01-06 01:23:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:09][root][INFO] - Training Epoch: 4/10, step 549/574 completed (loss: 0.04582832753658295, acc: 0.9599999785423279)
[2025-01-06 01:23:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:10][root][INFO] - Training Epoch: 4/10, step 550/574 completed (loss: 0.021392088383436203, acc: 1.0)
[2025-01-06 01:23:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:10][root][INFO] - Training Epoch: 4/10, step 551/574 completed (loss: 0.0412035807967186, acc: 0.9750000238418579)
[2025-01-06 01:23:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:10][root][INFO] - Training Epoch: 4/10, step 552/574 completed (loss: 0.043356262147426605, acc: 0.9714285731315613)
[2025-01-06 01:23:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:11][root][INFO] - Training Epoch: 4/10, step 553/574 completed (loss: 0.16153211891651154, acc: 0.956204354763031)
[2025-01-06 01:23:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:11][root][INFO] - Training Epoch: 4/10, step 554/574 completed (loss: 0.05388270691037178, acc: 0.9862068891525269)
[2025-01-06 01:23:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:11][root][INFO] - Training Epoch: 4/10, step 555/574 completed (loss: 0.1497790813446045, acc: 0.9857142567634583)
[2025-01-06 01:23:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:12][root][INFO] - Training Epoch: 4/10, step 556/574 completed (loss: 0.24491523206233978, acc: 0.9337748289108276)
[2025-01-06 01:23:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:12][root][INFO] - Training Epoch: 4/10, step 557/574 completed (loss: 0.0627792552113533, acc: 0.9743589758872986)
[2025-01-06 01:23:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:13][root][INFO] - Training Epoch: 4/10, step 558/574 completed (loss: 0.025572722777724266, acc: 1.0)
[2025-01-06 01:23:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:13][root][INFO] - Training Epoch: 4/10, step 559/574 completed (loss: 0.05325213819742203, acc: 0.9615384340286255)
[2025-01-06 01:23:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:13][root][INFO] - Training Epoch: 4/10, step 560/574 completed (loss: 0.0017364324303343892, acc: 1.0)
[2025-01-06 01:23:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:14][root][INFO] - Training Epoch: 4/10, step 561/574 completed (loss: 0.2858171761035919, acc: 0.9743589758872986)
[2025-01-06 01:23:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:14][root][INFO] - Training Epoch: 4/10, step 562/574 completed (loss: 0.14748142659664154, acc: 0.9555555582046509)
[2025-01-06 01:23:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:14][root][INFO] - Training Epoch: 4/10, step 563/574 completed (loss: 0.13769471645355225, acc: 0.9740259647369385)
[2025-01-06 01:23:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:15][root][INFO] - Training Epoch: 4/10, step 564/574 completed (loss: 0.011711892671883106, acc: 1.0)
[2025-01-06 01:23:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:15][root][INFO] - Training Epoch: 4/10, step 565/574 completed (loss: 0.06642382591962814, acc: 0.9655172228813171)
[2025-01-06 01:23:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:45][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.1154, device='cuda:0') eval_epoch_loss=tensor(0.7492, device='cuda:0') eval_epoch_acc=tensor(0.8448, device='cuda:0')
[2025-01-06 01:23:45][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:23:45][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:23:46][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_4_step_566_loss_0.7492378950119019/model.pt
[2025-01-06 01:23:46][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:23:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:46][root][INFO] - Training Epoch: 4/10, step 566/574 completed (loss: 0.10738175362348557, acc: 0.9642857313156128)
[2025-01-06 01:23:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:46][root][INFO] - Training Epoch: 4/10, step 567/574 completed (loss: 0.0026657767593860626, acc: 1.0)
[2025-01-06 01:23:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:47][root][INFO] - Training Epoch: 4/10, step 568/574 completed (loss: 0.004107438959181309, acc: 1.0)
[2025-01-06 01:23:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:47][root][INFO] - Training Epoch: 4/10, step 569/574 completed (loss: 0.11200448870658875, acc: 0.9786096215248108)
[2025-01-06 01:23:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:47][root][INFO] - Training Epoch: 4/10, step 570/574 completed (loss: 0.003864176804199815, acc: 1.0)
[2025-01-06 01:23:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:48][root][INFO] - Training Epoch: 4/10, step 571/574 completed (loss: 0.01451493427157402, acc: 0.9914529919624329)
[2025-01-06 01:23:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:48][root][INFO] - Training Epoch: 4/10, step 572/574 completed (loss: 0.2148391604423523, acc: 0.918367326259613)
[2025-01-06 01:23:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:48][root][INFO] - Training Epoch: 4/10, step 573/574 completed (loss: 0.11717749387025833, acc: 0.9811320900917053)
[2025-01-06 01:23:49][slam_llm.utils.train_utils][INFO] - Epoch 4: train_perplexity=1.2091, train_epoch_loss=0.1899, epoch time 354.24308686703444s
[2025-01-06 01:23:49][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 13 GB
[2025-01-06 01:23:49][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 14 GB
[2025-01-06 01:23:49][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 13 GB
[2025-01-06 01:23:49][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 12
[2025-01-06 01:23:49][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2025-01-06 01:23:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:49][root][INFO] - Training Epoch: 5/10, step 0/574 completed (loss: 0.01660514622926712, acc: 1.0)
[2025-01-06 01:23:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:50][root][INFO] - Training Epoch: 5/10, step 1/574 completed (loss: 0.03390007093548775, acc: 1.0)
[2025-01-06 01:23:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:50][root][INFO] - Training Epoch: 5/10, step 2/574 completed (loss: 0.5597116947174072, acc: 0.9189189076423645)
[2025-01-06 01:23:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:51][root][INFO] - Training Epoch: 5/10, step 3/574 completed (loss: 0.0100776432082057, acc: 1.0)
[2025-01-06 01:23:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:51][root][INFO] - Training Epoch: 5/10, step 4/574 completed (loss: 0.07594954967498779, acc: 0.9729729890823364)
[2025-01-06 01:23:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:51][root][INFO] - Training Epoch: 5/10, step 5/574 completed (loss: 0.018001452088356018, acc: 1.0)
[2025-01-06 01:23:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:52][root][INFO] - Training Epoch: 5/10, step 6/574 completed (loss: 0.17811495065689087, acc: 0.9387755393981934)
[2025-01-06 01:23:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:52][root][INFO] - Training Epoch: 5/10, step 7/574 completed (loss: 0.018380049616098404, acc: 1.0)
[2025-01-06 01:23:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:52][root][INFO] - Training Epoch: 5/10, step 8/574 completed (loss: 0.05484870821237564, acc: 0.9545454382896423)
[2025-01-06 01:23:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:53][root][INFO] - Training Epoch: 5/10, step 9/574 completed (loss: 0.001398535561747849, acc: 1.0)
[2025-01-06 01:23:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:53][root][INFO] - Training Epoch: 5/10, step 10/574 completed (loss: 0.03515641391277313, acc: 0.9629629850387573)
[2025-01-06 01:23:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:53][root][INFO] - Training Epoch: 5/10, step 11/574 completed (loss: 0.09382610023021698, acc: 0.9487179517745972)
[2025-01-06 01:23:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:54][root][INFO] - Training Epoch: 5/10, step 12/574 completed (loss: 0.011907785199582577, acc: 1.0)
[2025-01-06 01:23:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:54][root][INFO] - Training Epoch: 5/10, step 13/574 completed (loss: 0.08189965039491653, acc: 0.97826087474823)
[2025-01-06 01:23:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:54][root][INFO] - Training Epoch: 5/10, step 14/574 completed (loss: 0.013241001404821873, acc: 1.0)
[2025-01-06 01:23:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:55][root][INFO] - Training Epoch: 5/10, step 15/574 completed (loss: 0.05698870122432709, acc: 0.9795918464660645)
[2025-01-06 01:23:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:55][root][INFO] - Training Epoch: 5/10, step 16/574 completed (loss: 0.0018444565357640386, acc: 1.0)
[2025-01-06 01:23:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:55][root][INFO] - Training Epoch: 5/10, step 17/574 completed (loss: 0.025469234213232994, acc: 1.0)
[2025-01-06 01:23:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:56][root][INFO] - Training Epoch: 5/10, step 18/574 completed (loss: 0.08021073043346405, acc: 0.9722222089767456)
[2025-01-06 01:23:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:56][root][INFO] - Training Epoch: 5/10, step 19/574 completed (loss: 0.05322107672691345, acc: 1.0)
[2025-01-06 01:23:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:57][root][INFO] - Training Epoch: 5/10, step 20/574 completed (loss: 0.009410485625267029, acc: 1.0)
[2025-01-06 01:23:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:57][root][INFO] - Training Epoch: 5/10, step 21/574 completed (loss: 0.001719304476864636, acc: 1.0)
[2025-01-06 01:23:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:57][root][INFO] - Training Epoch: 5/10, step 22/574 completed (loss: 0.0069836825132369995, acc: 1.0)
[2025-01-06 01:23:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:58][root][INFO] - Training Epoch: 5/10, step 23/574 completed (loss: 0.04341956973075867, acc: 1.0)
[2025-01-06 01:23:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:58][root][INFO] - Training Epoch: 5/10, step 24/574 completed (loss: 0.03395194932818413, acc: 1.0)
[2025-01-06 01:23:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:58][root][INFO] - Training Epoch: 5/10, step 25/574 completed (loss: 0.106379434466362, acc: 0.9622641801834106)
[2025-01-06 01:23:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:23:59][root][INFO] - Training Epoch: 5/10, step 26/574 completed (loss: 0.11081089079380035, acc: 0.9863013625144958)
[2025-01-06 01:23:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:00][root][INFO] - Training Epoch: 5/10, step 27/574 completed (loss: 0.46106505393981934, acc: 0.8537549376487732)
[2025-01-06 01:24:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:00][root][INFO] - Training Epoch: 5/10, step 28/574 completed (loss: 0.022539444267749786, acc: 1.0)
[2025-01-06 01:24:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:01][root][INFO] - Training Epoch: 5/10, step 29/574 completed (loss: 0.13871921598911285, acc: 0.9638554453849792)
[2025-01-06 01:24:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:01][root][INFO] - Training Epoch: 5/10, step 30/574 completed (loss: 0.09180906414985657, acc: 0.9753086566925049)
[2025-01-06 01:24:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:01][root][INFO] - Training Epoch: 5/10, step 31/574 completed (loss: 0.04703620821237564, acc: 1.0)
[2025-01-06 01:24:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:02][root][INFO] - Training Epoch: 5/10, step 32/574 completed (loss: 0.052823565900325775, acc: 0.9629629850387573)
[2025-01-06 01:24:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:02][root][INFO] - Training Epoch: 5/10, step 33/574 completed (loss: 0.008719949051737785, acc: 1.0)
[2025-01-06 01:24:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:02][root][INFO] - Training Epoch: 5/10, step 34/574 completed (loss: 0.1467677652835846, acc: 0.9495798349380493)
[2025-01-06 01:24:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:03][root][INFO] - Training Epoch: 5/10, step 35/574 completed (loss: 0.029669221490621567, acc: 0.9836065769195557)
[2025-01-06 01:24:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:03][root][INFO] - Training Epoch: 5/10, step 36/574 completed (loss: 0.14925691485404968, acc: 0.9365079402923584)
[2025-01-06 01:24:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:03][root][INFO] - Training Epoch: 5/10, step 37/574 completed (loss: 0.16651470959186554, acc: 0.9661017060279846)
[2025-01-06 01:24:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:04][root][INFO] - Training Epoch: 5/10, step 38/574 completed (loss: 0.046023473143577576, acc: 1.0)
[2025-01-06 01:24:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:04][root][INFO] - Training Epoch: 5/10, step 39/574 completed (loss: 0.010108904913067818, acc: 1.0)
[2025-01-06 01:24:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:04][root][INFO] - Training Epoch: 5/10, step 40/574 completed (loss: 0.10061465203762054, acc: 0.9615384340286255)
[2025-01-06 01:24:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:05][root][INFO] - Training Epoch: 5/10, step 41/574 completed (loss: 0.06594770401716232, acc: 0.9729729890823364)
[2025-01-06 01:24:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:05][root][INFO] - Training Epoch: 5/10, step 42/574 completed (loss: 0.19569559395313263, acc: 0.9538461565971375)
[2025-01-06 01:24:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:06][root][INFO] - Training Epoch: 5/10, step 43/574 completed (loss: 0.22517862915992737, acc: 0.9494949579238892)
[2025-01-06 01:24:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:06][root][INFO] - Training Epoch: 5/10, step 44/574 completed (loss: 0.23028552532196045, acc: 0.938144326210022)
[2025-01-06 01:24:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:06][root][INFO] - Training Epoch: 5/10, step 45/574 completed (loss: 0.14046388864517212, acc: 0.9558823704719543)
[2025-01-06 01:24:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:07][root][INFO] - Training Epoch: 5/10, step 46/574 completed (loss: 0.06071082130074501, acc: 1.0)
[2025-01-06 01:24:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:07][root][INFO] - Training Epoch: 5/10, step 47/574 completed (loss: 0.03040466271340847, acc: 1.0)
[2025-01-06 01:24:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:07][root][INFO] - Training Epoch: 5/10, step 48/574 completed (loss: 0.1283913105726242, acc: 0.9285714030265808)
[2025-01-06 01:24:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:08][root][INFO] - Training Epoch: 5/10, step 49/574 completed (loss: 0.0057349856942892075, acc: 1.0)
[2025-01-06 01:24:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:08][root][INFO] - Training Epoch: 5/10, step 50/574 completed (loss: 0.3581145405769348, acc: 0.8771929740905762)
[2025-01-06 01:24:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:09][root][INFO] - Training Epoch: 5/10, step 51/574 completed (loss: 0.2533422112464905, acc: 0.9365079402923584)
[2025-01-06 01:24:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:09][root][INFO] - Training Epoch: 5/10, step 52/574 completed (loss: 0.22011516988277435, acc: 0.9154929518699646)
[2025-01-06 01:24:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:09][root][INFO] - Training Epoch: 5/10, step 53/574 completed (loss: 0.8960555195808411, acc: 0.746666669845581)
[2025-01-06 01:24:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:10][root][INFO] - Training Epoch: 5/10, step 54/574 completed (loss: 0.22673049569129944, acc: 0.9189189076423645)
[2025-01-06 01:24:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:10][root][INFO] - Training Epoch: 5/10, step 55/574 completed (loss: 0.00512132840231061, acc: 1.0)
[2025-01-06 01:24:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:13][root][INFO] - Training Epoch: 5/10, step 56/574 completed (loss: 0.9546047449111938, acc: 0.6962457299232483)
[2025-01-06 01:24:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:14][root][INFO] - Training Epoch: 5/10, step 57/574 completed (loss: 0.9924182891845703, acc: 0.7254902124404907)
[2025-01-06 01:24:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:15][root][INFO] - Training Epoch: 5/10, step 58/574 completed (loss: 0.5381298065185547, acc: 0.8238636255264282)
[2025-01-06 01:24:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:16][root][INFO] - Training Epoch: 5/10, step 59/574 completed (loss: 0.07952892780303955, acc: 0.9779411554336548)
[2025-01-06 01:24:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:16][root][INFO] - Training Epoch: 5/10, step 60/574 completed (loss: 0.4547921121120453, acc: 0.8550724387168884)
[2025-01-06 01:24:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:17][root][INFO] - Training Epoch: 5/10, step 61/574 completed (loss: 0.2392096072435379, acc: 0.9375)
[2025-01-06 01:24:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:17][root][INFO] - Training Epoch: 5/10, step 62/574 completed (loss: 0.04950325936079025, acc: 1.0)
[2025-01-06 01:24:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:17][root][INFO] - Training Epoch: 5/10, step 63/574 completed (loss: 0.04775679111480713, acc: 0.9722222089767456)
[2025-01-06 01:24:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:18][root][INFO] - Training Epoch: 5/10, step 64/574 completed (loss: 0.021696731448173523, acc: 1.0)
[2025-01-06 01:24:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:18][root][INFO] - Training Epoch: 5/10, step 65/574 completed (loss: 0.011471165344119072, acc: 1.0)
[2025-01-06 01:24:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:19][root][INFO] - Training Epoch: 5/10, step 66/574 completed (loss: 0.24379651248455048, acc: 0.9464285969734192)
[2025-01-06 01:24:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:19][root][INFO] - Training Epoch: 5/10, step 67/574 completed (loss: 0.05440446361899376, acc: 1.0)
[2025-01-06 01:24:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:19][root][INFO] - Training Epoch: 5/10, step 68/574 completed (loss: 0.0008991159265860915, acc: 1.0)
[2025-01-06 01:24:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:20][root][INFO] - Training Epoch: 5/10, step 69/574 completed (loss: 0.04048463702201843, acc: 1.0)
[2025-01-06 01:24:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:20][root][INFO] - Training Epoch: 5/10, step 70/574 completed (loss: 0.04926832765340805, acc: 1.0)
[2025-01-06 01:24:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:20][root][INFO] - Training Epoch: 5/10, step 71/574 completed (loss: 0.3917418420314789, acc: 0.875)
[2025-01-06 01:24:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:21][root][INFO] - Training Epoch: 5/10, step 72/574 completed (loss: 0.28192993998527527, acc: 0.89682537317276)
[2025-01-06 01:24:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:21][root][INFO] - Training Epoch: 5/10, step 73/574 completed (loss: 0.6798601746559143, acc: 0.7846153974533081)
[2025-01-06 01:24:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:21][root][INFO] - Training Epoch: 5/10, step 74/574 completed (loss: 0.40063580870628357, acc: 0.8775510191917419)
[2025-01-06 01:24:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:22][root][INFO] - Training Epoch: 5/10, step 75/574 completed (loss: 0.5527570247650146, acc: 0.8358209133148193)
[2025-01-06 01:24:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:22][root][INFO] - Training Epoch: 5/10, step 76/574 completed (loss: 0.8195779323577881, acc: 0.7518247961997986)
[2025-01-06 01:24:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:22][root][INFO] - Training Epoch: 5/10, step 77/574 completed (loss: 0.0027252216823399067, acc: 1.0)
[2025-01-06 01:24:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:23][root][INFO] - Training Epoch: 5/10, step 78/574 completed (loss: 0.02336827665567398, acc: 1.0)
[2025-01-06 01:24:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:23][root][INFO] - Training Epoch: 5/10, step 79/574 completed (loss: 0.143771693110466, acc: 0.9696969985961914)
[2025-01-06 01:24:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:24][root][INFO] - Training Epoch: 5/10, step 80/574 completed (loss: 0.013255228288471699, acc: 1.0)
[2025-01-06 01:24:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:24][root][INFO] - Training Epoch: 5/10, step 81/574 completed (loss: 0.11759453266859055, acc: 0.942307710647583)
[2025-01-06 01:24:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:24][root][INFO] - Training Epoch: 5/10, step 82/574 completed (loss: 0.11471159011125565, acc: 0.9807692170143127)
[2025-01-06 01:24:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:25][root][INFO] - Training Epoch: 5/10, step 83/574 completed (loss: 0.034202732145786285, acc: 1.0)
[2025-01-06 01:24:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:25][root][INFO] - Training Epoch: 5/10, step 84/574 completed (loss: 0.14895890653133392, acc: 0.9130434989929199)
[2025-01-06 01:24:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:25][root][INFO] - Training Epoch: 5/10, step 85/574 completed (loss: 0.07382180541753769, acc: 0.9800000190734863)
[2025-01-06 01:24:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:26][root][INFO] - Training Epoch: 5/10, step 86/574 completed (loss: 0.04767230898141861, acc: 0.95652174949646)
[2025-01-06 01:24:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:26][root][INFO] - Training Epoch: 5/10, step 87/574 completed (loss: 0.16451789438724518, acc: 0.9399999976158142)
[2025-01-06 01:24:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:26][root][INFO] - Training Epoch: 5/10, step 88/574 completed (loss: 0.23603905737400055, acc: 0.9417475461959839)
[2025-01-06 01:24:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:28][root][INFO] - Training Epoch: 5/10, step 89/574 completed (loss: 0.5144832730293274, acc: 0.844660222530365)
[2025-01-06 01:24:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:28][root][INFO] - Training Epoch: 5/10, step 90/574 completed (loss: 0.5783466100692749, acc: 0.8225806355476379)
[2025-01-06 01:24:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:29][root][INFO] - Training Epoch: 5/10, step 91/574 completed (loss: 0.6620256304740906, acc: 0.818965494632721)
[2025-01-06 01:24:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:30][root][INFO] - Training Epoch: 5/10, step 92/574 completed (loss: 0.28326651453971863, acc: 0.9052631855010986)
[2025-01-06 01:24:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:31][root][INFO] - Training Epoch: 5/10, step 93/574 completed (loss: 0.5513370037078857, acc: 0.8514851331710815)
[2025-01-06 01:24:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:31][root][INFO] - Training Epoch: 5/10, step 94/574 completed (loss: 0.20921345055103302, acc: 0.9516128897666931)
[2025-01-06 01:24:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:32][root][INFO] - Training Epoch: 5/10, step 95/574 completed (loss: 0.25921958684921265, acc: 0.8840579986572266)
[2025-01-06 01:24:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:32][root][INFO] - Training Epoch: 5/10, step 96/574 completed (loss: 0.4633651077747345, acc: 0.8571428656578064)
[2025-01-06 01:24:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:32][root][INFO] - Training Epoch: 5/10, step 97/574 completed (loss: 0.40754276514053345, acc: 0.8942307829856873)
[2025-01-06 01:24:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:33][root][INFO] - Training Epoch: 5/10, step 98/574 completed (loss: 0.4905973970890045, acc: 0.8540145754814148)
[2025-01-06 01:24:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:33][root][INFO] - Training Epoch: 5/10, step 99/574 completed (loss: 0.4076608121395111, acc: 0.8656716346740723)
[2025-01-06 01:24:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:33][root][INFO] - Training Epoch: 5/10, step 100/574 completed (loss: 0.037580158561468124, acc: 1.0)
[2025-01-06 01:24:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:34][root][INFO] - Training Epoch: 5/10, step 101/574 completed (loss: 0.0009425808093510568, acc: 1.0)
[2025-01-06 01:24:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:34][root][INFO] - Training Epoch: 5/10, step 102/574 completed (loss: 0.007348355837166309, acc: 1.0)
[2025-01-06 01:24:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:35][root][INFO] - Training Epoch: 5/10, step 103/574 completed (loss: 0.0026503638364374638, acc: 1.0)
[2025-01-06 01:24:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:35][root][INFO] - Training Epoch: 5/10, step 104/574 completed (loss: 0.1464981734752655, acc: 0.9482758641242981)
[2025-01-06 01:24:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:35][root][INFO] - Training Epoch: 5/10, step 105/574 completed (loss: 0.008757298812270164, acc: 1.0)
[2025-01-06 01:24:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:36][root][INFO] - Training Epoch: 5/10, step 106/574 completed (loss: 0.043796490877866745, acc: 1.0)
[2025-01-06 01:24:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:36][root][INFO] - Training Epoch: 5/10, step 107/574 completed (loss: 0.01013028621673584, acc: 1.0)
[2025-01-06 01:24:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:36][root][INFO] - Training Epoch: 5/10, step 108/574 completed (loss: 0.01028294488787651, acc: 1.0)
[2025-01-06 01:24:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:37][root][INFO] - Training Epoch: 5/10, step 109/574 completed (loss: 0.008463176898658276, acc: 1.0)
[2025-01-06 01:24:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:37][root][INFO] - Training Epoch: 5/10, step 110/574 completed (loss: 0.04265333712100983, acc: 0.9692307710647583)
[2025-01-06 01:24:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:37][root][INFO] - Training Epoch: 5/10, step 111/574 completed (loss: 0.17384043335914612, acc: 0.9473684430122375)
[2025-01-06 01:24:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:38][root][INFO] - Training Epoch: 5/10, step 112/574 completed (loss: 0.26819559931755066, acc: 0.9122806787490845)
[2025-01-06 01:24:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:38][root][INFO] - Training Epoch: 5/10, step 113/574 completed (loss: 0.05779882147908211, acc: 0.9743589758872986)
[2025-01-06 01:24:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:38][root][INFO] - Training Epoch: 5/10, step 114/574 completed (loss: 0.04020087420940399, acc: 1.0)
[2025-01-06 01:24:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:39][root][INFO] - Training Epoch: 5/10, step 115/574 completed (loss: 0.0017902744002640247, acc: 1.0)
[2025-01-06 01:24:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:39][root][INFO] - Training Epoch: 5/10, step 116/574 completed (loss: 0.07865625619888306, acc: 0.9682539701461792)
[2025-01-06 01:24:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:39][root][INFO] - Training Epoch: 5/10, step 117/574 completed (loss: 0.14130929112434387, acc: 0.9430894255638123)
[2025-01-06 01:24:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:40][root][INFO] - Training Epoch: 5/10, step 118/574 completed (loss: 0.03947267681360245, acc: 0.9838709831237793)
[2025-01-06 01:24:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:41][root][INFO] - Training Epoch: 5/10, step 119/574 completed (loss: 0.3250516653060913, acc: 0.9049429893493652)
[2025-01-06 01:24:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:41][root][INFO] - Training Epoch: 5/10, step 120/574 completed (loss: 0.05054334178566933, acc: 1.0)
[2025-01-06 01:24:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:41][root][INFO] - Training Epoch: 5/10, step 121/574 completed (loss: 0.08193360269069672, acc: 0.9807692170143127)
[2025-01-06 01:24:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:42][root][INFO] - Training Epoch: 5/10, step 122/574 completed (loss: 0.0031406215857714415, acc: 1.0)
[2025-01-06 01:24:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:42][root][INFO] - Training Epoch: 5/10, step 123/574 completed (loss: 0.023872749879956245, acc: 1.0)
[2025-01-06 01:24:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:43][root][INFO] - Training Epoch: 5/10, step 124/574 completed (loss: 0.48608702421188354, acc: 0.8895705342292786)
[2025-01-06 01:24:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:43][root][INFO] - Training Epoch: 5/10, step 125/574 completed (loss: 0.3414955735206604, acc: 0.9166666865348816)
[2025-01-06 01:24:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:43][root][INFO] - Training Epoch: 5/10, step 126/574 completed (loss: 0.5375179052352905, acc: 0.8583333492279053)
[2025-01-06 01:24:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:44][root][INFO] - Training Epoch: 5/10, step 127/574 completed (loss: 0.2365550547838211, acc: 0.9166666865348816)
[2025-01-06 01:24:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:44][root][INFO] - Training Epoch: 5/10, step 128/574 completed (loss: 0.29582005739212036, acc: 0.9230769276618958)
[2025-01-06 01:24:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:45][root][INFO] - Training Epoch: 5/10, step 129/574 completed (loss: 0.45058080554008484, acc: 0.904411792755127)
[2025-01-06 01:24:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:45][root][INFO] - Training Epoch: 5/10, step 130/574 completed (loss: 0.041716769337654114, acc: 1.0)
[2025-01-06 01:24:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:45][root][INFO] - Training Epoch: 5/10, step 131/574 completed (loss: 0.04225165396928787, acc: 1.0)
[2025-01-06 01:24:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:46][root][INFO] - Training Epoch: 5/10, step 132/574 completed (loss: 0.026739949360489845, acc: 1.0)
[2025-01-06 01:24:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:46][root][INFO] - Training Epoch: 5/10, step 133/574 completed (loss: 0.013790574856102467, acc: 1.0)
[2025-01-06 01:24:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:46][root][INFO] - Training Epoch: 5/10, step 134/574 completed (loss: 0.031652726233005524, acc: 1.0)
[2025-01-06 01:24:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:24:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:17][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.0317, device='cuda:0') eval_epoch_loss=tensor(0.7089, device='cuda:0') eval_epoch_acc=tensor(0.8498, device='cuda:0')
[2025-01-06 01:25:17][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:25:17][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:25:17][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_5_step_135_loss_0.7088739275932312/model.pt
[2025-01-06 01:25:17][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:25:17][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 5 is 0.8498251438140869
[2025-01-06 01:25:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:17][root][INFO] - Training Epoch: 5/10, step 135/574 completed (loss: 0.007859908044338226, acc: 1.0)
[2025-01-06 01:25:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:18][root][INFO] - Training Epoch: 5/10, step 136/574 completed (loss: 0.3374413847923279, acc: 0.9285714030265808)
[2025-01-06 01:25:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:18][root][INFO] - Training Epoch: 5/10, step 137/574 completed (loss: 0.0424632653594017, acc: 1.0)
[2025-01-06 01:25:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:18][root][INFO] - Training Epoch: 5/10, step 138/574 completed (loss: 0.25670281052589417, acc: 0.9130434989929199)
[2025-01-06 01:25:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:19][root][INFO] - Training Epoch: 5/10, step 139/574 completed (loss: 0.01272617932409048, acc: 1.0)
[2025-01-06 01:25:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:19][root][INFO] - Training Epoch: 5/10, step 140/574 completed (loss: 0.036792878061532974, acc: 1.0)
[2025-01-06 01:25:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:19][root][INFO] - Training Epoch: 5/10, step 141/574 completed (loss: 0.09292290359735489, acc: 0.9677419066429138)
[2025-01-06 01:25:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:20][root][INFO] - Training Epoch: 5/10, step 142/574 completed (loss: 0.12656164169311523, acc: 0.9729729890823364)
[2025-01-06 01:25:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:20][root][INFO] - Training Epoch: 5/10, step 143/574 completed (loss: 0.2681470811367035, acc: 0.8947368264198303)
[2025-01-06 01:25:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:21][root][INFO] - Training Epoch: 5/10, step 144/574 completed (loss: 0.27588269114494324, acc: 0.9029850959777832)
[2025-01-06 01:25:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:21][root][INFO] - Training Epoch: 5/10, step 145/574 completed (loss: 0.4843291938304901, acc: 0.9081632494926453)
[2025-01-06 01:25:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:22][root][INFO] - Training Epoch: 5/10, step 146/574 completed (loss: 0.3556733727455139, acc: 0.8617021441459656)
[2025-01-06 01:25:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:22][root][INFO] - Training Epoch: 5/10, step 147/574 completed (loss: 0.19375401735305786, acc: 0.9571428298950195)
[2025-01-06 01:25:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:22][root][INFO] - Training Epoch: 5/10, step 148/574 completed (loss: 0.024217883124947548, acc: 1.0)
[2025-01-06 01:25:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:23][root][INFO] - Training Epoch: 5/10, step 149/574 completed (loss: 0.21219143271446228, acc: 0.95652174949646)
[2025-01-06 01:25:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:23][root][INFO] - Training Epoch: 5/10, step 150/574 completed (loss: 0.043290600180625916, acc: 0.9655172228813171)
[2025-01-06 01:25:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:23][root][INFO] - Training Epoch: 5/10, step 151/574 completed (loss: 0.33255451917648315, acc: 0.9347826242446899)
[2025-01-06 01:25:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:24][root][INFO] - Training Epoch: 5/10, step 152/574 completed (loss: 0.09728389233350754, acc: 0.9661017060279846)
[2025-01-06 01:25:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:24][root][INFO] - Training Epoch: 5/10, step 153/574 completed (loss: 0.0854366272687912, acc: 0.9824561476707458)
[2025-01-06 01:25:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:24][root][INFO] - Training Epoch: 5/10, step 154/574 completed (loss: 0.20196668803691864, acc: 0.9054054021835327)
[2025-01-06 01:25:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:25][root][INFO] - Training Epoch: 5/10, step 155/574 completed (loss: 0.2726328670978546, acc: 0.9642857313156128)
[2025-01-06 01:25:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:25][root][INFO] - Training Epoch: 5/10, step 156/574 completed (loss: 0.004490775987505913, acc: 1.0)
[2025-01-06 01:25:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:25][root][INFO] - Training Epoch: 5/10, step 157/574 completed (loss: 0.26216766238212585, acc: 0.8947368264198303)
[2025-01-06 01:25:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:27][root][INFO] - Training Epoch: 5/10, step 158/574 completed (loss: 0.47215601801872253, acc: 0.837837815284729)
[2025-01-06 01:25:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:27][root][INFO] - Training Epoch: 5/10, step 159/574 completed (loss: 0.373520165681839, acc: 0.8888888955116272)
[2025-01-06 01:25:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:28][root][INFO] - Training Epoch: 5/10, step 160/574 completed (loss: 0.4445546269416809, acc: 0.8488371968269348)
[2025-01-06 01:25:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:28][root][INFO] - Training Epoch: 5/10, step 161/574 completed (loss: 0.33882996439933777, acc: 0.9176470637321472)
[2025-01-06 01:25:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:29][root][INFO] - Training Epoch: 5/10, step 162/574 completed (loss: 0.4056873023509979, acc: 0.9101123809814453)
[2025-01-06 01:25:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:29][root][INFO] - Training Epoch: 5/10, step 163/574 completed (loss: 0.16270987689495087, acc: 0.9545454382896423)
[2025-01-06 01:25:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:29][root][INFO] - Training Epoch: 5/10, step 164/574 completed (loss: 0.004262459930032492, acc: 1.0)
[2025-01-06 01:25:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:30][root][INFO] - Training Epoch: 5/10, step 165/574 completed (loss: 0.29046565294265747, acc: 0.8965517282485962)
[2025-01-06 01:25:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:30][root][INFO] - Training Epoch: 5/10, step 166/574 completed (loss: 0.2609015703201294, acc: 0.918367326259613)
[2025-01-06 01:25:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:30][root][INFO] - Training Epoch: 5/10, step 167/574 completed (loss: 0.16097165644168854, acc: 0.9800000190734863)
[2025-01-06 01:25:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:31][root][INFO] - Training Epoch: 5/10, step 168/574 completed (loss: 0.1391068547964096, acc: 0.9583333134651184)
[2025-01-06 01:25:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:31][root][INFO] - Training Epoch: 5/10, step 169/574 completed (loss: 0.5360598564147949, acc: 0.8627451062202454)
[2025-01-06 01:25:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:32][root][INFO] - Training Epoch: 5/10, step 170/574 completed (loss: 0.3781038522720337, acc: 0.9109588861465454)
[2025-01-06 01:25:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:32][root][INFO] - Training Epoch: 5/10, step 171/574 completed (loss: 0.0659874826669693, acc: 0.9583333134651184)
[2025-01-06 01:25:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:33][root][INFO] - Training Epoch: 5/10, step 172/574 completed (loss: 0.021246878430247307, acc: 1.0)
[2025-01-06 01:25:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:33][root][INFO] - Training Epoch: 5/10, step 173/574 completed (loss: 0.19279655814170837, acc: 0.9642857313156128)
[2025-01-06 01:25:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:34][root][INFO] - Training Epoch: 5/10, step 174/574 completed (loss: 0.49962785840034485, acc: 0.8230088353157043)
[2025-01-06 01:25:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:34][root][INFO] - Training Epoch: 5/10, step 175/574 completed (loss: 0.35172539949417114, acc: 0.8840579986572266)
[2025-01-06 01:25:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:34][root][INFO] - Training Epoch: 5/10, step 176/574 completed (loss: 0.15167908370494843, acc: 0.9431818127632141)
[2025-01-06 01:25:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:35][root][INFO] - Training Epoch: 5/10, step 177/574 completed (loss: 0.5061529278755188, acc: 0.8625954389572144)
[2025-01-06 01:25:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:36][root][INFO] - Training Epoch: 5/10, step 178/574 completed (loss: 0.5370325446128845, acc: 0.8518518805503845)
[2025-01-06 01:25:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:36][root][INFO] - Training Epoch: 5/10, step 179/574 completed (loss: 0.12936542928218842, acc: 0.9672130942344666)
[2025-01-06 01:25:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:36][root][INFO] - Training Epoch: 5/10, step 180/574 completed (loss: 0.02056131698191166, acc: 1.0)
[2025-01-06 01:25:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:37][root][INFO] - Training Epoch: 5/10, step 181/574 completed (loss: 0.003101334208622575, acc: 1.0)
[2025-01-06 01:25:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:37][root][INFO] - Training Epoch: 5/10, step 182/574 completed (loss: 0.07103672623634338, acc: 0.9642857313156128)
[2025-01-06 01:25:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:37][root][INFO] - Training Epoch: 5/10, step 183/574 completed (loss: 0.060200102627277374, acc: 0.9878048896789551)
[2025-01-06 01:25:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:38][root][INFO] - Training Epoch: 5/10, step 184/574 completed (loss: 0.25940585136413574, acc: 0.9456193447113037)
[2025-01-06 01:25:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:38][root][INFO] - Training Epoch: 5/10, step 185/574 completed (loss: 0.32192036509513855, acc: 0.9308357238769531)
[2025-01-06 01:25:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:39][root][INFO] - Training Epoch: 5/10, step 186/574 completed (loss: 0.23560908436775208, acc: 0.9375)
[2025-01-06 01:25:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:39][root][INFO] - Training Epoch: 5/10, step 187/574 completed (loss: 0.35336530208587646, acc: 0.904315173625946)
[2025-01-06 01:25:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:40][root][INFO] - Training Epoch: 5/10, step 188/574 completed (loss: 0.2704228162765503, acc: 0.9145907759666443)
[2025-01-06 01:25:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:40][root][INFO] - Training Epoch: 5/10, step 189/574 completed (loss: 0.02998211607336998, acc: 1.0)
[2025-01-06 01:25:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:41][root][INFO] - Training Epoch: 5/10, step 190/574 completed (loss: 0.30883923172950745, acc: 0.8837209343910217)
[2025-01-06 01:25:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:41][root][INFO] - Training Epoch: 5/10, step 191/574 completed (loss: 0.5856488347053528, acc: 0.8253968358039856)
[2025-01-06 01:25:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:42][root][INFO] - Training Epoch: 5/10, step 192/574 completed (loss: 0.44558092951774597, acc: 0.8333333134651184)
[2025-01-06 01:25:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:43][root][INFO] - Training Epoch: 5/10, step 193/574 completed (loss: 0.23792169988155365, acc: 0.9411764740943909)
[2025-01-06 01:25:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:44][root][INFO] - Training Epoch: 5/10, step 194/574 completed (loss: 0.48076948523521423, acc: 0.8580247163772583)
[2025-01-06 01:25:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:45][root][INFO] - Training Epoch: 5/10, step 195/574 completed (loss: 0.17351564764976501, acc: 0.9677419066429138)
[2025-01-06 01:25:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:45][root][INFO] - Training Epoch: 5/10, step 196/574 completed (loss: 0.0014337111497297883, acc: 1.0)
[2025-01-06 01:25:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:46][root][INFO] - Training Epoch: 5/10, step 197/574 completed (loss: 0.04727910831570625, acc: 1.0)
[2025-01-06 01:25:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:46][root][INFO] - Training Epoch: 5/10, step 198/574 completed (loss: 0.15846773982048035, acc: 0.9264705777168274)
[2025-01-06 01:25:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:46][root][INFO] - Training Epoch: 5/10, step 199/574 completed (loss: 0.42729368805885315, acc: 0.8676470518112183)
[2025-01-06 01:25:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:47][root][INFO] - Training Epoch: 5/10, step 200/574 completed (loss: 0.24065454304218292, acc: 0.9322034120559692)
[2025-01-06 01:25:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:47][root][INFO] - Training Epoch: 5/10, step 201/574 completed (loss: 0.24413318932056427, acc: 0.9029850959777832)
[2025-01-06 01:25:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:47][root][INFO] - Training Epoch: 5/10, step 202/574 completed (loss: 0.2862491309642792, acc: 0.893203854560852)
[2025-01-06 01:25:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:48][root][INFO] - Training Epoch: 5/10, step 203/574 completed (loss: 0.1091911569237709, acc: 0.9841269850730896)
[2025-01-06 01:25:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:48][root][INFO] - Training Epoch: 5/10, step 204/574 completed (loss: 0.022766204550862312, acc: 1.0)
[2025-01-06 01:25:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:49][root][INFO] - Training Epoch: 5/10, step 205/574 completed (loss: 0.0859808400273323, acc: 0.9820627570152283)
[2025-01-06 01:25:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:49][root][INFO] - Training Epoch: 5/10, step 206/574 completed (loss: 0.20507581532001495, acc: 0.9212598204612732)
[2025-01-06 01:25:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:49][root][INFO] - Training Epoch: 5/10, step 207/574 completed (loss: 0.12494514137506485, acc: 0.9655172228813171)
[2025-01-06 01:25:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:50][root][INFO] - Training Epoch: 5/10, step 208/574 completed (loss: 0.2490309327840805, acc: 0.9239130616188049)
[2025-01-06 01:25:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:50][root][INFO] - Training Epoch: 5/10, step 209/574 completed (loss: 0.14441224932670593, acc: 0.9649805426597595)
[2025-01-06 01:25:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:50][root][INFO] - Training Epoch: 5/10, step 210/574 completed (loss: 0.031778253614902496, acc: 0.989130437374115)
[2025-01-06 01:25:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:51][root][INFO] - Training Epoch: 5/10, step 211/574 completed (loss: 0.008585343137383461, acc: 1.0)
[2025-01-06 01:25:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:51][root][INFO] - Training Epoch: 5/10, step 212/574 completed (loss: 0.01520487479865551, acc: 1.0)
[2025-01-06 01:25:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:52][root][INFO] - Training Epoch: 5/10, step 213/574 completed (loss: 0.10187780112028122, acc: 0.957446813583374)
[2025-01-06 01:25:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:52][root][INFO] - Training Epoch: 5/10, step 214/574 completed (loss: 0.06416916847229004, acc: 0.9769230484962463)
[2025-01-06 01:25:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:53][root][INFO] - Training Epoch: 5/10, step 215/574 completed (loss: 0.019784657284617424, acc: 1.0)
[2025-01-06 01:25:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:53][root][INFO] - Training Epoch: 5/10, step 216/574 completed (loss: 0.04695798456668854, acc: 0.9767441749572754)
[2025-01-06 01:25:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:54][root][INFO] - Training Epoch: 5/10, step 217/574 completed (loss: 0.07656590640544891, acc: 0.9729729890823364)
[2025-01-06 01:25:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:54][root][INFO] - Training Epoch: 5/10, step 218/574 completed (loss: 0.07302849739789963, acc: 0.9555555582046509)
[2025-01-06 01:25:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:54][root][INFO] - Training Epoch: 5/10, step 219/574 completed (loss: 0.10227382183074951, acc: 0.9696969985961914)
[2025-01-06 01:25:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:55][root][INFO] - Training Epoch: 5/10, step 220/574 completed (loss: 0.0014644465409219265, acc: 1.0)
[2025-01-06 01:25:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:55][root][INFO] - Training Epoch: 5/10, step 221/574 completed (loss: 0.0030209391843527555, acc: 1.0)
[2025-01-06 01:25:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:55][root][INFO] - Training Epoch: 5/10, step 222/574 completed (loss: 0.26853618025779724, acc: 0.9230769276618958)
[2025-01-06 01:25:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:56][root][INFO] - Training Epoch: 5/10, step 223/574 completed (loss: 0.1675121784210205, acc: 0.9619565010070801)
[2025-01-06 01:25:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:57][root][INFO] - Training Epoch: 5/10, step 224/574 completed (loss: 0.27963316440582275, acc: 0.9090909361839294)
[2025-01-06 01:25:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:57][root][INFO] - Training Epoch: 5/10, step 225/574 completed (loss: 0.37277400493621826, acc: 0.8829787373542786)
[2025-01-06 01:25:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:58][root][INFO] - Training Epoch: 5/10, step 226/574 completed (loss: 0.05453601852059364, acc: 1.0)
[2025-01-06 01:25:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:58][root][INFO] - Training Epoch: 5/10, step 227/574 completed (loss: 0.2670617997646332, acc: 0.9666666388511658)
[2025-01-06 01:25:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:58][root][INFO] - Training Epoch: 5/10, step 228/574 completed (loss: 0.30241692066192627, acc: 0.9534883499145508)
[2025-01-06 01:25:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:59][root][INFO] - Training Epoch: 5/10, step 229/574 completed (loss: 0.32384830713272095, acc: 0.8999999761581421)
[2025-01-06 01:25:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:59][root][INFO] - Training Epoch: 5/10, step 230/574 completed (loss: 0.7010190486907959, acc: 0.821052610874176)
[2025-01-06 01:25:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:25:59][root][INFO] - Training Epoch: 5/10, step 231/574 completed (loss: 0.8114289045333862, acc: 0.7666666507720947)
[2025-01-06 01:25:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:00][root][INFO] - Training Epoch: 5/10, step 232/574 completed (loss: 0.8005346655845642, acc: 0.7722222208976746)
[2025-01-06 01:26:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:00][root][INFO] - Training Epoch: 5/10, step 233/574 completed (loss: 0.9961376190185547, acc: 0.7018348574638367)
[2025-01-06 01:26:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:01][root][INFO] - Training Epoch: 5/10, step 234/574 completed (loss: 0.6755216121673584, acc: 0.7923076748847961)
[2025-01-06 01:26:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:01][root][INFO] - Training Epoch: 5/10, step 235/574 completed (loss: 0.020527567714452744, acc: 1.0)
[2025-01-06 01:26:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:01][root][INFO] - Training Epoch: 5/10, step 236/574 completed (loss: 0.029804738238453865, acc: 1.0)
[2025-01-06 01:26:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:02][root][INFO] - Training Epoch: 5/10, step 237/574 completed (loss: 0.19802136719226837, acc: 0.9090909361839294)
[2025-01-06 01:26:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:02][root][INFO] - Training Epoch: 5/10, step 238/574 completed (loss: 0.4209010601043701, acc: 0.8888888955116272)
[2025-01-06 01:26:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:02][root][INFO] - Training Epoch: 5/10, step 239/574 completed (loss: 0.12851358950138092, acc: 0.9428571462631226)
[2025-01-06 01:26:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:03][root][INFO] - Training Epoch: 5/10, step 240/574 completed (loss: 0.2802499532699585, acc: 0.9318181872367859)
[2025-01-06 01:26:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:03][root][INFO] - Training Epoch: 5/10, step 241/574 completed (loss: 0.11004795879125595, acc: 0.9772727489471436)
[2025-01-06 01:26:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:04][root][INFO] - Training Epoch: 5/10, step 242/574 completed (loss: 0.46786198019981384, acc: 0.8709677457809448)
[2025-01-06 01:26:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:04][root][INFO] - Training Epoch: 5/10, step 243/574 completed (loss: 0.2871505618095398, acc: 0.9318181872367859)
[2025-01-06 01:26:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:05][root][INFO] - Training Epoch: 5/10, step 244/574 completed (loss: 0.0003037721908185631, acc: 1.0)
[2025-01-06 01:26:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:05][root][INFO] - Training Epoch: 5/10, step 245/574 completed (loss: 0.10667803883552551, acc: 0.9615384340286255)
[2025-01-06 01:26:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:05][root][INFO] - Training Epoch: 5/10, step 246/574 completed (loss: 0.0012507331557571888, acc: 1.0)
[2025-01-06 01:26:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:06][root][INFO] - Training Epoch: 5/10, step 247/574 completed (loss: 0.028841596096754074, acc: 1.0)
[2025-01-06 01:26:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:06][root][INFO] - Training Epoch: 5/10, step 248/574 completed (loss: 0.08121296018362045, acc: 0.9729729890823364)
[2025-01-06 01:26:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:06][root][INFO] - Training Epoch: 5/10, step 249/574 completed (loss: 0.02353774383664131, acc: 1.0)
[2025-01-06 01:26:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:07][root][INFO] - Training Epoch: 5/10, step 250/574 completed (loss: 0.0028469280805438757, acc: 1.0)
[2025-01-06 01:26:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:07][root][INFO] - Training Epoch: 5/10, step 251/574 completed (loss: 0.15782222151756287, acc: 0.970588207244873)
[2025-01-06 01:26:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:07][root][INFO] - Training Epoch: 5/10, step 252/574 completed (loss: 0.004371787887066603, acc: 1.0)
[2025-01-06 01:26:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:08][root][INFO] - Training Epoch: 5/10, step 253/574 completed (loss: 0.05143548548221588, acc: 0.9599999785423279)
[2025-01-06 01:26:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:08][root][INFO] - Training Epoch: 5/10, step 254/574 completed (loss: 0.01112521905452013, acc: 1.0)
[2025-01-06 01:26:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:08][root][INFO] - Training Epoch: 5/10, step 255/574 completed (loss: 0.04430421069264412, acc: 0.9677419066429138)
[2025-01-06 01:26:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:09][root][INFO] - Training Epoch: 5/10, step 256/574 completed (loss: 0.1615438163280487, acc: 0.9473684430122375)
[2025-01-06 01:26:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:09][root][INFO] - Training Epoch: 5/10, step 257/574 completed (loss: 0.08468752354383469, acc: 0.9857142567634583)
[2025-01-06 01:26:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:09][root][INFO] - Training Epoch: 5/10, step 258/574 completed (loss: 0.12818659842014313, acc: 0.9605262875556946)
[2025-01-06 01:26:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:10][root][INFO] - Training Epoch: 5/10, step 259/574 completed (loss: 0.14081159234046936, acc: 0.9433962106704712)
[2025-01-06 01:26:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:11][root][INFO] - Training Epoch: 5/10, step 260/574 completed (loss: 0.11950688809156418, acc: 0.9666666388511658)
[2025-01-06 01:26:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:11][root][INFO] - Training Epoch: 5/10, step 261/574 completed (loss: 0.021977722644805908, acc: 1.0)
[2025-01-06 01:26:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:11][root][INFO] - Training Epoch: 5/10, step 262/574 completed (loss: 0.01852884329855442, acc: 1.0)
[2025-01-06 01:26:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:12][root][INFO] - Training Epoch: 5/10, step 263/574 completed (loss: 0.21894940733909607, acc: 0.9733333587646484)
[2025-01-06 01:26:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:12][root][INFO] - Training Epoch: 5/10, step 264/574 completed (loss: 0.08694851398468018, acc: 0.9583333134651184)
[2025-01-06 01:26:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:13][root][INFO] - Training Epoch: 5/10, step 265/574 completed (loss: 0.7172345519065857, acc: 0.7919999957084656)
[2025-01-06 01:26:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:13][root][INFO] - Training Epoch: 5/10, step 266/574 completed (loss: 0.4181017577648163, acc: 0.9101123809814453)
[2025-01-06 01:26:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:13][root][INFO] - Training Epoch: 5/10, step 267/574 completed (loss: 0.28304198384284973, acc: 0.9324324131011963)
[2025-01-06 01:26:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:14][root][INFO] - Training Epoch: 5/10, step 268/574 completed (loss: 0.07747028023004532, acc: 0.982758641242981)
[2025-01-06 01:26:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:14][root][INFO] - Training Epoch: 5/10, step 269/574 completed (loss: 0.008657287806272507, acc: 1.0)
[2025-01-06 01:26:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:15][root][INFO] - Training Epoch: 5/10, step 270/574 completed (loss: 0.01782812923192978, acc: 1.0)
[2025-01-06 01:26:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:15][root][INFO] - Training Epoch: 5/10, step 271/574 completed (loss: 0.10387726873159409, acc: 0.96875)
[2025-01-06 01:26:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:15][root][INFO] - Training Epoch: 5/10, step 272/574 completed (loss: 0.007469704374670982, acc: 1.0)
[2025-01-06 01:26:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:16][root][INFO] - Training Epoch: 5/10, step 273/574 completed (loss: 0.17929385602474213, acc: 0.949999988079071)
[2025-01-06 01:26:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:16][root][INFO] - Training Epoch: 5/10, step 274/574 completed (loss: 0.00902919564396143, acc: 1.0)
[2025-01-06 01:26:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:16][root][INFO] - Training Epoch: 5/10, step 275/574 completed (loss: 0.040432192385196686, acc: 1.0)
[2025-01-06 01:26:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:17][root][INFO] - Training Epoch: 5/10, step 276/574 completed (loss: 0.020371921360492706, acc: 1.0)
[2025-01-06 01:26:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:17][root][INFO] - Training Epoch: 5/10, step 277/574 completed (loss: 0.0027712166775017977, acc: 1.0)
[2025-01-06 01:26:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:48][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.1413, device='cuda:0') eval_epoch_loss=tensor(0.7614, device='cuda:0') eval_epoch_acc=tensor(0.8405, device='cuda:0')
[2025-01-06 01:26:48][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:26:48][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:26:48][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_5_step_278_loss_0.7614323496818542/model.pt
[2025-01-06 01:26:48][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:26:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:49][root][INFO] - Training Epoch: 5/10, step 278/574 completed (loss: 0.09446119517087936, acc: 0.978723406791687)
[2025-01-06 01:26:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:49][root][INFO] - Training Epoch: 5/10, step 279/574 completed (loss: 0.1000872477889061, acc: 0.9791666865348816)
[2025-01-06 01:26:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:49][root][INFO] - Training Epoch: 5/10, step 280/574 completed (loss: 0.02282707579433918, acc: 1.0)
[2025-01-06 01:26:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:50][root][INFO] - Training Epoch: 5/10, step 281/574 completed (loss: 0.18277877569198608, acc: 0.9397590160369873)
[2025-01-06 01:26:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:50][root][INFO] - Training Epoch: 5/10, step 282/574 completed (loss: 0.19619005918502808, acc: 0.9444444179534912)
[2025-01-06 01:26:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:51][root][INFO] - Training Epoch: 5/10, step 283/574 completed (loss: 0.0338355228304863, acc: 0.9736841917037964)
[2025-01-06 01:26:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:51][root][INFO] - Training Epoch: 5/10, step 284/574 completed (loss: 0.019937559962272644, acc: 1.0)
[2025-01-06 01:26:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:51][root][INFO] - Training Epoch: 5/10, step 285/574 completed (loss: 0.002314798766747117, acc: 1.0)
[2025-01-06 01:26:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:52][root][INFO] - Training Epoch: 5/10, step 286/574 completed (loss: 0.15163522958755493, acc: 0.9453125)
[2025-01-06 01:26:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:52][root][INFO] - Training Epoch: 5/10, step 287/574 completed (loss: 0.2741069197654724, acc: 0.9120000004768372)
[2025-01-06 01:26:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:52][root][INFO] - Training Epoch: 5/10, step 288/574 completed (loss: 0.05927163362503052, acc: 0.9780219793319702)
[2025-01-06 01:26:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:53][root][INFO] - Training Epoch: 5/10, step 289/574 completed (loss: 0.10194288939237595, acc: 0.9627329111099243)
[2025-01-06 01:26:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:53][root][INFO] - Training Epoch: 5/10, step 290/574 completed (loss: 0.2736797034740448, acc: 0.9175257682800293)
[2025-01-06 01:26:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:54][root][INFO] - Training Epoch: 5/10, step 291/574 completed (loss: 0.028385134413838387, acc: 1.0)
[2025-01-06 01:26:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:54][root][INFO] - Training Epoch: 5/10, step 292/574 completed (loss: 0.02146364375948906, acc: 1.0)
[2025-01-06 01:26:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:54][root][INFO] - Training Epoch: 5/10, step 293/574 completed (loss: 0.03666216507554054, acc: 0.982758641242981)
[2025-01-06 01:26:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:55][root][INFO] - Training Epoch: 5/10, step 294/574 completed (loss: 0.10942082107067108, acc: 0.9272727370262146)
[2025-01-06 01:26:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:55][root][INFO] - Training Epoch: 5/10, step 295/574 completed (loss: 0.26366832852363586, acc: 0.8969072103500366)
[2025-01-06 01:26:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:56][root][INFO] - Training Epoch: 5/10, step 296/574 completed (loss: 0.04374275356531143, acc: 0.982758641242981)
[2025-01-06 01:26:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:56][root][INFO] - Training Epoch: 5/10, step 297/574 completed (loss: 0.006078848149627447, acc: 1.0)
[2025-01-06 01:26:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:56][root][INFO] - Training Epoch: 5/10, step 298/574 completed (loss: 0.023590795695781708, acc: 1.0)
[2025-01-06 01:26:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:57][root][INFO] - Training Epoch: 5/10, step 299/574 completed (loss: 0.02168828621506691, acc: 0.9821428656578064)
[2025-01-06 01:26:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:57][root][INFO] - Training Epoch: 5/10, step 300/574 completed (loss: 0.0028441089671105146, acc: 1.0)
[2025-01-06 01:26:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:57][root][INFO] - Training Epoch: 5/10, step 301/574 completed (loss: 0.03537972643971443, acc: 1.0)
[2025-01-06 01:26:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:58][root][INFO] - Training Epoch: 5/10, step 302/574 completed (loss: 0.05564550682902336, acc: 0.9811320900917053)
[2025-01-06 01:26:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:58][root][INFO] - Training Epoch: 5/10, step 303/574 completed (loss: 0.0020473836921155453, acc: 1.0)
[2025-01-06 01:26:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:58][root][INFO] - Training Epoch: 5/10, step 304/574 completed (loss: 0.0013468454126268625, acc: 1.0)
[2025-01-06 01:26:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:59][root][INFO] - Training Epoch: 5/10, step 305/574 completed (loss: 0.08951439708471298, acc: 0.9508196711540222)
[2025-01-06 01:26:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:26:59][root][INFO] - Training Epoch: 5/10, step 306/574 completed (loss: 0.0700717493891716, acc: 0.9666666388511658)
[2025-01-06 01:26:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:00][root][INFO] - Training Epoch: 5/10, step 307/574 completed (loss: 0.00030118582071736455, acc: 1.0)
[2025-01-06 01:27:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:00][root][INFO] - Training Epoch: 5/10, step 308/574 completed (loss: 0.029163412749767303, acc: 1.0)
[2025-01-06 01:27:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:00][root][INFO] - Training Epoch: 5/10, step 309/574 completed (loss: 0.028318140655755997, acc: 1.0)
[2025-01-06 01:27:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:01][root][INFO] - Training Epoch: 5/10, step 310/574 completed (loss: 0.03489932790398598, acc: 0.9879518151283264)
[2025-01-06 01:27:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:01][root][INFO] - Training Epoch: 5/10, step 311/574 completed (loss: 0.05316273868083954, acc: 0.9743589758872986)
[2025-01-06 01:27:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:01][root][INFO] - Training Epoch: 5/10, step 312/574 completed (loss: 0.020654898136854172, acc: 1.0)
[2025-01-06 01:27:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:02][root][INFO] - Training Epoch: 5/10, step 313/574 completed (loss: 0.0013623674167320132, acc: 1.0)
[2025-01-06 01:27:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:02][root][INFO] - Training Epoch: 5/10, step 314/574 completed (loss: 0.0025221758987754583, acc: 1.0)
[2025-01-06 01:27:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:02][root][INFO] - Training Epoch: 5/10, step 315/574 completed (loss: 0.029099240899086, acc: 1.0)
[2025-01-06 01:27:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:03][root][INFO] - Training Epoch: 5/10, step 316/574 completed (loss: 0.12304261326789856, acc: 0.9677419066429138)
[2025-01-06 01:27:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:03][root][INFO] - Training Epoch: 5/10, step 317/574 completed (loss: 0.02577589824795723, acc: 0.9850746393203735)
[2025-01-06 01:27:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:03][root][INFO] - Training Epoch: 5/10, step 318/574 completed (loss: 0.01253843866288662, acc: 1.0)
[2025-01-06 01:27:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:04][root][INFO] - Training Epoch: 5/10, step 319/574 completed (loss: 0.006728836335241795, acc: 1.0)
[2025-01-06 01:27:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:04][root][INFO] - Training Epoch: 5/10, step 320/574 completed (loss: 0.00852762907743454, acc: 1.0)
[2025-01-06 01:27:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:04][root][INFO] - Training Epoch: 5/10, step 321/574 completed (loss: 0.005945785436779261, acc: 1.0)
[2025-01-06 01:27:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:05][root][INFO] - Training Epoch: 5/10, step 322/574 completed (loss: 0.2586607336997986, acc: 0.8888888955116272)
[2025-01-06 01:27:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:05][root][INFO] - Training Epoch: 5/10, step 323/574 completed (loss: 0.17567719519138336, acc: 0.9428571462631226)
[2025-01-06 01:27:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:06][root][INFO] - Training Epoch: 5/10, step 324/574 completed (loss: 0.16564202308654785, acc: 0.9743589758872986)
[2025-01-06 01:27:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:06][root][INFO] - Training Epoch: 5/10, step 325/574 completed (loss: 0.22119805216789246, acc: 0.9756097793579102)
[2025-01-06 01:27:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:06][root][INFO] - Training Epoch: 5/10, step 326/574 completed (loss: 0.25683867931365967, acc: 0.8947368264198303)
[2025-01-06 01:27:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:07][root][INFO] - Training Epoch: 5/10, step 327/574 completed (loss: 0.007524657994508743, acc: 1.0)
[2025-01-06 01:27:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:07][root][INFO] - Training Epoch: 5/10, step 328/574 completed (loss: 0.004108463879674673, acc: 1.0)
[2025-01-06 01:27:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:07][root][INFO] - Training Epoch: 5/10, step 329/574 completed (loss: 0.06992152333259583, acc: 0.9629629850387573)
[2025-01-06 01:27:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:08][root][INFO] - Training Epoch: 5/10, step 330/574 completed (loss: 0.006346498150378466, acc: 1.0)
[2025-01-06 01:27:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:08][root][INFO] - Training Epoch: 5/10, step 331/574 completed (loss: 0.10947614163160324, acc: 0.9677419066429138)
[2025-01-06 01:27:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:08][root][INFO] - Training Epoch: 5/10, step 332/574 completed (loss: 0.08551617711782455, acc: 0.9824561476707458)
[2025-01-06 01:27:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:09][root][INFO] - Training Epoch: 5/10, step 333/574 completed (loss: 0.0021991438698023558, acc: 1.0)
[2025-01-06 01:27:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:09][root][INFO] - Training Epoch: 5/10, step 334/574 completed (loss: 0.020380409434437752, acc: 1.0)
[2025-01-06 01:27:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:10][root][INFO] - Training Epoch: 5/10, step 335/574 completed (loss: 0.002545527881011367, acc: 1.0)
[2025-01-06 01:27:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:10][root][INFO] - Training Epoch: 5/10, step 336/574 completed (loss: 0.0440489687025547, acc: 1.0)
[2025-01-06 01:27:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:10][root][INFO] - Training Epoch: 5/10, step 337/574 completed (loss: 0.2829391062259674, acc: 0.9195402264595032)
[2025-01-06 01:27:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:11][root][INFO] - Training Epoch: 5/10, step 338/574 completed (loss: 0.4618622660636902, acc: 0.8297872543334961)
[2025-01-06 01:27:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:11][root][INFO] - Training Epoch: 5/10, step 339/574 completed (loss: 0.30443599820137024, acc: 0.891566276550293)
[2025-01-06 01:27:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:11][root][INFO] - Training Epoch: 5/10, step 340/574 completed (loss: 0.00040581266512162983, acc: 1.0)
[2025-01-06 01:27:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:12][root][INFO] - Training Epoch: 5/10, step 341/574 completed (loss: 0.007873283699154854, acc: 1.0)
[2025-01-06 01:27:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:12][root][INFO] - Training Epoch: 5/10, step 342/574 completed (loss: 0.09778737276792526, acc: 0.9518072009086609)
[2025-01-06 01:27:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:12][root][INFO] - Training Epoch: 5/10, step 343/574 completed (loss: 0.20232631266117096, acc: 0.9433962106704712)
[2025-01-06 01:27:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:13][root][INFO] - Training Epoch: 5/10, step 344/574 completed (loss: 0.06882838159799576, acc: 0.9746835231781006)
[2025-01-06 01:27:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:13][root][INFO] - Training Epoch: 5/10, step 345/574 completed (loss: 0.014903794042766094, acc: 1.0)
[2025-01-06 01:27:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:13][root][INFO] - Training Epoch: 5/10, step 346/574 completed (loss: 0.024662338197231293, acc: 1.0)
[2025-01-06 01:27:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:14][root][INFO] - Training Epoch: 5/10, step 347/574 completed (loss: 0.1598929464817047, acc: 0.949999988079071)
[2025-01-06 01:27:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:14][root][INFO] - Training Epoch: 5/10, step 348/574 completed (loss: 0.0062452200800180435, acc: 1.0)
[2025-01-06 01:27:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:14][root][INFO] - Training Epoch: 5/10, step 349/574 completed (loss: 0.15310314297676086, acc: 0.9722222089767456)
[2025-01-06 01:27:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:15][root][INFO] - Training Epoch: 5/10, step 350/574 completed (loss: 0.09164083749055862, acc: 0.9767441749572754)
[2025-01-06 01:27:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:15][root][INFO] - Training Epoch: 5/10, step 351/574 completed (loss: 0.004340869374573231, acc: 1.0)
[2025-01-06 01:27:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:16][root][INFO] - Training Epoch: 5/10, step 352/574 completed (loss: 0.12306564301252365, acc: 0.9555555582046509)
[2025-01-06 01:27:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:16][root][INFO] - Training Epoch: 5/10, step 353/574 completed (loss: 0.1826259344816208, acc: 0.95652174949646)
[2025-01-06 01:27:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:16][root][INFO] - Training Epoch: 5/10, step 354/574 completed (loss: 0.011533782817423344, acc: 1.0)
[2025-01-06 01:27:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:17][root][INFO] - Training Epoch: 5/10, step 355/574 completed (loss: 0.30068182945251465, acc: 0.901098906993866)
[2025-01-06 01:27:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:17][root][INFO] - Training Epoch: 5/10, step 356/574 completed (loss: 0.2712782025337219, acc: 0.9304347634315491)
[2025-01-06 01:27:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:17][root][INFO] - Training Epoch: 5/10, step 357/574 completed (loss: 0.17081762850284576, acc: 0.9130434989929199)
[2025-01-06 01:27:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:18][root][INFO] - Training Epoch: 5/10, step 358/574 completed (loss: 0.22060920298099518, acc: 0.9591836929321289)
[2025-01-06 01:27:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:18][root][INFO] - Training Epoch: 5/10, step 359/574 completed (loss: 0.0013140140799805522, acc: 1.0)
[2025-01-06 01:27:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:19][root][INFO] - Training Epoch: 5/10, step 360/574 completed (loss: 0.055173177272081375, acc: 0.9615384340286255)
[2025-01-06 01:27:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:19][root][INFO] - Training Epoch: 5/10, step 361/574 completed (loss: 0.1326172947883606, acc: 0.9756097793579102)
[2025-01-06 01:27:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:19][root][INFO] - Training Epoch: 5/10, step 362/574 completed (loss: 0.019437653943896294, acc: 1.0)
[2025-01-06 01:27:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:19][root][INFO] - Training Epoch: 5/10, step 363/574 completed (loss: 0.006996853742748499, acc: 1.0)
[2025-01-06 01:27:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:20][root][INFO] - Training Epoch: 5/10, step 364/574 completed (loss: 0.10821381211280823, acc: 0.9512194991111755)
[2025-01-06 01:27:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:20][root][INFO] - Training Epoch: 5/10, step 365/574 completed (loss: 0.009212777018547058, acc: 1.0)
[2025-01-06 01:27:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:20][root][INFO] - Training Epoch: 5/10, step 366/574 completed (loss: 0.00030203265487216413, acc: 1.0)
[2025-01-06 01:27:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:21][root][INFO] - Training Epoch: 5/10, step 367/574 completed (loss: 0.0007721219444647431, acc: 1.0)
[2025-01-06 01:27:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:21][root][INFO] - Training Epoch: 5/10, step 368/574 completed (loss: 0.05201243981719017, acc: 0.9642857313156128)
[2025-01-06 01:27:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:21][root][INFO] - Training Epoch: 5/10, step 369/574 completed (loss: 0.5281215906143188, acc: 0.90625)
[2025-01-06 01:27:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:22][root][INFO] - Training Epoch: 5/10, step 370/574 completed (loss: 0.2180103361606598, acc: 0.9151515364646912)
[2025-01-06 01:27:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:23][root][INFO] - Training Epoch: 5/10, step 371/574 completed (loss: 0.09272973984479904, acc: 0.9716981053352356)
[2025-01-06 01:27:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:23][root][INFO] - Training Epoch: 5/10, step 372/574 completed (loss: 0.0451531782746315, acc: 1.0)
[2025-01-06 01:27:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:24][root][INFO] - Training Epoch: 5/10, step 373/574 completed (loss: 0.06956909596920013, acc: 0.9642857313156128)
[2025-01-06 01:27:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:24][root][INFO] - Training Epoch: 5/10, step 374/574 completed (loss: 0.07852821797132492, acc: 0.9714285731315613)
[2025-01-06 01:27:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:24][root][INFO] - Training Epoch: 5/10, step 375/574 completed (loss: 0.01037752628326416, acc: 1.0)
[2025-01-06 01:27:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:25][root][INFO] - Training Epoch: 5/10, step 376/574 completed (loss: 0.027696343138813972, acc: 1.0)
[2025-01-06 01:27:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:25][root][INFO] - Training Epoch: 5/10, step 377/574 completed (loss: 0.007215121295303106, acc: 1.0)
[2025-01-06 01:27:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:25][root][INFO] - Training Epoch: 5/10, step 378/574 completed (loss: 0.004399087745696306, acc: 1.0)
[2025-01-06 01:27:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:26][root][INFO] - Training Epoch: 5/10, step 379/574 completed (loss: 0.11784379929304123, acc: 0.9580838084220886)
[2025-01-06 01:27:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:26][root][INFO] - Training Epoch: 5/10, step 380/574 completed (loss: 0.23974987864494324, acc: 0.932330846786499)
[2025-01-06 01:27:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:27][root][INFO] - Training Epoch: 5/10, step 381/574 completed (loss: 0.3169262111186981, acc: 0.903743326663971)
[2025-01-06 01:27:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:28][root][INFO] - Training Epoch: 5/10, step 382/574 completed (loss: 0.02841075137257576, acc: 0.9909909963607788)
[2025-01-06 01:27:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:28][root][INFO] - Training Epoch: 5/10, step 383/574 completed (loss: 0.05896434932947159, acc: 0.9642857313156128)
[2025-01-06 01:27:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:29][root][INFO] - Training Epoch: 5/10, step 384/574 completed (loss: 0.004937656223773956, acc: 1.0)
[2025-01-06 01:27:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:29][root][INFO] - Training Epoch: 5/10, step 385/574 completed (loss: 0.003989015705883503, acc: 1.0)
[2025-01-06 01:27:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:29][root][INFO] - Training Epoch: 5/10, step 386/574 completed (loss: 0.002729849424213171, acc: 1.0)
[2025-01-06 01:27:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:30][root][INFO] - Training Epoch: 5/10, step 387/574 completed (loss: 0.0024785855785012245, acc: 1.0)
[2025-01-06 01:27:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:30][root][INFO] - Training Epoch: 5/10, step 388/574 completed (loss: 0.024568496271967888, acc: 1.0)
[2025-01-06 01:27:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:30][root][INFO] - Training Epoch: 5/10, step 389/574 completed (loss: 0.1497591882944107, acc: 0.949999988079071)
[2025-01-06 01:27:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:31][root][INFO] - Training Epoch: 5/10, step 390/574 completed (loss: 0.03892127051949501, acc: 1.0)
[2025-01-06 01:27:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:31][root][INFO] - Training Epoch: 5/10, step 391/574 completed (loss: 0.19336281716823578, acc: 0.9444444179534912)
[2025-01-06 01:27:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:31][root][INFO] - Training Epoch: 5/10, step 392/574 completed (loss: 0.3023039698600769, acc: 0.9029126167297363)
[2025-01-06 01:27:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:32][root][INFO] - Training Epoch: 5/10, step 393/574 completed (loss: 0.6074501276016235, acc: 0.8235294222831726)
[2025-01-06 01:27:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:32][root][INFO] - Training Epoch: 5/10, step 394/574 completed (loss: 0.305763304233551, acc: 0.8799999952316284)
[2025-01-06 01:27:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:33][root][INFO] - Training Epoch: 5/10, step 395/574 completed (loss: 0.30826830863952637, acc: 0.9027777910232544)
[2025-01-06 01:27:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:33][root][INFO] - Training Epoch: 5/10, step 396/574 completed (loss: 0.10212334990501404, acc: 0.9767441749572754)
[2025-01-06 01:27:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:33][root][INFO] - Training Epoch: 5/10, step 397/574 completed (loss: 0.005621037911623716, acc: 1.0)
[2025-01-06 01:27:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:34][root][INFO] - Training Epoch: 5/10, step 398/574 completed (loss: 0.07207099348306656, acc: 0.9767441749572754)
[2025-01-06 01:27:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:34][root][INFO] - Training Epoch: 5/10, step 399/574 completed (loss: 0.021836290135979652, acc: 1.0)
[2025-01-06 01:27:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:35][root][INFO] - Training Epoch: 5/10, step 400/574 completed (loss: 0.12026084214448929, acc: 0.9558823704719543)
[2025-01-06 01:27:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:35][root][INFO] - Training Epoch: 5/10, step 401/574 completed (loss: 0.09340032190084457, acc: 0.9599999785423279)
[2025-01-06 01:27:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:35][root][INFO] - Training Epoch: 5/10, step 402/574 completed (loss: 0.00695204408839345, acc: 1.0)
[2025-01-06 01:27:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:36][root][INFO] - Training Epoch: 5/10, step 403/574 completed (loss: 0.03928856924176216, acc: 1.0)
[2025-01-06 01:27:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:36][root][INFO] - Training Epoch: 5/10, step 404/574 completed (loss: 0.007080798037350178, acc: 1.0)
[2025-01-06 01:27:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:36][root][INFO] - Training Epoch: 5/10, step 405/574 completed (loss: 0.0016479798359796405, acc: 1.0)
[2025-01-06 01:27:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:37][root][INFO] - Training Epoch: 5/10, step 406/574 completed (loss: 0.010044828988611698, acc: 1.0)
[2025-01-06 01:27:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:37][root][INFO] - Training Epoch: 5/10, step 407/574 completed (loss: 0.003189267124980688, acc: 1.0)
[2025-01-06 01:27:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:37][root][INFO] - Training Epoch: 5/10, step 408/574 completed (loss: 0.0371885821223259, acc: 0.9629629850387573)
[2025-01-06 01:27:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:38][root][INFO] - Training Epoch: 5/10, step 409/574 completed (loss: 0.004197196569293737, acc: 1.0)
[2025-01-06 01:27:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:38][root][INFO] - Training Epoch: 5/10, step 410/574 completed (loss: 0.015606967732310295, acc: 1.0)
[2025-01-06 01:27:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:38][root][INFO] - Training Epoch: 5/10, step 411/574 completed (loss: 0.008727753534913063, acc: 1.0)
[2025-01-06 01:27:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:39][root][INFO] - Training Epoch: 5/10, step 412/574 completed (loss: 0.011051137931644917, acc: 1.0)
[2025-01-06 01:27:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:39][root][INFO] - Training Epoch: 5/10, step 413/574 completed (loss: 0.05002026632428169, acc: 0.9696969985961914)
[2025-01-06 01:27:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:40][root][INFO] - Training Epoch: 5/10, step 414/574 completed (loss: 0.004536377731710672, acc: 1.0)
[2025-01-06 01:27:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:40][root][INFO] - Training Epoch: 5/10, step 415/574 completed (loss: 0.08348539471626282, acc: 0.9607843160629272)
[2025-01-06 01:27:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:40][root][INFO] - Training Epoch: 5/10, step 416/574 completed (loss: 0.009596670046448708, acc: 1.0)
[2025-01-06 01:27:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:41][root][INFO] - Training Epoch: 5/10, step 417/574 completed (loss: 0.01742870733141899, acc: 1.0)
[2025-01-06 01:27:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:41][root][INFO] - Training Epoch: 5/10, step 418/574 completed (loss: 0.04620359092950821, acc: 0.9750000238418579)
[2025-01-06 01:27:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:41][root][INFO] - Training Epoch: 5/10, step 419/574 completed (loss: 0.011067907325923443, acc: 1.0)
[2025-01-06 01:27:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:42][root][INFO] - Training Epoch: 5/10, step 420/574 completed (loss: 0.004527214914560318, acc: 1.0)
[2025-01-06 01:27:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:27:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:13][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.1171, device='cuda:0') eval_epoch_loss=tensor(0.7500, device='cuda:0') eval_epoch_acc=tensor(0.8393, device='cuda:0')
[2025-01-06 01:28:13][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:28:13][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:28:13][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_5_step_421_loss_0.7500268816947937/model.pt
[2025-01-06 01:28:13][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:28:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:13][root][INFO] - Training Epoch: 5/10, step 421/574 completed (loss: 0.10452965646982193, acc: 0.9666666388511658)
[2025-01-06 01:28:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:14][root][INFO] - Training Epoch: 5/10, step 422/574 completed (loss: 0.018454141914844513, acc: 1.0)
[2025-01-06 01:28:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:14][root][INFO] - Training Epoch: 5/10, step 423/574 completed (loss: 0.03676685690879822, acc: 1.0)
[2025-01-06 01:28:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:14][root][INFO] - Training Epoch: 5/10, step 424/574 completed (loss: 0.24243271350860596, acc: 0.9629629850387573)
[2025-01-06 01:28:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:15][root][INFO] - Training Epoch: 5/10, step 425/574 completed (loss: 0.014624170958995819, acc: 1.0)
[2025-01-06 01:28:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:15][root][INFO] - Training Epoch: 5/10, step 426/574 completed (loss: 0.017646925523877144, acc: 1.0)
[2025-01-06 01:28:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:15][root][INFO] - Training Epoch: 5/10, step 427/574 completed (loss: 0.06657545268535614, acc: 0.9729729890823364)
[2025-01-06 01:28:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:16][root][INFO] - Training Epoch: 5/10, step 428/574 completed (loss: 0.0016812423709779978, acc: 1.0)
[2025-01-06 01:28:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:16][root][INFO] - Training Epoch: 5/10, step 429/574 completed (loss: 0.0011645958293229342, acc: 1.0)
[2025-01-06 01:28:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:16][root][INFO] - Training Epoch: 5/10, step 430/574 completed (loss: 0.00035198446130380034, acc: 1.0)
[2025-01-06 01:28:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:17][root][INFO] - Training Epoch: 5/10, step 431/574 completed (loss: 0.012645240873098373, acc: 1.0)
[2025-01-06 01:28:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:17][root][INFO] - Training Epoch: 5/10, step 432/574 completed (loss: 0.0059364126063883305, acc: 1.0)
[2025-01-06 01:28:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:17][root][INFO] - Training Epoch: 5/10, step 433/574 completed (loss: 0.040030986070632935, acc: 0.9722222089767456)
[2025-01-06 01:28:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:18][root][INFO] - Training Epoch: 5/10, step 434/574 completed (loss: 0.010070081800222397, acc: 1.0)
[2025-01-06 01:28:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:18][root][INFO] - Training Epoch: 5/10, step 435/574 completed (loss: 0.004772393964231014, acc: 1.0)
[2025-01-06 01:28:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:18][root][INFO] - Training Epoch: 5/10, step 436/574 completed (loss: 0.20980127155780792, acc: 0.9722222089767456)
[2025-01-06 01:28:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:19][root][INFO] - Training Epoch: 5/10, step 437/574 completed (loss: 0.1260678470134735, acc: 0.9772727489471436)
[2025-01-06 01:28:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:19][root][INFO] - Training Epoch: 5/10, step 438/574 completed (loss: 0.2717028558254242, acc: 0.9523809552192688)
[2025-01-06 01:28:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:19][root][INFO] - Training Epoch: 5/10, step 439/574 completed (loss: 0.01512401644140482, acc: 1.0)
[2025-01-06 01:28:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:20][root][INFO] - Training Epoch: 5/10, step 440/574 completed (loss: 0.07508666813373566, acc: 0.9696969985961914)
[2025-01-06 01:28:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:21][root][INFO] - Training Epoch: 5/10, step 441/574 completed (loss: 0.44525375962257385, acc: 0.8399999737739563)
[2025-01-06 01:28:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:21][root][INFO] - Training Epoch: 5/10, step 442/574 completed (loss: 0.27424052357673645, acc: 0.9274193644523621)
[2025-01-06 01:28:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:22][root][INFO] - Training Epoch: 5/10, step 443/574 completed (loss: 0.2262672632932663, acc: 0.9402984976768494)
[2025-01-06 01:28:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:22][root][INFO] - Training Epoch: 5/10, step 444/574 completed (loss: 0.027545830234885216, acc: 1.0)
[2025-01-06 01:28:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:22][root][INFO] - Training Epoch: 5/10, step 445/574 completed (loss: 0.04043794050812721, acc: 0.9772727489471436)
[2025-01-06 01:28:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:23][root][INFO] - Training Epoch: 5/10, step 446/574 completed (loss: 0.01881680265069008, acc: 1.0)
[2025-01-06 01:28:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:23][root][INFO] - Training Epoch: 5/10, step 447/574 completed (loss: 0.03372962772846222, acc: 1.0)
[2025-01-06 01:28:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:24][root][INFO] - Training Epoch: 5/10, step 448/574 completed (loss: 0.00234905444085598, acc: 1.0)
[2025-01-06 01:28:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:24][root][INFO] - Training Epoch: 5/10, step 449/574 completed (loss: 0.052459243685007095, acc: 0.9850746393203735)
[2025-01-06 01:28:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:24][root][INFO] - Training Epoch: 5/10, step 450/574 completed (loss: 0.019190283492207527, acc: 0.9861111044883728)
[2025-01-06 01:28:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:25][root][INFO] - Training Epoch: 5/10, step 451/574 completed (loss: 0.02142396941781044, acc: 0.989130437374115)
[2025-01-06 01:28:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:25][root][INFO] - Training Epoch: 5/10, step 452/574 completed (loss: 0.029013773426413536, acc: 0.9871794581413269)
[2025-01-06 01:28:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:25][root][INFO] - Training Epoch: 5/10, step 453/574 completed (loss: 0.21008527278900146, acc: 0.9473684430122375)
[2025-01-06 01:28:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:26][root][INFO] - Training Epoch: 5/10, step 454/574 completed (loss: 0.1563054621219635, acc: 0.9795918464660645)
[2025-01-06 01:28:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:26][root][INFO] - Training Epoch: 5/10, step 455/574 completed (loss: 0.06612078845500946, acc: 0.9696969985961914)
[2025-01-06 01:28:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:27][root][INFO] - Training Epoch: 5/10, step 456/574 completed (loss: 0.18034416437149048, acc: 0.9484536051750183)
[2025-01-06 01:28:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:27][root][INFO] - Training Epoch: 5/10, step 457/574 completed (loss: 0.0019379350123926997, acc: 1.0)
[2025-01-06 01:28:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:27][root][INFO] - Training Epoch: 5/10, step 458/574 completed (loss: 0.08632851392030716, acc: 0.9593023061752319)
[2025-01-06 01:28:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:28][root][INFO] - Training Epoch: 5/10, step 459/574 completed (loss: 0.0069190082140266895, acc: 1.0)
[2025-01-06 01:28:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:28][root][INFO] - Training Epoch: 5/10, step 460/574 completed (loss: 0.10769517719745636, acc: 0.9753086566925049)
[2025-01-06 01:28:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:28][root][INFO] - Training Epoch: 5/10, step 461/574 completed (loss: 0.0522071048617363, acc: 0.9722222089767456)
[2025-01-06 01:28:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:29][root][INFO] - Training Epoch: 5/10, step 462/574 completed (loss: 0.00971379317343235, acc: 1.0)
[2025-01-06 01:28:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:29][root][INFO] - Training Epoch: 5/10, step 463/574 completed (loss: 0.004807473160326481, acc: 1.0)
[2025-01-06 01:28:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:29][root][INFO] - Training Epoch: 5/10, step 464/574 completed (loss: 0.033539820462465286, acc: 1.0)
[2025-01-06 01:28:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:30][root][INFO] - Training Epoch: 5/10, step 465/574 completed (loss: 0.023321649059653282, acc: 1.0)
[2025-01-06 01:28:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:30][root][INFO] - Training Epoch: 5/10, step 466/574 completed (loss: 0.4438745379447937, acc: 0.8554216623306274)
[2025-01-06 01:28:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:30][root][INFO] - Training Epoch: 5/10, step 467/574 completed (loss: 0.047820378094911575, acc: 0.9819819927215576)
[2025-01-06 01:28:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:31][root][INFO] - Training Epoch: 5/10, step 468/574 completed (loss: 0.22840796411037445, acc: 0.9514563083648682)
[2025-01-06 01:28:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:31][root][INFO] - Training Epoch: 5/10, step 469/574 completed (loss: 0.12421952188014984, acc: 0.9674796462059021)
[2025-01-06 01:28:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:31][root][INFO] - Training Epoch: 5/10, step 470/574 completed (loss: 0.04841454699635506, acc: 1.0)
[2025-01-06 01:28:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:32][root][INFO] - Training Epoch: 5/10, step 471/574 completed (loss: 0.06086162105202675, acc: 0.9642857313156128)
[2025-01-06 01:28:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:32][root][INFO] - Training Epoch: 5/10, step 472/574 completed (loss: 0.2937150299549103, acc: 0.9313725233078003)
[2025-01-06 01:28:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:32][root][INFO] - Training Epoch: 5/10, step 473/574 completed (loss: 0.3485242426395416, acc: 0.8864628672599792)
[2025-01-06 01:28:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:33][root][INFO] - Training Epoch: 5/10, step 474/574 completed (loss: 0.11789926141500473, acc: 0.9583333134651184)
[2025-01-06 01:28:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:33][root][INFO] - Training Epoch: 5/10, step 475/574 completed (loss: 0.11121195554733276, acc: 0.9570552110671997)
[2025-01-06 01:28:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:33][root][INFO] - Training Epoch: 5/10, step 476/574 completed (loss: 0.12485118210315704, acc: 0.9640287756919861)
[2025-01-06 01:28:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:34][root][INFO] - Training Epoch: 5/10, step 477/574 completed (loss: 0.27059218287467957, acc: 0.9045225977897644)
[2025-01-06 01:28:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:34][root][INFO] - Training Epoch: 5/10, step 478/574 completed (loss: 0.09250347316265106, acc: 0.9722222089767456)
[2025-01-06 01:28:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:34][root][INFO] - Training Epoch: 5/10, step 479/574 completed (loss: 0.420729398727417, acc: 0.9090909361839294)
[2025-01-06 01:28:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:35][root][INFO] - Training Epoch: 5/10, step 480/574 completed (loss: 0.07489307969808578, acc: 0.9629629850387573)
[2025-01-06 01:28:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:35][root][INFO] - Training Epoch: 5/10, step 481/574 completed (loss: 0.4295447766780853, acc: 0.949999988079071)
[2025-01-06 01:28:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:35][root][INFO] - Training Epoch: 5/10, step 482/574 completed (loss: 0.07226525992155075, acc: 0.949999988079071)
[2025-01-06 01:28:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:36][root][INFO] - Training Epoch: 5/10, step 483/574 completed (loss: 0.09842870384454727, acc: 0.982758641242981)
[2025-01-06 01:28:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:36][root][INFO] - Training Epoch: 5/10, step 484/574 completed (loss: 0.0071039083413779736, acc: 1.0)
[2025-01-06 01:28:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:37][root][INFO] - Training Epoch: 5/10, step 485/574 completed (loss: 0.19251787662506104, acc: 0.9473684430122375)
[2025-01-06 01:28:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:37][root][INFO] - Training Epoch: 5/10, step 486/574 completed (loss: 0.24987651407718658, acc: 0.8888888955116272)
[2025-01-06 01:28:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:37][root][INFO] - Training Epoch: 5/10, step 487/574 completed (loss: 0.2659749686717987, acc: 0.9047619104385376)
[2025-01-06 01:28:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:38][root][INFO] - Training Epoch: 5/10, step 488/574 completed (loss: 0.0461856871843338, acc: 1.0)
[2025-01-06 01:28:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:38][root][INFO] - Training Epoch: 5/10, step 489/574 completed (loss: 0.18177473545074463, acc: 0.9384615421295166)
[2025-01-06 01:28:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:38][root][INFO] - Training Epoch: 5/10, step 490/574 completed (loss: 0.0033078519627451897, acc: 1.0)
[2025-01-06 01:28:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:39][root][INFO] - Training Epoch: 5/10, step 491/574 completed (loss: 0.25311359763145447, acc: 0.9655172228813171)
[2025-01-06 01:28:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:39][root][INFO] - Training Epoch: 5/10, step 492/574 completed (loss: 0.14560841023921967, acc: 0.9607843160629272)
[2025-01-06 01:28:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:39][root][INFO] - Training Epoch: 5/10, step 493/574 completed (loss: 0.013998077251017094, acc: 1.0)
[2025-01-06 01:28:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:40][root][INFO] - Training Epoch: 5/10, step 494/574 completed (loss: 0.4144653379917145, acc: 0.8947368264198303)
[2025-01-06 01:28:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:40][root][INFO] - Training Epoch: 5/10, step 495/574 completed (loss: 0.06976091116666794, acc: 0.9473684430122375)
[2025-01-06 01:28:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:40][root][INFO] - Training Epoch: 5/10, step 496/574 completed (loss: 0.15440905094146729, acc: 0.9553571343421936)
[2025-01-06 01:28:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:41][root][INFO] - Training Epoch: 5/10, step 497/574 completed (loss: 0.1851135641336441, acc: 0.9550561904907227)
[2025-01-06 01:28:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:41][root][INFO] - Training Epoch: 5/10, step 498/574 completed (loss: 0.35232192277908325, acc: 0.898876428604126)
[2025-01-06 01:28:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:41][root][INFO] - Training Epoch: 5/10, step 499/574 completed (loss: 0.47656288743019104, acc: 0.8510638475418091)
[2025-01-06 01:28:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:42][root][INFO] - Training Epoch: 5/10, step 500/574 completed (loss: 0.169070765376091, acc: 0.945652186870575)
[2025-01-06 01:28:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:42][root][INFO] - Training Epoch: 5/10, step 501/574 completed (loss: 0.025912852957844734, acc: 1.0)
[2025-01-06 01:28:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:42][root][INFO] - Training Epoch: 5/10, step 502/574 completed (loss: 0.0398080088198185, acc: 0.9615384340286255)
[2025-01-06 01:28:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:43][root][INFO] - Training Epoch: 5/10, step 503/574 completed (loss: 0.08954206109046936, acc: 0.9629629850387573)
[2025-01-06 01:28:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:43][root][INFO] - Training Epoch: 5/10, step 504/574 completed (loss: 0.024503683671355247, acc: 1.0)
[2025-01-06 01:28:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:43][root][INFO] - Training Epoch: 5/10, step 505/574 completed (loss: 0.13710598647594452, acc: 0.9622641801834106)
[2025-01-06 01:28:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:44][root][INFO] - Training Epoch: 5/10, step 506/574 completed (loss: 0.3394685685634613, acc: 0.931034505367279)
[2025-01-06 01:28:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:44][root][INFO] - Training Epoch: 5/10, step 507/574 completed (loss: 0.41750025749206543, acc: 0.8918918967247009)
[2025-01-06 01:28:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:45][root][INFO] - Training Epoch: 5/10, step 508/574 completed (loss: 0.216444730758667, acc: 0.9718309640884399)
[2025-01-06 01:28:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:45][root][INFO] - Training Epoch: 5/10, step 509/574 completed (loss: 0.3286808431148529, acc: 0.8999999761581421)
[2025-01-06 01:28:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:45][root][INFO] - Training Epoch: 5/10, step 510/574 completed (loss: 0.13996492326259613, acc: 0.8999999761581421)
[2025-01-06 01:28:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:46][root][INFO] - Training Epoch: 5/10, step 511/574 completed (loss: 0.28194722533226013, acc: 0.8846153616905212)
[2025-01-06 01:28:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:48][root][INFO] - Training Epoch: 5/10, step 512/574 completed (loss: 0.43594223260879517, acc: 0.8714285492897034)
[2025-01-06 01:28:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:49][root][INFO] - Training Epoch: 5/10, step 513/574 completed (loss: 0.08139365911483765, acc: 0.9920634627342224)
[2025-01-06 01:28:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:49][root][INFO] - Training Epoch: 5/10, step 514/574 completed (loss: 0.2305796593427658, acc: 0.9285714030265808)
[2025-01-06 01:28:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:50][root][INFO] - Training Epoch: 5/10, step 515/574 completed (loss: 0.011579722166061401, acc: 1.0)
[2025-01-06 01:28:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:50][root][INFO] - Training Epoch: 5/10, step 516/574 completed (loss: 0.09883275628089905, acc: 0.9444444179534912)
[2025-01-06 01:28:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:51][root][INFO] - Training Epoch: 5/10, step 517/574 completed (loss: 0.001978945219889283, acc: 1.0)
[2025-01-06 01:28:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:51][root][INFO] - Training Epoch: 5/10, step 518/574 completed (loss: 0.008585695177316666, acc: 1.0)
[2025-01-06 01:28:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:52][root][INFO] - Training Epoch: 5/10, step 519/574 completed (loss: 0.02068377658724785, acc: 1.0)
[2025-01-06 01:28:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:52][root][INFO] - Training Epoch: 5/10, step 520/574 completed (loss: 0.04240727052092552, acc: 1.0)
[2025-01-06 01:28:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:53][root][INFO] - Training Epoch: 5/10, step 521/574 completed (loss: 0.4430408477783203, acc: 0.8813559412956238)
[2025-01-06 01:28:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:53][root][INFO] - Training Epoch: 5/10, step 522/574 completed (loss: 0.05589281767606735, acc: 0.9925373196601868)
[2025-01-06 01:28:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:54][root][INFO] - Training Epoch: 5/10, step 523/574 completed (loss: 0.11298269033432007, acc: 0.970802903175354)
[2025-01-06 01:28:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:54][root][INFO] - Training Epoch: 5/10, step 524/574 completed (loss: 0.3337634205818176, acc: 0.8799999952316284)
[2025-01-06 01:28:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:55][root][INFO] - Training Epoch: 5/10, step 525/574 completed (loss: 0.004224235191941261, acc: 1.0)
[2025-01-06 01:28:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:55][root][INFO] - Training Epoch: 5/10, step 526/574 completed (loss: 0.0761677473783493, acc: 0.9807692170143127)
[2025-01-06 01:28:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:55][root][INFO] - Training Epoch: 5/10, step 527/574 completed (loss: 0.034623391926288605, acc: 1.0)
[2025-01-06 01:28:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:56][root][INFO] - Training Epoch: 5/10, step 528/574 completed (loss: 0.24272117018699646, acc: 0.9344262480735779)
[2025-01-06 01:28:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:56][root][INFO] - Training Epoch: 5/10, step 529/574 completed (loss: 0.05331287905573845, acc: 1.0)
[2025-01-06 01:28:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:56][root][INFO] - Training Epoch: 5/10, step 530/574 completed (loss: 0.1711321920156479, acc: 0.930232584476471)
[2025-01-06 01:28:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:57][root][INFO] - Training Epoch: 5/10, step 531/574 completed (loss: 0.5134637355804443, acc: 0.8636363744735718)
[2025-01-06 01:28:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:57][root][INFO] - Training Epoch: 5/10, step 532/574 completed (loss: 0.2144843190908432, acc: 0.8867924809455872)
[2025-01-06 01:28:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:57][root][INFO] - Training Epoch: 5/10, step 533/574 completed (loss: 0.12504634261131287, acc: 0.9318181872367859)
[2025-01-06 01:28:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:58][root][INFO] - Training Epoch: 5/10, step 534/574 completed (loss: 0.023393232375383377, acc: 1.0)
[2025-01-06 01:28:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:58][root][INFO] - Training Epoch: 5/10, step 535/574 completed (loss: 0.05179436877369881, acc: 1.0)
[2025-01-06 01:28:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:58][root][INFO] - Training Epoch: 5/10, step 536/574 completed (loss: 0.034872088581323624, acc: 1.0)
[2025-01-06 01:28:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:59][root][INFO] - Training Epoch: 5/10, step 537/574 completed (loss: 0.1572466641664505, acc: 0.9230769276618958)
[2025-01-06 01:28:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:28:59][root][INFO] - Training Epoch: 5/10, step 538/574 completed (loss: 0.2063066065311432, acc: 0.953125)
[2025-01-06 01:28:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:00][root][INFO] - Training Epoch: 5/10, step 539/574 completed (loss: 0.1663832813501358, acc: 0.90625)
[2025-01-06 01:29:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:00][root][INFO] - Training Epoch: 5/10, step 540/574 completed (loss: 0.037075985223054886, acc: 1.0)
[2025-01-06 01:29:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:00][root][INFO] - Training Epoch: 5/10, step 541/574 completed (loss: 0.02561674267053604, acc: 1.0)
[2025-01-06 01:29:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:01][root][INFO] - Training Epoch: 5/10, step 542/574 completed (loss: 0.002386718988418579, acc: 1.0)
[2025-01-06 01:29:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:01][root][INFO] - Training Epoch: 5/10, step 543/574 completed (loss: 0.043421436101198196, acc: 0.95652174949646)
[2025-01-06 01:29:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:01][root][INFO] - Training Epoch: 5/10, step 544/574 completed (loss: 0.0202823244035244, acc: 1.0)
[2025-01-06 01:29:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:02][root][INFO] - Training Epoch: 5/10, step 545/574 completed (loss: 0.031188564375042915, acc: 1.0)
[2025-01-06 01:29:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:02][root][INFO] - Training Epoch: 5/10, step 546/574 completed (loss: 0.011009562760591507, acc: 1.0)
[2025-01-06 01:29:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:02][root][INFO] - Training Epoch: 5/10, step 547/574 completed (loss: 0.0007066137040965259, acc: 1.0)
[2025-01-06 01:29:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:03][root][INFO] - Training Epoch: 5/10, step 548/574 completed (loss: 0.05549639090895653, acc: 0.9677419066429138)
[2025-01-06 01:29:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:03][root][INFO] - Training Epoch: 5/10, step 549/574 completed (loss: 0.009497375227510929, acc: 1.0)
[2025-01-06 01:29:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:03][root][INFO] - Training Epoch: 5/10, step 550/574 completed (loss: 0.23069506883621216, acc: 0.8787878751754761)
[2025-01-06 01:29:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:04][root][INFO] - Training Epoch: 5/10, step 551/574 completed (loss: 0.12355102598667145, acc: 0.949999988079071)
[2025-01-06 01:29:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:04][root][INFO] - Training Epoch: 5/10, step 552/574 completed (loss: 0.02707132138311863, acc: 0.9857142567634583)
[2025-01-06 01:29:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:05][root][INFO] - Training Epoch: 5/10, step 553/574 completed (loss: 0.06976743787527084, acc: 0.970802903175354)
[2025-01-06 01:29:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:05][root][INFO] - Training Epoch: 5/10, step 554/574 completed (loss: 0.07701417058706284, acc: 0.9655172228813171)
[2025-01-06 01:29:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:05][root][INFO] - Training Epoch: 5/10, step 555/574 completed (loss: 0.15124979615211487, acc: 0.9357143044471741)
[2025-01-06 01:29:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:06][root][INFO] - Training Epoch: 5/10, step 556/574 completed (loss: 0.16770590841770172, acc: 0.9536423683166504)
[2025-01-06 01:29:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:06][root][INFO] - Training Epoch: 5/10, step 557/574 completed (loss: 0.04416178539395332, acc: 0.9914529919624329)
[2025-01-06 01:29:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:06][root][INFO] - Training Epoch: 5/10, step 558/574 completed (loss: 0.027139313519001007, acc: 1.0)
[2025-01-06 01:29:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:07][root][INFO] - Training Epoch: 5/10, step 559/574 completed (loss: 0.024734126403927803, acc: 1.0)
[2025-01-06 01:29:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:07][root][INFO] - Training Epoch: 5/10, step 560/574 completed (loss: 0.0007507042610086501, acc: 1.0)
[2025-01-06 01:29:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:07][root][INFO] - Training Epoch: 5/10, step 561/574 completed (loss: 0.19279740750789642, acc: 0.9487179517745972)
[2025-01-06 01:29:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:08][root][INFO] - Training Epoch: 5/10, step 562/574 completed (loss: 0.06538917124271393, acc: 0.9666666388511658)
[2025-01-06 01:29:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:08][root][INFO] - Training Epoch: 5/10, step 563/574 completed (loss: 0.13165071606636047, acc: 0.9740259647369385)
[2025-01-06 01:29:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:38][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.1991, device='cuda:0') eval_epoch_loss=tensor(0.7880, device='cuda:0') eval_epoch_acc=tensor(0.8423, device='cuda:0')
[2025-01-06 01:29:38][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:29:38][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:29:38][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_5_step_564_loss_0.7880285978317261/model.pt
[2025-01-06 01:29:38][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:29:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:38][root][INFO] - Training Epoch: 5/10, step 564/574 completed (loss: 0.025892918929457664, acc: 1.0)
[2025-01-06 01:29:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:39][root][INFO] - Training Epoch: 5/10, step 565/574 completed (loss: 0.03370610624551773, acc: 1.0)
[2025-01-06 01:29:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:39][root][INFO] - Training Epoch: 5/10, step 566/574 completed (loss: 0.09954212605953217, acc: 0.9642857313156128)
[2025-01-06 01:29:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:39][root][INFO] - Training Epoch: 5/10, step 567/574 completed (loss: 0.013459471054375172, acc: 1.0)
[2025-01-06 01:29:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:40][root][INFO] - Training Epoch: 5/10, step 568/574 completed (loss: 0.04657291620969772, acc: 0.9629629850387573)
[2025-01-06 01:29:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:40][root][INFO] - Training Epoch: 5/10, step 569/574 completed (loss: 0.15703339874744415, acc: 0.9572192430496216)
[2025-01-06 01:29:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:40][root][INFO] - Training Epoch: 5/10, step 570/574 completed (loss: 0.015539342537522316, acc: 1.0)
[2025-01-06 01:29:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:41][root][INFO] - Training Epoch: 5/10, step 571/574 completed (loss: 0.03763257712125778, acc: 0.9914529919624329)
[2025-01-06 01:29:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:41][root][INFO] - Training Epoch: 5/10, step 572/574 completed (loss: 0.13224737346172333, acc: 0.9591836929321289)
[2025-01-06 01:29:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:41][root][INFO] - Training Epoch: 5/10, step 573/574 completed (loss: 0.10142943263053894, acc: 0.9748427867889404)
[2025-01-06 01:29:42][slam_llm.utils.train_utils][INFO] - Epoch 5: train_perplexity=1.1540, train_epoch_loss=0.1433, epoch time 353.23452988639474s
[2025-01-06 01:29:42][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 13 GB
[2025-01-06 01:29:42][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 15 GB
[2025-01-06 01:29:42][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 13 GB
[2025-01-06 01:29:42][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 14
[2025-01-06 01:29:42][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2025-01-06 01:29:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:43][root][INFO] - Training Epoch: 6/10, step 0/574 completed (loss: 0.03650952875614166, acc: 1.0)
[2025-01-06 01:29:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:43][root][INFO] - Training Epoch: 6/10, step 1/574 completed (loss: 0.1182468980550766, acc: 0.9599999785423279)
[2025-01-06 01:29:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:44][root][INFO] - Training Epoch: 6/10, step 2/574 completed (loss: 0.3303297460079193, acc: 0.9459459185600281)
[2025-01-06 01:29:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:44][root][INFO] - Training Epoch: 6/10, step 3/574 completed (loss: 0.05277125537395477, acc: 0.9736841917037964)
[2025-01-06 01:29:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:44][root][INFO] - Training Epoch: 6/10, step 4/574 completed (loss: 0.1685672253370285, acc: 0.9459459185600281)
[2025-01-06 01:29:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:45][root][INFO] - Training Epoch: 6/10, step 5/574 completed (loss: 0.00391375133767724, acc: 1.0)
[2025-01-06 01:29:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:45][root][INFO] - Training Epoch: 6/10, step 6/574 completed (loss: 0.1358402967453003, acc: 0.918367326259613)
[2025-01-06 01:29:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:45][root][INFO] - Training Epoch: 6/10, step 7/574 completed (loss: 0.1875970959663391, acc: 0.9666666388511658)
[2025-01-06 01:29:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:46][root][INFO] - Training Epoch: 6/10, step 8/574 completed (loss: 0.012026667594909668, acc: 1.0)
[2025-01-06 01:29:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:46][root][INFO] - Training Epoch: 6/10, step 9/574 completed (loss: 0.07162021100521088, acc: 0.9615384340286255)
[2025-01-06 01:29:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:46][root][INFO] - Training Epoch: 6/10, step 10/574 completed (loss: 0.0032022190280258656, acc: 1.0)
[2025-01-06 01:29:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:47][root][INFO] - Training Epoch: 6/10, step 11/574 completed (loss: 0.09005831182003021, acc: 0.9743589758872986)
[2025-01-06 01:29:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:47][root][INFO] - Training Epoch: 6/10, step 12/574 completed (loss: 0.01929617114365101, acc: 1.0)
[2025-01-06 01:29:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:48][root][INFO] - Training Epoch: 6/10, step 13/574 completed (loss: 0.03383411094546318, acc: 1.0)
[2025-01-06 01:29:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:48][root][INFO] - Training Epoch: 6/10, step 14/574 completed (loss: 0.020708736032247543, acc: 1.0)
[2025-01-06 01:29:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:48][root][INFO] - Training Epoch: 6/10, step 15/574 completed (loss: 0.05101849511265755, acc: 1.0)
[2025-01-06 01:29:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:49][root][INFO] - Training Epoch: 6/10, step 16/574 completed (loss: 0.25824078917503357, acc: 0.9473684430122375)
[2025-01-06 01:29:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:49][root][INFO] - Training Epoch: 6/10, step 17/574 completed (loss: 0.02589741162955761, acc: 1.0)
[2025-01-06 01:29:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:49][root][INFO] - Training Epoch: 6/10, step 18/574 completed (loss: 0.03409735858440399, acc: 1.0)
[2025-01-06 01:29:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:50][root][INFO] - Training Epoch: 6/10, step 19/574 completed (loss: 0.07857441157102585, acc: 0.9473684430122375)
[2025-01-06 01:29:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:50][root][INFO] - Training Epoch: 6/10, step 20/574 completed (loss: 0.0071403151378035545, acc: 1.0)
[2025-01-06 01:29:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:50][root][INFO] - Training Epoch: 6/10, step 21/574 completed (loss: 0.04018312320113182, acc: 1.0)
[2025-01-06 01:29:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:51][root][INFO] - Training Epoch: 6/10, step 22/574 completed (loss: 0.019435223191976547, acc: 1.0)
[2025-01-06 01:29:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:51][root][INFO] - Training Epoch: 6/10, step 23/574 completed (loss: 0.252669095993042, acc: 0.9523809552192688)
[2025-01-06 01:29:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:52][root][INFO] - Training Epoch: 6/10, step 24/574 completed (loss: 0.009375734254717827, acc: 1.0)
[2025-01-06 01:29:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:52][root][INFO] - Training Epoch: 6/10, step 25/574 completed (loss: 0.08388864994049072, acc: 0.9811320900917053)
[2025-01-06 01:29:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:52][root][INFO] - Training Epoch: 6/10, step 26/574 completed (loss: 0.24925537407398224, acc: 0.9178082346916199)
[2025-01-06 01:29:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:54][root][INFO] - Training Epoch: 6/10, step 27/574 completed (loss: 0.37279650568962097, acc: 0.8972331881523132)
[2025-01-06 01:29:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:54][root][INFO] - Training Epoch: 6/10, step 28/574 completed (loss: 0.054241012781858444, acc: 0.9767441749572754)
[2025-01-06 01:29:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:54][root][INFO] - Training Epoch: 6/10, step 29/574 completed (loss: 0.11230344325304031, acc: 0.9397590160369873)
[2025-01-06 01:29:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:55][root][INFO] - Training Epoch: 6/10, step 30/574 completed (loss: 0.09408427774906158, acc: 0.9629629850387573)
[2025-01-06 01:29:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:55][root][INFO] - Training Epoch: 6/10, step 31/574 completed (loss: 0.08853346109390259, acc: 0.9642857313156128)
[2025-01-06 01:29:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:55][root][INFO] - Training Epoch: 6/10, step 32/574 completed (loss: 0.006982472725212574, acc: 1.0)
[2025-01-06 01:29:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:56][root][INFO] - Training Epoch: 6/10, step 33/574 completed (loss: 0.011103869415819645, acc: 1.0)
[2025-01-06 01:29:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:56][root][INFO] - Training Epoch: 6/10, step 34/574 completed (loss: 0.10677780956029892, acc: 0.9831932783126831)
[2025-01-06 01:29:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:56][root][INFO] - Training Epoch: 6/10, step 35/574 completed (loss: 0.06652642786502838, acc: 0.9672130942344666)
[2025-01-06 01:29:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:57][root][INFO] - Training Epoch: 6/10, step 36/574 completed (loss: 0.1930590569972992, acc: 0.9047619104385376)
[2025-01-06 01:29:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:57][root][INFO] - Training Epoch: 6/10, step 37/574 completed (loss: 0.18122322857379913, acc: 0.9661017060279846)
[2025-01-06 01:29:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:58][root][INFO] - Training Epoch: 6/10, step 38/574 completed (loss: 0.04578394070267677, acc: 0.9885057210922241)
[2025-01-06 01:29:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:58][root][INFO] - Training Epoch: 6/10, step 39/574 completed (loss: 0.056825216859579086, acc: 1.0)
[2025-01-06 01:29:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:58][root][INFO] - Training Epoch: 6/10, step 40/574 completed (loss: 0.037654001265764236, acc: 1.0)
[2025-01-06 01:29:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:59][root][INFO] - Training Epoch: 6/10, step 41/574 completed (loss: 0.18566887080669403, acc: 0.9594594836235046)
[2025-01-06 01:29:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:59][root][INFO] - Training Epoch: 6/10, step 42/574 completed (loss: 0.2692771553993225, acc: 0.9384615421295166)
[2025-01-06 01:29:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:29:59][root][INFO] - Training Epoch: 6/10, step 43/574 completed (loss: 0.3755378723144531, acc: 0.9090909361839294)
[2025-01-06 01:30:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:00][root][INFO] - Training Epoch: 6/10, step 44/574 completed (loss: 0.14549823105335236, acc: 0.9484536051750183)
[2025-01-06 01:30:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:00][root][INFO] - Training Epoch: 6/10, step 45/574 completed (loss: 0.12707878649234772, acc: 0.9632353186607361)
[2025-01-06 01:30:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:01][root][INFO] - Training Epoch: 6/10, step 46/574 completed (loss: 0.005124996416270733, acc: 1.0)
[2025-01-06 01:30:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:01][root][INFO] - Training Epoch: 6/10, step 47/574 completed (loss: 0.17473629117012024, acc: 0.9629629850387573)
[2025-01-06 01:30:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:01][root][INFO] - Training Epoch: 6/10, step 48/574 completed (loss: 0.017331983894109726, acc: 1.0)
[2025-01-06 01:30:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:02][root][INFO] - Training Epoch: 6/10, step 49/574 completed (loss: 0.003418693784624338, acc: 1.0)
[2025-01-06 01:30:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:02][root][INFO] - Training Epoch: 6/10, step 50/574 completed (loss: 0.2126428335905075, acc: 0.9298245906829834)
[2025-01-06 01:30:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:02][root][INFO] - Training Epoch: 6/10, step 51/574 completed (loss: 0.10820740461349487, acc: 0.9365079402923584)
[2025-01-06 01:30:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:03][root][INFO] - Training Epoch: 6/10, step 52/574 completed (loss: 0.22845740616321564, acc: 0.9295774698257446)
[2025-01-06 01:30:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:03][root][INFO] - Training Epoch: 6/10, step 53/574 completed (loss: 0.5182448625564575, acc: 0.8199999928474426)
[2025-01-06 01:30:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:04][root][INFO] - Training Epoch: 6/10, step 54/574 completed (loss: 0.09214067459106445, acc: 0.9729729890823364)
[2025-01-06 01:30:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:04][root][INFO] - Training Epoch: 6/10, step 55/574 completed (loss: 0.10808281600475311, acc: 0.9230769276618958)
[2025-01-06 01:30:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:07][root][INFO] - Training Epoch: 6/10, step 56/574 completed (loss: 0.662148118019104, acc: 0.7747440338134766)
[2025-01-06 01:30:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:08][root][INFO] - Training Epoch: 6/10, step 57/574 completed (loss: 0.8399643301963806, acc: 0.7603485584259033)
[2025-01-06 01:30:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:09][root][INFO] - Training Epoch: 6/10, step 58/574 completed (loss: 0.47580936551094055, acc: 0.8465909361839294)
[2025-01-06 01:30:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:09][root][INFO] - Training Epoch: 6/10, step 59/574 completed (loss: 0.12737159430980682, acc: 0.9632353186607361)
[2025-01-06 01:30:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:10][root][INFO] - Training Epoch: 6/10, step 60/574 completed (loss: 0.3534499704837799, acc: 0.8913043737411499)
[2025-01-06 01:30:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:10][root][INFO] - Training Epoch: 6/10, step 61/574 completed (loss: 0.13555563986301422, acc: 0.9624999761581421)
[2025-01-06 01:30:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:11][root][INFO] - Training Epoch: 6/10, step 62/574 completed (loss: 0.043638262897729874, acc: 1.0)
[2025-01-06 01:30:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:11][root][INFO] - Training Epoch: 6/10, step 63/574 completed (loss: 0.2540118098258972, acc: 0.9166666865348816)
[2025-01-06 01:30:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:12][root][INFO] - Training Epoch: 6/10, step 64/574 completed (loss: 0.055409256368875504, acc: 0.96875)
[2025-01-06 01:30:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:12][root][INFO] - Training Epoch: 6/10, step 65/574 completed (loss: 0.028882376849651337, acc: 1.0)
[2025-01-06 01:30:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:12][root][INFO] - Training Epoch: 6/10, step 66/574 completed (loss: 0.14609725773334503, acc: 0.9642857313156128)
[2025-01-06 01:30:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:13][root][INFO] - Training Epoch: 6/10, step 67/574 completed (loss: 0.02271166257560253, acc: 1.0)
[2025-01-06 01:30:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:13][root][INFO] - Training Epoch: 6/10, step 68/574 completed (loss: 0.005287297535687685, acc: 1.0)
[2025-01-06 01:30:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:13][root][INFO] - Training Epoch: 6/10, step 69/574 completed (loss: 0.02710566483438015, acc: 1.0)
[2025-01-06 01:30:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:14][root][INFO] - Training Epoch: 6/10, step 70/574 completed (loss: 0.047220051288604736, acc: 1.0)
[2025-01-06 01:30:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:14][root][INFO] - Training Epoch: 6/10, step 71/574 completed (loss: 0.2042800933122635, acc: 0.9558823704719543)
[2025-01-06 01:30:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:14][root][INFO] - Training Epoch: 6/10, step 72/574 completed (loss: 0.21783210337162018, acc: 0.920634925365448)
[2025-01-06 01:30:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:15][root][INFO] - Training Epoch: 6/10, step 73/574 completed (loss: 0.5042523741722107, acc: 0.8410256505012512)
[2025-01-06 01:30:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:15][root][INFO] - Training Epoch: 6/10, step 74/574 completed (loss: 0.22981661558151245, acc: 0.9387755393981934)
[2025-01-06 01:30:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:16][root][INFO] - Training Epoch: 6/10, step 75/574 completed (loss: 0.4059169590473175, acc: 0.888059675693512)
[2025-01-06 01:30:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:16][root][INFO] - Training Epoch: 6/10, step 76/574 completed (loss: 0.697751522064209, acc: 0.7992700934410095)
[2025-01-06 01:30:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:16][root][INFO] - Training Epoch: 6/10, step 77/574 completed (loss: 0.004244180396199226, acc: 1.0)
[2025-01-06 01:30:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:17][root][INFO] - Training Epoch: 6/10, step 78/574 completed (loss: 0.0056671216152608395, acc: 1.0)
[2025-01-06 01:30:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:17][root][INFO] - Training Epoch: 6/10, step 79/574 completed (loss: 0.07033957540988922, acc: 0.9696969985961914)
[2025-01-06 01:30:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:17][root][INFO] - Training Epoch: 6/10, step 80/574 completed (loss: 0.012716993689537048, acc: 1.0)
[2025-01-06 01:30:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:18][root][INFO] - Training Epoch: 6/10, step 81/574 completed (loss: 0.08677180856466293, acc: 0.9615384340286255)
[2025-01-06 01:30:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:18][root][INFO] - Training Epoch: 6/10, step 82/574 completed (loss: 0.07504735887050629, acc: 0.9807692170143127)
[2025-01-06 01:30:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:19][root][INFO] - Training Epoch: 6/10, step 83/574 completed (loss: 0.022191425785422325, acc: 1.0)
[2025-01-06 01:30:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:19][root][INFO] - Training Epoch: 6/10, step 84/574 completed (loss: 0.046143539249897, acc: 1.0)
[2025-01-06 01:30:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:19][root][INFO] - Training Epoch: 6/10, step 85/574 completed (loss: 0.0841284841299057, acc: 0.9800000190734863)
[2025-01-06 01:30:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:20][root][INFO] - Training Epoch: 6/10, step 86/574 completed (loss: 0.05804302543401718, acc: 0.95652174949646)
[2025-01-06 01:30:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:20][root][INFO] - Training Epoch: 6/10, step 87/574 completed (loss: 0.06592926383018494, acc: 0.9800000190734863)
[2025-01-06 01:30:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:20][root][INFO] - Training Epoch: 6/10, step 88/574 completed (loss: 0.3357716500759125, acc: 0.9029126167297363)
[2025-01-06 01:30:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:22][root][INFO] - Training Epoch: 6/10, step 89/574 completed (loss: 0.5254918932914734, acc: 0.8495145440101624)
[2025-01-06 01:30:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:22][root][INFO] - Training Epoch: 6/10, step 90/574 completed (loss: 0.4647981822490692, acc: 0.8870967626571655)
[2025-01-06 01:30:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:23][root][INFO] - Training Epoch: 6/10, step 91/574 completed (loss: 0.44534915685653687, acc: 0.875)
[2025-01-06 01:30:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:24][root][INFO] - Training Epoch: 6/10, step 92/574 completed (loss: 0.2542400658130646, acc: 0.9157894849777222)
[2025-01-06 01:30:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:25][root][INFO] - Training Epoch: 6/10, step 93/574 completed (loss: 0.4932478964328766, acc: 0.8811880946159363)
[2025-01-06 01:30:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:25][root][INFO] - Training Epoch: 6/10, step 94/574 completed (loss: 0.21016378700733185, acc: 0.9516128897666931)
[2025-01-06 01:30:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:26][root][INFO] - Training Epoch: 6/10, step 95/574 completed (loss: 0.11403991281986237, acc: 0.9710144996643066)
[2025-01-06 01:30:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:26][root][INFO] - Training Epoch: 6/10, step 96/574 completed (loss: 0.37388578057289124, acc: 0.8823529481887817)
[2025-01-06 01:30:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:27][root][INFO] - Training Epoch: 6/10, step 97/574 completed (loss: 0.2606305480003357, acc: 0.9134615659713745)
[2025-01-06 01:30:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:27][root][INFO] - Training Epoch: 6/10, step 98/574 completed (loss: 0.3025721311569214, acc: 0.9343065619468689)
[2025-01-06 01:30:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:27][root][INFO] - Training Epoch: 6/10, step 99/574 completed (loss: 0.19858239591121674, acc: 0.9253731369972229)
[2025-01-06 01:30:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:28][root][INFO] - Training Epoch: 6/10, step 100/574 completed (loss: 0.054068613797426224, acc: 1.0)
[2025-01-06 01:30:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:28][root][INFO] - Training Epoch: 6/10, step 101/574 completed (loss: 0.0025099795311689377, acc: 1.0)
[2025-01-06 01:30:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:28][root][INFO] - Training Epoch: 6/10, step 102/574 completed (loss: 0.00740353437140584, acc: 1.0)
[2025-01-06 01:30:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:29][root][INFO] - Training Epoch: 6/10, step 103/574 completed (loss: 0.003968818578869104, acc: 1.0)
[2025-01-06 01:30:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:29][root][INFO] - Training Epoch: 6/10, step 104/574 completed (loss: 0.055359840393066406, acc: 0.982758641242981)
[2025-01-06 01:30:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:30][root][INFO] - Training Epoch: 6/10, step 105/574 completed (loss: 0.013414776884019375, acc: 1.0)
[2025-01-06 01:30:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:30][root][INFO] - Training Epoch: 6/10, step 106/574 completed (loss: 0.002970757195726037, acc: 1.0)
[2025-01-06 01:30:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:30][root][INFO] - Training Epoch: 6/10, step 107/574 completed (loss: 0.008628655225038528, acc: 1.0)
[2025-01-06 01:30:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:30][root][INFO] - Training Epoch: 6/10, step 108/574 completed (loss: 0.018435893580317497, acc: 1.0)
[2025-01-06 01:30:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:31][root][INFO] - Training Epoch: 6/10, step 109/574 completed (loss: 0.009877899661660194, acc: 1.0)
[2025-01-06 01:30:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:31][root][INFO] - Training Epoch: 6/10, step 110/574 completed (loss: 0.015659578144550323, acc: 1.0)
[2025-01-06 01:30:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:31][root][INFO] - Training Epoch: 6/10, step 111/574 completed (loss: 0.07383082062005997, acc: 0.9649122953414917)
[2025-01-06 01:30:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:32][root][INFO] - Training Epoch: 6/10, step 112/574 completed (loss: 0.3472055494785309, acc: 0.9649122953414917)
[2025-01-06 01:30:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:32][root][INFO] - Training Epoch: 6/10, step 113/574 completed (loss: 0.019955093041062355, acc: 1.0)
[2025-01-06 01:30:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:33][root][INFO] - Training Epoch: 6/10, step 114/574 completed (loss: 0.0352177694439888, acc: 1.0)
[2025-01-06 01:30:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:33][root][INFO] - Training Epoch: 6/10, step 115/574 completed (loss: 0.09160766005516052, acc: 0.9545454382896423)
[2025-01-06 01:30:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:33][root][INFO] - Training Epoch: 6/10, step 116/574 completed (loss: 0.043869465589523315, acc: 0.9841269850730896)
[2025-01-06 01:30:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:34][root][INFO] - Training Epoch: 6/10, step 117/574 completed (loss: 0.10488398373126984, acc: 0.9593495726585388)
[2025-01-06 01:30:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:34][root][INFO] - Training Epoch: 6/10, step 118/574 completed (loss: 0.036726076155900955, acc: 0.9838709831237793)
[2025-01-06 01:30:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:35][root][INFO] - Training Epoch: 6/10, step 119/574 completed (loss: 0.33518892526626587, acc: 0.9049429893493652)
[2025-01-06 01:30:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:35][root][INFO] - Training Epoch: 6/10, step 120/574 completed (loss: 0.023998497053980827, acc: 0.9866666793823242)
[2025-01-06 01:30:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:36][root][INFO] - Training Epoch: 6/10, step 121/574 completed (loss: 0.06980709731578827, acc: 0.942307710647583)
[2025-01-06 01:30:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:36][root][INFO] - Training Epoch: 6/10, step 122/574 completed (loss: 0.028036082163453102, acc: 1.0)
[2025-01-06 01:30:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:36][root][INFO] - Training Epoch: 6/10, step 123/574 completed (loss: 0.0038307940121740103, acc: 1.0)
[2025-01-06 01:30:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:37][root][INFO] - Training Epoch: 6/10, step 124/574 completed (loss: 0.3228626549243927, acc: 0.9141104221343994)
[2025-01-06 01:30:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:37][root][INFO] - Training Epoch: 6/10, step 125/574 completed (loss: 0.2725054621696472, acc: 0.9305555820465088)
[2025-01-06 01:30:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:37][root][INFO] - Training Epoch: 6/10, step 126/574 completed (loss: 0.3578115403652191, acc: 0.9166666865348816)
[2025-01-06 01:30:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:38][root][INFO] - Training Epoch: 6/10, step 127/574 completed (loss: 0.20011213421821594, acc: 0.9523809552192688)
[2025-01-06 01:30:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:38][root][INFO] - Training Epoch: 6/10, step 128/574 completed (loss: 0.2053147554397583, acc: 0.9487179517745972)
[2025-01-06 01:30:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:38][root][INFO] - Training Epoch: 6/10, step 129/574 completed (loss: 0.2274349480867386, acc: 0.9191176295280457)
[2025-01-06 01:30:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:39][root][INFO] - Training Epoch: 6/10, step 130/574 completed (loss: 0.10724695026874542, acc: 0.9615384340286255)
[2025-01-06 01:30:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:39][root][INFO] - Training Epoch: 6/10, step 131/574 completed (loss: 0.08985039591789246, acc: 0.95652174949646)
[2025-01-06 01:30:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:39][root][INFO] - Training Epoch: 6/10, step 132/574 completed (loss: 0.050413746386766434, acc: 1.0)
[2025-01-06 01:30:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:30:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:10][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.0603, device='cuda:0') eval_epoch_loss=tensor(0.7228, device='cuda:0') eval_epoch_acc=tensor(0.8469, device='cuda:0')
[2025-01-06 01:31:10][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:31:10][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:31:11][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_6_step_133_loss_0.7228496074676514/model.pt
[2025-01-06 01:31:11][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:31:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:11][root][INFO] - Training Epoch: 6/10, step 133/574 completed (loss: 0.03444460779428482, acc: 1.0)
[2025-01-06 01:31:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:11][root][INFO] - Training Epoch: 6/10, step 134/574 completed (loss: 0.046977922320365906, acc: 1.0)
[2025-01-06 01:31:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:12][root][INFO] - Training Epoch: 6/10, step 135/574 completed (loss: 0.12710198760032654, acc: 0.9615384340286255)
[2025-01-06 01:31:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:12][root][INFO] - Training Epoch: 6/10, step 136/574 completed (loss: 0.04089578986167908, acc: 1.0)
[2025-01-06 01:31:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:13][root][INFO] - Training Epoch: 6/10, step 137/574 completed (loss: 0.06025998666882515, acc: 1.0)
[2025-01-06 01:31:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:13][root][INFO] - Training Epoch: 6/10, step 138/574 completed (loss: 0.06474440544843674, acc: 0.95652174949646)
[2025-01-06 01:31:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:14][root][INFO] - Training Epoch: 6/10, step 139/574 completed (loss: 0.0029855328612029552, acc: 1.0)
[2025-01-06 01:31:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:14][root][INFO] - Training Epoch: 6/10, step 140/574 completed (loss: 0.021389760076999664, acc: 1.0)
[2025-01-06 01:31:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:14][root][INFO] - Training Epoch: 6/10, step 141/574 completed (loss: 0.059531450271606445, acc: 0.9677419066429138)
[2025-01-06 01:31:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:15][root][INFO] - Training Epoch: 6/10, step 142/574 completed (loss: 0.15124273300170898, acc: 0.9729729890823364)
[2025-01-06 01:31:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:15][root][INFO] - Training Epoch: 6/10, step 143/574 completed (loss: 0.14486879110336304, acc: 0.9561403393745422)
[2025-01-06 01:31:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:16][root][INFO] - Training Epoch: 6/10, step 144/574 completed (loss: 0.13679252564907074, acc: 0.9402984976768494)
[2025-01-06 01:31:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:16][root][INFO] - Training Epoch: 6/10, step 145/574 completed (loss: 0.10097069293260574, acc: 0.9795918464660645)
[2025-01-06 01:31:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:16][root][INFO] - Training Epoch: 6/10, step 146/574 completed (loss: 0.21924401819705963, acc: 0.936170220375061)
[2025-01-06 01:31:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:17][root][INFO] - Training Epoch: 6/10, step 147/574 completed (loss: 0.09883362799882889, acc: 0.9714285731315613)
[2025-01-06 01:31:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:17][root][INFO] - Training Epoch: 6/10, step 148/574 completed (loss: 0.009242304600775242, acc: 1.0)
[2025-01-06 01:31:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:18][root][INFO] - Training Epoch: 6/10, step 149/574 completed (loss: 0.18873775005340576, acc: 0.95652174949646)
[2025-01-06 01:31:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:18][root][INFO] - Training Epoch: 6/10, step 150/574 completed (loss: 0.010923308320343494, acc: 1.0)
[2025-01-06 01:31:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:18][root][INFO] - Training Epoch: 6/10, step 151/574 completed (loss: 0.1651451736688614, acc: 0.9347826242446899)
[2025-01-06 01:31:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:19][root][INFO] - Training Epoch: 6/10, step 152/574 completed (loss: 0.1476735770702362, acc: 0.9491525292396545)
[2025-01-06 01:31:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:19][root][INFO] - Training Epoch: 6/10, step 153/574 completed (loss: 0.23755231499671936, acc: 0.9649122953414917)
[2025-01-06 01:31:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:19][root][INFO] - Training Epoch: 6/10, step 154/574 completed (loss: 0.14391855895519257, acc: 0.9189189076423645)
[2025-01-06 01:31:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:20][root][INFO] - Training Epoch: 6/10, step 155/574 completed (loss: 0.08606299012899399, acc: 0.9642857313156128)
[2025-01-06 01:31:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:20][root][INFO] - Training Epoch: 6/10, step 156/574 completed (loss: 0.255880743265152, acc: 0.9130434989929199)
[2025-01-06 01:31:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:20][root][INFO] - Training Epoch: 6/10, step 157/574 completed (loss: 0.346884548664093, acc: 0.8947368264198303)
[2025-01-06 01:31:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:22][root][INFO] - Training Epoch: 6/10, step 158/574 completed (loss: 0.25656408071517944, acc: 0.9054054021835327)
[2025-01-06 01:31:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:22][root][INFO] - Training Epoch: 6/10, step 159/574 completed (loss: 0.5718026757240295, acc: 0.8333333134651184)
[2025-01-06 01:31:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:23][root][INFO] - Training Epoch: 6/10, step 160/574 completed (loss: 0.3385399580001831, acc: 0.8720930218696594)
[2025-01-06 01:31:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:23][root][INFO] - Training Epoch: 6/10, step 161/574 completed (loss: 0.358117014169693, acc: 0.8941176533699036)
[2025-01-06 01:31:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:24][root][INFO] - Training Epoch: 6/10, step 162/574 completed (loss: 0.2719663381576538, acc: 0.932584285736084)
[2025-01-06 01:31:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:24][root][INFO] - Training Epoch: 6/10, step 163/574 completed (loss: 0.07393158227205276, acc: 0.9772727489471436)
[2025-01-06 01:31:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:25][root][INFO] - Training Epoch: 6/10, step 164/574 completed (loss: 0.11941118538379669, acc: 0.9523809552192688)
[2025-01-06 01:31:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:25][root][INFO] - Training Epoch: 6/10, step 165/574 completed (loss: 0.22014465928077698, acc: 0.931034505367279)
[2025-01-06 01:31:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:25][root][INFO] - Training Epoch: 6/10, step 166/574 completed (loss: 0.042566221207380295, acc: 0.9795918464660645)
[2025-01-06 01:31:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:26][root][INFO] - Training Epoch: 6/10, step 167/574 completed (loss: 0.014325905591249466, acc: 1.0)
[2025-01-06 01:31:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:26][root][INFO] - Training Epoch: 6/10, step 168/574 completed (loss: 0.10290935635566711, acc: 0.9722222089767456)
[2025-01-06 01:31:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:26][root][INFO] - Training Epoch: 6/10, step 169/574 completed (loss: 0.381963849067688, acc: 0.8823529481887817)
[2025-01-06 01:31:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:28][root][INFO] - Training Epoch: 6/10, step 170/574 completed (loss: 0.36752015352249146, acc: 0.9041095972061157)
[2025-01-06 01:31:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:28][root][INFO] - Training Epoch: 6/10, step 171/574 completed (loss: 0.153365820646286, acc: 0.9583333134651184)
[2025-01-06 01:31:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:28][root][INFO] - Training Epoch: 6/10, step 172/574 completed (loss: 0.21721000969409943, acc: 0.8888888955116272)
[2025-01-06 01:31:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:29][root][INFO] - Training Epoch: 6/10, step 173/574 completed (loss: 0.0165774617344141, acc: 1.0)
[2025-01-06 01:31:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:29][root][INFO] - Training Epoch: 6/10, step 174/574 completed (loss: 0.5674290060997009, acc: 0.8407079577445984)
[2025-01-06 01:31:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:29][root][INFO] - Training Epoch: 6/10, step 175/574 completed (loss: 0.1275276243686676, acc: 0.9420289993286133)
[2025-01-06 01:31:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:30][root][INFO] - Training Epoch: 6/10, step 176/574 completed (loss: 0.1307777464389801, acc: 0.9318181872367859)
[2025-01-06 01:31:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:31][root][INFO] - Training Epoch: 6/10, step 177/574 completed (loss: 0.3723434805870056, acc: 0.8702290058135986)
[2025-01-06 01:31:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:31][root][INFO] - Training Epoch: 6/10, step 178/574 completed (loss: 0.32359614968299866, acc: 0.9185185432434082)
[2025-01-06 01:31:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:32][root][INFO] - Training Epoch: 6/10, step 179/574 completed (loss: 0.03549972549080849, acc: 0.9836065769195557)
[2025-01-06 01:31:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:32][root][INFO] - Training Epoch: 6/10, step 180/574 completed (loss: 0.0020635323598980904, acc: 1.0)
[2025-01-06 01:31:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:32][root][INFO] - Training Epoch: 6/10, step 181/574 completed (loss: 0.2606298625469208, acc: 0.9599999785423279)
[2025-01-06 01:31:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:33][root][INFO] - Training Epoch: 6/10, step 182/574 completed (loss: 0.01518244482576847, acc: 1.0)
[2025-01-06 01:31:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:33][root][INFO] - Training Epoch: 6/10, step 183/574 completed (loss: 0.02291400544345379, acc: 0.9878048896789551)
[2025-01-06 01:31:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:33][root][INFO] - Training Epoch: 6/10, step 184/574 completed (loss: 0.23321567475795746, acc: 0.9425981640815735)
[2025-01-06 01:31:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:34][root][INFO] - Training Epoch: 6/10, step 185/574 completed (loss: 0.23617488145828247, acc: 0.9365994334220886)
[2025-01-06 01:31:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:34][root][INFO] - Training Epoch: 6/10, step 186/574 completed (loss: 0.23318776488304138, acc: 0.921875)
[2025-01-06 01:31:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:35][root][INFO] - Training Epoch: 6/10, step 187/574 completed (loss: 0.26366111636161804, acc: 0.9212007522583008)
[2025-01-06 01:31:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:35][root][INFO] - Training Epoch: 6/10, step 188/574 completed (loss: 0.24416446685791016, acc: 0.935943067073822)
[2025-01-06 01:31:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:35][root][INFO] - Training Epoch: 6/10, step 189/574 completed (loss: 0.14010868966579437, acc: 0.9599999785423279)
[2025-01-06 01:31:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:36][root][INFO] - Training Epoch: 6/10, step 190/574 completed (loss: 0.13047197461128235, acc: 0.9651162624359131)
[2025-01-06 01:31:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:37][root][INFO] - Training Epoch: 6/10, step 191/574 completed (loss: 0.5491508841514587, acc: 0.8730158805847168)
[2025-01-06 01:31:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:38][root][INFO] - Training Epoch: 6/10, step 192/574 completed (loss: 0.19701680541038513, acc: 0.9469696879386902)
[2025-01-06 01:31:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:38][root][INFO] - Training Epoch: 6/10, step 193/574 completed (loss: 0.14766769111156464, acc: 0.9764705896377563)
[2025-01-06 01:31:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:40][root][INFO] - Training Epoch: 6/10, step 194/574 completed (loss: 0.3210596740245819, acc: 0.895061731338501)
[2025-01-06 01:31:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:40][root][INFO] - Training Epoch: 6/10, step 195/574 completed (loss: 0.05605609714984894, acc: 1.0)
[2025-01-06 01:31:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:41][root][INFO] - Training Epoch: 6/10, step 196/574 completed (loss: 0.01982317492365837, acc: 1.0)
[2025-01-06 01:31:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:41][root][INFO] - Training Epoch: 6/10, step 197/574 completed (loss: 0.04062856733798981, acc: 1.0)
[2025-01-06 01:31:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:41][root][INFO] - Training Epoch: 6/10, step 198/574 completed (loss: 0.1742628663778305, acc: 0.970588207244873)
[2025-01-06 01:31:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:42][root][INFO] - Training Epoch: 6/10, step 199/574 completed (loss: 0.26767390966415405, acc: 0.9264705777168274)
[2025-01-06 01:31:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:42][root][INFO] - Training Epoch: 6/10, step 200/574 completed (loss: 0.22173435986042023, acc: 0.9237288236618042)
[2025-01-06 01:31:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:42][root][INFO] - Training Epoch: 6/10, step 201/574 completed (loss: 0.2635229825973511, acc: 0.8805969953536987)
[2025-01-06 01:31:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:43][root][INFO] - Training Epoch: 6/10, step 202/574 completed (loss: 0.18126805126667023, acc: 0.9320388436317444)
[2025-01-06 01:31:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:43][root][INFO] - Training Epoch: 6/10, step 203/574 completed (loss: 0.13232649862766266, acc: 0.9682539701461792)
[2025-01-06 01:31:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:44][root][INFO] - Training Epoch: 6/10, step 204/574 completed (loss: 0.015369324944913387, acc: 1.0)
[2025-01-06 01:31:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:44][root][INFO] - Training Epoch: 6/10, step 205/574 completed (loss: 0.07968297600746155, acc: 0.9775784611701965)
[2025-01-06 01:31:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:44][root][INFO] - Training Epoch: 6/10, step 206/574 completed (loss: 0.16003850102424622, acc: 0.9527559280395508)
[2025-01-06 01:31:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:45][root][INFO] - Training Epoch: 6/10, step 207/574 completed (loss: 0.08632442355155945, acc: 0.9525862336158752)
[2025-01-06 01:31:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:45][root][INFO] - Training Epoch: 6/10, step 208/574 completed (loss: 0.15566182136535645, acc: 0.967391312122345)
[2025-01-06 01:31:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:45][root][INFO] - Training Epoch: 6/10, step 209/574 completed (loss: 0.12713995575904846, acc: 0.9649805426597595)
[2025-01-06 01:31:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:46][root][INFO] - Training Epoch: 6/10, step 210/574 completed (loss: 0.0707005187869072, acc: 0.97826087474823)
[2025-01-06 01:31:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:46][root][INFO] - Training Epoch: 6/10, step 211/574 completed (loss: 0.008575263433158398, acc: 1.0)
[2025-01-06 01:31:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:47][root][INFO] - Training Epoch: 6/10, step 212/574 completed (loss: 0.007990442216396332, acc: 1.0)
[2025-01-06 01:31:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:47][root][INFO] - Training Epoch: 6/10, step 213/574 completed (loss: 0.04738704860210419, acc: 0.978723406791687)
[2025-01-06 01:31:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:48][root][INFO] - Training Epoch: 6/10, step 214/574 completed (loss: 0.026809087023139, acc: 0.9923076629638672)
[2025-01-06 01:31:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:48][root][INFO] - Training Epoch: 6/10, step 215/574 completed (loss: 0.11006941646337509, acc: 0.9594594836235046)
[2025-01-06 01:31:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:48][root][INFO] - Training Epoch: 6/10, step 216/574 completed (loss: 0.021342534571886063, acc: 0.9883720874786377)
[2025-01-06 01:31:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:49][root][INFO] - Training Epoch: 6/10, step 217/574 completed (loss: 0.07269305735826492, acc: 0.9819819927215576)
[2025-01-06 01:31:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:49][root][INFO] - Training Epoch: 6/10, step 218/574 completed (loss: 0.040745366364717484, acc: 0.9888888597488403)
[2025-01-06 01:31:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:50][root][INFO] - Training Epoch: 6/10, step 219/574 completed (loss: 0.02504030056297779, acc: 1.0)
[2025-01-06 01:31:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:50][root][INFO] - Training Epoch: 6/10, step 220/574 completed (loss: 0.03342768922448158, acc: 1.0)
[2025-01-06 01:31:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:50][root][INFO] - Training Epoch: 6/10, step 221/574 completed (loss: 0.03327949345111847, acc: 1.0)
[2025-01-06 01:31:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:51][root][INFO] - Training Epoch: 6/10, step 222/574 completed (loss: 0.22141963243484497, acc: 0.9038461446762085)
[2025-01-06 01:31:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:51][root][INFO] - Training Epoch: 6/10, step 223/574 completed (loss: 0.11132761836051941, acc: 0.9619565010070801)
[2025-01-06 01:31:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:52][root][INFO] - Training Epoch: 6/10, step 224/574 completed (loss: 0.21553954482078552, acc: 0.9375)
[2025-01-06 01:31:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:52][root][INFO] - Training Epoch: 6/10, step 225/574 completed (loss: 0.18663443624973297, acc: 0.9042553305625916)
[2025-01-06 01:31:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:53][root][INFO] - Training Epoch: 6/10, step 226/574 completed (loss: 0.09352244436740875, acc: 0.9622641801834106)
[2025-01-06 01:31:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:53][root][INFO] - Training Epoch: 6/10, step 227/574 completed (loss: 0.04877977445721626, acc: 0.9666666388511658)
[2025-01-06 01:31:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:53][root][INFO] - Training Epoch: 6/10, step 228/574 completed (loss: 0.09304754436016083, acc: 0.9767441749572754)
[2025-01-06 01:31:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:54][root][INFO] - Training Epoch: 6/10, step 229/574 completed (loss: 0.30190640687942505, acc: 0.9333333373069763)
[2025-01-06 01:31:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:54][root][INFO] - Training Epoch: 6/10, step 230/574 completed (loss: 0.7360243201255798, acc: 0.75789475440979)
[2025-01-06 01:31:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:54][root][INFO] - Training Epoch: 6/10, step 231/574 completed (loss: 0.42897307872772217, acc: 0.8888888955116272)
[2025-01-06 01:31:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:55][root][INFO] - Training Epoch: 6/10, step 232/574 completed (loss: 0.634574294090271, acc: 0.800000011920929)
[2025-01-06 01:31:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:55][root][INFO] - Training Epoch: 6/10, step 233/574 completed (loss: 0.9545538425445557, acc: 0.7247706651687622)
[2025-01-06 01:31:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:56][root][INFO] - Training Epoch: 6/10, step 234/574 completed (loss: 0.601556658744812, acc: 0.8153846263885498)
[2025-01-06 01:31:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:56][root][INFO] - Training Epoch: 6/10, step 235/574 completed (loss: 0.024033622816205025, acc: 1.0)
[2025-01-06 01:31:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:57][root][INFO] - Training Epoch: 6/10, step 236/574 completed (loss: 0.009581254795193672, acc: 1.0)
[2025-01-06 01:31:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:57][root][INFO] - Training Epoch: 6/10, step 237/574 completed (loss: 0.4288305342197418, acc: 0.9090909361839294)
[2025-01-06 01:31:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:57][root][INFO] - Training Epoch: 6/10, step 238/574 completed (loss: 0.03607817366719246, acc: 1.0)
[2025-01-06 01:31:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:58][root][INFO] - Training Epoch: 6/10, step 239/574 completed (loss: 0.2136107087135315, acc: 0.9428571462631226)
[2025-01-06 01:31:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:58][root][INFO] - Training Epoch: 6/10, step 240/574 completed (loss: 0.10690510272979736, acc: 0.9772727489471436)
[2025-01-06 01:31:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:58][root][INFO] - Training Epoch: 6/10, step 241/574 completed (loss: 0.06620409339666367, acc: 1.0)
[2025-01-06 01:31:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:59][root][INFO] - Training Epoch: 6/10, step 242/574 completed (loss: 0.2694966197013855, acc: 0.9193548560142517)
[2025-01-06 01:31:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:31:59][root][INFO] - Training Epoch: 6/10, step 243/574 completed (loss: 0.39043551683425903, acc: 0.8636363744735718)
[2025-01-06 01:32:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:00][root][INFO] - Training Epoch: 6/10, step 244/574 completed (loss: 0.00025455429567955434, acc: 1.0)
[2025-01-06 01:32:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:00][root][INFO] - Training Epoch: 6/10, step 245/574 completed (loss: 0.22359831631183624, acc: 0.9615384340286255)
[2025-01-06 01:32:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:00][root][INFO] - Training Epoch: 6/10, step 246/574 completed (loss: 0.0027582915499806404, acc: 1.0)
[2025-01-06 01:32:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:01][root][INFO] - Training Epoch: 6/10, step 247/574 completed (loss: 0.008493723347783089, acc: 1.0)
[2025-01-06 01:32:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:01][root][INFO] - Training Epoch: 6/10, step 248/574 completed (loss: 0.01944374106824398, acc: 1.0)
[2025-01-06 01:32:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:01][root][INFO] - Training Epoch: 6/10, step 249/574 completed (loss: 0.025205660611391068, acc: 1.0)
[2025-01-06 01:32:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:02][root][INFO] - Training Epoch: 6/10, step 250/574 completed (loss: 0.00389865436591208, acc: 1.0)
[2025-01-06 01:32:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:02][root][INFO] - Training Epoch: 6/10, step 251/574 completed (loss: 0.020787781104445457, acc: 1.0)
[2025-01-06 01:32:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:03][root][INFO] - Training Epoch: 6/10, step 252/574 completed (loss: 0.005202674772590399, acc: 1.0)
[2025-01-06 01:32:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:03][root][INFO] - Training Epoch: 6/10, step 253/574 completed (loss: 0.017133379355072975, acc: 1.0)
[2025-01-06 01:32:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:03][root][INFO] - Training Epoch: 6/10, step 254/574 completed (loss: 0.0001236781827174127, acc: 1.0)
[2025-01-06 01:32:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:04][root][INFO] - Training Epoch: 6/10, step 255/574 completed (loss: 0.1035512313246727, acc: 0.9354838728904724)
[2025-01-06 01:32:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:04][root][INFO] - Training Epoch: 6/10, step 256/574 completed (loss: 0.010251899249851704, acc: 1.0)
[2025-01-06 01:32:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:04][root][INFO] - Training Epoch: 6/10, step 257/574 completed (loss: 0.04868520423769951, acc: 1.0)
[2025-01-06 01:32:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:05][root][INFO] - Training Epoch: 6/10, step 258/574 completed (loss: 0.017298752442002296, acc: 0.9868420958518982)
[2025-01-06 01:32:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:05][root][INFO] - Training Epoch: 6/10, step 259/574 completed (loss: 0.05634129419922829, acc: 0.9905660152435303)
[2025-01-06 01:32:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:06][root][INFO] - Training Epoch: 6/10, step 260/574 completed (loss: 0.0911596342921257, acc: 0.9666666388511658)
[2025-01-06 01:32:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:06][root][INFO] - Training Epoch: 6/10, step 261/574 completed (loss: 0.0040955254808068275, acc: 1.0)
[2025-01-06 01:32:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:07][root][INFO] - Training Epoch: 6/10, step 262/574 completed (loss: 0.024853233247995377, acc: 1.0)
[2025-01-06 01:32:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:07][root][INFO] - Training Epoch: 6/10, step 263/574 completed (loss: 0.3255406320095062, acc: 0.9066666960716248)
[2025-01-06 01:32:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:07][root][INFO] - Training Epoch: 6/10, step 264/574 completed (loss: 0.11774390935897827, acc: 0.9583333134651184)
[2025-01-06 01:32:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:08][root][INFO] - Training Epoch: 6/10, step 265/574 completed (loss: 0.6238560676574707, acc: 0.8240000009536743)
[2025-01-06 01:32:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:08][root][INFO] - Training Epoch: 6/10, step 266/574 completed (loss: 0.22616499662399292, acc: 0.9213483333587646)
[2025-01-06 01:32:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:09][root][INFO] - Training Epoch: 6/10, step 267/574 completed (loss: 0.19348643720149994, acc: 0.9324324131011963)
[2025-01-06 01:32:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:09][root][INFO] - Training Epoch: 6/10, step 268/574 completed (loss: 0.16994260251522064, acc: 0.931034505367279)
[2025-01-06 01:32:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:10][root][INFO] - Training Epoch: 6/10, step 269/574 completed (loss: 0.07196524739265442, acc: 0.9545454382896423)
[2025-01-06 01:32:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:10][root][INFO] - Training Epoch: 6/10, step 270/574 completed (loss: 0.01747133582830429, acc: 1.0)
[2025-01-06 01:32:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:10][root][INFO] - Training Epoch: 6/10, step 271/574 completed (loss: 0.08751257508993149, acc: 0.96875)
[2025-01-06 01:32:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:11][root][INFO] - Training Epoch: 6/10, step 272/574 completed (loss: 0.0018845272716134787, acc: 1.0)
[2025-01-06 01:32:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:11][root][INFO] - Training Epoch: 6/10, step 273/574 completed (loss: 0.2075362205505371, acc: 0.9833333492279053)
[2025-01-06 01:32:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:11][root][INFO] - Training Epoch: 6/10, step 274/574 completed (loss: 0.28238430619239807, acc: 0.9375)
[2025-01-06 01:32:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:12][root][INFO] - Training Epoch: 6/10, step 275/574 completed (loss: 0.006143494509160519, acc: 1.0)
[2025-01-06 01:32:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:41][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.1688, device='cuda:0') eval_epoch_loss=tensor(0.7742, device='cuda:0') eval_epoch_acc=tensor(0.8434, device='cuda:0')
[2025-01-06 01:32:41][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:32:41][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:32:42][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_6_step_276_loss_0.7741731405258179/model.pt
[2025-01-06 01:32:42][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:32:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:42][root][INFO] - Training Epoch: 6/10, step 276/574 completed (loss: 0.10278045386075974, acc: 0.9655172228813171)
[2025-01-06 01:32:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:43][root][INFO] - Training Epoch: 6/10, step 277/574 completed (loss: 0.006748638115823269, acc: 1.0)
[2025-01-06 01:32:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:43][root][INFO] - Training Epoch: 6/10, step 278/574 completed (loss: 0.02653028815984726, acc: 1.0)
[2025-01-06 01:32:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:43][root][INFO] - Training Epoch: 6/10, step 279/574 completed (loss: 0.012382294982671738, acc: 1.0)
[2025-01-06 01:32:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:44][root][INFO] - Training Epoch: 6/10, step 280/574 completed (loss: 0.0070374878123402596, acc: 1.0)
[2025-01-06 01:32:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:44][root][INFO] - Training Epoch: 6/10, step 281/574 completed (loss: 0.11910182237625122, acc: 0.9397590160369873)
[2025-01-06 01:32:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:44][root][INFO] - Training Epoch: 6/10, step 282/574 completed (loss: 0.30774420499801636, acc: 0.9074074029922485)
[2025-01-06 01:32:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:45][root][INFO] - Training Epoch: 6/10, step 283/574 completed (loss: 0.07193896919488907, acc: 0.9736841917037964)
[2025-01-06 01:32:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:45][root][INFO] - Training Epoch: 6/10, step 284/574 completed (loss: 0.037578992545604706, acc: 1.0)
[2025-01-06 01:32:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:45][root][INFO] - Training Epoch: 6/10, step 285/574 completed (loss: 0.11379307508468628, acc: 0.949999988079071)
[2025-01-06 01:32:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:46][root][INFO] - Training Epoch: 6/10, step 286/574 completed (loss: 0.10909318923950195, acc: 0.9609375)
[2025-01-06 01:32:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:46][root][INFO] - Training Epoch: 6/10, step 287/574 completed (loss: 0.16687768697738647, acc: 0.9520000219345093)
[2025-01-06 01:32:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:47][root][INFO] - Training Epoch: 6/10, step 288/574 completed (loss: 0.1068543791770935, acc: 0.9560439586639404)
[2025-01-06 01:32:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:47][root][INFO] - Training Epoch: 6/10, step 289/574 completed (loss: 0.05441426485776901, acc: 0.9813664555549622)
[2025-01-06 01:32:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:47][root][INFO] - Training Epoch: 6/10, step 290/574 completed (loss: 0.23541338741779327, acc: 0.9329897165298462)
[2025-01-06 01:32:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:48][root][INFO] - Training Epoch: 6/10, step 291/574 completed (loss: 0.004299132153391838, acc: 1.0)
[2025-01-06 01:32:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:48][root][INFO] - Training Epoch: 6/10, step 292/574 completed (loss: 0.05241621285676956, acc: 0.976190447807312)
[2025-01-06 01:32:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:48][root][INFO] - Training Epoch: 6/10, step 293/574 completed (loss: 0.045889295637607574, acc: 0.982758641242981)
[2025-01-06 01:32:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:49][root][INFO] - Training Epoch: 6/10, step 294/574 completed (loss: 0.0370635911822319, acc: 0.9818181991577148)
[2025-01-06 01:32:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:49][root][INFO] - Training Epoch: 6/10, step 295/574 completed (loss: 0.18212302029132843, acc: 0.9484536051750183)
[2025-01-06 01:32:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:50][root][INFO] - Training Epoch: 6/10, step 296/574 completed (loss: 0.16754846274852753, acc: 0.9482758641242981)
[2025-01-06 01:32:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:50][root][INFO] - Training Epoch: 6/10, step 297/574 completed (loss: 0.009146523661911488, acc: 1.0)
[2025-01-06 01:32:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:51][root][INFO] - Training Epoch: 6/10, step 298/574 completed (loss: 0.08572299778461456, acc: 0.9736841917037964)
[2025-01-06 01:32:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:51][root][INFO] - Training Epoch: 6/10, step 299/574 completed (loss: 0.16222499310970306, acc: 0.9821428656578064)
[2025-01-06 01:32:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:51][root][INFO] - Training Epoch: 6/10, step 300/574 completed (loss: 0.005702645052224398, acc: 1.0)
[2025-01-06 01:32:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:52][root][INFO] - Training Epoch: 6/10, step 301/574 completed (loss: 0.07783173769712448, acc: 0.9811320900917053)
[2025-01-06 01:32:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:52][root][INFO] - Training Epoch: 6/10, step 302/574 completed (loss: 0.011260507628321648, acc: 1.0)
[2025-01-06 01:32:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:52][root][INFO] - Training Epoch: 6/10, step 303/574 completed (loss: 0.0067502595484256744, acc: 1.0)
[2025-01-06 01:32:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:53][root][INFO] - Training Epoch: 6/10, step 304/574 completed (loss: 0.01666737161576748, acc: 1.0)
[2025-01-06 01:32:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:53][root][INFO] - Training Epoch: 6/10, step 305/574 completed (loss: 0.04518572613596916, acc: 0.9672130942344666)
[2025-01-06 01:32:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:53][root][INFO] - Training Epoch: 6/10, step 306/574 completed (loss: 0.00464399391785264, acc: 1.0)
[2025-01-06 01:32:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:54][root][INFO] - Training Epoch: 6/10, step 307/574 completed (loss: 0.0007529830327257514, acc: 1.0)
[2025-01-06 01:32:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:54][root][INFO] - Training Epoch: 6/10, step 308/574 completed (loss: 0.024814048781991005, acc: 1.0)
[2025-01-06 01:32:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:55][root][INFO] - Training Epoch: 6/10, step 309/574 completed (loss: 0.044500455260276794, acc: 0.9583333134651184)
[2025-01-06 01:32:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:55][root][INFO] - Training Epoch: 6/10, step 310/574 completed (loss: 0.016330182552337646, acc: 1.0)
[2025-01-06 01:32:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:55][root][INFO] - Training Epoch: 6/10, step 311/574 completed (loss: 0.08074148744344711, acc: 0.9615384340286255)
[2025-01-06 01:32:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:56][root][INFO] - Training Epoch: 6/10, step 312/574 completed (loss: 0.03496692702174187, acc: 0.9795918464660645)
[2025-01-06 01:32:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:56][root][INFO] - Training Epoch: 6/10, step 313/574 completed (loss: 0.008213474415242672, acc: 1.0)
[2025-01-06 01:32:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:56][root][INFO] - Training Epoch: 6/10, step 314/574 completed (loss: 0.0016600271919742227, acc: 1.0)
[2025-01-06 01:32:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:57][root][INFO] - Training Epoch: 6/10, step 315/574 completed (loss: 0.006424207240343094, acc: 1.0)
[2025-01-06 01:32:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:57][root][INFO] - Training Epoch: 6/10, step 316/574 completed (loss: 0.029801633208990097, acc: 1.0)
[2025-01-06 01:32:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:57][root][INFO] - Training Epoch: 6/10, step 317/574 completed (loss: 0.007052839267998934, acc: 1.0)
[2025-01-06 01:32:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:58][root][INFO] - Training Epoch: 6/10, step 318/574 completed (loss: 0.017113305628299713, acc: 0.9903846383094788)
[2025-01-06 01:32:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:58][root][INFO] - Training Epoch: 6/10, step 319/574 completed (loss: 0.03795073181390762, acc: 0.9777777791023254)
[2025-01-06 01:32:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:58][root][INFO] - Training Epoch: 6/10, step 320/574 completed (loss: 0.0034215592313557863, acc: 1.0)
[2025-01-06 01:32:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:59][root][INFO] - Training Epoch: 6/10, step 321/574 completed (loss: 0.0016244164435192943, acc: 1.0)
[2025-01-06 01:32:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:32:59][root][INFO] - Training Epoch: 6/10, step 322/574 completed (loss: 0.10584460943937302, acc: 0.9629629850387573)
[2025-01-06 01:32:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:00][root][INFO] - Training Epoch: 6/10, step 323/574 completed (loss: 0.09594405442476273, acc: 0.9714285731315613)
[2025-01-06 01:33:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:00][root][INFO] - Training Epoch: 6/10, step 324/574 completed (loss: 0.12302635610103607, acc: 0.9487179517745972)
[2025-01-06 01:33:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:00][root][INFO] - Training Epoch: 6/10, step 325/574 completed (loss: 0.21803715825080872, acc: 0.9268292784690857)
[2025-01-06 01:33:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:01][root][INFO] - Training Epoch: 6/10, step 326/574 completed (loss: 0.049975864589214325, acc: 1.0)
[2025-01-06 01:33:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:01][root][INFO] - Training Epoch: 6/10, step 327/574 completed (loss: 0.16882078349590302, acc: 0.9473684430122375)
[2025-01-06 01:33:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:01][root][INFO] - Training Epoch: 6/10, step 328/574 completed (loss: 0.019254982471466064, acc: 1.0)
[2025-01-06 01:33:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:02][root][INFO] - Training Epoch: 6/10, step 329/574 completed (loss: 0.007463687099516392, acc: 1.0)
[2025-01-06 01:33:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:02][root][INFO] - Training Epoch: 6/10, step 330/574 completed (loss: 0.0011243380140513182, acc: 1.0)
[2025-01-06 01:33:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:02][root][INFO] - Training Epoch: 6/10, step 331/574 completed (loss: 0.032213110476732254, acc: 0.9838709831237793)
[2025-01-06 01:33:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:03][root][INFO] - Training Epoch: 6/10, step 332/574 completed (loss: 0.024699347093701363, acc: 0.9824561476707458)
[2025-01-06 01:33:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:03][root][INFO] - Training Epoch: 6/10, step 333/574 completed (loss: 0.0793459564447403, acc: 0.96875)
[2025-01-06 01:33:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:04][root][INFO] - Training Epoch: 6/10, step 334/574 completed (loss: 0.004616448190063238, acc: 1.0)
[2025-01-06 01:33:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:04][root][INFO] - Training Epoch: 6/10, step 335/574 completed (loss: 0.005080194678157568, acc: 1.0)
[2025-01-06 01:33:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:04][root][INFO] - Training Epoch: 6/10, step 336/574 completed (loss: 0.15831510722637177, acc: 0.9399999976158142)
[2025-01-06 01:33:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:05][root][INFO] - Training Epoch: 6/10, step 337/574 completed (loss: 0.08464670926332474, acc: 0.977011501789093)
[2025-01-06 01:33:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:05][root][INFO] - Training Epoch: 6/10, step 338/574 completed (loss: 0.40119507908821106, acc: 0.8617021441459656)
[2025-01-06 01:33:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:05][root][INFO] - Training Epoch: 6/10, step 339/574 completed (loss: 0.10754992067813873, acc: 0.9879518151283264)
[2025-01-06 01:33:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:06][root][INFO] - Training Epoch: 6/10, step 340/574 completed (loss: 0.011337706819176674, acc: 1.0)
[2025-01-06 01:33:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:06][root][INFO] - Training Epoch: 6/10, step 341/574 completed (loss: 0.10827485471963882, acc: 0.9487179517745972)
[2025-01-06 01:33:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:06][root][INFO] - Training Epoch: 6/10, step 342/574 completed (loss: 0.2397916167974472, acc: 0.9638554453849792)
[2025-01-06 01:33:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:07][root][INFO] - Training Epoch: 6/10, step 343/574 completed (loss: 0.05721766874194145, acc: 0.9811320900917053)
[2025-01-06 01:33:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:07][root][INFO] - Training Epoch: 6/10, step 344/574 completed (loss: 0.00411476194858551, acc: 1.0)
[2025-01-06 01:33:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:07][root][INFO] - Training Epoch: 6/10, step 345/574 completed (loss: 0.02766335755586624, acc: 0.9803921580314636)
[2025-01-06 01:33:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:08][root][INFO] - Training Epoch: 6/10, step 346/574 completed (loss: 0.014090297743678093, acc: 1.0)
[2025-01-06 01:33:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:08][root][INFO] - Training Epoch: 6/10, step 347/574 completed (loss: 0.0032794601283967495, acc: 1.0)
[2025-01-06 01:33:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:08][root][INFO] - Training Epoch: 6/10, step 348/574 completed (loss: 0.008913015015423298, acc: 1.0)
[2025-01-06 01:33:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:09][root][INFO] - Training Epoch: 6/10, step 349/574 completed (loss: 0.3275739848613739, acc: 0.8888888955116272)
[2025-01-06 01:33:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:09][root][INFO] - Training Epoch: 6/10, step 350/574 completed (loss: 0.321812242269516, acc: 0.9069767594337463)
[2025-01-06 01:33:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:09][root][INFO] - Training Epoch: 6/10, step 351/574 completed (loss: 0.07366607338190079, acc: 0.9487179517745972)
[2025-01-06 01:33:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:10][root][INFO] - Training Epoch: 6/10, step 352/574 completed (loss: 0.03825896233320236, acc: 1.0)
[2025-01-06 01:33:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:10][root][INFO] - Training Epoch: 6/10, step 353/574 completed (loss: 0.016304904595017433, acc: 1.0)
[2025-01-06 01:33:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:10][root][INFO] - Training Epoch: 6/10, step 354/574 completed (loss: 0.011945128440856934, acc: 1.0)
[2025-01-06 01:33:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:11][root][INFO] - Training Epoch: 6/10, step 355/574 completed (loss: 0.18585726618766785, acc: 0.9560439586639404)
[2025-01-06 01:33:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:11][root][INFO] - Training Epoch: 6/10, step 356/574 completed (loss: 0.14753605425357819, acc: 0.947826087474823)
[2025-01-06 01:33:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:12][root][INFO] - Training Epoch: 6/10, step 357/574 completed (loss: 0.08494336158037186, acc: 0.967391312122345)
[2025-01-06 01:33:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:12][root][INFO] - Training Epoch: 6/10, step 358/574 completed (loss: 0.033474043011665344, acc: 1.0)
[2025-01-06 01:33:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:12][root][INFO] - Training Epoch: 6/10, step 359/574 completed (loss: 0.0014425860717892647, acc: 1.0)
[2025-01-06 01:33:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:13][root][INFO] - Training Epoch: 6/10, step 360/574 completed (loss: 0.0031132192816585302, acc: 1.0)
[2025-01-06 01:33:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:13][root][INFO] - Training Epoch: 6/10, step 361/574 completed (loss: 0.0136609161272645, acc: 1.0)
[2025-01-06 01:33:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:13][root][INFO] - Training Epoch: 6/10, step 362/574 completed (loss: 0.007079548202455044, acc: 1.0)
[2025-01-06 01:33:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:14][root][INFO] - Training Epoch: 6/10, step 363/574 completed (loss: 0.015617243945598602, acc: 1.0)
[2025-01-06 01:33:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:14][root][INFO] - Training Epoch: 6/10, step 364/574 completed (loss: 0.01963130570948124, acc: 1.0)
[2025-01-06 01:33:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:15][root][INFO] - Training Epoch: 6/10, step 365/574 completed (loss: 0.017830319702625275, acc: 1.0)
[2025-01-06 01:33:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:15][root][INFO] - Training Epoch: 6/10, step 366/574 completed (loss: 0.00047169686877168715, acc: 1.0)
[2025-01-06 01:33:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:15][root][INFO] - Training Epoch: 6/10, step 367/574 completed (loss: 0.036772605031728745, acc: 0.95652174949646)
[2025-01-06 01:33:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:15][root][INFO] - Training Epoch: 6/10, step 368/574 completed (loss: 0.0074937790632247925, acc: 1.0)
[2025-01-06 01:33:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:16][root][INFO] - Training Epoch: 6/10, step 369/574 completed (loss: 0.5571556091308594, acc: 0.9375)
[2025-01-06 01:33:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:16][root][INFO] - Training Epoch: 6/10, step 370/574 completed (loss: 0.1721702218055725, acc: 0.9575757384300232)
[2025-01-06 01:33:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:17][root][INFO] - Training Epoch: 6/10, step 371/574 completed (loss: 0.09283622354269028, acc: 0.9811320900917053)
[2025-01-06 01:33:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:17][root][INFO] - Training Epoch: 6/10, step 372/574 completed (loss: 0.021678809076547623, acc: 1.0)
[2025-01-06 01:33:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:18][root][INFO] - Training Epoch: 6/10, step 373/574 completed (loss: 0.05487345904111862, acc: 0.9821428656578064)
[2025-01-06 01:33:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:18][root][INFO] - Training Epoch: 6/10, step 374/574 completed (loss: 0.005277516786009073, acc: 1.0)
[2025-01-06 01:33:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:19][root][INFO] - Training Epoch: 6/10, step 375/574 completed (loss: 5.768975097453222e-05, acc: 1.0)
[2025-01-06 01:33:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:19][root][INFO] - Training Epoch: 6/10, step 376/574 completed (loss: 0.0018339331727474928, acc: 1.0)
[2025-01-06 01:33:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:19][root][INFO] - Training Epoch: 6/10, step 377/574 completed (loss: 0.03645722195506096, acc: 0.9791666865348816)
[2025-01-06 01:33:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:20][root][INFO] - Training Epoch: 6/10, step 378/574 completed (loss: 0.04974592477083206, acc: 0.9789473414421082)
[2025-01-06 01:33:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:20][root][INFO] - Training Epoch: 6/10, step 379/574 completed (loss: 0.06765305995941162, acc: 0.9820359349250793)
[2025-01-06 01:33:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:21][root][INFO] - Training Epoch: 6/10, step 380/574 completed (loss: 0.1481207311153412, acc: 0.9624060392379761)
[2025-01-06 01:33:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:22][root][INFO] - Training Epoch: 6/10, step 381/574 completed (loss: 0.23079340159893036, acc: 0.9197860956192017)
[2025-01-06 01:33:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:23][root][INFO] - Training Epoch: 6/10, step 382/574 completed (loss: 0.024394946172833443, acc: 0.9909909963607788)
[2025-01-06 01:33:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:23][root][INFO] - Training Epoch: 6/10, step 383/574 completed (loss: 0.015704449266195297, acc: 1.0)
[2025-01-06 01:33:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:23][root][INFO] - Training Epoch: 6/10, step 384/574 completed (loss: 0.003938536159694195, acc: 1.0)
[2025-01-06 01:33:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:24][root][INFO] - Training Epoch: 6/10, step 385/574 completed (loss: 0.07293940335512161, acc: 0.96875)
[2025-01-06 01:33:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:24][root][INFO] - Training Epoch: 6/10, step 386/574 completed (loss: 0.00027825275901705027, acc: 1.0)
[2025-01-06 01:33:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:24][root][INFO] - Training Epoch: 6/10, step 387/574 completed (loss: 0.0003093126288149506, acc: 1.0)
[2025-01-06 01:33:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:25][root][INFO] - Training Epoch: 6/10, step 388/574 completed (loss: 0.00030087196500971913, acc: 1.0)
[2025-01-06 01:33:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:25][root][INFO] - Training Epoch: 6/10, step 389/574 completed (loss: 0.0003158801409881562, acc: 1.0)
[2025-01-06 01:33:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:25][root][INFO] - Training Epoch: 6/10, step 390/574 completed (loss: 0.038304537534713745, acc: 1.0)
[2025-01-06 01:33:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:26][root][INFO] - Training Epoch: 6/10, step 391/574 completed (loss: 0.0765577182173729, acc: 0.9629629850387573)
[2025-01-06 01:33:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:26][root][INFO] - Training Epoch: 6/10, step 392/574 completed (loss: 0.26807278394699097, acc: 0.8737863898277283)
[2025-01-06 01:33:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:26][root][INFO] - Training Epoch: 6/10, step 393/574 completed (loss: 0.31856846809387207, acc: 0.9264705777168274)
[2025-01-06 01:33:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:27][root][INFO] - Training Epoch: 6/10, step 394/574 completed (loss: 0.19633714854717255, acc: 0.9200000166893005)
[2025-01-06 01:33:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:27][root][INFO] - Training Epoch: 6/10, step 395/574 completed (loss: 0.15157100558280945, acc: 0.9513888955116272)
[2025-01-06 01:33:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:28][root][INFO] - Training Epoch: 6/10, step 396/574 completed (loss: 0.05287773162126541, acc: 0.9767441749572754)
[2025-01-06 01:33:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:28][root][INFO] - Training Epoch: 6/10, step 397/574 completed (loss: 0.019059354439377785, acc: 1.0)
[2025-01-06 01:33:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:28][root][INFO] - Training Epoch: 6/10, step 398/574 completed (loss: 0.03688112273812294, acc: 0.9767441749572754)
[2025-01-06 01:33:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:29][root][INFO] - Training Epoch: 6/10, step 399/574 completed (loss: 0.0520535372197628, acc: 0.9599999785423279)
[2025-01-06 01:33:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:29][root][INFO] - Training Epoch: 6/10, step 400/574 completed (loss: 0.07973217964172363, acc: 0.970588207244873)
[2025-01-06 01:33:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:29][root][INFO] - Training Epoch: 6/10, step 401/574 completed (loss: 0.0974743664264679, acc: 0.9466666579246521)
[2025-01-06 01:33:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:30][root][INFO] - Training Epoch: 6/10, step 402/574 completed (loss: 0.06665624678134918, acc: 0.9696969985961914)
[2025-01-06 01:33:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:30][root][INFO] - Training Epoch: 6/10, step 403/574 completed (loss: 0.2049250453710556, acc: 0.9696969985961914)
[2025-01-06 01:33:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:30][root][INFO] - Training Epoch: 6/10, step 404/574 completed (loss: 0.013647007755935192, acc: 1.0)
[2025-01-06 01:33:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:31][root][INFO] - Training Epoch: 6/10, step 405/574 completed (loss: 0.06309269368648529, acc: 0.9629629850387573)
[2025-01-06 01:33:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:31][root][INFO] - Training Epoch: 6/10, step 406/574 completed (loss: 0.0005689347162842751, acc: 1.0)
[2025-01-06 01:33:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:31][root][INFO] - Training Epoch: 6/10, step 407/574 completed (loss: 0.0024057577829807997, acc: 1.0)
[2025-01-06 01:33:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:32][root][INFO] - Training Epoch: 6/10, step 408/574 completed (loss: 0.28494465351104736, acc: 0.9629629850387573)
[2025-01-06 01:33:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:32][root][INFO] - Training Epoch: 6/10, step 409/574 completed (loss: 0.006250203587114811, acc: 1.0)
[2025-01-06 01:33:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:33][root][INFO] - Training Epoch: 6/10, step 410/574 completed (loss: 0.00683211162686348, acc: 1.0)
[2025-01-06 01:33:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:33][root][INFO] - Training Epoch: 6/10, step 411/574 completed (loss: 0.012379731051623821, acc: 1.0)
[2025-01-06 01:33:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:33][root][INFO] - Training Epoch: 6/10, step 412/574 completed (loss: 0.0051358952187001705, acc: 1.0)
[2025-01-06 01:33:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:34][root][INFO] - Training Epoch: 6/10, step 413/574 completed (loss: 0.010620251297950745, acc: 1.0)
[2025-01-06 01:33:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:34][root][INFO] - Training Epoch: 6/10, step 414/574 completed (loss: 0.002715068869292736, acc: 1.0)
[2025-01-06 01:33:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:34][root][INFO] - Training Epoch: 6/10, step 415/574 completed (loss: 0.1901063770055771, acc: 0.9607843160629272)
[2025-01-06 01:33:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:35][root][INFO] - Training Epoch: 6/10, step 416/574 completed (loss: 0.06578825414180756, acc: 0.9615384340286255)
[2025-01-06 01:33:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:35][root][INFO] - Training Epoch: 6/10, step 417/574 completed (loss: 0.33157864212989807, acc: 0.8888888955116272)
[2025-01-06 01:33:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:35][root][INFO] - Training Epoch: 6/10, step 418/574 completed (loss: 0.10794515907764435, acc: 0.949999988079071)
[2025-01-06 01:33:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:33:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:06][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.3787, device='cuda:0') eval_epoch_loss=tensor(0.8666, device='cuda:0') eval_epoch_acc=tensor(0.8354, device='cuda:0')
[2025-01-06 01:34:06][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:34:06][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:34:06][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_6_step_419_loss_0.8665545582771301/model.pt
[2025-01-06 01:34:06][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:34:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:07][root][INFO] - Training Epoch: 6/10, step 419/574 completed (loss: 0.0403631255030632, acc: 0.949999988079071)
[2025-01-06 01:34:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:07][root][INFO] - Training Epoch: 6/10, step 420/574 completed (loss: 0.0008651833049952984, acc: 1.0)
[2025-01-06 01:34:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:07][root][INFO] - Training Epoch: 6/10, step 421/574 completed (loss: 0.06004168093204498, acc: 0.9666666388511658)
[2025-01-06 01:34:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:08][root][INFO] - Training Epoch: 6/10, step 422/574 completed (loss: 0.03393124043941498, acc: 1.0)
[2025-01-06 01:34:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:08][root][INFO] - Training Epoch: 6/10, step 423/574 completed (loss: 0.0268156286329031, acc: 1.0)
[2025-01-06 01:34:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:08][root][INFO] - Training Epoch: 6/10, step 424/574 completed (loss: 0.5041995048522949, acc: 0.8888888955116272)
[2025-01-06 01:34:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:09][root][INFO] - Training Epoch: 6/10, step 425/574 completed (loss: 0.0038165340665727854, acc: 1.0)
[2025-01-06 01:34:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:09][root][INFO] - Training Epoch: 6/10, step 426/574 completed (loss: 0.002311698393896222, acc: 1.0)
[2025-01-06 01:34:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:09][root][INFO] - Training Epoch: 6/10, step 427/574 completed (loss: 0.019584326073527336, acc: 1.0)
[2025-01-06 01:34:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:10][root][INFO] - Training Epoch: 6/10, step 428/574 completed (loss: 0.07819552719593048, acc: 0.9629629850387573)
[2025-01-06 01:34:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:10][root][INFO] - Training Epoch: 6/10, step 429/574 completed (loss: 0.06525228917598724, acc: 0.95652174949646)
[2025-01-06 01:34:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:10][root][INFO] - Training Epoch: 6/10, step 430/574 completed (loss: 0.002940624486654997, acc: 1.0)
[2025-01-06 01:34:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:11][root][INFO] - Training Epoch: 6/10, step 431/574 completed (loss: 0.010417391546070576, acc: 1.0)
[2025-01-06 01:34:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:11][root][INFO] - Training Epoch: 6/10, step 432/574 completed (loss: 0.0014634733088314533, acc: 1.0)
[2025-01-06 01:34:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:11][root][INFO] - Training Epoch: 6/10, step 433/574 completed (loss: 0.2998598515987396, acc: 0.9722222089767456)
[2025-01-06 01:34:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:12][root][INFO] - Training Epoch: 6/10, step 434/574 completed (loss: 0.0012305256677791476, acc: 1.0)
[2025-01-06 01:34:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:12][root][INFO] - Training Epoch: 6/10, step 435/574 completed (loss: 0.31058669090270996, acc: 0.9696969985961914)
[2025-01-06 01:34:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:12][root][INFO] - Training Epoch: 6/10, step 436/574 completed (loss: 0.0741790384054184, acc: 0.9444444179534912)
[2025-01-06 01:34:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:13][root][INFO] - Training Epoch: 6/10, step 437/574 completed (loss: 0.004655696451663971, acc: 1.0)
[2025-01-06 01:34:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:13][root][INFO] - Training Epoch: 6/10, step 438/574 completed (loss: 0.00584357650950551, acc: 1.0)
[2025-01-06 01:34:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:13][root][INFO] - Training Epoch: 6/10, step 439/574 completed (loss: 0.09041236340999603, acc: 0.9487179517745972)
[2025-01-06 01:34:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:14][root][INFO] - Training Epoch: 6/10, step 440/574 completed (loss: 0.06544823944568634, acc: 0.9696969985961914)
[2025-01-06 01:34:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:15][root][INFO] - Training Epoch: 6/10, step 441/574 completed (loss: 0.3161803185939789, acc: 0.8960000276565552)
[2025-01-06 01:34:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:15][root][INFO] - Training Epoch: 6/10, step 442/574 completed (loss: 0.21029715240001678, acc: 0.9516128897666931)
[2025-01-06 01:34:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:16][root][INFO] - Training Epoch: 6/10, step 443/574 completed (loss: 0.3050900995731354, acc: 0.9054726362228394)
[2025-01-06 01:34:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:16][root][INFO] - Training Epoch: 6/10, step 444/574 completed (loss: 0.01969217136502266, acc: 1.0)
[2025-01-06 01:34:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:16][root][INFO] - Training Epoch: 6/10, step 445/574 completed (loss: 0.02409030869603157, acc: 1.0)
[2025-01-06 01:34:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:17][root][INFO] - Training Epoch: 6/10, step 446/574 completed (loss: 0.0667891576886177, acc: 1.0)
[2025-01-06 01:34:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:17][root][INFO] - Training Epoch: 6/10, step 447/574 completed (loss: 0.011525972746312618, acc: 1.0)
[2025-01-06 01:34:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:17][root][INFO] - Training Epoch: 6/10, step 448/574 completed (loss: 0.005001655779778957, acc: 1.0)
[2025-01-06 01:34:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:18][root][INFO] - Training Epoch: 6/10, step 449/574 completed (loss: 0.04772862792015076, acc: 0.9850746393203735)
[2025-01-06 01:34:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:18][root][INFO] - Training Epoch: 6/10, step 450/574 completed (loss: 0.008020654320716858, acc: 1.0)
[2025-01-06 01:34:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:19][root][INFO] - Training Epoch: 6/10, step 451/574 completed (loss: 0.03955338895320892, acc: 0.989130437374115)
[2025-01-06 01:34:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:19][root][INFO] - Training Epoch: 6/10, step 452/574 completed (loss: 0.05055515095591545, acc: 0.9871794581413269)
[2025-01-06 01:34:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:19][root][INFO] - Training Epoch: 6/10, step 453/574 completed (loss: 0.10913335531949997, acc: 0.9605262875556946)
[2025-01-06 01:34:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:20][root][INFO] - Training Epoch: 6/10, step 454/574 completed (loss: 0.07201651483774185, acc: 0.9795918464660645)
[2025-01-06 01:34:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:20][root][INFO] - Training Epoch: 6/10, step 455/574 completed (loss: 0.01937311328947544, acc: 1.0)
[2025-01-06 01:34:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:20][root][INFO] - Training Epoch: 6/10, step 456/574 completed (loss: 0.05822399631142616, acc: 0.9896907210350037)
[2025-01-06 01:34:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:21][root][INFO] - Training Epoch: 6/10, step 457/574 completed (loss: 0.004780067130923271, acc: 1.0)
[2025-01-06 01:34:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:21][root][INFO] - Training Epoch: 6/10, step 458/574 completed (loss: 0.09583868831396103, acc: 0.9709302186965942)
[2025-01-06 01:34:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:21][root][INFO] - Training Epoch: 6/10, step 459/574 completed (loss: 0.028777381405234337, acc: 0.9821428656578064)
[2025-01-06 01:34:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:22][root][INFO] - Training Epoch: 6/10, step 460/574 completed (loss: 0.06011819839477539, acc: 0.9876543283462524)
[2025-01-06 01:34:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:22][root][INFO] - Training Epoch: 6/10, step 461/574 completed (loss: 0.18969778716564178, acc: 0.9444444179534912)
[2025-01-06 01:34:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:23][root][INFO] - Training Epoch: 6/10, step 462/574 completed (loss: 0.0016833062982186675, acc: 1.0)
[2025-01-06 01:34:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:23][root][INFO] - Training Epoch: 6/10, step 463/574 completed (loss: 0.01620902493596077, acc: 1.0)
[2025-01-06 01:34:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:23][root][INFO] - Training Epoch: 6/10, step 464/574 completed (loss: 0.1164911761879921, acc: 0.95652174949646)
[2025-01-06 01:34:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:24][root][INFO] - Training Epoch: 6/10, step 465/574 completed (loss: 0.048767607659101486, acc: 0.988095223903656)
[2025-01-06 01:34:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:24][root][INFO] - Training Epoch: 6/10, step 466/574 completed (loss: 0.17328746616840363, acc: 0.9397590160369873)
[2025-01-06 01:34:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:24][root][INFO] - Training Epoch: 6/10, step 467/574 completed (loss: 0.022092659026384354, acc: 1.0)
[2025-01-06 01:34:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:25][root][INFO] - Training Epoch: 6/10, step 468/574 completed (loss: 0.1390576809644699, acc: 0.9611650705337524)
[2025-01-06 01:34:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:25][root][INFO] - Training Epoch: 6/10, step 469/574 completed (loss: 0.05604574456810951, acc: 0.9918699264526367)
[2025-01-06 01:34:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:25][root][INFO] - Training Epoch: 6/10, step 470/574 completed (loss: 0.002740591997280717, acc: 1.0)
[2025-01-06 01:34:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:26][root][INFO] - Training Epoch: 6/10, step 471/574 completed (loss: 0.033907923847436905, acc: 1.0)
[2025-01-06 01:34:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:26][root][INFO] - Training Epoch: 6/10, step 472/574 completed (loss: 0.19647091627120972, acc: 0.9215686321258545)
[2025-01-06 01:34:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:26][root][INFO] - Training Epoch: 6/10, step 473/574 completed (loss: 0.2770191431045532, acc: 0.9213973879814148)
[2025-01-06 01:34:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:27][root][INFO] - Training Epoch: 6/10, step 474/574 completed (loss: 0.06410545110702515, acc: 0.9895833134651184)
[2025-01-06 01:34:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:27][root][INFO] - Training Epoch: 6/10, step 475/574 completed (loss: 0.12280554324388504, acc: 0.9570552110671997)
[2025-01-06 01:34:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:28][root][INFO] - Training Epoch: 6/10, step 476/574 completed (loss: 0.09822164475917816, acc: 0.9784172773361206)
[2025-01-06 01:34:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:28][root][INFO] - Training Epoch: 6/10, step 477/574 completed (loss: 0.22111378610134125, acc: 0.929648220539093)
[2025-01-06 01:34:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:28][root][INFO] - Training Epoch: 6/10, step 478/574 completed (loss: 0.03624119982123375, acc: 1.0)
[2025-01-06 01:34:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:29][root][INFO] - Training Epoch: 6/10, step 479/574 completed (loss: 0.10281414538621902, acc: 0.9696969985961914)
[2025-01-06 01:34:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:29][root][INFO] - Training Epoch: 6/10, step 480/574 completed (loss: 0.07865174859762192, acc: 0.9629629850387573)
[2025-01-06 01:34:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:29][root][INFO] - Training Epoch: 6/10, step 481/574 completed (loss: 0.04255694895982742, acc: 1.0)
[2025-01-06 01:34:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:30][root][INFO] - Training Epoch: 6/10, step 482/574 completed (loss: 0.01955919899046421, acc: 1.0)
[2025-01-06 01:34:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:30][root][INFO] - Training Epoch: 6/10, step 483/574 completed (loss: 0.20777660608291626, acc: 0.9482758641242981)
[2025-01-06 01:34:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:30][root][INFO] - Training Epoch: 6/10, step 484/574 completed (loss: 0.009448567405343056, acc: 1.0)
[2025-01-06 01:34:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:31][root][INFO] - Training Epoch: 6/10, step 485/574 completed (loss: 0.048354994505643845, acc: 1.0)
[2025-01-06 01:34:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:31][root][INFO] - Training Epoch: 6/10, step 486/574 completed (loss: 0.052473898977041245, acc: 1.0)
[2025-01-06 01:34:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:31][root][INFO] - Training Epoch: 6/10, step 487/574 completed (loss: 0.0580231174826622, acc: 1.0)
[2025-01-06 01:34:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:32][root][INFO] - Training Epoch: 6/10, step 488/574 completed (loss: 0.005302578676491976, acc: 1.0)
[2025-01-06 01:34:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:32][root][INFO] - Training Epoch: 6/10, step 489/574 completed (loss: 0.09929175674915314, acc: 0.9384615421295166)
[2025-01-06 01:34:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:32][root][INFO] - Training Epoch: 6/10, step 490/574 completed (loss: 0.007131831720471382, acc: 1.0)
[2025-01-06 01:34:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:33][root][INFO] - Training Epoch: 6/10, step 491/574 completed (loss: 0.11710484325885773, acc: 0.9655172228813171)
[2025-01-06 01:34:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:33][root][INFO] - Training Epoch: 6/10, step 492/574 completed (loss: 0.07066133618354797, acc: 0.9607843160629272)
[2025-01-06 01:34:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:33][root][INFO] - Training Epoch: 6/10, step 493/574 completed (loss: 0.21575890481472015, acc: 0.931034505367279)
[2025-01-06 01:34:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:34][root][INFO] - Training Epoch: 6/10, step 494/574 completed (loss: 0.010928419418632984, acc: 1.0)
[2025-01-06 01:34:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:34][root][INFO] - Training Epoch: 6/10, step 495/574 completed (loss: 0.018533017486333847, acc: 1.0)
[2025-01-06 01:34:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:34][root][INFO] - Training Epoch: 6/10, step 496/574 completed (loss: 0.1753067672252655, acc: 0.9553571343421936)
[2025-01-06 01:34:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:35][root][INFO] - Training Epoch: 6/10, step 497/574 completed (loss: 0.07102975249290466, acc: 0.9887640476226807)
[2025-01-06 01:34:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:35][root][INFO] - Training Epoch: 6/10, step 498/574 completed (loss: 0.14242906868457794, acc: 0.932584285736084)
[2025-01-06 01:34:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:36][root][INFO] - Training Epoch: 6/10, step 499/574 completed (loss: 0.30605366826057434, acc: 0.9078013896942139)
[2025-01-06 01:34:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:36][root][INFO] - Training Epoch: 6/10, step 500/574 completed (loss: 0.10286932438611984, acc: 0.95652174949646)
[2025-01-06 01:34:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:36][root][INFO] - Training Epoch: 6/10, step 501/574 completed (loss: 0.3207860589027405, acc: 0.9200000166893005)
[2025-01-06 01:34:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:37][root][INFO] - Training Epoch: 6/10, step 502/574 completed (loss: 0.0019147115526720881, acc: 1.0)
[2025-01-06 01:34:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:37][root][INFO] - Training Epoch: 6/10, step 503/574 completed (loss: 0.005275324452668428, acc: 1.0)
[2025-01-06 01:34:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:37][root][INFO] - Training Epoch: 6/10, step 504/574 completed (loss: 0.019997937604784966, acc: 1.0)
[2025-01-06 01:34:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:38][root][INFO] - Training Epoch: 6/10, step 505/574 completed (loss: 0.14181473851203918, acc: 0.9433962106704712)
[2025-01-06 01:34:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:38][root][INFO] - Training Epoch: 6/10, step 506/574 completed (loss: 0.1292939931154251, acc: 0.9655172228813171)
[2025-01-06 01:34:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:39][root][INFO] - Training Epoch: 6/10, step 507/574 completed (loss: 0.26838618516921997, acc: 0.8918918967247009)
[2025-01-06 01:34:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:39][root][INFO] - Training Epoch: 6/10, step 508/574 completed (loss: 0.18714389204978943, acc: 0.9577465057373047)
[2025-01-06 01:34:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:39][root][INFO] - Training Epoch: 6/10, step 509/574 completed (loss: 0.000955109135247767, acc: 1.0)
[2025-01-06 01:34:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:40][root][INFO] - Training Epoch: 6/10, step 510/574 completed (loss: 0.0064267488196492195, acc: 1.0)
[2025-01-06 01:34:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:40][root][INFO] - Training Epoch: 6/10, step 511/574 completed (loss: 0.010189591906964779, acc: 1.0)
[2025-01-06 01:34:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:43][root][INFO] - Training Epoch: 6/10, step 512/574 completed (loss: 0.5551205277442932, acc: 0.8571428656578064)
[2025-01-06 01:34:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:44][root][INFO] - Training Epoch: 6/10, step 513/574 completed (loss: 0.08642581105232239, acc: 0.9682539701461792)
[2025-01-06 01:34:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:44][root][INFO] - Training Epoch: 6/10, step 514/574 completed (loss: 0.20126888155937195, acc: 0.8928571343421936)
[2025-01-06 01:34:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:44][root][INFO] - Training Epoch: 6/10, step 515/574 completed (loss: 0.022330408915877342, acc: 0.9833333492279053)
[2025-01-06 01:34:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:45][root][INFO] - Training Epoch: 6/10, step 516/574 completed (loss: 0.0345921590924263, acc: 0.9722222089767456)
[2025-01-06 01:34:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:45][root][INFO] - Training Epoch: 6/10, step 517/574 completed (loss: 0.0004664086445700377, acc: 1.0)
[2025-01-06 01:34:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:46][root][INFO] - Training Epoch: 6/10, step 518/574 completed (loss: 0.030905498191714287, acc: 1.0)
[2025-01-06 01:34:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:46][root][INFO] - Training Epoch: 6/10, step 519/574 completed (loss: 0.05054134130477905, acc: 1.0)
[2025-01-06 01:34:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:46][root][INFO] - Training Epoch: 6/10, step 520/574 completed (loss: 0.5095783472061157, acc: 0.9259259104728699)
[2025-01-06 01:34:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:47][root][INFO] - Training Epoch: 6/10, step 521/574 completed (loss: 0.3542916774749756, acc: 0.8813559412956238)
[2025-01-06 01:34:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:48][root][INFO] - Training Epoch: 6/10, step 522/574 completed (loss: 0.06299909949302673, acc: 0.9850746393203735)
[2025-01-06 01:34:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:48][root][INFO] - Training Epoch: 6/10, step 523/574 completed (loss: 0.17304140329360962, acc: 0.956204354763031)
[2025-01-06 01:34:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:49][root][INFO] - Training Epoch: 6/10, step 524/574 completed (loss: 0.4118534028530121, acc: 0.875)
[2025-01-06 01:34:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:49][root][INFO] - Training Epoch: 6/10, step 525/574 completed (loss: 0.02921699546277523, acc: 0.9814814925193787)
[2025-01-06 01:34:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:49][root][INFO] - Training Epoch: 6/10, step 526/574 completed (loss: 0.016085751354694366, acc: 1.0)
[2025-01-06 01:34:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:50][root][INFO] - Training Epoch: 6/10, step 527/574 completed (loss: 0.0896524041891098, acc: 0.9523809552192688)
[2025-01-06 01:34:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:50][root][INFO] - Training Epoch: 6/10, step 528/574 completed (loss: 0.1842847317457199, acc: 0.9180327653884888)
[2025-01-06 01:34:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:50][root][INFO] - Training Epoch: 6/10, step 529/574 completed (loss: 0.035196974873542786, acc: 1.0)
[2025-01-06 01:34:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:51][root][INFO] - Training Epoch: 6/10, step 530/574 completed (loss: 0.13062314689159393, acc: 0.930232584476471)
[2025-01-06 01:34:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:51][root][INFO] - Training Epoch: 6/10, step 531/574 completed (loss: 0.19665144383907318, acc: 0.9545454382896423)
[2025-01-06 01:34:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:51][root][INFO] - Training Epoch: 6/10, step 532/574 completed (loss: 0.07910272479057312, acc: 0.9811320900917053)
[2025-01-06 01:34:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:52][root][INFO] - Training Epoch: 6/10, step 533/574 completed (loss: 0.030659638345241547, acc: 1.0)
[2025-01-06 01:34:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:52][root][INFO] - Training Epoch: 6/10, step 534/574 completed (loss: 0.023824870586395264, acc: 1.0)
[2025-01-06 01:34:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:52][root][INFO] - Training Epoch: 6/10, step 535/574 completed (loss: 0.003496424062177539, acc: 1.0)
[2025-01-06 01:34:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:53][root][INFO] - Training Epoch: 6/10, step 536/574 completed (loss: 0.0031364017631858587, acc: 1.0)
[2025-01-06 01:34:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:53][root][INFO] - Training Epoch: 6/10, step 537/574 completed (loss: 0.06455197930335999, acc: 0.9846153855323792)
[2025-01-06 01:34:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:54][root][INFO] - Training Epoch: 6/10, step 538/574 completed (loss: 0.0736374482512474, acc: 0.984375)
[2025-01-06 01:34:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:54][root][INFO] - Training Epoch: 6/10, step 539/574 completed (loss: 0.23044487833976746, acc: 0.9375)
[2025-01-06 01:34:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:54][root][INFO] - Training Epoch: 6/10, step 540/574 completed (loss: 0.028455840423703194, acc: 1.0)
[2025-01-06 01:34:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:55][root][INFO] - Training Epoch: 6/10, step 541/574 completed (loss: 0.00232579349540174, acc: 1.0)
[2025-01-06 01:34:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:55][root][INFO] - Training Epoch: 6/10, step 542/574 completed (loss: 0.04843852296471596, acc: 0.9677419066429138)
[2025-01-06 01:34:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:55][root][INFO] - Training Epoch: 6/10, step 543/574 completed (loss: 0.008078722283244133, acc: 1.0)
[2025-01-06 01:34:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:56][root][INFO] - Training Epoch: 6/10, step 544/574 completed (loss: 0.020670881494879723, acc: 1.0)
[2025-01-06 01:34:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:56][root][INFO] - Training Epoch: 6/10, step 545/574 completed (loss: 0.03377458080649376, acc: 0.9756097793579102)
[2025-01-06 01:34:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:56][root][INFO] - Training Epoch: 6/10, step 546/574 completed (loss: 0.0012175824958831072, acc: 1.0)
[2025-01-06 01:34:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:57][root][INFO] - Training Epoch: 6/10, step 547/574 completed (loss: 0.0034773023799061775, acc: 1.0)
[2025-01-06 01:34:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:57][root][INFO] - Training Epoch: 6/10, step 548/574 completed (loss: 0.30864179134368896, acc: 0.9354838728904724)
[2025-01-06 01:34:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:57][root][INFO] - Training Epoch: 6/10, step 549/574 completed (loss: 0.0006257833447307348, acc: 1.0)
[2025-01-06 01:34:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:58][root][INFO] - Training Epoch: 6/10, step 550/574 completed (loss: 0.03985165059566498, acc: 1.0)
[2025-01-06 01:34:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:58][root][INFO] - Training Epoch: 6/10, step 551/574 completed (loss: 0.0026926775462925434, acc: 1.0)
[2025-01-06 01:34:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:59][root][INFO] - Training Epoch: 6/10, step 552/574 completed (loss: 0.07615789771080017, acc: 0.9714285731315613)
[2025-01-06 01:34:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:59][root][INFO] - Training Epoch: 6/10, step 553/574 completed (loss: 0.05246060714125633, acc: 0.9927007555961609)
[2025-01-06 01:34:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:34:59][root][INFO] - Training Epoch: 6/10, step 554/574 completed (loss: 0.03351413831114769, acc: 0.9862068891525269)
[2025-01-06 01:34:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:00][root][INFO] - Training Epoch: 6/10, step 555/574 completed (loss: 0.09798713773488998, acc: 0.9785714149475098)
[2025-01-06 01:35:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:00][root][INFO] - Training Epoch: 6/10, step 556/574 completed (loss: 0.1596389263868332, acc: 0.9470198750495911)
[2025-01-06 01:35:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:00][root][INFO] - Training Epoch: 6/10, step 557/574 completed (loss: 0.019674403592944145, acc: 0.9914529919624329)
[2025-01-06 01:35:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:01][root][INFO] - Training Epoch: 6/10, step 558/574 completed (loss: 0.06741105765104294, acc: 0.9599999785423279)
[2025-01-06 01:35:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:01][root][INFO] - Training Epoch: 6/10, step 559/574 completed (loss: 0.006015316117554903, acc: 1.0)
[2025-01-06 01:35:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:01][root][INFO] - Training Epoch: 6/10, step 560/574 completed (loss: 0.013220962136983871, acc: 1.0)
[2025-01-06 01:35:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:02][root][INFO] - Training Epoch: 6/10, step 561/574 completed (loss: 0.28635546565055847, acc: 0.9743589758872986)
[2025-01-06 01:35:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:33][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.1280, device='cuda:0') eval_epoch_loss=tensor(0.7552, device='cuda:0') eval_epoch_acc=tensor(0.8425, device='cuda:0')
[2025-01-06 01:35:33][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:35:33][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:35:33][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_6_step_562_loss_0.7552012205123901/model.pt
[2025-01-06 01:35:33][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:35:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:33][root][INFO] - Training Epoch: 6/10, step 562/574 completed (loss: 0.08381180465221405, acc: 0.9666666388511658)
[2025-01-06 01:35:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:34][root][INFO] - Training Epoch: 6/10, step 563/574 completed (loss: 0.037585366517305374, acc: 0.9740259647369385)
[2025-01-06 01:35:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:34][root][INFO] - Training Epoch: 6/10, step 564/574 completed (loss: 0.0033771756570786238, acc: 1.0)
[2025-01-06 01:35:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:34][root][INFO] - Training Epoch: 6/10, step 565/574 completed (loss: 0.026690417900681496, acc: 1.0)
[2025-01-06 01:35:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:35][root][INFO] - Training Epoch: 6/10, step 566/574 completed (loss: 0.039883796125650406, acc: 0.976190447807312)
[2025-01-06 01:35:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:35][root][INFO] - Training Epoch: 6/10, step 567/574 completed (loss: 0.006542564835399389, acc: 1.0)
[2025-01-06 01:35:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:35][root][INFO] - Training Epoch: 6/10, step 568/574 completed (loss: 0.008308377116918564, acc: 1.0)
[2025-01-06 01:35:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:36][root][INFO] - Training Epoch: 6/10, step 569/574 completed (loss: 0.09307005256414413, acc: 0.9839572310447693)
[2025-01-06 01:35:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:36][root][INFO] - Training Epoch: 6/10, step 570/574 completed (loss: 0.003872403409332037, acc: 1.0)
[2025-01-06 01:35:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:36][root][INFO] - Training Epoch: 6/10, step 571/574 completed (loss: 0.04260778799653053, acc: 0.9829059839248657)
[2025-01-06 01:35:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:37][root][INFO] - Training Epoch: 6/10, step 572/574 completed (loss: 0.09259326756000519, acc: 0.9642857313156128)
[2025-01-06 01:35:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:37][root][INFO] - Training Epoch: 6/10, step 573/574 completed (loss: 0.11526312679052353, acc: 0.9622641801834106)
[2025-01-06 01:35:37][slam_llm.utils.train_utils][INFO] - Epoch 6: train_perplexity=1.1228, train_epoch_loss=0.1158, epoch time 355.4939832240343s
[2025-01-06 01:35:37][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 13 GB
[2025-01-06 01:35:37][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 15 GB
[2025-01-06 01:35:37][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 13 GB
[2025-01-06 01:35:37][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 17
[2025-01-06 01:35:37][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2025-01-06 01:35:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:38][root][INFO] - Training Epoch: 7/10, step 0/574 completed (loss: 0.019515499472618103, acc: 1.0)
[2025-01-06 01:35:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:39][root][INFO] - Training Epoch: 7/10, step 1/574 completed (loss: 0.04647856578230858, acc: 1.0)
[2025-01-06 01:35:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:39][root][INFO] - Training Epoch: 7/10, step 2/574 completed (loss: 0.21153946220874786, acc: 0.9459459185600281)
[2025-01-06 01:35:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:39][root][INFO] - Training Epoch: 7/10, step 3/574 completed (loss: 0.045258570462465286, acc: 1.0)
[2025-01-06 01:35:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:40][root][INFO] - Training Epoch: 7/10, step 4/574 completed (loss: 0.028284665197134018, acc: 1.0)
[2025-01-06 01:35:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:40][root][INFO] - Training Epoch: 7/10, step 5/574 completed (loss: 0.010175665840506554, acc: 1.0)
[2025-01-06 01:35:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:40][root][INFO] - Training Epoch: 7/10, step 6/574 completed (loss: 0.035106111317873, acc: 1.0)
[2025-01-06 01:35:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:41][root][INFO] - Training Epoch: 7/10, step 7/574 completed (loss: 0.30322885513305664, acc: 0.9333333373069763)
[2025-01-06 01:35:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:41][root][INFO] - Training Epoch: 7/10, step 8/574 completed (loss: 0.0032159248366951942, acc: 1.0)
[2025-01-06 01:35:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:41][root][INFO] - Training Epoch: 7/10, step 9/574 completed (loss: 0.0017803417285904288, acc: 1.0)
[2025-01-06 01:35:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:42][root][INFO] - Training Epoch: 7/10, step 10/574 completed (loss: 0.012927062809467316, acc: 1.0)
[2025-01-06 01:35:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:42][root][INFO] - Training Epoch: 7/10, step 11/574 completed (loss: 0.05215383321046829, acc: 0.9743589758872986)
[2025-01-06 01:35:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:43][root][INFO] - Training Epoch: 7/10, step 12/574 completed (loss: 0.0399191789329052, acc: 0.9696969985961914)
[2025-01-06 01:35:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:43][root][INFO] - Training Epoch: 7/10, step 13/574 completed (loss: 0.024987690150737762, acc: 1.0)
[2025-01-06 01:35:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:43][root][INFO] - Training Epoch: 7/10, step 14/574 completed (loss: 0.06991589069366455, acc: 0.9803921580314636)
[2025-01-06 01:35:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:44][root][INFO] - Training Epoch: 7/10, step 15/574 completed (loss: 0.02045130915939808, acc: 1.0)
[2025-01-06 01:35:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:44][root][INFO] - Training Epoch: 7/10, step 16/574 completed (loss: 0.0073695434257388115, acc: 1.0)
[2025-01-06 01:35:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:44][root][INFO] - Training Epoch: 7/10, step 17/574 completed (loss: 0.011028748005628586, acc: 1.0)
[2025-01-06 01:35:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:45][root][INFO] - Training Epoch: 7/10, step 18/574 completed (loss: 0.04542434215545654, acc: 1.0)
[2025-01-06 01:35:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:45][root][INFO] - Training Epoch: 7/10, step 19/574 completed (loss: 0.008897869847714901, acc: 1.0)
[2025-01-06 01:35:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:45][root][INFO] - Training Epoch: 7/10, step 20/574 completed (loss: 0.06100032478570938, acc: 0.9615384340286255)
[2025-01-06 01:35:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:46][root][INFO] - Training Epoch: 7/10, step 21/574 completed (loss: 0.005601371638476849, acc: 1.0)
[2025-01-06 01:35:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:46][root][INFO] - Training Epoch: 7/10, step 22/574 completed (loss: 0.17027989029884338, acc: 0.9599999785423279)
[2025-01-06 01:35:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:46][root][INFO] - Training Epoch: 7/10, step 23/574 completed (loss: 0.010779076255857944, acc: 1.0)
[2025-01-06 01:35:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:47][root][INFO] - Training Epoch: 7/10, step 24/574 completed (loss: 0.003698104526847601, acc: 1.0)
[2025-01-06 01:35:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:47][root][INFO] - Training Epoch: 7/10, step 25/574 completed (loss: 0.048016294836997986, acc: 0.9811320900917053)
[2025-01-06 01:35:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:47][root][INFO] - Training Epoch: 7/10, step 26/574 completed (loss: 0.18685896694660187, acc: 0.931506872177124)
[2025-01-06 01:35:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:49][root][INFO] - Training Epoch: 7/10, step 27/574 completed (loss: 0.26866891980171204, acc: 0.9011857509613037)
[2025-01-06 01:35:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:49][root][INFO] - Training Epoch: 7/10, step 28/574 completed (loss: 0.06531868129968643, acc: 0.9767441749572754)
[2025-01-06 01:35:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:49][root][INFO] - Training Epoch: 7/10, step 29/574 completed (loss: 0.05077093839645386, acc: 0.9759036302566528)
[2025-01-06 01:35:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:50][root][INFO] - Training Epoch: 7/10, step 30/574 completed (loss: 0.1600164771080017, acc: 0.9753086566925049)
[2025-01-06 01:35:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:50][root][INFO] - Training Epoch: 7/10, step 31/574 completed (loss: 0.034518592059612274, acc: 1.0)
[2025-01-06 01:35:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:51][root][INFO] - Training Epoch: 7/10, step 32/574 completed (loss: 0.1485964059829712, acc: 0.9629629850387573)
[2025-01-06 01:35:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:51][root][INFO] - Training Epoch: 7/10, step 33/574 completed (loss: 0.0010247942991554737, acc: 1.0)
[2025-01-06 01:35:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:51][root][INFO] - Training Epoch: 7/10, step 34/574 completed (loss: 0.08794452995061874, acc: 0.9663865566253662)
[2025-01-06 01:35:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:52][root][INFO] - Training Epoch: 7/10, step 35/574 completed (loss: 0.045450810343027115, acc: 1.0)
[2025-01-06 01:35:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:52][root][INFO] - Training Epoch: 7/10, step 36/574 completed (loss: 0.025321882218122482, acc: 1.0)
[2025-01-06 01:35:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:52][root][INFO] - Training Epoch: 7/10, step 37/574 completed (loss: 0.038049716502428055, acc: 0.9830508232116699)
[2025-01-06 01:35:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:53][root][INFO] - Training Epoch: 7/10, step 38/574 completed (loss: 0.031463395804166794, acc: 0.9885057210922241)
[2025-01-06 01:35:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:53][root][INFO] - Training Epoch: 7/10, step 39/574 completed (loss: 0.0031307849567383528, acc: 1.0)
[2025-01-06 01:35:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:54][root][INFO] - Training Epoch: 7/10, step 40/574 completed (loss: 0.003833635011687875, acc: 1.0)
[2025-01-06 01:35:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:54][root][INFO] - Training Epoch: 7/10, step 41/574 completed (loss: 0.29298773407936096, acc: 0.9594594836235046)
[2025-01-06 01:35:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:54][root][INFO] - Training Epoch: 7/10, step 42/574 completed (loss: 0.15815187990665436, acc: 0.9538461565971375)
[2025-01-06 01:35:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:55][root][INFO] - Training Epoch: 7/10, step 43/574 completed (loss: 0.1264532506465912, acc: 0.9595959782600403)
[2025-01-06 01:35:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:55][root][INFO] - Training Epoch: 7/10, step 44/574 completed (loss: 0.06448759138584137, acc: 0.9793814420700073)
[2025-01-06 01:35:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:56][root][INFO] - Training Epoch: 7/10, step 45/574 completed (loss: 0.07313098013401031, acc: 0.9779411554336548)
[2025-01-06 01:35:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:56][root][INFO] - Training Epoch: 7/10, step 46/574 completed (loss: 0.17896977066993713, acc: 0.9615384340286255)
[2025-01-06 01:35:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:56][root][INFO] - Training Epoch: 7/10, step 47/574 completed (loss: 0.0008371869917027652, acc: 1.0)
[2025-01-06 01:35:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:57][root][INFO] - Training Epoch: 7/10, step 48/574 completed (loss: 0.12013112753629684, acc: 0.9642857313156128)
[2025-01-06 01:35:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:57][root][INFO] - Training Epoch: 7/10, step 49/574 completed (loss: 0.0028592152521014214, acc: 1.0)
[2025-01-06 01:35:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:57][root][INFO] - Training Epoch: 7/10, step 50/574 completed (loss: 0.042181357741355896, acc: 0.9824561476707458)
[2025-01-06 01:35:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:58][root][INFO] - Training Epoch: 7/10, step 51/574 completed (loss: 0.08721275627613068, acc: 0.9841269850730896)
[2025-01-06 01:35:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:58][root][INFO] - Training Epoch: 7/10, step 52/574 completed (loss: 0.28296929597854614, acc: 0.9154929518699646)
[2025-01-06 01:35:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:59][root][INFO] - Training Epoch: 7/10, step 53/574 completed (loss: 0.5314255952835083, acc: 0.846666693687439)
[2025-01-06 01:35:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:59][root][INFO] - Training Epoch: 7/10, step 54/574 completed (loss: 0.11573171615600586, acc: 0.9459459185600281)
[2025-01-06 01:35:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:35:59][root][INFO] - Training Epoch: 7/10, step 55/574 completed (loss: 0.003823786973953247, acc: 1.0)
[2025-01-06 01:36:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:02][root][INFO] - Training Epoch: 7/10, step 56/574 completed (loss: 0.48197436332702637, acc: 0.8464163541793823)
[2025-01-06 01:36:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:03][root][INFO] - Training Epoch: 7/10, step 57/574 completed (loss: 0.8407737016677856, acc: 0.7625272274017334)
[2025-01-06 01:36:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:04][root][INFO] - Training Epoch: 7/10, step 58/574 completed (loss: 0.284329891204834, acc: 0.9034090638160706)
[2025-01-06 01:36:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:05][root][INFO] - Training Epoch: 7/10, step 59/574 completed (loss: 0.10783999413251877, acc: 0.9338235259056091)
[2025-01-06 01:36:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:05][root][INFO] - Training Epoch: 7/10, step 60/574 completed (loss: 0.31568643450737, acc: 0.9057971239089966)
[2025-01-06 01:36:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:06][root][INFO] - Training Epoch: 7/10, step 61/574 completed (loss: 0.11168378591537476, acc: 0.9624999761581421)
[2025-01-06 01:36:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:06][root][INFO] - Training Epoch: 7/10, step 62/574 completed (loss: 0.030324630439281464, acc: 0.970588207244873)
[2025-01-06 01:36:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:06][root][INFO] - Training Epoch: 7/10, step 63/574 completed (loss: 0.04671858251094818, acc: 1.0)
[2025-01-06 01:36:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:07][root][INFO] - Training Epoch: 7/10, step 64/574 completed (loss: 0.012237422168254852, acc: 1.0)
[2025-01-06 01:36:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:07][root][INFO] - Training Epoch: 7/10, step 65/574 completed (loss: 0.015476793050765991, acc: 1.0)
[2025-01-06 01:36:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:08][root][INFO] - Training Epoch: 7/10, step 66/574 completed (loss: 0.08010502904653549, acc: 0.9821428656578064)
[2025-01-06 01:36:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:08][root][INFO] - Training Epoch: 7/10, step 67/574 completed (loss: 0.05469227954745293, acc: 0.9833333492279053)
[2025-01-06 01:36:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:08][root][INFO] - Training Epoch: 7/10, step 68/574 completed (loss: 0.005668175406754017, acc: 1.0)
[2025-01-06 01:36:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:09][root][INFO] - Training Epoch: 7/10, step 69/574 completed (loss: 0.15863163769245148, acc: 0.9444444179534912)
[2025-01-06 01:36:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:09][root][INFO] - Training Epoch: 7/10, step 70/574 completed (loss: 0.11931376904249191, acc: 0.9696969985961914)
[2025-01-06 01:36:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:09][root][INFO] - Training Epoch: 7/10, step 71/574 completed (loss: 0.2926276624202728, acc: 0.8970588445663452)
[2025-01-06 01:36:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:10][root][INFO] - Training Epoch: 7/10, step 72/574 completed (loss: 0.10711457580327988, acc: 0.9682539701461792)
[2025-01-06 01:36:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:10][root][INFO] - Training Epoch: 7/10, step 73/574 completed (loss: 0.39264601469039917, acc: 0.8564102649688721)
[2025-01-06 01:36:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:11][root][INFO] - Training Epoch: 7/10, step 74/574 completed (loss: 0.22265025973320007, acc: 0.9387755393981934)
[2025-01-06 01:36:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:11][root][INFO] - Training Epoch: 7/10, step 75/574 completed (loss: 0.2618279457092285, acc: 0.9477611780166626)
[2025-01-06 01:36:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:11][root][INFO] - Training Epoch: 7/10, step 76/574 completed (loss: 0.5701901316642761, acc: 0.8248175382614136)
[2025-01-06 01:36:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:12][root][INFO] - Training Epoch: 7/10, step 77/574 completed (loss: 0.019943654537200928, acc: 1.0)
[2025-01-06 01:36:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:12][root][INFO] - Training Epoch: 7/10, step 78/574 completed (loss: 0.007899765856564045, acc: 1.0)
[2025-01-06 01:36:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:12][root][INFO] - Training Epoch: 7/10, step 79/574 completed (loss: 0.0038891241420060396, acc: 1.0)
[2025-01-06 01:36:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:13][root][INFO] - Training Epoch: 7/10, step 80/574 completed (loss: 0.0801958367228508, acc: 0.9615384340286255)
[2025-01-06 01:36:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:13][root][INFO] - Training Epoch: 7/10, step 81/574 completed (loss: 0.07830814272165298, acc: 0.9807692170143127)
[2025-01-06 01:36:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:14][root][INFO] - Training Epoch: 7/10, step 82/574 completed (loss: 0.04552099481225014, acc: 1.0)
[2025-01-06 01:36:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:14][root][INFO] - Training Epoch: 7/10, step 83/574 completed (loss: 0.04169739782810211, acc: 0.96875)
[2025-01-06 01:36:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:14][root][INFO] - Training Epoch: 7/10, step 84/574 completed (loss: 0.044817935675382614, acc: 0.9855072498321533)
[2025-01-06 01:36:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:15][root][INFO] - Training Epoch: 7/10, step 85/574 completed (loss: 0.008650748059153557, acc: 1.0)
[2025-01-06 01:36:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:15][root][INFO] - Training Epoch: 7/10, step 86/574 completed (loss: 0.03442057594656944, acc: 1.0)
[2025-01-06 01:36:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:16][root][INFO] - Training Epoch: 7/10, step 87/574 completed (loss: 0.19386161863803864, acc: 0.8999999761581421)
[2025-01-06 01:36:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:16][root][INFO] - Training Epoch: 7/10, step 88/574 completed (loss: 0.18593253195285797, acc: 0.9320388436317444)
[2025-01-06 01:36:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:17][root][INFO] - Training Epoch: 7/10, step 89/574 completed (loss: 0.3557494878768921, acc: 0.917475700378418)
[2025-01-06 01:36:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:18][root][INFO] - Training Epoch: 7/10, step 90/574 completed (loss: 0.36923348903656006, acc: 0.9139785170555115)
[2025-01-06 01:36:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:19][root][INFO] - Training Epoch: 7/10, step 91/574 completed (loss: 0.32213711738586426, acc: 0.9094827771186829)
[2025-01-06 01:36:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:19][root][INFO] - Training Epoch: 7/10, step 92/574 completed (loss: 0.2323738932609558, acc: 0.9263157844543457)
[2025-01-06 01:36:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:20][root][INFO] - Training Epoch: 7/10, step 93/574 completed (loss: 0.32330283522605896, acc: 0.9108911156654358)
[2025-01-06 01:36:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:21][root][INFO] - Training Epoch: 7/10, step 94/574 completed (loss: 0.3511098027229309, acc: 0.9032257795333862)
[2025-01-06 01:36:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:21][root][INFO] - Training Epoch: 7/10, step 95/574 completed (loss: 0.1996980905532837, acc: 0.9275362491607666)
[2025-01-06 01:36:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:21][root][INFO] - Training Epoch: 7/10, step 96/574 completed (loss: 0.20772044360637665, acc: 0.9327731132507324)
[2025-01-06 01:36:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:22][root][INFO] - Training Epoch: 7/10, step 97/574 completed (loss: 0.16586217284202576, acc: 0.942307710647583)
[2025-01-06 01:36:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:22][root][INFO] - Training Epoch: 7/10, step 98/574 completed (loss: 0.2147744745016098, acc: 0.9270073175430298)
[2025-01-06 01:36:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:23][root][INFO] - Training Epoch: 7/10, step 99/574 completed (loss: 0.17267727851867676, acc: 0.9402984976768494)
[2025-01-06 01:36:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:23][root][INFO] - Training Epoch: 7/10, step 100/574 completed (loss: 0.005457508377730846, acc: 1.0)
[2025-01-06 01:36:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:23][root][INFO] - Training Epoch: 7/10, step 101/574 completed (loss: 0.003983341157436371, acc: 1.0)
[2025-01-06 01:36:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:24][root][INFO] - Training Epoch: 7/10, step 102/574 completed (loss: 0.013032638467848301, acc: 1.0)
[2025-01-06 01:36:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:24][root][INFO] - Training Epoch: 7/10, step 103/574 completed (loss: 0.004256175830960274, acc: 1.0)
[2025-01-06 01:36:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:24][root][INFO] - Training Epoch: 7/10, step 104/574 completed (loss: 0.016622208058834076, acc: 1.0)
[2025-01-06 01:36:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:25][root][INFO] - Training Epoch: 7/10, step 105/574 completed (loss: 0.0228901207447052, acc: 1.0)
[2025-01-06 01:36:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:25][root][INFO] - Training Epoch: 7/10, step 106/574 completed (loss: 0.007345764432102442, acc: 1.0)
[2025-01-06 01:36:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:25][root][INFO] - Training Epoch: 7/10, step 107/574 completed (loss: 0.02421589009463787, acc: 1.0)
[2025-01-06 01:36:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:26][root][INFO] - Training Epoch: 7/10, step 108/574 completed (loss: 0.0005762280197814107, acc: 1.0)
[2025-01-06 01:36:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:26][root][INFO] - Training Epoch: 7/10, step 109/574 completed (loss: 0.004784358665347099, acc: 1.0)
[2025-01-06 01:36:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:27][root][INFO] - Training Epoch: 7/10, step 110/574 completed (loss: 0.018768111243844032, acc: 0.9846153855323792)
[2025-01-06 01:36:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:27][root][INFO] - Training Epoch: 7/10, step 111/574 completed (loss: 0.01053573563694954, acc: 1.0)
[2025-01-06 01:36:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:27][root][INFO] - Training Epoch: 7/10, step 112/574 completed (loss: 0.1883438527584076, acc: 0.9649122953414917)
[2025-01-06 01:36:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:28][root][INFO] - Training Epoch: 7/10, step 113/574 completed (loss: 0.04039511829614639, acc: 0.9743589758872986)
[2025-01-06 01:36:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:28][root][INFO] - Training Epoch: 7/10, step 114/574 completed (loss: 0.01578638330101967, acc: 1.0)
[2025-01-06 01:36:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:28][root][INFO] - Training Epoch: 7/10, step 115/574 completed (loss: 0.0002520004636608064, acc: 1.0)
[2025-01-06 01:36:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:29][root][INFO] - Training Epoch: 7/10, step 116/574 completed (loss: 0.018119558691978455, acc: 1.0)
[2025-01-06 01:36:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:29][root][INFO] - Training Epoch: 7/10, step 117/574 completed (loss: 0.1154458224773407, acc: 0.9593495726585388)
[2025-01-06 01:36:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:30][root][INFO] - Training Epoch: 7/10, step 118/574 completed (loss: 0.011962966062128544, acc: 1.0)
[2025-01-06 01:36:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:30][root][INFO] - Training Epoch: 7/10, step 119/574 completed (loss: 0.19788327813148499, acc: 0.9391635060310364)
[2025-01-06 01:36:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:31][root][INFO] - Training Epoch: 7/10, step 120/574 completed (loss: 0.024258136749267578, acc: 0.9866666793823242)
[2025-01-06 01:36:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:31][root][INFO] - Training Epoch: 7/10, step 121/574 completed (loss: 0.014914309605956078, acc: 1.0)
[2025-01-06 01:36:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:31][root][INFO] - Training Epoch: 7/10, step 122/574 completed (loss: 0.011586061678826809, acc: 1.0)
[2025-01-06 01:36:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:32][root][INFO] - Training Epoch: 7/10, step 123/574 completed (loss: 0.09579954296350479, acc: 0.9473684430122375)
[2025-01-06 01:36:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:32][root][INFO] - Training Epoch: 7/10, step 124/574 completed (loss: 0.2590107023715973, acc: 0.9325153231620789)
[2025-01-06 01:36:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:33][root][INFO] - Training Epoch: 7/10, step 125/574 completed (loss: 0.33705395460128784, acc: 0.9166666865348816)
[2025-01-06 01:36:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:33][root][INFO] - Training Epoch: 7/10, step 126/574 completed (loss: 0.23220199346542358, acc: 0.9166666865348816)
[2025-01-06 01:36:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:33][root][INFO] - Training Epoch: 7/10, step 127/574 completed (loss: 0.21145617961883545, acc: 0.9464285969734192)
[2025-01-06 01:36:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:34][root][INFO] - Training Epoch: 7/10, step 128/574 completed (loss: 0.24534496665000916, acc: 0.9487179517745972)
[2025-01-06 01:36:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:34][root][INFO] - Training Epoch: 7/10, step 129/574 completed (loss: 0.27293628454208374, acc: 0.9191176295280457)
[2025-01-06 01:36:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:35][root][INFO] - Training Epoch: 7/10, step 130/574 completed (loss: 0.015179576352238655, acc: 1.0)
[2025-01-06 01:36:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:36:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:06][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.1767, device='cuda:0') eval_epoch_loss=tensor(0.7778, device='cuda:0') eval_epoch_acc=tensor(0.8451, device='cuda:0')
[2025-01-06 01:37:06][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:37:06][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:37:06][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_7_step_131_loss_0.7778241634368896/model.pt
[2025-01-06 01:37:06][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:37:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:06][root][INFO] - Training Epoch: 7/10, step 131/574 completed (loss: 0.07742335647344589, acc: 0.95652174949646)
[2025-01-06 01:37:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:07][root][INFO] - Training Epoch: 7/10, step 132/574 completed (loss: 0.14617107808589935, acc: 0.96875)
[2025-01-06 01:37:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:07][root][INFO] - Training Epoch: 7/10, step 133/574 completed (loss: 0.0530773289501667, acc: 1.0)
[2025-01-06 01:37:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:07][root][INFO] - Training Epoch: 7/10, step 134/574 completed (loss: 0.030222268775105476, acc: 1.0)
[2025-01-06 01:37:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:08][root][INFO] - Training Epoch: 7/10, step 135/574 completed (loss: 0.030847657471895218, acc: 1.0)
[2025-01-06 01:37:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:08][root][INFO] - Training Epoch: 7/10, step 136/574 completed (loss: 0.00973086804151535, acc: 1.0)
[2025-01-06 01:37:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:08][root][INFO] - Training Epoch: 7/10, step 137/574 completed (loss: 0.01786564476788044, acc: 1.0)
[2025-01-06 01:37:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:09][root][INFO] - Training Epoch: 7/10, step 138/574 completed (loss: 0.11791455000638962, acc: 0.95652174949646)
[2025-01-06 01:37:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:09][root][INFO] - Training Epoch: 7/10, step 139/574 completed (loss: 0.04110027849674225, acc: 0.9523809552192688)
[2025-01-06 01:37:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:09][root][INFO] - Training Epoch: 7/10, step 140/574 completed (loss: 0.01826714351773262, acc: 1.0)
[2025-01-06 01:37:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:10][root][INFO] - Training Epoch: 7/10, step 141/574 completed (loss: 0.16874730587005615, acc: 0.9677419066429138)
[2025-01-06 01:37:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:10][root][INFO] - Training Epoch: 7/10, step 142/574 completed (loss: 0.2016930878162384, acc: 0.9729729890823364)
[2025-01-06 01:37:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:11][root][INFO] - Training Epoch: 7/10, step 143/574 completed (loss: 0.11278266459703445, acc: 0.9736841917037964)
[2025-01-06 01:37:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:11][root][INFO] - Training Epoch: 7/10, step 144/574 completed (loss: 0.15360550582408905, acc: 0.9402984976768494)
[2025-01-06 01:37:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:11][root][INFO] - Training Epoch: 7/10, step 145/574 completed (loss: 0.08095937222242355, acc: 0.9591836929321289)
[2025-01-06 01:37:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:12][root][INFO] - Training Epoch: 7/10, step 146/574 completed (loss: 0.10597966611385345, acc: 0.957446813583374)
[2025-01-06 01:37:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:12][root][INFO] - Training Epoch: 7/10, step 147/574 completed (loss: 0.08626621216535568, acc: 0.9714285731315613)
[2025-01-06 01:37:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:13][root][INFO] - Training Epoch: 7/10, step 148/574 completed (loss: 0.09369028359651566, acc: 0.9642857313156128)
[2025-01-06 01:37:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:13][root][INFO] - Training Epoch: 7/10, step 149/574 completed (loss: 0.09072756767272949, acc: 0.95652174949646)
[2025-01-06 01:37:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:13][root][INFO] - Training Epoch: 7/10, step 150/574 completed (loss: 0.005090627819299698, acc: 1.0)
[2025-01-06 01:37:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:14][root][INFO] - Training Epoch: 7/10, step 151/574 completed (loss: 0.051774267107248306, acc: 0.97826087474823)
[2025-01-06 01:37:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:14][root][INFO] - Training Epoch: 7/10, step 152/574 completed (loss: 0.3816600739955902, acc: 0.9152542352676392)
[2025-01-06 01:37:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:14][root][INFO] - Training Epoch: 7/10, step 153/574 completed (loss: 0.041435226798057556, acc: 1.0)
[2025-01-06 01:37:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:15][root][INFO] - Training Epoch: 7/10, step 154/574 completed (loss: 0.08646588027477264, acc: 0.9459459185600281)
[2025-01-06 01:37:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:15][root][INFO] - Training Epoch: 7/10, step 155/574 completed (loss: 0.024678515270352364, acc: 1.0)
[2025-01-06 01:37:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:15][root][INFO] - Training Epoch: 7/10, step 156/574 completed (loss: 0.05780632048845291, acc: 0.95652174949646)
[2025-01-06 01:37:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:16][root][INFO] - Training Epoch: 7/10, step 157/574 completed (loss: 0.3181530237197876, acc: 0.9473684430122375)
[2025-01-06 01:37:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:17][root][INFO] - Training Epoch: 7/10, step 158/574 completed (loss: 0.21444706618785858, acc: 0.9324324131011963)
[2025-01-06 01:37:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:18][root][INFO] - Training Epoch: 7/10, step 159/574 completed (loss: 0.19078131020069122, acc: 0.9629629850387573)
[2025-01-06 01:37:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:18][root][INFO] - Training Epoch: 7/10, step 160/574 completed (loss: 0.2542167007923126, acc: 0.9534883499145508)
[2025-01-06 01:37:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:19][root][INFO] - Training Epoch: 7/10, step 161/574 completed (loss: 0.18901580572128296, acc: 0.9411764740943909)
[2025-01-06 01:37:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:19][root][INFO] - Training Epoch: 7/10, step 162/574 completed (loss: 0.16152489185333252, acc: 0.9438202381134033)
[2025-01-06 01:37:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:20][root][INFO] - Training Epoch: 7/10, step 163/574 completed (loss: 0.05294065177440643, acc: 0.9772727489471436)
[2025-01-06 01:37:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:20][root][INFO] - Training Epoch: 7/10, step 164/574 completed (loss: 0.005611402913928032, acc: 1.0)
[2025-01-06 01:37:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:20][root][INFO] - Training Epoch: 7/10, step 165/574 completed (loss: 0.035801660269498825, acc: 0.9655172228813171)
[2025-01-06 01:37:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:21][root][INFO] - Training Epoch: 7/10, step 166/574 completed (loss: 0.043214213103055954, acc: 0.9795918464660645)
[2025-01-06 01:37:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:21][root][INFO] - Training Epoch: 7/10, step 167/574 completed (loss: 0.026251569390296936, acc: 1.0)
[2025-01-06 01:37:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:21][root][INFO] - Training Epoch: 7/10, step 168/574 completed (loss: 0.13923178613185883, acc: 0.9444444179534912)
[2025-01-06 01:37:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:22][root][INFO] - Training Epoch: 7/10, step 169/574 completed (loss: 0.2610861659049988, acc: 0.9313725233078003)
[2025-01-06 01:37:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:23][root][INFO] - Training Epoch: 7/10, step 170/574 completed (loss: 0.1428515911102295, acc: 0.9452054500579834)
[2025-01-06 01:37:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:23][root][INFO] - Training Epoch: 7/10, step 171/574 completed (loss: 0.0026973795611411333, acc: 1.0)
[2025-01-06 01:37:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:23][root][INFO] - Training Epoch: 7/10, step 172/574 completed (loss: 0.010039901360869408, acc: 1.0)
[2025-01-06 01:37:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:24][root][INFO] - Training Epoch: 7/10, step 173/574 completed (loss: 0.005784688983112574, acc: 1.0)
[2025-01-06 01:37:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:24][root][INFO] - Training Epoch: 7/10, step 174/574 completed (loss: 0.4223771095275879, acc: 0.8584070801734924)
[2025-01-06 01:37:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:25][root][INFO] - Training Epoch: 7/10, step 175/574 completed (loss: 0.1650850474834442, acc: 0.9855072498321533)
[2025-01-06 01:37:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:25][root][INFO] - Training Epoch: 7/10, step 176/574 completed (loss: 0.09358447045087814, acc: 0.9659090638160706)
[2025-01-06 01:37:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:26][root][INFO] - Training Epoch: 7/10, step 177/574 completed (loss: 0.26675617694854736, acc: 0.9007633328437805)
[2025-01-06 01:37:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:27][root][INFO] - Training Epoch: 7/10, step 178/574 completed (loss: 0.284026563167572, acc: 0.8962963223457336)
[2025-01-06 01:37:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:27][root][INFO] - Training Epoch: 7/10, step 179/574 completed (loss: 0.02904292568564415, acc: 1.0)
[2025-01-06 01:37:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:27][root][INFO] - Training Epoch: 7/10, step 180/574 completed (loss: 0.0029601778369396925, acc: 1.0)
[2025-01-06 01:37:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:28][root][INFO] - Training Epoch: 7/10, step 181/574 completed (loss: 0.0026843203231692314, acc: 1.0)
[2025-01-06 01:37:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:28][root][INFO] - Training Epoch: 7/10, step 182/574 completed (loss: 0.0008277992019429803, acc: 1.0)
[2025-01-06 01:37:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:28][root][INFO] - Training Epoch: 7/10, step 183/574 completed (loss: 0.005078284069895744, acc: 1.0)
[2025-01-06 01:37:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:29][root][INFO] - Training Epoch: 7/10, step 184/574 completed (loss: 0.15214022994041443, acc: 0.9546827673912048)
[2025-01-06 01:37:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:29][root][INFO] - Training Epoch: 7/10, step 185/574 completed (loss: 0.19917899370193481, acc: 0.9365994334220886)
[2025-01-06 01:37:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:30][root][INFO] - Training Epoch: 7/10, step 186/574 completed (loss: 0.1823299676179886, acc: 0.956250011920929)
[2025-01-06 01:37:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:30][root][INFO] - Training Epoch: 7/10, step 187/574 completed (loss: 0.30774596333503723, acc: 0.9155722260475159)
[2025-01-06 01:37:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:31][root][INFO] - Training Epoch: 7/10, step 188/574 completed (loss: 0.20227022469043732, acc: 0.9501779079437256)
[2025-01-06 01:37:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:31][root][INFO] - Training Epoch: 7/10, step 189/574 completed (loss: 0.030864829197525978, acc: 1.0)
[2025-01-06 01:37:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:31][root][INFO] - Training Epoch: 7/10, step 190/574 completed (loss: 0.15693140029907227, acc: 0.9651162624359131)
[2025-01-06 01:37:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:32][root][INFO] - Training Epoch: 7/10, step 191/574 completed (loss: 0.43806275725364685, acc: 0.8650793433189392)
[2025-01-06 01:37:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:33][root][INFO] - Training Epoch: 7/10, step 192/574 completed (loss: 0.17778313159942627, acc: 0.9318181872367859)
[2025-01-06 01:37:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:34][root][INFO] - Training Epoch: 7/10, step 193/574 completed (loss: 0.08494293689727783, acc: 0.9764705896377563)
[2025-01-06 01:37:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:35][root][INFO] - Training Epoch: 7/10, step 194/574 completed (loss: 0.2639086842536926, acc: 0.9197530746459961)
[2025-01-06 01:37:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:36][root][INFO] - Training Epoch: 7/10, step 195/574 completed (loss: 0.02639143355190754, acc: 1.0)
[2025-01-06 01:37:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:36][root][INFO] - Training Epoch: 7/10, step 196/574 completed (loss: 0.009360876865684986, acc: 1.0)
[2025-01-06 01:37:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:37][root][INFO] - Training Epoch: 7/10, step 197/574 completed (loss: 0.05492456629872322, acc: 1.0)
[2025-01-06 01:37:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:37][root][INFO] - Training Epoch: 7/10, step 198/574 completed (loss: 0.1248420998454094, acc: 0.970588207244873)
[2025-01-06 01:37:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:37][root][INFO] - Training Epoch: 7/10, step 199/574 completed (loss: 0.13751114904880524, acc: 0.9411764740943909)
[2025-01-06 01:37:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:38][root][INFO] - Training Epoch: 7/10, step 200/574 completed (loss: 0.06879187375307083, acc: 0.9745762944221497)
[2025-01-06 01:37:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:38][root][INFO] - Training Epoch: 7/10, step 201/574 completed (loss: 0.14114916324615479, acc: 0.9701492786407471)
[2025-01-06 01:37:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:38][root][INFO] - Training Epoch: 7/10, step 202/574 completed (loss: 0.0833720713853836, acc: 0.9805825352668762)
[2025-01-06 01:37:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:39][root][INFO] - Training Epoch: 7/10, step 203/574 completed (loss: 0.030761128291487694, acc: 1.0)
[2025-01-06 01:37:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:39][root][INFO] - Training Epoch: 7/10, step 204/574 completed (loss: 0.03001752868294716, acc: 0.9890109896659851)
[2025-01-06 01:37:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:39][root][INFO] - Training Epoch: 7/10, step 205/574 completed (loss: 0.05016353353857994, acc: 0.9865471124649048)
[2025-01-06 01:37:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:40][root][INFO] - Training Epoch: 7/10, step 206/574 completed (loss: 0.13473190367221832, acc: 0.9566929340362549)
[2025-01-06 01:37:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:40][root][INFO] - Training Epoch: 7/10, step 207/574 completed (loss: 0.058386724442243576, acc: 0.9741379022598267)
[2025-01-06 01:37:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:40][root][INFO] - Training Epoch: 7/10, step 208/574 completed (loss: 0.14840631186962128, acc: 0.97826087474823)
[2025-01-06 01:37:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:41][root][INFO] - Training Epoch: 7/10, step 209/574 completed (loss: 0.08886583894491196, acc: 0.9727626442909241)
[2025-01-06 01:37:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:41][root][INFO] - Training Epoch: 7/10, step 210/574 completed (loss: 0.011005682870745659, acc: 1.0)
[2025-01-06 01:37:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:41][root][INFO] - Training Epoch: 7/10, step 211/574 completed (loss: 0.00320043065585196, acc: 1.0)
[2025-01-06 01:37:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:42][root][INFO] - Training Epoch: 7/10, step 212/574 completed (loss: 0.0012535888236016035, acc: 1.0)
[2025-01-06 01:37:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:42][root][INFO] - Training Epoch: 7/10, step 213/574 completed (loss: 0.06108666583895683, acc: 0.978723406791687)
[2025-01-06 01:37:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:43][root][INFO] - Training Epoch: 7/10, step 214/574 completed (loss: 0.17009757459163666, acc: 0.9538461565971375)
[2025-01-06 01:37:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:43][root][INFO] - Training Epoch: 7/10, step 215/574 completed (loss: 0.015086052939295769, acc: 1.0)
[2025-01-06 01:37:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:43][root][INFO] - Training Epoch: 7/10, step 216/574 completed (loss: 0.2107488214969635, acc: 0.9651162624359131)
[2025-01-06 01:37:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:44][root][INFO] - Training Epoch: 7/10, step 217/574 completed (loss: 0.031748220324516296, acc: 0.9909909963607788)
[2025-01-06 01:37:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:44][root][INFO] - Training Epoch: 7/10, step 218/574 completed (loss: 0.05476197600364685, acc: 0.9777777791023254)
[2025-01-06 01:37:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:45][root][INFO] - Training Epoch: 7/10, step 219/574 completed (loss: 0.010502456687390804, acc: 1.0)
[2025-01-06 01:37:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:45][root][INFO] - Training Epoch: 7/10, step 220/574 completed (loss: 0.0006591911078430712, acc: 1.0)
[2025-01-06 01:37:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:45][root][INFO] - Training Epoch: 7/10, step 221/574 completed (loss: 0.49174433946609497, acc: 0.9599999785423279)
[2025-01-06 01:37:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:46][root][INFO] - Training Epoch: 7/10, step 222/574 completed (loss: 0.0636175349354744, acc: 0.9615384340286255)
[2025-01-06 01:37:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:47][root][INFO] - Training Epoch: 7/10, step 223/574 completed (loss: 0.1269647628068924, acc: 0.9619565010070801)
[2025-01-06 01:37:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:47][root][INFO] - Training Epoch: 7/10, step 224/574 completed (loss: 0.1997871845960617, acc: 0.9261363744735718)
[2025-01-06 01:37:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:47][root][INFO] - Training Epoch: 7/10, step 225/574 completed (loss: 0.09205380827188492, acc: 0.978723406791687)
[2025-01-06 01:37:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:48][root][INFO] - Training Epoch: 7/10, step 226/574 completed (loss: 0.1837429702281952, acc: 0.9622641801834106)
[2025-01-06 01:37:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:48][root][INFO] - Training Epoch: 7/10, step 227/574 completed (loss: 0.08198990672826767, acc: 0.9666666388511658)
[2025-01-06 01:37:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:49][root][INFO] - Training Epoch: 7/10, step 228/574 completed (loss: 0.0664806216955185, acc: 0.9534883499145508)
[2025-01-06 01:37:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:49][root][INFO] - Training Epoch: 7/10, step 229/574 completed (loss: 0.04274637624621391, acc: 1.0)
[2025-01-06 01:37:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:49][root][INFO] - Training Epoch: 7/10, step 230/574 completed (loss: 0.2636665999889374, acc: 0.8947368264198303)
[2025-01-06 01:37:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:50][root][INFO] - Training Epoch: 7/10, step 231/574 completed (loss: 0.13618801534175873, acc: 0.9666666388511658)
[2025-01-06 01:37:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:50][root][INFO] - Training Epoch: 7/10, step 232/574 completed (loss: 0.3892672657966614, acc: 0.8888888955116272)
[2025-01-06 01:37:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:51][root][INFO] - Training Epoch: 7/10, step 233/574 completed (loss: 0.6560089588165283, acc: 0.8073394298553467)
[2025-01-06 01:37:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:51][root][INFO] - Training Epoch: 7/10, step 234/574 completed (loss: 0.32222577929496765, acc: 0.8999999761581421)
[2025-01-06 01:37:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:51][root][INFO] - Training Epoch: 7/10, step 235/574 completed (loss: 0.010298676788806915, acc: 1.0)
[2025-01-06 01:37:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:52][root][INFO] - Training Epoch: 7/10, step 236/574 completed (loss: 0.009245045483112335, acc: 1.0)
[2025-01-06 01:37:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:52][root][INFO] - Training Epoch: 7/10, step 237/574 completed (loss: 0.16573375463485718, acc: 0.9545454382896423)
[2025-01-06 01:37:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:53][root][INFO] - Training Epoch: 7/10, step 238/574 completed (loss: 0.04256931319832802, acc: 1.0)
[2025-01-06 01:37:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:53][root][INFO] - Training Epoch: 7/10, step 239/574 completed (loss: 0.19421543180942535, acc: 0.9428571462631226)
[2025-01-06 01:37:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:53][root][INFO] - Training Epoch: 7/10, step 240/574 completed (loss: 0.033989761024713516, acc: 0.9772727489471436)
[2025-01-06 01:37:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:54][root][INFO] - Training Epoch: 7/10, step 241/574 completed (loss: 0.03233114257454872, acc: 1.0)
[2025-01-06 01:37:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:54][root][INFO] - Training Epoch: 7/10, step 242/574 completed (loss: 0.15497852861881256, acc: 0.9354838728904724)
[2025-01-06 01:37:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:55][root][INFO] - Training Epoch: 7/10, step 243/574 completed (loss: 0.15622510015964508, acc: 0.9545454382896423)
[2025-01-06 01:37:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:55][root][INFO] - Training Epoch: 7/10, step 244/574 completed (loss: 0.0001724840112728998, acc: 1.0)
[2025-01-06 01:37:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:55][root][INFO] - Training Epoch: 7/10, step 245/574 completed (loss: 0.043016429990530014, acc: 0.9615384340286255)
[2025-01-06 01:37:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:56][root][INFO] - Training Epoch: 7/10, step 246/574 completed (loss: 0.004387064836919308, acc: 1.0)
[2025-01-06 01:37:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:56][root][INFO] - Training Epoch: 7/10, step 247/574 completed (loss: 0.002580296481028199, acc: 1.0)
[2025-01-06 01:37:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:56][root][INFO] - Training Epoch: 7/10, step 248/574 completed (loss: 0.017680147662758827, acc: 1.0)
[2025-01-06 01:37:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:57][root][INFO] - Training Epoch: 7/10, step 249/574 completed (loss: 0.06872845441102982, acc: 0.9729729890823364)
[2025-01-06 01:37:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:57][root][INFO] - Training Epoch: 7/10, step 250/574 completed (loss: 0.07741031795740128, acc: 0.9729729890823364)
[2025-01-06 01:37:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:57][root][INFO] - Training Epoch: 7/10, step 251/574 completed (loss: 0.17646491527557373, acc: 0.9558823704719543)
[2025-01-06 01:37:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:58][root][INFO] - Training Epoch: 7/10, step 252/574 completed (loss: 0.012346658855676651, acc: 1.0)
[2025-01-06 01:37:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:58][root][INFO] - Training Epoch: 7/10, step 253/574 completed (loss: 0.09001408517360687, acc: 0.9599999785423279)
[2025-01-06 01:37:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:58][root][INFO] - Training Epoch: 7/10, step 254/574 completed (loss: 0.0001622780109755695, acc: 1.0)
[2025-01-06 01:37:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:59][root][INFO] - Training Epoch: 7/10, step 255/574 completed (loss: 0.057955436408519745, acc: 0.9677419066429138)
[2025-01-06 01:37:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:59][root][INFO] - Training Epoch: 7/10, step 256/574 completed (loss: 0.0070353914052248, acc: 1.0)
[2025-01-06 01:37:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:37:59][root][INFO] - Training Epoch: 7/10, step 257/574 completed (loss: 0.011362412944436073, acc: 1.0)
[2025-01-06 01:37:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:00][root][INFO] - Training Epoch: 7/10, step 258/574 completed (loss: 0.010955623351037502, acc: 1.0)
[2025-01-06 01:38:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:00][root][INFO] - Training Epoch: 7/10, step 259/574 completed (loss: 0.06684543192386627, acc: 0.9622641801834106)
[2025-01-06 01:38:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:01][root][INFO] - Training Epoch: 7/10, step 260/574 completed (loss: 0.1468321532011032, acc: 0.9333333373069763)
[2025-01-06 01:38:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:01][root][INFO] - Training Epoch: 7/10, step 261/574 completed (loss: 0.014718770980834961, acc: 1.0)
[2025-01-06 01:38:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:01][root][INFO] - Training Epoch: 7/10, step 262/574 completed (loss: 0.014731014147400856, acc: 1.0)
[2025-01-06 01:38:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:02][root][INFO] - Training Epoch: 7/10, step 263/574 completed (loss: 0.1923944056034088, acc: 0.9200000166893005)
[2025-01-06 01:38:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:02][root][INFO] - Training Epoch: 7/10, step 264/574 completed (loss: 0.11709165573120117, acc: 0.9583333134651184)
[2025-01-06 01:38:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:03][root][INFO] - Training Epoch: 7/10, step 265/574 completed (loss: 0.4827558100223541, acc: 0.8880000114440918)
[2025-01-06 01:38:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:03][root][INFO] - Training Epoch: 7/10, step 266/574 completed (loss: 0.19757358729839325, acc: 0.932584285736084)
[2025-01-06 01:38:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:04][root][INFO] - Training Epoch: 7/10, step 267/574 completed (loss: 0.12554271519184113, acc: 0.9594594836235046)
[2025-01-06 01:38:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:04][root][INFO] - Training Epoch: 7/10, step 268/574 completed (loss: 0.09033602476119995, acc: 0.982758641242981)
[2025-01-06 01:38:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:05][root][INFO] - Training Epoch: 7/10, step 269/574 completed (loss: 0.006208099890500307, acc: 1.0)
[2025-01-06 01:38:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:05][root][INFO] - Training Epoch: 7/10, step 270/574 completed (loss: 0.0007797161233611405, acc: 1.0)
[2025-01-06 01:38:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:05][root][INFO] - Training Epoch: 7/10, step 271/574 completed (loss: 0.08434763550758362, acc: 0.96875)
[2025-01-06 01:38:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:06][root][INFO] - Training Epoch: 7/10, step 272/574 completed (loss: 0.001203657011501491, acc: 1.0)
[2025-01-06 01:38:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:06][root][INFO] - Training Epoch: 7/10, step 273/574 completed (loss: 0.1635536104440689, acc: 0.9666666388511658)
[2025-01-06 01:38:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:37][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.2601, device='cuda:0') eval_epoch_loss=tensor(0.8154, device='cuda:0') eval_epoch_acc=tensor(0.8363, device='cuda:0')
[2025-01-06 01:38:37][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:38:37][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:38:37][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_7_step_274_loss_0.8154280781745911/model.pt
[2025-01-06 01:38:37][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:38:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:38][root][INFO] - Training Epoch: 7/10, step 274/574 completed (loss: 0.026400936767458916, acc: 0.96875)
[2025-01-06 01:38:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:38][root][INFO] - Training Epoch: 7/10, step 275/574 completed (loss: 0.0469999685883522, acc: 0.9666666388511658)
[2025-01-06 01:38:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:38][root][INFO] - Training Epoch: 7/10, step 276/574 completed (loss: 0.10622889548540115, acc: 0.9655172228813171)
[2025-01-06 01:38:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:39][root][INFO] - Training Epoch: 7/10, step 277/574 completed (loss: 0.0033378119114786386, acc: 1.0)
[2025-01-06 01:38:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:39][root][INFO] - Training Epoch: 7/10, step 278/574 completed (loss: 0.0265080276876688, acc: 0.978723406791687)
[2025-01-06 01:38:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:40][root][INFO] - Training Epoch: 7/10, step 279/574 completed (loss: 0.1574205905199051, acc: 0.9375)
[2025-01-06 01:38:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:40][root][INFO] - Training Epoch: 7/10, step 280/574 completed (loss: 0.006453373935073614, acc: 1.0)
[2025-01-06 01:38:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:40][root][INFO] - Training Epoch: 7/10, step 281/574 completed (loss: 0.08450701087713242, acc: 0.9759036302566528)
[2025-01-06 01:38:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:41][root][INFO] - Training Epoch: 7/10, step 282/574 completed (loss: 0.2902686893939972, acc: 0.9444444179534912)
[2025-01-06 01:38:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:41][root][INFO] - Training Epoch: 7/10, step 283/574 completed (loss: 0.06753000617027283, acc: 0.9736841917037964)
[2025-01-06 01:38:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:41][root][INFO] - Training Epoch: 7/10, step 284/574 completed (loss: 0.15338435769081116, acc: 0.970588207244873)
[2025-01-06 01:38:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:42][root][INFO] - Training Epoch: 7/10, step 285/574 completed (loss: 0.03156743198633194, acc: 0.9750000238418579)
[2025-01-06 01:38:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:42][root][INFO] - Training Epoch: 7/10, step 286/574 completed (loss: 0.08213499933481216, acc: 0.9765625)
[2025-01-06 01:38:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:43][root][INFO] - Training Epoch: 7/10, step 287/574 completed (loss: 0.13301940262317657, acc: 0.9440000057220459)
[2025-01-06 01:38:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:43][root][INFO] - Training Epoch: 7/10, step 288/574 completed (loss: 0.04176202788949013, acc: 0.9890109896659851)
[2025-01-06 01:38:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:43][root][INFO] - Training Epoch: 7/10, step 289/574 completed (loss: 0.11447305977344513, acc: 0.9503105878829956)
[2025-01-06 01:38:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:44][root][INFO] - Training Epoch: 7/10, step 290/574 completed (loss: 0.1241694763302803, acc: 0.9536082744598389)
[2025-01-06 01:38:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:44][root][INFO] - Training Epoch: 7/10, step 291/574 completed (loss: 0.005828123074024916, acc: 1.0)
[2025-01-06 01:38:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:44][root][INFO] - Training Epoch: 7/10, step 292/574 completed (loss: 0.12030690908432007, acc: 0.976190447807312)
[2025-01-06 01:38:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:45][root][INFO] - Training Epoch: 7/10, step 293/574 completed (loss: 0.02125271037220955, acc: 0.982758641242981)
[2025-01-06 01:38:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:45][root][INFO] - Training Epoch: 7/10, step 294/574 completed (loss: 0.03578713908791542, acc: 0.9818181991577148)
[2025-01-06 01:38:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:46][root][INFO] - Training Epoch: 7/10, step 295/574 completed (loss: 0.15123897790908813, acc: 0.9536082744598389)
[2025-01-06 01:38:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:46][root][INFO] - Training Epoch: 7/10, step 296/574 completed (loss: 0.07427627593278885, acc: 0.982758641242981)
[2025-01-06 01:38:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:46][root][INFO] - Training Epoch: 7/10, step 297/574 completed (loss: 0.013074018992483616, acc: 1.0)
[2025-01-06 01:38:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:47][root][INFO] - Training Epoch: 7/10, step 298/574 completed (loss: 0.18187205493450165, acc: 0.9473684430122375)
[2025-01-06 01:38:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:47][root][INFO] - Training Epoch: 7/10, step 299/574 completed (loss: 0.025965729728341103, acc: 0.9821428656578064)
[2025-01-06 01:38:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:47][root][INFO] - Training Epoch: 7/10, step 300/574 completed (loss: 0.0010529060382395983, acc: 1.0)
[2025-01-06 01:38:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:48][root][INFO] - Training Epoch: 7/10, step 301/574 completed (loss: 0.017541123554110527, acc: 1.0)
[2025-01-06 01:38:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:48][root][INFO] - Training Epoch: 7/10, step 302/574 completed (loss: 0.05892422795295715, acc: 0.9811320900917053)
[2025-01-06 01:38:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:48][root][INFO] - Training Epoch: 7/10, step 303/574 completed (loss: 0.011053591035306454, acc: 1.0)
[2025-01-06 01:38:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:49][root][INFO] - Training Epoch: 7/10, step 304/574 completed (loss: 0.000926839595194906, acc: 1.0)
[2025-01-06 01:38:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:49][root][INFO] - Training Epoch: 7/10, step 305/574 completed (loss: 0.04565110430121422, acc: 0.9836065769195557)
[2025-01-06 01:38:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:50][root][INFO] - Training Epoch: 7/10, step 306/574 completed (loss: 0.006886618677526712, acc: 1.0)
[2025-01-06 01:38:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:50][root][INFO] - Training Epoch: 7/10, step 307/574 completed (loss: 0.00533638708293438, acc: 1.0)
[2025-01-06 01:38:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:50][root][INFO] - Training Epoch: 7/10, step 308/574 completed (loss: 0.010679432190954685, acc: 1.0)
[2025-01-06 01:38:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:51][root][INFO] - Training Epoch: 7/10, step 309/574 completed (loss: 0.017344733700156212, acc: 1.0)
[2025-01-06 01:38:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:51][root][INFO] - Training Epoch: 7/10, step 310/574 completed (loss: 0.052681416273117065, acc: 0.9759036302566528)
[2025-01-06 01:38:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:51][root][INFO] - Training Epoch: 7/10, step 311/574 completed (loss: 0.09433547407388687, acc: 0.9615384340286255)
[2025-01-06 01:38:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:52][root][INFO] - Training Epoch: 7/10, step 312/574 completed (loss: 0.011476748622953892, acc: 1.0)
[2025-01-06 01:38:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:52][root][INFO] - Training Epoch: 7/10, step 313/574 completed (loss: 0.0007533243042416871, acc: 1.0)
[2025-01-06 01:38:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:52][root][INFO] - Training Epoch: 7/10, step 314/574 completed (loss: 0.022413725033402443, acc: 1.0)
[2025-01-06 01:38:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:53][root][INFO] - Training Epoch: 7/10, step 315/574 completed (loss: 0.08833938837051392, acc: 0.9677419066429138)
[2025-01-06 01:38:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:53][root][INFO] - Training Epoch: 7/10, step 316/574 completed (loss: 0.01752205565571785, acc: 1.0)
[2025-01-06 01:38:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:53][root][INFO] - Training Epoch: 7/10, step 317/574 completed (loss: 0.010176915675401688, acc: 1.0)
[2025-01-06 01:38:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:54][root][INFO] - Training Epoch: 7/10, step 318/574 completed (loss: 0.015773454681038857, acc: 0.9903846383094788)
[2025-01-06 01:38:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:54][root][INFO] - Training Epoch: 7/10, step 319/574 completed (loss: 0.04290485754609108, acc: 0.9777777791023254)
[2025-01-06 01:38:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:54][root][INFO] - Training Epoch: 7/10, step 320/574 completed (loss: 0.022302469238638878, acc: 0.9838709831237793)
[2025-01-06 01:38:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:55][root][INFO] - Training Epoch: 7/10, step 321/574 completed (loss: 0.0028391508385539055, acc: 1.0)
[2025-01-06 01:38:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:55][root][INFO] - Training Epoch: 7/10, step 322/574 completed (loss: 0.07311786711215973, acc: 0.9629629850387573)
[2025-01-06 01:38:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:56][root][INFO] - Training Epoch: 7/10, step 323/574 completed (loss: 0.0611710399389267, acc: 0.9714285731315613)
[2025-01-06 01:38:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:56][root][INFO] - Training Epoch: 7/10, step 324/574 completed (loss: 0.06970439851284027, acc: 0.9743589758872986)
[2025-01-06 01:38:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:56][root][INFO] - Training Epoch: 7/10, step 325/574 completed (loss: 0.19354526698589325, acc: 0.9512194991111755)
[2025-01-06 01:38:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:57][root][INFO] - Training Epoch: 7/10, step 326/574 completed (loss: 0.11536350101232529, acc: 0.9736841917037964)
[2025-01-06 01:38:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:57][root][INFO] - Training Epoch: 7/10, step 327/574 completed (loss: 0.009510700590908527, acc: 1.0)
[2025-01-06 01:38:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:57][root][INFO] - Training Epoch: 7/10, step 328/574 completed (loss: 0.0021870818454772234, acc: 1.0)
[2025-01-06 01:38:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:58][root][INFO] - Training Epoch: 7/10, step 329/574 completed (loss: 0.0074240900576114655, acc: 1.0)
[2025-01-06 01:38:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:58][root][INFO] - Training Epoch: 7/10, step 330/574 completed (loss: 0.00390580203384161, acc: 1.0)
[2025-01-06 01:38:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:58][root][INFO] - Training Epoch: 7/10, step 331/574 completed (loss: 0.023772476240992546, acc: 0.9838709831237793)
[2025-01-06 01:38:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:59][root][INFO] - Training Epoch: 7/10, step 332/574 completed (loss: 0.003704830538481474, acc: 1.0)
[2025-01-06 01:38:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:38:59][root][INFO] - Training Epoch: 7/10, step 333/574 completed (loss: 0.018664969131350517, acc: 1.0)
[2025-01-06 01:38:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:00][root][INFO] - Training Epoch: 7/10, step 334/574 completed (loss: 0.0031985712703317404, acc: 1.0)
[2025-01-06 01:39:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:00][root][INFO] - Training Epoch: 7/10, step 335/574 completed (loss: 0.004075898323208094, acc: 1.0)
[2025-01-06 01:39:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:00][root][INFO] - Training Epoch: 7/10, step 336/574 completed (loss: 0.08674630522727966, acc: 0.9800000190734863)
[2025-01-06 01:39:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:01][root][INFO] - Training Epoch: 7/10, step 337/574 completed (loss: 0.10853679478168488, acc: 0.9655172228813171)
[2025-01-06 01:39:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:01][root][INFO] - Training Epoch: 7/10, step 338/574 completed (loss: 0.3190551698207855, acc: 0.9255319237709045)
[2025-01-06 01:39:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:01][root][INFO] - Training Epoch: 7/10, step 339/574 completed (loss: 0.14779354631900787, acc: 0.9638554453849792)
[2025-01-06 01:39:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:02][root][INFO] - Training Epoch: 7/10, step 340/574 completed (loss: 0.0019452780252322555, acc: 1.0)
[2025-01-06 01:39:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:02][root][INFO] - Training Epoch: 7/10, step 341/574 completed (loss: 0.008283154107630253, acc: 1.0)
[2025-01-06 01:39:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:02][root][INFO] - Training Epoch: 7/10, step 342/574 completed (loss: 0.06293142586946487, acc: 0.9638554453849792)
[2025-01-06 01:39:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:03][root][INFO] - Training Epoch: 7/10, step 343/574 completed (loss: 0.04235818609595299, acc: 1.0)
[2025-01-06 01:39:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:03][root][INFO] - Training Epoch: 7/10, step 344/574 completed (loss: 0.03413727134466171, acc: 0.9620253443717957)
[2025-01-06 01:39:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:03][root][INFO] - Training Epoch: 7/10, step 345/574 completed (loss: 0.009280181489884853, acc: 1.0)
[2025-01-06 01:39:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:04][root][INFO] - Training Epoch: 7/10, step 346/574 completed (loss: 0.022553108632564545, acc: 0.9850746393203735)
[2025-01-06 01:39:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:04][root][INFO] - Training Epoch: 7/10, step 347/574 completed (loss: 0.471735417842865, acc: 0.949999988079071)
[2025-01-06 01:39:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:04][root][INFO] - Training Epoch: 7/10, step 348/574 completed (loss: 0.04508783668279648, acc: 0.9599999785423279)
[2025-01-06 01:39:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:05][root][INFO] - Training Epoch: 7/10, step 349/574 completed (loss: 0.3549598157405853, acc: 0.8888888955116272)
[2025-01-06 01:39:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:05][root][INFO] - Training Epoch: 7/10, step 350/574 completed (loss: 0.09857074916362762, acc: 0.9534883499145508)
[2025-01-06 01:39:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:06][root][INFO] - Training Epoch: 7/10, step 351/574 completed (loss: 0.0018379153916612267, acc: 1.0)
[2025-01-06 01:39:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:06][root][INFO] - Training Epoch: 7/10, step 352/574 completed (loss: 0.054468415677547455, acc: 0.9777777791023254)
[2025-01-06 01:39:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:06][root][INFO] - Training Epoch: 7/10, step 353/574 completed (loss: 0.05275563523173332, acc: 0.95652174949646)
[2025-01-06 01:39:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:07][root][INFO] - Training Epoch: 7/10, step 354/574 completed (loss: 0.01785902865231037, acc: 1.0)
[2025-01-06 01:39:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:07][root][INFO] - Training Epoch: 7/10, step 355/574 completed (loss: 0.1279093474149704, acc: 0.9230769276618958)
[2025-01-06 01:39:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:07][root][INFO] - Training Epoch: 7/10, step 356/574 completed (loss: 0.1837042272090912, acc: 0.947826087474823)
[2025-01-06 01:39:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:08][root][INFO] - Training Epoch: 7/10, step 357/574 completed (loss: 0.055721256881952286, acc: 0.97826087474823)
[2025-01-06 01:39:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:08][root][INFO] - Training Epoch: 7/10, step 358/574 completed (loss: 0.05625090003013611, acc: 0.9591836929321289)
[2025-01-06 01:39:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:08][root][INFO] - Training Epoch: 7/10, step 359/574 completed (loss: 0.0005571426590904593, acc: 1.0)
[2025-01-06 01:39:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:09][root][INFO] - Training Epoch: 7/10, step 360/574 completed (loss: 0.005219758488237858, acc: 1.0)
[2025-01-06 01:39:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:09][root][INFO] - Training Epoch: 7/10, step 361/574 completed (loss: 0.017200864851474762, acc: 1.0)
[2025-01-06 01:39:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:09][root][INFO] - Training Epoch: 7/10, step 362/574 completed (loss: 0.007620353251695633, acc: 1.0)
[2025-01-06 01:39:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:10][root][INFO] - Training Epoch: 7/10, step 363/574 completed (loss: 0.007254365365952253, acc: 1.0)
[2025-01-06 01:39:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:10][root][INFO] - Training Epoch: 7/10, step 364/574 completed (loss: 0.012015004642307758, acc: 1.0)
[2025-01-06 01:39:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:10][root][INFO] - Training Epoch: 7/10, step 365/574 completed (loss: 0.008884460665285587, acc: 1.0)
[2025-01-06 01:39:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:11][root][INFO] - Training Epoch: 7/10, step 366/574 completed (loss: 0.0002334020537091419, acc: 1.0)
[2025-01-06 01:39:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:11][root][INFO] - Training Epoch: 7/10, step 367/574 completed (loss: 0.0016073889564722776, acc: 1.0)
[2025-01-06 01:39:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:11][root][INFO] - Training Epoch: 7/10, step 368/574 completed (loss: 0.010247534140944481, acc: 1.0)
[2025-01-06 01:39:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:12][root][INFO] - Training Epoch: 7/10, step 369/574 completed (loss: 0.024130798876285553, acc: 1.0)
[2025-01-06 01:39:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:12][root][INFO] - Training Epoch: 7/10, step 370/574 completed (loss: 0.16233505308628082, acc: 0.9515151381492615)
[2025-01-06 01:39:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:13][root][INFO] - Training Epoch: 7/10, step 371/574 completed (loss: 0.041520487517118454, acc: 0.9905660152435303)
[2025-01-06 01:39:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:14][root][INFO] - Training Epoch: 7/10, step 372/574 completed (loss: 0.03347424790263176, acc: 1.0)
[2025-01-06 01:39:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:14][root][INFO] - Training Epoch: 7/10, step 373/574 completed (loss: 0.020574605092406273, acc: 1.0)
[2025-01-06 01:39:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:14][root][INFO] - Training Epoch: 7/10, step 374/574 completed (loss: 0.002661748556420207, acc: 1.0)
[2025-01-06 01:39:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:15][root][INFO] - Training Epoch: 7/10, step 375/574 completed (loss: 0.00012478102871682495, acc: 1.0)
[2025-01-06 01:39:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:15][root][INFO] - Training Epoch: 7/10, step 376/574 completed (loss: 0.0001411221455782652, acc: 1.0)
[2025-01-06 01:39:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:15][root][INFO] - Training Epoch: 7/10, step 377/574 completed (loss: 0.004346113186329603, acc: 1.0)
[2025-01-06 01:39:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:16][root][INFO] - Training Epoch: 7/10, step 378/574 completed (loss: 0.001367542427033186, acc: 1.0)
[2025-01-06 01:39:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:16][root][INFO] - Training Epoch: 7/10, step 379/574 completed (loss: 0.07205692678689957, acc: 0.9640718698501587)
[2025-01-06 01:39:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:17][root][INFO] - Training Epoch: 7/10, step 380/574 completed (loss: 0.061434343457221985, acc: 0.9774436354637146)
[2025-01-06 01:39:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:18][root][INFO] - Training Epoch: 7/10, step 381/574 completed (loss: 0.20552727580070496, acc: 0.9304812550544739)
[2025-01-06 01:39:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:18][root][INFO] - Training Epoch: 7/10, step 382/574 completed (loss: 0.05466679111123085, acc: 0.9819819927215576)
[2025-01-06 01:39:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:19][root][INFO] - Training Epoch: 7/10, step 383/574 completed (loss: 0.008230520412325859, acc: 1.0)
[2025-01-06 01:39:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:19][root][INFO] - Training Epoch: 7/10, step 384/574 completed (loss: 0.0008319303160533309, acc: 1.0)
[2025-01-06 01:39:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:19][root][INFO] - Training Epoch: 7/10, step 385/574 completed (loss: 0.00252925674431026, acc: 1.0)
[2025-01-06 01:39:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:20][root][INFO] - Training Epoch: 7/10, step 386/574 completed (loss: 0.0004702662117779255, acc: 1.0)
[2025-01-06 01:39:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:20][root][INFO] - Training Epoch: 7/10, step 387/574 completed (loss: 0.0002993001544382423, acc: 1.0)
[2025-01-06 01:39:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:20][root][INFO] - Training Epoch: 7/10, step 388/574 completed (loss: 0.0003045813355129212, acc: 1.0)
[2025-01-06 01:39:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:21][root][INFO] - Training Epoch: 7/10, step 389/574 completed (loss: 0.0007363929180428386, acc: 1.0)
[2025-01-06 01:39:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:21][root][INFO] - Training Epoch: 7/10, step 390/574 completed (loss: 0.01739899069070816, acc: 1.0)
[2025-01-06 01:39:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:21][root][INFO] - Training Epoch: 7/10, step 391/574 completed (loss: 0.05434027314186096, acc: 0.9814814925193787)
[2025-01-06 01:39:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:22][root][INFO] - Training Epoch: 7/10, step 392/574 completed (loss: 0.18040433526039124, acc: 0.9417475461959839)
[2025-01-06 01:39:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:22][root][INFO] - Training Epoch: 7/10, step 393/574 completed (loss: 0.23734420537948608, acc: 0.9191176295280457)
[2025-01-06 01:39:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:23][root][INFO] - Training Epoch: 7/10, step 394/574 completed (loss: 0.09434735029935837, acc: 0.9733333587646484)
[2025-01-06 01:39:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:23][root][INFO] - Training Epoch: 7/10, step 395/574 completed (loss: 0.11566837877035141, acc: 0.9583333134651184)
[2025-01-06 01:39:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:23][root][INFO] - Training Epoch: 7/10, step 396/574 completed (loss: 0.05374637246131897, acc: 0.9767441749572754)
[2025-01-06 01:39:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:24][root][INFO] - Training Epoch: 7/10, step 397/574 completed (loss: 0.0029464776162058115, acc: 1.0)
[2025-01-06 01:39:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:24][root][INFO] - Training Epoch: 7/10, step 398/574 completed (loss: 0.019684825092554092, acc: 1.0)
[2025-01-06 01:39:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:24][root][INFO] - Training Epoch: 7/10, step 399/574 completed (loss: 0.006554256193339825, acc: 1.0)
[2025-01-06 01:39:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:25][root][INFO] - Training Epoch: 7/10, step 400/574 completed (loss: 0.027875587344169617, acc: 0.9852941036224365)
[2025-01-06 01:39:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:25][root][INFO] - Training Epoch: 7/10, step 401/574 completed (loss: 0.07542995363473892, acc: 0.9733333587646484)
[2025-01-06 01:39:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:26][root][INFO] - Training Epoch: 7/10, step 402/574 completed (loss: 0.06707337498664856, acc: 0.9696969985961914)
[2025-01-06 01:39:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:26][root][INFO] - Training Epoch: 7/10, step 403/574 completed (loss: 0.18123899400234222, acc: 0.9696969985961914)
[2025-01-06 01:39:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:26][root][INFO] - Training Epoch: 7/10, step 404/574 completed (loss: 0.003408104181289673, acc: 1.0)
[2025-01-06 01:39:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:27][root][INFO] - Training Epoch: 7/10, step 405/574 completed (loss: 0.009408817626535892, acc: 1.0)
[2025-01-06 01:39:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:27][root][INFO] - Training Epoch: 7/10, step 406/574 completed (loss: 0.0006336006335914135, acc: 1.0)
[2025-01-06 01:39:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:27][root][INFO] - Training Epoch: 7/10, step 407/574 completed (loss: 0.02898101694881916, acc: 0.9722222089767456)
[2025-01-06 01:39:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:28][root][INFO] - Training Epoch: 7/10, step 408/574 completed (loss: 0.0007976027554832399, acc: 1.0)
[2025-01-06 01:39:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:28][root][INFO] - Training Epoch: 7/10, step 409/574 completed (loss: 0.00313750677742064, acc: 1.0)
[2025-01-06 01:39:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:28][root][INFO] - Training Epoch: 7/10, step 410/574 completed (loss: 0.011249745264649391, acc: 1.0)
[2025-01-06 01:39:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:29][root][INFO] - Training Epoch: 7/10, step 411/574 completed (loss: 0.0029088675510138273, acc: 1.0)
[2025-01-06 01:39:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:29][root][INFO] - Training Epoch: 7/10, step 412/574 completed (loss: 0.0003526675282046199, acc: 1.0)
[2025-01-06 01:39:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:29][root][INFO] - Training Epoch: 7/10, step 413/574 completed (loss: 0.007526574190706015, acc: 1.0)
[2025-01-06 01:39:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:30][root][INFO] - Training Epoch: 7/10, step 414/574 completed (loss: 0.04291164502501488, acc: 0.9545454382896423)
[2025-01-06 01:39:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:30][root][INFO] - Training Epoch: 7/10, step 415/574 completed (loss: 0.1018039882183075, acc: 0.9607843160629272)
[2025-01-06 01:39:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:30][root][INFO] - Training Epoch: 7/10, step 416/574 completed (loss: 0.005555709823966026, acc: 1.0)
[2025-01-06 01:39:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:39:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:01][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.2787, device='cuda:0') eval_epoch_loss=tensor(0.8236, device='cuda:0') eval_epoch_acc=tensor(0.8426, device='cuda:0')
[2025-01-06 01:40:01][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:40:01][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:40:01][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_7_step_417_loss_0.8236241936683655/model.pt
[2025-01-06 01:40:01][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:40:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:02][root][INFO] - Training Epoch: 7/10, step 417/574 completed (loss: 0.011305004358291626, acc: 1.0)
[2025-01-06 01:40:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:02][root][INFO] - Training Epoch: 7/10, step 418/574 completed (loss: 0.011938373558223248, acc: 1.0)
[2025-01-06 01:40:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:02][root][INFO] - Training Epoch: 7/10, step 419/574 completed (loss: 0.22693702578544617, acc: 0.949999988079071)
[2025-01-06 01:40:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:03][root][INFO] - Training Epoch: 7/10, step 420/574 completed (loss: 0.0007845332729630172, acc: 1.0)
[2025-01-06 01:40:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:03][root][INFO] - Training Epoch: 7/10, step 421/574 completed (loss: 0.008661055937409401, acc: 1.0)
[2025-01-06 01:40:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:03][root][INFO] - Training Epoch: 7/10, step 422/574 completed (loss: 0.11600328236818314, acc: 0.96875)
[2025-01-06 01:40:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:04][root][INFO] - Training Epoch: 7/10, step 423/574 completed (loss: 0.014451139606535435, acc: 1.0)
[2025-01-06 01:40:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:04][root][INFO] - Training Epoch: 7/10, step 424/574 completed (loss: 0.0009275208576582372, acc: 1.0)
[2025-01-06 01:40:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:04][root][INFO] - Training Epoch: 7/10, step 425/574 completed (loss: 0.01735064759850502, acc: 1.0)
[2025-01-06 01:40:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:05][root][INFO] - Training Epoch: 7/10, step 426/574 completed (loss: 0.13675351440906525, acc: 0.95652174949646)
[2025-01-06 01:40:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:05][root][INFO] - Training Epoch: 7/10, step 427/574 completed (loss: 0.01909846067428589, acc: 1.0)
[2025-01-06 01:40:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:05][root][INFO] - Training Epoch: 7/10, step 428/574 completed (loss: 0.02365647442638874, acc: 1.0)
[2025-01-06 01:40:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:06][root][INFO] - Training Epoch: 7/10, step 429/574 completed (loss: 0.26695042848587036, acc: 0.95652174949646)
[2025-01-06 01:40:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:06][root][INFO] - Training Epoch: 7/10, step 430/574 completed (loss: 0.003588673658668995, acc: 1.0)
[2025-01-06 01:40:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:06][root][INFO] - Training Epoch: 7/10, step 431/574 completed (loss: 0.00012633552250918, acc: 1.0)
[2025-01-06 01:40:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:07][root][INFO] - Training Epoch: 7/10, step 432/574 completed (loss: 0.00030957473791204393, acc: 1.0)
[2025-01-06 01:40:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:07][root][INFO] - Training Epoch: 7/10, step 433/574 completed (loss: 0.015981055796146393, acc: 1.0)
[2025-01-06 01:40:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:07][root][INFO] - Training Epoch: 7/10, step 434/574 completed (loss: 0.0002923218999058008, acc: 1.0)
[2025-01-06 01:40:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:08][root][INFO] - Training Epoch: 7/10, step 435/574 completed (loss: 0.0006086572539061308, acc: 1.0)
[2025-01-06 01:40:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:08][root][INFO] - Training Epoch: 7/10, step 436/574 completed (loss: 0.02176552265882492, acc: 1.0)
[2025-01-06 01:40:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:08][root][INFO] - Training Epoch: 7/10, step 437/574 completed (loss: 0.001143458066508174, acc: 1.0)
[2025-01-06 01:40:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:09][root][INFO] - Training Epoch: 7/10, step 438/574 completed (loss: 0.00024546580971218646, acc: 1.0)
[2025-01-06 01:40:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:09][root][INFO] - Training Epoch: 7/10, step 439/574 completed (loss: 0.11644061654806137, acc: 0.9743589758872986)
[2025-01-06 01:40:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:10][root][INFO] - Training Epoch: 7/10, step 440/574 completed (loss: 0.014279845170676708, acc: 1.0)
[2025-01-06 01:40:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:10][root][INFO] - Training Epoch: 7/10, step 441/574 completed (loss: 0.1713217794895172, acc: 0.9279999732971191)
[2025-01-06 01:40:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:11][root][INFO] - Training Epoch: 7/10, step 442/574 completed (loss: 0.14502660930156708, acc: 0.9677419066429138)
[2025-01-06 01:40:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:11][root][INFO] - Training Epoch: 7/10, step 443/574 completed (loss: 0.13813497126102448, acc: 0.9452736377716064)
[2025-01-06 01:40:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:12][root][INFO] - Training Epoch: 7/10, step 444/574 completed (loss: 0.10317393392324448, acc: 0.9811320900917053)
[2025-01-06 01:40:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:12][root][INFO] - Training Epoch: 7/10, step 445/574 completed (loss: 0.014047667384147644, acc: 1.0)
[2025-01-06 01:40:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:13][root][INFO] - Training Epoch: 7/10, step 446/574 completed (loss: 0.001991748809814453, acc: 1.0)
[2025-01-06 01:40:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:13][root][INFO] - Training Epoch: 7/10, step 447/574 completed (loss: 0.006641075015068054, acc: 1.0)
[2025-01-06 01:40:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:13][root][INFO] - Training Epoch: 7/10, step 448/574 completed (loss: 0.0047341203317046165, acc: 1.0)
[2025-01-06 01:40:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:14][root][INFO] - Training Epoch: 7/10, step 449/574 completed (loss: 0.0036334169562906027, acc: 1.0)
[2025-01-06 01:40:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:14][root][INFO] - Training Epoch: 7/10, step 450/574 completed (loss: 0.019349228590726852, acc: 1.0)
[2025-01-06 01:40:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:14][root][INFO] - Training Epoch: 7/10, step 451/574 completed (loss: 0.04561459645628929, acc: 0.989130437374115)
[2025-01-06 01:40:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:15][root][INFO] - Training Epoch: 7/10, step 452/574 completed (loss: 0.02704150229692459, acc: 0.9871794581413269)
[2025-01-06 01:40:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:15][root][INFO] - Training Epoch: 7/10, step 453/574 completed (loss: 0.17364175617694855, acc: 0.9473684430122375)
[2025-01-06 01:40:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:15][root][INFO] - Training Epoch: 7/10, step 454/574 completed (loss: 0.014118644408881664, acc: 1.0)
[2025-01-06 01:40:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:16][root][INFO] - Training Epoch: 7/10, step 455/574 completed (loss: 0.18505771458148956, acc: 0.939393937587738)
[2025-01-06 01:40:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:16][root][INFO] - Training Epoch: 7/10, step 456/574 completed (loss: 0.044343866407871246, acc: 0.9896907210350037)
[2025-01-06 01:40:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:16][root][INFO] - Training Epoch: 7/10, step 457/574 completed (loss: 0.0029291652608662844, acc: 1.0)
[2025-01-06 01:40:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:17][root][INFO] - Training Epoch: 7/10, step 458/574 completed (loss: 0.0882648453116417, acc: 0.9709302186965942)
[2025-01-06 01:40:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:17][root][INFO] - Training Epoch: 7/10, step 459/574 completed (loss: 0.008007684722542763, acc: 1.0)
[2025-01-06 01:40:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:18][root][INFO] - Training Epoch: 7/10, step 460/574 completed (loss: 0.023689113557338715, acc: 0.9876543283462524)
[2025-01-06 01:40:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:18][root][INFO] - Training Epoch: 7/10, step 461/574 completed (loss: 0.027645962312817574, acc: 1.0)
[2025-01-06 01:40:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:18][root][INFO] - Training Epoch: 7/10, step 462/574 completed (loss: 0.0017984033329412341, acc: 1.0)
[2025-01-06 01:40:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:19][root][INFO] - Training Epoch: 7/10, step 463/574 completed (loss: 0.027823276817798615, acc: 1.0)
[2025-01-06 01:40:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:19][root][INFO] - Training Epoch: 7/10, step 464/574 completed (loss: 0.0019584600813686848, acc: 1.0)
[2025-01-06 01:40:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:19][root][INFO] - Training Epoch: 7/10, step 465/574 completed (loss: 0.021847853437066078, acc: 1.0)
[2025-01-06 01:40:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:20][root][INFO] - Training Epoch: 7/10, step 466/574 completed (loss: 0.07459460943937302, acc: 0.9638554453849792)
[2025-01-06 01:40:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:20][root][INFO] - Training Epoch: 7/10, step 467/574 completed (loss: 0.06703509390354156, acc: 0.9819819927215576)
[2025-01-06 01:40:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:20][root][INFO] - Training Epoch: 7/10, step 468/574 completed (loss: 0.08362433314323425, acc: 0.9611650705337524)
[2025-01-06 01:40:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:21][root][INFO] - Training Epoch: 7/10, step 469/574 completed (loss: 0.07358501106500626, acc: 0.9837398529052734)
[2025-01-06 01:40:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:21][root][INFO] - Training Epoch: 7/10, step 470/574 completed (loss: 0.0061713457107543945, acc: 1.0)
[2025-01-06 01:40:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:22][root][INFO] - Training Epoch: 7/10, step 471/574 completed (loss: 0.002168989507481456, acc: 1.0)
[2025-01-06 01:40:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:22][root][INFO] - Training Epoch: 7/10, step 472/574 completed (loss: 0.14380857348442078, acc: 0.9607843160629272)
[2025-01-06 01:40:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:22][root][INFO] - Training Epoch: 7/10, step 473/574 completed (loss: 0.21393685042858124, acc: 0.9388646483421326)
[2025-01-06 01:40:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:23][root][INFO] - Training Epoch: 7/10, step 474/574 completed (loss: 0.07840154320001602, acc: 0.96875)
[2025-01-06 01:40:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:23][root][INFO] - Training Epoch: 7/10, step 475/574 completed (loss: 0.1060294434428215, acc: 0.9447852969169617)
[2025-01-06 01:40:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:23][root][INFO] - Training Epoch: 7/10, step 476/574 completed (loss: 0.126651793718338, acc: 0.9568345546722412)
[2025-01-06 01:40:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:24][root][INFO] - Training Epoch: 7/10, step 477/574 completed (loss: 0.1352698653936386, acc: 0.9497487545013428)
[2025-01-06 01:40:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:24][root][INFO] - Training Epoch: 7/10, step 478/574 completed (loss: 0.03016582503914833, acc: 1.0)
[2025-01-06 01:40:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:25][root][INFO] - Training Epoch: 7/10, step 479/574 completed (loss: 0.013277355581521988, acc: 1.0)
[2025-01-06 01:40:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:25][root][INFO] - Training Epoch: 7/10, step 480/574 completed (loss: 0.0037172299344092607, acc: 1.0)
[2025-01-06 01:40:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:25][root][INFO] - Training Epoch: 7/10, step 481/574 completed (loss: 0.009587913751602173, acc: 1.0)
[2025-01-06 01:40:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:26][root][INFO] - Training Epoch: 7/10, step 482/574 completed (loss: 0.0567130371928215, acc: 0.949999988079071)
[2025-01-06 01:40:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:26][root][INFO] - Training Epoch: 7/10, step 483/574 completed (loss: 0.03556423634290695, acc: 1.0)
[2025-01-06 01:40:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:26][root][INFO] - Training Epoch: 7/10, step 484/574 completed (loss: 0.005540528334677219, acc: 1.0)
[2025-01-06 01:40:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:27][root][INFO] - Training Epoch: 7/10, step 485/574 completed (loss: 0.013481169939041138, acc: 1.0)
[2025-01-06 01:40:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:27][root][INFO] - Training Epoch: 7/10, step 486/574 completed (loss: 0.04530573636293411, acc: 1.0)
[2025-01-06 01:40:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:27][root][INFO] - Training Epoch: 7/10, step 487/574 completed (loss: 0.02456699125468731, acc: 1.0)
[2025-01-06 01:40:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:28][root][INFO] - Training Epoch: 7/10, step 488/574 completed (loss: 0.14755471050739288, acc: 0.9090909361839294)
[2025-01-06 01:40:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:28][root][INFO] - Training Epoch: 7/10, step 489/574 completed (loss: 0.09162013232707977, acc: 0.9538461565971375)
[2025-01-06 01:40:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:28][root][INFO] - Training Epoch: 7/10, step 490/574 completed (loss: 0.027152955532073975, acc: 1.0)
[2025-01-06 01:40:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:29][root][INFO] - Training Epoch: 7/10, step 491/574 completed (loss: 0.015701932832598686, acc: 1.0)
[2025-01-06 01:40:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:29][root][INFO] - Training Epoch: 7/10, step 492/574 completed (loss: 0.017256610095500946, acc: 1.0)
[2025-01-06 01:40:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:29][root][INFO] - Training Epoch: 7/10, step 493/574 completed (loss: 0.328881174325943, acc: 0.9655172228813171)
[2025-01-06 01:40:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:30][root][INFO] - Training Epoch: 7/10, step 494/574 completed (loss: 0.0017246680799871683, acc: 1.0)
[2025-01-06 01:40:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:30][root][INFO] - Training Epoch: 7/10, step 495/574 completed (loss: 0.0017056249780580401, acc: 1.0)
[2025-01-06 01:40:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:30][root][INFO] - Training Epoch: 7/10, step 496/574 completed (loss: 0.1001960039138794, acc: 0.9821428656578064)
[2025-01-06 01:40:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:31][root][INFO] - Training Epoch: 7/10, step 497/574 completed (loss: 0.04855649545788765, acc: 0.9775280952453613)
[2025-01-06 01:40:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:31][root][INFO] - Training Epoch: 7/10, step 498/574 completed (loss: 0.10136664658784866, acc: 0.9775280952453613)
[2025-01-06 01:40:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:31][root][INFO] - Training Epoch: 7/10, step 499/574 completed (loss: 0.3792698085308075, acc: 0.9078013896942139)
[2025-01-06 01:40:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:32][root][INFO] - Training Epoch: 7/10, step 500/574 completed (loss: 0.08042880147695541, acc: 0.989130437374115)
[2025-01-06 01:40:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:32][root][INFO] - Training Epoch: 7/10, step 501/574 completed (loss: 0.04273270070552826, acc: 0.9599999785423279)
[2025-01-06 01:40:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:32][root][INFO] - Training Epoch: 7/10, step 502/574 completed (loss: 0.00016825793136376888, acc: 1.0)
[2025-01-06 01:40:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:33][root][INFO] - Training Epoch: 7/10, step 503/574 completed (loss: 0.05811520293354988, acc: 0.9629629850387573)
[2025-01-06 01:40:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:33][root][INFO] - Training Epoch: 7/10, step 504/574 completed (loss: 0.013323948718607426, acc: 1.0)
[2025-01-06 01:40:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:33][root][INFO] - Training Epoch: 7/10, step 505/574 completed (loss: 0.12966549396514893, acc: 0.9433962106704712)
[2025-01-06 01:40:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:34][root][INFO] - Training Epoch: 7/10, step 506/574 completed (loss: 0.06296263635158539, acc: 0.9655172228813171)
[2025-01-06 01:40:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:34][root][INFO] - Training Epoch: 7/10, step 507/574 completed (loss: 0.17899222671985626, acc: 0.9369369149208069)
[2025-01-06 01:40:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:35][root][INFO] - Training Epoch: 7/10, step 508/574 completed (loss: 0.06759771704673767, acc: 0.9718309640884399)
[2025-01-06 01:40:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:35][root][INFO] - Training Epoch: 7/10, step 509/574 completed (loss: 0.000433055916801095, acc: 1.0)
[2025-01-06 01:40:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:36][root][INFO] - Training Epoch: 7/10, step 510/574 completed (loss: 0.004792148247361183, acc: 1.0)
[2025-01-06 01:40:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:36][root][INFO] - Training Epoch: 7/10, step 511/574 completed (loss: 0.01494678296148777, acc: 1.0)
[2025-01-06 01:40:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:39][root][INFO] - Training Epoch: 7/10, step 512/574 completed (loss: 0.16956357657909393, acc: 0.9714285731315613)
[2025-01-06 01:40:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:39][root][INFO] - Training Epoch: 7/10, step 513/574 completed (loss: 0.07422038167715073, acc: 0.976190447807312)
[2025-01-06 01:40:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:40][root][INFO] - Training Epoch: 7/10, step 514/574 completed (loss: 0.11884276568889618, acc: 0.9642857313156128)
[2025-01-06 01:40:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:40][root][INFO] - Training Epoch: 7/10, step 515/574 completed (loss: 0.003118910826742649, acc: 1.0)
[2025-01-06 01:40:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:41][root][INFO] - Training Epoch: 7/10, step 516/574 completed (loss: 0.18929354846477509, acc: 0.9583333134651184)
[2025-01-06 01:40:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:41][root][INFO] - Training Epoch: 7/10, step 517/574 completed (loss: 0.0013070199638605118, acc: 1.0)
[2025-01-06 01:40:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:41][root][INFO] - Training Epoch: 7/10, step 518/574 completed (loss: 0.09675164520740509, acc: 0.9677419066429138)
[2025-01-06 01:40:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:42][root][INFO] - Training Epoch: 7/10, step 519/574 completed (loss: 0.00855074729770422, acc: 1.0)
[2025-01-06 01:40:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:42][root][INFO] - Training Epoch: 7/10, step 520/574 completed (loss: 0.0005940741975791752, acc: 1.0)
[2025-01-06 01:40:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:43][root][INFO] - Training Epoch: 7/10, step 521/574 completed (loss: 0.3439781665802002, acc: 0.8983050584793091)
[2025-01-06 01:40:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:43][root][INFO] - Training Epoch: 7/10, step 522/574 completed (loss: 0.04255610331892967, acc: 0.9776119589805603)
[2025-01-06 01:40:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:44][root][INFO] - Training Epoch: 7/10, step 523/574 completed (loss: 0.059504494071006775, acc: 0.9781022071838379)
[2025-01-06 01:40:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:44][root][INFO] - Training Epoch: 7/10, step 524/574 completed (loss: 0.31291335821151733, acc: 0.9150000214576721)
[2025-01-06 01:40:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:45][root][INFO] - Training Epoch: 7/10, step 525/574 completed (loss: 0.0013502262299880385, acc: 1.0)
[2025-01-06 01:40:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:45][root][INFO] - Training Epoch: 7/10, step 526/574 completed (loss: 0.007851474918425083, acc: 1.0)
[2025-01-06 01:40:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:45][root][INFO] - Training Epoch: 7/10, step 527/574 completed (loss: 0.013267145492136478, acc: 1.0)
[2025-01-06 01:40:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:46][root][INFO] - Training Epoch: 7/10, step 528/574 completed (loss: 0.06137589365243912, acc: 0.9508196711540222)
[2025-01-06 01:40:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:46][root][INFO] - Training Epoch: 7/10, step 529/574 completed (loss: 0.018823204562067986, acc: 1.0)
[2025-01-06 01:40:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:46][root][INFO] - Training Epoch: 7/10, step 530/574 completed (loss: 0.0761757642030716, acc: 0.9767441749572754)
[2025-01-06 01:40:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:47][root][INFO] - Training Epoch: 7/10, step 531/574 completed (loss: 0.04342443495988846, acc: 1.0)
[2025-01-06 01:40:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:47][root][INFO] - Training Epoch: 7/10, step 532/574 completed (loss: 0.02448495849967003, acc: 1.0)
[2025-01-06 01:40:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:47][root][INFO] - Training Epoch: 7/10, step 533/574 completed (loss: 0.05448979139328003, acc: 0.9772727489471436)
[2025-01-06 01:40:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:48][root][INFO] - Training Epoch: 7/10, step 534/574 completed (loss: 0.007318529300391674, acc: 1.0)
[2025-01-06 01:40:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:48][root][INFO] - Training Epoch: 7/10, step 535/574 completed (loss: 0.004004199989140034, acc: 1.0)
[2025-01-06 01:40:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:48][root][INFO] - Training Epoch: 7/10, step 536/574 completed (loss: 0.0011148892808705568, acc: 1.0)
[2025-01-06 01:40:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:49][root][INFO] - Training Epoch: 7/10, step 537/574 completed (loss: 0.04522997885942459, acc: 0.9846153855323792)
[2025-01-06 01:40:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:49][root][INFO] - Training Epoch: 7/10, step 538/574 completed (loss: 0.03751780837774277, acc: 0.984375)
[2025-01-06 01:40:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:49][root][INFO] - Training Epoch: 7/10, step 539/574 completed (loss: 0.011906512081623077, acc: 1.0)
[2025-01-06 01:40:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:50][root][INFO] - Training Epoch: 7/10, step 540/574 completed (loss: 0.10676797479391098, acc: 0.939393937587738)
[2025-01-06 01:40:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:50][root][INFO] - Training Epoch: 7/10, step 541/574 completed (loss: 0.20841281116008759, acc: 0.9375)
[2025-01-06 01:40:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:50][root][INFO] - Training Epoch: 7/10, step 542/574 completed (loss: 0.0015115150017663836, acc: 1.0)
[2025-01-06 01:40:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:51][root][INFO] - Training Epoch: 7/10, step 543/574 completed (loss: 0.00764613738283515, acc: 1.0)
[2025-01-06 01:40:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:51][root][INFO] - Training Epoch: 7/10, step 544/574 completed (loss: 0.0019644161220639944, acc: 1.0)
[2025-01-06 01:40:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:51][root][INFO] - Training Epoch: 7/10, step 545/574 completed (loss: 0.001206541433930397, acc: 1.0)
[2025-01-06 01:40:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:52][root][INFO] - Training Epoch: 7/10, step 546/574 completed (loss: 0.0007388510275632143, acc: 1.0)
[2025-01-06 01:40:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:52][root][INFO] - Training Epoch: 7/10, step 547/574 completed (loss: 0.0004928445559926331, acc: 1.0)
[2025-01-06 01:40:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:52][root][INFO] - Training Epoch: 7/10, step 548/574 completed (loss: 0.018118122592568398, acc: 1.0)
[2025-01-06 01:40:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:53][root][INFO] - Training Epoch: 7/10, step 549/574 completed (loss: 9.62316116783768e-05, acc: 1.0)
[2025-01-06 01:40:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:53][root][INFO] - Training Epoch: 7/10, step 550/574 completed (loss: 0.006722953636199236, acc: 1.0)
[2025-01-06 01:40:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:53][root][INFO] - Training Epoch: 7/10, step 551/574 completed (loss: 0.07035758346319199, acc: 0.9750000238418579)
[2025-01-06 01:40:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:54][root][INFO] - Training Epoch: 7/10, step 552/574 completed (loss: 0.002982927020639181, acc: 1.0)
[2025-01-06 01:40:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:54][root][INFO] - Training Epoch: 7/10, step 553/574 completed (loss: 0.024643460288643837, acc: 1.0)
[2025-01-06 01:40:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:55][root][INFO] - Training Epoch: 7/10, step 554/574 completed (loss: 0.05663010850548744, acc: 0.9862068891525269)
[2025-01-06 01:40:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:55][root][INFO] - Training Epoch: 7/10, step 555/574 completed (loss: 0.02546153962612152, acc: 0.9928571581840515)
[2025-01-06 01:40:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:55][root][INFO] - Training Epoch: 7/10, step 556/574 completed (loss: 0.03475892171263695, acc: 0.9867549538612366)
[2025-01-06 01:40:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:55][root][INFO] - Training Epoch: 7/10, step 557/574 completed (loss: 0.01496187038719654, acc: 1.0)
[2025-01-06 01:40:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:56][root][INFO] - Training Epoch: 7/10, step 558/574 completed (loss: 0.002112473826855421, acc: 1.0)
[2025-01-06 01:40:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:56][root][INFO] - Training Epoch: 7/10, step 559/574 completed (loss: 0.0022105632815510035, acc: 1.0)
[2025-01-06 01:40:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:40:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:27][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.2633, device='cuda:0') eval_epoch_loss=tensor(0.8168, device='cuda:0') eval_epoch_acc=tensor(0.8458, device='cuda:0')
[2025-01-06 01:41:27][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:41:27][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:41:27][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_7_step_560_loss_0.81681889295578/model.pt
[2025-01-06 01:41:27][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:41:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:27][root][INFO] - Training Epoch: 7/10, step 560/574 completed (loss: 0.0028806044720113277, acc: 1.0)
[2025-01-06 01:41:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:28][root][INFO] - Training Epoch: 7/10, step 561/574 completed (loss: 0.0039250836707651615, acc: 1.0)
[2025-01-06 01:41:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:28][root][INFO] - Training Epoch: 7/10, step 562/574 completed (loss: 0.10928919166326523, acc: 0.9555555582046509)
[2025-01-06 01:41:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:29][root][INFO] - Training Epoch: 7/10, step 563/574 completed (loss: 0.015708016231656075, acc: 1.0)
[2025-01-06 01:41:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:29][root][INFO] - Training Epoch: 7/10, step 564/574 completed (loss: 0.054684653878211975, acc: 0.9791666865348816)
[2025-01-06 01:41:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:29][root][INFO] - Training Epoch: 7/10, step 565/574 completed (loss: 0.013247818686068058, acc: 1.0)
[2025-01-06 01:41:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:30][root][INFO] - Training Epoch: 7/10, step 566/574 completed (loss: 0.014055706560611725, acc: 1.0)
[2025-01-06 01:41:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:30][root][INFO] - Training Epoch: 7/10, step 567/574 completed (loss: 0.022829309105873108, acc: 0.9736841917037964)
[2025-01-06 01:41:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:30][root][INFO] - Training Epoch: 7/10, step 568/574 completed (loss: 0.0016166218556463718, acc: 1.0)
[2025-01-06 01:41:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:31][root][INFO] - Training Epoch: 7/10, step 569/574 completed (loss: 0.03127351775765419, acc: 0.9839572310447693)
[2025-01-06 01:41:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:31][root][INFO] - Training Epoch: 7/10, step 570/574 completed (loss: 0.011792424134910107, acc: 1.0)
[2025-01-06 01:41:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:31][root][INFO] - Training Epoch: 7/10, step 571/574 completed (loss: 0.010877089574933052, acc: 1.0)
[2025-01-06 01:41:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:32][root][INFO] - Training Epoch: 7/10, step 572/574 completed (loss: 0.05540525168180466, acc: 0.9693877696990967)
[2025-01-06 01:41:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:32][root][INFO] - Training Epoch: 7/10, step 573/574 completed (loss: 0.0437561497092247, acc: 0.9874213933944702)
[2025-01-06 01:41:32][slam_llm.utils.train_utils][INFO] - Epoch 7: train_perplexity=1.0855, train_epoch_loss=0.0820, epoch time 354.92431953176856s
[2025-01-06 01:41:32][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 13 GB
[2025-01-06 01:41:32][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 18 GB
[2025-01-06 01:41:32][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 13 GB
[2025-01-06 01:41:32][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 19
[2025-01-06 01:41:32][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2025-01-06 01:41:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:33][root][INFO] - Training Epoch: 8/10, step 0/574 completed (loss: 0.043390993028879166, acc: 0.9629629850387573)
[2025-01-06 01:41:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:34][root][INFO] - Training Epoch: 8/10, step 1/574 completed (loss: 0.0054265172220766544, acc: 1.0)
[2025-01-06 01:41:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:34][root][INFO] - Training Epoch: 8/10, step 2/574 completed (loss: 0.26747551560401917, acc: 0.9189189076423645)
[2025-01-06 01:41:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:34][root][INFO] - Training Epoch: 8/10, step 3/574 completed (loss: 0.009364563971757889, acc: 1.0)
[2025-01-06 01:41:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:35][root][INFO] - Training Epoch: 8/10, step 4/574 completed (loss: 0.04240557178854942, acc: 0.9729729890823364)
[2025-01-06 01:41:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:35][root][INFO] - Training Epoch: 8/10, step 5/574 completed (loss: 0.0032665752805769444, acc: 1.0)
[2025-01-06 01:41:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:35][root][INFO] - Training Epoch: 8/10, step 6/574 completed (loss: 0.010323858819901943, acc: 1.0)
[2025-01-06 01:41:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:36][root][INFO] - Training Epoch: 8/10, step 7/574 completed (loss: 0.009973255917429924, acc: 1.0)
[2025-01-06 01:41:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:36][root][INFO] - Training Epoch: 8/10, step 8/574 completed (loss: 0.0006703244871459901, acc: 1.0)
[2025-01-06 01:41:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:36][root][INFO] - Training Epoch: 8/10, step 9/574 completed (loss: 0.0016043982468545437, acc: 1.0)
[2025-01-06 01:41:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:37][root][INFO] - Training Epoch: 8/10, step 10/574 completed (loss: 0.002638993551954627, acc: 1.0)
[2025-01-06 01:41:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:37][root][INFO] - Training Epoch: 8/10, step 11/574 completed (loss: 0.0168659258633852, acc: 1.0)
[2025-01-06 01:41:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:37][root][INFO] - Training Epoch: 8/10, step 12/574 completed (loss: 0.008061721920967102, acc: 1.0)
[2025-01-06 01:41:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:38][root][INFO] - Training Epoch: 8/10, step 13/574 completed (loss: 0.024637706577777863, acc: 0.97826087474823)
[2025-01-06 01:41:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:38][root][INFO] - Training Epoch: 8/10, step 14/574 completed (loss: 0.032832495868206024, acc: 0.9803921580314636)
[2025-01-06 01:41:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:39][root][INFO] - Training Epoch: 8/10, step 15/574 completed (loss: 0.019686821848154068, acc: 1.0)
[2025-01-06 01:41:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:39][root][INFO] - Training Epoch: 8/10, step 16/574 completed (loss: 0.005507681518793106, acc: 1.0)
[2025-01-06 01:41:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:39][root][INFO] - Training Epoch: 8/10, step 17/574 completed (loss: 0.0021003286819905043, acc: 1.0)
[2025-01-06 01:41:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:40][root][INFO] - Training Epoch: 8/10, step 18/574 completed (loss: 0.011952072381973267, acc: 1.0)
[2025-01-06 01:41:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:40][root][INFO] - Training Epoch: 8/10, step 19/574 completed (loss: 0.023721693083643913, acc: 1.0)
[2025-01-06 01:41:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:40][root][INFO] - Training Epoch: 8/10, step 20/574 completed (loss: 0.000608331523835659, acc: 1.0)
[2025-01-06 01:41:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:41][root][INFO] - Training Epoch: 8/10, step 21/574 completed (loss: 0.004296568222343922, acc: 1.0)
[2025-01-06 01:41:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:41][root][INFO] - Training Epoch: 8/10, step 22/574 completed (loss: 0.1540120542049408, acc: 0.9599999785423279)
[2025-01-06 01:41:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:41][root][INFO] - Training Epoch: 8/10, step 23/574 completed (loss: 0.0012211956782266498, acc: 1.0)
[2025-01-06 01:41:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:42][root][INFO] - Training Epoch: 8/10, step 24/574 completed (loss: 0.00044344281195662916, acc: 1.0)
[2025-01-06 01:41:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:42][root][INFO] - Training Epoch: 8/10, step 25/574 completed (loss: 0.01664971560239792, acc: 1.0)
[2025-01-06 01:41:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:42][root][INFO] - Training Epoch: 8/10, step 26/574 completed (loss: 0.13766832649707794, acc: 0.9726027250289917)
[2025-01-06 01:41:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:44][root][INFO] - Training Epoch: 8/10, step 27/574 completed (loss: 0.17245765030384064, acc: 0.9446640610694885)
[2025-01-06 01:41:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:44][root][INFO] - Training Epoch: 8/10, step 28/574 completed (loss: 0.045227278023958206, acc: 0.9767441749572754)
[2025-01-06 01:41:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:44][root][INFO] - Training Epoch: 8/10, step 29/574 completed (loss: 0.03810454159975052, acc: 1.0)
[2025-01-06 01:41:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:45][root][INFO] - Training Epoch: 8/10, step 30/574 completed (loss: 0.05018419772386551, acc: 0.9876543283462524)
[2025-01-06 01:41:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:45][root][INFO] - Training Epoch: 8/10, step 31/574 completed (loss: 0.005041311029344797, acc: 1.0)
[2025-01-06 01:41:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:45][root][INFO] - Training Epoch: 8/10, step 32/574 completed (loss: 0.00539772491902113, acc: 1.0)
[2025-01-06 01:41:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:46][root][INFO] - Training Epoch: 8/10, step 33/574 completed (loss: 0.0004215584194753319, acc: 1.0)
[2025-01-06 01:41:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:46][root][INFO] - Training Epoch: 8/10, step 34/574 completed (loss: 0.03391440212726593, acc: 0.9831932783126831)
[2025-01-06 01:41:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:46][root][INFO] - Training Epoch: 8/10, step 35/574 completed (loss: 0.046405620872974396, acc: 0.9836065769195557)
[2025-01-06 01:41:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:47][root][INFO] - Training Epoch: 8/10, step 36/574 completed (loss: 0.04728071019053459, acc: 0.9841269850730896)
[2025-01-06 01:41:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:47][root][INFO] - Training Epoch: 8/10, step 37/574 completed (loss: 0.007857056334614754, acc: 1.0)
[2025-01-06 01:41:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:47][root][INFO] - Training Epoch: 8/10, step 38/574 completed (loss: 0.02038412168622017, acc: 1.0)
[2025-01-06 01:41:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:48][root][INFO] - Training Epoch: 8/10, step 39/574 completed (loss: 0.001065222080796957, acc: 1.0)
[2025-01-06 01:41:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:48][root][INFO] - Training Epoch: 8/10, step 40/574 completed (loss: 0.02945811301469803, acc: 1.0)
[2025-01-06 01:41:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:49][root][INFO] - Training Epoch: 8/10, step 41/574 completed (loss: 0.0048699649050831795, acc: 1.0)
[2025-01-06 01:41:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:49][root][INFO] - Training Epoch: 8/10, step 42/574 completed (loss: 0.14198745787143707, acc: 0.9692307710647583)
[2025-01-06 01:41:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:49][root][INFO] - Training Epoch: 8/10, step 43/574 completed (loss: 0.036221474409103394, acc: 1.0)
[2025-01-06 01:41:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:50][root][INFO] - Training Epoch: 8/10, step 44/574 completed (loss: 0.08765605837106705, acc: 0.969072163105011)
[2025-01-06 01:41:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:50][root][INFO] - Training Epoch: 8/10, step 45/574 completed (loss: 0.06714996695518494, acc: 0.9852941036224365)
[2025-01-06 01:41:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:51][root][INFO] - Training Epoch: 8/10, step 46/574 completed (loss: 0.057978589087724686, acc: 0.9615384340286255)
[2025-01-06 01:41:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:51][root][INFO] - Training Epoch: 8/10, step 47/574 completed (loss: 0.0045273457653820515, acc: 1.0)
[2025-01-06 01:41:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:51][root][INFO] - Training Epoch: 8/10, step 48/574 completed (loss: 0.01638660579919815, acc: 1.0)
[2025-01-06 01:41:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:52][root][INFO] - Training Epoch: 8/10, step 49/574 completed (loss: 0.016902746632695198, acc: 1.0)
[2025-01-06 01:41:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:52][root][INFO] - Training Epoch: 8/10, step 50/574 completed (loss: 0.022677762433886528, acc: 1.0)
[2025-01-06 01:41:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:52][root][INFO] - Training Epoch: 8/10, step 51/574 completed (loss: 0.11542688310146332, acc: 0.9523809552192688)
[2025-01-06 01:41:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:53][root][INFO] - Training Epoch: 8/10, step 52/574 completed (loss: 0.058952104300260544, acc: 0.98591548204422)
[2025-01-06 01:41:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:53][root][INFO] - Training Epoch: 8/10, step 53/574 completed (loss: 0.25670337677001953, acc: 0.9266666769981384)
[2025-01-06 01:41:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:53][root][INFO] - Training Epoch: 8/10, step 54/574 completed (loss: 0.5198224782943726, acc: 0.9189189076423645)
[2025-01-06 01:41:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:54][root][INFO] - Training Epoch: 8/10, step 55/574 completed (loss: 0.00019811184029094875, acc: 1.0)
[2025-01-06 01:41:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:57][root][INFO] - Training Epoch: 8/10, step 56/574 completed (loss: 0.3926876187324524, acc: 0.8464163541793823)
[2025-01-06 01:41:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:58][root][INFO] - Training Epoch: 8/10, step 57/574 completed (loss: 0.578173041343689, acc: 0.843137264251709)
[2025-01-06 01:41:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:59][root][INFO] - Training Epoch: 8/10, step 58/574 completed (loss: 0.21694308519363403, acc: 0.9545454382896423)
[2025-01-06 01:41:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:41:59][root][INFO] - Training Epoch: 8/10, step 59/574 completed (loss: 0.011733283288776875, acc: 1.0)
[2025-01-06 01:41:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:00][root][INFO] - Training Epoch: 8/10, step 60/574 completed (loss: 0.12141184508800507, acc: 0.9492753744125366)
[2025-01-06 01:42:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:00][root][INFO] - Training Epoch: 8/10, step 61/574 completed (loss: 0.08540375530719757, acc: 0.9750000238418579)
[2025-01-06 01:42:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:00][root][INFO] - Training Epoch: 8/10, step 62/574 completed (loss: 0.011612392961978912, acc: 1.0)
[2025-01-06 01:42:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:01][root][INFO] - Training Epoch: 8/10, step 63/574 completed (loss: 0.01684807427227497, acc: 1.0)
[2025-01-06 01:42:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:01][root][INFO] - Training Epoch: 8/10, step 64/574 completed (loss: 0.018200155347585678, acc: 0.984375)
[2025-01-06 01:42:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:02][root][INFO] - Training Epoch: 8/10, step 65/574 completed (loss: 0.004731911700218916, acc: 1.0)
[2025-01-06 01:42:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:02][root][INFO] - Training Epoch: 8/10, step 66/574 completed (loss: 0.10871506482362747, acc: 0.9642857313156128)
[2025-01-06 01:42:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:02][root][INFO] - Training Epoch: 8/10, step 67/574 completed (loss: 0.04812009632587433, acc: 0.9833333492279053)
[2025-01-06 01:42:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:03][root][INFO] - Training Epoch: 8/10, step 68/574 completed (loss: 0.00017563867731951177, acc: 1.0)
[2025-01-06 01:42:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:03][root][INFO] - Training Epoch: 8/10, step 69/574 completed (loss: 0.1260225772857666, acc: 0.9722222089767456)
[2025-01-06 01:42:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:03][root][INFO] - Training Epoch: 8/10, step 70/574 completed (loss: 0.007271077949553728, acc: 1.0)
[2025-01-06 01:42:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:04][root][INFO] - Training Epoch: 8/10, step 71/574 completed (loss: 0.160374715924263, acc: 0.9411764740943909)
[2025-01-06 01:42:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:04][root][INFO] - Training Epoch: 8/10, step 72/574 completed (loss: 0.09265090525150299, acc: 0.9603174328804016)
[2025-01-06 01:42:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:04][root][INFO] - Training Epoch: 8/10, step 73/574 completed (loss: 0.2446167916059494, acc: 0.9230769276618958)
[2025-01-06 01:42:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:05][root][INFO] - Training Epoch: 8/10, step 74/574 completed (loss: 0.1894208937883377, acc: 0.9693877696990967)
[2025-01-06 01:42:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:05][root][INFO] - Training Epoch: 8/10, step 75/574 completed (loss: 0.13069015741348267, acc: 0.9626865386962891)
[2025-01-06 01:42:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:05][root][INFO] - Training Epoch: 8/10, step 76/574 completed (loss: 0.3630726933479309, acc: 0.8832116723060608)
[2025-01-06 01:42:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:06][root][INFO] - Training Epoch: 8/10, step 77/574 completed (loss: 0.0014312678249552846, acc: 1.0)
[2025-01-06 01:42:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:06][root][INFO] - Training Epoch: 8/10, step 78/574 completed (loss: 0.00385264097712934, acc: 1.0)
[2025-01-06 01:42:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:06][root][INFO] - Training Epoch: 8/10, step 79/574 completed (loss: 0.0333683043718338, acc: 1.0)
[2025-01-06 01:42:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:07][root][INFO] - Training Epoch: 8/10, step 80/574 completed (loss: 0.2381707727909088, acc: 0.9230769276618958)
[2025-01-06 01:42:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:07][root][INFO] - Training Epoch: 8/10, step 81/574 completed (loss: 0.003195317229256034, acc: 1.0)
[2025-01-06 01:42:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:07][root][INFO] - Training Epoch: 8/10, step 82/574 completed (loss: 0.008028930984437466, acc: 1.0)
[2025-01-06 01:42:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:08][root][INFO] - Training Epoch: 8/10, step 83/574 completed (loss: 0.030828090384602547, acc: 1.0)
[2025-01-06 01:42:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:08][root][INFO] - Training Epoch: 8/10, step 84/574 completed (loss: 0.02525128610432148, acc: 0.9855072498321533)
[2025-01-06 01:42:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:09][root][INFO] - Training Epoch: 8/10, step 85/574 completed (loss: 0.0028687329031527042, acc: 1.0)
[2025-01-06 01:42:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:09][root][INFO] - Training Epoch: 8/10, step 86/574 completed (loss: 0.024707607924938202, acc: 1.0)
[2025-01-06 01:42:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:09][root][INFO] - Training Epoch: 8/10, step 87/574 completed (loss: 0.09044806659221649, acc: 0.9599999785423279)
[2025-01-06 01:42:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:10][root][INFO] - Training Epoch: 8/10, step 88/574 completed (loss: 0.06080701947212219, acc: 0.9902912378311157)
[2025-01-06 01:42:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:11][root][INFO] - Training Epoch: 8/10, step 89/574 completed (loss: 0.3091241717338562, acc: 0.917475700378418)
[2025-01-06 01:42:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:12][root][INFO] - Training Epoch: 8/10, step 90/574 completed (loss: 0.23992745578289032, acc: 0.9193548560142517)
[2025-01-06 01:42:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:13][root][INFO] - Training Epoch: 8/10, step 91/574 completed (loss: 0.2603229880332947, acc: 0.9267241358757019)
[2025-01-06 01:42:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:13][root][INFO] - Training Epoch: 8/10, step 92/574 completed (loss: 0.09700430929660797, acc: 0.9789473414421082)
[2025-01-06 01:42:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:14][root][INFO] - Training Epoch: 8/10, step 93/574 completed (loss: 0.24291467666625977, acc: 0.9009901285171509)
[2025-01-06 01:42:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:15][root][INFO] - Training Epoch: 8/10, step 94/574 completed (loss: 0.13662023842334747, acc: 0.9677419066429138)
[2025-01-06 01:42:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:15][root][INFO] - Training Epoch: 8/10, step 95/574 completed (loss: 0.030753903090953827, acc: 0.9855072498321533)
[2025-01-06 01:42:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:15][root][INFO] - Training Epoch: 8/10, step 96/574 completed (loss: 0.19440858066082, acc: 0.9327731132507324)
[2025-01-06 01:42:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:16][root][INFO] - Training Epoch: 8/10, step 97/574 completed (loss: 0.09957598894834518, acc: 0.9711538553237915)
[2025-01-06 01:42:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:16][root][INFO] - Training Epoch: 8/10, step 98/574 completed (loss: 0.1355164796113968, acc: 0.9343065619468689)
[2025-01-06 01:42:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:17][root][INFO] - Training Epoch: 8/10, step 99/574 completed (loss: 0.19772082567214966, acc: 0.9552238583564758)
[2025-01-06 01:42:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:17][root][INFO] - Training Epoch: 8/10, step 100/574 completed (loss: 0.01258201152086258, acc: 1.0)
[2025-01-06 01:42:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:17][root][INFO] - Training Epoch: 8/10, step 101/574 completed (loss: 0.0010328078642487526, acc: 1.0)
[2025-01-06 01:42:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:18][root][INFO] - Training Epoch: 8/10, step 102/574 completed (loss: 0.07597982883453369, acc: 0.95652174949646)
[2025-01-06 01:42:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:18][root][INFO] - Training Epoch: 8/10, step 103/574 completed (loss: 0.009660604409873486, acc: 1.0)
[2025-01-06 01:42:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:18][root][INFO] - Training Epoch: 8/10, step 104/574 completed (loss: 0.015140121802687645, acc: 1.0)
[2025-01-06 01:42:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:19][root][INFO] - Training Epoch: 8/10, step 105/574 completed (loss: 0.004259107168763876, acc: 1.0)
[2025-01-06 01:42:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:19][root][INFO] - Training Epoch: 8/10, step 106/574 completed (loss: 0.0077884020283818245, acc: 1.0)
[2025-01-06 01:42:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:19][root][INFO] - Training Epoch: 8/10, step 107/574 completed (loss: 0.0009580337791703641, acc: 1.0)
[2025-01-06 01:42:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:20][root][INFO] - Training Epoch: 8/10, step 108/574 completed (loss: 0.0015903905732557178, acc: 1.0)
[2025-01-06 01:42:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:20][root][INFO] - Training Epoch: 8/10, step 109/574 completed (loss: 0.006712037138640881, acc: 1.0)
[2025-01-06 01:42:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:20][root][INFO] - Training Epoch: 8/10, step 110/574 completed (loss: 0.01990404538810253, acc: 1.0)
[2025-01-06 01:42:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:21][root][INFO] - Training Epoch: 8/10, step 111/574 completed (loss: 0.02370285615324974, acc: 0.9824561476707458)
[2025-01-06 01:42:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:21][root][INFO] - Training Epoch: 8/10, step 112/574 completed (loss: 0.12101709097623825, acc: 0.9473684430122375)
[2025-01-06 01:42:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:22][root][INFO] - Training Epoch: 8/10, step 113/574 completed (loss: 0.04595359414815903, acc: 0.9743589758872986)
[2025-01-06 01:42:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:22][root][INFO] - Training Epoch: 8/10, step 114/574 completed (loss: 0.003998809959739447, acc: 1.0)
[2025-01-06 01:42:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:22][root][INFO] - Training Epoch: 8/10, step 115/574 completed (loss: 0.00021152160479687154, acc: 1.0)
[2025-01-06 01:42:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:23][root][INFO] - Training Epoch: 8/10, step 116/574 completed (loss: 0.02271444723010063, acc: 1.0)
[2025-01-06 01:42:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:23][root][INFO] - Training Epoch: 8/10, step 117/574 completed (loss: 0.12886200845241547, acc: 0.9674796462059021)
[2025-01-06 01:42:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:23][root][INFO] - Training Epoch: 8/10, step 118/574 completed (loss: 0.007862296886742115, acc: 1.0)
[2025-01-06 01:42:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:24][root][INFO] - Training Epoch: 8/10, step 119/574 completed (loss: 0.10864797979593277, acc: 0.9619771838188171)
[2025-01-06 01:42:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:25][root][INFO] - Training Epoch: 8/10, step 120/574 completed (loss: 0.00418383814394474, acc: 1.0)
[2025-01-06 01:42:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:25][root][INFO] - Training Epoch: 8/10, step 121/574 completed (loss: 0.006821786519140005, acc: 1.0)
[2025-01-06 01:42:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:25][root][INFO] - Training Epoch: 8/10, step 122/574 completed (loss: 0.000995021895505488, acc: 1.0)
[2025-01-06 01:42:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:26][root][INFO] - Training Epoch: 8/10, step 123/574 completed (loss: 0.00890713557600975, acc: 1.0)
[2025-01-06 01:42:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:26][root][INFO] - Training Epoch: 8/10, step 124/574 completed (loss: 0.09174796938896179, acc: 0.9754601120948792)
[2025-01-06 01:42:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:27][root][INFO] - Training Epoch: 8/10, step 125/574 completed (loss: 0.1622065156698227, acc: 0.9513888955116272)
[2025-01-06 01:42:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:27][root][INFO] - Training Epoch: 8/10, step 126/574 completed (loss: 0.12856006622314453, acc: 0.9666666388511658)
[2025-01-06 01:42:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:27][root][INFO] - Training Epoch: 8/10, step 127/574 completed (loss: 0.059990376234054565, acc: 0.9821428656578064)
[2025-01-06 01:42:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:28][root][INFO] - Training Epoch: 8/10, step 128/574 completed (loss: 0.1615035980939865, acc: 0.9692307710647583)
[2025-01-06 01:42:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:58][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.2409, device='cuda:0') eval_epoch_loss=tensor(0.8069, device='cuda:0') eval_epoch_acc=tensor(0.8502, device='cuda:0')
[2025-01-06 01:42:58][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:42:58][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:42:58][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_8_step_129_loss_0.8068996071815491/model.pt
[2025-01-06 01:42:58][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:42:58][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 8 is 0.8502339124679565
[2025-01-06 01:42:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:58][root][INFO] - Training Epoch: 8/10, step 129/574 completed (loss: 0.1721368432044983, acc: 0.9485294222831726)
[2025-01-06 01:42:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:59][root][INFO] - Training Epoch: 8/10, step 130/574 completed (loss: 0.013749880716204643, acc: 1.0)
[2025-01-06 01:42:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:59][root][INFO] - Training Epoch: 8/10, step 131/574 completed (loss: 0.014283097349107265, acc: 1.0)
[2025-01-06 01:42:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:42:59][root][INFO] - Training Epoch: 8/10, step 132/574 completed (loss: 0.013399003073573112, acc: 1.0)
[2025-01-06 01:43:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:00][root][INFO] - Training Epoch: 8/10, step 133/574 completed (loss: 0.01650106906890869, acc: 1.0)
[2025-01-06 01:43:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:00][root][INFO] - Training Epoch: 8/10, step 134/574 completed (loss: 0.19536925852298737, acc: 0.9428571462631226)
[2025-01-06 01:43:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:00][root][INFO] - Training Epoch: 8/10, step 135/574 completed (loss: 0.02954651042819023, acc: 1.0)
[2025-01-06 01:43:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:01][root][INFO] - Training Epoch: 8/10, step 136/574 completed (loss: 0.07998014986515045, acc: 0.9523809552192688)
[2025-01-06 01:43:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:01][root][INFO] - Training Epoch: 8/10, step 137/574 completed (loss: 0.04369180276989937, acc: 0.9666666388511658)
[2025-01-06 01:43:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:01][root][INFO] - Training Epoch: 8/10, step 138/574 completed (loss: 0.015774602070450783, acc: 1.0)
[2025-01-06 01:43:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:02][root][INFO] - Training Epoch: 8/10, step 139/574 completed (loss: 0.005721895955502987, acc: 1.0)
[2025-01-06 01:43:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:02][root][INFO] - Training Epoch: 8/10, step 140/574 completed (loss: 0.024972757324576378, acc: 1.0)
[2025-01-06 01:43:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:03][root][INFO] - Training Epoch: 8/10, step 141/574 completed (loss: 0.028289692476391792, acc: 1.0)
[2025-01-06 01:43:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:03][root][INFO] - Training Epoch: 8/10, step 142/574 completed (loss: 0.012252876535058022, acc: 1.0)
[2025-01-06 01:43:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:03][root][INFO] - Training Epoch: 8/10, step 143/574 completed (loss: 0.09305419772863388, acc: 0.9649122953414917)
[2025-01-06 01:43:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:04][root][INFO] - Training Epoch: 8/10, step 144/574 completed (loss: 0.2741970717906952, acc: 0.9253731369972229)
[2025-01-06 01:43:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:04][root][INFO] - Training Epoch: 8/10, step 145/574 completed (loss: 0.037315063178539276, acc: 0.9897959232330322)
[2025-01-06 01:43:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:05][root][INFO] - Training Epoch: 8/10, step 146/574 completed (loss: 0.10925205051898956, acc: 0.9468085169792175)
[2025-01-06 01:43:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:05][root][INFO] - Training Epoch: 8/10, step 147/574 completed (loss: 0.02479853853583336, acc: 1.0)
[2025-01-06 01:43:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:05][root][INFO] - Training Epoch: 8/10, step 148/574 completed (loss: 0.004123592749238014, acc: 1.0)
[2025-01-06 01:43:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:06][root][INFO] - Training Epoch: 8/10, step 149/574 completed (loss: 0.11105087399482727, acc: 0.9130434989929199)
[2025-01-06 01:43:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:06][root][INFO] - Training Epoch: 8/10, step 150/574 completed (loss: 0.004461185075342655, acc: 1.0)
[2025-01-06 01:43:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:06][root][INFO] - Training Epoch: 8/10, step 151/574 completed (loss: 0.010681225918233395, acc: 1.0)
[2025-01-06 01:43:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:07][root][INFO] - Training Epoch: 8/10, step 152/574 completed (loss: 0.044864799827337265, acc: 1.0)
[2025-01-06 01:43:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:07][root][INFO] - Training Epoch: 8/10, step 153/574 completed (loss: 0.030348608270287514, acc: 0.9824561476707458)
[2025-01-06 01:43:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:07][root][INFO] - Training Epoch: 8/10, step 154/574 completed (loss: 0.19787706434726715, acc: 0.9729729890823364)
[2025-01-06 01:43:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:08][root][INFO] - Training Epoch: 8/10, step 155/574 completed (loss: 0.0013124661054462194, acc: 1.0)
[2025-01-06 01:43:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:08][root][INFO] - Training Epoch: 8/10, step 156/574 completed (loss: 0.007650356739759445, acc: 1.0)
[2025-01-06 01:43:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:08][root][INFO] - Training Epoch: 8/10, step 157/574 completed (loss: 0.022548120468854904, acc: 1.0)
[2025-01-06 01:43:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:10][root][INFO] - Training Epoch: 8/10, step 158/574 completed (loss: 0.18191900849342346, acc: 0.9459459185600281)
[2025-01-06 01:43:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:10][root][INFO] - Training Epoch: 8/10, step 159/574 completed (loss: 0.18390896916389465, acc: 0.9629629850387573)
[2025-01-06 01:43:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:11][root][INFO] - Training Epoch: 8/10, step 160/574 completed (loss: 0.24769611656665802, acc: 0.9418604373931885)
[2025-01-06 01:43:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:11][root][INFO] - Training Epoch: 8/10, step 161/574 completed (loss: 0.19201651215553284, acc: 0.9411764740943909)
[2025-01-06 01:43:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:12][root][INFO] - Training Epoch: 8/10, step 162/574 completed (loss: 0.15667614340782166, acc: 0.9775280952453613)
[2025-01-06 01:43:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:12][root][INFO] - Training Epoch: 8/10, step 163/574 completed (loss: 0.03357897326350212, acc: 1.0)
[2025-01-06 01:43:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:12][root][INFO] - Training Epoch: 8/10, step 164/574 completed (loss: 0.004700514022260904, acc: 1.0)
[2025-01-06 01:43:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:13][root][INFO] - Training Epoch: 8/10, step 165/574 completed (loss: 0.0367036834359169, acc: 0.9655172228813171)
[2025-01-06 01:43:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:13][root][INFO] - Training Epoch: 8/10, step 166/574 completed (loss: 0.01289879996329546, acc: 1.0)
[2025-01-06 01:43:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:13][root][INFO] - Training Epoch: 8/10, step 167/574 completed (loss: 0.04775461554527283, acc: 0.9800000190734863)
[2025-01-06 01:43:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:14][root][INFO] - Training Epoch: 8/10, step 168/574 completed (loss: 0.13837699592113495, acc: 0.9583333134651184)
[2025-01-06 01:43:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:14][root][INFO] - Training Epoch: 8/10, step 169/574 completed (loss: 0.04722657799720764, acc: 1.0)
[2025-01-06 01:43:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:15][root][INFO] - Training Epoch: 8/10, step 170/574 completed (loss: 0.09393414109945297, acc: 0.965753436088562)
[2025-01-06 01:43:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:16][root][INFO] - Training Epoch: 8/10, step 171/574 completed (loss: 0.010525349527597427, acc: 1.0)
[2025-01-06 01:43:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:16][root][INFO] - Training Epoch: 8/10, step 172/574 completed (loss: 0.0014293730491772294, acc: 1.0)
[2025-01-06 01:43:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:16][root][INFO] - Training Epoch: 8/10, step 173/574 completed (loss: 0.01672198437154293, acc: 1.0)
[2025-01-06 01:43:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:17][root][INFO] - Training Epoch: 8/10, step 174/574 completed (loss: 0.21028535068035126, acc: 0.9026548862457275)
[2025-01-06 01:43:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:17][root][INFO] - Training Epoch: 8/10, step 175/574 completed (loss: 0.07163790613412857, acc: 0.9710144996643066)
[2025-01-06 01:43:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:17][root][INFO] - Training Epoch: 8/10, step 176/574 completed (loss: 0.10272400081157684, acc: 0.9772727489471436)
[2025-01-06 01:43:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:18][root][INFO] - Training Epoch: 8/10, step 177/574 completed (loss: 0.2168138325214386, acc: 0.9465649127960205)
[2025-01-06 01:43:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:19][root][INFO] - Training Epoch: 8/10, step 178/574 completed (loss: 0.13199381530284882, acc: 0.9629629850387573)
[2025-01-06 01:43:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:19][root][INFO] - Training Epoch: 8/10, step 179/574 completed (loss: 0.02486003190279007, acc: 1.0)
[2025-01-06 01:43:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:20][root][INFO] - Training Epoch: 8/10, step 180/574 completed (loss: 0.00029916767380200326, acc: 1.0)
[2025-01-06 01:43:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:20][root][INFO] - Training Epoch: 8/10, step 181/574 completed (loss: 0.031624529510736465, acc: 0.9599999785423279)
[2025-01-06 01:43:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:20][root][INFO] - Training Epoch: 8/10, step 182/574 completed (loss: 0.0036022700369358063, acc: 1.0)
[2025-01-06 01:43:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:21][root][INFO] - Training Epoch: 8/10, step 183/574 completed (loss: 0.003989300224930048, acc: 1.0)
[2025-01-06 01:43:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:21][root][INFO] - Training Epoch: 8/10, step 184/574 completed (loss: 0.11188622564077377, acc: 0.9637462496757507)
[2025-01-06 01:43:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:21][root][INFO] - Training Epoch: 8/10, step 185/574 completed (loss: 0.13665907084941864, acc: 0.9567723274230957)
[2025-01-06 01:43:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:22][root][INFO] - Training Epoch: 8/10, step 186/574 completed (loss: 0.11329196393489838, acc: 0.956250011920929)
[2025-01-06 01:43:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:22][root][INFO] - Training Epoch: 8/10, step 187/574 completed (loss: 0.20227603614330292, acc: 0.9380863308906555)
[2025-01-06 01:43:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:23][root][INFO] - Training Epoch: 8/10, step 188/574 completed (loss: 0.12308697402477264, acc: 0.9644128084182739)
[2025-01-06 01:43:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:23][root][INFO] - Training Epoch: 8/10, step 189/574 completed (loss: 0.00248716096393764, acc: 1.0)
[2025-01-06 01:43:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:24][root][INFO] - Training Epoch: 8/10, step 190/574 completed (loss: 0.06892111897468567, acc: 0.9883720874786377)
[2025-01-06 01:43:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:25][root][INFO] - Training Epoch: 8/10, step 191/574 completed (loss: 0.3304632902145386, acc: 0.8888888955116272)
[2025-01-06 01:43:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:26][root][INFO] - Training Epoch: 8/10, step 192/574 completed (loss: 0.18296122550964355, acc: 0.9242424368858337)
[2025-01-06 01:43:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:26][root][INFO] - Training Epoch: 8/10, step 193/574 completed (loss: 0.05544279143214226, acc: 0.9882352948188782)
[2025-01-06 01:43:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:27][root][INFO] - Training Epoch: 8/10, step 194/574 completed (loss: 0.2154521495103836, acc: 0.9629629850387573)
[2025-01-06 01:43:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:28][root][INFO] - Training Epoch: 8/10, step 195/574 completed (loss: 0.03838811814785004, acc: 1.0)
[2025-01-06 01:43:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:29][root][INFO] - Training Epoch: 8/10, step 196/574 completed (loss: 0.02341805398464203, acc: 1.0)
[2025-01-06 01:43:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:29][root][INFO] - Training Epoch: 8/10, step 197/574 completed (loss: 0.0026823836378753185, acc: 1.0)
[2025-01-06 01:43:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:29][root][INFO] - Training Epoch: 8/10, step 198/574 completed (loss: 0.023016221821308136, acc: 1.0)
[2025-01-06 01:43:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:30][root][INFO] - Training Epoch: 8/10, step 199/574 completed (loss: 0.05634801462292671, acc: 0.9852941036224365)
[2025-01-06 01:43:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:30][root][INFO] - Training Epoch: 8/10, step 200/574 completed (loss: 0.04658733308315277, acc: 0.9745762944221497)
[2025-01-06 01:43:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:30][root][INFO] - Training Epoch: 8/10, step 201/574 completed (loss: 0.08863367885351181, acc: 0.9776119589805603)
[2025-01-06 01:43:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:31][root][INFO] - Training Epoch: 8/10, step 202/574 completed (loss: 0.07083098590373993, acc: 0.9805825352668762)
[2025-01-06 01:43:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:31][root][INFO] - Training Epoch: 8/10, step 203/574 completed (loss: 0.06979997456073761, acc: 0.9682539701461792)
[2025-01-06 01:43:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:31][root][INFO] - Training Epoch: 8/10, step 204/574 completed (loss: 0.02117823250591755, acc: 0.9890109896659851)
[2025-01-06 01:43:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:32][root][INFO] - Training Epoch: 8/10, step 205/574 completed (loss: 0.03481800854206085, acc: 0.9865471124649048)
[2025-01-06 01:43:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:32][root][INFO] - Training Epoch: 8/10, step 206/574 completed (loss: 0.07081490755081177, acc: 0.9763779640197754)
[2025-01-06 01:43:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:33][root][INFO] - Training Epoch: 8/10, step 207/574 completed (loss: 0.0492389053106308, acc: 0.9741379022598267)
[2025-01-06 01:43:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:33][root][INFO] - Training Epoch: 8/10, step 208/574 completed (loss: 0.11497239023447037, acc: 0.9746376872062683)
[2025-01-06 01:43:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:33][root][INFO] - Training Epoch: 8/10, step 209/574 completed (loss: 0.03575313463807106, acc: 0.9844357967376709)
[2025-01-06 01:43:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:34][root][INFO] - Training Epoch: 8/10, step 210/574 completed (loss: 0.01270385179668665, acc: 1.0)
[2025-01-06 01:43:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:34][root][INFO] - Training Epoch: 8/10, step 211/574 completed (loss: 0.0050841742195189, acc: 1.0)
[2025-01-06 01:43:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:34][root][INFO] - Training Epoch: 8/10, step 212/574 completed (loss: 0.0010332464007660747, acc: 1.0)
[2025-01-06 01:43:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:35][root][INFO] - Training Epoch: 8/10, step 213/574 completed (loss: 0.0014974031364545226, acc: 1.0)
[2025-01-06 01:43:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:36][root][INFO] - Training Epoch: 8/10, step 214/574 completed (loss: 0.029934056103229523, acc: 0.9923076629638672)
[2025-01-06 01:43:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:36][root][INFO] - Training Epoch: 8/10, step 215/574 completed (loss: 0.005458451807498932, acc: 1.0)
[2025-01-06 01:43:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:36][root][INFO] - Training Epoch: 8/10, step 216/574 completed (loss: 0.027686651796102524, acc: 0.9883720874786377)
[2025-01-06 01:43:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:37][root][INFO] - Training Epoch: 8/10, step 217/574 completed (loss: 0.009527793154120445, acc: 1.0)
[2025-01-06 01:43:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:37][root][INFO] - Training Epoch: 8/10, step 218/574 completed (loss: 0.01066676340997219, acc: 1.0)
[2025-01-06 01:43:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:37][root][INFO] - Training Epoch: 8/10, step 219/574 completed (loss: 0.005289873108267784, acc: 1.0)
[2025-01-06 01:43:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:38][root][INFO] - Training Epoch: 8/10, step 220/574 completed (loss: 0.0012336497893556952, acc: 1.0)
[2025-01-06 01:43:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:38][root][INFO] - Training Epoch: 8/10, step 221/574 completed (loss: 0.001154061988927424, acc: 1.0)
[2025-01-06 01:43:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:39][root][INFO] - Training Epoch: 8/10, step 222/574 completed (loss: 0.020510368049144745, acc: 1.0)
[2025-01-06 01:43:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:39][root][INFO] - Training Epoch: 8/10, step 223/574 completed (loss: 0.09751839190721512, acc: 0.95652174949646)
[2025-01-06 01:43:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:40][root][INFO] - Training Epoch: 8/10, step 224/574 completed (loss: 0.17816825211048126, acc: 0.9488636255264282)
[2025-01-06 01:43:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:40][root][INFO] - Training Epoch: 8/10, step 225/574 completed (loss: 0.042482633143663406, acc: 0.9893617033958435)
[2025-01-06 01:43:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:41][root][INFO] - Training Epoch: 8/10, step 226/574 completed (loss: 0.055178556591272354, acc: 0.9811320900917053)
[2025-01-06 01:43:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:41][root][INFO] - Training Epoch: 8/10, step 227/574 completed (loss: 0.037829115986824036, acc: 0.9833333492279053)
[2025-01-06 01:43:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:41][root][INFO] - Training Epoch: 8/10, step 228/574 completed (loss: 0.1276955008506775, acc: 0.9534883499145508)
[2025-01-06 01:43:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:42][root][INFO] - Training Epoch: 8/10, step 229/574 completed (loss: 0.030166734009981155, acc: 0.9666666388511658)
[2025-01-06 01:43:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:42][root][INFO] - Training Epoch: 8/10, step 230/574 completed (loss: 0.22491252422332764, acc: 0.8947368264198303)
[2025-01-06 01:43:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:43][root][INFO] - Training Epoch: 8/10, step 231/574 completed (loss: 0.08001605421304703, acc: 0.9777777791023254)
[2025-01-06 01:43:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:43][root][INFO] - Training Epoch: 8/10, step 232/574 completed (loss: 0.32611793279647827, acc: 0.8888888955116272)
[2025-01-06 01:43:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:44][root][INFO] - Training Epoch: 8/10, step 233/574 completed (loss: 0.42038464546203613, acc: 0.8394495248794556)
[2025-01-06 01:43:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:44][root][INFO] - Training Epoch: 8/10, step 234/574 completed (loss: 0.24124518036842346, acc: 0.8999999761581421)
[2025-01-06 01:43:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:44][root][INFO] - Training Epoch: 8/10, step 235/574 completed (loss: 0.0014713252894580364, acc: 1.0)
[2025-01-06 01:43:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:45][root][INFO] - Training Epoch: 8/10, step 236/574 completed (loss: 0.0011513630161061883, acc: 1.0)
[2025-01-06 01:43:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:45][root][INFO] - Training Epoch: 8/10, step 237/574 completed (loss: 0.22735753655433655, acc: 0.9090909361839294)
[2025-01-06 01:43:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:45][root][INFO] - Training Epoch: 8/10, step 238/574 completed (loss: 0.005882642697542906, acc: 1.0)
[2025-01-06 01:43:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:46][root][INFO] - Training Epoch: 8/10, step 239/574 completed (loss: 0.046743445098400116, acc: 0.9714285731315613)
[2025-01-06 01:43:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:46][root][INFO] - Training Epoch: 8/10, step 240/574 completed (loss: 0.028126852586865425, acc: 1.0)
[2025-01-06 01:43:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:46][root][INFO] - Training Epoch: 8/10, step 241/574 completed (loss: 0.004234789405018091, acc: 1.0)
[2025-01-06 01:43:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:47][root][INFO] - Training Epoch: 8/10, step 242/574 completed (loss: 0.09814141690731049, acc: 0.9838709831237793)
[2025-01-06 01:43:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:48][root][INFO] - Training Epoch: 8/10, step 243/574 completed (loss: 0.11036942154169083, acc: 0.9545454382896423)
[2025-01-06 01:43:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:48][root][INFO] - Training Epoch: 8/10, step 244/574 completed (loss: 9.484888141741976e-05, acc: 1.0)
[2025-01-06 01:43:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:48][root][INFO] - Training Epoch: 8/10, step 245/574 completed (loss: 0.002437618561089039, acc: 1.0)
[2025-01-06 01:43:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:49][root][INFO] - Training Epoch: 8/10, step 246/574 completed (loss: 0.0004830717807635665, acc: 1.0)
[2025-01-06 01:43:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:49][root][INFO] - Training Epoch: 8/10, step 247/574 completed (loss: 0.0002856479841284454, acc: 1.0)
[2025-01-06 01:43:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:49][root][INFO] - Training Epoch: 8/10, step 248/574 completed (loss: 0.0031580247450619936, acc: 1.0)
[2025-01-06 01:43:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:50][root][INFO] - Training Epoch: 8/10, step 249/574 completed (loss: 0.028028562664985657, acc: 0.9729729890823364)
[2025-01-06 01:43:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:50][root][INFO] - Training Epoch: 8/10, step 250/574 completed (loss: 0.023819511756300926, acc: 0.9729729890823364)
[2025-01-06 01:43:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:50][root][INFO] - Training Epoch: 8/10, step 251/574 completed (loss: 0.056745536625385284, acc: 0.9852941036224365)
[2025-01-06 01:43:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:51][root][INFO] - Training Epoch: 8/10, step 252/574 completed (loss: 0.015333786606788635, acc: 1.0)
[2025-01-06 01:43:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:51][root][INFO] - Training Epoch: 8/10, step 253/574 completed (loss: 0.0030838754028081894, acc: 1.0)
[2025-01-06 01:43:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:51][root][INFO] - Training Epoch: 8/10, step 254/574 completed (loss: 8.778781921137124e-05, acc: 1.0)
[2025-01-06 01:43:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:52][root][INFO] - Training Epoch: 8/10, step 255/574 completed (loss: 0.0013387516373768449, acc: 1.0)
[2025-01-06 01:43:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:52][root][INFO] - Training Epoch: 8/10, step 256/574 completed (loss: 0.0012067030183970928, acc: 1.0)
[2025-01-06 01:43:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:52][root][INFO] - Training Epoch: 8/10, step 257/574 completed (loss: 0.027582457289099693, acc: 0.9857142567634583)
[2025-01-06 01:43:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:53][root][INFO] - Training Epoch: 8/10, step 258/574 completed (loss: 0.04371999576687813, acc: 0.9868420958518982)
[2025-01-06 01:43:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:53][root][INFO] - Training Epoch: 8/10, step 259/574 completed (loss: 0.017924342304468155, acc: 1.0)
[2025-01-06 01:43:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:54][root][INFO] - Training Epoch: 8/10, step 260/574 completed (loss: 0.049282193183898926, acc: 0.9750000238418579)
[2025-01-06 01:43:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:54][root][INFO] - Training Epoch: 8/10, step 261/574 completed (loss: 0.004254703875631094, acc: 1.0)
[2025-01-06 01:43:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:55][root][INFO] - Training Epoch: 8/10, step 262/574 completed (loss: 0.018152007833123207, acc: 1.0)
[2025-01-06 01:43:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:55][root][INFO] - Training Epoch: 8/10, step 263/574 completed (loss: 0.1761491745710373, acc: 0.9599999785423279)
[2025-01-06 01:43:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:55][root][INFO] - Training Epoch: 8/10, step 264/574 completed (loss: 0.14980857074260712, acc: 0.9583333134651184)
[2025-01-06 01:43:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:56][root][INFO] - Training Epoch: 8/10, step 265/574 completed (loss: 0.4584125876426697, acc: 0.9039999842643738)
[2025-01-06 01:43:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:57][root][INFO] - Training Epoch: 8/10, step 266/574 completed (loss: 0.06252673268318176, acc: 0.9775280952453613)
[2025-01-06 01:43:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:57][root][INFO] - Training Epoch: 8/10, step 267/574 completed (loss: 0.1591499149799347, acc: 0.9729729890823364)
[2025-01-06 01:43:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:57][root][INFO] - Training Epoch: 8/10, step 268/574 completed (loss: 0.029770879074931145, acc: 1.0)
[2025-01-06 01:43:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:58][root][INFO] - Training Epoch: 8/10, step 269/574 completed (loss: 0.001152311684563756, acc: 1.0)
[2025-01-06 01:43:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:58][root][INFO] - Training Epoch: 8/10, step 270/574 completed (loss: 0.00476619740948081, acc: 1.0)
[2025-01-06 01:43:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:58][root][INFO] - Training Epoch: 8/10, step 271/574 completed (loss: 0.022899359464645386, acc: 1.0)
[2025-01-06 01:43:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:43:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:29][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.2899, device='cuda:0') eval_epoch_loss=tensor(0.8285, device='cuda:0') eval_epoch_acc=tensor(0.8504, device='cuda:0')
[2025-01-06 01:44:29][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:44:29][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:44:29][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_8_step_272_loss_0.8285099864006042/model.pt
[2025-01-06 01:44:29][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:44:29][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 8 is 0.8504191637039185
[2025-01-06 01:44:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:29][root][INFO] - Training Epoch: 8/10, step 272/574 completed (loss: 0.09231914579868317, acc: 0.9666666388511658)
[2025-01-06 01:44:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:30][root][INFO] - Training Epoch: 8/10, step 273/574 completed (loss: 0.12398984283208847, acc: 0.949999988079071)
[2025-01-06 01:44:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:30][root][INFO] - Training Epoch: 8/10, step 274/574 completed (loss: 0.001351614249870181, acc: 1.0)
[2025-01-06 01:44:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:30][root][INFO] - Training Epoch: 8/10, step 275/574 completed (loss: 0.009633520618081093, acc: 1.0)
[2025-01-06 01:44:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:31][root][INFO] - Training Epoch: 8/10, step 276/574 completed (loss: 0.09038740396499634, acc: 0.9655172228813171)
[2025-01-06 01:44:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:31][root][INFO] - Training Epoch: 8/10, step 277/574 completed (loss: 0.008086038753390312, acc: 1.0)
[2025-01-06 01:44:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:31][root][INFO] - Training Epoch: 8/10, step 278/574 completed (loss: 0.002916793804615736, acc: 1.0)
[2025-01-06 01:44:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:32][root][INFO] - Training Epoch: 8/10, step 279/574 completed (loss: 0.06287730485200882, acc: 0.9583333134651184)
[2025-01-06 01:44:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:32][root][INFO] - Training Epoch: 8/10, step 280/574 completed (loss: 0.0019864668138325214, acc: 1.0)
[2025-01-06 01:44:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:33][root][INFO] - Training Epoch: 8/10, step 281/574 completed (loss: 0.05900232493877411, acc: 0.9638554453849792)
[2025-01-06 01:44:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:33][root][INFO] - Training Epoch: 8/10, step 282/574 completed (loss: 0.051142994314432144, acc: 0.9814814925193787)
[2025-01-06 01:44:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:33][root][INFO] - Training Epoch: 8/10, step 283/574 completed (loss: 0.012315365485846996, acc: 1.0)
[2025-01-06 01:44:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:34][root][INFO] - Training Epoch: 8/10, step 284/574 completed (loss: 0.02489110641181469, acc: 1.0)
[2025-01-06 01:44:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:34][root][INFO] - Training Epoch: 8/10, step 285/574 completed (loss: 0.007633143104612827, acc: 1.0)
[2025-01-06 01:44:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:34][root][INFO] - Training Epoch: 8/10, step 286/574 completed (loss: 0.0348881296813488, acc: 0.9921875)
[2025-01-06 01:44:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:35][root][INFO] - Training Epoch: 8/10, step 287/574 completed (loss: 0.16770420968532562, acc: 0.9599999785423279)
[2025-01-06 01:44:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:35][root][INFO] - Training Epoch: 8/10, step 288/574 completed (loss: 0.015618466772139072, acc: 1.0)
[2025-01-06 01:44:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:35][root][INFO] - Training Epoch: 8/10, step 289/574 completed (loss: 0.04566014185547829, acc: 0.9937888383865356)
[2025-01-06 01:44:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:36][root][INFO] - Training Epoch: 8/10, step 290/574 completed (loss: 0.11025065928697586, acc: 0.9587628841400146)
[2025-01-06 01:44:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:36][root][INFO] - Training Epoch: 8/10, step 291/574 completed (loss: 0.00194785266648978, acc: 1.0)
[2025-01-06 01:44:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:36][root][INFO] - Training Epoch: 8/10, step 292/574 completed (loss: 0.00570607790723443, acc: 1.0)
[2025-01-06 01:44:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:37][root][INFO] - Training Epoch: 8/10, step 293/574 completed (loss: 0.011305335909128189, acc: 1.0)
[2025-01-06 01:44:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:37][root][INFO] - Training Epoch: 8/10, step 294/574 completed (loss: 0.013254846446216106, acc: 1.0)
[2025-01-06 01:44:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:38][root][INFO] - Training Epoch: 8/10, step 295/574 completed (loss: 0.08984319120645523, acc: 0.9639175534248352)
[2025-01-06 01:44:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:38][root][INFO] - Training Epoch: 8/10, step 296/574 completed (loss: 0.023063628003001213, acc: 1.0)
[2025-01-06 01:44:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:39][root][INFO] - Training Epoch: 8/10, step 297/574 completed (loss: 0.06620880216360092, acc: 0.9629629850387573)
[2025-01-06 01:44:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:39][root][INFO] - Training Epoch: 8/10, step 298/574 completed (loss: 0.04256582632660866, acc: 1.0)
[2025-01-06 01:44:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:39][root][INFO] - Training Epoch: 8/10, step 299/574 completed (loss: 0.002712056739255786, acc: 1.0)
[2025-01-06 01:44:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:40][root][INFO] - Training Epoch: 8/10, step 300/574 completed (loss: 0.0031110134441405535, acc: 1.0)
[2025-01-06 01:44:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:40][root][INFO] - Training Epoch: 8/10, step 301/574 completed (loss: 0.0021946902852505445, acc: 1.0)
[2025-01-06 01:44:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:40][root][INFO] - Training Epoch: 8/10, step 302/574 completed (loss: 0.0050110300071537495, acc: 1.0)
[2025-01-06 01:44:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:41][root][INFO] - Training Epoch: 8/10, step 303/574 completed (loss: 0.002736029215157032, acc: 1.0)
[2025-01-06 01:44:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:41][root][INFO] - Training Epoch: 8/10, step 304/574 completed (loss: 0.006831202656030655, acc: 1.0)
[2025-01-06 01:44:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:41][root][INFO] - Training Epoch: 8/10, step 305/574 completed (loss: 0.03250986337661743, acc: 0.9836065769195557)
[2025-01-06 01:44:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:42][root][INFO] - Training Epoch: 8/10, step 306/574 completed (loss: 0.00497967517003417, acc: 1.0)
[2025-01-06 01:44:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:42][root][INFO] - Training Epoch: 8/10, step 307/574 completed (loss: 0.006930535659193993, acc: 1.0)
[2025-01-06 01:44:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:42][root][INFO] - Training Epoch: 8/10, step 308/574 completed (loss: 0.05892959609627724, acc: 0.9855072498321533)
[2025-01-06 01:44:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:43][root][INFO] - Training Epoch: 8/10, step 309/574 completed (loss: 0.004647634457796812, acc: 1.0)
[2025-01-06 01:44:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:43][root][INFO] - Training Epoch: 8/10, step 310/574 completed (loss: 0.08548454940319061, acc: 0.9759036302566528)
[2025-01-06 01:44:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:44][root][INFO] - Training Epoch: 8/10, step 311/574 completed (loss: 0.013090885244309902, acc: 1.0)
[2025-01-06 01:44:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:44][root][INFO] - Training Epoch: 8/10, step 312/574 completed (loss: 0.0015165313379839063, acc: 1.0)
[2025-01-06 01:44:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:44][root][INFO] - Training Epoch: 8/10, step 313/574 completed (loss: 0.00032548047602176666, acc: 1.0)
[2025-01-06 01:44:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:45][root][INFO] - Training Epoch: 8/10, step 314/574 completed (loss: 0.00048085497110150754, acc: 1.0)
[2025-01-06 01:44:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:45][root][INFO] - Training Epoch: 8/10, step 315/574 completed (loss: 0.0036376069765537977, acc: 1.0)
[2025-01-06 01:44:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:45][root][INFO] - Training Epoch: 8/10, step 316/574 completed (loss: 0.04531170427799225, acc: 0.9677419066429138)
[2025-01-06 01:44:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:46][root][INFO] - Training Epoch: 8/10, step 317/574 completed (loss: 0.009984046220779419, acc: 1.0)
[2025-01-06 01:44:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:46][root][INFO] - Training Epoch: 8/10, step 318/574 completed (loss: 0.005495448596775532, acc: 1.0)
[2025-01-06 01:44:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:46][root][INFO] - Training Epoch: 8/10, step 319/574 completed (loss: 0.004198300652205944, acc: 1.0)
[2025-01-06 01:44:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:47][root][INFO] - Training Epoch: 8/10, step 320/574 completed (loss: 0.0017310979310423136, acc: 1.0)
[2025-01-06 01:44:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:47][root][INFO] - Training Epoch: 8/10, step 321/574 completed (loss: 0.001722340239211917, acc: 1.0)
[2025-01-06 01:44:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:47][root][INFO] - Training Epoch: 8/10, step 322/574 completed (loss: 0.07980827242136002, acc: 0.9629629850387573)
[2025-01-06 01:44:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:48][root][INFO] - Training Epoch: 8/10, step 323/574 completed (loss: 0.016374165192246437, acc: 1.0)
[2025-01-06 01:44:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:48][root][INFO] - Training Epoch: 8/10, step 324/574 completed (loss: 0.08874022215604782, acc: 0.9487179517745972)
[2025-01-06 01:44:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:48][root][INFO] - Training Epoch: 8/10, step 325/574 completed (loss: 0.03513818979263306, acc: 1.0)
[2025-01-06 01:44:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:49][root][INFO] - Training Epoch: 8/10, step 326/574 completed (loss: 0.011911657638847828, acc: 1.0)
[2025-01-06 01:44:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:49][root][INFO] - Training Epoch: 8/10, step 327/574 completed (loss: 0.00593662029132247, acc: 1.0)
[2025-01-06 01:44:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:49][root][INFO] - Training Epoch: 8/10, step 328/574 completed (loss: 0.0038817881140857935, acc: 1.0)
[2025-01-06 01:44:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:50][root][INFO] - Training Epoch: 8/10, step 329/574 completed (loss: 0.001116527826525271, acc: 1.0)
[2025-01-06 01:44:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:50][root][INFO] - Training Epoch: 8/10, step 330/574 completed (loss: 0.0017506645526736975, acc: 1.0)
[2025-01-06 01:44:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:50][root][INFO] - Training Epoch: 8/10, step 331/574 completed (loss: 0.016029030084609985, acc: 1.0)
[2025-01-06 01:44:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:51][root][INFO] - Training Epoch: 8/10, step 332/574 completed (loss: 0.028486300259828568, acc: 0.9824561476707458)
[2025-01-06 01:44:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:51][root][INFO] - Training Epoch: 8/10, step 333/574 completed (loss: 0.001500685466453433, acc: 1.0)
[2025-01-06 01:44:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:52][root][INFO] - Training Epoch: 8/10, step 334/574 completed (loss: 0.029353361576795578, acc: 0.9666666388511658)
[2025-01-06 01:44:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:52][root][INFO] - Training Epoch: 8/10, step 335/574 completed (loss: 0.0051716407760977745, acc: 1.0)
[2025-01-06 01:44:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:52][root][INFO] - Training Epoch: 8/10, step 336/574 completed (loss: 0.07128539681434631, acc: 0.9800000190734863)
[2025-01-06 01:44:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:53][root][INFO] - Training Epoch: 8/10, step 337/574 completed (loss: 0.11651840060949326, acc: 0.9425287246704102)
[2025-01-06 01:44:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:53][root][INFO] - Training Epoch: 8/10, step 338/574 completed (loss: 0.11303127557039261, acc: 0.957446813583374)
[2025-01-06 01:44:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:53][root][INFO] - Training Epoch: 8/10, step 339/574 completed (loss: 0.05647262930870056, acc: 0.9879518151283264)
[2025-01-06 01:44:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:54][root][INFO] - Training Epoch: 8/10, step 340/574 completed (loss: 0.00013484068040270358, acc: 1.0)
[2025-01-06 01:44:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:54][root][INFO] - Training Epoch: 8/10, step 341/574 completed (loss: 0.004770258441567421, acc: 1.0)
[2025-01-06 01:44:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:54][root][INFO] - Training Epoch: 8/10, step 342/574 completed (loss: 0.018052298575639725, acc: 1.0)
[2025-01-06 01:44:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:55][root][INFO] - Training Epoch: 8/10, step 343/574 completed (loss: 0.028430594131350517, acc: 0.9811320900917053)
[2025-01-06 01:44:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:55][root][INFO] - Training Epoch: 8/10, step 344/574 completed (loss: 0.05063510686159134, acc: 0.9873417615890503)
[2025-01-06 01:44:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:55][root][INFO] - Training Epoch: 8/10, step 345/574 completed (loss: 0.003530298126861453, acc: 1.0)
[2025-01-06 01:44:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:56][root][INFO] - Training Epoch: 8/10, step 346/574 completed (loss: 0.012549939565360546, acc: 1.0)
[2025-01-06 01:44:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:56][root][INFO] - Training Epoch: 8/10, step 347/574 completed (loss: 0.06050931289792061, acc: 0.949999988079071)
[2025-01-06 01:44:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:56][root][INFO] - Training Epoch: 8/10, step 348/574 completed (loss: 0.04024483636021614, acc: 0.9599999785423279)
[2025-01-06 01:44:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:57][root][INFO] - Training Epoch: 8/10, step 349/574 completed (loss: 0.010799579322338104, acc: 1.0)
[2025-01-06 01:44:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:57][root][INFO] - Training Epoch: 8/10, step 350/574 completed (loss: 0.01872173137962818, acc: 1.0)
[2025-01-06 01:44:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:58][root][INFO] - Training Epoch: 8/10, step 351/574 completed (loss: 0.0074407015927135944, acc: 1.0)
[2025-01-06 01:44:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:58][root][INFO] - Training Epoch: 8/10, step 352/574 completed (loss: 0.06926774233579636, acc: 0.9555555582046509)
[2025-01-06 01:44:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:58][root][INFO] - Training Epoch: 8/10, step 353/574 completed (loss: 0.000156004200107418, acc: 1.0)
[2025-01-06 01:44:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:59][root][INFO] - Training Epoch: 8/10, step 354/574 completed (loss: 0.005388950929045677, acc: 1.0)
[2025-01-06 01:44:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:59][root][INFO] - Training Epoch: 8/10, step 355/574 completed (loss: 0.06670922785997391, acc: 0.9780219793319702)
[2025-01-06 01:44:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:44:59][root][INFO] - Training Epoch: 8/10, step 356/574 completed (loss: 0.12388886511325836, acc: 0.9652174115180969)
[2025-01-06 01:45:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:00][root][INFO] - Training Epoch: 8/10, step 357/574 completed (loss: 0.058413390070199966, acc: 0.989130437374115)
[2025-01-06 01:45:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:00][root][INFO] - Training Epoch: 8/10, step 358/574 completed (loss: 0.00951690599322319, acc: 1.0)
[2025-01-06 01:45:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:00][root][INFO] - Training Epoch: 8/10, step 359/574 completed (loss: 0.00022035282745491713, acc: 1.0)
[2025-01-06 01:45:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:01][root][INFO] - Training Epoch: 8/10, step 360/574 completed (loss: 0.0030891289934515953, acc: 1.0)
[2025-01-06 01:45:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:01][root][INFO] - Training Epoch: 8/10, step 361/574 completed (loss: 0.00435449555516243, acc: 1.0)
[2025-01-06 01:45:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:02][root][INFO] - Training Epoch: 8/10, step 362/574 completed (loss: 0.08285774290561676, acc: 0.9777777791023254)
[2025-01-06 01:45:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:02][root][INFO] - Training Epoch: 8/10, step 363/574 completed (loss: 0.008391983807086945, acc: 1.0)
[2025-01-06 01:45:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:02][root][INFO] - Training Epoch: 8/10, step 364/574 completed (loss: 0.021231291815638542, acc: 1.0)
[2025-01-06 01:45:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:03][root][INFO] - Training Epoch: 8/10, step 365/574 completed (loss: 0.04587321728467941, acc: 0.9696969985961914)
[2025-01-06 01:45:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:03][root][INFO] - Training Epoch: 8/10, step 366/574 completed (loss: 0.00020197458798065782, acc: 1.0)
[2025-01-06 01:45:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:03][root][INFO] - Training Epoch: 8/10, step 367/574 completed (loss: 0.0008795014582574368, acc: 1.0)
[2025-01-06 01:45:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:04][root][INFO] - Training Epoch: 8/10, step 368/574 completed (loss: 0.002267546486109495, acc: 1.0)
[2025-01-06 01:45:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:04][root][INFO] - Training Epoch: 8/10, step 369/574 completed (loss: 0.018079886212944984, acc: 1.0)
[2025-01-06 01:45:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:05][root][INFO] - Training Epoch: 8/10, step 370/574 completed (loss: 0.11376544088125229, acc: 0.9575757384300232)
[2025-01-06 01:45:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:05][root][INFO] - Training Epoch: 8/10, step 371/574 completed (loss: 0.0790441706776619, acc: 0.9811320900917053)
[2025-01-06 01:45:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:06][root][INFO] - Training Epoch: 8/10, step 372/574 completed (loss: 0.019787680357694626, acc: 0.9888888597488403)
[2025-01-06 01:45:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:06][root][INFO] - Training Epoch: 8/10, step 373/574 completed (loss: 0.007926201447844505, acc: 1.0)
[2025-01-06 01:45:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:06][root][INFO] - Training Epoch: 8/10, step 374/574 completed (loss: 0.002207444980740547, acc: 1.0)
[2025-01-06 01:45:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:07][root][INFO] - Training Epoch: 8/10, step 375/574 completed (loss: 8.163628808688372e-05, acc: 1.0)
[2025-01-06 01:45:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:07][root][INFO] - Training Epoch: 8/10, step 376/574 completed (loss: 0.0009022592566907406, acc: 1.0)
[2025-01-06 01:45:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:07][root][INFO] - Training Epoch: 8/10, step 377/574 completed (loss: 0.004908423870801926, acc: 1.0)
[2025-01-06 01:45:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:08][root][INFO] - Training Epoch: 8/10, step 378/574 completed (loss: 0.0006012468365952373, acc: 1.0)
[2025-01-06 01:45:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:08][root][INFO] - Training Epoch: 8/10, step 379/574 completed (loss: 0.02885940484702587, acc: 0.9880239367485046)
[2025-01-06 01:45:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:09][root][INFO] - Training Epoch: 8/10, step 380/574 completed (loss: 0.04327383264899254, acc: 0.9849624037742615)
[2025-01-06 01:45:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:10][root][INFO] - Training Epoch: 8/10, step 381/574 completed (loss: 0.2118217498064041, acc: 0.9465240836143494)
[2025-01-06 01:45:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:11][root][INFO] - Training Epoch: 8/10, step 382/574 completed (loss: 0.012565302662551403, acc: 1.0)
[2025-01-06 01:45:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:11][root][INFO] - Training Epoch: 8/10, step 383/574 completed (loss: 0.00407779635861516, acc: 1.0)
[2025-01-06 01:45:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:11][root][INFO] - Training Epoch: 8/10, step 384/574 completed (loss: 0.0021512294188141823, acc: 1.0)
[2025-01-06 01:45:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:12][root][INFO] - Training Epoch: 8/10, step 385/574 completed (loss: 0.00044492442975752056, acc: 1.0)
[2025-01-06 01:45:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:12][root][INFO] - Training Epoch: 8/10, step 386/574 completed (loss: 0.0012531790416687727, acc: 1.0)
[2025-01-06 01:45:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:12][root][INFO] - Training Epoch: 8/10, step 387/574 completed (loss: 0.0004974150797352195, acc: 1.0)
[2025-01-06 01:45:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:13][root][INFO] - Training Epoch: 8/10, step 388/574 completed (loss: 0.0008173391688615084, acc: 1.0)
[2025-01-06 01:45:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:13][root][INFO] - Training Epoch: 8/10, step 389/574 completed (loss: 8.72557720867917e-05, acc: 1.0)
[2025-01-06 01:45:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:13][root][INFO] - Training Epoch: 8/10, step 390/574 completed (loss: 0.22258183360099792, acc: 0.9523809552192688)
[2025-01-06 01:45:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:14][root][INFO] - Training Epoch: 8/10, step 391/574 completed (loss: 0.17329546809196472, acc: 0.9444444179534912)
[2025-01-06 01:45:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:14][root][INFO] - Training Epoch: 8/10, step 392/574 completed (loss: 0.16826827824115753, acc: 0.9417475461959839)
[2025-01-06 01:45:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:14][root][INFO] - Training Epoch: 8/10, step 393/574 completed (loss: 0.12916608154773712, acc: 0.9632353186607361)
[2025-01-06 01:45:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:15][root][INFO] - Training Epoch: 8/10, step 394/574 completed (loss: 0.11226365715265274, acc: 0.9599999785423279)
[2025-01-06 01:45:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:15][root][INFO] - Training Epoch: 8/10, step 395/574 completed (loss: 0.07255228608846664, acc: 0.9722222089767456)
[2025-01-06 01:45:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:16][root][INFO] - Training Epoch: 8/10, step 396/574 completed (loss: 0.10370869934558868, acc: 0.9767441749572754)
[2025-01-06 01:45:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:16][root][INFO] - Training Epoch: 8/10, step 397/574 completed (loss: 0.048118382692337036, acc: 0.9583333134651184)
[2025-01-06 01:45:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:16][root][INFO] - Training Epoch: 8/10, step 398/574 completed (loss: 0.0044067357666790485, acc: 1.0)
[2025-01-06 01:45:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:17][root][INFO] - Training Epoch: 8/10, step 399/574 completed (loss: 0.0022365720942616463, acc: 1.0)
[2025-01-06 01:45:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:17][root][INFO] - Training Epoch: 8/10, step 400/574 completed (loss: 0.023350393399596214, acc: 0.9852941036224365)
[2025-01-06 01:45:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:17][root][INFO] - Training Epoch: 8/10, step 401/574 completed (loss: 0.09354156255722046, acc: 0.9599999785423279)
[2025-01-06 01:45:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:18][root][INFO] - Training Epoch: 8/10, step 402/574 completed (loss: 0.0013483210932463408, acc: 1.0)
[2025-01-06 01:45:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:18][root][INFO] - Training Epoch: 8/10, step 403/574 completed (loss: 0.044899385422468185, acc: 0.9696969985961914)
[2025-01-06 01:45:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:18][root][INFO] - Training Epoch: 8/10, step 404/574 completed (loss: 0.00983953196555376, acc: 1.0)
[2025-01-06 01:45:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:19][root][INFO] - Training Epoch: 8/10, step 405/574 completed (loss: 0.00041160976979881525, acc: 1.0)
[2025-01-06 01:45:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:19][root][INFO] - Training Epoch: 8/10, step 406/574 completed (loss: 0.0007595508359372616, acc: 1.0)
[2025-01-06 01:45:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:19][root][INFO] - Training Epoch: 8/10, step 407/574 completed (loss: 0.005715389735996723, acc: 1.0)
[2025-01-06 01:45:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:20][root][INFO] - Training Epoch: 8/10, step 408/574 completed (loss: 0.004127028398215771, acc: 1.0)
[2025-01-06 01:45:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:20][root][INFO] - Training Epoch: 8/10, step 409/574 completed (loss: 0.0014401033986359835, acc: 1.0)
[2025-01-06 01:45:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:20][root][INFO] - Training Epoch: 8/10, step 410/574 completed (loss: 0.0038717181887477636, acc: 1.0)
[2025-01-06 01:45:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:21][root][INFO] - Training Epoch: 8/10, step 411/574 completed (loss: 0.0029499230440706015, acc: 1.0)
[2025-01-06 01:45:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:21][root][INFO] - Training Epoch: 8/10, step 412/574 completed (loss: 0.0035849723499268293, acc: 1.0)
[2025-01-06 01:45:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:22][root][INFO] - Training Epoch: 8/10, step 413/574 completed (loss: 0.19151246547698975, acc: 0.9696969985961914)
[2025-01-06 01:45:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:22][root][INFO] - Training Epoch: 8/10, step 414/574 completed (loss: 0.00040571793215349317, acc: 1.0)
[2025-01-06 01:45:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:52][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.3589, device='cuda:0') eval_epoch_loss=tensor(0.8582, device='cuda:0') eval_epoch_acc=tensor(0.8407, device='cuda:0')
[2025-01-06 01:45:52][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:45:52][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:45:53][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_8_step_415_loss_0.8582107424736023/model.pt
[2025-01-06 01:45:53][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:45:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:53][root][INFO] - Training Epoch: 8/10, step 415/574 completed (loss: 0.05138351395726204, acc: 0.9803921580314636)
[2025-01-06 01:45:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:53][root][INFO] - Training Epoch: 8/10, step 416/574 completed (loss: 0.026830347254872322, acc: 1.0)
[2025-01-06 01:45:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:54][root][INFO] - Training Epoch: 8/10, step 417/574 completed (loss: 0.3249455392360687, acc: 0.9444444179534912)
[2025-01-06 01:45:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:54][root][INFO] - Training Epoch: 8/10, step 418/574 completed (loss: 0.0008444407139904797, acc: 1.0)
[2025-01-06 01:45:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:54][root][INFO] - Training Epoch: 8/10, step 419/574 completed (loss: 0.12530259788036346, acc: 0.949999988079071)
[2025-01-06 01:45:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:55][root][INFO] - Training Epoch: 8/10, step 420/574 completed (loss: 0.0018188580870628357, acc: 1.0)
[2025-01-06 01:45:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:55][root][INFO] - Training Epoch: 8/10, step 421/574 completed (loss: 0.0024456302635371685, acc: 1.0)
[2025-01-06 01:45:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:55][root][INFO] - Training Epoch: 8/10, step 422/574 completed (loss: 0.18965457379817963, acc: 0.96875)
[2025-01-06 01:45:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:56][root][INFO] - Training Epoch: 8/10, step 423/574 completed (loss: 0.01689954847097397, acc: 1.0)
[2025-01-06 01:45:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:56][root][INFO] - Training Epoch: 8/10, step 424/574 completed (loss: 0.0017887079156935215, acc: 1.0)
[2025-01-06 01:45:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:56][root][INFO] - Training Epoch: 8/10, step 425/574 completed (loss: 0.05111249163746834, acc: 0.9696969985961914)
[2025-01-06 01:45:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:57][root][INFO] - Training Epoch: 8/10, step 426/574 completed (loss: 0.0010927643161267042, acc: 1.0)
[2025-01-06 01:45:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:57][root][INFO] - Training Epoch: 8/10, step 427/574 completed (loss: 0.01507255807518959, acc: 1.0)
[2025-01-06 01:45:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:58][root][INFO] - Training Epoch: 8/10, step 428/574 completed (loss: 0.09058918058872223, acc: 0.9629629850387573)
[2025-01-06 01:45:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:58][root][INFO] - Training Epoch: 8/10, step 429/574 completed (loss: 0.006644159089773893, acc: 1.0)
[2025-01-06 01:45:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:58][root][INFO] - Training Epoch: 8/10, step 430/574 completed (loss: 0.007725905627012253, acc: 1.0)
[2025-01-06 01:45:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:59][root][INFO] - Training Epoch: 8/10, step 431/574 completed (loss: 0.0005515171214938164, acc: 1.0)
[2025-01-06 01:45:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:59][root][INFO] - Training Epoch: 8/10, step 432/574 completed (loss: 0.0034667341969907284, acc: 1.0)
[2025-01-06 01:45:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:45:59][root][INFO] - Training Epoch: 8/10, step 433/574 completed (loss: 0.032014038413763046, acc: 1.0)
[2025-01-06 01:46:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:00][root][INFO] - Training Epoch: 8/10, step 434/574 completed (loss: 0.000495226529892534, acc: 1.0)
[2025-01-06 01:46:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:00][root][INFO] - Training Epoch: 8/10, step 435/574 completed (loss: 0.03132608160376549, acc: 0.9696969985961914)
[2025-01-06 01:46:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:00][root][INFO] - Training Epoch: 8/10, step 436/574 completed (loss: 0.009070155210793018, acc: 1.0)
[2025-01-06 01:46:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:01][root][INFO] - Training Epoch: 8/10, step 437/574 completed (loss: 0.003200518898665905, acc: 1.0)
[2025-01-06 01:46:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:01][root][INFO] - Training Epoch: 8/10, step 438/574 completed (loss: 0.003992669750005007, acc: 1.0)
[2025-01-06 01:46:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:01][root][INFO] - Training Epoch: 8/10, step 439/574 completed (loss: 0.0030055001843720675, acc: 1.0)
[2025-01-06 01:46:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:02][root][INFO] - Training Epoch: 8/10, step 440/574 completed (loss: 0.07971023768186569, acc: 0.9848484992980957)
[2025-01-06 01:46:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:03][root][INFO] - Training Epoch: 8/10, step 441/574 completed (loss: 0.22413313388824463, acc: 0.9279999732971191)
[2025-01-06 01:46:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:03][root][INFO] - Training Epoch: 8/10, step 442/574 completed (loss: 0.1596708446741104, acc: 0.9516128897666931)
[2025-01-06 01:46:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:04][root][INFO] - Training Epoch: 8/10, step 443/574 completed (loss: 0.15184670686721802, acc: 0.9601989984512329)
[2025-01-06 01:46:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:04][root][INFO] - Training Epoch: 8/10, step 444/574 completed (loss: 0.08549059927463531, acc: 0.9622641801834106)
[2025-01-06 01:46:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:04][root][INFO] - Training Epoch: 8/10, step 445/574 completed (loss: 0.010271884500980377, acc: 1.0)
[2025-01-06 01:46:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:05][root][INFO] - Training Epoch: 8/10, step 446/574 completed (loss: 0.0045910184271633625, acc: 1.0)
[2025-01-06 01:46:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:05][root][INFO] - Training Epoch: 8/10, step 447/574 completed (loss: 0.006881068926304579, acc: 1.0)
[2025-01-06 01:46:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:05][root][INFO] - Training Epoch: 8/10, step 448/574 completed (loss: 0.012968932278454304, acc: 1.0)
[2025-01-06 01:46:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:06][root][INFO] - Training Epoch: 8/10, step 449/574 completed (loss: 0.003675731597468257, acc: 1.0)
[2025-01-06 01:46:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:06][root][INFO] - Training Epoch: 8/10, step 450/574 completed (loss: 0.008564364165067673, acc: 1.0)
[2025-01-06 01:46:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:07][root][INFO] - Training Epoch: 8/10, step 451/574 completed (loss: 0.026584478095173836, acc: 0.989130437374115)
[2025-01-06 01:46:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:07][root][INFO] - Training Epoch: 8/10, step 452/574 completed (loss: 0.011211495846509933, acc: 1.0)
[2025-01-06 01:46:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:07][root][INFO] - Training Epoch: 8/10, step 453/574 completed (loss: 0.10641469806432724, acc: 0.9605262875556946)
[2025-01-06 01:46:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:08][root][INFO] - Training Epoch: 8/10, step 454/574 completed (loss: 0.006795104593038559, acc: 1.0)
[2025-01-06 01:46:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:08][root][INFO] - Training Epoch: 8/10, step 455/574 completed (loss: 0.008837677538394928, acc: 1.0)
[2025-01-06 01:46:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:08][root][INFO] - Training Epoch: 8/10, step 456/574 completed (loss: 0.11739453673362732, acc: 0.969072163105011)
[2025-01-06 01:46:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:09][root][INFO] - Training Epoch: 8/10, step 457/574 completed (loss: 0.0047323089092969894, acc: 1.0)
[2025-01-06 01:46:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:09][root][INFO] - Training Epoch: 8/10, step 458/574 completed (loss: 0.08819825947284698, acc: 0.9825581312179565)
[2025-01-06 01:46:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:09][root][INFO] - Training Epoch: 8/10, step 459/574 completed (loss: 0.0035860652569681406, acc: 1.0)
[2025-01-06 01:46:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:10][root][INFO] - Training Epoch: 8/10, step 460/574 completed (loss: 0.054296962916851044, acc: 0.9753086566925049)
[2025-01-06 01:46:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:10][root][INFO] - Training Epoch: 8/10, step 461/574 completed (loss: 0.00314327422529459, acc: 1.0)
[2025-01-06 01:46:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:10][root][INFO] - Training Epoch: 8/10, step 462/574 completed (loss: 0.0007547722198069096, acc: 1.0)
[2025-01-06 01:46:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:11][root][INFO] - Training Epoch: 8/10, step 463/574 completed (loss: 0.0074808294884860516, acc: 1.0)
[2025-01-06 01:46:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:11][root][INFO] - Training Epoch: 8/10, step 464/574 completed (loss: 0.016743941232562065, acc: 1.0)
[2025-01-06 01:46:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:12][root][INFO] - Training Epoch: 8/10, step 465/574 completed (loss: 0.04646392911672592, acc: 0.976190447807312)
[2025-01-06 01:46:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:12][root][INFO] - Training Epoch: 8/10, step 466/574 completed (loss: 0.11659540981054306, acc: 0.9759036302566528)
[2025-01-06 01:46:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:12][root][INFO] - Training Epoch: 8/10, step 467/574 completed (loss: 0.005689102225005627, acc: 1.0)
[2025-01-06 01:46:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:13][root][INFO] - Training Epoch: 8/10, step 468/574 completed (loss: 0.0857100784778595, acc: 0.9708737730979919)
[2025-01-06 01:46:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:13][root][INFO] - Training Epoch: 8/10, step 469/574 completed (loss: 0.06169712916016579, acc: 0.9837398529052734)
[2025-01-06 01:46:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:13][root][INFO] - Training Epoch: 8/10, step 470/574 completed (loss: 0.006054277997463942, acc: 1.0)
[2025-01-06 01:46:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:14][root][INFO] - Training Epoch: 8/10, step 471/574 completed (loss: 0.015844101086258888, acc: 1.0)
[2025-01-06 01:46:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:14][root][INFO] - Training Epoch: 8/10, step 472/574 completed (loss: 0.061634279787540436, acc: 0.9901960492134094)
[2025-01-06 01:46:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:14][root][INFO] - Training Epoch: 8/10, step 473/574 completed (loss: 0.19552268087863922, acc: 0.9563318490982056)
[2025-01-06 01:46:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:15][root][INFO] - Training Epoch: 8/10, step 474/574 completed (loss: 0.039355721324682236, acc: 0.9895833134651184)
[2025-01-06 01:46:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:15][root][INFO] - Training Epoch: 8/10, step 475/574 completed (loss: 0.0656237080693245, acc: 0.9754601120948792)
[2025-01-06 01:46:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:15][root][INFO] - Training Epoch: 8/10, step 476/574 completed (loss: 0.055001672357320786, acc: 0.9784172773361206)
[2025-01-06 01:46:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:16][root][INFO] - Training Epoch: 8/10, step 477/574 completed (loss: 0.14083673059940338, acc: 0.9447236061096191)
[2025-01-06 01:46:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:16][root][INFO] - Training Epoch: 8/10, step 478/574 completed (loss: 0.0592939518392086, acc: 0.9722222089767456)
[2025-01-06 01:46:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:17][root][INFO] - Training Epoch: 8/10, step 479/574 completed (loss: 0.012182543985545635, acc: 1.0)
[2025-01-06 01:46:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:17][root][INFO] - Training Epoch: 8/10, step 480/574 completed (loss: 0.00346627039834857, acc: 1.0)
[2025-01-06 01:46:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:17][root][INFO] - Training Epoch: 8/10, step 481/574 completed (loss: 0.007750650402158499, acc: 1.0)
[2025-01-06 01:46:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:17][root][INFO] - Training Epoch: 8/10, step 482/574 completed (loss: 0.26151520013809204, acc: 0.949999988079071)
[2025-01-06 01:46:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:18][root][INFO] - Training Epoch: 8/10, step 483/574 completed (loss: 0.02481386810541153, acc: 1.0)
[2025-01-06 01:46:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:18][root][INFO] - Training Epoch: 8/10, step 484/574 completed (loss: 0.004658353514969349, acc: 1.0)
[2025-01-06 01:46:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:19][root][INFO] - Training Epoch: 8/10, step 485/574 completed (loss: 0.002805357798933983, acc: 1.0)
[2025-01-06 01:46:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:19][root][INFO] - Training Epoch: 8/10, step 486/574 completed (loss: 0.1553017646074295, acc: 0.9629629850387573)
[2025-01-06 01:46:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:19][root][INFO] - Training Epoch: 8/10, step 487/574 completed (loss: 0.0021668586414307356, acc: 1.0)
[2025-01-06 01:46:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:20][root][INFO] - Training Epoch: 8/10, step 488/574 completed (loss: 0.4201395809650421, acc: 0.9545454382896423)
[2025-01-06 01:46:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:20][root][INFO] - Training Epoch: 8/10, step 489/574 completed (loss: 0.058668531477451324, acc: 0.9846153855323792)
[2025-01-06 01:46:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:20][root][INFO] - Training Epoch: 8/10, step 490/574 completed (loss: 0.004702267237007618, acc: 1.0)
[2025-01-06 01:46:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:21][root][INFO] - Training Epoch: 8/10, step 491/574 completed (loss: 0.012404787354171276, acc: 1.0)
[2025-01-06 01:46:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:21][root][INFO] - Training Epoch: 8/10, step 492/574 completed (loss: 0.020038718357682228, acc: 0.9803921580314636)
[2025-01-06 01:46:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:21][root][INFO] - Training Epoch: 8/10, step 493/574 completed (loss: 0.005159914027899504, acc: 1.0)
[2025-01-06 01:46:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:22][root][INFO] - Training Epoch: 8/10, step 494/574 completed (loss: 0.0019273224752396345, acc: 1.0)
[2025-01-06 01:46:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:22][root][INFO] - Training Epoch: 8/10, step 495/574 completed (loss: 0.0071848127990961075, acc: 1.0)
[2025-01-06 01:46:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:22][root][INFO] - Training Epoch: 8/10, step 496/574 completed (loss: 0.07875948399305344, acc: 0.9553571343421936)
[2025-01-06 01:46:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:23][root][INFO] - Training Epoch: 8/10, step 497/574 completed (loss: 0.04067270830273628, acc: 0.9887640476226807)
[2025-01-06 01:46:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:23][root][INFO] - Training Epoch: 8/10, step 498/574 completed (loss: 0.17376495897769928, acc: 0.9550561904907227)
[2025-01-06 01:46:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:24][root][INFO] - Training Epoch: 8/10, step 499/574 completed (loss: 0.21226447820663452, acc: 0.9432623982429504)
[2025-01-06 01:46:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:24][root][INFO] - Training Epoch: 8/10, step 500/574 completed (loss: 0.19580137729644775, acc: 0.9347826242446899)
[2025-01-06 01:46:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:24][root][INFO] - Training Epoch: 8/10, step 501/574 completed (loss: 0.0003006323822773993, acc: 1.0)
[2025-01-06 01:46:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:25][root][INFO] - Training Epoch: 8/10, step 502/574 completed (loss: 0.00018690506112761796, acc: 1.0)
[2025-01-06 01:46:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:25][root][INFO] - Training Epoch: 8/10, step 503/574 completed (loss: 0.11169376969337463, acc: 0.9629629850387573)
[2025-01-06 01:46:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:25][root][INFO] - Training Epoch: 8/10, step 504/574 completed (loss: 0.06387251615524292, acc: 0.9629629850387573)
[2025-01-06 01:46:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:26][root][INFO] - Training Epoch: 8/10, step 505/574 completed (loss: 0.05629126355051994, acc: 0.9811320900917053)
[2025-01-06 01:46:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:26][root][INFO] - Training Epoch: 8/10, step 506/574 completed (loss: 0.012528615072369576, acc: 1.0)
[2025-01-06 01:46:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:27][root][INFO] - Training Epoch: 8/10, step 507/574 completed (loss: 0.29093509912490845, acc: 0.9099099040031433)
[2025-01-06 01:46:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:27][root][INFO] - Training Epoch: 8/10, step 508/574 completed (loss: 0.09906763583421707, acc: 0.9436619877815247)
[2025-01-06 01:46:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:27][root][INFO] - Training Epoch: 8/10, step 509/574 completed (loss: 0.0013162099057808518, acc: 1.0)
[2025-01-06 01:46:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:28][root][INFO] - Training Epoch: 8/10, step 510/574 completed (loss: 0.012768294662237167, acc: 1.0)
[2025-01-06 01:46:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:28][root][INFO] - Training Epoch: 8/10, step 511/574 completed (loss: 0.0038779578171670437, acc: 1.0)
[2025-01-06 01:46:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:31][root][INFO] - Training Epoch: 8/10, step 512/574 completed (loss: 0.1458173543214798, acc: 0.9428571462631226)
[2025-01-06 01:46:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:31][root][INFO] - Training Epoch: 8/10, step 513/574 completed (loss: 0.024399401620030403, acc: 0.9920634627342224)
[2025-01-06 01:46:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:32][root][INFO] - Training Epoch: 8/10, step 514/574 completed (loss: 0.023487064987421036, acc: 1.0)
[2025-01-06 01:46:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:32][root][INFO] - Training Epoch: 8/10, step 515/574 completed (loss: 0.004872446414083242, acc: 1.0)
[2025-01-06 01:46:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:33][root][INFO] - Training Epoch: 8/10, step 516/574 completed (loss: 0.05371077358722687, acc: 0.9722222089767456)
[2025-01-06 01:46:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:33][root][INFO] - Training Epoch: 8/10, step 517/574 completed (loss: 0.00021758103684987873, acc: 1.0)
[2025-01-06 01:46:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:33][root][INFO] - Training Epoch: 8/10, step 518/574 completed (loss: 0.0030901602003723383, acc: 1.0)
[2025-01-06 01:46:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:34][root][INFO] - Training Epoch: 8/10, step 519/574 completed (loss: 0.017522919923067093, acc: 1.0)
[2025-01-06 01:46:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:34][root][INFO] - Training Epoch: 8/10, step 520/574 completed (loss: 0.005920272320508957, acc: 1.0)
[2025-01-06 01:46:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:35][root][INFO] - Training Epoch: 8/10, step 521/574 completed (loss: 0.17878864705562592, acc: 0.9406779408454895)
[2025-01-06 01:46:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:35][root][INFO] - Training Epoch: 8/10, step 522/574 completed (loss: 0.0045841410756111145, acc: 1.0)
[2025-01-06 01:46:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:36][root][INFO] - Training Epoch: 8/10, step 523/574 completed (loss: 0.07738455384969711, acc: 0.970802903175354)
[2025-01-06 01:46:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:36][root][INFO] - Training Epoch: 8/10, step 524/574 completed (loss: 0.13130050897598267, acc: 0.9599999785423279)
[2025-01-06 01:46:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:37][root][INFO] - Training Epoch: 8/10, step 525/574 completed (loss: 0.002842443995177746, acc: 1.0)
[2025-01-06 01:46:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:37][root][INFO] - Training Epoch: 8/10, step 526/574 completed (loss: 0.008282424882054329, acc: 1.0)
[2025-01-06 01:46:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:37][root][INFO] - Training Epoch: 8/10, step 527/574 completed (loss: 0.008290235884487629, acc: 1.0)
[2025-01-06 01:46:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:38][root][INFO] - Training Epoch: 8/10, step 528/574 completed (loss: 0.06961189210414886, acc: 0.9836065769195557)
[2025-01-06 01:46:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:38][root][INFO] - Training Epoch: 8/10, step 529/574 completed (loss: 0.009712470695376396, acc: 1.0)
[2025-01-06 01:46:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:38][root][INFO] - Training Epoch: 8/10, step 530/574 completed (loss: 0.07528164982795715, acc: 0.9767441749572754)
[2025-01-06 01:46:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:39][root][INFO] - Training Epoch: 8/10, step 531/574 completed (loss: 0.09504590183496475, acc: 0.9545454382896423)
[2025-01-06 01:46:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:39][root][INFO] - Training Epoch: 8/10, step 532/574 completed (loss: 0.0478450208902359, acc: 1.0)
[2025-01-06 01:46:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:39][root][INFO] - Training Epoch: 8/10, step 533/574 completed (loss: 0.031161339953541756, acc: 1.0)
[2025-01-06 01:46:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:40][root][INFO] - Training Epoch: 8/10, step 534/574 completed (loss: 0.019078118726611137, acc: 1.0)
[2025-01-06 01:46:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:40][root][INFO] - Training Epoch: 8/10, step 535/574 completed (loss: 0.0006802105344831944, acc: 1.0)
[2025-01-06 01:46:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:40][root][INFO] - Training Epoch: 8/10, step 536/574 completed (loss: 0.008543088100850582, acc: 1.0)
[2025-01-06 01:46:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:41][root][INFO] - Training Epoch: 8/10, step 537/574 completed (loss: 0.10323863476514816, acc: 0.9692307710647583)
[2025-01-06 01:46:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:41][root][INFO] - Training Epoch: 8/10, step 538/574 completed (loss: 0.021302562206983566, acc: 1.0)
[2025-01-06 01:46:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:42][root][INFO] - Training Epoch: 8/10, step 539/574 completed (loss: 0.014322265982627869, acc: 1.0)
[2025-01-06 01:46:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:42][root][INFO] - Training Epoch: 8/10, step 540/574 completed (loss: 0.012161806225776672, acc: 1.0)
[2025-01-06 01:46:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:42][root][INFO] - Training Epoch: 8/10, step 541/574 completed (loss: 0.048624925315380096, acc: 1.0)
[2025-01-06 01:46:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:43][root][INFO] - Training Epoch: 8/10, step 542/574 completed (loss: 0.003400871530175209, acc: 1.0)
[2025-01-06 01:46:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:43][root][INFO] - Training Epoch: 8/10, step 543/574 completed (loss: 0.11292660981416702, acc: 0.95652174949646)
[2025-01-06 01:46:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:43][root][INFO] - Training Epoch: 8/10, step 544/574 completed (loss: 0.22992680966854095, acc: 0.9333333373069763)
[2025-01-06 01:46:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:44][root][INFO] - Training Epoch: 8/10, step 545/574 completed (loss: 0.0065188175067305565, acc: 1.0)
[2025-01-06 01:46:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:44][root][INFO] - Training Epoch: 8/10, step 546/574 completed (loss: 0.0007665461744181812, acc: 1.0)
[2025-01-06 01:46:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:44][root][INFO] - Training Epoch: 8/10, step 547/574 completed (loss: 0.015606217086315155, acc: 1.0)
[2025-01-06 01:46:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:45][root][INFO] - Training Epoch: 8/10, step 548/574 completed (loss: 0.001585738966241479, acc: 1.0)
[2025-01-06 01:46:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:45][root][INFO] - Training Epoch: 8/10, step 549/574 completed (loss: 0.0010610786266624928, acc: 1.0)
[2025-01-06 01:46:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:45][root][INFO] - Training Epoch: 8/10, step 550/574 completed (loss: 0.005416269414126873, acc: 1.0)
[2025-01-06 01:46:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:46][root][INFO] - Training Epoch: 8/10, step 551/574 completed (loss: 0.028837066143751144, acc: 0.9750000238418579)
[2025-01-06 01:46:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:46][root][INFO] - Training Epoch: 8/10, step 552/574 completed (loss: 0.1248411014676094, acc: 0.9714285731315613)
[2025-01-06 01:46:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:47][root][INFO] - Training Epoch: 8/10, step 553/574 completed (loss: 0.03082343004643917, acc: 0.985401451587677)
[2025-01-06 01:46:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:47][root][INFO] - Training Epoch: 8/10, step 554/574 completed (loss: 0.014466311782598495, acc: 0.9931034445762634)
[2025-01-06 01:46:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:47][root][INFO] - Training Epoch: 8/10, step 555/574 completed (loss: 0.025142820551991463, acc: 0.9857142567634583)
[2025-01-06 01:46:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:48][root][INFO] - Training Epoch: 8/10, step 556/574 completed (loss: 0.052334997802972794, acc: 0.9735099077224731)
[2025-01-06 01:46:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:48][root][INFO] - Training Epoch: 8/10, step 557/574 completed (loss: 0.015231844037771225, acc: 0.9914529919624329)
[2025-01-06 01:46:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:46:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:18][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.3389, device='cuda:0') eval_epoch_loss=tensor(0.8497, device='cuda:0') eval_epoch_acc=tensor(0.8424, device='cuda:0')
[2025-01-06 01:47:18][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:47:18][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:47:19][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_8_step_558_loss_0.849675178527832/model.pt
[2025-01-06 01:47:19][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:47:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:19][root][INFO] - Training Epoch: 8/10, step 558/574 completed (loss: 0.008727817796170712, acc: 1.0)
[2025-01-06 01:47:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:19][root][INFO] - Training Epoch: 8/10, step 559/574 completed (loss: 0.022611528635025024, acc: 1.0)
[2025-01-06 01:47:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:20][root][INFO] - Training Epoch: 8/10, step 560/574 completed (loss: 0.0006678560166619718, acc: 1.0)
[2025-01-06 01:47:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:20][root][INFO] - Training Epoch: 8/10, step 561/574 completed (loss: 0.14627684652805328, acc: 0.9743589758872986)
[2025-01-06 01:47:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:21][root][INFO] - Training Epoch: 8/10, step 562/574 completed (loss: 0.22301432490348816, acc: 0.9555555582046509)
[2025-01-06 01:47:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:21][root][INFO] - Training Epoch: 8/10, step 563/574 completed (loss: 0.012065861374139786, acc: 1.0)
[2025-01-06 01:47:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:21][root][INFO] - Training Epoch: 8/10, step 564/574 completed (loss: 0.038624998182058334, acc: 0.9791666865348816)
[2025-01-06 01:47:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:22][root][INFO] - Training Epoch: 8/10, step 565/574 completed (loss: 0.06342020630836487, acc: 0.982758641242981)
[2025-01-06 01:47:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:22][root][INFO] - Training Epoch: 8/10, step 566/574 completed (loss: 0.01206880621612072, acc: 1.0)
[2025-01-06 01:47:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:22][root][INFO] - Training Epoch: 8/10, step 567/574 completed (loss: 0.0011823448585346341, acc: 1.0)
[2025-01-06 01:47:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:23][root][INFO] - Training Epoch: 8/10, step 568/574 completed (loss: 0.004882153123617172, acc: 1.0)
[2025-01-06 01:47:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:23][root][INFO] - Training Epoch: 8/10, step 569/574 completed (loss: 0.06131460890173912, acc: 0.9893048405647278)
[2025-01-06 01:47:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:23][root][INFO] - Training Epoch: 8/10, step 570/574 completed (loss: 0.0007885186350904405, acc: 1.0)
[2025-01-06 01:47:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:24][root][INFO] - Training Epoch: 8/10, step 571/574 completed (loss: 0.007590972352772951, acc: 1.0)
[2025-01-06 01:47:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:24][root][INFO] - Training Epoch: 8/10, step 572/574 completed (loss: 0.051294758915901184, acc: 0.9897959232330322)
[2025-01-06 01:47:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:25][root][INFO] - Training Epoch: 8/10, step 573/574 completed (loss: 0.0457347109913826, acc: 0.9811320900917053)
[2025-01-06 01:47:25][slam_llm.utils.train_utils][INFO] - Epoch 8: train_perplexity=1.0582, train_epoch_loss=0.0565, epoch time 352.5695807337761s
[2025-01-06 01:47:25][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 13 GB
[2025-01-06 01:47:25][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 15 GB
[2025-01-06 01:47:25][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 13 GB
[2025-01-06 01:47:25][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 22
[2025-01-06 01:47:25][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2025-01-06 01:47:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:26][root][INFO] - Training Epoch: 9/10, step 0/574 completed (loss: 0.0023318917956203222, acc: 1.0)
[2025-01-06 01:47:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:26][root][INFO] - Training Epoch: 9/10, step 1/574 completed (loss: 0.002202319446951151, acc: 1.0)
[2025-01-06 01:47:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:26][root][INFO] - Training Epoch: 9/10, step 2/574 completed (loss: 0.02475140057504177, acc: 1.0)
[2025-01-06 01:47:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:27][root][INFO] - Training Epoch: 9/10, step 3/574 completed (loss: 0.05095674470067024, acc: 0.9736841917037964)
[2025-01-06 01:47:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:27][root][INFO] - Training Epoch: 9/10, step 4/574 completed (loss: 0.015289321541786194, acc: 1.0)
[2025-01-06 01:47:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:28][root][INFO] - Training Epoch: 9/10, step 5/574 completed (loss: 0.009136230684816837, acc: 1.0)
[2025-01-06 01:47:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:28][root][INFO] - Training Epoch: 9/10, step 6/574 completed (loss: 0.04227451980113983, acc: 0.9795918464660645)
[2025-01-06 01:47:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:28][root][INFO] - Training Epoch: 9/10, step 7/574 completed (loss: 0.00458995345979929, acc: 1.0)
[2025-01-06 01:47:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:28][root][INFO] - Training Epoch: 9/10, step 8/574 completed (loss: 0.0009745506104081869, acc: 1.0)
[2025-01-06 01:47:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:29][root][INFO] - Training Epoch: 9/10, step 9/574 completed (loss: 0.0011771071003749967, acc: 1.0)
[2025-01-06 01:47:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:29][root][INFO] - Training Epoch: 9/10, step 10/574 completed (loss: 0.0011743897339329123, acc: 1.0)
[2025-01-06 01:47:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:30][root][INFO] - Training Epoch: 9/10, step 11/574 completed (loss: 0.0028595563489943743, acc: 1.0)
[2025-01-06 01:47:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:30][root][INFO] - Training Epoch: 9/10, step 12/574 completed (loss: 0.0014286440564319491, acc: 1.0)
[2025-01-06 01:47:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:30][root][INFO] - Training Epoch: 9/10, step 13/574 completed (loss: 0.0041366503573954105, acc: 1.0)
[2025-01-06 01:47:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:31][root][INFO] - Training Epoch: 9/10, step 14/574 completed (loss: 0.017680643126368523, acc: 1.0)
[2025-01-06 01:47:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:31][root][INFO] - Training Epoch: 9/10, step 15/574 completed (loss: 0.02607795223593712, acc: 0.9795918464660645)
[2025-01-06 01:47:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:31][root][INFO] - Training Epoch: 9/10, step 16/574 completed (loss: 0.0013892841525375843, acc: 1.0)
[2025-01-06 01:47:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:32][root][INFO] - Training Epoch: 9/10, step 17/574 completed (loss: 0.02535981871187687, acc: 1.0)
[2025-01-06 01:47:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:32][root][INFO] - Training Epoch: 9/10, step 18/574 completed (loss: 0.006156274117529392, acc: 1.0)
[2025-01-06 01:47:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:32][root][INFO] - Training Epoch: 9/10, step 19/574 completed (loss: 0.046487342566251755, acc: 0.9473684430122375)
[2025-01-06 01:47:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:33][root][INFO] - Training Epoch: 9/10, step 20/574 completed (loss: 0.0027320210356265306, acc: 1.0)
[2025-01-06 01:47:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:33][root][INFO] - Training Epoch: 9/10, step 21/574 completed (loss: 0.0037227803841233253, acc: 1.0)
[2025-01-06 01:47:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:34][root][INFO] - Training Epoch: 9/10, step 22/574 completed (loss: 0.04440680891275406, acc: 0.9599999785423279)
[2025-01-06 01:47:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:34][root][INFO] - Training Epoch: 9/10, step 23/574 completed (loss: 0.005047729704529047, acc: 1.0)
[2025-01-06 01:47:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:34][root][INFO] - Training Epoch: 9/10, step 24/574 completed (loss: 0.0006259400397539139, acc: 1.0)
[2025-01-06 01:47:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:35][root][INFO] - Training Epoch: 9/10, step 25/574 completed (loss: 0.01579407788813114, acc: 1.0)
[2025-01-06 01:47:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:35][root][INFO] - Training Epoch: 9/10, step 26/574 completed (loss: 0.05575505271553993, acc: 0.9863013625144958)
[2025-01-06 01:47:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:36][root][INFO] - Training Epoch: 9/10, step 27/574 completed (loss: 0.2506677508354187, acc: 0.9209486246109009)
[2025-01-06 01:47:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:37][root][INFO] - Training Epoch: 9/10, step 28/574 completed (loss: 0.010805551894009113, acc: 1.0)
[2025-01-06 01:47:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:37][root][INFO] - Training Epoch: 9/10, step 29/574 completed (loss: 0.0551212914288044, acc: 0.9759036302566528)
[2025-01-06 01:47:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:37][root][INFO] - Training Epoch: 9/10, step 30/574 completed (loss: 0.01813134178519249, acc: 1.0)
[2025-01-06 01:47:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:38][root][INFO] - Training Epoch: 9/10, step 31/574 completed (loss: 0.012551786378026009, acc: 1.0)
[2025-01-06 01:47:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:38][root][INFO] - Training Epoch: 9/10, step 32/574 completed (loss: 0.0018703421810641885, acc: 1.0)
[2025-01-06 01:47:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:38][root][INFO] - Training Epoch: 9/10, step 33/574 completed (loss: 0.002599524799734354, acc: 1.0)
[2025-01-06 01:47:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:39][root][INFO] - Training Epoch: 9/10, step 34/574 completed (loss: 0.02626294270157814, acc: 0.9915966391563416)
[2025-01-06 01:47:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:39][root][INFO] - Training Epoch: 9/10, step 35/574 completed (loss: 0.004134449642151594, acc: 1.0)
[2025-01-06 01:47:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:39][root][INFO] - Training Epoch: 9/10, step 36/574 completed (loss: 0.022225165739655495, acc: 1.0)
[2025-01-06 01:47:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:40][root][INFO] - Training Epoch: 9/10, step 37/574 completed (loss: 0.021762609481811523, acc: 1.0)
[2025-01-06 01:47:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:40][root][INFO] - Training Epoch: 9/10, step 38/574 completed (loss: 0.053510308265686035, acc: 0.9885057210922241)
[2025-01-06 01:47:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:41][root][INFO] - Training Epoch: 9/10, step 39/574 completed (loss: 0.0071692923083901405, acc: 1.0)
[2025-01-06 01:47:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:41][root][INFO] - Training Epoch: 9/10, step 40/574 completed (loss: 0.06265123933553696, acc: 0.9615384340286255)
[2025-01-06 01:47:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:41][root][INFO] - Training Epoch: 9/10, step 41/574 completed (loss: 0.053542207926511765, acc: 0.9729729890823364)
[2025-01-06 01:47:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:42][root][INFO] - Training Epoch: 9/10, step 42/574 completed (loss: 0.06297094374895096, acc: 0.9538461565971375)
[2025-01-06 01:47:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:42][root][INFO] - Training Epoch: 9/10, step 43/574 completed (loss: 0.06488726288080215, acc: 0.9797979593276978)
[2025-01-06 01:47:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:43][root][INFO] - Training Epoch: 9/10, step 44/574 completed (loss: 0.0701722726225853, acc: 0.9896907210350037)
[2025-01-06 01:47:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:43][root][INFO] - Training Epoch: 9/10, step 45/574 completed (loss: 0.04113219678401947, acc: 0.9852941036224365)
[2025-01-06 01:47:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:43][root][INFO] - Training Epoch: 9/10, step 46/574 completed (loss: 0.2855221927165985, acc: 0.9615384340286255)
[2025-01-06 01:47:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:44][root][INFO] - Training Epoch: 9/10, step 47/574 completed (loss: 0.0021900152787566185, acc: 1.0)
[2025-01-06 01:47:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:44][root][INFO] - Training Epoch: 9/10, step 48/574 completed (loss: 0.0030058010015636683, acc: 1.0)
[2025-01-06 01:47:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:44][root][INFO] - Training Epoch: 9/10, step 49/574 completed (loss: 0.00039607807411812246, acc: 1.0)
[2025-01-06 01:47:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:45][root][INFO] - Training Epoch: 9/10, step 50/574 completed (loss: 0.0360187366604805, acc: 0.9824561476707458)
[2025-01-06 01:47:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:45][root][INFO] - Training Epoch: 9/10, step 51/574 completed (loss: 0.029832933098077774, acc: 0.9841269850730896)
[2025-01-06 01:47:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:45][root][INFO] - Training Epoch: 9/10, step 52/574 completed (loss: 0.12556777894496918, acc: 0.9577465057373047)
[2025-01-06 01:47:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:46][root][INFO] - Training Epoch: 9/10, step 53/574 completed (loss: 0.354116827249527, acc: 0.9066666960716248)
[2025-01-06 01:47:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:46][root][INFO] - Training Epoch: 9/10, step 54/574 completed (loss: 0.011706686578691006, acc: 1.0)
[2025-01-06 01:47:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:47][root][INFO] - Training Epoch: 9/10, step 55/574 completed (loss: 0.0025225868448615074, acc: 1.0)
[2025-01-06 01:47:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:50][root][INFO] - Training Epoch: 9/10, step 56/574 completed (loss: 0.3456701338291168, acc: 0.8873720169067383)
[2025-01-06 01:47:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:51][root][INFO] - Training Epoch: 9/10, step 57/574 completed (loss: 0.49725449085235596, acc: 0.8540304899215698)
[2025-01-06 01:47:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:52][root][INFO] - Training Epoch: 9/10, step 58/574 completed (loss: 0.16303877532482147, acc: 0.9488636255264282)
[2025-01-06 01:47:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:52][root][INFO] - Training Epoch: 9/10, step 59/574 completed (loss: 0.013490524142980576, acc: 1.0)
[2025-01-06 01:47:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:53][root][INFO] - Training Epoch: 9/10, step 60/574 completed (loss: 0.17361100018024445, acc: 0.9420289993286133)
[2025-01-06 01:47:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:53][root][INFO] - Training Epoch: 9/10, step 61/574 completed (loss: 0.0459863618016243, acc: 0.987500011920929)
[2025-01-06 01:47:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:53][root][INFO] - Training Epoch: 9/10, step 62/574 completed (loss: 0.0016326751792803407, acc: 1.0)
[2025-01-06 01:47:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:54][root][INFO] - Training Epoch: 9/10, step 63/574 completed (loss: 0.0019313833909109235, acc: 1.0)
[2025-01-06 01:47:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:54][root][INFO] - Training Epoch: 9/10, step 64/574 completed (loss: 0.00495602423325181, acc: 1.0)
[2025-01-06 01:47:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:54][root][INFO] - Training Epoch: 9/10, step 65/574 completed (loss: 0.002693323651328683, acc: 1.0)
[2025-01-06 01:47:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:55][root][INFO] - Training Epoch: 9/10, step 66/574 completed (loss: 0.0407169945538044, acc: 0.9821428656578064)
[2025-01-06 01:47:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:55][root][INFO] - Training Epoch: 9/10, step 67/574 completed (loss: 0.007223390508443117, acc: 1.0)
[2025-01-06 01:47:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:56][root][INFO] - Training Epoch: 9/10, step 68/574 completed (loss: 0.0004213885113131255, acc: 1.0)
[2025-01-06 01:47:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:56][root][INFO] - Training Epoch: 9/10, step 69/574 completed (loss: 0.07289334386587143, acc: 0.9722222089767456)
[2025-01-06 01:47:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:56][root][INFO] - Training Epoch: 9/10, step 70/574 completed (loss: 0.009813360869884491, acc: 1.0)
[2025-01-06 01:47:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:57][root][INFO] - Training Epoch: 9/10, step 71/574 completed (loss: 0.2325371503829956, acc: 0.9485294222831726)
[2025-01-06 01:47:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:57][root][INFO] - Training Epoch: 9/10, step 72/574 completed (loss: 0.07152356952428818, acc: 0.9841269850730896)
[2025-01-06 01:47:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:57][root][INFO] - Training Epoch: 9/10, step 73/574 completed (loss: 0.2565337121486664, acc: 0.9333333373069763)
[2025-01-06 01:47:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:58][root][INFO] - Training Epoch: 9/10, step 74/574 completed (loss: 0.024523576721549034, acc: 0.9897959232330322)
[2025-01-06 01:47:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:58][root][INFO] - Training Epoch: 9/10, step 75/574 completed (loss: 0.09575251489877701, acc: 0.9776119589805603)
[2025-01-06 01:47:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:58][root][INFO] - Training Epoch: 9/10, step 76/574 completed (loss: 0.3300589919090271, acc: 0.8759124279022217)
[2025-01-06 01:47:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:59][root][INFO] - Training Epoch: 9/10, step 77/574 completed (loss: 0.00029621447902172804, acc: 1.0)
[2025-01-06 01:47:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:47:59][root][INFO] - Training Epoch: 9/10, step 78/574 completed (loss: 0.0008827997371554375, acc: 1.0)
[2025-01-06 01:47:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:00][root][INFO] - Training Epoch: 9/10, step 79/574 completed (loss: 0.001734934514388442, acc: 1.0)
[2025-01-06 01:48:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:00][root][INFO] - Training Epoch: 9/10, step 80/574 completed (loss: 0.0006043206085450947, acc: 1.0)
[2025-01-06 01:48:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:00][root][INFO] - Training Epoch: 9/10, step 81/574 completed (loss: 0.006964616011828184, acc: 1.0)
[2025-01-06 01:48:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:01][root][INFO] - Training Epoch: 9/10, step 82/574 completed (loss: 0.020212896168231964, acc: 1.0)
[2025-01-06 01:48:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:01][root][INFO] - Training Epoch: 9/10, step 83/574 completed (loss: 0.523918867111206, acc: 0.9375)
[2025-01-06 01:48:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:01][root][INFO] - Training Epoch: 9/10, step 84/574 completed (loss: 0.035630982369184494, acc: 0.9855072498321533)
[2025-01-06 01:48:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:02][root][INFO] - Training Epoch: 9/10, step 85/574 completed (loss: 0.0671144500374794, acc: 0.9800000190734863)
[2025-01-06 01:48:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:02][root][INFO] - Training Epoch: 9/10, step 86/574 completed (loss: 0.000955873285420239, acc: 1.0)
[2025-01-06 01:48:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:03][root][INFO] - Training Epoch: 9/10, step 87/574 completed (loss: 0.16895098984241486, acc: 0.9599999785423279)
[2025-01-06 01:48:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:03][root][INFO] - Training Epoch: 9/10, step 88/574 completed (loss: 0.09068118780851364, acc: 0.9611650705337524)
[2025-01-06 01:48:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:04][root][INFO] - Training Epoch: 9/10, step 89/574 completed (loss: 0.12137433141469955, acc: 0.946601927280426)
[2025-01-06 01:48:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:05][root][INFO] - Training Epoch: 9/10, step 90/574 completed (loss: 0.22648905217647552, acc: 0.9247311949729919)
[2025-01-06 01:48:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:06][root][INFO] - Training Epoch: 9/10, step 91/574 completed (loss: 0.2727415859699249, acc: 0.9396551847457886)
[2025-01-06 01:48:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:06][root][INFO] - Training Epoch: 9/10, step 92/574 completed (loss: 0.04929584264755249, acc: 0.9894737005233765)
[2025-01-06 01:48:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:07][root][INFO] - Training Epoch: 9/10, step 93/574 completed (loss: 0.2288847118616104, acc: 0.9504950642585754)
[2025-01-06 01:48:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:08][root][INFO] - Training Epoch: 9/10, step 94/574 completed (loss: 0.02764211781322956, acc: 1.0)
[2025-01-06 01:48:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:08][root][INFO] - Training Epoch: 9/10, step 95/574 completed (loss: 0.06414689868688583, acc: 0.9710144996643066)
[2025-01-06 01:48:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:08][root][INFO] - Training Epoch: 9/10, step 96/574 completed (loss: 0.15488281846046448, acc: 0.9495798349380493)
[2025-01-06 01:48:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:09][root][INFO] - Training Epoch: 9/10, step 97/574 completed (loss: 0.08665701001882553, acc: 0.9807692170143127)
[2025-01-06 01:48:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:09][root][INFO] - Training Epoch: 9/10, step 98/574 completed (loss: 0.059117142111063004, acc: 0.985401451587677)
[2025-01-06 01:48:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:10][root][INFO] - Training Epoch: 9/10, step 99/574 completed (loss: 0.20091009140014648, acc: 0.9402984976768494)
[2025-01-06 01:48:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:10][root][INFO] - Training Epoch: 9/10, step 100/574 completed (loss: 0.007883453741669655, acc: 1.0)
[2025-01-06 01:48:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:10][root][INFO] - Training Epoch: 9/10, step 101/574 completed (loss: 0.00036116529372520745, acc: 1.0)
[2025-01-06 01:48:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:11][root][INFO] - Training Epoch: 9/10, step 102/574 completed (loss: 0.002710917964577675, acc: 1.0)
[2025-01-06 01:48:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:11][root][INFO] - Training Epoch: 9/10, step 103/574 completed (loss: 0.052861712872982025, acc: 0.9772727489471436)
[2025-01-06 01:48:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:11][root][INFO] - Training Epoch: 9/10, step 104/574 completed (loss: 0.02143961936235428, acc: 1.0)
[2025-01-06 01:48:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:12][root][INFO] - Training Epoch: 9/10, step 105/574 completed (loss: 0.014828967861831188, acc: 1.0)
[2025-01-06 01:48:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:12][root][INFO] - Training Epoch: 9/10, step 106/574 completed (loss: 0.0068488698452711105, acc: 1.0)
[2025-01-06 01:48:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:12][root][INFO] - Training Epoch: 9/10, step 107/574 completed (loss: 0.0001314463297603652, acc: 1.0)
[2025-01-06 01:48:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:13][root][INFO] - Training Epoch: 9/10, step 108/574 completed (loss: 0.00034013419644907117, acc: 1.0)
[2025-01-06 01:48:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:13][root][INFO] - Training Epoch: 9/10, step 109/574 completed (loss: 0.0020339603070169687, acc: 1.0)
[2025-01-06 01:48:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:14][root][INFO] - Training Epoch: 9/10, step 110/574 completed (loss: 0.0024445257149636745, acc: 1.0)
[2025-01-06 01:48:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:14][root][INFO] - Training Epoch: 9/10, step 111/574 completed (loss: 0.01220853254199028, acc: 1.0)
[2025-01-06 01:48:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:14][root][INFO] - Training Epoch: 9/10, step 112/574 completed (loss: 0.11201956868171692, acc: 0.9649122953414917)
[2025-01-06 01:48:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:15][root][INFO] - Training Epoch: 9/10, step 113/574 completed (loss: 0.11624018102884293, acc: 0.9487179517745972)
[2025-01-06 01:48:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:15][root][INFO] - Training Epoch: 9/10, step 114/574 completed (loss: 0.004647546447813511, acc: 1.0)
[2025-01-06 01:48:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:16][root][INFO] - Training Epoch: 9/10, step 115/574 completed (loss: 0.0013064263621345162, acc: 1.0)
[2025-01-06 01:48:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:16][root][INFO] - Training Epoch: 9/10, step 116/574 completed (loss: 0.062005944550037384, acc: 0.9841269850730896)
[2025-01-06 01:48:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:16][root][INFO] - Training Epoch: 9/10, step 117/574 completed (loss: 0.02193031832575798, acc: 0.9918699264526367)
[2025-01-06 01:48:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:17][root][INFO] - Training Epoch: 9/10, step 118/574 completed (loss: 0.006329917814582586, acc: 1.0)
[2025-01-06 01:48:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:18][root][INFO] - Training Epoch: 9/10, step 119/574 completed (loss: 0.12903787195682526, acc: 0.9733840227127075)
[2025-01-06 01:48:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:18][root][INFO] - Training Epoch: 9/10, step 120/574 completed (loss: 0.04046162590384483, acc: 0.9866666793823242)
[2025-01-06 01:48:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:18][root][INFO] - Training Epoch: 9/10, step 121/574 completed (loss: 0.0047364868223667145, acc: 1.0)
[2025-01-06 01:48:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:19][root][INFO] - Training Epoch: 9/10, step 122/574 completed (loss: 0.0010183979757130146, acc: 1.0)
[2025-01-06 01:48:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:19][root][INFO] - Training Epoch: 9/10, step 123/574 completed (loss: 0.0016087971162050962, acc: 1.0)
[2025-01-06 01:48:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:19][root][INFO] - Training Epoch: 9/10, step 124/574 completed (loss: 0.09186072647571564, acc: 0.9815950989723206)
[2025-01-06 01:48:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:20][root][INFO] - Training Epoch: 9/10, step 125/574 completed (loss: 0.05512479320168495, acc: 0.9861111044883728)
[2025-01-06 01:48:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:20][root][INFO] - Training Epoch: 9/10, step 126/574 completed (loss: 0.17084594070911407, acc: 0.949999988079071)
[2025-01-06 01:48:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:51][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.2247, device='cuda:0') eval_epoch_loss=tensor(0.7996, device='cuda:0') eval_epoch_acc=tensor(0.8509, device='cuda:0')
[2025-01-06 01:48:51][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:48:51][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:48:51][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_9_step_127_loss_0.7996403574943542/model.pt
[2025-01-06 01:48:51][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:48:51][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 9 is 0.8509207367897034
[2025-01-06 01:48:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:51][root][INFO] - Training Epoch: 9/10, step 127/574 completed (loss: 0.06510607898235321, acc: 0.976190447807312)
[2025-01-06 01:48:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:52][root][INFO] - Training Epoch: 9/10, step 128/574 completed (loss: 0.1529974341392517, acc: 0.9692307710647583)
[2025-01-06 01:48:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:52][root][INFO] - Training Epoch: 9/10, step 129/574 completed (loss: 0.07696311920881271, acc: 0.9852941036224365)
[2025-01-06 01:48:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:53][root][INFO] - Training Epoch: 9/10, step 130/574 completed (loss: 0.034159280359745026, acc: 1.0)
[2025-01-06 01:48:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:53][root][INFO] - Training Epoch: 9/10, step 131/574 completed (loss: 0.002500210190191865, acc: 1.0)
[2025-01-06 01:48:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:53][root][INFO] - Training Epoch: 9/10, step 132/574 completed (loss: 0.0036162161268293858, acc: 1.0)
[2025-01-06 01:48:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:54][root][INFO] - Training Epoch: 9/10, step 133/574 completed (loss: 0.0024046937469393015, acc: 1.0)
[2025-01-06 01:48:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:54][root][INFO] - Training Epoch: 9/10, step 134/574 completed (loss: 0.14647988975048065, acc: 0.9714285731315613)
[2025-01-06 01:48:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:54][root][INFO] - Training Epoch: 9/10, step 135/574 completed (loss: 0.004260709509253502, acc: 1.0)
[2025-01-06 01:48:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:55][root][INFO] - Training Epoch: 9/10, step 136/574 completed (loss: 0.005410739220678806, acc: 1.0)
[2025-01-06 01:48:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:55][root][INFO] - Training Epoch: 9/10, step 137/574 completed (loss: 0.2017696499824524, acc: 0.9666666388511658)
[2025-01-06 01:48:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:55][root][INFO] - Training Epoch: 9/10, step 138/574 completed (loss: 0.0012679891660809517, acc: 1.0)
[2025-01-06 01:48:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:56][root][INFO] - Training Epoch: 9/10, step 139/574 completed (loss: 0.0014102112036198378, acc: 1.0)
[2025-01-06 01:48:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:56][root][INFO] - Training Epoch: 9/10, step 140/574 completed (loss: 0.0011401831870898604, acc: 1.0)
[2025-01-06 01:48:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:56][root][INFO] - Training Epoch: 9/10, step 141/574 completed (loss: 0.0026681546587496996, acc: 1.0)
[2025-01-06 01:48:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:57][root][INFO] - Training Epoch: 9/10, step 142/574 completed (loss: 0.015393493697047234, acc: 1.0)
[2025-01-06 01:48:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:57][root][INFO] - Training Epoch: 9/10, step 143/574 completed (loss: 0.07024964690208435, acc: 0.9649122953414917)
[2025-01-06 01:48:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:58][root][INFO] - Training Epoch: 9/10, step 144/574 completed (loss: 0.11084144562482834, acc: 0.9626865386962891)
[2025-01-06 01:48:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:58][root][INFO] - Training Epoch: 9/10, step 145/574 completed (loss: 0.03712652251124382, acc: 0.9897959232330322)
[2025-01-06 01:48:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:58][root][INFO] - Training Epoch: 9/10, step 146/574 completed (loss: 0.24715681374073029, acc: 0.9042553305625916)
[2025-01-06 01:48:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:59][root][INFO] - Training Epoch: 9/10, step 147/574 completed (loss: 0.03049507550895214, acc: 0.9857142567634583)
[2025-01-06 01:48:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:59][root][INFO] - Training Epoch: 9/10, step 148/574 completed (loss: 0.04953545331954956, acc: 1.0)
[2025-01-06 01:48:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:48:59][root][INFO] - Training Epoch: 9/10, step 149/574 completed (loss: 0.0012346070725470781, acc: 1.0)
[2025-01-06 01:49:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:00][root][INFO] - Training Epoch: 9/10, step 150/574 completed (loss: 0.010959151200950146, acc: 1.0)
[2025-01-06 01:49:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:00][root][INFO] - Training Epoch: 9/10, step 151/574 completed (loss: 0.01810169219970703, acc: 1.0)
[2025-01-06 01:49:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:00][root][INFO] - Training Epoch: 9/10, step 152/574 completed (loss: 0.0095845190808177, acc: 1.0)
[2025-01-06 01:49:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:01][root][INFO] - Training Epoch: 9/10, step 153/574 completed (loss: 0.02366291731595993, acc: 1.0)
[2025-01-06 01:49:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:01][root][INFO] - Training Epoch: 9/10, step 154/574 completed (loss: 0.06229257583618164, acc: 0.9729729890823364)
[2025-01-06 01:49:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:01][root][INFO] - Training Epoch: 9/10, step 155/574 completed (loss: 0.02259550429880619, acc: 1.0)
[2025-01-06 01:49:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:02][root][INFO] - Training Epoch: 9/10, step 156/574 completed (loss: 0.19739973545074463, acc: 0.95652174949646)
[2025-01-06 01:49:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:02][root][INFO] - Training Epoch: 9/10, step 157/574 completed (loss: 0.5817663073539734, acc: 0.8421052694320679)
[2025-01-06 01:49:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:04][root][INFO] - Training Epoch: 9/10, step 158/574 completed (loss: 0.19085459411144257, acc: 0.9324324131011963)
[2025-01-06 01:49:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:04][root][INFO] - Training Epoch: 9/10, step 159/574 completed (loss: 0.31036967039108276, acc: 0.9444444179534912)
[2025-01-06 01:49:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:04][root][INFO] - Training Epoch: 9/10, step 160/574 completed (loss: 0.16525088250637054, acc: 0.9418604373931885)
[2025-01-06 01:49:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:05][root][INFO] - Training Epoch: 9/10, step 161/574 completed (loss: 0.17217670381069183, acc: 0.9529411792755127)
[2025-01-06 01:49:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:05][root][INFO] - Training Epoch: 9/10, step 162/574 completed (loss: 0.1313820332288742, acc: 0.966292142868042)
[2025-01-06 01:49:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:06][root][INFO] - Training Epoch: 9/10, step 163/574 completed (loss: 0.018539678305387497, acc: 1.0)
[2025-01-06 01:49:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:06][root][INFO] - Training Epoch: 9/10, step 164/574 completed (loss: 0.006765895057469606, acc: 1.0)
[2025-01-06 01:49:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:06][root][INFO] - Training Epoch: 9/10, step 165/574 completed (loss: 0.11461041122674942, acc: 0.931034505367279)
[2025-01-06 01:49:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:07][root][INFO] - Training Epoch: 9/10, step 166/574 completed (loss: 0.003803685074672103, acc: 1.0)
[2025-01-06 01:49:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:07][root][INFO] - Training Epoch: 9/10, step 167/574 completed (loss: 0.02905714325606823, acc: 0.9800000190734863)
[2025-01-06 01:49:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:08][root][INFO] - Training Epoch: 9/10, step 168/574 completed (loss: 0.01618359051644802, acc: 1.0)
[2025-01-06 01:49:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:08][root][INFO] - Training Epoch: 9/10, step 169/574 completed (loss: 0.09439052641391754, acc: 0.9607843160629272)
[2025-01-06 01:49:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:09][root][INFO] - Training Epoch: 9/10, step 170/574 completed (loss: 0.18059618771076202, acc: 0.9452054500579834)
[2025-01-06 01:49:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:09][root][INFO] - Training Epoch: 9/10, step 171/574 completed (loss: 0.0011626621708273888, acc: 1.0)
[2025-01-06 01:49:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:10][root][INFO] - Training Epoch: 9/10, step 172/574 completed (loss: 0.09340845048427582, acc: 0.9629629850387573)
[2025-01-06 01:49:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:10][root][INFO] - Training Epoch: 9/10, step 173/574 completed (loss: 0.020640024915337563, acc: 1.0)
[2025-01-06 01:49:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:11][root][INFO] - Training Epoch: 9/10, step 174/574 completed (loss: 0.18011268973350525, acc: 0.9380530714988708)
[2025-01-06 01:49:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:11][root][INFO] - Training Epoch: 9/10, step 175/574 completed (loss: 0.1395214945077896, acc: 0.9420289993286133)
[2025-01-06 01:49:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:11][root][INFO] - Training Epoch: 9/10, step 176/574 completed (loss: 0.02590312995016575, acc: 0.9886363744735718)
[2025-01-06 01:49:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:12][root][INFO] - Training Epoch: 9/10, step 177/574 completed (loss: 0.13068102300167084, acc: 0.9389312863349915)
[2025-01-06 01:49:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:13][root][INFO] - Training Epoch: 9/10, step 178/574 completed (loss: 0.07331464439630508, acc: 0.9777777791023254)
[2025-01-06 01:49:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:13][root][INFO] - Training Epoch: 9/10, step 179/574 completed (loss: 0.020951732993125916, acc: 1.0)
[2025-01-06 01:49:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:14][root][INFO] - Training Epoch: 9/10, step 180/574 completed (loss: 0.004875510465353727, acc: 1.0)
[2025-01-06 01:49:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:14][root][INFO] - Training Epoch: 9/10, step 181/574 completed (loss: 0.0019043725915253162, acc: 1.0)
[2025-01-06 01:49:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:14][root][INFO] - Training Epoch: 9/10, step 182/574 completed (loss: 0.0015138277085497975, acc: 1.0)
[2025-01-06 01:49:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:15][root][INFO] - Training Epoch: 9/10, step 183/574 completed (loss: 0.008992145769298077, acc: 1.0)
[2025-01-06 01:49:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:15][root][INFO] - Training Epoch: 9/10, step 184/574 completed (loss: 0.08566267788410187, acc: 0.9728096723556519)
[2025-01-06 01:49:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:15][root][INFO] - Training Epoch: 9/10, step 185/574 completed (loss: 0.0731973648071289, acc: 0.9827089309692383)
[2025-01-06 01:49:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:16][root][INFO] - Training Epoch: 9/10, step 186/574 completed (loss: 0.04633525386452675, acc: 0.984375)
[2025-01-06 01:49:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:16][root][INFO] - Training Epoch: 9/10, step 187/574 completed (loss: 0.14881165325641632, acc: 0.9474671483039856)
[2025-01-06 01:49:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:17][root][INFO] - Training Epoch: 9/10, step 188/574 completed (loss: 0.05551288649439812, acc: 0.9857650995254517)
[2025-01-06 01:49:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:17][root][INFO] - Training Epoch: 9/10, step 189/574 completed (loss: 0.017590614035725594, acc: 1.0)
[2025-01-06 01:49:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:18][root][INFO] - Training Epoch: 9/10, step 190/574 completed (loss: 0.07463519275188446, acc: 0.9534883499145508)
[2025-01-06 01:49:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:18][root][INFO] - Training Epoch: 9/10, step 191/574 completed (loss: 0.16336609423160553, acc: 0.9365079402923584)
[2025-01-06 01:49:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:19][root][INFO] - Training Epoch: 9/10, step 192/574 completed (loss: 0.16615013778209686, acc: 0.939393937587738)
[2025-01-06 01:49:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:20][root][INFO] - Training Epoch: 9/10, step 193/574 completed (loss: 0.1502813845872879, acc: 0.9647058844566345)
[2025-01-06 01:49:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:21][root][INFO] - Training Epoch: 9/10, step 194/574 completed (loss: 0.15651196241378784, acc: 0.9691358208656311)
[2025-01-06 01:49:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:22][root][INFO] - Training Epoch: 9/10, step 195/574 completed (loss: 0.05754515528678894, acc: 0.9838709831237793)
[2025-01-06 01:49:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:22][root][INFO] - Training Epoch: 9/10, step 196/574 completed (loss: 0.002445041900500655, acc: 1.0)
[2025-01-06 01:49:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:23][root][INFO] - Training Epoch: 9/10, step 197/574 completed (loss: 0.044423360377550125, acc: 0.9750000238418579)
[2025-01-06 01:49:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:23][root][INFO] - Training Epoch: 9/10, step 198/574 completed (loss: 0.018534788861870766, acc: 1.0)
[2025-01-06 01:49:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:24][root][INFO] - Training Epoch: 9/10, step 199/574 completed (loss: 0.09607309848070145, acc: 0.970588207244873)
[2025-01-06 01:49:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:24][root][INFO] - Training Epoch: 9/10, step 200/574 completed (loss: 0.1273832619190216, acc: 0.9491525292396545)
[2025-01-06 01:49:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:24][root][INFO] - Training Epoch: 9/10, step 201/574 completed (loss: 0.061061397194862366, acc: 0.9925373196601868)
[2025-01-06 01:49:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:25][root][INFO] - Training Epoch: 9/10, step 202/574 completed (loss: 0.08518365025520325, acc: 0.9708737730979919)
[2025-01-06 01:49:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:25][root][INFO] - Training Epoch: 9/10, step 203/574 completed (loss: 0.006756216753274202, acc: 1.0)
[2025-01-06 01:49:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:25][root][INFO] - Training Epoch: 9/10, step 204/574 completed (loss: 0.0023612945806235075, acc: 1.0)
[2025-01-06 01:49:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:26][root][INFO] - Training Epoch: 9/10, step 205/574 completed (loss: 0.020346375182271004, acc: 0.9910314083099365)
[2025-01-06 01:49:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:26][root][INFO] - Training Epoch: 9/10, step 206/574 completed (loss: 0.03587644547224045, acc: 0.9842519760131836)
[2025-01-06 01:49:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:26][root][INFO] - Training Epoch: 9/10, step 207/574 completed (loss: 0.012935309670865536, acc: 1.0)
[2025-01-06 01:49:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:27][root][INFO] - Training Epoch: 9/10, step 208/574 completed (loss: 0.06195973604917526, acc: 0.9855072498321533)
[2025-01-06 01:49:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:27][root][INFO] - Training Epoch: 9/10, step 209/574 completed (loss: 0.03647862374782562, acc: 0.9883268475532532)
[2025-01-06 01:49:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:27][root][INFO] - Training Epoch: 9/10, step 210/574 completed (loss: 0.039871424436569214, acc: 0.989130437374115)
[2025-01-06 01:49:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:28][root][INFO] - Training Epoch: 9/10, step 211/574 completed (loss: 0.007325244136154652, acc: 1.0)
[2025-01-06 01:49:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:28][root][INFO] - Training Epoch: 9/10, step 212/574 completed (loss: 0.002167736878618598, acc: 1.0)
[2025-01-06 01:49:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:29][root][INFO] - Training Epoch: 9/10, step 213/574 completed (loss: 0.02367938868701458, acc: 0.978723406791687)
[2025-01-06 01:49:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:29][root][INFO] - Training Epoch: 9/10, step 214/574 completed (loss: 0.05386098474264145, acc: 0.9923076629638672)
[2025-01-06 01:49:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:30][root][INFO] - Training Epoch: 9/10, step 215/574 completed (loss: 0.006561394315212965, acc: 1.0)
[2025-01-06 01:49:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:30][root][INFO] - Training Epoch: 9/10, step 216/574 completed (loss: 0.020653843879699707, acc: 0.9883720874786377)
[2025-01-06 01:49:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:30][root][INFO] - Training Epoch: 9/10, step 217/574 completed (loss: 0.011911244131624699, acc: 1.0)
[2025-01-06 01:49:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:31][root][INFO] - Training Epoch: 9/10, step 218/574 completed (loss: 0.005106302909553051, acc: 1.0)
[2025-01-06 01:49:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:31][root][INFO] - Training Epoch: 9/10, step 219/574 completed (loss: 0.0071328068152070045, acc: 1.0)
[2025-01-06 01:49:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:32][root][INFO] - Training Epoch: 9/10, step 220/574 completed (loss: 0.002067963359877467, acc: 1.0)
[2025-01-06 01:49:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:32][root][INFO] - Training Epoch: 9/10, step 221/574 completed (loss: 0.0003405326569918543, acc: 1.0)
[2025-01-06 01:49:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:32][root][INFO] - Training Epoch: 9/10, step 222/574 completed (loss: 0.00680173747241497, acc: 1.0)
[2025-01-06 01:49:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:33][root][INFO] - Training Epoch: 9/10, step 223/574 completed (loss: 0.07890794426202774, acc: 0.97826087474823)
[2025-01-06 01:49:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:34][root][INFO] - Training Epoch: 9/10, step 224/574 completed (loss: 0.07669834792613983, acc: 0.9829545617103577)
[2025-01-06 01:49:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:34][root][INFO] - Training Epoch: 9/10, step 225/574 completed (loss: 0.04139527678489685, acc: 0.9893617033958435)
[2025-01-06 01:49:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:34][root][INFO] - Training Epoch: 9/10, step 226/574 completed (loss: 0.06303026527166367, acc: 0.9622641801834106)
[2025-01-06 01:49:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:35][root][INFO] - Training Epoch: 9/10, step 227/574 completed (loss: 0.014667356386780739, acc: 1.0)
[2025-01-06 01:49:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:35][root][INFO] - Training Epoch: 9/10, step 228/574 completed (loss: 0.098067507147789, acc: 0.9534883499145508)
[2025-01-06 01:49:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:35][root][INFO] - Training Epoch: 9/10, step 229/574 completed (loss: 0.1669103056192398, acc: 0.8999999761581421)
[2025-01-06 01:49:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:36][root][INFO] - Training Epoch: 9/10, step 230/574 completed (loss: 0.4913907051086426, acc: 0.8842105269432068)
[2025-01-06 01:49:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:36][root][INFO] - Training Epoch: 9/10, step 231/574 completed (loss: 0.2734322249889374, acc: 0.8999999761581421)
[2025-01-06 01:49:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:37][root][INFO] - Training Epoch: 9/10, step 232/574 completed (loss: 0.3862982392311096, acc: 0.894444465637207)
[2025-01-06 01:49:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:37][root][INFO] - Training Epoch: 9/10, step 233/574 completed (loss: 0.32544171810150146, acc: 0.8899082541465759)
[2025-01-06 01:49:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:38][root][INFO] - Training Epoch: 9/10, step 234/574 completed (loss: 0.19489197432994843, acc: 0.9384615421295166)
[2025-01-06 01:49:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:38][root][INFO] - Training Epoch: 9/10, step 235/574 completed (loss: 0.06230950355529785, acc: 0.9473684430122375)
[2025-01-06 01:49:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:38][root][INFO] - Training Epoch: 9/10, step 236/574 completed (loss: 0.026147177442908287, acc: 1.0)
[2025-01-06 01:49:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:39][root][INFO] - Training Epoch: 9/10, step 237/574 completed (loss: 0.11713993549346924, acc: 0.9545454382896423)
[2025-01-06 01:49:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:39][root][INFO] - Training Epoch: 9/10, step 238/574 completed (loss: 0.09192696213722229, acc: 0.9629629850387573)
[2025-01-06 01:49:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:39][root][INFO] - Training Epoch: 9/10, step 239/574 completed (loss: 0.033149056136608124, acc: 0.9714285731315613)
[2025-01-06 01:49:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:40][root][INFO] - Training Epoch: 9/10, step 240/574 completed (loss: 0.03028736263513565, acc: 1.0)
[2025-01-06 01:49:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:40][root][INFO] - Training Epoch: 9/10, step 241/574 completed (loss: 0.03870264068245888, acc: 1.0)
[2025-01-06 01:49:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:41][root][INFO] - Training Epoch: 9/10, step 242/574 completed (loss: 0.08320695906877518, acc: 0.9838709831237793)
[2025-01-06 01:49:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:41][root][INFO] - Training Epoch: 9/10, step 243/574 completed (loss: 0.13543573021888733, acc: 0.9772727489471436)
[2025-01-06 01:49:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:41][root][INFO] - Training Epoch: 9/10, step 244/574 completed (loss: 0.0001269435160793364, acc: 1.0)
[2025-01-06 01:49:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:42][root][INFO] - Training Epoch: 9/10, step 245/574 completed (loss: 0.07473281025886536, acc: 0.9615384340286255)
[2025-01-06 01:49:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:42][root][INFO] - Training Epoch: 9/10, step 246/574 completed (loss: 0.001028776285238564, acc: 1.0)
[2025-01-06 01:49:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:42][root][INFO] - Training Epoch: 9/10, step 247/574 completed (loss: 0.017513196915388107, acc: 1.0)
[2025-01-06 01:49:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:43][root][INFO] - Training Epoch: 9/10, step 248/574 completed (loss: 0.02775946818292141, acc: 1.0)
[2025-01-06 01:49:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:43][root][INFO] - Training Epoch: 9/10, step 249/574 completed (loss: 0.018159693107008934, acc: 1.0)
[2025-01-06 01:49:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:43][root][INFO] - Training Epoch: 9/10, step 250/574 completed (loss: 0.0006953873089514673, acc: 1.0)
[2025-01-06 01:49:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:44][root][INFO] - Training Epoch: 9/10, step 251/574 completed (loss: 0.004771877080202103, acc: 1.0)
[2025-01-06 01:49:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:44][root][INFO] - Training Epoch: 9/10, step 252/574 completed (loss: 0.2497062385082245, acc: 0.9756097793579102)
[2025-01-06 01:49:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:45][root][INFO] - Training Epoch: 9/10, step 253/574 completed (loss: 0.019603382796049118, acc: 1.0)
[2025-01-06 01:49:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:45][root][INFO] - Training Epoch: 9/10, step 254/574 completed (loss: 0.00025277090026065707, acc: 1.0)
[2025-01-06 01:49:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:45][root][INFO] - Training Epoch: 9/10, step 255/574 completed (loss: 0.027462242171168327, acc: 0.9677419066429138)
[2025-01-06 01:49:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:45][root][INFO] - Training Epoch: 9/10, step 256/574 completed (loss: 0.01896912418305874, acc: 1.0)
[2025-01-06 01:49:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:46][root][INFO] - Training Epoch: 9/10, step 257/574 completed (loss: 0.012288237921893597, acc: 1.0)
[2025-01-06 01:49:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:46][root][INFO] - Training Epoch: 9/10, step 258/574 completed (loss: 0.0014467529254034162, acc: 1.0)
[2025-01-06 01:49:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:47][root][INFO] - Training Epoch: 9/10, step 259/574 completed (loss: 0.07832694798707962, acc: 0.9905660152435303)
[2025-01-06 01:49:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:47][root][INFO] - Training Epoch: 9/10, step 260/574 completed (loss: 0.07237011939287186, acc: 0.9916666746139526)
[2025-01-06 01:49:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:48][root][INFO] - Training Epoch: 9/10, step 261/574 completed (loss: 0.0136287622153759, acc: 1.0)
[2025-01-06 01:49:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:48][root][INFO] - Training Epoch: 9/10, step 262/574 completed (loss: 0.04214547947049141, acc: 1.0)
[2025-01-06 01:49:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:48][root][INFO] - Training Epoch: 9/10, step 263/574 completed (loss: 0.11067694425582886, acc: 0.9466666579246521)
[2025-01-06 01:49:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:49][root][INFO] - Training Epoch: 9/10, step 264/574 completed (loss: 0.27173686027526855, acc: 0.9583333134651184)
[2025-01-06 01:49:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:50][root][INFO] - Training Epoch: 9/10, step 265/574 completed (loss: 0.4203970432281494, acc: 0.8960000276565552)
[2025-01-06 01:49:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:50][root][INFO] - Training Epoch: 9/10, step 266/574 completed (loss: 0.10584487020969391, acc: 0.966292142868042)
[2025-01-06 01:49:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:50][root][INFO] - Training Epoch: 9/10, step 267/574 completed (loss: 0.23028641939163208, acc: 0.9459459185600281)
[2025-01-06 01:49:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:51][root][INFO] - Training Epoch: 9/10, step 268/574 completed (loss: 0.05569716542959213, acc: 1.0)
[2025-01-06 01:49:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:51][root][INFO] - Training Epoch: 9/10, step 269/574 completed (loss: 0.009891618974506855, acc: 1.0)
[2025-01-06 01:49:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:49:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:22][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.4981, device='cuda:0') eval_epoch_loss=tensor(0.9155, device='cuda:0') eval_epoch_acc=tensor(0.8340, device='cuda:0')
[2025-01-06 01:50:22][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:50:22][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:50:22][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_9_step_270_loss_0.9155341982841492/model.pt
[2025-01-06 01:50:22][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:50:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:22][root][INFO] - Training Epoch: 9/10, step 270/574 completed (loss: 0.010326758027076721, acc: 1.0)
[2025-01-06 01:50:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:23][root][INFO] - Training Epoch: 9/10, step 271/574 completed (loss: 0.0015931129455566406, acc: 1.0)
[2025-01-06 01:50:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:23][root][INFO] - Training Epoch: 9/10, step 272/574 completed (loss: 0.01843799464404583, acc: 1.0)
[2025-01-06 01:50:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:24][root][INFO] - Training Epoch: 9/10, step 273/574 completed (loss: 0.17333875596523285, acc: 0.9333333373069763)
[2025-01-06 01:50:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:24][root][INFO] - Training Epoch: 9/10, step 274/574 completed (loss: 0.010880783200263977, acc: 1.0)
[2025-01-06 01:50:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:24][root][INFO] - Training Epoch: 9/10, step 275/574 completed (loss: 0.09339175373315811, acc: 0.9666666388511658)
[2025-01-06 01:50:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:24][root][INFO] - Training Epoch: 9/10, step 276/574 completed (loss: 0.00569754047319293, acc: 1.0)
[2025-01-06 01:50:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:25][root][INFO] - Training Epoch: 9/10, step 277/574 completed (loss: 0.002490320708602667, acc: 1.0)
[2025-01-06 01:50:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:25][root][INFO] - Training Epoch: 9/10, step 278/574 completed (loss: 0.01962130516767502, acc: 1.0)
[2025-01-06 01:50:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:26][root][INFO] - Training Epoch: 9/10, step 279/574 completed (loss: 0.06325802952051163, acc: 0.9791666865348816)
[2025-01-06 01:50:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:26][root][INFO] - Training Epoch: 9/10, step 280/574 completed (loss: 0.0014208705397322774, acc: 1.0)
[2025-01-06 01:50:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:26][root][INFO] - Training Epoch: 9/10, step 281/574 completed (loss: 0.15091606974601746, acc: 0.9518072009086609)
[2025-01-06 01:50:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:27][root][INFO] - Training Epoch: 9/10, step 282/574 completed (loss: 0.10316883772611618, acc: 0.9722222089767456)
[2025-01-06 01:50:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:27][root][INFO] - Training Epoch: 9/10, step 283/574 completed (loss: 0.003362490562722087, acc: 1.0)
[2025-01-06 01:50:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:27][root][INFO] - Training Epoch: 9/10, step 284/574 completed (loss: 0.01142089907079935, acc: 1.0)
[2025-01-06 01:50:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:28][root][INFO] - Training Epoch: 9/10, step 285/574 completed (loss: 0.011951452121138573, acc: 1.0)
[2025-01-06 01:50:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:28][root][INFO] - Training Epoch: 9/10, step 286/574 completed (loss: 0.029136260971426964, acc: 0.9921875)
[2025-01-06 01:50:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:28][root][INFO] - Training Epoch: 9/10, step 287/574 completed (loss: 0.04674956202507019, acc: 0.984000027179718)
[2025-01-06 01:50:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:29][root][INFO] - Training Epoch: 9/10, step 288/574 completed (loss: 0.010808857157826424, acc: 1.0)
[2025-01-06 01:50:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:29][root][INFO] - Training Epoch: 9/10, step 289/574 completed (loss: 0.05178489908576012, acc: 0.9813664555549622)
[2025-01-06 01:50:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:30][root][INFO] - Training Epoch: 9/10, step 290/574 completed (loss: 0.05572180822491646, acc: 0.9793814420700073)
[2025-01-06 01:50:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:30][root][INFO] - Training Epoch: 9/10, step 291/574 completed (loss: 0.248528391122818, acc: 0.9545454382896423)
[2025-01-06 01:50:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:30][root][INFO] - Training Epoch: 9/10, step 292/574 completed (loss: 0.028461305424571037, acc: 0.976190447807312)
[2025-01-06 01:50:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:31][root][INFO] - Training Epoch: 9/10, step 293/574 completed (loss: 0.037491366267204285, acc: 0.9655172228813171)
[2025-01-06 01:50:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:31][root][INFO] - Training Epoch: 9/10, step 294/574 completed (loss: 0.03433050960302353, acc: 0.9818181991577148)
[2025-01-06 01:50:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:32][root][INFO] - Training Epoch: 9/10, step 295/574 completed (loss: 0.09349681437015533, acc: 0.9639175534248352)
[2025-01-06 01:50:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:32][root][INFO] - Training Epoch: 9/10, step 296/574 completed (loss: 0.0459492988884449, acc: 0.9655172228813171)
[2025-01-06 01:50:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:32][root][INFO] - Training Epoch: 9/10, step 297/574 completed (loss: 0.005638149566948414, acc: 1.0)
[2025-01-06 01:50:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:33][root][INFO] - Training Epoch: 9/10, step 298/574 completed (loss: 0.024521617218852043, acc: 1.0)
[2025-01-06 01:50:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:33][root][INFO] - Training Epoch: 9/10, step 299/574 completed (loss: 0.0027771475724875927, acc: 1.0)
[2025-01-06 01:50:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:33][root][INFO] - Training Epoch: 9/10, step 300/574 completed (loss: 0.06959494948387146, acc: 0.96875)
[2025-01-06 01:50:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:34][root][INFO] - Training Epoch: 9/10, step 301/574 completed (loss: 0.001992092002183199, acc: 1.0)
[2025-01-06 01:50:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:34][root][INFO] - Training Epoch: 9/10, step 302/574 completed (loss: 0.00046529454994015396, acc: 1.0)
[2025-01-06 01:50:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:34][root][INFO] - Training Epoch: 9/10, step 303/574 completed (loss: 0.009521961212158203, acc: 1.0)
[2025-01-06 01:50:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:35][root][INFO] - Training Epoch: 9/10, step 304/574 completed (loss: 0.0022263091523200274, acc: 1.0)
[2025-01-06 01:50:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:35][root][INFO] - Training Epoch: 9/10, step 305/574 completed (loss: 0.08956222981214523, acc: 0.9836065769195557)
[2025-01-06 01:50:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:35][root][INFO] - Training Epoch: 9/10, step 306/574 completed (loss: 0.0017444471595808864, acc: 1.0)
[2025-01-06 01:50:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:36][root][INFO] - Training Epoch: 9/10, step 307/574 completed (loss: 0.0002545354072935879, acc: 1.0)
[2025-01-06 01:50:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:36][root][INFO] - Training Epoch: 9/10, step 308/574 completed (loss: 0.03178253397345543, acc: 0.9710144996643066)
[2025-01-06 01:50:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:36][root][INFO] - Training Epoch: 9/10, step 309/574 completed (loss: 0.01219281367957592, acc: 1.0)
[2025-01-06 01:50:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:37][root][INFO] - Training Epoch: 9/10, step 310/574 completed (loss: 0.009872376918792725, acc: 1.0)
[2025-01-06 01:50:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:37][root][INFO] - Training Epoch: 9/10, step 311/574 completed (loss: 0.03902784362435341, acc: 0.9743589758872986)
[2025-01-06 01:50:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:38][root][INFO] - Training Epoch: 9/10, step 312/574 completed (loss: 0.006573486141860485, acc: 1.0)
[2025-01-06 01:50:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:38][root][INFO] - Training Epoch: 9/10, step 313/574 completed (loss: 0.017368502914905548, acc: 1.0)
[2025-01-06 01:50:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:38][root][INFO] - Training Epoch: 9/10, step 314/574 completed (loss: 0.0030985279008746147, acc: 1.0)
[2025-01-06 01:50:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:39][root][INFO] - Training Epoch: 9/10, step 315/574 completed (loss: 0.016281310468912125, acc: 1.0)
[2025-01-06 01:50:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:39][root][INFO] - Training Epoch: 9/10, step 316/574 completed (loss: 0.18520548939704895, acc: 0.9354838728904724)
[2025-01-06 01:50:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:39][root][INFO] - Training Epoch: 9/10, step 317/574 completed (loss: 0.23607897758483887, acc: 0.9104477763175964)
[2025-01-06 01:50:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:40][root][INFO] - Training Epoch: 9/10, step 318/574 completed (loss: 0.1401982456445694, acc: 0.9711538553237915)
[2025-01-06 01:50:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:40][root][INFO] - Training Epoch: 9/10, step 319/574 completed (loss: 0.03550601378083229, acc: 0.9777777791023254)
[2025-01-06 01:50:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:40][root][INFO] - Training Epoch: 9/10, step 320/574 completed (loss: 0.01687837392091751, acc: 1.0)
[2025-01-06 01:50:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:41][root][INFO] - Training Epoch: 9/10, step 321/574 completed (loss: 0.0017366845859214664, acc: 1.0)
[2025-01-06 01:50:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:41][root][INFO] - Training Epoch: 9/10, step 322/574 completed (loss: 0.17824764549732208, acc: 0.8888888955116272)
[2025-01-06 01:50:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:41][root][INFO] - Training Epoch: 9/10, step 323/574 completed (loss: 0.07403294742107391, acc: 0.9714285731315613)
[2025-01-06 01:50:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:42][root][INFO] - Training Epoch: 9/10, step 324/574 completed (loss: 0.13043247163295746, acc: 0.9487179517745972)
[2025-01-06 01:50:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:42][root][INFO] - Training Epoch: 9/10, step 325/574 completed (loss: 0.3842356503009796, acc: 0.8780487775802612)
[2025-01-06 01:50:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:42][root][INFO] - Training Epoch: 9/10, step 326/574 completed (loss: 0.14911167323589325, acc: 0.9736841917037964)
[2025-01-06 01:50:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:43][root][INFO] - Training Epoch: 9/10, step 327/574 completed (loss: 0.011496328748762608, acc: 1.0)
[2025-01-06 01:50:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:43][root][INFO] - Training Epoch: 9/10, step 328/574 completed (loss: 0.019658701494336128, acc: 1.0)
[2025-01-06 01:50:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:43][root][INFO] - Training Epoch: 9/10, step 329/574 completed (loss: 0.005360701121389866, acc: 1.0)
[2025-01-06 01:50:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:44][root][INFO] - Training Epoch: 9/10, step 330/574 completed (loss: 0.0029788471292704344, acc: 1.0)
[2025-01-06 01:50:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:44][root][INFO] - Training Epoch: 9/10, step 331/574 completed (loss: 0.08397827297449112, acc: 0.9838709831237793)
[2025-01-06 01:50:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:44][root][INFO] - Training Epoch: 9/10, step 332/574 completed (loss: 0.015676258131861687, acc: 1.0)
[2025-01-06 01:50:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:45][root][INFO] - Training Epoch: 9/10, step 333/574 completed (loss: 0.025998204946517944, acc: 1.0)
[2025-01-06 01:50:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:45][root][INFO] - Training Epoch: 9/10, step 334/574 completed (loss: 0.0011264631757512689, acc: 1.0)
[2025-01-06 01:50:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:46][root][INFO] - Training Epoch: 9/10, step 335/574 completed (loss: 0.007060479838401079, acc: 1.0)
[2025-01-06 01:50:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:46][root][INFO] - Training Epoch: 9/10, step 336/574 completed (loss: 0.08061401546001434, acc: 0.9599999785423279)
[2025-01-06 01:50:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:46][root][INFO] - Training Epoch: 9/10, step 337/574 completed (loss: 0.06643623858690262, acc: 0.977011501789093)
[2025-01-06 01:50:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:47][root][INFO] - Training Epoch: 9/10, step 338/574 completed (loss: 0.12271087616682053, acc: 0.978723406791687)
[2025-01-06 01:50:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:47][root][INFO] - Training Epoch: 9/10, step 339/574 completed (loss: 0.0644574761390686, acc: 0.9879518151283264)
[2025-01-06 01:50:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:47][root][INFO] - Training Epoch: 9/10, step 340/574 completed (loss: 0.0005829180590808392, acc: 1.0)
[2025-01-06 01:50:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:48][root][INFO] - Training Epoch: 9/10, step 341/574 completed (loss: 0.021461430937051773, acc: 1.0)
[2025-01-06 01:50:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:48][root][INFO] - Training Epoch: 9/10, step 342/574 completed (loss: 0.021865282207727432, acc: 1.0)
[2025-01-06 01:50:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:48][root][INFO] - Training Epoch: 9/10, step 343/574 completed (loss: 0.10914088040590286, acc: 0.9811320900917053)
[2025-01-06 01:50:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:49][root][INFO] - Training Epoch: 9/10, step 344/574 completed (loss: 0.002125413855537772, acc: 1.0)
[2025-01-06 01:50:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:49][root][INFO] - Training Epoch: 9/10, step 345/574 completed (loss: 0.0064727533608675, acc: 1.0)
[2025-01-06 01:50:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:49][root][INFO] - Training Epoch: 9/10, step 346/574 completed (loss: 0.02870165929198265, acc: 0.9850746393203735)
[2025-01-06 01:50:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:50][root][INFO] - Training Epoch: 9/10, step 347/574 completed (loss: 0.005863653961569071, acc: 1.0)
[2025-01-06 01:50:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:50][root][INFO] - Training Epoch: 9/10, step 348/574 completed (loss: 0.0008121135178953409, acc: 1.0)
[2025-01-06 01:50:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:51][root][INFO] - Training Epoch: 9/10, step 349/574 completed (loss: 0.03684663027524948, acc: 0.9722222089767456)
[2025-01-06 01:50:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:51][root][INFO] - Training Epoch: 9/10, step 350/574 completed (loss: 0.045232173055410385, acc: 0.9767441749572754)
[2025-01-06 01:50:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:51][root][INFO] - Training Epoch: 9/10, step 351/574 completed (loss: 0.00206637941300869, acc: 1.0)
[2025-01-06 01:50:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:52][root][INFO] - Training Epoch: 9/10, step 352/574 completed (loss: 0.12640808522701263, acc: 0.9777777791023254)
[2025-01-06 01:50:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:52][root][INFO] - Training Epoch: 9/10, step 353/574 completed (loss: 0.010082605294883251, acc: 1.0)
[2025-01-06 01:50:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:52][root][INFO] - Training Epoch: 9/10, step 354/574 completed (loss: 0.11790502816438675, acc: 0.9615384340286255)
[2025-01-06 01:50:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:53][root][INFO] - Training Epoch: 9/10, step 355/574 completed (loss: 0.17691947519779205, acc: 0.9560439586639404)
[2025-01-06 01:50:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:53][root][INFO] - Training Epoch: 9/10, step 356/574 completed (loss: 0.05921531096100807, acc: 0.9739130139350891)
[2025-01-06 01:50:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:54][root][INFO] - Training Epoch: 9/10, step 357/574 completed (loss: 0.08992274850606918, acc: 0.967391312122345)
[2025-01-06 01:50:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:54][root][INFO] - Training Epoch: 9/10, step 358/574 completed (loss: 0.008530538529157639, acc: 1.0)
[2025-01-06 01:50:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:54][root][INFO] - Training Epoch: 9/10, step 359/574 completed (loss: 0.00020606211910489947, acc: 1.0)
[2025-01-06 01:50:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:55][root][INFO] - Training Epoch: 9/10, step 360/574 completed (loss: 0.010887705720961094, acc: 1.0)
[2025-01-06 01:50:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:55][root][INFO] - Training Epoch: 9/10, step 361/574 completed (loss: 0.06500312685966492, acc: 0.9756097793579102)
[2025-01-06 01:50:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:55][root][INFO] - Training Epoch: 9/10, step 362/574 completed (loss: 0.03434500843286514, acc: 0.9777777791023254)
[2025-01-06 01:50:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:56][root][INFO] - Training Epoch: 9/10, step 363/574 completed (loss: 0.003998120781034231, acc: 1.0)
[2025-01-06 01:50:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:56][root][INFO] - Training Epoch: 9/10, step 364/574 completed (loss: 0.006800409406423569, acc: 1.0)
[2025-01-06 01:50:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:56][root][INFO] - Training Epoch: 9/10, step 365/574 completed (loss: 0.006526831537485123, acc: 1.0)
[2025-01-06 01:50:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:57][root][INFO] - Training Epoch: 9/10, step 366/574 completed (loss: 0.0017060822574421763, acc: 1.0)
[2025-01-06 01:50:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:57][root][INFO] - Training Epoch: 9/10, step 367/574 completed (loss: 0.0013440345646813512, acc: 1.0)
[2025-01-06 01:50:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:57][root][INFO] - Training Epoch: 9/10, step 368/574 completed (loss: 0.10097811371088028, acc: 0.9642857313156128)
[2025-01-06 01:50:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:58][root][INFO] - Training Epoch: 9/10, step 369/574 completed (loss: 0.010967168025672436, acc: 1.0)
[2025-01-06 01:50:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:58][root][INFO] - Training Epoch: 9/10, step 370/574 completed (loss: 0.10626079887151718, acc: 0.9636363387107849)
[2025-01-06 01:50:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:50:59][root][INFO] - Training Epoch: 9/10, step 371/574 completed (loss: 0.027263309806585312, acc: 0.9905660152435303)
[2025-01-06 01:50:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:00][root][INFO] - Training Epoch: 9/10, step 372/574 completed (loss: 0.017939170822501183, acc: 0.9888888597488403)
[2025-01-06 01:51:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:00][root][INFO] - Training Epoch: 9/10, step 373/574 completed (loss: 0.008501706644892693, acc: 1.0)
[2025-01-06 01:51:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:00][root][INFO] - Training Epoch: 9/10, step 374/574 completed (loss: 0.0029728214722126722, acc: 1.0)
[2025-01-06 01:51:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:01][root][INFO] - Training Epoch: 9/10, step 375/574 completed (loss: 0.0003904554178006947, acc: 1.0)
[2025-01-06 01:51:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:01][root][INFO] - Training Epoch: 9/10, step 376/574 completed (loss: 0.0004752415989059955, acc: 1.0)
[2025-01-06 01:51:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:01][root][INFO] - Training Epoch: 9/10, step 377/574 completed (loss: 0.018742645159363747, acc: 1.0)
[2025-01-06 01:51:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:02][root][INFO] - Training Epoch: 9/10, step 378/574 completed (loss: 0.0069579784758389, acc: 1.0)
[2025-01-06 01:51:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:02][root][INFO] - Training Epoch: 9/10, step 379/574 completed (loss: 0.05257461592555046, acc: 0.976047933101654)
[2025-01-06 01:51:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:03][root][INFO] - Training Epoch: 9/10, step 380/574 completed (loss: 0.02185218408703804, acc: 0.9924812316894531)
[2025-01-06 01:51:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:04][root][INFO] - Training Epoch: 9/10, step 381/574 completed (loss: 0.12718355655670166, acc: 0.9465240836143494)
[2025-01-06 01:51:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:04][root][INFO] - Training Epoch: 9/10, step 382/574 completed (loss: 0.005442493595182896, acc: 1.0)
[2025-01-06 01:51:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:05][root][INFO] - Training Epoch: 9/10, step 383/574 completed (loss: 0.0031544228550046682, acc: 1.0)
[2025-01-06 01:51:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:05][root][INFO] - Training Epoch: 9/10, step 384/574 completed (loss: 0.00034555987804196775, acc: 1.0)
[2025-01-06 01:51:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:05][root][INFO] - Training Epoch: 9/10, step 385/574 completed (loss: 0.018996749073266983, acc: 1.0)
[2025-01-06 01:51:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:06][root][INFO] - Training Epoch: 9/10, step 386/574 completed (loss: 0.000494284147862345, acc: 1.0)
[2025-01-06 01:51:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:06][root][INFO] - Training Epoch: 9/10, step 387/574 completed (loss: 0.018543265759944916, acc: 1.0)
[2025-01-06 01:51:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:07][root][INFO] - Training Epoch: 9/10, step 388/574 completed (loss: 0.0005714903236366808, acc: 1.0)
[2025-01-06 01:51:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:07][root][INFO] - Training Epoch: 9/10, step 389/574 completed (loss: 0.0006836258107796311, acc: 1.0)
[2025-01-06 01:51:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:07][root][INFO] - Training Epoch: 9/10, step 390/574 completed (loss: 0.3447816073894501, acc: 0.9523809552192688)
[2025-01-06 01:51:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:08][root][INFO] - Training Epoch: 9/10, step 391/574 completed (loss: 0.19986121356487274, acc: 0.9259259104728699)
[2025-01-06 01:51:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:08][root][INFO] - Training Epoch: 9/10, step 392/574 completed (loss: 0.11662063747644424, acc: 0.9611650705337524)
[2025-01-06 01:51:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:08][root][INFO] - Training Epoch: 9/10, step 393/574 completed (loss: 0.11298532783985138, acc: 0.9632353186607361)
[2025-01-06 01:51:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:09][root][INFO] - Training Epoch: 9/10, step 394/574 completed (loss: 0.192502960562706, acc: 0.9466666579246521)
[2025-01-06 01:51:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:09][root][INFO] - Training Epoch: 9/10, step 395/574 completed (loss: 0.07061588019132614, acc: 0.9652777910232544)
[2025-01-06 01:51:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:10][root][INFO] - Training Epoch: 9/10, step 396/574 completed (loss: 0.3974442481994629, acc: 0.9767441749572754)
[2025-01-06 01:51:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:10][root][INFO] - Training Epoch: 9/10, step 397/574 completed (loss: 0.0017474801279604435, acc: 1.0)
[2025-01-06 01:51:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:10][root][INFO] - Training Epoch: 9/10, step 398/574 completed (loss: 0.012765713967382908, acc: 1.0)
[2025-01-06 01:51:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:11][root][INFO] - Training Epoch: 9/10, step 399/574 completed (loss: 0.013714662753045559, acc: 1.0)
[2025-01-06 01:51:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:11][root][INFO] - Training Epoch: 9/10, step 400/574 completed (loss: 0.04137549176812172, acc: 0.970588207244873)
[2025-01-06 01:51:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:12][root][INFO] - Training Epoch: 9/10, step 401/574 completed (loss: 0.03392118215560913, acc: 0.9733333587646484)
[2025-01-06 01:51:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:12][root][INFO] - Training Epoch: 9/10, step 402/574 completed (loss: 0.003982766531407833, acc: 1.0)
[2025-01-06 01:51:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:12][root][INFO] - Training Epoch: 9/10, step 403/574 completed (loss: 0.07719550281763077, acc: 0.9696969985961914)
[2025-01-06 01:51:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:13][root][INFO] - Training Epoch: 9/10, step 404/574 completed (loss: 0.10423976927995682, acc: 0.9677419066429138)
[2025-01-06 01:51:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:13][root][INFO] - Training Epoch: 9/10, step 405/574 completed (loss: 0.0005437078652903438, acc: 1.0)
[2025-01-06 01:51:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:13][root][INFO] - Training Epoch: 9/10, step 406/574 completed (loss: 0.003194563090801239, acc: 1.0)
[2025-01-06 01:51:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:14][root][INFO] - Training Epoch: 9/10, step 407/574 completed (loss: 0.005290383938699961, acc: 1.0)
[2025-01-06 01:51:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:14][root][INFO] - Training Epoch: 9/10, step 408/574 completed (loss: 0.0028487632516771555, acc: 1.0)
[2025-01-06 01:51:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:15][root][INFO] - Training Epoch: 9/10, step 409/574 completed (loss: 0.07869850099086761, acc: 0.9615384340286255)
[2025-01-06 01:51:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:15][root][INFO] - Training Epoch: 9/10, step 410/574 completed (loss: 0.003583957441151142, acc: 1.0)
[2025-01-06 01:51:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:15][root][INFO] - Training Epoch: 9/10, step 411/574 completed (loss: 0.29534393548965454, acc: 0.9642857313156128)
[2025-01-06 01:51:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:15][root][INFO] - Training Epoch: 9/10, step 412/574 completed (loss: 0.0004478724440559745, acc: 1.0)
[2025-01-06 01:51:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:46][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.3304, device='cuda:0') eval_epoch_loss=tensor(0.8460, device='cuda:0') eval_epoch_acc=tensor(0.8449, device='cuda:0')
[2025-01-06 01:51:46][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:51:46][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:51:46][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_9_step_413_loss_0.8460263609886169/model.pt
[2025-01-06 01:51:46][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:51:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:47][root][INFO] - Training Epoch: 9/10, step 413/574 completed (loss: 0.00250973179936409, acc: 1.0)
[2025-01-06 01:51:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:47][root][INFO] - Training Epoch: 9/10, step 414/574 completed (loss: 0.00626661442220211, acc: 1.0)
[2025-01-06 01:51:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:48][root][INFO] - Training Epoch: 9/10, step 415/574 completed (loss: 0.01820741966366768, acc: 1.0)
[2025-01-06 01:51:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:48][root][INFO] - Training Epoch: 9/10, step 416/574 completed (loss: 0.08899358659982681, acc: 0.9230769276618958)
[2025-01-06 01:51:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:48][root][INFO] - Training Epoch: 9/10, step 417/574 completed (loss: 0.009173272177577019, acc: 1.0)
[2025-01-06 01:51:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:49][root][INFO] - Training Epoch: 9/10, step 418/574 completed (loss: 0.0024548815563321114, acc: 1.0)
[2025-01-06 01:51:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:49][root][INFO] - Training Epoch: 9/10, step 419/574 completed (loss: 0.014353032223880291, acc: 1.0)
[2025-01-06 01:51:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:49][root][INFO] - Training Epoch: 9/10, step 420/574 completed (loss: 0.009961375966668129, acc: 1.0)
[2025-01-06 01:51:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:50][root][INFO] - Training Epoch: 9/10, step 421/574 completed (loss: 0.0049168989062309265, acc: 1.0)
[2025-01-06 01:51:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:50][root][INFO] - Training Epoch: 9/10, step 422/574 completed (loss: 0.004082581959664822, acc: 1.0)
[2025-01-06 01:51:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:50][root][INFO] - Training Epoch: 9/10, step 423/574 completed (loss: 0.07600724697113037, acc: 0.9444444179534912)
[2025-01-06 01:51:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:51][root][INFO] - Training Epoch: 9/10, step 424/574 completed (loss: 0.0009400390554219484, acc: 1.0)
[2025-01-06 01:51:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:51][root][INFO] - Training Epoch: 9/10, step 425/574 completed (loss: 0.0034864379558712244, acc: 1.0)
[2025-01-06 01:51:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:51][root][INFO] - Training Epoch: 9/10, step 426/574 completed (loss: 0.017112767323851585, acc: 1.0)
[2025-01-06 01:51:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:52][root][INFO] - Training Epoch: 9/10, step 427/574 completed (loss: 0.0459129698574543, acc: 0.9729729890823364)
[2025-01-06 01:51:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:52][root][INFO] - Training Epoch: 9/10, step 428/574 completed (loss: 0.002547015668824315, acc: 1.0)
[2025-01-06 01:51:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:52][root][INFO] - Training Epoch: 9/10, step 429/574 completed (loss: 0.022489655762910843, acc: 1.0)
[2025-01-06 01:51:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:53][root][INFO] - Training Epoch: 9/10, step 430/574 completed (loss: 0.0003312470798846334, acc: 1.0)
[2025-01-06 01:51:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:53][root][INFO] - Training Epoch: 9/10, step 431/574 completed (loss: 8.788981358520687e-05, acc: 1.0)
[2025-01-06 01:51:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:54][root][INFO] - Training Epoch: 9/10, step 432/574 completed (loss: 0.0038030443247407675, acc: 1.0)
[2025-01-06 01:51:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:54][root][INFO] - Training Epoch: 9/10, step 433/574 completed (loss: 0.019142307341098785, acc: 1.0)
[2025-01-06 01:51:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:54][root][INFO] - Training Epoch: 9/10, step 434/574 completed (loss: 0.004406125750392675, acc: 1.0)
[2025-01-06 01:51:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:55][root][INFO] - Training Epoch: 9/10, step 435/574 completed (loss: 0.0015329165617004037, acc: 1.0)
[2025-01-06 01:51:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:55][root][INFO] - Training Epoch: 9/10, step 436/574 completed (loss: 0.010952846147119999, acc: 1.0)
[2025-01-06 01:51:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:55][root][INFO] - Training Epoch: 9/10, step 437/574 completed (loss: 0.0007369770319201052, acc: 1.0)
[2025-01-06 01:51:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:56][root][INFO] - Training Epoch: 9/10, step 438/574 completed (loss: 0.0001789806119631976, acc: 1.0)
[2025-01-06 01:51:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:56][root][INFO] - Training Epoch: 9/10, step 439/574 completed (loss: 0.0020729689858853817, acc: 1.0)
[2025-01-06 01:51:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:57][root][INFO] - Training Epoch: 9/10, step 440/574 completed (loss: 0.005208233837038279, acc: 1.0)
[2025-01-06 01:51:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:57][root][INFO] - Training Epoch: 9/10, step 441/574 completed (loss: 0.10617008805274963, acc: 0.9520000219345093)
[2025-01-06 01:51:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:58][root][INFO] - Training Epoch: 9/10, step 442/574 completed (loss: 0.06368493288755417, acc: 0.9838709831237793)
[2025-01-06 01:51:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:58][root][INFO] - Training Epoch: 9/10, step 443/574 completed (loss: 0.15800981223583221, acc: 0.9552238583564758)
[2025-01-06 01:51:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:59][root][INFO] - Training Epoch: 9/10, step 444/574 completed (loss: 0.03765947371721268, acc: 0.9811320900917053)
[2025-01-06 01:51:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:59][root][INFO] - Training Epoch: 9/10, step 445/574 completed (loss: 0.0021387210581451654, acc: 1.0)
[2025-01-06 01:51:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:51:59][root][INFO] - Training Epoch: 9/10, step 446/574 completed (loss: 0.0027819043025374413, acc: 1.0)
[2025-01-06 01:52:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:00][root][INFO] - Training Epoch: 9/10, step 447/574 completed (loss: 0.003916459158062935, acc: 1.0)
[2025-01-06 01:52:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:00][root][INFO] - Training Epoch: 9/10, step 448/574 completed (loss: 0.0012903802562505007, acc: 1.0)
[2025-01-06 01:52:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:01][root][INFO] - Training Epoch: 9/10, step 449/574 completed (loss: 0.0031821876764297485, acc: 1.0)
[2025-01-06 01:52:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:01][root][INFO] - Training Epoch: 9/10, step 450/574 completed (loss: 0.0422988086938858, acc: 0.9722222089767456)
[2025-01-06 01:52:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:01][root][INFO] - Training Epoch: 9/10, step 451/574 completed (loss: 0.0017571481876075268, acc: 1.0)
[2025-01-06 01:52:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:02][root][INFO] - Training Epoch: 9/10, step 452/574 completed (loss: 0.0015799780376255512, acc: 1.0)
[2025-01-06 01:52:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:02][root][INFO] - Training Epoch: 9/10, step 453/574 completed (loss: 0.03156723454594612, acc: 0.9868420958518982)
[2025-01-06 01:52:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:02][root][INFO] - Training Epoch: 9/10, step 454/574 completed (loss: 0.011533940210938454, acc: 1.0)
[2025-01-06 01:52:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:03][root][INFO] - Training Epoch: 9/10, step 455/574 completed (loss: 0.004655405413359404, acc: 1.0)
[2025-01-06 01:52:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:03][root][INFO] - Training Epoch: 9/10, step 456/574 completed (loss: 0.006438530050218105, acc: 1.0)
[2025-01-06 01:52:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:03][root][INFO] - Training Epoch: 9/10, step 457/574 completed (loss: 0.004082856234163046, acc: 1.0)
[2025-01-06 01:52:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:04][root][INFO] - Training Epoch: 9/10, step 458/574 completed (loss: 0.03530976176261902, acc: 0.9883720874786377)
[2025-01-06 01:52:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:04][root][INFO] - Training Epoch: 9/10, step 459/574 completed (loss: 0.027403151616454124, acc: 0.9821428656578064)
[2025-01-06 01:52:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:04][root][INFO] - Training Epoch: 9/10, step 460/574 completed (loss: 0.0035880974028259516, acc: 1.0)
[2025-01-06 01:52:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:05][root][INFO] - Training Epoch: 9/10, step 461/574 completed (loss: 0.0006939673912711442, acc: 1.0)
[2025-01-06 01:52:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:05][root][INFO] - Training Epoch: 9/10, step 462/574 completed (loss: 0.0008344416273757815, acc: 1.0)
[2025-01-06 01:52:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:06][root][INFO] - Training Epoch: 9/10, step 463/574 completed (loss: 0.0008532719220966101, acc: 1.0)
[2025-01-06 01:52:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:06][root][INFO] - Training Epoch: 9/10, step 464/574 completed (loss: 0.001881068223156035, acc: 1.0)
[2025-01-06 01:52:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:06][root][INFO] - Training Epoch: 9/10, step 465/574 completed (loss: 0.017972620204091072, acc: 0.988095223903656)
[2025-01-06 01:52:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:07][root][INFO] - Training Epoch: 9/10, step 466/574 completed (loss: 0.1626819521188736, acc: 0.9518072009086609)
[2025-01-06 01:52:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:07][root][INFO] - Training Epoch: 9/10, step 467/574 completed (loss: 0.008479774929583073, acc: 1.0)
[2025-01-06 01:52:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:07][root][INFO] - Training Epoch: 9/10, step 468/574 completed (loss: 0.0855918824672699, acc: 0.9902912378311157)
[2025-01-06 01:52:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:08][root][INFO] - Training Epoch: 9/10, step 469/574 completed (loss: 0.058916885405778885, acc: 0.9674796462059021)
[2025-01-06 01:52:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:08][root][INFO] - Training Epoch: 9/10, step 470/574 completed (loss: 0.0016266020247712731, acc: 1.0)
[2025-01-06 01:52:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:08][root][INFO] - Training Epoch: 9/10, step 471/574 completed (loss: 0.0017433026805520058, acc: 1.0)
[2025-01-06 01:52:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:09][root][INFO] - Training Epoch: 9/10, step 472/574 completed (loss: 0.07473568618297577, acc: 0.9803921580314636)
[2025-01-06 01:52:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:09][root][INFO] - Training Epoch: 9/10, step 473/574 completed (loss: 0.11543567478656769, acc: 0.9650654792785645)
[2025-01-06 01:52:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:09][root][INFO] - Training Epoch: 9/10, step 474/574 completed (loss: 0.03463837131857872, acc: 0.9895833134651184)
[2025-01-06 01:52:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:10][root][INFO] - Training Epoch: 9/10, step 475/574 completed (loss: 0.0333399772644043, acc: 0.987730085849762)
[2025-01-06 01:52:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:10][root][INFO] - Training Epoch: 9/10, step 476/574 completed (loss: 0.03303827345371246, acc: 0.9928057789802551)
[2025-01-06 01:52:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:11][root][INFO] - Training Epoch: 9/10, step 477/574 completed (loss: 0.11573300510644913, acc: 0.9648241400718689)
[2025-01-06 01:52:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:11][root][INFO] - Training Epoch: 9/10, step 478/574 completed (loss: 0.01099095493555069, acc: 1.0)
[2025-01-06 01:52:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:11][root][INFO] - Training Epoch: 9/10, step 479/574 completed (loss: 0.00919348280876875, acc: 1.0)
[2025-01-06 01:52:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:12][root][INFO] - Training Epoch: 9/10, step 480/574 completed (loss: 0.005142726469784975, acc: 1.0)
[2025-01-06 01:52:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:12][root][INFO] - Training Epoch: 9/10, step 481/574 completed (loss: 0.03032848611474037, acc: 1.0)
[2025-01-06 01:52:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:12][root][INFO] - Training Epoch: 9/10, step 482/574 completed (loss: 0.023172203451395035, acc: 1.0)
[2025-01-06 01:52:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:13][root][INFO] - Training Epoch: 9/10, step 483/574 completed (loss: 0.03194300830364227, acc: 0.982758641242981)
[2025-01-06 01:52:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:13][root][INFO] - Training Epoch: 9/10, step 484/574 completed (loss: 0.001785099389962852, acc: 1.0)
[2025-01-06 01:52:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:13][root][INFO] - Training Epoch: 9/10, step 485/574 completed (loss: 0.0025258404202759266, acc: 1.0)
[2025-01-06 01:52:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:14][root][INFO] - Training Epoch: 9/10, step 486/574 completed (loss: 0.024011535570025444, acc: 1.0)
[2025-01-06 01:52:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:14][root][INFO] - Training Epoch: 9/10, step 487/574 completed (loss: 0.02780253253877163, acc: 1.0)
[2025-01-06 01:52:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:15][root][INFO] - Training Epoch: 9/10, step 488/574 completed (loss: 0.028139593079686165, acc: 1.0)
[2025-01-06 01:52:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:15][root][INFO] - Training Epoch: 9/10, step 489/574 completed (loss: 0.033337462693452835, acc: 0.9846153855323792)
[2025-01-06 01:52:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:15][root][INFO] - Training Epoch: 9/10, step 490/574 completed (loss: 0.012978744693100452, acc: 1.0)
[2025-01-06 01:52:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:16][root][INFO] - Training Epoch: 9/10, step 491/574 completed (loss: 0.0012850489001721144, acc: 1.0)
[2025-01-06 01:52:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:16][root][INFO] - Training Epoch: 9/10, step 492/574 completed (loss: 0.09483310580253601, acc: 0.9411764740943909)
[2025-01-06 01:52:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:16][root][INFO] - Training Epoch: 9/10, step 493/574 completed (loss: 0.0012598385801538825, acc: 1.0)
[2025-01-06 01:52:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:17][root][INFO] - Training Epoch: 9/10, step 494/574 completed (loss: 0.011718346737325191, acc: 1.0)
[2025-01-06 01:52:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:17][root][INFO] - Training Epoch: 9/10, step 495/574 completed (loss: 0.05176949501037598, acc: 1.0)
[2025-01-06 01:52:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:17][root][INFO] - Training Epoch: 9/10, step 496/574 completed (loss: 0.14046761393547058, acc: 0.9642857313156128)
[2025-01-06 01:52:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:18][root][INFO] - Training Epoch: 9/10, step 497/574 completed (loss: 0.03053954988718033, acc: 0.9887640476226807)
[2025-01-06 01:52:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:18][root][INFO] - Training Epoch: 9/10, step 498/574 completed (loss: 0.08758804947137833, acc: 0.9775280952453613)
[2025-01-06 01:52:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:18][root][INFO] - Training Epoch: 9/10, step 499/574 completed (loss: 0.1098250076174736, acc: 0.9716312289237976)
[2025-01-06 01:52:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:19][root][INFO] - Training Epoch: 9/10, step 500/574 completed (loss: 0.01788531243801117, acc: 1.0)
[2025-01-06 01:52:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:19][root][INFO] - Training Epoch: 9/10, step 501/574 completed (loss: 0.00028149803983978927, acc: 1.0)
[2025-01-06 01:52:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:19][root][INFO] - Training Epoch: 9/10, step 502/574 completed (loss: 0.0002384159161010757, acc: 1.0)
[2025-01-06 01:52:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:20][root][INFO] - Training Epoch: 9/10, step 503/574 completed (loss: 0.09372380375862122, acc: 0.9629629850387573)
[2025-01-06 01:52:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:20][root][INFO] - Training Epoch: 9/10, step 504/574 completed (loss: 0.00418301485478878, acc: 1.0)
[2025-01-06 01:52:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:20][root][INFO] - Training Epoch: 9/10, step 505/574 completed (loss: 0.11824899911880493, acc: 0.9433962106704712)
[2025-01-06 01:52:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:21][root][INFO] - Training Epoch: 9/10, step 506/574 completed (loss: 0.013553301803767681, acc: 1.0)
[2025-01-06 01:52:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:21][root][INFO] - Training Epoch: 9/10, step 507/574 completed (loss: 0.13658836483955383, acc: 0.954954981803894)
[2025-01-06 01:52:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:22][root][INFO] - Training Epoch: 9/10, step 508/574 completed (loss: 0.058141715824604034, acc: 0.98591548204422)
[2025-01-06 01:52:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:22][root][INFO] - Training Epoch: 9/10, step 509/574 completed (loss: 0.001432062708772719, acc: 1.0)
[2025-01-06 01:52:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:23][root][INFO] - Training Epoch: 9/10, step 510/574 completed (loss: 0.02270282432436943, acc: 1.0)
[2025-01-06 01:52:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:23][root][INFO] - Training Epoch: 9/10, step 511/574 completed (loss: 0.0006599128246307373, acc: 1.0)
[2025-01-06 01:52:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:27][root][INFO] - Training Epoch: 9/10, step 512/574 completed (loss: 0.11496125906705856, acc: 0.9714285731315613)
[2025-01-06 01:52:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:27][root][INFO] - Training Epoch: 9/10, step 513/574 completed (loss: 0.007418385706841946, acc: 1.0)
[2025-01-06 01:52:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:28][root][INFO] - Training Epoch: 9/10, step 514/574 completed (loss: 0.005284321028739214, acc: 1.0)
[2025-01-06 01:52:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:28][root][INFO] - Training Epoch: 9/10, step 515/574 completed (loss: 0.08918964117765427, acc: 0.9833333492279053)
[2025-01-06 01:52:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:29][root][INFO] - Training Epoch: 9/10, step 516/574 completed (loss: 0.018289577215909958, acc: 1.0)
[2025-01-06 01:52:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:29][root][INFO] - Training Epoch: 9/10, step 517/574 completed (loss: 0.00012538990995381027, acc: 1.0)
[2025-01-06 01:52:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:29][root][INFO] - Training Epoch: 9/10, step 518/574 completed (loss: 0.0049879723228514194, acc: 1.0)
[2025-01-06 01:52:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:30][root][INFO] - Training Epoch: 9/10, step 519/574 completed (loss: 0.0009503521141596138, acc: 1.0)
[2025-01-06 01:52:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:30][root][INFO] - Training Epoch: 9/10, step 520/574 completed (loss: 0.003591901157051325, acc: 1.0)
[2025-01-06 01:52:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:31][root][INFO] - Training Epoch: 9/10, step 521/574 completed (loss: 0.1321638971567154, acc: 0.9533898234367371)
[2025-01-06 01:52:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:31][root][INFO] - Training Epoch: 9/10, step 522/574 completed (loss: 0.007177669554948807, acc: 1.0)
[2025-01-06 01:52:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:32][root][INFO] - Training Epoch: 9/10, step 523/574 completed (loss: 0.027626268565654755, acc: 0.985401451587677)
[2025-01-06 01:52:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:32][root][INFO] - Training Epoch: 9/10, step 524/574 completed (loss: 0.09318351745605469, acc: 0.9700000286102295)
[2025-01-06 01:52:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:33][root][INFO] - Training Epoch: 9/10, step 525/574 completed (loss: 0.004458137787878513, acc: 1.0)
[2025-01-06 01:52:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:33][root][INFO] - Training Epoch: 9/10, step 526/574 completed (loss: 0.013070032000541687, acc: 1.0)
[2025-01-06 01:52:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:33][root][INFO] - Training Epoch: 9/10, step 527/574 completed (loss: 0.007107219658792019, acc: 1.0)
[2025-01-06 01:52:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:34][root][INFO] - Training Epoch: 9/10, step 528/574 completed (loss: 0.08168597519397736, acc: 0.9836065769195557)
[2025-01-06 01:52:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:34][root][INFO] - Training Epoch: 9/10, step 529/574 completed (loss: 0.012549731880426407, acc: 1.0)
[2025-01-06 01:52:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:34][root][INFO] - Training Epoch: 9/10, step 530/574 completed (loss: 0.20303168892860413, acc: 0.9767441749572754)
[2025-01-06 01:52:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:35][root][INFO] - Training Epoch: 9/10, step 531/574 completed (loss: 0.08595750480890274, acc: 0.9545454382896423)
[2025-01-06 01:52:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:35][root][INFO] - Training Epoch: 9/10, step 532/574 completed (loss: 0.019283683970570564, acc: 1.0)
[2025-01-06 01:52:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:35][root][INFO] - Training Epoch: 9/10, step 533/574 completed (loss: 0.06751937419176102, acc: 0.9545454382896423)
[2025-01-06 01:52:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:36][root][INFO] - Training Epoch: 9/10, step 534/574 completed (loss: 0.007493353448808193, acc: 1.0)
[2025-01-06 01:52:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:36][root][INFO] - Training Epoch: 9/10, step 535/574 completed (loss: 0.0007489125127904117, acc: 1.0)
[2025-01-06 01:52:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:36][root][INFO] - Training Epoch: 9/10, step 536/574 completed (loss: 0.00030663967481814325, acc: 1.0)
[2025-01-06 01:52:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:37][root][INFO] - Training Epoch: 9/10, step 537/574 completed (loss: 0.014830228872597218, acc: 1.0)
[2025-01-06 01:52:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:37][root][INFO] - Training Epoch: 9/10, step 538/574 completed (loss: 0.01878339797258377, acc: 1.0)
[2025-01-06 01:52:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:37][root][INFO] - Training Epoch: 9/10, step 539/574 completed (loss: 0.01192639023065567, acc: 1.0)
[2025-01-06 01:52:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:38][root][INFO] - Training Epoch: 9/10, step 540/574 completed (loss: 0.1116180270910263, acc: 0.9696969985961914)
[2025-01-06 01:52:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:38][root][INFO] - Training Epoch: 9/10, step 541/574 completed (loss: 0.0001885856909211725, acc: 1.0)
[2025-01-06 01:52:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:38][root][INFO] - Training Epoch: 9/10, step 542/574 completed (loss: 0.0013748055789619684, acc: 1.0)
[2025-01-06 01:52:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:39][root][INFO] - Training Epoch: 9/10, step 543/574 completed (loss: 7.966999692143872e-05, acc: 1.0)
[2025-01-06 01:52:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:39][root][INFO] - Training Epoch: 9/10, step 544/574 completed (loss: 0.002882526256144047, acc: 1.0)
[2025-01-06 01:52:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:40][root][INFO] - Training Epoch: 9/10, step 545/574 completed (loss: 0.0013112809974700212, acc: 1.0)
[2025-01-06 01:52:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:40][root][INFO] - Training Epoch: 9/10, step 546/574 completed (loss: 0.00044313955004327, acc: 1.0)
[2025-01-06 01:52:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:40][root][INFO] - Training Epoch: 9/10, step 547/574 completed (loss: 0.008569256402552128, acc: 1.0)
[2025-01-06 01:52:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:41][root][INFO] - Training Epoch: 9/10, step 548/574 completed (loss: 0.007790464907884598, acc: 1.0)
[2025-01-06 01:52:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:41][root][INFO] - Training Epoch: 9/10, step 549/574 completed (loss: 6.596343882847577e-05, acc: 1.0)
[2025-01-06 01:52:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:41][root][INFO] - Training Epoch: 9/10, step 550/574 completed (loss: 0.0003662233939394355, acc: 1.0)
[2025-01-06 01:52:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:42][root][INFO] - Training Epoch: 9/10, step 551/574 completed (loss: 0.009443074464797974, acc: 1.0)
[2025-01-06 01:52:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:42][root][INFO] - Training Epoch: 9/10, step 552/574 completed (loss: 0.03455333039164543, acc: 0.9857142567634583)
[2025-01-06 01:52:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:42][root][INFO] - Training Epoch: 9/10, step 553/574 completed (loss: 0.02605460211634636, acc: 0.9927007555961609)
[2025-01-06 01:52:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:43][root][INFO] - Training Epoch: 9/10, step 554/574 completed (loss: 0.005436011124402285, acc: 1.0)
[2025-01-06 01:52:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:43][root][INFO] - Training Epoch: 9/10, step 555/574 completed (loss: 0.04570702463388443, acc: 0.9928571581840515)
[2025-01-06 01:52:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:52:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:14][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.3443, device='cuda:0') eval_epoch_loss=tensor(0.8520, device='cuda:0') eval_epoch_acc=tensor(0.8467, device='cuda:0')
[2025-01-06 01:53:14][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:53:14][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:53:14][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_9_step_556_loss_0.8519677519798279/model.pt
[2025-01-06 01:53:14][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:53:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:14][root][INFO] - Training Epoch: 9/10, step 556/574 completed (loss: 0.041201863437891006, acc: 0.9867549538612366)
[2025-01-06 01:53:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:14][root][INFO] - Training Epoch: 9/10, step 557/574 completed (loss: 0.003367386292666197, acc: 1.0)
[2025-01-06 01:53:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:15][root][INFO] - Training Epoch: 9/10, step 558/574 completed (loss: 0.008254889398813248, acc: 1.0)
[2025-01-06 01:53:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:15][root][INFO] - Training Epoch: 9/10, step 559/574 completed (loss: 0.001680163200944662, acc: 1.0)
[2025-01-06 01:53:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:16][root][INFO] - Training Epoch: 9/10, step 560/574 completed (loss: 0.002552745398133993, acc: 1.0)
[2025-01-06 01:53:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:16][root][INFO] - Training Epoch: 9/10, step 561/574 completed (loss: 0.0008339118794538081, acc: 1.0)
[2025-01-06 01:53:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:16][root][INFO] - Training Epoch: 9/10, step 562/574 completed (loss: 0.0640798807144165, acc: 0.9888888597488403)
[2025-01-06 01:53:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:16][root][INFO] - Training Epoch: 9/10, step 563/574 completed (loss: 0.011230145581066608, acc: 1.0)
[2025-01-06 01:53:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:17][root][INFO] - Training Epoch: 9/10, step 564/574 completed (loss: 0.10989297181367874, acc: 0.9583333134651184)
[2025-01-06 01:53:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:17][root][INFO] - Training Epoch: 9/10, step 565/574 completed (loss: 0.0019118025666102767, acc: 1.0)
[2025-01-06 01:53:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:18][root][INFO] - Training Epoch: 9/10, step 566/574 completed (loss: 0.004154922440648079, acc: 1.0)
[2025-01-06 01:53:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:18][root][INFO] - Training Epoch: 9/10, step 567/574 completed (loss: 0.001556669594720006, acc: 1.0)
[2025-01-06 01:53:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:18][root][INFO] - Training Epoch: 9/10, step 568/574 completed (loss: 0.0007623964338563383, acc: 1.0)
[2025-01-06 01:53:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:19][root][INFO] - Training Epoch: 9/10, step 569/574 completed (loss: 0.010691424831748009, acc: 1.0)
[2025-01-06 01:53:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:19][root][INFO] - Training Epoch: 9/10, step 570/574 completed (loss: 0.0005357257323339581, acc: 1.0)
[2025-01-06 01:53:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:19][root][INFO] - Training Epoch: 9/10, step 571/574 completed (loss: 0.038576193153858185, acc: 0.9829059839248657)
[2025-01-06 01:53:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:20][root][INFO] - Training Epoch: 9/10, step 572/574 completed (loss: 0.06725715845823288, acc: 0.9846938848495483)
[2025-01-06 01:53:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:20][root][INFO] - Training Epoch: 9/10, step 573/574 completed (loss: 0.04017625376582146, acc: 0.9874213933944702)
[2025-01-06 01:53:20][slam_llm.utils.train_utils][INFO] - Epoch 9: train_perplexity=1.0554, train_epoch_loss=0.0539, epoch time 355.56035758554935s
[2025-01-06 01:53:20][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 13 GB
[2025-01-06 01:53:20][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 13 GB
[2025-01-06 01:53:20][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 13 GB
[2025-01-06 01:53:20][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 25
[2025-01-06 01:53:20][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2025-01-06 01:53:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:21][root][INFO] - Training Epoch: 10/10, step 0/574 completed (loss: 0.0029369371477514505, acc: 1.0)
[2025-01-06 01:53:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:22][root][INFO] - Training Epoch: 10/10, step 1/574 completed (loss: 0.0015467180637642741, acc: 1.0)
[2025-01-06 01:53:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:22][root][INFO] - Training Epoch: 10/10, step 2/574 completed (loss: 0.19833588600158691, acc: 0.9729729890823364)
[2025-01-06 01:53:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:22][root][INFO] - Training Epoch: 10/10, step 3/574 completed (loss: 0.05357402190566063, acc: 0.9736841917037964)
[2025-01-06 01:53:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:23][root][INFO] - Training Epoch: 10/10, step 4/574 completed (loss: 0.012631895020604134, acc: 1.0)
[2025-01-06 01:53:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:23][root][INFO] - Training Epoch: 10/10, step 5/574 completed (loss: 0.003936867229640484, acc: 1.0)
[2025-01-06 01:53:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:24][root][INFO] - Training Epoch: 10/10, step 6/574 completed (loss: 0.015076788142323494, acc: 1.0)
[2025-01-06 01:53:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:24][root][INFO] - Training Epoch: 10/10, step 7/574 completed (loss: 0.16870489716529846, acc: 0.9333333373069763)
[2025-01-06 01:53:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:24][root][INFO] - Training Epoch: 10/10, step 8/574 completed (loss: 0.0011538135586306453, acc: 1.0)
[2025-01-06 01:53:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:25][root][INFO] - Training Epoch: 10/10, step 9/574 completed (loss: 0.00031564306118525565, acc: 1.0)
[2025-01-06 01:53:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:25][root][INFO] - Training Epoch: 10/10, step 10/574 completed (loss: 0.0004317212151363492, acc: 1.0)
[2025-01-06 01:53:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:25][root][INFO] - Training Epoch: 10/10, step 11/574 completed (loss: 0.0026764050126075745, acc: 1.0)
[2025-01-06 01:53:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:26][root][INFO] - Training Epoch: 10/10, step 12/574 completed (loss: 0.001108884229324758, acc: 1.0)
[2025-01-06 01:53:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:26][root][INFO] - Training Epoch: 10/10, step 13/574 completed (loss: 0.008046210743486881, acc: 1.0)
[2025-01-06 01:53:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:26][root][INFO] - Training Epoch: 10/10, step 14/574 completed (loss: 0.0025035578291863203, acc: 1.0)
[2025-01-06 01:53:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:27][root][INFO] - Training Epoch: 10/10, step 15/574 completed (loss: 0.047826189547777176, acc: 0.9795918464660645)
[2025-01-06 01:53:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:27][root][INFO] - Training Epoch: 10/10, step 16/574 completed (loss: 0.0010933547746390104, acc: 1.0)
[2025-01-06 01:53:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:27][root][INFO] - Training Epoch: 10/10, step 17/574 completed (loss: 0.04639187827706337, acc: 0.9583333134651184)
[2025-01-06 01:53:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:28][root][INFO] - Training Epoch: 10/10, step 18/574 completed (loss: 0.1693691611289978, acc: 0.9166666865348816)
[2025-01-06 01:53:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:28][root][INFO] - Training Epoch: 10/10, step 19/574 completed (loss: 0.0001472974690841511, acc: 1.0)
[2025-01-06 01:53:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:28][root][INFO] - Training Epoch: 10/10, step 20/574 completed (loss: 0.0017680040327832103, acc: 1.0)
[2025-01-06 01:53:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:29][root][INFO] - Training Epoch: 10/10, step 21/574 completed (loss: 0.466810405254364, acc: 0.8965517282485962)
[2025-01-06 01:53:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:29][root][INFO] - Training Epoch: 10/10, step 22/574 completed (loss: 0.002184020821005106, acc: 1.0)
[2025-01-06 01:53:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:29][root][INFO] - Training Epoch: 10/10, step 23/574 completed (loss: 0.001863132114522159, acc: 1.0)
[2025-01-06 01:53:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:30][root][INFO] - Training Epoch: 10/10, step 24/574 completed (loss: 0.00028688766178674996, acc: 1.0)
[2025-01-06 01:53:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:30][root][INFO] - Training Epoch: 10/10, step 25/574 completed (loss: 0.010092739947140217, acc: 1.0)
[2025-01-06 01:53:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:31][root][INFO] - Training Epoch: 10/10, step 26/574 completed (loss: 0.0389663390815258, acc: 0.9863013625144958)
[2025-01-06 01:53:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:32][root][INFO] - Training Epoch: 10/10, step 27/574 completed (loss: 0.248093381524086, acc: 0.9130434989929199)
[2025-01-06 01:53:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:32][root][INFO] - Training Epoch: 10/10, step 28/574 completed (loss: 0.03324653208255768, acc: 1.0)
[2025-01-06 01:53:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:33][root][INFO] - Training Epoch: 10/10, step 29/574 completed (loss: 0.0852813795208931, acc: 0.9638554453849792)
[2025-01-06 01:53:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:33][root][INFO] - Training Epoch: 10/10, step 30/574 completed (loss: 0.041376397013664246, acc: 0.9876543283462524)
[2025-01-06 01:53:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:33][root][INFO] - Training Epoch: 10/10, step 31/574 completed (loss: 0.06268567591905594, acc: 0.9642857313156128)
[2025-01-06 01:53:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:34][root][INFO] - Training Epoch: 10/10, step 32/574 completed (loss: 0.0016194136114791036, acc: 1.0)
[2025-01-06 01:53:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:34][root][INFO] - Training Epoch: 10/10, step 33/574 completed (loss: 0.0016312143998220563, acc: 1.0)
[2025-01-06 01:53:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:35][root][INFO] - Training Epoch: 10/10, step 34/574 completed (loss: 0.1003522276878357, acc: 0.9663865566253662)
[2025-01-06 01:53:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:35][root][INFO] - Training Epoch: 10/10, step 35/574 completed (loss: 0.06728069484233856, acc: 0.9836065769195557)
[2025-01-06 01:53:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:35][root][INFO] - Training Epoch: 10/10, step 36/574 completed (loss: 0.019469404593110085, acc: 1.0)
[2025-01-06 01:53:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:36][root][INFO] - Training Epoch: 10/10, step 37/574 completed (loss: 0.05488436669111252, acc: 0.9830508232116699)
[2025-01-06 01:53:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:36][root][INFO] - Training Epoch: 10/10, step 38/574 completed (loss: 0.053571343421936035, acc: 0.977011501789093)
[2025-01-06 01:53:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:36][root][INFO] - Training Epoch: 10/10, step 39/574 completed (loss: 0.008490840904414654, acc: 1.0)
[2025-01-06 01:53:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:37][root][INFO] - Training Epoch: 10/10, step 40/574 completed (loss: 0.0024237600155174732, acc: 1.0)
[2025-01-06 01:53:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:37][root][INFO] - Training Epoch: 10/10, step 41/574 completed (loss: 0.012705975212156773, acc: 1.0)
[2025-01-06 01:53:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:37][root][INFO] - Training Epoch: 10/10, step 42/574 completed (loss: 0.11351669579744339, acc: 0.9384615421295166)
[2025-01-06 01:53:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:38][root][INFO] - Training Epoch: 10/10, step 43/574 completed (loss: 0.03294120356440544, acc: 0.9898989796638489)
[2025-01-06 01:53:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:38][root][INFO] - Training Epoch: 10/10, step 44/574 completed (loss: 0.1267559826374054, acc: 0.9587628841400146)
[2025-01-06 01:53:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:39][root][INFO] - Training Epoch: 10/10, step 45/574 completed (loss: 0.06655995547771454, acc: 0.970588207244873)
[2025-01-06 01:53:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:39][root][INFO] - Training Epoch: 10/10, step 46/574 completed (loss: 0.0050719198770821095, acc: 1.0)
[2025-01-06 01:53:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:39][root][INFO] - Training Epoch: 10/10, step 47/574 completed (loss: 0.004163473378866911, acc: 1.0)
[2025-01-06 01:53:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:40][root][INFO] - Training Epoch: 10/10, step 48/574 completed (loss: 0.004916488192975521, acc: 1.0)
[2025-01-06 01:53:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:40][root][INFO] - Training Epoch: 10/10, step 49/574 completed (loss: 0.010193384252488613, acc: 1.0)
[2025-01-06 01:53:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:41][root][INFO] - Training Epoch: 10/10, step 50/574 completed (loss: 0.03789353370666504, acc: 0.9824561476707458)
[2025-01-06 01:53:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:41][root][INFO] - Training Epoch: 10/10, step 51/574 completed (loss: 0.049991317093372345, acc: 0.9841269850730896)
[2025-01-06 01:53:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:41][root][INFO] - Training Epoch: 10/10, step 52/574 completed (loss: 0.11890307813882828, acc: 0.9577465057373047)
[2025-01-06 01:53:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:42][root][INFO] - Training Epoch: 10/10, step 53/574 completed (loss: 0.2510673403739929, acc: 0.9200000166893005)
[2025-01-06 01:53:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:42][root][INFO] - Training Epoch: 10/10, step 54/574 completed (loss: 0.025983735918998718, acc: 1.0)
[2025-01-06 01:53:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:42][root][INFO] - Training Epoch: 10/10, step 55/574 completed (loss: 0.012225131504237652, acc: 1.0)
[2025-01-06 01:53:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:45][root][INFO] - Training Epoch: 10/10, step 56/574 completed (loss: 0.48515021800994873, acc: 0.8430033922195435)
[2025-01-06 01:53:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:47][root][INFO] - Training Epoch: 10/10, step 57/574 completed (loss: 0.6332116723060608, acc: 0.8235294222831726)
[2025-01-06 01:53:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:47][root][INFO] - Training Epoch: 10/10, step 58/574 completed (loss: 0.2881629467010498, acc: 0.9090909361839294)
[2025-01-06 01:53:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:48][root][INFO] - Training Epoch: 10/10, step 59/574 completed (loss: 0.07672756910324097, acc: 0.9558823704719543)
[2025-01-06 01:53:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:48][root][INFO] - Training Epoch: 10/10, step 60/574 completed (loss: 0.15977327525615692, acc: 0.9492753744125366)
[2025-01-06 01:53:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:49][root][INFO] - Training Epoch: 10/10, step 61/574 completed (loss: 0.06877533346414566, acc: 0.9750000238418579)
[2025-01-06 01:53:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:49][root][INFO] - Training Epoch: 10/10, step 62/574 completed (loss: 0.6028721332550049, acc: 0.8823529481887817)
[2025-01-06 01:53:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:49][root][INFO] - Training Epoch: 10/10, step 63/574 completed (loss: 0.008150864392518997, acc: 1.0)
[2025-01-06 01:53:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:50][root][INFO] - Training Epoch: 10/10, step 64/574 completed (loss: 0.009115797467529774, acc: 1.0)
[2025-01-06 01:53:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:50][root][INFO] - Training Epoch: 10/10, step 65/574 completed (loss: 0.008030044846236706, acc: 1.0)
[2025-01-06 01:53:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:51][root][INFO] - Training Epoch: 10/10, step 66/574 completed (loss: 0.10146220773458481, acc: 0.9642857313156128)
[2025-01-06 01:53:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:51][root][INFO] - Training Epoch: 10/10, step 67/574 completed (loss: 0.01165917981415987, acc: 1.0)
[2025-01-06 01:53:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:51][root][INFO] - Training Epoch: 10/10, step 68/574 completed (loss: 0.0011033922201022506, acc: 1.0)
[2025-01-06 01:53:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:52][root][INFO] - Training Epoch: 10/10, step 69/574 completed (loss: 0.11483757942914963, acc: 0.9722222089767456)
[2025-01-06 01:53:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:52][root][INFO] - Training Epoch: 10/10, step 70/574 completed (loss: 0.035113658756017685, acc: 1.0)
[2025-01-06 01:53:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:52][root][INFO] - Training Epoch: 10/10, step 71/574 completed (loss: 0.1750110685825348, acc: 0.9338235259056091)
[2025-01-06 01:53:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:53][root][INFO] - Training Epoch: 10/10, step 72/574 completed (loss: 0.40859636664390564, acc: 0.9047619104385376)
[2025-01-06 01:53:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:53][root][INFO] - Training Epoch: 10/10, step 73/574 completed (loss: 0.2816126048564911, acc: 0.8871794939041138)
[2025-01-06 01:53:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:53][root][INFO] - Training Epoch: 10/10, step 74/574 completed (loss: 0.12122111767530441, acc: 0.9489796161651611)
[2025-01-06 01:53:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:54][root][INFO] - Training Epoch: 10/10, step 75/574 completed (loss: 0.20618151128292084, acc: 0.9552238583564758)
[2025-01-06 01:53:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:54][root][INFO] - Training Epoch: 10/10, step 76/574 completed (loss: 0.3557737171649933, acc: 0.8722627758979797)
[2025-01-06 01:53:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:55][root][INFO] - Training Epoch: 10/10, step 77/574 completed (loss: 0.0008231888641603291, acc: 1.0)
[2025-01-06 01:53:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:55][root][INFO] - Training Epoch: 10/10, step 78/574 completed (loss: 0.009971002116799355, acc: 1.0)
[2025-01-06 01:53:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:55][root][INFO] - Training Epoch: 10/10, step 79/574 completed (loss: 0.1483173668384552, acc: 0.9696969985961914)
[2025-01-06 01:53:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:56][root][INFO] - Training Epoch: 10/10, step 80/574 completed (loss: 0.004322175867855549, acc: 1.0)
[2025-01-06 01:53:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:56][root][INFO] - Training Epoch: 10/10, step 81/574 completed (loss: 0.09311067312955856, acc: 0.9807692170143127)
[2025-01-06 01:53:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:56][root][INFO] - Training Epoch: 10/10, step 82/574 completed (loss: 0.09179837256669998, acc: 0.9807692170143127)
[2025-01-06 01:53:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:57][root][INFO] - Training Epoch: 10/10, step 83/574 completed (loss: 0.17722652852535248, acc: 0.96875)
[2025-01-06 01:53:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:57][root][INFO] - Training Epoch: 10/10, step 84/574 completed (loss: 0.031788475811481476, acc: 1.0)
[2025-01-06 01:53:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:58][root][INFO] - Training Epoch: 10/10, step 85/574 completed (loss: 0.14397497475147247, acc: 0.9399999976158142)
[2025-01-06 01:53:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:58][root][INFO] - Training Epoch: 10/10, step 86/574 completed (loss: 0.0060478500090539455, acc: 1.0)
[2025-01-06 01:53:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:58][root][INFO] - Training Epoch: 10/10, step 87/574 completed (loss: 0.015568830072879791, acc: 1.0)
[2025-01-06 01:53:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:53:59][root][INFO] - Training Epoch: 10/10, step 88/574 completed (loss: 0.18370705842971802, acc: 0.9417475461959839)
[2025-01-06 01:53:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:00][root][INFO] - Training Epoch: 10/10, step 89/574 completed (loss: 0.2482244223356247, acc: 0.9271844625473022)
[2025-01-06 01:54:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:01][root][INFO] - Training Epoch: 10/10, step 90/574 completed (loss: 0.250973641872406, acc: 0.9247311949729919)
[2025-01-06 01:54:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:01][root][INFO] - Training Epoch: 10/10, step 91/574 completed (loss: 0.2772587537765503, acc: 0.931034505367279)
[2025-01-06 01:54:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:02][root][INFO] - Training Epoch: 10/10, step 92/574 completed (loss: 0.12469594180583954, acc: 0.9684210419654846)
[2025-01-06 01:54:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:03][root][INFO] - Training Epoch: 10/10, step 93/574 completed (loss: 0.23358233273029327, acc: 0.9306930899620056)
[2025-01-06 01:54:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:03][root][INFO] - Training Epoch: 10/10, step 94/574 completed (loss: 0.1124773919582367, acc: 0.9677419066429138)
[2025-01-06 01:54:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:04][root][INFO] - Training Epoch: 10/10, step 95/574 completed (loss: 0.14412562549114227, acc: 0.95652174949646)
[2025-01-06 01:54:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:04][root][INFO] - Training Epoch: 10/10, step 96/574 completed (loss: 0.16206905245780945, acc: 0.9831932783126831)
[2025-01-06 01:54:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:05][root][INFO] - Training Epoch: 10/10, step 97/574 completed (loss: 0.22757218778133392, acc: 0.9519230723381042)
[2025-01-06 01:54:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:05][root][INFO] - Training Epoch: 10/10, step 98/574 completed (loss: 0.16040554642677307, acc: 0.970802903175354)
[2025-01-06 01:54:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:05][root][INFO] - Training Epoch: 10/10, step 99/574 completed (loss: 0.15454241633415222, acc: 0.9402984976768494)
[2025-01-06 01:54:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:06][root][INFO] - Training Epoch: 10/10, step 100/574 completed (loss: 0.19124501943588257, acc: 0.8999999761581421)
[2025-01-06 01:54:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:06][root][INFO] - Training Epoch: 10/10, step 101/574 completed (loss: 0.004447187762707472, acc: 1.0)
[2025-01-06 01:54:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:06][root][INFO] - Training Epoch: 10/10, step 102/574 completed (loss: 0.003407147480174899, acc: 1.0)
[2025-01-06 01:54:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:07][root][INFO] - Training Epoch: 10/10, step 103/574 completed (loss: 0.006019987631589174, acc: 1.0)
[2025-01-06 01:54:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:07][root][INFO] - Training Epoch: 10/10, step 104/574 completed (loss: 0.027768122032284737, acc: 1.0)
[2025-01-06 01:54:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:07][root][INFO] - Training Epoch: 10/10, step 105/574 completed (loss: 0.02063242718577385, acc: 0.9767441749572754)
[2025-01-06 01:54:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:08][root][INFO] - Training Epoch: 10/10, step 106/574 completed (loss: 0.0825691819190979, acc: 0.9599999785423279)
[2025-01-06 01:54:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:08][root][INFO] - Training Epoch: 10/10, step 107/574 completed (loss: 0.04450448229908943, acc: 0.9411764740943909)
[2025-01-06 01:54:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:08][root][INFO] - Training Epoch: 10/10, step 108/574 completed (loss: 0.021408196538686752, acc: 1.0)
[2025-01-06 01:54:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:09][root][INFO] - Training Epoch: 10/10, step 109/574 completed (loss: 0.33139505982398987, acc: 0.976190447807312)
[2025-01-06 01:54:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:09][root][INFO] - Training Epoch: 10/10, step 110/574 completed (loss: 0.005534104071557522, acc: 1.0)
[2025-01-06 01:54:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:10][root][INFO] - Training Epoch: 10/10, step 111/574 completed (loss: 0.08174901455640793, acc: 0.9649122953414917)
[2025-01-06 01:54:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:10][root][INFO] - Training Epoch: 10/10, step 112/574 completed (loss: 0.12474896013736725, acc: 0.9649122953414917)
[2025-01-06 01:54:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:10][root][INFO] - Training Epoch: 10/10, step 113/574 completed (loss: 0.2947307527065277, acc: 0.9487179517745972)
[2025-01-06 01:54:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:11][root][INFO] - Training Epoch: 10/10, step 114/574 completed (loss: 0.11359567195177078, acc: 0.9795918464660645)
[2025-01-06 01:54:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:11][root][INFO] - Training Epoch: 10/10, step 115/574 completed (loss: 0.012177086435258389, acc: 1.0)
[2025-01-06 01:54:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:11][root][INFO] - Training Epoch: 10/10, step 116/574 completed (loss: 0.17864513397216797, acc: 0.920634925365448)
[2025-01-06 01:54:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:12][root][INFO] - Training Epoch: 10/10, step 117/574 completed (loss: 0.0546652115881443, acc: 0.9918699264526367)
[2025-01-06 01:54:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:12][root][INFO] - Training Epoch: 10/10, step 118/574 completed (loss: 0.04230303689837456, acc: 0.9838709831237793)
[2025-01-06 01:54:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:13][root][INFO] - Training Epoch: 10/10, step 119/574 completed (loss: 0.24725736677646637, acc: 0.9163498282432556)
[2025-01-06 01:54:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:13][root][INFO] - Training Epoch: 10/10, step 120/574 completed (loss: 0.08442696183919907, acc: 0.9733333587646484)
[2025-01-06 01:54:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:14][root][INFO] - Training Epoch: 10/10, step 121/574 completed (loss: 0.10454095155000687, acc: 0.942307710647583)
[2025-01-06 01:54:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:14][root][INFO] - Training Epoch: 10/10, step 122/574 completed (loss: 0.0035907693672925234, acc: 1.0)
[2025-01-06 01:54:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:14][root][INFO] - Training Epoch: 10/10, step 123/574 completed (loss: 0.05253498628735542, acc: 1.0)
[2025-01-06 01:54:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:15][root][INFO] - Training Epoch: 10/10, step 124/574 completed (loss: 0.18553565442562103, acc: 0.9509202241897583)
[2025-01-06 01:54:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:45][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.2642, device='cuda:0') eval_epoch_loss=tensor(0.8172, device='cuda:0') eval_epoch_acc=tensor(0.8357, device='cuda:0')
[2025-01-06 01:54:45][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:54:45][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:54:46][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_10_step_125_loss_0.8172417283058167/model.pt
[2025-01-06 01:54:46][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:54:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:46][root][INFO] - Training Epoch: 10/10, step 125/574 completed (loss: 0.16857537627220154, acc: 0.9444444179534912)
[2025-01-06 01:54:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:46][root][INFO] - Training Epoch: 10/10, step 126/574 completed (loss: 0.19131912291049957, acc: 0.925000011920929)
[2025-01-06 01:54:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:47][root][INFO] - Training Epoch: 10/10, step 127/574 completed (loss: 0.12894323468208313, acc: 0.976190447807312)
[2025-01-06 01:54:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:47][root][INFO] - Training Epoch: 10/10, step 128/574 completed (loss: 0.27576714754104614, acc: 0.8974359035491943)
[2025-01-06 01:54:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:47][root][INFO] - Training Epoch: 10/10, step 129/574 completed (loss: 0.2018844038248062, acc: 0.9338235259056091)
[2025-01-06 01:54:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:48][root][INFO] - Training Epoch: 10/10, step 130/574 completed (loss: 0.19163672626018524, acc: 0.9230769276618958)
[2025-01-06 01:54:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:48][root][INFO] - Training Epoch: 10/10, step 131/574 completed (loss: 0.021210506558418274, acc: 1.0)
[2025-01-06 01:54:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:48][root][INFO] - Training Epoch: 10/10, step 132/574 completed (loss: 0.07985052466392517, acc: 0.9375)
[2025-01-06 01:54:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:49][root][INFO] - Training Epoch: 10/10, step 133/574 completed (loss: 0.2792234718799591, acc: 0.95652174949646)
[2025-01-06 01:54:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:49][root][INFO] - Training Epoch: 10/10, step 134/574 completed (loss: 0.3456108868122101, acc: 0.9714285731315613)
[2025-01-06 01:54:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:49][root][INFO] - Training Epoch: 10/10, step 135/574 completed (loss: 0.020931780338287354, acc: 1.0)
[2025-01-06 01:54:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:50][root][INFO] - Training Epoch: 10/10, step 136/574 completed (loss: 0.024429040029644966, acc: 1.0)
[2025-01-06 01:54:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:50][root][INFO] - Training Epoch: 10/10, step 137/574 completed (loss: 0.13512635231018066, acc: 0.9333333373069763)
[2025-01-06 01:54:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:50][root][INFO] - Training Epoch: 10/10, step 138/574 completed (loss: 0.023221895098686218, acc: 1.0)
[2025-01-06 01:54:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:51][root][INFO] - Training Epoch: 10/10, step 139/574 completed (loss: 0.026102174073457718, acc: 1.0)
[2025-01-06 01:54:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:51][root][INFO] - Training Epoch: 10/10, step 140/574 completed (loss: 0.24274827539920807, acc: 0.9230769276618958)
[2025-01-06 01:54:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:52][root][INFO] - Training Epoch: 10/10, step 141/574 completed (loss: 0.02807653695344925, acc: 1.0)
[2025-01-06 01:54:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:52][root][INFO] - Training Epoch: 10/10, step 142/574 completed (loss: 0.5593087673187256, acc: 0.9459459185600281)
[2025-01-06 01:54:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:52][root][INFO] - Training Epoch: 10/10, step 143/574 completed (loss: 0.21079444885253906, acc: 0.9473684430122375)
[2025-01-06 01:54:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:53][root][INFO] - Training Epoch: 10/10, step 144/574 completed (loss: 0.10025397688150406, acc: 0.9701492786407471)
[2025-01-06 01:54:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:53][root][INFO] - Training Epoch: 10/10, step 145/574 completed (loss: 0.05941396206617355, acc: 0.9795918464660645)
[2025-01-06 01:54:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:54][root][INFO] - Training Epoch: 10/10, step 146/574 completed (loss: 0.13029882311820984, acc: 0.978723406791687)
[2025-01-06 01:54:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:54][root][INFO] - Training Epoch: 10/10, step 147/574 completed (loss: 0.08133114129304886, acc: 0.9571428298950195)
[2025-01-06 01:54:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:54][root][INFO] - Training Epoch: 10/10, step 148/574 completed (loss: 0.049021653831005096, acc: 0.9642857313156128)
[2025-01-06 01:54:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:55][root][INFO] - Training Epoch: 10/10, step 149/574 completed (loss: 0.20807108283042908, acc: 0.95652174949646)
[2025-01-06 01:54:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:55][root][INFO] - Training Epoch: 10/10, step 150/574 completed (loss: 0.4231494665145874, acc: 0.8965517282485962)
[2025-01-06 01:54:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:55][root][INFO] - Training Epoch: 10/10, step 151/574 completed (loss: 0.014474381692707539, acc: 1.0)
[2025-01-06 01:54:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:56][root][INFO] - Training Epoch: 10/10, step 152/574 completed (loss: 0.03605297580361366, acc: 0.9830508232116699)
[2025-01-06 01:54:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:56][root][INFO] - Training Epoch: 10/10, step 153/574 completed (loss: 0.11190374940633774, acc: 0.9473684430122375)
[2025-01-06 01:54:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:56][root][INFO] - Training Epoch: 10/10, step 154/574 completed (loss: 0.06902976334095001, acc: 0.9864864945411682)
[2025-01-06 01:54:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:57][root][INFO] - Training Epoch: 10/10, step 155/574 completed (loss: 0.018515128642320633, acc: 1.0)
[2025-01-06 01:54:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:57][root][INFO] - Training Epoch: 10/10, step 156/574 completed (loss: 0.022834908217191696, acc: 1.0)
[2025-01-06 01:54:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:57][root][INFO] - Training Epoch: 10/10, step 157/574 completed (loss: 0.23702140152454376, acc: 0.9473684430122375)
[2025-01-06 01:54:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:59][root][INFO] - Training Epoch: 10/10, step 158/574 completed (loss: 0.20812426507472992, acc: 0.9054054021835327)
[2025-01-06 01:54:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:54:59][root][INFO] - Training Epoch: 10/10, step 159/574 completed (loss: 0.06619856506586075, acc: 0.9814814925193787)
[2025-01-06 01:54:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:00][root][INFO] - Training Epoch: 10/10, step 160/574 completed (loss: 0.17472586035728455, acc: 0.9534883499145508)
[2025-01-06 01:55:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:00][root][INFO] - Training Epoch: 10/10, step 161/574 completed (loss: 0.23542137444019318, acc: 0.929411768913269)
[2025-01-06 01:55:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:01][root][INFO] - Training Epoch: 10/10, step 162/574 completed (loss: 0.24840226769447327, acc: 0.9213483333587646)
[2025-01-06 01:55:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:01][root][INFO] - Training Epoch: 10/10, step 163/574 completed (loss: 0.11814546585083008, acc: 0.9772727489471436)
[2025-01-06 01:55:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:02][root][INFO] - Training Epoch: 10/10, step 164/574 completed (loss: 0.005766895599663258, acc: 1.0)
[2025-01-06 01:55:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:02][root][INFO] - Training Epoch: 10/10, step 165/574 completed (loss: 0.4014769196510315, acc: 0.9655172228813171)
[2025-01-06 01:55:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:02][root][INFO] - Training Epoch: 10/10, step 166/574 completed (loss: 0.0023359479382634163, acc: 1.0)
[2025-01-06 01:55:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:02][root][INFO] - Training Epoch: 10/10, step 167/574 completed (loss: 0.02034205012023449, acc: 1.0)
[2025-01-06 01:55:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:03][root][INFO] - Training Epoch: 10/10, step 168/574 completed (loss: 0.03718020021915436, acc: 1.0)
[2025-01-06 01:55:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:03][root][INFO] - Training Epoch: 10/10, step 169/574 completed (loss: 0.2753172218799591, acc: 0.9215686321258545)
[2025-01-06 01:55:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:04][root][INFO] - Training Epoch: 10/10, step 170/574 completed (loss: 0.10368107259273529, acc: 0.9726027250289917)
[2025-01-06 01:55:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:05][root][INFO] - Training Epoch: 10/10, step 171/574 completed (loss: 0.011422461830079556, acc: 1.0)
[2025-01-06 01:55:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:05][root][INFO] - Training Epoch: 10/10, step 172/574 completed (loss: 0.012655309401452541, acc: 1.0)
[2025-01-06 01:55:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:05][root][INFO] - Training Epoch: 10/10, step 173/574 completed (loss: 0.20822405815124512, acc: 0.9285714030265808)
[2025-01-06 01:55:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:06][root][INFO] - Training Epoch: 10/10, step 174/574 completed (loss: 0.22426816821098328, acc: 0.9380530714988708)
[2025-01-06 01:55:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:06][root][INFO] - Training Epoch: 10/10, step 175/574 completed (loss: 0.05689537897706032, acc: 0.9855072498321533)
[2025-01-06 01:55:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:06][root][INFO] - Training Epoch: 10/10, step 176/574 completed (loss: 0.020539050921797752, acc: 1.0)
[2025-01-06 01:55:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:07][root][INFO] - Training Epoch: 10/10, step 177/574 completed (loss: 0.21233943104743958, acc: 0.9465649127960205)
[2025-01-06 01:55:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:08][root][INFO] - Training Epoch: 10/10, step 178/574 completed (loss: 0.1523573398590088, acc: 0.9777777791023254)
[2025-01-06 01:55:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:08][root][INFO] - Training Epoch: 10/10, step 179/574 completed (loss: 0.21064111590385437, acc: 0.9672130942344666)
[2025-01-06 01:55:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:09][root][INFO] - Training Epoch: 10/10, step 180/574 completed (loss: 0.0311067346483469, acc: 1.0)
[2025-01-06 01:55:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:09][root][INFO] - Training Epoch: 10/10, step 181/574 completed (loss: 0.0014859922230243683, acc: 1.0)
[2025-01-06 01:55:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:10][root][INFO] - Training Epoch: 10/10, step 182/574 completed (loss: 0.12747550010681152, acc: 0.9642857313156128)
[2025-01-06 01:55:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:10][root][INFO] - Training Epoch: 10/10, step 183/574 completed (loss: 0.025896389037370682, acc: 0.9878048896789551)
[2025-01-06 01:55:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:10][root][INFO] - Training Epoch: 10/10, step 184/574 completed (loss: 0.09738237410783768, acc: 0.9607250690460205)
[2025-01-06 01:55:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:10][root][INFO] - Training Epoch: 10/10, step 185/574 completed (loss: 0.14248481392860413, acc: 0.9625360369682312)
[2025-01-06 01:55:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:11][root][INFO] - Training Epoch: 10/10, step 186/574 completed (loss: 0.11566100269556046, acc: 0.9624999761581421)
[2025-01-06 01:55:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:12][root][INFO] - Training Epoch: 10/10, step 187/574 completed (loss: 0.2343498170375824, acc: 0.9287054538726807)
[2025-01-06 01:55:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:12][root][INFO] - Training Epoch: 10/10, step 188/574 completed (loss: 0.1507086604833603, acc: 0.9537366628646851)
[2025-01-06 01:55:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:12][root][INFO] - Training Epoch: 10/10, step 189/574 completed (loss: 0.07646014541387558, acc: 0.9599999785423279)
[2025-01-06 01:55:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:13][root][INFO] - Training Epoch: 10/10, step 190/574 completed (loss: 0.19080685079097748, acc: 0.930232584476471)
[2025-01-06 01:55:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:14][root][INFO] - Training Epoch: 10/10, step 191/574 completed (loss: 0.23632222414016724, acc: 0.920634925365448)
[2025-01-06 01:55:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:15][root][INFO] - Training Epoch: 10/10, step 192/574 completed (loss: 0.2843417823314667, acc: 0.9090909361839294)
[2025-01-06 01:55:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:15][root][INFO] - Training Epoch: 10/10, step 193/574 completed (loss: 0.08827156573534012, acc: 0.9764705896377563)
[2025-01-06 01:55:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:16][root][INFO] - Training Epoch: 10/10, step 194/574 completed (loss: 0.22877107560634613, acc: 0.9135802388191223)
[2025-01-06 01:55:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:17][root][INFO] - Training Epoch: 10/10, step 195/574 completed (loss: 0.07675711065530777, acc: 0.9516128897666931)
[2025-01-06 01:55:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:18][root][INFO] - Training Epoch: 10/10, step 196/574 completed (loss: 0.006214431952685118, acc: 1.0)
[2025-01-06 01:55:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:18][root][INFO] - Training Epoch: 10/10, step 197/574 completed (loss: 0.029926244169473648, acc: 1.0)
[2025-01-06 01:55:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:18][root][INFO] - Training Epoch: 10/10, step 198/574 completed (loss: 0.2385483682155609, acc: 0.8970588445663452)
[2025-01-06 01:55:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:19][root][INFO] - Training Epoch: 10/10, step 199/574 completed (loss: 0.07226437330245972, acc: 0.9779411554336548)
[2025-01-06 01:55:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:19][root][INFO] - Training Epoch: 10/10, step 200/574 completed (loss: 0.07260408997535706, acc: 0.9830508232116699)
[2025-01-06 01:55:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:19][root][INFO] - Training Epoch: 10/10, step 201/574 completed (loss: 0.1809970885515213, acc: 0.9328358173370361)
[2025-01-06 01:55:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:20][root][INFO] - Training Epoch: 10/10, step 202/574 completed (loss: 0.25854527950286865, acc: 0.9320388436317444)
[2025-01-06 01:55:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:20][root][INFO] - Training Epoch: 10/10, step 203/574 completed (loss: 0.015506225638091564, acc: 1.0)
[2025-01-06 01:55:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:21][root][INFO] - Training Epoch: 10/10, step 204/574 completed (loss: 0.013906127773225307, acc: 1.0)
[2025-01-06 01:55:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:21][root][INFO] - Training Epoch: 10/10, step 205/574 completed (loss: 0.0415964350104332, acc: 0.9820627570152283)
[2025-01-06 01:55:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:21][root][INFO] - Training Epoch: 10/10, step 206/574 completed (loss: 0.10485807061195374, acc: 0.960629940032959)
[2025-01-06 01:55:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:22][root][INFO] - Training Epoch: 10/10, step 207/574 completed (loss: 0.0327489860355854, acc: 0.9913793206214905)
[2025-01-06 01:55:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:22][root][INFO] - Training Epoch: 10/10, step 208/574 completed (loss: 0.061218101531267166, acc: 0.9818840622901917)
[2025-01-06 01:55:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:22][root][INFO] - Training Epoch: 10/10, step 209/574 completed (loss: 0.04477052390575409, acc: 0.9805447459220886)
[2025-01-06 01:55:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:23][root][INFO] - Training Epoch: 10/10, step 210/574 completed (loss: 0.02396564930677414, acc: 1.0)
[2025-01-06 01:55:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:23][root][INFO] - Training Epoch: 10/10, step 211/574 completed (loss: 0.004248607903718948, acc: 1.0)
[2025-01-06 01:55:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:23][root][INFO] - Training Epoch: 10/10, step 212/574 completed (loss: 0.020042115822434425, acc: 1.0)
[2025-01-06 01:55:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:24][root][INFO] - Training Epoch: 10/10, step 213/574 completed (loss: 0.0022742380388081074, acc: 1.0)
[2025-01-06 01:55:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:24][root][INFO] - Training Epoch: 10/10, step 214/574 completed (loss: 0.04253729060292244, acc: 0.9846153855323792)
[2025-01-06 01:55:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:25][root][INFO] - Training Epoch: 10/10, step 215/574 completed (loss: 0.011410189792513847, acc: 1.0)
[2025-01-06 01:55:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:25][root][INFO] - Training Epoch: 10/10, step 216/574 completed (loss: 0.02903801016509533, acc: 0.9883720874786377)
[2025-01-06 01:55:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:26][root][INFO] - Training Epoch: 10/10, step 217/574 completed (loss: 0.010098196566104889, acc: 1.0)
[2025-01-06 01:55:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:26][root][INFO] - Training Epoch: 10/10, step 218/574 completed (loss: 0.009111756458878517, acc: 1.0)
[2025-01-06 01:55:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:26][root][INFO] - Training Epoch: 10/10, step 219/574 completed (loss: 0.0010321050649508834, acc: 1.0)
[2025-01-06 01:55:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:27][root][INFO] - Training Epoch: 10/10, step 220/574 completed (loss: 0.0038839306216686964, acc: 1.0)
[2025-01-06 01:55:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:27][root][INFO] - Training Epoch: 10/10, step 221/574 completed (loss: 0.03791205585002899, acc: 0.9599999785423279)
[2025-01-06 01:55:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:28][root][INFO] - Training Epoch: 10/10, step 222/574 completed (loss: 0.11787737905979156, acc: 0.9807692170143127)
[2025-01-06 01:55:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:28][root][INFO] - Training Epoch: 10/10, step 223/574 completed (loss: 0.0320531465113163, acc: 0.989130437374115)
[2025-01-06 01:55:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:29][root][INFO] - Training Epoch: 10/10, step 224/574 completed (loss: 0.1346920281648636, acc: 0.9431818127632141)
[2025-01-06 01:55:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:29][root][INFO] - Training Epoch: 10/10, step 225/574 completed (loss: 0.13662761449813843, acc: 0.957446813583374)
[2025-01-06 01:55:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:30][root][INFO] - Training Epoch: 10/10, step 226/574 completed (loss: 0.10006213933229446, acc: 0.9622641801834106)
[2025-01-06 01:55:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:30][root][INFO] - Training Epoch: 10/10, step 227/574 completed (loss: 0.025510210543870926, acc: 0.9833333492279053)
[2025-01-06 01:55:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:30][root][INFO] - Training Epoch: 10/10, step 228/574 completed (loss: 0.06408718228340149, acc: 0.9767441749572754)
[2025-01-06 01:55:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:31][root][INFO] - Training Epoch: 10/10, step 229/574 completed (loss: 0.12699292600154877, acc: 0.9666666388511658)
[2025-01-06 01:55:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:31][root][INFO] - Training Epoch: 10/10, step 230/574 completed (loss: 0.19386455416679382, acc: 0.9052631855010986)
[2025-01-06 01:55:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:31][root][INFO] - Training Epoch: 10/10, step 231/574 completed (loss: 0.17973293364048004, acc: 0.9444444179534912)
[2025-01-06 01:55:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:32][root][INFO] - Training Epoch: 10/10, step 232/574 completed (loss: 0.2643616199493408, acc: 0.9111111164093018)
[2025-01-06 01:55:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:32][root][INFO] - Training Epoch: 10/10, step 233/574 completed (loss: 0.38760921359062195, acc: 0.857798159122467)
[2025-01-06 01:55:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:33][root][INFO] - Training Epoch: 10/10, step 234/574 completed (loss: 0.3682307302951813, acc: 0.8999999761581421)
[2025-01-06 01:55:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:33][root][INFO] - Training Epoch: 10/10, step 235/574 completed (loss: 0.011793112382292747, acc: 1.0)
[2025-01-06 01:55:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:33][root][INFO] - Training Epoch: 10/10, step 236/574 completed (loss: 0.004458076786249876, acc: 1.0)
[2025-01-06 01:55:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:34][root][INFO] - Training Epoch: 10/10, step 237/574 completed (loss: 0.03917492926120758, acc: 0.9545454382896423)
[2025-01-06 01:55:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:34][root][INFO] - Training Epoch: 10/10, step 238/574 completed (loss: 0.10975364595651627, acc: 0.9629629850387573)
[2025-01-06 01:55:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:34][root][INFO] - Training Epoch: 10/10, step 239/574 completed (loss: 0.015967819839715958, acc: 1.0)
[2025-01-06 01:55:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:35][root][INFO] - Training Epoch: 10/10, step 240/574 completed (loss: 0.02158300392329693, acc: 1.0)
[2025-01-06 01:55:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:35][root][INFO] - Training Epoch: 10/10, step 241/574 completed (loss: 0.01920510083436966, acc: 1.0)
[2025-01-06 01:55:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:36][root][INFO] - Training Epoch: 10/10, step 242/574 completed (loss: 0.1272139847278595, acc: 0.9354838728904724)
[2025-01-06 01:55:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:36][root][INFO] - Training Epoch: 10/10, step 243/574 completed (loss: 0.12041544914245605, acc: 0.9772727489471436)
[2025-01-06 01:55:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:37][root][INFO] - Training Epoch: 10/10, step 244/574 completed (loss: 0.00037418221472762525, acc: 1.0)
[2025-01-06 01:55:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:37][root][INFO] - Training Epoch: 10/10, step 245/574 completed (loss: 0.002027768874540925, acc: 1.0)
[2025-01-06 01:55:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:37][root][INFO] - Training Epoch: 10/10, step 246/574 completed (loss: 0.01152857020497322, acc: 1.0)
[2025-01-06 01:55:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:38][root][INFO] - Training Epoch: 10/10, step 247/574 completed (loss: 0.1183541864156723, acc: 0.949999988079071)
[2025-01-06 01:55:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:38][root][INFO] - Training Epoch: 10/10, step 248/574 completed (loss: 0.01839873008430004, acc: 1.0)
[2025-01-06 01:55:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:38][root][INFO] - Training Epoch: 10/10, step 249/574 completed (loss: 0.009552511386573315, acc: 1.0)
[2025-01-06 01:55:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:39][root][INFO] - Training Epoch: 10/10, step 250/574 completed (loss: 0.0014830719446763396, acc: 1.0)
[2025-01-06 01:55:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:39][root][INFO] - Training Epoch: 10/10, step 251/574 completed (loss: 0.014495547860860825, acc: 1.0)
[2025-01-06 01:55:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:39][root][INFO] - Training Epoch: 10/10, step 252/574 completed (loss: 0.08738905936479568, acc: 0.9512194991111755)
[2025-01-06 01:55:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:40][root][INFO] - Training Epoch: 10/10, step 253/574 completed (loss: 0.044749654829502106, acc: 0.9599999785423279)
[2025-01-06 01:55:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:40][root][INFO] - Training Epoch: 10/10, step 254/574 completed (loss: 0.0002195485431002453, acc: 1.0)
[2025-01-06 01:55:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:40][root][INFO] - Training Epoch: 10/10, step 255/574 completed (loss: 0.0013091200962662697, acc: 1.0)
[2025-01-06 01:55:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:41][root][INFO] - Training Epoch: 10/10, step 256/574 completed (loss: 0.018978960812091827, acc: 0.9824561476707458)
[2025-01-06 01:55:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:41][root][INFO] - Training Epoch: 10/10, step 257/574 completed (loss: 0.005642104893922806, acc: 1.0)
[2025-01-06 01:55:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:42][root][INFO] - Training Epoch: 10/10, step 258/574 completed (loss: 0.004451964050531387, acc: 1.0)
[2025-01-06 01:55:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:42][root][INFO] - Training Epoch: 10/10, step 259/574 completed (loss: 0.029995283111929893, acc: 0.9811320900917053)
[2025-01-06 01:55:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:43][root][INFO] - Training Epoch: 10/10, step 260/574 completed (loss: 0.06606677174568176, acc: 0.9916666746139526)
[2025-01-06 01:55:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:43][root][INFO] - Training Epoch: 10/10, step 261/574 completed (loss: 0.006497444584965706, acc: 1.0)
[2025-01-06 01:55:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:43][root][INFO] - Training Epoch: 10/10, step 262/574 completed (loss: 0.08795583993196487, acc: 0.9354838728904724)
[2025-01-06 01:55:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:44][root][INFO] - Training Epoch: 10/10, step 263/574 completed (loss: 0.09305752068758011, acc: 0.9599999785423279)
[2025-01-06 01:55:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:44][root][INFO] - Training Epoch: 10/10, step 264/574 completed (loss: 0.04464079439640045, acc: 0.9791666865348816)
[2025-01-06 01:55:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:45][root][INFO] - Training Epoch: 10/10, step 265/574 completed (loss: 0.2412308007478714, acc: 0.8960000276565552)
[2025-01-06 01:55:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:45][root][INFO] - Training Epoch: 10/10, step 266/574 completed (loss: 0.25767946243286133, acc: 0.932584285736084)
[2025-01-06 01:55:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:45][root][INFO] - Training Epoch: 10/10, step 267/574 completed (loss: 0.06851757317781448, acc: 0.9729729890823364)
[2025-01-06 01:55:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:55:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:15][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.3542, device='cuda:0') eval_epoch_loss=tensor(0.8562, device='cuda:0') eval_epoch_acc=tensor(0.8367, device='cuda:0')
[2025-01-06 01:56:15][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:56:15][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:56:15][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_10_step_268_loss_0.8561995625495911/model.pt
[2025-01-06 01:56:15][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:56:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:16][root][INFO] - Training Epoch: 10/10, step 268/574 completed (loss: 0.04974689334630966, acc: 0.982758641242981)
[2025-01-06 01:56:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:16][root][INFO] - Training Epoch: 10/10, step 269/574 completed (loss: 0.01180043164640665, acc: 1.0)
[2025-01-06 01:56:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:16][root][INFO] - Training Epoch: 10/10, step 270/574 completed (loss: 0.08873064070940018, acc: 0.9545454382896423)
[2025-01-06 01:56:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:17][root][INFO] - Training Epoch: 10/10, step 271/574 completed (loss: 0.10582813620567322, acc: 0.96875)
[2025-01-06 01:56:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:17][root][INFO] - Training Epoch: 10/10, step 272/574 completed (loss: 0.002067785244435072, acc: 1.0)
[2025-01-06 01:56:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:17][root][INFO] - Training Epoch: 10/10, step 273/574 completed (loss: 0.07950938493013382, acc: 0.9666666388511658)
[2025-01-06 01:56:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:18][root][INFO] - Training Epoch: 10/10, step 274/574 completed (loss: 0.003057696158066392, acc: 1.0)
[2025-01-06 01:56:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:18][root][INFO] - Training Epoch: 10/10, step 275/574 completed (loss: 0.001121282228268683, acc: 1.0)
[2025-01-06 01:56:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:19][root][INFO] - Training Epoch: 10/10, step 276/574 completed (loss: 0.002997845644131303, acc: 1.0)
[2025-01-06 01:56:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:19][root][INFO] - Training Epoch: 10/10, step 277/574 completed (loss: 0.035964980721473694, acc: 0.9599999785423279)
[2025-01-06 01:56:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:19][root][INFO] - Training Epoch: 10/10, step 278/574 completed (loss: 0.003547720843926072, acc: 1.0)
[2025-01-06 01:56:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:20][root][INFO] - Training Epoch: 10/10, step 279/574 completed (loss: 0.008053376339375973, acc: 1.0)
[2025-01-06 01:56:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:20][root][INFO] - Training Epoch: 10/10, step 280/574 completed (loss: 0.016169309616088867, acc: 1.0)
[2025-01-06 01:56:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:20][root][INFO] - Training Epoch: 10/10, step 281/574 completed (loss: 0.053867436945438385, acc: 0.9759036302566528)
[2025-01-06 01:56:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:21][root][INFO] - Training Epoch: 10/10, step 282/574 completed (loss: 0.14608080685138702, acc: 0.9351851940155029)
[2025-01-06 01:56:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:21][root][INFO] - Training Epoch: 10/10, step 283/574 completed (loss: 0.0077332318760454655, acc: 1.0)
[2025-01-06 01:56:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:21][root][INFO] - Training Epoch: 10/10, step 284/574 completed (loss: 0.03847416117787361, acc: 0.970588207244873)
[2025-01-06 01:56:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:22][root][INFO] - Training Epoch: 10/10, step 285/574 completed (loss: 0.24897277355194092, acc: 0.949999988079071)
[2025-01-06 01:56:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:22][root][INFO] - Training Epoch: 10/10, step 286/574 completed (loss: 0.03347201645374298, acc: 0.984375)
[2025-01-06 01:56:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:22][root][INFO] - Training Epoch: 10/10, step 287/574 completed (loss: 0.032881755381822586, acc: 0.9919999837875366)
[2025-01-06 01:56:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:23][root][INFO] - Training Epoch: 10/10, step 288/574 completed (loss: 0.010809012688696384, acc: 1.0)
[2025-01-06 01:56:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:23][root][INFO] - Training Epoch: 10/10, step 289/574 completed (loss: 0.032206155359745026, acc: 0.9875776171684265)
[2025-01-06 01:56:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:24][root][INFO] - Training Epoch: 10/10, step 290/574 completed (loss: 0.09078062325716019, acc: 0.969072163105011)
[2025-01-06 01:56:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:24][root][INFO] - Training Epoch: 10/10, step 291/574 completed (loss: 0.004222743678838015, acc: 1.0)
[2025-01-06 01:56:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:24][root][INFO] - Training Epoch: 10/10, step 292/574 completed (loss: 0.009676818735897541, acc: 1.0)
[2025-01-06 01:56:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:25][root][INFO] - Training Epoch: 10/10, step 293/574 completed (loss: 0.0037401681765913963, acc: 1.0)
[2025-01-06 01:56:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:25][root][INFO] - Training Epoch: 10/10, step 294/574 completed (loss: 0.12109221518039703, acc: 0.9818181991577148)
[2025-01-06 01:56:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:26][root][INFO] - Training Epoch: 10/10, step 295/574 completed (loss: 0.10793378949165344, acc: 0.9484536051750183)
[2025-01-06 01:56:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:26][root][INFO] - Training Epoch: 10/10, step 296/574 completed (loss: 0.04776451364159584, acc: 0.982758641242981)
[2025-01-06 01:56:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:26][root][INFO] - Training Epoch: 10/10, step 297/574 completed (loss: 0.012950396165251732, acc: 1.0)
[2025-01-06 01:56:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:27][root][INFO] - Training Epoch: 10/10, step 298/574 completed (loss: 0.008272063918411732, acc: 1.0)
[2025-01-06 01:56:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:27][root][INFO] - Training Epoch: 10/10, step 299/574 completed (loss: 0.06283443421125412, acc: 0.9821428656578064)
[2025-01-06 01:56:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:27][root][INFO] - Training Epoch: 10/10, step 300/574 completed (loss: 0.026062536984682083, acc: 1.0)
[2025-01-06 01:56:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:28][root][INFO] - Training Epoch: 10/10, step 301/574 completed (loss: 0.030744444578886032, acc: 0.9811320900917053)
[2025-01-06 01:56:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:28][root][INFO] - Training Epoch: 10/10, step 302/574 completed (loss: 0.0011964469449594617, acc: 1.0)
[2025-01-06 01:56:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:29][root][INFO] - Training Epoch: 10/10, step 303/574 completed (loss: 0.02024485170841217, acc: 1.0)
[2025-01-06 01:56:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:29][root][INFO] - Training Epoch: 10/10, step 304/574 completed (loss: 0.0009142106864601374, acc: 1.0)
[2025-01-06 01:56:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:29][root][INFO] - Training Epoch: 10/10, step 305/574 completed (loss: 0.08504187315702438, acc: 0.9836065769195557)
[2025-01-06 01:56:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:30][root][INFO] - Training Epoch: 10/10, step 306/574 completed (loss: 0.12546692788600922, acc: 0.9333333373069763)
[2025-01-06 01:56:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:30][root][INFO] - Training Epoch: 10/10, step 307/574 completed (loss: 0.0002289762196596712, acc: 1.0)
[2025-01-06 01:56:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:30][root][INFO] - Training Epoch: 10/10, step 308/574 completed (loss: 0.015814315527677536, acc: 1.0)
[2025-01-06 01:56:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:31][root][INFO] - Training Epoch: 10/10, step 309/574 completed (loss: 0.0145728075876832, acc: 1.0)
[2025-01-06 01:56:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:31][root][INFO] - Training Epoch: 10/10, step 310/574 completed (loss: 0.031179962679743767, acc: 0.9879518151283264)
[2025-01-06 01:56:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:31][root][INFO] - Training Epoch: 10/10, step 311/574 completed (loss: 0.012639830820262432, acc: 1.0)
[2025-01-06 01:56:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:32][root][INFO] - Training Epoch: 10/10, step 312/574 completed (loss: 0.03820883855223656, acc: 0.9897959232330322)
[2025-01-06 01:56:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:32][root][INFO] - Training Epoch: 10/10, step 313/574 completed (loss: 0.0003852272348012775, acc: 1.0)
[2025-01-06 01:56:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:32][root][INFO] - Training Epoch: 10/10, step 314/574 completed (loss: 0.046146322041749954, acc: 0.9583333134651184)
[2025-01-06 01:56:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:33][root][INFO] - Training Epoch: 10/10, step 315/574 completed (loss: 0.0030404343269765377, acc: 1.0)
[2025-01-06 01:56:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:33][root][INFO] - Training Epoch: 10/10, step 316/574 completed (loss: 0.03117748536169529, acc: 1.0)
[2025-01-06 01:56:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:33][root][INFO] - Training Epoch: 10/10, step 317/574 completed (loss: 0.016267042607069016, acc: 1.0)
[2025-01-06 01:56:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:34][root][INFO] - Training Epoch: 10/10, step 318/574 completed (loss: 0.06306000798940659, acc: 0.9615384340286255)
[2025-01-06 01:56:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:34][root][INFO] - Training Epoch: 10/10, step 319/574 completed (loss: 0.10138452053070068, acc: 0.9777777791023254)
[2025-01-06 01:56:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:34][root][INFO] - Training Epoch: 10/10, step 320/574 completed (loss: 0.01746898703277111, acc: 1.0)
[2025-01-06 01:56:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:35][root][INFO] - Training Epoch: 10/10, step 321/574 completed (loss: 0.01365992147475481, acc: 1.0)
[2025-01-06 01:56:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:35][root][INFO] - Training Epoch: 10/10, step 322/574 completed (loss: 0.20510746538639069, acc: 0.9259259104728699)
[2025-01-06 01:56:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:35][root][INFO] - Training Epoch: 10/10, step 323/574 completed (loss: 0.16716523468494415, acc: 0.9714285731315613)
[2025-01-06 01:56:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:36][root][INFO] - Training Epoch: 10/10, step 324/574 completed (loss: 0.10749946534633636, acc: 0.9743589758872986)
[2025-01-06 01:56:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:36][root][INFO] - Training Epoch: 10/10, step 325/574 completed (loss: 0.08034245669841766, acc: 0.9756097793579102)
[2025-01-06 01:56:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:37][root][INFO] - Training Epoch: 10/10, step 326/574 completed (loss: 0.013786116614937782, acc: 1.0)
[2025-01-06 01:56:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:37][root][INFO] - Training Epoch: 10/10, step 327/574 completed (loss: 0.007365919183939695, acc: 1.0)
[2025-01-06 01:56:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:37][root][INFO] - Training Epoch: 10/10, step 328/574 completed (loss: 0.015598916448652744, acc: 1.0)
[2025-01-06 01:56:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:38][root][INFO] - Training Epoch: 10/10, step 329/574 completed (loss: 0.0009133673156611621, acc: 1.0)
[2025-01-06 01:56:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:38][root][INFO] - Training Epoch: 10/10, step 330/574 completed (loss: 0.001926404656842351, acc: 1.0)
[2025-01-06 01:56:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:38][root][INFO] - Training Epoch: 10/10, step 331/574 completed (loss: 0.0032712318934500217, acc: 1.0)
[2025-01-06 01:56:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:39][root][INFO] - Training Epoch: 10/10, step 332/574 completed (loss: 0.013847194612026215, acc: 1.0)
[2025-01-06 01:56:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:39][root][INFO] - Training Epoch: 10/10, step 333/574 completed (loss: 0.0038282047025859356, acc: 1.0)
[2025-01-06 01:56:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:39][root][INFO] - Training Epoch: 10/10, step 334/574 completed (loss: 0.0005897677619941533, acc: 1.0)
[2025-01-06 01:56:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:40][root][INFO] - Training Epoch: 10/10, step 335/574 completed (loss: 0.004404235631227493, acc: 1.0)
[2025-01-06 01:56:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:40][root][INFO] - Training Epoch: 10/10, step 336/574 completed (loss: 0.28314635157585144, acc: 0.9399999976158142)
[2025-01-06 01:56:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:41][root][INFO] - Training Epoch: 10/10, step 337/574 completed (loss: 0.15636380016803741, acc: 0.977011501789093)
[2025-01-06 01:56:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:41][root][INFO] - Training Epoch: 10/10, step 338/574 completed (loss: 0.22883819043636322, acc: 0.936170220375061)
[2025-01-06 01:56:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:41][root][INFO] - Training Epoch: 10/10, step 339/574 completed (loss: 0.0689842700958252, acc: 0.9759036302566528)
[2025-01-06 01:56:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:42][root][INFO] - Training Epoch: 10/10, step 340/574 completed (loss: 0.0021729934960603714, acc: 1.0)
[2025-01-06 01:56:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:42][root][INFO] - Training Epoch: 10/10, step 341/574 completed (loss: 0.07106254994869232, acc: 0.9743589758872986)
[2025-01-06 01:56:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:42][root][INFO] - Training Epoch: 10/10, step 342/574 completed (loss: 0.009261559695005417, acc: 1.0)
[2025-01-06 01:56:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:43][root][INFO] - Training Epoch: 10/10, step 343/574 completed (loss: 0.02696065790951252, acc: 1.0)
[2025-01-06 01:56:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:43][root][INFO] - Training Epoch: 10/10, step 344/574 completed (loss: 0.00443960539996624, acc: 1.0)
[2025-01-06 01:56:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:43][root][INFO] - Training Epoch: 10/10, step 345/574 completed (loss: 0.015508615411818027, acc: 1.0)
[2025-01-06 01:56:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:44][root][INFO] - Training Epoch: 10/10, step 346/574 completed (loss: 0.032715361565351486, acc: 0.9850746393203735)
[2025-01-06 01:56:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:44][root][INFO] - Training Epoch: 10/10, step 347/574 completed (loss: 0.0026455537881702185, acc: 1.0)
[2025-01-06 01:56:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:44][root][INFO] - Training Epoch: 10/10, step 348/574 completed (loss: 0.026805350556969643, acc: 1.0)
[2025-01-06 01:56:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:45][root][INFO] - Training Epoch: 10/10, step 349/574 completed (loss: 0.004405782092362642, acc: 1.0)
[2025-01-06 01:56:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:45][root][INFO] - Training Epoch: 10/10, step 350/574 completed (loss: 0.012688903138041496, acc: 1.0)
[2025-01-06 01:56:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:46][root][INFO] - Training Epoch: 10/10, step 351/574 completed (loss: 0.031018737703561783, acc: 0.9743589758872986)
[2025-01-06 01:56:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:46][root][INFO] - Training Epoch: 10/10, step 352/574 completed (loss: 0.024238867685198784, acc: 1.0)
[2025-01-06 01:56:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:46][root][INFO] - Training Epoch: 10/10, step 353/574 completed (loss: 0.00025102385552600026, acc: 1.0)
[2025-01-06 01:56:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:47][root][INFO] - Training Epoch: 10/10, step 354/574 completed (loss: 0.11678077280521393, acc: 0.9615384340286255)
[2025-01-06 01:56:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:47][root][INFO] - Training Epoch: 10/10, step 355/574 completed (loss: 0.07717341929674149, acc: 0.9670329689979553)
[2025-01-06 01:56:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:48][root][INFO] - Training Epoch: 10/10, step 356/574 completed (loss: 0.09351008385419846, acc: 0.9652174115180969)
[2025-01-06 01:56:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:48][root][INFO] - Training Epoch: 10/10, step 357/574 completed (loss: 0.03967699781060219, acc: 0.989130437374115)
[2025-01-06 01:56:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:48][root][INFO] - Training Epoch: 10/10, step 358/574 completed (loss: 0.008762581273913383, acc: 1.0)
[2025-01-06 01:56:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:49][root][INFO] - Training Epoch: 10/10, step 359/574 completed (loss: 0.00033042370341718197, acc: 1.0)
[2025-01-06 01:56:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:49][root][INFO] - Training Epoch: 10/10, step 360/574 completed (loss: 0.04666519537568092, acc: 0.9615384340286255)
[2025-01-06 01:56:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:49][root][INFO] - Training Epoch: 10/10, step 361/574 completed (loss: 0.0341765433549881, acc: 0.9756097793579102)
[2025-01-06 01:56:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:50][root][INFO] - Training Epoch: 10/10, step 362/574 completed (loss: 0.020853325724601746, acc: 1.0)
[2025-01-06 01:56:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:50][root][INFO] - Training Epoch: 10/10, step 363/574 completed (loss: 0.0028938674367964268, acc: 1.0)
[2025-01-06 01:56:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:50][root][INFO] - Training Epoch: 10/10, step 364/574 completed (loss: 0.001574972178786993, acc: 1.0)
[2025-01-06 01:56:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:51][root][INFO] - Training Epoch: 10/10, step 365/574 completed (loss: 0.0179872028529644, acc: 1.0)
[2025-01-06 01:56:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:51][root][INFO] - Training Epoch: 10/10, step 366/574 completed (loss: 0.00028862591716460884, acc: 1.0)
[2025-01-06 01:56:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:51][root][INFO] - Training Epoch: 10/10, step 367/574 completed (loss: 0.31579816341400146, acc: 0.95652174949646)
[2025-01-06 01:56:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:52][root][INFO] - Training Epoch: 10/10, step 368/574 completed (loss: 0.0038414180744439363, acc: 1.0)
[2025-01-06 01:56:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:52][root][INFO] - Training Epoch: 10/10, step 369/574 completed (loss: 0.007435758598148823, acc: 1.0)
[2025-01-06 01:56:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:53][root][INFO] - Training Epoch: 10/10, step 370/574 completed (loss: 0.16482584178447723, acc: 0.9454545378684998)
[2025-01-06 01:56:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:54][root][INFO] - Training Epoch: 10/10, step 371/574 completed (loss: 0.30453428626060486, acc: 0.9150943160057068)
[2025-01-06 01:56:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:54][root][INFO] - Training Epoch: 10/10, step 372/574 completed (loss: 0.08333894610404968, acc: 0.9666666388511658)
[2025-01-06 01:56:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:54][root][INFO] - Training Epoch: 10/10, step 373/574 completed (loss: 0.0032566965091973543, acc: 1.0)
[2025-01-06 01:56:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:55][root][INFO] - Training Epoch: 10/10, step 374/574 completed (loss: 0.03983330726623535, acc: 0.9714285731315613)
[2025-01-06 01:56:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:55][root][INFO] - Training Epoch: 10/10, step 375/574 completed (loss: 0.00016075893654488027, acc: 1.0)
[2025-01-06 01:56:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:55][root][INFO] - Training Epoch: 10/10, step 376/574 completed (loss: 0.0005186046473681927, acc: 1.0)
[2025-01-06 01:56:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:56][root][INFO] - Training Epoch: 10/10, step 377/574 completed (loss: 0.1705799102783203, acc: 0.9583333134651184)
[2025-01-06 01:56:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:56][root][INFO] - Training Epoch: 10/10, step 378/574 completed (loss: 0.0006649066926911473, acc: 1.0)
[2025-01-06 01:56:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:57][root][INFO] - Training Epoch: 10/10, step 379/574 completed (loss: 0.22249890863895416, acc: 0.9580838084220886)
[2025-01-06 01:56:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:57][root][INFO] - Training Epoch: 10/10, step 380/574 completed (loss: 0.09267041087150574, acc: 0.9624060392379761)
[2025-01-06 01:56:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:58][root][INFO] - Training Epoch: 10/10, step 381/574 completed (loss: 0.4234820306301117, acc: 0.866310179233551)
[2025-01-06 01:56:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:59][root][INFO] - Training Epoch: 10/10, step 382/574 completed (loss: 0.13252736628055573, acc: 0.9729729890823364)
[2025-01-06 01:56:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:59][root][INFO] - Training Epoch: 10/10, step 383/574 completed (loss: 0.22380460798740387, acc: 0.9285714030265808)
[2025-01-06 01:56:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:56:59][root][INFO] - Training Epoch: 10/10, step 384/574 completed (loss: 0.042549069970846176, acc: 1.0)
[2025-01-06 01:57:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:00][root][INFO] - Training Epoch: 10/10, step 385/574 completed (loss: 0.0016023332718759775, acc: 1.0)
[2025-01-06 01:57:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:00][root][INFO] - Training Epoch: 10/10, step 386/574 completed (loss: 0.00043978754547424614, acc: 1.0)
[2025-01-06 01:57:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:00][root][INFO] - Training Epoch: 10/10, step 387/574 completed (loss: 0.015559215098619461, acc: 1.0)
[2025-01-06 01:57:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:01][root][INFO] - Training Epoch: 10/10, step 388/574 completed (loss: 0.0019108392298221588, acc: 1.0)
[2025-01-06 01:57:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:01][root][INFO] - Training Epoch: 10/10, step 389/574 completed (loss: 0.0035348087549209595, acc: 1.0)
[2025-01-06 01:57:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:02][root][INFO] - Training Epoch: 10/10, step 390/574 completed (loss: 0.05951718986034393, acc: 0.9523809552192688)
[2025-01-06 01:57:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:02][root][INFO] - Training Epoch: 10/10, step 391/574 completed (loss: 0.4749755859375, acc: 0.8518518805503845)
[2025-01-06 01:57:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:02][root][INFO] - Training Epoch: 10/10, step 392/574 completed (loss: 0.13958413898944855, acc: 0.9514563083648682)
[2025-01-06 01:57:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:03][root][INFO] - Training Epoch: 10/10, step 393/574 completed (loss: 0.24260257184505463, acc: 0.9485294222831726)
[2025-01-06 01:57:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:03][root][INFO] - Training Epoch: 10/10, step 394/574 completed (loss: 0.17908351123332977, acc: 0.9399999976158142)
[2025-01-06 01:57:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:04][root][INFO] - Training Epoch: 10/10, step 395/574 completed (loss: 0.10348773747682571, acc: 0.9652777910232544)
[2025-01-06 01:57:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:04][root][INFO] - Training Epoch: 10/10, step 396/574 completed (loss: 0.35274767875671387, acc: 0.9069767594337463)
[2025-01-06 01:57:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:04][root][INFO] - Training Epoch: 10/10, step 397/574 completed (loss: 0.05196629464626312, acc: 0.9583333134651184)
[2025-01-06 01:57:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:05][root][INFO] - Training Epoch: 10/10, step 398/574 completed (loss: 0.07269832491874695, acc: 0.9534883499145508)
[2025-01-06 01:57:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:05][root][INFO] - Training Epoch: 10/10, step 399/574 completed (loss: 0.03359493613243103, acc: 0.9599999785423279)
[2025-01-06 01:57:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:06][root][INFO] - Training Epoch: 10/10, step 400/574 completed (loss: 0.06459391862154007, acc: 0.970588207244873)
[2025-01-06 01:57:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:06][root][INFO] - Training Epoch: 10/10, step 401/574 completed (loss: 0.0698658674955368, acc: 0.9733333587646484)
[2025-01-06 01:57:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:06][root][INFO] - Training Epoch: 10/10, step 402/574 completed (loss: 0.09570543467998505, acc: 0.9696969985961914)
[2025-01-06 01:57:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:07][root][INFO] - Training Epoch: 10/10, step 403/574 completed (loss: 0.17522434890270233, acc: 0.939393937587738)
[2025-01-06 01:57:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:07][root][INFO] - Training Epoch: 10/10, step 404/574 completed (loss: 0.0022212257608771324, acc: 1.0)
[2025-01-06 01:57:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:07][root][INFO] - Training Epoch: 10/10, step 405/574 completed (loss: 0.0009897778509184718, acc: 1.0)
[2025-01-06 01:57:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:08][root][INFO] - Training Epoch: 10/10, step 406/574 completed (loss: 0.0017001793021336198, acc: 1.0)
[2025-01-06 01:57:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:08][root][INFO] - Training Epoch: 10/10, step 407/574 completed (loss: 0.0009872573427855968, acc: 1.0)
[2025-01-06 01:57:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:08][root][INFO] - Training Epoch: 10/10, step 408/574 completed (loss: 0.014955838210880756, acc: 1.0)
[2025-01-06 01:57:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:09][root][INFO] - Training Epoch: 10/10, step 409/574 completed (loss: 0.001880465541034937, acc: 1.0)
[2025-01-06 01:57:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:09][root][INFO] - Training Epoch: 10/10, step 410/574 completed (loss: 0.00420594634488225, acc: 1.0)
[2025-01-06 01:57:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:40][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.3421, device='cuda:0') eval_epoch_loss=tensor(0.8510, device='cuda:0') eval_epoch_acc=tensor(0.8320, device='cuda:0')
[2025-01-06 01:57:40][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:57:40][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:57:40][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_10_step_411_loss_0.8510420918464661/model.pt
[2025-01-06 01:57:40][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:57:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:40][root][INFO] - Training Epoch: 10/10, step 411/574 completed (loss: 0.002109107095748186, acc: 1.0)
[2025-01-06 01:57:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:41][root][INFO] - Training Epoch: 10/10, step 412/574 completed (loss: 0.004000147804617882, acc: 1.0)
[2025-01-06 01:57:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:41][root][INFO] - Training Epoch: 10/10, step 413/574 completed (loss: 0.016528796404600143, acc: 1.0)
[2025-01-06 01:57:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:41][root][INFO] - Training Epoch: 10/10, step 414/574 completed (loss: 0.01021659281104803, acc: 1.0)
[2025-01-06 01:57:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:42][root][INFO] - Training Epoch: 10/10, step 415/574 completed (loss: 0.053619105368852615, acc: 0.9803921580314636)
[2025-01-06 01:57:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:42][root][INFO] - Training Epoch: 10/10, step 416/574 completed (loss: 0.19456185400485992, acc: 0.9230769276618958)
[2025-01-06 01:57:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:42][root][INFO] - Training Epoch: 10/10, step 417/574 completed (loss: 0.010898488573729992, acc: 1.0)
[2025-01-06 01:57:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:43][root][INFO] - Training Epoch: 10/10, step 418/574 completed (loss: 0.038052696734666824, acc: 0.9750000238418579)
[2025-01-06 01:57:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:43][root][INFO] - Training Epoch: 10/10, step 419/574 completed (loss: 0.0013654439244419336, acc: 1.0)
[2025-01-06 01:57:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:43][root][INFO] - Training Epoch: 10/10, step 420/574 completed (loss: 0.002002798253670335, acc: 1.0)
[2025-01-06 01:57:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:44][root][INFO] - Training Epoch: 10/10, step 421/574 completed (loss: 0.00951576791703701, acc: 1.0)
[2025-01-06 01:57:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:44][root][INFO] - Training Epoch: 10/10, step 422/574 completed (loss: 0.27009204030036926, acc: 0.96875)
[2025-01-06 01:57:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:44][root][INFO] - Training Epoch: 10/10, step 423/574 completed (loss: 0.18600144982337952, acc: 0.9166666865348816)
[2025-01-06 01:57:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:45][root][INFO] - Training Epoch: 10/10, step 424/574 completed (loss: 0.00251159630715847, acc: 1.0)
[2025-01-06 01:57:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:45][root][INFO] - Training Epoch: 10/10, step 425/574 completed (loss: 0.13576026260852814, acc: 0.939393937587738)
[2025-01-06 01:57:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:46][root][INFO] - Training Epoch: 10/10, step 426/574 completed (loss: 0.0022060777992010117, acc: 1.0)
[2025-01-06 01:57:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:46][root][INFO] - Training Epoch: 10/10, step 427/574 completed (loss: 0.09111285954713821, acc: 0.9729729890823364)
[2025-01-06 01:57:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:46][root][INFO] - Training Epoch: 10/10, step 428/574 completed (loss: 0.002433589193969965, acc: 1.0)
[2025-01-06 01:57:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:47][root][INFO] - Training Epoch: 10/10, step 429/574 completed (loss: 0.004723924677819014, acc: 1.0)
[2025-01-06 01:57:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:47][root][INFO] - Training Epoch: 10/10, step 430/574 completed (loss: 0.0005540368147194386, acc: 1.0)
[2025-01-06 01:57:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:47][root][INFO] - Training Epoch: 10/10, step 431/574 completed (loss: 0.00027353523182682693, acc: 1.0)
[2025-01-06 01:57:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:48][root][INFO] - Training Epoch: 10/10, step 432/574 completed (loss: 0.21824541687965393, acc: 0.95652174949646)
[2025-01-06 01:57:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:48][root][INFO] - Training Epoch: 10/10, step 433/574 completed (loss: 0.005152626428753138, acc: 1.0)
[2025-01-06 01:57:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:48][root][INFO] - Training Epoch: 10/10, step 434/574 completed (loss: 0.0033042002469301224, acc: 1.0)
[2025-01-06 01:57:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:49][root][INFO] - Training Epoch: 10/10, step 435/574 completed (loss: 0.0009390856721438468, acc: 1.0)
[2025-01-06 01:57:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:49][root][INFO] - Training Epoch: 10/10, step 436/574 completed (loss: 0.0025286227464675903, acc: 1.0)
[2025-01-06 01:57:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:49][root][INFO] - Training Epoch: 10/10, step 437/574 completed (loss: 0.0035489776637405157, acc: 1.0)
[2025-01-06 01:57:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:50][root][INFO] - Training Epoch: 10/10, step 438/574 completed (loss: 0.0003746102738659829, acc: 1.0)
[2025-01-06 01:57:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:50][root][INFO] - Training Epoch: 10/10, step 439/574 completed (loss: 0.010146459564566612, acc: 1.0)
[2025-01-06 01:57:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:51][root][INFO] - Training Epoch: 10/10, step 440/574 completed (loss: 0.0547964908182621, acc: 0.9848484992980957)
[2025-01-06 01:57:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:51][root][INFO] - Training Epoch: 10/10, step 441/574 completed (loss: 0.1959177702665329, acc: 0.9359999895095825)
[2025-01-06 01:57:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:52][root][INFO] - Training Epoch: 10/10, step 442/574 completed (loss: 0.16387560963630676, acc: 0.9516128897666931)
[2025-01-06 01:57:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:52][root][INFO] - Training Epoch: 10/10, step 443/574 completed (loss: 0.1431778371334076, acc: 0.9552238583564758)
[2025-01-06 01:57:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:53][root][INFO] - Training Epoch: 10/10, step 444/574 completed (loss: 0.007753957062959671, acc: 1.0)
[2025-01-06 01:57:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:53][root][INFO] - Training Epoch: 10/10, step 445/574 completed (loss: 0.019235355779528618, acc: 1.0)
[2025-01-06 01:57:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:54][root][INFO] - Training Epoch: 10/10, step 446/574 completed (loss: 0.07700688391923904, acc: 0.95652174949646)
[2025-01-06 01:57:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:54][root][INFO] - Training Epoch: 10/10, step 447/574 completed (loss: 0.025265436619520187, acc: 1.0)
[2025-01-06 01:57:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:54][root][INFO] - Training Epoch: 10/10, step 448/574 completed (loss: 0.018931355327367783, acc: 1.0)
[2025-01-06 01:57:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:55][root][INFO] - Training Epoch: 10/10, step 449/574 completed (loss: 0.017036978155374527, acc: 0.9850746393203735)
[2025-01-06 01:57:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:55][root][INFO] - Training Epoch: 10/10, step 450/574 completed (loss: 0.07191751152276993, acc: 0.9861111044883728)
[2025-01-06 01:57:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:55][root][INFO] - Training Epoch: 10/10, step 451/574 completed (loss: 0.004773770458996296, acc: 1.0)
[2025-01-06 01:57:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:56][root][INFO] - Training Epoch: 10/10, step 452/574 completed (loss: 0.054149381816387177, acc: 0.9871794581413269)
[2025-01-06 01:57:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:56][root][INFO] - Training Epoch: 10/10, step 453/574 completed (loss: 0.023553315550088882, acc: 1.0)
[2025-01-06 01:57:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:56][root][INFO] - Training Epoch: 10/10, step 454/574 completed (loss: 0.04506249353289604, acc: 0.9795918464660645)
[2025-01-06 01:57:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:57][root][INFO] - Training Epoch: 10/10, step 455/574 completed (loss: 0.11869139224290848, acc: 0.9696969985961914)
[2025-01-06 01:57:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:57][root][INFO] - Training Epoch: 10/10, step 456/574 completed (loss: 0.062334172427654266, acc: 0.9793814420700073)
[2025-01-06 01:57:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:57][root][INFO] - Training Epoch: 10/10, step 457/574 completed (loss: 0.005217660218477249, acc: 1.0)
[2025-01-06 01:57:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:58][root][INFO] - Training Epoch: 10/10, step 458/574 completed (loss: 0.048737648874521255, acc: 0.9883720874786377)
[2025-01-06 01:57:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:58][root][INFO] - Training Epoch: 10/10, step 459/574 completed (loss: 0.007335221860557795, acc: 1.0)
[2025-01-06 01:57:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:59][root][INFO] - Training Epoch: 10/10, step 460/574 completed (loss: 0.009782346896827221, acc: 1.0)
[2025-01-06 01:57:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:59][root][INFO] - Training Epoch: 10/10, step 461/574 completed (loss: 0.007317913230508566, acc: 1.0)
[2025-01-06 01:57:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:57:59][root][INFO] - Training Epoch: 10/10, step 462/574 completed (loss: 0.0424436517059803, acc: 0.96875)
[2025-01-06 01:57:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:00][root][INFO] - Training Epoch: 10/10, step 463/574 completed (loss: 0.00670046079903841, acc: 1.0)
[2025-01-06 01:58:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:00][root][INFO] - Training Epoch: 10/10, step 464/574 completed (loss: 0.007721847854554653, acc: 1.0)
[2025-01-06 01:58:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:00][root][INFO] - Training Epoch: 10/10, step 465/574 completed (loss: 0.05105957016348839, acc: 0.976190447807312)
[2025-01-06 01:58:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:01][root][INFO] - Training Epoch: 10/10, step 466/574 completed (loss: 0.13002358376979828, acc: 0.9518072009086609)
[2025-01-06 01:58:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:01][root][INFO] - Training Epoch: 10/10, step 467/574 completed (loss: 0.020584305748343468, acc: 1.0)
[2025-01-06 01:58:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:01][root][INFO] - Training Epoch: 10/10, step 468/574 completed (loss: 0.07880942523479462, acc: 0.9514563083648682)
[2025-01-06 01:58:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:02][root][INFO] - Training Epoch: 10/10, step 469/574 completed (loss: 0.05930594727396965, acc: 0.9837398529052734)
[2025-01-06 01:58:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:02][root][INFO] - Training Epoch: 10/10, step 470/574 completed (loss: 0.006343680899590254, acc: 1.0)
[2025-01-06 01:58:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:02][root][INFO] - Training Epoch: 10/10, step 471/574 completed (loss: 0.01743675023317337, acc: 1.0)
[2025-01-06 01:58:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:03][root][INFO] - Training Epoch: 10/10, step 472/574 completed (loss: 0.09251676499843597, acc: 0.970588207244873)
[2025-01-06 01:58:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:03][root][INFO] - Training Epoch: 10/10, step 473/574 completed (loss: 0.2191888839006424, acc: 0.9170305728912354)
[2025-01-06 01:58:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:04][root][INFO] - Training Epoch: 10/10, step 474/574 completed (loss: 0.058819547295570374, acc: 1.0)
[2025-01-06 01:58:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:04][root][INFO] - Training Epoch: 10/10, step 475/574 completed (loss: 0.04784340038895607, acc: 0.987730085849762)
[2025-01-06 01:58:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:04][root][INFO] - Training Epoch: 10/10, step 476/574 completed (loss: 0.058490313589572906, acc: 0.9784172773361206)
[2025-01-06 01:58:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:05][root][INFO] - Training Epoch: 10/10, step 477/574 completed (loss: 0.14232325553894043, acc: 0.9547738432884216)
[2025-01-06 01:58:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:05][root][INFO] - Training Epoch: 10/10, step 478/574 completed (loss: 0.08375479280948639, acc: 0.9722222089767456)
[2025-01-06 01:58:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:05][root][INFO] - Training Epoch: 10/10, step 479/574 completed (loss: 0.012264139950275421, acc: 1.0)
[2025-01-06 01:58:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:06][root][INFO] - Training Epoch: 10/10, step 480/574 completed (loss: 0.04347110167145729, acc: 0.9629629850387573)
[2025-01-06 01:58:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:06][root][INFO] - Training Epoch: 10/10, step 481/574 completed (loss: 0.0271021518856287, acc: 1.0)
[2025-01-06 01:58:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:06][root][INFO] - Training Epoch: 10/10, step 482/574 completed (loss: 0.006005284376442432, acc: 1.0)
[2025-01-06 01:58:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:07][root][INFO] - Training Epoch: 10/10, step 483/574 completed (loss: 0.0893520638346672, acc: 0.982758641242981)
[2025-01-06 01:58:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:07][root][INFO] - Training Epoch: 10/10, step 484/574 completed (loss: 0.008695847354829311, acc: 1.0)
[2025-01-06 01:58:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:08][root][INFO] - Training Epoch: 10/10, step 485/574 completed (loss: 0.008623845875263214, acc: 1.0)
[2025-01-06 01:58:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:08][root][INFO] - Training Epoch: 10/10, step 486/574 completed (loss: 0.025880327448248863, acc: 1.0)
[2025-01-06 01:58:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:08][root][INFO] - Training Epoch: 10/10, step 487/574 completed (loss: 0.07194610685110092, acc: 0.9523809552192688)
[2025-01-06 01:58:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:09][root][INFO] - Training Epoch: 10/10, step 488/574 completed (loss: 0.013401065021753311, acc: 1.0)
[2025-01-06 01:58:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:09][root][INFO] - Training Epoch: 10/10, step 489/574 completed (loss: 0.05913960933685303, acc: 0.9846153855323792)
[2025-01-06 01:58:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:09][root][INFO] - Training Epoch: 10/10, step 490/574 completed (loss: 0.014709905721247196, acc: 1.0)
[2025-01-06 01:58:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:10][root][INFO] - Training Epoch: 10/10, step 491/574 completed (loss: 0.02228948101401329, acc: 1.0)
[2025-01-06 01:58:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:10][root][INFO] - Training Epoch: 10/10, step 492/574 completed (loss: 0.013465720228850842, acc: 1.0)
[2025-01-06 01:58:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:11][root][INFO] - Training Epoch: 10/10, step 493/574 completed (loss: 0.10733041167259216, acc: 0.9655172228813171)
[2025-01-06 01:58:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:11][root][INFO] - Training Epoch: 10/10, step 494/574 completed (loss: 0.017769604921340942, acc: 1.0)
[2025-01-06 01:58:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:11][root][INFO] - Training Epoch: 10/10, step 495/574 completed (loss: 0.00252496893517673, acc: 1.0)
[2025-01-06 01:58:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:12][root][INFO] - Training Epoch: 10/10, step 496/574 completed (loss: 0.14095909893512726, acc: 0.9553571343421936)
[2025-01-06 01:58:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:12][root][INFO] - Training Epoch: 10/10, step 497/574 completed (loss: 0.0115726413205266, acc: 1.0)
[2025-01-06 01:58:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:12][root][INFO] - Training Epoch: 10/10, step 498/574 completed (loss: 0.07636206597089767, acc: 0.9775280952453613)
[2025-01-06 01:58:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:13][root][INFO] - Training Epoch: 10/10, step 499/574 completed (loss: 0.11364364624023438, acc: 0.9432623982429504)
[2025-01-06 01:58:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:13][root][INFO] - Training Epoch: 10/10, step 500/574 completed (loss: 0.07883641868829727, acc: 0.967391312122345)
[2025-01-06 01:58:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:13][root][INFO] - Training Epoch: 10/10, step 501/574 completed (loss: 0.00013919472985435277, acc: 1.0)
[2025-01-06 01:58:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:14][root][INFO] - Training Epoch: 10/10, step 502/574 completed (loss: 0.0008944747969508171, acc: 1.0)
[2025-01-06 01:58:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:14][root][INFO] - Training Epoch: 10/10, step 503/574 completed (loss: 0.012607288546860218, acc: 1.0)
[2025-01-06 01:58:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:14][root][INFO] - Training Epoch: 10/10, step 504/574 completed (loss: 0.001443596207536757, acc: 1.0)
[2025-01-06 01:58:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:15][root][INFO] - Training Epoch: 10/10, step 505/574 completed (loss: 0.12905873358249664, acc: 0.9622641801834106)
[2025-01-06 01:58:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:15][root][INFO] - Training Epoch: 10/10, step 506/574 completed (loss: 0.14532221853733063, acc: 0.931034505367279)
[2025-01-06 01:58:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:16][root][INFO] - Training Epoch: 10/10, step 507/574 completed (loss: 0.13600145280361176, acc: 0.9459459185600281)
[2025-01-06 01:58:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:16][root][INFO] - Training Epoch: 10/10, step 508/574 completed (loss: 0.08007891476154327, acc: 0.98591548204422)
[2025-01-06 01:58:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:16][root][INFO] - Training Epoch: 10/10, step 509/574 completed (loss: 0.031647514551877975, acc: 1.0)
[2025-01-06 01:58:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:17][root][INFO] - Training Epoch: 10/10, step 510/574 completed (loss: 0.0020125515293329954, acc: 1.0)
[2025-01-06 01:58:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:17][root][INFO] - Training Epoch: 10/10, step 511/574 completed (loss: 0.000721406249795109, acc: 1.0)
[2025-01-06 01:58:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:20][root][INFO] - Training Epoch: 10/10, step 512/574 completed (loss: 0.28609976172447205, acc: 0.9214285612106323)
[2025-01-06 01:58:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:21][root][INFO] - Training Epoch: 10/10, step 513/574 completed (loss: 0.02040845900774002, acc: 0.9920634627342224)
[2025-01-06 01:58:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:21][root][INFO] - Training Epoch: 10/10, step 514/574 completed (loss: 0.00730228703469038, acc: 1.0)
[2025-01-06 01:58:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:21][root][INFO] - Training Epoch: 10/10, step 515/574 completed (loss: 0.0021403480786830187, acc: 1.0)
[2025-01-06 01:58:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:22][root][INFO] - Training Epoch: 10/10, step 516/574 completed (loss: 0.004158306401222944, acc: 1.0)
[2025-01-06 01:58:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:22][root][INFO] - Training Epoch: 10/10, step 517/574 completed (loss: 5.040785254095681e-05, acc: 1.0)
[2025-01-06 01:58:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:23][root][INFO] - Training Epoch: 10/10, step 518/574 completed (loss: 0.0008419225341640413, acc: 1.0)
[2025-01-06 01:58:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:23][root][INFO] - Training Epoch: 10/10, step 519/574 completed (loss: 0.009192727506160736, acc: 1.0)
[2025-01-06 01:58:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:23][root][INFO] - Training Epoch: 10/10, step 520/574 completed (loss: 0.002271819394081831, acc: 1.0)
[2025-01-06 01:58:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:24][root][INFO] - Training Epoch: 10/10, step 521/574 completed (loss: 0.18172813951969147, acc: 0.944915235042572)
[2025-01-06 01:58:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:25][root][INFO] - Training Epoch: 10/10, step 522/574 completed (loss: 0.03302884101867676, acc: 0.9850746393203735)
[2025-01-06 01:58:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:25][root][INFO] - Training Epoch: 10/10, step 523/574 completed (loss: 0.039782747626304626, acc: 0.9927007555961609)
[2025-01-06 01:58:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:26][root][INFO] - Training Epoch: 10/10, step 524/574 completed (loss: 0.13866929709911346, acc: 0.9649999737739563)
[2025-01-06 01:58:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:26][root][INFO] - Training Epoch: 10/10, step 525/574 completed (loss: 0.0017695740098133683, acc: 1.0)
[2025-01-06 01:58:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:26][root][INFO] - Training Epoch: 10/10, step 526/574 completed (loss: 0.05858074128627777, acc: 0.9807692170143127)
[2025-01-06 01:58:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:27][root][INFO] - Training Epoch: 10/10, step 527/574 completed (loss: 0.0016014764551073313, acc: 1.0)
[2025-01-06 01:58:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:27][root][INFO] - Training Epoch: 10/10, step 528/574 completed (loss: 0.026750581339001656, acc: 1.0)
[2025-01-06 01:58:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:27][root][INFO] - Training Epoch: 10/10, step 529/574 completed (loss: 0.1772567182779312, acc: 0.9661017060279846)
[2025-01-06 01:58:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:28][root][INFO] - Training Epoch: 10/10, step 530/574 completed (loss: 0.09581182152032852, acc: 0.930232584476471)
[2025-01-06 01:58:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:28][root][INFO] - Training Epoch: 10/10, step 531/574 completed (loss: 0.028864651918411255, acc: 0.9772727489471436)
[2025-01-06 01:58:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:28][root][INFO] - Training Epoch: 10/10, step 532/574 completed (loss: 0.019939696416258812, acc: 1.0)
[2025-01-06 01:58:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:29][root][INFO] - Training Epoch: 10/10, step 533/574 completed (loss: 0.30958291888237, acc: 0.9545454382896423)
[2025-01-06 01:58:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:29][root][INFO] - Training Epoch: 10/10, step 534/574 completed (loss: 0.042422033846378326, acc: 0.9599999785423279)
[2025-01-06 01:58:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:30][root][INFO] - Training Epoch: 10/10, step 535/574 completed (loss: 0.003029387444257736, acc: 1.0)
[2025-01-06 01:58:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:30][root][INFO] - Training Epoch: 10/10, step 536/574 completed (loss: 0.0006382535793818533, acc: 1.0)
[2025-01-06 01:58:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:30][root][INFO] - Training Epoch: 10/10, step 537/574 completed (loss: 0.021715480834245682, acc: 1.0)
[2025-01-06 01:58:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:31][root][INFO] - Training Epoch: 10/10, step 538/574 completed (loss: 0.07283084839582443, acc: 0.96875)
[2025-01-06 01:58:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:31][root][INFO] - Training Epoch: 10/10, step 539/574 completed (loss: 0.05815725773572922, acc: 0.96875)
[2025-01-06 01:58:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:31][root][INFO] - Training Epoch: 10/10, step 540/574 completed (loss: 0.00432848185300827, acc: 1.0)
[2025-01-06 01:58:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:32][root][INFO] - Training Epoch: 10/10, step 541/574 completed (loss: 0.01606176793575287, acc: 1.0)
[2025-01-06 01:58:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:32][root][INFO] - Training Epoch: 10/10, step 542/574 completed (loss: 0.0007287028711289167, acc: 1.0)
[2025-01-06 01:58:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:32][root][INFO] - Training Epoch: 10/10, step 543/574 completed (loss: 0.0001487671979703009, acc: 1.0)
[2025-01-06 01:58:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:33][root][INFO] - Training Epoch: 10/10, step 544/574 completed (loss: 0.0014621264999732375, acc: 1.0)
[2025-01-06 01:58:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:33][root][INFO] - Training Epoch: 10/10, step 545/574 completed (loss: 0.0007576695643365383, acc: 1.0)
[2025-01-06 01:58:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:33][root][INFO] - Training Epoch: 10/10, step 546/574 completed (loss: 0.0007865703082643449, acc: 1.0)
[2025-01-06 01:58:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:34][root][INFO] - Training Epoch: 10/10, step 547/574 completed (loss: 0.002601534826681018, acc: 1.0)
[2025-01-06 01:58:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:34][root][INFO] - Training Epoch: 10/10, step 548/574 completed (loss: 0.0009483133908361197, acc: 1.0)
[2025-01-06 01:58:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:34][root][INFO] - Training Epoch: 10/10, step 549/574 completed (loss: 0.00015567913942504674, acc: 1.0)
[2025-01-06 01:58:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:35][root][INFO] - Training Epoch: 10/10, step 550/574 completed (loss: 0.0020707580260932446, acc: 1.0)
[2025-01-06 01:58:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:35][root][INFO] - Training Epoch: 10/10, step 551/574 completed (loss: 0.0005306691164150834, acc: 1.0)
[2025-01-06 01:58:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:35][root][INFO] - Training Epoch: 10/10, step 552/574 completed (loss: 0.03954950347542763, acc: 0.9857142567634583)
[2025-01-06 01:58:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:36][root][INFO] - Training Epoch: 10/10, step 553/574 completed (loss: 0.039643991738557816, acc: 0.9927007555961609)
[2025-01-06 01:58:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:58:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:07][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(2.4237, device='cuda:0') eval_epoch_loss=tensor(0.8853, device='cuda:0') eval_epoch_acc=tensor(0.8429, device='cuda:0')
[2025-01-06 01:59:07][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-01-06 01:59:07][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-01-06 01:59:07][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_10_step_554_loss_0.8852918744087219/model.pt
[2025-01-06 01:59:07][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme directory
[2025-01-06 01:59:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:08][root][INFO] - Training Epoch: 10/10, step 554/574 completed (loss: 0.008330055512487888, acc: 1.0)
[2025-01-06 01:59:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:08][root][INFO] - Training Epoch: 10/10, step 555/574 completed (loss: 0.021630913019180298, acc: 0.9928571581840515)
[2025-01-06 01:59:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:08][root][INFO] - Training Epoch: 10/10, step 556/574 completed (loss: 0.07589615881443024, acc: 0.9801324605941772)
[2025-01-06 01:59:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:09][root][INFO] - Training Epoch: 10/10, step 557/574 completed (loss: 0.01634647138416767, acc: 1.0)
[2025-01-06 01:59:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:09][root][INFO] - Training Epoch: 10/10, step 558/574 completed (loss: 0.00013440000475384295, acc: 1.0)
[2025-01-06 01:59:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:09][root][INFO] - Training Epoch: 10/10, step 559/574 completed (loss: 0.019170226529240608, acc: 1.0)
[2025-01-06 01:59:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:10][root][INFO] - Training Epoch: 10/10, step 560/574 completed (loss: 0.0612415112555027, acc: 0.9615384340286255)
[2025-01-06 01:59:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:10][root][INFO] - Training Epoch: 10/10, step 561/574 completed (loss: 0.0008099832921288908, acc: 1.0)
[2025-01-06 01:59:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:10][root][INFO] - Training Epoch: 10/10, step 562/574 completed (loss: 0.03619910404086113, acc: 0.9888888597488403)
[2025-01-06 01:59:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:11][root][INFO] - Training Epoch: 10/10, step 563/574 completed (loss: 0.01364351250231266, acc: 1.0)
[2025-01-06 01:59:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:11][root][INFO] - Training Epoch: 10/10, step 564/574 completed (loss: 0.3654511868953705, acc: 0.9375)
[2025-01-06 01:59:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:11][root][INFO] - Training Epoch: 10/10, step 565/574 completed (loss: 0.01631811633706093, acc: 1.0)
[2025-01-06 01:59:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:12][root][INFO] - Training Epoch: 10/10, step 566/574 completed (loss: 0.029778102412819862, acc: 0.988095223903656)
[2025-01-06 01:59:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:12][root][INFO] - Training Epoch: 10/10, step 567/574 completed (loss: 0.002479459159076214, acc: 1.0)
[2025-01-06 01:59:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:13][root][INFO] - Training Epoch: 10/10, step 568/574 completed (loss: 0.0003889691724907607, acc: 1.0)
[2025-01-06 01:59:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:13][root][INFO] - Training Epoch: 10/10, step 569/574 completed (loss: 0.05322834849357605, acc: 0.9839572310447693)
[2025-01-06 01:59:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:13][root][INFO] - Training Epoch: 10/10, step 570/574 completed (loss: 0.00031588340061716735, acc: 1.0)
[2025-01-06 01:59:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:14][root][INFO] - Training Epoch: 10/10, step 571/574 completed (loss: 0.007834614254534245, acc: 1.0)
[2025-01-06 01:59:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:14][root][INFO] - Training Epoch: 10/10, step 572/574 completed (loss: 0.0667801946401596, acc: 0.9744898080825806)
[2025-01-06 01:59:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:14][root][INFO] - Training Epoch: 10/10, step 573/574 completed (loss: 0.03310428932309151, acc: 0.9874213933944702)
[2025-01-06 01:59:15][slam_llm.utils.train_utils][INFO] - Epoch 10: train_perplexity=1.0792, train_epoch_loss=0.0762, epoch time 354.213001050055s
[2025-01-06 01:59:15][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 13 GB
[2025-01-06 01:59:15][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 20 GB
[2025-01-06 01:59:15][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 13 GB
[2025-01-06 01:59:15][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 28
[2025-01-06 01:59:15][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 6 GB
[2025-01-06 01:59:15][root][INFO] - Key: avg_train_prep, Value: 1.3758896589279175
[2025-01-06 01:59:15][root][INFO] - Key: avg_train_loss, Value: 0.2635788321495056
[2025-01-06 01:59:15][root][INFO] - Key: avg_train_acc, Value: 0.9292565584182739
[2025-01-06 01:59:15][root][INFO] - Key: avg_eval_prep, Value: 2.1604232788085938
[2025-01-06 01:59:15][root][INFO] - Key: avg_eval_loss, Value: 0.7636927962303162
[2025-01-06 01:59:15][root][INFO] - Key: avg_eval_acc, Value: 0.8368111848831177
[2025-01-06 01:59:15][root][INFO] - Key: avg_epoch_time, Value: 356.1918168451637
[2025-01-06 01:59:15][root][INFO] - Key: avg_checkpoint_time, Value: 0.2639585996046662
Selected lowest loss checkpoint: asr_epoch_2_step_284_loss_0.5588149428367615
Checkpoint file: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_2_step_284_loss_0.5588149428367615/model.pt
ckpt_folder /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_2_step_284_loss_0.5588149428367615
[2025-01-06 01:59:40][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'resume_step': 0, 'resume_epoch': 0, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True, 'freeze_encoder2': False, 'save_embedding': False}
[2025-01-06 01:59:40][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': <ShardingStrategy.NO_SHARD: 3>, 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2025-01-06 01:59:40][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'llm_inference_config': 'test_config.json', 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'qformer_layers': 8, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': '', 'identifier': 'ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme'}
[2025-01-06 01:59:42][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2025-01-06 01:59:47][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-01-06 01:59:47][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2025-01-06 01:59:47][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-01-06 01:59:47][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2025-01-06 01:59:52][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-01-06 01:59:52][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2025-01-06 01:59:52][slam_llm.models.slam_model][INFO] - setup peft...
trainable params: 5,636,096 || all params: 1,241,450,496 || trainable%: 0.4539928106807088
[2025-01-06 01:59:52][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-01-06 01:59:52][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2025-01-06 01:59:52][slam_llm.utils.train_utils][INFO] - --> Module linear
[2025-01-06 01:59:52][slam_llm.utils.train_utils][INFO] - --> linear has 14.68416 Million params

[2025-01-06 01:59:52][slam_model_asr.py][INFO] - loading other parts from: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/asr_epoch_2_step_284_loss_0.5588149428367615/model.pt
[2025-01-06 01:59:52][slam_llm.utils.train_utils][INFO] - --> Model asr
[2025-01-06 01:59:52][slam_llm.utils.train_utils][INFO] - --> asr has 20.320256 Million params

[2025-01-06 01:59:54][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': None, 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/psst_phoneme/test.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2025-01-06 01:59:55][root][INFO] - --> Training Set Length = 652
[2025-01-06 01:59:55][root][INFO] - =====================================
Loaded LLM Config: {'max_new_tokens': 200, 'num_beams': 4, 'do_sample': False, 'min_length': 1, 'top_p': 1.0, 'repetition_penalty': 2.0, 'length_penalty': 1.0, 'temperature': 1.0, 'no_repeat_ngram_size': 1}
[2025-01-06 01:59:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 01:59:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:00:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:01:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:02:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:02:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:02:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:02:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:02:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:02:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:02:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:02:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:02:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:02:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:02:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:02:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:02:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:02:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:02:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:02:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:02:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:02:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:02:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:02:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:02:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:02:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:02:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:02:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:02:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:02:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:02:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:02:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:02:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:02:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:02:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:02:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:02:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:02:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:02:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:02:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:02:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:02:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:02:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:02:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:02:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:02:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:02:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:02:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:02:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:02:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:02:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:02:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:02:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:02:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:02:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:02:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:02:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:02:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:03:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:03:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:03:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:03:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:03:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:03:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:03:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:03:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:03:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:03:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:03:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:03:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:03:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:03:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:03:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-01-06 02:03:29][slam_llm.models.slam_model][INFO] - modality encoder
Using folder: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme
Using GT file: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/decode_test_beam4_gt_20250102_010125
Using PRED file: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_phoneme_wavlm_llama32_1b_linear_peft_transfer_psst_phoneme/decode_test_beam4_pred_20250102_010125
Combined WER: 0.6258015267175573

Filtering repeated words...

Found 6 repeated lines in total.
Repeated lines are:
- AH M N EH S AH N IH Z D UH R IH NG AH DH IY DH EY DH EY DH EY DH EY DH EY DH EY DH EY DH EY DH EY DH EY DH EY DH EY DH EY DH EY DH EY DH EY DH EY DH EY DH EY DH EY DH EY DH EY DH EY DH EY DH EY DH EY DH EY DH EY DH EY DH EY DH EY DH EY DH EY DH EY DH EY DH EY DH EY DH EY DH EY DH EY DH EY DH EY DH EY DH EY DH EY DH EY DH EY DH EY DH EY DH EY DH EY DH EY DH EY DH EY DH EY DH EY DH EY DH EY DH EY DH EY DH
- AH Y EH S Y EH S Y EH S Y EH S Y EH S Y EH S Y EH S Y EH S Y EH S Y EH S Y EH S Y EH S Y EH S Y EH S Y EH S Y EH S Y EH S Y EH S Y EH S Y EH S Y EH S Y EH S Y EH S Y EH S Y EH S Y EH S Y EH S Y EH S Y EH S Y EH S Y EH S Y EH S Y EH S Y EH S Y EH S Y EH S Y EH S Y EH S Y EH S Y EH S Y EH S Y EH S Y EH S Y EH S Y EH S Y EH S Y EH S Y EH S Y EH S Y EH S Y EH S Y EH S Y EH S Y EH S Y EH S Y EH S Y EH S Y EH S Y EH S Y EH S Y EH S Y EH S Y EH S Y EH S Y EH S Y EH S Y
- SH S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S
- DH IH S IH T S AH IH T S OW P AA R IY K AH K AH K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R K AA R
- AH T UW S IH T S IH T S IH T S IH T S IH T S IH T S IH T S IH T S IH T S IH T S IH T S IH T S IH T S IH T S IH T S IH T S IH T S IH T S IH T S IH T S IH T S IH T S IH T S IH T S IH T S IH T S IH T S IH T S IH T S IH T S IH T S IH T S IH T S IH T S IH T S IH T S IH T S IH T S IH T S IH T S IH T S IH T S IH T S IH T S IH T S IH T S IH T S IH T S IH T S IH T S IH T S IH T S IH T S IH T S IH T S IH T S IH T S IH T S IH T S IH T S IH T S IH T S IH T S IH T S IH T S IH
- DH IY S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S S
Filtered Combined WER: 0.4701108162946777
