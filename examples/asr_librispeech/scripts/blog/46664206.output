/work/van-speech-nlp/jindaznb/slamenv/bin/python
task_flag: all
train_data_folder: ami
test_data_folder: ami
use_peft: true
seed: 
debug: 
Is test_run? 
freeze_encoder: true
Is save_embedding? false
projector_transfer_learning: true
transfer_data_folder: librispeech-100
llm_inference_config: repetition_penalty
eval_ckpt: best
----------
----------
Final identifier: ami_wavlm_llama32_1b_linear_peft
ckpt_folder: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_wavlm_llama32_1b_linear_peft/asr_epoch_2_step_26970_loss_0.6441742181777954



----- Transfer Learning Information -----
Resume Epoch: 1
Resume Step: 0
Train Data Path: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/librispeech-100/train.jsonl
Validation Data Path: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/librispeech-100/validation.jsonl
Test Data Path: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/librispeech-100/test.jsonl
Identifier: ami_wavlm_llama32_1b_linear_peft_transfer_librispeech-100
Output Directory: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_wavlm_llama32_1b_linear_peft_transfer_librispeech-100
----------------------------------------
----------------------------------------
Resume epoch: 1
Resume step: 0
[2025-02-13 18:50:25][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 2, 'resume_step': 0, 'resume_epoch': 1, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 3000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': True, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_wavlm_llama32_1b_linear_peft_transfer_librispeech-100', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': False, 'freeze_encoder': True, 'freeze_encoder2': True, 'save_embedding': False}
[2025-02-13 18:50:25][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': True, 'sharding_strategy': <ShardingStrategy.NO_SHARD: 3>, 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2025-02-13 18:50:25][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'llm_inference_config': 'repetition_penalty', 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'qformer_layers': 8, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': '', 'identifier': 'ami_wavlm_llama32_1b_linear_peft_transfer_librispeech-100'}
[2025-02-13 18:50:25][root][INFO] - log_config: {'use_wandb': True, 'wandb_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/wandb_log', 'wandb_entity_name': 'jindaz-work', 'wandb_project_name': 'SLAM-LLM', 'wandb_exp_name': 'ami_wavlm_llama32_1b_linear_peft_transfer_librispeech-100', 'log_file': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/log/2025-02-13_18-50-25.txt', 'log_interval': 5}
[2025-02-13 18:50:51][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2025-02-13 18:50:55][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-02-13 18:50:55][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2025-02-13 18:50:55][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-02-13 18:50:55][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2025-02-13 18:51:00][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-02-13 18:51:00][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2025-02-13 18:51:00][slam_llm.models.slam_model][INFO] - setup peft...
trainable params: 5,636,096 || all params: 1,241,450,496 || trainable%: 0.4539928106807088
[2025-02-13 18:51:00][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-02-13 18:51:00][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2025-02-13 18:51:00][slam_llm.utils.train_utils][INFO] - --> Module linear
[2025-02-13 18:51:00][slam_llm.utils.train_utils][INFO] - --> linear has 14.68416 Million params

[2025-02-13 18:51:00][slam_model_asr.py][INFO] - loading other parts from: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_wavlm_llama32_1b_linear_peft/asr_epoch_2_step_26970_loss_0.6441742181777954/model.pt
[2025-02-13 18:51:00][slam_llm.utils.train_utils][INFO] - --> Model asr
[2025-02-13 18:51:00][slam_llm.utils.train_utils][INFO] - --> asr has 20.320256 Million params

[2025-02-13 18:51:02][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/librispeech-100/train.jsonl', 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/librispeech-100/validation.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': False, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2025-02-13 18:51:04][root][INFO] - --> Training Set Length = 28539
[2025-02-13 18:51:04][root][INFO] - --> Validation Set Length = 2703
[2025-02-13 18:51:04][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2025-02-13 18:51:04][slam_llm.utils.config_utils][INFO] - Using batching strategy: custom
[2025-02-13 18:51:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:06][root][INFO] - Training Epoch: 1/2, step 0/7134 completed (loss: 0.9019126892089844, acc: 0.8269230723381042)
[2025-02-13 18:51:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:07][root][INFO] - Training Epoch: 1/2, step 1/7134 completed (loss: 0.8121305108070374, acc: 0.8535031676292419)
[2025-02-13 18:51:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:07][root][INFO] - Training Epoch: 1/2, step 2/7134 completed (loss: 0.6978769302368164, acc: 0.8409090638160706)
[2025-02-13 18:51:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:08][root][INFO] - Training Epoch: 1/2, step 3/7134 completed (loss: 0.6046122908592224, acc: 0.8662790656089783)
[2025-02-13 18:51:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:08][root][INFO] - Training Epoch: 1/2, step 4/7134 completed (loss: 0.9372994303703308, acc: 0.7798742055892944)
[2025-02-13 18:51:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:09][root][INFO] - Training Epoch: 1/2, step 5/7134 completed (loss: 0.7100447416305542, acc: 0.8603351712226868)
[2025-02-13 18:51:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:09][root][INFO] - Training Epoch: 1/2, step 6/7134 completed (loss: 0.4683217406272888, acc: 0.8873239159584045)
[2025-02-13 18:51:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:09][root][INFO] - Training Epoch: 1/2, step 7/7134 completed (loss: 1.079785943031311, acc: 0.7526881694793701)
[2025-02-13 18:51:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:10][root][INFO] - Training Epoch: 1/2, step 8/7134 completed (loss: 0.9150280952453613, acc: 0.8109756112098694)
[2025-02-13 18:51:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:10][root][INFO] - Training Epoch: 1/2, step 9/7134 completed (loss: 0.5421480536460876, acc: 0.8807947039604187)
[2025-02-13 18:51:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:11][root][INFO] - Training Epoch: 1/2, step 10/7134 completed (loss: 0.9131265878677368, acc: 0.7869822382926941)
[2025-02-13 18:51:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:11][root][INFO] - Training Epoch: 1/2, step 11/7134 completed (loss: 1.2755078077316284, acc: 0.7583333253860474)
[2025-02-13 18:51:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:12][root][INFO] - Training Epoch: 1/2, step 12/7134 completed (loss: 0.7725328803062439, acc: 0.8265895843505859)
[2025-02-13 18:51:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:12][root][INFO] - Training Epoch: 1/2, step 13/7134 completed (loss: 0.9134834408760071, acc: 0.7977527976036072)
[2025-02-13 18:51:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:13][root][INFO] - Training Epoch: 1/2, step 14/7134 completed (loss: 0.7446181774139404, acc: 0.8175675868988037)
[2025-02-13 18:51:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:13][root][INFO] - Training Epoch: 1/2, step 15/7134 completed (loss: 0.49015045166015625, acc: 0.8768116235733032)
[2025-02-13 18:51:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:13][root][INFO] - Training Epoch: 1/2, step 16/7134 completed (loss: 0.4658382833003998, acc: 0.9122806787490845)
[2025-02-13 18:51:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:14][root][INFO] - Training Epoch: 1/2, step 17/7134 completed (loss: 1.0793969631195068, acc: 0.7765957713127136)
[2025-02-13 18:51:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:14][root][INFO] - Training Epoch: 1/2, step 18/7134 completed (loss: 0.4430907368659973, acc: 0.891566276550293)
[2025-02-13 18:51:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:15][root][INFO] - Training Epoch: 1/2, step 19/7134 completed (loss: 0.8630238771438599, acc: 0.8092485666275024)
[2025-02-13 18:51:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:15][root][INFO] - Training Epoch: 1/2, step 20/7134 completed (loss: 0.7064003944396973, acc: 0.8295454382896423)
[2025-02-13 18:51:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:16][root][INFO] - Training Epoch: 1/2, step 21/7134 completed (loss: 0.9899234771728516, acc: 0.8011363744735718)
[2025-02-13 18:51:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:16][root][INFO] - Training Epoch: 1/2, step 22/7134 completed (loss: 0.3879227638244629, acc: 0.89570552110672)
[2025-02-13 18:51:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:16][root][INFO] - Training Epoch: 1/2, step 23/7134 completed (loss: 0.5717078447341919, acc: 0.8644067645072937)
[2025-02-13 18:51:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:17][root][INFO] - Training Epoch: 1/2, step 24/7134 completed (loss: 0.8836106061935425, acc: 0.7722222208976746)
[2025-02-13 18:51:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:17][root][INFO] - Training Epoch: 1/2, step 25/7134 completed (loss: 0.9287585020065308, acc: 0.8165680766105652)
[2025-02-13 18:51:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:18][root][INFO] - Training Epoch: 1/2, step 26/7134 completed (loss: 0.7153675556182861, acc: 0.8466257452964783)
[2025-02-13 18:51:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:18][root][INFO] - Training Epoch: 1/2, step 27/7134 completed (loss: 0.4291525185108185, acc: 0.9102563858032227)
[2025-02-13 18:51:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:19][root][INFO] - Training Epoch: 1/2, step 28/7134 completed (loss: 1.3572661876678467, acc: 0.720812201499939)
[2025-02-13 18:51:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:19][root][INFO] - Training Epoch: 1/2, step 29/7134 completed (loss: 0.5765016078948975, acc: 0.8757396340370178)
[2025-02-13 18:51:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:19][root][INFO] - Training Epoch: 1/2, step 30/7134 completed (loss: 1.1281874179840088, acc: 0.7777777910232544)
[2025-02-13 18:51:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:20][root][INFO] - Training Epoch: 1/2, step 31/7134 completed (loss: 0.9071044325828552, acc: 0.8186046481132507)
[2025-02-13 18:51:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:20][root][INFO] - Training Epoch: 1/2, step 32/7134 completed (loss: 1.1823369264602661, acc: 0.7663043737411499)
[2025-02-13 18:51:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:21][root][INFO] - Training Epoch: 1/2, step 33/7134 completed (loss: 1.0272108316421509, acc: 0.7692307829856873)
[2025-02-13 18:51:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:21][root][INFO] - Training Epoch: 1/2, step 34/7134 completed (loss: 0.8547794818878174, acc: 0.8142856955528259)
[2025-02-13 18:51:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:21][root][INFO] - Training Epoch: 1/2, step 35/7134 completed (loss: 0.8632139563560486, acc: 0.8156862854957581)
[2025-02-13 18:51:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:22][root][INFO] - Training Epoch: 1/2, step 36/7134 completed (loss: 0.9717245697975159, acc: 0.7880184054374695)
[2025-02-13 18:51:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:22][root][INFO] - Training Epoch: 1/2, step 37/7134 completed (loss: 0.9167193174362183, acc: 0.8042327761650085)
[2025-02-13 18:51:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:23][root][INFO] - Training Epoch: 1/2, step 38/7134 completed (loss: 0.7899074554443359, acc: 0.8364779949188232)
[2025-02-13 18:51:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:23][root][INFO] - Training Epoch: 1/2, step 39/7134 completed (loss: 0.6062451601028442, acc: 0.847953200340271)
[2025-02-13 18:51:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:24][root][INFO] - Training Epoch: 1/2, step 40/7134 completed (loss: 0.6877654194831848, acc: 0.8392857313156128)
[2025-02-13 18:51:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:24][root][INFO] - Training Epoch: 1/2, step 41/7134 completed (loss: 0.6697558760643005, acc: 0.8829787373542786)
[2025-02-13 18:51:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:25][root][INFO] - Training Epoch: 1/2, step 42/7134 completed (loss: 0.4547341763973236, acc: 0.8888888955116272)
[2025-02-13 18:51:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:25][root][INFO] - Training Epoch: 1/2, step 43/7134 completed (loss: 0.5488417744636536, acc: 0.8212290406227112)
[2025-02-13 18:51:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:25][root][INFO] - Training Epoch: 1/2, step 44/7134 completed (loss: 0.4889908730983734, acc: 0.8983957171440125)
[2025-02-13 18:51:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:26][root][INFO] - Training Epoch: 1/2, step 45/7134 completed (loss: 0.7227822542190552, acc: 0.8349056839942932)
[2025-02-13 18:51:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:26][root][INFO] - Training Epoch: 1/2, step 46/7134 completed (loss: 0.8443659543991089, acc: 0.8081395626068115)
[2025-02-13 18:51:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:27][root][INFO] - Training Epoch: 1/2, step 47/7134 completed (loss: 0.6712507605552673, acc: 0.8379888534545898)
[2025-02-13 18:51:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:27][root][INFO] - Training Epoch: 1/2, step 48/7134 completed (loss: 0.6481905579566956, acc: 0.8502415418624878)
[2025-02-13 18:51:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:27][root][INFO] - Training Epoch: 1/2, step 49/7134 completed (loss: 0.6905813813209534, acc: 0.8294117450714111)
[2025-02-13 18:51:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:28][root][INFO] - Training Epoch: 1/2, step 50/7134 completed (loss: 0.6423398852348328, acc: 0.844660222530365)
[2025-02-13 18:51:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:28][root][INFO] - Training Epoch: 1/2, step 51/7134 completed (loss: 0.6839908957481384, acc: 0.8571428656578064)
[2025-02-13 18:51:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:29][root][INFO] - Training Epoch: 1/2, step 52/7134 completed (loss: 0.35589325428009033, acc: 0.9271523356437683)
[2025-02-13 18:51:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:29][root][INFO] - Training Epoch: 1/2, step 53/7134 completed (loss: 0.4524907171726227, acc: 0.8972973227500916)
[2025-02-13 18:51:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:30][root][INFO] - Training Epoch: 1/2, step 54/7134 completed (loss: 0.5914773941040039, acc: 0.8619047403335571)
[2025-02-13 18:51:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:30][root][INFO] - Training Epoch: 1/2, step 55/7134 completed (loss: 0.7783483266830444, acc: 0.8586956262588501)
[2025-02-13 18:51:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:30][root][INFO] - Training Epoch: 1/2, step 56/7134 completed (loss: 0.44905588030815125, acc: 0.8999999761581421)
[2025-02-13 18:51:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:31][root][INFO] - Training Epoch: 1/2, step 57/7134 completed (loss: 1.319800853729248, acc: 0.7637362480163574)
[2025-02-13 18:51:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:31][root][INFO] - Training Epoch: 1/2, step 58/7134 completed (loss: 1.0128127336502075, acc: 0.8212290406227112)
[2025-02-13 18:51:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:32][root][INFO] - Training Epoch: 1/2, step 59/7134 completed (loss: 0.7084370255470276, acc: 0.8502673506736755)
[2025-02-13 18:51:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:32][root][INFO] - Training Epoch: 1/2, step 60/7134 completed (loss: 0.8856815099716187, acc: 0.797468364238739)
[2025-02-13 18:51:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:33][root][INFO] - Training Epoch: 1/2, step 61/7134 completed (loss: 0.8096670508384705, acc: 0.8435754179954529)
[2025-02-13 18:51:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:33][root][INFO] - Training Epoch: 1/2, step 62/7134 completed (loss: 0.8420912623405457, acc: 0.8370786309242249)
[2025-02-13 18:51:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:33][root][INFO] - Training Epoch: 1/2, step 63/7134 completed (loss: 0.8809944987297058, acc: 0.8131868243217468)
[2025-02-13 18:51:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:34][root][INFO] - Training Epoch: 1/2, step 64/7134 completed (loss: 0.9359240531921387, acc: 0.7823529243469238)
[2025-02-13 18:51:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:34][root][INFO] - Training Epoch: 1/2, step 65/7134 completed (loss: 1.4742705821990967, acc: 0.686170220375061)
[2025-02-13 18:51:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:35][root][INFO] - Training Epoch: 1/2, step 66/7134 completed (loss: 0.9468445181846619, acc: 0.8257575631141663)
[2025-02-13 18:51:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:35][root][INFO] - Training Epoch: 1/2, step 67/7134 completed (loss: 0.975424587726593, acc: 0.7873563170433044)
[2025-02-13 18:51:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:36][root][INFO] - Training Epoch: 1/2, step 68/7134 completed (loss: 1.387322187423706, acc: 0.7191011309623718)
[2025-02-13 18:51:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:36][root][INFO] - Training Epoch: 1/2, step 69/7134 completed (loss: 1.0656925439834595, acc: 0.7883597612380981)
[2025-02-13 18:51:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:37][root][INFO] - Training Epoch: 1/2, step 70/7134 completed (loss: 0.8381526470184326, acc: 0.8125)
[2025-02-13 18:51:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:37][root][INFO] - Training Epoch: 1/2, step 71/7134 completed (loss: 0.740955114364624, acc: 0.8181818127632141)
[2025-02-13 18:51:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:37][root][INFO] - Training Epoch: 1/2, step 72/7134 completed (loss: 0.8220117092132568, acc: 0.8165680766105652)
[2025-02-13 18:51:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:38][root][INFO] - Training Epoch: 1/2, step 73/7134 completed (loss: 1.173948884010315, acc: 0.7919074892997742)
[2025-02-13 18:51:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:38][root][INFO] - Training Epoch: 1/2, step 74/7134 completed (loss: 0.8012929558753967, acc: 0.8720930218696594)
[2025-02-13 18:51:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:39][root][INFO] - Training Epoch: 1/2, step 75/7134 completed (loss: 1.2998857498168945, acc: 0.761904776096344)
[2025-02-13 18:51:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:39][root][INFO] - Training Epoch: 1/2, step 76/7134 completed (loss: 1.09721040725708, acc: 0.7554348111152649)
[2025-02-13 18:51:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:40][root][INFO] - Training Epoch: 1/2, step 77/7134 completed (loss: 1.1125049591064453, acc: 0.8342541456222534)
[2025-02-13 18:51:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:40][root][INFO] - Training Epoch: 1/2, step 78/7134 completed (loss: 0.9394587874412537, acc: 0.7615894079208374)
[2025-02-13 18:51:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:40][root][INFO] - Training Epoch: 1/2, step 79/7134 completed (loss: 0.852863073348999, acc: 0.8399999737739563)
[2025-02-13 18:51:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:41][root][INFO] - Training Epoch: 1/2, step 80/7134 completed (loss: 0.44738367199897766, acc: 0.915730357170105)
[2025-02-13 18:51:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:41][root][INFO] - Training Epoch: 1/2, step 81/7134 completed (loss: 1.1066240072250366, acc: 0.7830687761306763)
[2025-02-13 18:51:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:42][root][INFO] - Training Epoch: 1/2, step 82/7134 completed (loss: 0.7442240715026855, acc: 0.849397599697113)
[2025-02-13 18:51:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:42][root][INFO] - Training Epoch: 1/2, step 83/7134 completed (loss: 1.1592586040496826, acc: 0.792553186416626)
[2025-02-13 18:51:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:42][root][INFO] - Training Epoch: 1/2, step 84/7134 completed (loss: 0.6975273489952087, acc: 0.8345323801040649)
[2025-02-13 18:51:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:43][root][INFO] - Training Epoch: 1/2, step 85/7134 completed (loss: 1.0196492671966553, acc: 0.7699999809265137)
[2025-02-13 18:51:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:43][root][INFO] - Training Epoch: 1/2, step 86/7134 completed (loss: 1.1828476190567017, acc: 0.7142857313156128)
[2025-02-13 18:51:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:44][root][INFO] - Training Epoch: 1/2, step 87/7134 completed (loss: 1.0957353115081787, acc: 0.7467532753944397)
[2025-02-13 18:51:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:44][root][INFO] - Training Epoch: 1/2, step 88/7134 completed (loss: 0.9598627686500549, acc: 0.7650602459907532)
[2025-02-13 18:51:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:44][root][INFO] - Training Epoch: 1/2, step 89/7134 completed (loss: 0.7992634773254395, acc: 0.8399999737739563)
[2025-02-13 18:51:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:45][root][INFO] - Training Epoch: 1/2, step 90/7134 completed (loss: 0.9531844258308411, acc: 0.8151260614395142)
[2025-02-13 18:51:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:45][root][INFO] - Training Epoch: 1/2, step 91/7134 completed (loss: 0.980280339717865, acc: 0.7685950398445129)
[2025-02-13 18:51:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:46][root][INFO] - Training Epoch: 1/2, step 92/7134 completed (loss: 0.9460271596908569, acc: 0.7662337422370911)
[2025-02-13 18:51:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:46][root][INFO] - Training Epoch: 1/2, step 93/7134 completed (loss: 0.8455222845077515, acc: 0.8399999737739563)
[2025-02-13 18:51:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:47][root][INFO] - Training Epoch: 1/2, step 94/7134 completed (loss: 1.3322213888168335, acc: 0.801886796951294)
[2025-02-13 18:51:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:47][root][INFO] - Training Epoch: 1/2, step 95/7134 completed (loss: 0.9373943209648132, acc: 0.8048780560493469)
[2025-02-13 18:51:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:47][root][INFO] - Training Epoch: 1/2, step 96/7134 completed (loss: 0.38909924030303955, acc: 0.9252336621284485)
[2025-02-13 18:51:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:48][root][INFO] - Training Epoch: 1/2, step 97/7134 completed (loss: 2.71097731590271, acc: 0.5)
[2025-02-13 18:51:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:48][root][INFO] - Training Epoch: 1/2, step 98/7134 completed (loss: 1.3252978324890137, acc: 0.7394366264343262)
[2025-02-13 18:51:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:48][root][INFO] - Training Epoch: 1/2, step 99/7134 completed (loss: 0.8919774889945984, acc: 0.800000011920929)
[2025-02-13 18:51:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:49][root][INFO] - Training Epoch: 1/2, step 100/7134 completed (loss: 1.2740615606307983, acc: 0.7348066568374634)
[2025-02-13 18:51:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:49][root][INFO] - Training Epoch: 1/2, step 101/7134 completed (loss: 1.2426398992538452, acc: 0.734375)
[2025-02-13 18:51:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:50][root][INFO] - Training Epoch: 1/2, step 102/7134 completed (loss: 0.4655967354774475, acc: 0.8907103538513184)
[2025-02-13 18:51:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:50][root][INFO] - Training Epoch: 1/2, step 103/7134 completed (loss: 0.811650276184082, acc: 0.7876712083816528)
[2025-02-13 18:51:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:51][root][INFO] - Training Epoch: 1/2, step 104/7134 completed (loss: 0.4993036389350891, acc: 0.8933333158493042)
[2025-02-13 18:51:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:51][root][INFO] - Training Epoch: 1/2, step 105/7134 completed (loss: 0.45386791229248047, acc: 0.8731343150138855)
[2025-02-13 18:51:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:51][root][INFO] - Training Epoch: 1/2, step 106/7134 completed (loss: 0.5163083076477051, acc: 0.8819875717163086)
[2025-02-13 18:51:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:52][root][INFO] - Training Epoch: 1/2, step 107/7134 completed (loss: 2.021839141845703, acc: 0.6686046719551086)
[2025-02-13 18:51:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:52][root][INFO] - Training Epoch: 1/2, step 108/7134 completed (loss: 0.7470939755439758, acc: 0.7783783674240112)
[2025-02-13 18:51:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:53][root][INFO] - Training Epoch: 1/2, step 109/7134 completed (loss: 0.9397556781768799, acc: 0.8015872836112976)
[2025-02-13 18:51:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:53][root][INFO] - Training Epoch: 1/2, step 110/7134 completed (loss: 0.7308395504951477, acc: 0.8547486066818237)
[2025-02-13 18:51:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:53][root][INFO] - Training Epoch: 1/2, step 111/7134 completed (loss: 0.8164733052253723, acc: 0.8030303120613098)
[2025-02-13 18:51:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:54][root][INFO] - Training Epoch: 1/2, step 112/7134 completed (loss: 1.037961721420288, acc: 0.7986111044883728)
[2025-02-13 18:51:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:54][root][INFO] - Training Epoch: 1/2, step 113/7134 completed (loss: 0.4695141017436981, acc: 0.8882681727409363)
[2025-02-13 18:51:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:55][root][INFO] - Training Epoch: 1/2, step 114/7134 completed (loss: 0.8524275422096252, acc: 0.7771428823471069)
[2025-02-13 18:51:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:55][root][INFO] - Training Epoch: 1/2, step 115/7134 completed (loss: 1.5748860836029053, acc: 0.6336633563041687)
[2025-02-13 18:51:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:55][root][INFO] - Training Epoch: 1/2, step 116/7134 completed (loss: 1.0695431232452393, acc: 0.7873563170433044)
[2025-02-13 18:51:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:56][root][INFO] - Training Epoch: 1/2, step 117/7134 completed (loss: 0.6621460914611816, acc: 0.842424213886261)
[2025-02-13 18:51:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:56][root][INFO] - Training Epoch: 1/2, step 118/7134 completed (loss: 1.1157066822052002, acc: 0.7722222208976746)
[2025-02-13 18:51:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:57][root][INFO] - Training Epoch: 1/2, step 119/7134 completed (loss: 0.7998127341270447, acc: 0.7978723645210266)
[2025-02-13 18:51:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:57][root][INFO] - Training Epoch: 1/2, step 120/7134 completed (loss: 0.6742401123046875, acc: 0.8690476417541504)
[2025-02-13 18:51:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:58][root][INFO] - Training Epoch: 1/2, step 121/7134 completed (loss: 0.8210602402687073, acc: 0.8379888534545898)
[2025-02-13 18:51:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:58][root][INFO] - Training Epoch: 1/2, step 122/7134 completed (loss: 0.7765080332756042, acc: 0.8186812996864319)
[2025-02-13 18:51:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:58][root][INFO] - Training Epoch: 1/2, step 123/7134 completed (loss: 0.649980902671814, acc: 0.8553459048271179)
[2025-02-13 18:51:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:59][root][INFO] - Training Epoch: 1/2, step 124/7134 completed (loss: 0.7300766706466675, acc: 0.8194444179534912)
[2025-02-13 18:51:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:51:59][root][INFO] - Training Epoch: 1/2, step 125/7134 completed (loss: 1.171390414237976, acc: 0.7598039507865906)
[2025-02-13 18:51:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:00][root][INFO] - Training Epoch: 1/2, step 126/7134 completed (loss: 0.8944870829582214, acc: 0.7978723645210266)
[2025-02-13 18:52:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:00][root][INFO] - Training Epoch: 1/2, step 127/7134 completed (loss: 0.7980196475982666, acc: 0.84375)
[2025-02-13 18:52:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:00][root][INFO] - Training Epoch: 1/2, step 128/7134 completed (loss: 0.7131009697914124, acc: 0.8516746163368225)
[2025-02-13 18:52:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:01][root][INFO] - Training Epoch: 1/2, step 129/7134 completed (loss: 0.7481622099876404, acc: 0.8435754179954529)
[2025-02-13 18:52:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:01][root][INFO] - Training Epoch: 1/2, step 130/7134 completed (loss: 0.5667217373847961, acc: 0.8588235378265381)
[2025-02-13 18:52:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:02][root][INFO] - Training Epoch: 1/2, step 131/7134 completed (loss: 0.47981610894203186, acc: 0.8771929740905762)
[2025-02-13 18:52:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:02][root][INFO] - Training Epoch: 1/2, step 132/7134 completed (loss: 0.9276512861251831, acc: 0.8616352081298828)
[2025-02-13 18:52:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:03][root][INFO] - Training Epoch: 1/2, step 133/7134 completed (loss: 0.7272520065307617, acc: 0.845588207244873)
[2025-02-13 18:52:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:03][root][INFO] - Training Epoch: 1/2, step 134/7134 completed (loss: 0.5640668272972107, acc: 0.8796992301940918)
[2025-02-13 18:52:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:03][root][INFO] - Training Epoch: 1/2, step 135/7134 completed (loss: 0.5880385637283325, acc: 0.8730158805847168)
[2025-02-13 18:52:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:04][root][INFO] - Training Epoch: 1/2, step 136/7134 completed (loss: 0.6780862808227539, acc: 0.85326087474823)
[2025-02-13 18:52:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:04][root][INFO] - Training Epoch: 1/2, step 137/7134 completed (loss: 0.9852436184883118, acc: 0.7939698696136475)
[2025-02-13 18:52:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:05][root][INFO] - Training Epoch: 1/2, step 138/7134 completed (loss: 0.8362275958061218, acc: 0.801075279712677)
[2025-02-13 18:52:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:05][root][INFO] - Training Epoch: 1/2, step 139/7134 completed (loss: 0.8448521494865417, acc: 0.8085106611251831)
[2025-02-13 18:52:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:06][root][INFO] - Training Epoch: 1/2, step 140/7134 completed (loss: 0.550274133682251, acc: 0.8711656332015991)
[2025-02-13 18:52:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:06][root][INFO] - Training Epoch: 1/2, step 141/7134 completed (loss: 0.6320724487304688, acc: 0.8450000286102295)
[2025-02-13 18:52:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:06][root][INFO] - Training Epoch: 1/2, step 142/7134 completed (loss: 0.7654228210449219, acc: 0.8248587846755981)
[2025-02-13 18:52:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:07][root][INFO] - Training Epoch: 1/2, step 143/7134 completed (loss: 0.8683980703353882, acc: 0.7845304012298584)
[2025-02-13 18:52:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:07][root][INFO] - Training Epoch: 1/2, step 144/7134 completed (loss: 1.0684839487075806, acc: 0.7567567825317383)
[2025-02-13 18:52:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:08][root][INFO] - Training Epoch: 1/2, step 145/7134 completed (loss: 1.5364669561386108, acc: 0.6891191601753235)
[2025-02-13 18:52:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:08][root][INFO] - Training Epoch: 1/2, step 146/7134 completed (loss: 1.2639844417572021, acc: 0.7175140976905823)
[2025-02-13 18:52:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:09][root][INFO] - Training Epoch: 1/2, step 147/7134 completed (loss: 1.203404188156128, acc: 0.7310344576835632)
[2025-02-13 18:52:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:09][root][INFO] - Training Epoch: 1/2, step 148/7134 completed (loss: 2.077833414077759, acc: 0.6129032373428345)
[2025-02-13 18:52:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:09][root][INFO] - Training Epoch: 1/2, step 149/7134 completed (loss: 1.2897247076034546, acc: 0.7258883118629456)
[2025-02-13 18:52:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:10][root][INFO] - Training Epoch: 1/2, step 150/7134 completed (loss: 1.4874656200408936, acc: 0.7015706896781921)
[2025-02-13 18:52:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:10][root][INFO] - Training Epoch: 1/2, step 151/7134 completed (loss: 0.7994905114173889, acc: 0.8172042965888977)
[2025-02-13 18:52:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:11][root][INFO] - Training Epoch: 1/2, step 152/7134 completed (loss: 0.6644353270530701, acc: 0.8516746163368225)
[2025-02-13 18:52:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:11][root][INFO] - Training Epoch: 1/2, step 153/7134 completed (loss: 1.1050070524215698, acc: 0.7845304012298584)
[2025-02-13 18:52:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:12][root][INFO] - Training Epoch: 1/2, step 154/7134 completed (loss: 1.1396653652191162, acc: 0.7385621070861816)
[2025-02-13 18:52:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:12][root][INFO] - Training Epoch: 1/2, step 155/7134 completed (loss: 1.0124626159667969, acc: 0.8032786846160889)
[2025-02-13 18:52:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:12][root][INFO] - Training Epoch: 1/2, step 156/7134 completed (loss: 0.7027908563613892, acc: 0.8374384045600891)
[2025-02-13 18:52:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:13][root][INFO] - Training Epoch: 1/2, step 157/7134 completed (loss: 0.8241589069366455, acc: 0.820105791091919)
[2025-02-13 18:52:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:13][root][INFO] - Training Epoch: 1/2, step 158/7134 completed (loss: 0.7372870445251465, acc: 0.8362573385238647)
[2025-02-13 18:52:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:14][root][INFO] - Training Epoch: 1/2, step 159/7134 completed (loss: 0.8210300803184509, acc: 0.8092485666275024)
[2025-02-13 18:52:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:14][root][INFO] - Training Epoch: 1/2, step 160/7134 completed (loss: 0.595937967300415, acc: 0.8559321761131287)
[2025-02-13 18:52:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:14][root][INFO] - Training Epoch: 1/2, step 161/7134 completed (loss: 0.2026684433221817, acc: 0.9459459185600281)
[2025-02-13 18:52:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:15][root][INFO] - Training Epoch: 1/2, step 162/7134 completed (loss: 0.3470255434513092, acc: 0.9135802388191223)
[2025-02-13 18:52:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:15][root][INFO] - Training Epoch: 1/2, step 163/7134 completed (loss: 0.5354194045066833, acc: 0.8633093237876892)
[2025-02-13 18:52:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:16][root][INFO] - Training Epoch: 1/2, step 164/7134 completed (loss: 0.5176241993904114, acc: 0.9090909361839294)
[2025-02-13 18:52:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:16][root][INFO] - Training Epoch: 1/2, step 165/7134 completed (loss: 0.4831729531288147, acc: 0.8899999856948853)
[2025-02-13 18:52:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:16][root][INFO] - Training Epoch: 1/2, step 166/7134 completed (loss: 0.33243855834007263, acc: 0.9473684430122375)
[2025-02-13 18:52:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:17][root][INFO] - Training Epoch: 1/2, step 167/7134 completed (loss: 2.9001758098602295, acc: 0.5722891688346863)
[2025-02-13 18:52:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:17][root][INFO] - Training Epoch: 1/2, step 168/7134 completed (loss: 5.467520236968994, acc: 0.26363635063171387)
[2025-02-13 18:52:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:18][root][INFO] - Training Epoch: 1/2, step 169/7134 completed (loss: 4.621039390563965, acc: 0.3671875)
[2025-02-13 18:52:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:18][root][INFO] - Training Epoch: 1/2, step 170/7134 completed (loss: 3.495572328567505, acc: 0.5178571343421936)
[2025-02-13 18:52:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:18][root][INFO] - Training Epoch: 1/2, step 171/7134 completed (loss: 3.7073886394500732, acc: 0.4554455578327179)
[2025-02-13 18:52:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:19][root][INFO] - Training Epoch: 1/2, step 172/7134 completed (loss: 1.3518116474151611, acc: 0.6899224519729614)
[2025-02-13 18:52:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:19][root][INFO] - Training Epoch: 1/2, step 173/7134 completed (loss: 1.3403635025024414, acc: 0.7200000286102295)
[2025-02-13 18:52:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:20][root][INFO] - Training Epoch: 1/2, step 174/7134 completed (loss: 2.4282491207122803, acc: 0.6052631735801697)
[2025-02-13 18:52:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:20][root][INFO] - Training Epoch: 1/2, step 175/7134 completed (loss: 1.327721357345581, acc: 0.7218543291091919)
[2025-02-13 18:52:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:20][root][INFO] - Training Epoch: 1/2, step 176/7134 completed (loss: 1.4500247240066528, acc: 0.693989098072052)
[2025-02-13 18:52:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:21][root][INFO] - Training Epoch: 1/2, step 177/7134 completed (loss: 1.643052339553833, acc: 0.658682644367218)
[2025-02-13 18:52:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:21][root][INFO] - Training Epoch: 1/2, step 178/7134 completed (loss: 1.174690842628479, acc: 0.7792207598686218)
[2025-02-13 18:52:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:21][root][INFO] - Training Epoch: 1/2, step 179/7134 completed (loss: 0.9122105836868286, acc: 0.7664233446121216)
[2025-02-13 18:52:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:22][root][INFO] - Training Epoch: 1/2, step 180/7134 completed (loss: 0.7253128886222839, acc: 0.8255033493041992)
[2025-02-13 18:52:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:22][root][INFO] - Training Epoch: 1/2, step 181/7134 completed (loss: 0.5766994953155518, acc: 0.8689655065536499)
[2025-02-13 18:52:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:23][root][INFO] - Training Epoch: 1/2, step 182/7134 completed (loss: 0.7327085733413696, acc: 0.8385093212127686)
[2025-02-13 18:52:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:23][root][INFO] - Training Epoch: 1/2, step 183/7134 completed (loss: 0.6870710253715515, acc: 0.8630136847496033)
[2025-02-13 18:52:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:24][root][INFO] - Training Epoch: 1/2, step 184/7134 completed (loss: 0.7047881484031677, acc: 0.8203125)
[2025-02-13 18:52:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:24][root][INFO] - Training Epoch: 1/2, step 185/7134 completed (loss: 0.4880821704864502, acc: 0.8775510191917419)
[2025-02-13 18:52:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:24][root][INFO] - Training Epoch: 1/2, step 186/7134 completed (loss: 0.7708748579025269, acc: 0.8358209133148193)
[2025-02-13 18:52:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:25][root][INFO] - Training Epoch: 1/2, step 187/7134 completed (loss: 0.8156408071517944, acc: 0.8496732115745544)
[2025-02-13 18:52:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:25][root][INFO] - Training Epoch: 1/2, step 188/7134 completed (loss: 1.1728014945983887, acc: 0.7621951103210449)
[2025-02-13 18:52:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:26][root][INFO] - Training Epoch: 1/2, step 189/7134 completed (loss: 0.8656017184257507, acc: 0.8196721076965332)
[2025-02-13 18:52:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:26][root][INFO] - Training Epoch: 1/2, step 190/7134 completed (loss: 1.23762845993042, acc: 0.7611940503120422)
[2025-02-13 18:52:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:27][root][INFO] - Training Epoch: 1/2, step 191/7134 completed (loss: 0.8595094084739685, acc: 0.7980769276618958)
[2025-02-13 18:52:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:27][root][INFO] - Training Epoch: 1/2, step 192/7134 completed (loss: 0.5791751742362976, acc: 0.895348846912384)
[2025-02-13 18:52:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:27][root][INFO] - Training Epoch: 1/2, step 193/7134 completed (loss: 0.9063098430633545, acc: 0.7784430980682373)
[2025-02-13 18:52:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:28][root][INFO] - Training Epoch: 1/2, step 194/7134 completed (loss: 1.1722482442855835, acc: 0.7735849022865295)
[2025-02-13 18:52:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:28][root][INFO] - Training Epoch: 1/2, step 195/7134 completed (loss: 0.9404245018959045, acc: 0.7578125)
[2025-02-13 18:52:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:29][root][INFO] - Training Epoch: 1/2, step 196/7134 completed (loss: 1.225926160812378, acc: 0.7653631567955017)
[2025-02-13 18:52:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:29][root][INFO] - Training Epoch: 1/2, step 197/7134 completed (loss: 0.6793047785758972, acc: 0.837837815284729)
[2025-02-13 18:52:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:29][root][INFO] - Training Epoch: 1/2, step 198/7134 completed (loss: 0.7096331715583801, acc: 0.8407643437385559)
[2025-02-13 18:52:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:30][root][INFO] - Training Epoch: 1/2, step 199/7134 completed (loss: 1.2729642391204834, acc: 0.7573529481887817)
[2025-02-13 18:52:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:30][root][INFO] - Training Epoch: 1/2, step 200/7134 completed (loss: 0.7797853946685791, acc: 0.8299319744110107)
[2025-02-13 18:52:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:31][root][INFO] - Training Epoch: 1/2, step 201/7134 completed (loss: 0.4829502999782562, acc: 0.8865247964859009)
[2025-02-13 18:52:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:31][root][INFO] - Training Epoch: 1/2, step 202/7134 completed (loss: 0.6372147798538208, acc: 0.8709677457809448)
[2025-02-13 18:52:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:31][root][INFO] - Training Epoch: 1/2, step 203/7134 completed (loss: 0.8935532569885254, acc: 0.8106508851051331)
[2025-02-13 18:52:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:32][root][INFO] - Training Epoch: 1/2, step 204/7134 completed (loss: 1.0795809030532837, acc: 0.7798742055892944)
[2025-02-13 18:52:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:32][root][INFO] - Training Epoch: 1/2, step 205/7134 completed (loss: 1.115307331085205, acc: 0.7450000047683716)
[2025-02-13 18:52:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:32][root][INFO] - Training Epoch: 1/2, step 206/7134 completed (loss: 0.6403391361236572, acc: 0.8472222089767456)
[2025-02-13 18:52:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:33][root][INFO] - Training Epoch: 1/2, step 207/7134 completed (loss: 0.6593737006187439, acc: 0.8503401279449463)
[2025-02-13 18:52:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:33][root][INFO] - Training Epoch: 1/2, step 208/7134 completed (loss: 1.2150596380233765, acc: 0.7357142567634583)
[2025-02-13 18:52:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:34][root][INFO] - Training Epoch: 1/2, step 209/7134 completed (loss: 1.3365674018859863, acc: 0.7374301552772522)
[2025-02-13 18:52:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:34][root][INFO] - Training Epoch: 1/2, step 210/7134 completed (loss: 1.0288556814193726, acc: 0.784140944480896)
[2025-02-13 18:52:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:34][root][INFO] - Training Epoch: 1/2, step 211/7134 completed (loss: 0.8376590609550476, acc: 0.8445945978164673)
[2025-02-13 18:52:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:35][root][INFO] - Training Epoch: 1/2, step 212/7134 completed (loss: 0.9941600561141968, acc: 0.8264462947845459)
[2025-02-13 18:52:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:35][root][INFO] - Training Epoch: 1/2, step 213/7134 completed (loss: 1.1531884670257568, acc: 0.7333333492279053)
[2025-02-13 18:52:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:36][root][INFO] - Training Epoch: 1/2, step 214/7134 completed (loss: 0.9318845868110657, acc: 0.7722772359848022)
[2025-02-13 18:52:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:36][root][INFO] - Training Epoch: 1/2, step 215/7134 completed (loss: 0.924148678779602, acc: 0.7857142686843872)
[2025-02-13 18:52:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:36][root][INFO] - Training Epoch: 1/2, step 216/7134 completed (loss: 0.7160788774490356, acc: 0.8370786309242249)
[2025-02-13 18:52:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:37][root][INFO] - Training Epoch: 1/2, step 217/7134 completed (loss: 0.6472188830375671, acc: 0.8385416865348816)
[2025-02-13 18:52:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:37][root][INFO] - Training Epoch: 1/2, step 218/7134 completed (loss: 0.41825631260871887, acc: 0.9197860956192017)
[2025-02-13 18:52:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:38][root][INFO] - Training Epoch: 1/2, step 219/7134 completed (loss: 0.45569396018981934, acc: 0.8684210777282715)
[2025-02-13 18:52:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:38][root][INFO] - Training Epoch: 1/2, step 220/7134 completed (loss: 0.47543638944625854, acc: 0.8782608509063721)
[2025-02-13 18:52:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:38][root][INFO] - Training Epoch: 1/2, step 221/7134 completed (loss: 0.5869627594947815, acc: 0.8500000238418579)
[2025-02-13 18:52:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:39][root][INFO] - Training Epoch: 1/2, step 222/7134 completed (loss: 0.6475427746772766, acc: 0.8612716794013977)
[2025-02-13 18:52:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:39][root][INFO] - Training Epoch: 1/2, step 223/7134 completed (loss: 0.9092125296592712, acc: 0.7675675749778748)
[2025-02-13 18:52:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:39][root][INFO] - Training Epoch: 1/2, step 224/7134 completed (loss: 1.3460803031921387, acc: 0.7318435907363892)
[2025-02-13 18:52:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:40][root][INFO] - Training Epoch: 1/2, step 225/7134 completed (loss: 1.2289066314697266, acc: 0.7573964595794678)
[2025-02-13 18:52:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:40][root][INFO] - Training Epoch: 1/2, step 226/7134 completed (loss: 0.7158864736557007, acc: 0.8571428656578064)
[2025-02-13 18:52:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:41][root][INFO] - Training Epoch: 1/2, step 227/7134 completed (loss: 0.9580804705619812, acc: 0.7872340679168701)
[2025-02-13 18:52:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:41][root][INFO] - Training Epoch: 1/2, step 228/7134 completed (loss: 1.0549288988113403, acc: 0.772455096244812)
[2025-02-13 18:52:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:42][root][INFO] - Training Epoch: 1/2, step 229/7134 completed (loss: 1.1198253631591797, acc: 0.7257142663002014)
[2025-02-13 18:52:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:42][root][INFO] - Training Epoch: 1/2, step 230/7134 completed (loss: 0.8176780343055725, acc: 0.7900552749633789)
[2025-02-13 18:52:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:42][root][INFO] - Training Epoch: 1/2, step 231/7134 completed (loss: 0.7784075736999512, acc: 0.7719298005104065)
[2025-02-13 18:52:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:43][root][INFO] - Training Epoch: 1/2, step 232/7134 completed (loss: 0.8470577597618103, acc: 0.811188817024231)
[2025-02-13 18:52:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:43][root][INFO] - Training Epoch: 1/2, step 233/7134 completed (loss: 0.6528781652450562, acc: 0.8613138794898987)
[2025-02-13 18:52:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:44][root][INFO] - Training Epoch: 1/2, step 234/7134 completed (loss: 0.7692620754241943, acc: 0.8169013857841492)
[2025-02-13 18:52:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:44][root][INFO] - Training Epoch: 1/2, step 235/7134 completed (loss: 0.8438536524772644, acc: 0.8048780560493469)
[2025-02-13 18:52:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:44][root][INFO] - Training Epoch: 1/2, step 236/7134 completed (loss: 0.49027150869369507, acc: 0.8675496578216553)
[2025-02-13 18:52:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:45][root][INFO] - Training Epoch: 1/2, step 237/7134 completed (loss: 0.6145948171615601, acc: 0.8152866363525391)
[2025-02-13 18:52:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:45][root][INFO] - Training Epoch: 1/2, step 238/7134 completed (loss: 1.0074269771575928, acc: 0.7862595319747925)
[2025-02-13 18:52:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:46][root][INFO] - Training Epoch: 1/2, step 239/7134 completed (loss: 0.808284342288971, acc: 0.801886796951294)
[2025-02-13 18:52:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:46][root][INFO] - Training Epoch: 1/2, step 240/7134 completed (loss: 0.699144184589386, acc: 0.8543689250946045)
[2025-02-13 18:52:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:46][root][INFO] - Training Epoch: 1/2, step 241/7134 completed (loss: 0.817905068397522, acc: 0.8059701323509216)
[2025-02-13 18:52:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:47][root][INFO] - Training Epoch: 1/2, step 242/7134 completed (loss: 1.0701799392700195, acc: 0.7785235047340393)
[2025-02-13 18:52:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:47][root][INFO] - Training Epoch: 1/2, step 243/7134 completed (loss: 1.3845343589782715, acc: 0.7103448510169983)
[2025-02-13 18:52:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:48][root][INFO] - Training Epoch: 1/2, step 244/7134 completed (loss: 0.5166013836860657, acc: 0.8992805480957031)
[2025-02-13 18:52:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:48][root][INFO] - Training Epoch: 1/2, step 245/7134 completed (loss: 0.7563040852546692, acc: 0.8278688788414001)
[2025-02-13 18:52:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:48][root][INFO] - Training Epoch: 1/2, step 246/7134 completed (loss: 0.6231201887130737, acc: 0.8623188138008118)
[2025-02-13 18:52:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:49][root][INFO] - Training Epoch: 1/2, step 247/7134 completed (loss: 0.9327262043952942, acc: 0.7777777910232544)
[2025-02-13 18:52:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:49][root][INFO] - Training Epoch: 1/2, step 248/7134 completed (loss: 0.755972146987915, acc: 0.8455284833908081)
[2025-02-13 18:52:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:50][root][INFO] - Training Epoch: 1/2, step 249/7134 completed (loss: 0.7786350846290588, acc: 0.8152173757553101)
[2025-02-13 18:52:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:50][root][INFO] - Training Epoch: 1/2, step 250/7134 completed (loss: 0.7335624694824219, acc: 0.8211920261383057)
[2025-02-13 18:52:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:50][root][INFO] - Training Epoch: 1/2, step 251/7134 completed (loss: 0.6022124290466309, acc: 0.8705036044120789)
[2025-02-13 18:52:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:51][root][INFO] - Training Epoch: 1/2, step 252/7134 completed (loss: 0.5036121606826782, acc: 0.8992805480957031)
[2025-02-13 18:52:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:51][root][INFO] - Training Epoch: 1/2, step 253/7134 completed (loss: 0.49797940254211426, acc: 0.8640000224113464)
[2025-02-13 18:52:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:51][root][INFO] - Training Epoch: 1/2, step 254/7134 completed (loss: 0.5801449418067932, acc: 0.8684210777282715)
[2025-02-13 18:52:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:52][root][INFO] - Training Epoch: 1/2, step 255/7134 completed (loss: 0.5061379671096802, acc: 0.8782051205635071)
[2025-02-13 18:52:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:52][root][INFO] - Training Epoch: 1/2, step 256/7134 completed (loss: 1.294772982597351, acc: 0.7567567825317383)
[2025-02-13 18:52:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:53][root][INFO] - Training Epoch: 1/2, step 257/7134 completed (loss: 0.6058393716812134, acc: 0.9024389982223511)
[2025-02-13 18:52:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:53][root][INFO] - Training Epoch: 1/2, step 258/7134 completed (loss: 0.6054975986480713, acc: 0.8790322542190552)
[2025-02-13 18:52:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:54][root][INFO] - Training Epoch: 1/2, step 259/7134 completed (loss: 0.6939603090286255, acc: 0.8484848737716675)
[2025-02-13 18:52:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:54][root][INFO] - Training Epoch: 1/2, step 260/7134 completed (loss: 0.7277365326881409, acc: 0.8571428656578064)
[2025-02-13 18:52:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:54][root][INFO] - Training Epoch: 1/2, step 261/7134 completed (loss: 0.6583605408668518, acc: 0.8148148059844971)
[2025-02-13 18:52:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:55][root][INFO] - Training Epoch: 1/2, step 262/7134 completed (loss: 0.33266299962997437, acc: 0.9105691313743591)
[2025-02-13 18:52:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:55][root][INFO] - Training Epoch: 1/2, step 263/7134 completed (loss: 0.4420756697654724, acc: 0.895061731338501)
[2025-02-13 18:52:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:56][root][INFO] - Training Epoch: 1/2, step 264/7134 completed (loss: 0.25042909383773804, acc: 0.9647058844566345)
[2025-02-13 18:52:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:56][root][INFO] - Training Epoch: 1/2, step 265/7134 completed (loss: 0.5573073625564575, acc: 0.8764045238494873)
[2025-02-13 18:52:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:56][root][INFO] - Training Epoch: 1/2, step 266/7134 completed (loss: 0.40879034996032715, acc: 0.9010416865348816)
[2025-02-13 18:52:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:57][root][INFO] - Training Epoch: 1/2, step 267/7134 completed (loss: 0.5630731582641602, acc: 0.8723404407501221)
[2025-02-13 18:52:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:57][root][INFO] - Training Epoch: 1/2, step 268/7134 completed (loss: 0.7502787709236145, acc: 0.8484848737716675)
[2025-02-13 18:52:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:58][root][INFO] - Training Epoch: 1/2, step 269/7134 completed (loss: 0.7086264491081238, acc: 0.8273809552192688)
[2025-02-13 18:52:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:58][root][INFO] - Training Epoch: 1/2, step 270/7134 completed (loss: 0.723843514919281, acc: 0.856249988079071)
[2025-02-13 18:52:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:59][root][INFO] - Training Epoch: 1/2, step 271/7134 completed (loss: 0.5334339737892151, acc: 0.9055555462837219)
[2025-02-13 18:52:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:59][root][INFO] - Training Epoch: 1/2, step 272/7134 completed (loss: 0.6491653919219971, acc: 0.826815664768219)
[2025-02-13 18:52:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:52:59][root][INFO] - Training Epoch: 1/2, step 273/7134 completed (loss: 0.5802538394927979, acc: 0.8776595592498779)
[2025-02-13 18:52:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:00][root][INFO] - Training Epoch: 1/2, step 274/7134 completed (loss: 0.5815573334693909, acc: 0.859649121761322)
[2025-02-13 18:53:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:00][root][INFO] - Training Epoch: 1/2, step 275/7134 completed (loss: 0.48306748270988464, acc: 0.8832487463951111)
[2025-02-13 18:53:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:01][root][INFO] - Training Epoch: 1/2, step 276/7134 completed (loss: 0.5119745135307312, acc: 0.8917526006698608)
[2025-02-13 18:53:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:01][root][INFO] - Training Epoch: 1/2, step 277/7134 completed (loss: 0.8119924068450928, acc: 0.837837815284729)
[2025-02-13 18:53:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:01][root][INFO] - Training Epoch: 1/2, step 278/7134 completed (loss: 0.7688122391700745, acc: 0.8229166865348816)
[2025-02-13 18:53:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:02][root][INFO] - Training Epoch: 1/2, step 279/7134 completed (loss: 0.7197362780570984, acc: 0.8108108043670654)
[2025-02-13 18:53:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:02][root][INFO] - Training Epoch: 1/2, step 280/7134 completed (loss: 0.7503896355628967, acc: 0.8240000009536743)
[2025-02-13 18:53:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:03][root][INFO] - Training Epoch: 1/2, step 281/7134 completed (loss: 0.6730103492736816, acc: 0.8559321761131287)
[2025-02-13 18:53:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:03][root][INFO] - Training Epoch: 1/2, step 282/7134 completed (loss: 1.04072904586792, acc: 0.797468364238739)
[2025-02-13 18:53:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:03][root][INFO] - Training Epoch: 1/2, step 283/7134 completed (loss: 0.6704809069633484, acc: 0.847328245639801)
[2025-02-13 18:53:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:04][root][INFO] - Training Epoch: 1/2, step 284/7134 completed (loss: 0.5764148235321045, acc: 0.8881118893623352)
[2025-02-13 18:53:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:04][root][INFO] - Training Epoch: 1/2, step 285/7134 completed (loss: 0.7864733934402466, acc: 0.8370370268821716)
[2025-02-13 18:53:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:05][root][INFO] - Training Epoch: 1/2, step 286/7134 completed (loss: 0.6158493161201477, acc: 0.8402777910232544)
[2025-02-13 18:53:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:05][root][INFO] - Training Epoch: 1/2, step 287/7134 completed (loss: 0.5875616669654846, acc: 0.8657718300819397)
[2025-02-13 18:53:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:05][root][INFO] - Training Epoch: 1/2, step 288/7134 completed (loss: 0.6681053042411804, acc: 0.8661417365074158)
[2025-02-13 18:53:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:06][root][INFO] - Training Epoch: 1/2, step 289/7134 completed (loss: 0.5963464975357056, acc: 0.8449612259864807)
[2025-02-13 18:53:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:06][root][INFO] - Training Epoch: 1/2, step 290/7134 completed (loss: 0.5607593655586243, acc: 0.8701298832893372)
[2025-02-13 18:53:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:06][root][INFO] - Training Epoch: 1/2, step 291/7134 completed (loss: 0.8204681277275085, acc: 0.8163265585899353)
[2025-02-13 18:53:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:07][root][INFO] - Training Epoch: 1/2, step 292/7134 completed (loss: 0.9617876410484314, acc: 0.790123462677002)
[2025-02-13 18:53:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:07][root][INFO] - Training Epoch: 1/2, step 293/7134 completed (loss: 0.8600752353668213, acc: 0.7964601516723633)
[2025-02-13 18:53:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:08][root][INFO] - Training Epoch: 1/2, step 294/7134 completed (loss: 0.7649468183517456, acc: 0.805084764957428)
[2025-02-13 18:53:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:08][root][INFO] - Training Epoch: 1/2, step 295/7134 completed (loss: 0.9474836587905884, acc: 0.7820512652397156)
[2025-02-13 18:53:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:08][root][INFO] - Training Epoch: 1/2, step 296/7134 completed (loss: 0.41237813234329224, acc: 0.9090909361839294)
[2025-02-13 18:53:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:09][root][INFO] - Training Epoch: 1/2, step 297/7134 completed (loss: 0.4225768744945526, acc: 0.9008264541625977)
[2025-02-13 18:53:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:09][root][INFO] - Training Epoch: 1/2, step 298/7134 completed (loss: 0.5783656239509583, acc: 0.8769230842590332)
[2025-02-13 18:53:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:10][root][INFO] - Training Epoch: 1/2, step 299/7134 completed (loss: 0.2618742883205414, acc: 0.9416058659553528)
[2025-02-13 18:53:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:10][root][INFO] - Training Epoch: 1/2, step 300/7134 completed (loss: 0.21523964405059814, acc: 0.9275362491607666)
[2025-02-13 18:53:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:10][root][INFO] - Training Epoch: 1/2, step 301/7134 completed (loss: 0.4732876718044281, acc: 0.8741258978843689)
[2025-02-13 18:53:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:11][root][INFO] - Training Epoch: 1/2, step 302/7134 completed (loss: 0.3598865866661072, acc: 0.9009901285171509)
[2025-02-13 18:53:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:11][root][INFO] - Training Epoch: 1/2, step 303/7134 completed (loss: 0.5284061431884766, acc: 0.8648648858070374)
[2025-02-13 18:53:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:11][root][INFO] - Training Epoch: 1/2, step 304/7134 completed (loss: 0.3236311376094818, acc: 0.9306930899620056)
[2025-02-13 18:53:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:12][root][INFO] - Training Epoch: 1/2, step 305/7134 completed (loss: 0.5409392714500427, acc: 0.8984375)
[2025-02-13 18:53:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:12][root][INFO] - Training Epoch: 1/2, step 306/7134 completed (loss: 0.23688852787017822, acc: 0.95652174949646)
[2025-02-13 18:53:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:13][root][INFO] - Training Epoch: 1/2, step 307/7134 completed (loss: 0.6253372430801392, acc: 0.8691588640213013)
[2025-02-13 18:53:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:13][root][INFO] - Training Epoch: 1/2, step 308/7134 completed (loss: 0.34174469113349915, acc: 0.913385808467865)
[2025-02-13 18:53:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:13][root][INFO] - Training Epoch: 1/2, step 309/7134 completed (loss: 0.4091430902481079, acc: 0.8938053250312805)
[2025-02-13 18:53:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:14][root][INFO] - Training Epoch: 1/2, step 310/7134 completed (loss: 0.40544623136520386, acc: 0.8914728760719299)
[2025-02-13 18:53:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:14][root][INFO] - Training Epoch: 1/2, step 311/7134 completed (loss: 0.3463086783885956, acc: 0.902255654335022)
[2025-02-13 18:53:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:15][root][INFO] - Training Epoch: 1/2, step 312/7134 completed (loss: 0.27246707677841187, acc: 0.9508196711540222)
[2025-02-13 18:53:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:15][root][INFO] - Training Epoch: 1/2, step 313/7134 completed (loss: 0.3299570381641388, acc: 0.9279999732971191)
[2025-02-13 18:53:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:15][root][INFO] - Training Epoch: 1/2, step 314/7134 completed (loss: 0.40476036071777344, acc: 0.8962963223457336)
[2025-02-13 18:53:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:16][root][INFO] - Training Epoch: 1/2, step 315/7134 completed (loss: 0.6983152627944946, acc: 0.8500000238418579)
[2025-02-13 18:53:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:16][root][INFO] - Training Epoch: 1/2, step 316/7134 completed (loss: 0.48337242007255554, acc: 0.8828828930854797)
[2025-02-13 18:53:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:16][root][INFO] - Training Epoch: 1/2, step 317/7134 completed (loss: 0.17270036041736603, acc: 0.9620253443717957)
[2025-02-13 18:53:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:17][root][INFO] - Training Epoch: 1/2, step 318/7134 completed (loss: 0.5666846036911011, acc: 0.8548387289047241)
[2025-02-13 18:53:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:17][root][INFO] - Training Epoch: 1/2, step 319/7134 completed (loss: 0.7384957671165466, acc: 0.8322981595993042)
[2025-02-13 18:53:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:18][root][INFO] - Training Epoch: 1/2, step 320/7134 completed (loss: 0.6179811358451843, acc: 0.863095223903656)
[2025-02-13 18:53:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:18][root][INFO] - Training Epoch: 1/2, step 321/7134 completed (loss: 0.6118836402893066, acc: 0.8678160905838013)
[2025-02-13 18:53:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:18][root][INFO] - Training Epoch: 1/2, step 322/7134 completed (loss: 0.6776738166809082, acc: 0.8235294222831726)
[2025-02-13 18:53:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:19][root][INFO] - Training Epoch: 1/2, step 323/7134 completed (loss: 1.0117974281311035, acc: 0.7551020383834839)
[2025-02-13 18:53:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:19][root][INFO] - Training Epoch: 1/2, step 324/7134 completed (loss: 0.9450506567955017, acc: 0.8287037014961243)
[2025-02-13 18:53:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:20][root][INFO] - Training Epoch: 1/2, step 325/7134 completed (loss: 0.5977489948272705, acc: 0.8806818127632141)
[2025-02-13 18:53:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:20][root][INFO] - Training Epoch: 1/2, step 326/7134 completed (loss: 0.5891013145446777, acc: 0.8595505356788635)
[2025-02-13 18:53:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:20][root][INFO] - Training Epoch: 1/2, step 327/7134 completed (loss: 0.5581627488136292, acc: 0.8787878751754761)
[2025-02-13 18:53:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:21][root][INFO] - Training Epoch: 1/2, step 328/7134 completed (loss: 0.5082125663757324, acc: 0.8812500238418579)
[2025-02-13 18:53:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:21][root][INFO] - Training Epoch: 1/2, step 329/7134 completed (loss: 0.4779382646083832, acc: 0.9004974961280823)
[2025-02-13 18:53:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:22][root][INFO] - Training Epoch: 1/2, step 330/7134 completed (loss: 0.5173633694648743, acc: 0.890350878238678)
[2025-02-13 18:53:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:22][root][INFO] - Training Epoch: 1/2, step 331/7134 completed (loss: 0.33074018359184265, acc: 0.90625)
[2025-02-13 18:53:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:22][root][INFO] - Training Epoch: 1/2, step 332/7134 completed (loss: 0.2112232744693756, acc: 0.9353233575820923)
[2025-02-13 18:53:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:23][root][INFO] - Training Epoch: 1/2, step 333/7134 completed (loss: 0.5765556693077087, acc: 0.8387096524238586)
[2025-02-13 18:53:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:23][root][INFO] - Training Epoch: 1/2, step 334/7134 completed (loss: 0.5767192840576172, acc: 0.8516483306884766)
[2025-02-13 18:53:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:24][root][INFO] - Training Epoch: 1/2, step 335/7134 completed (loss: 0.4136192202568054, acc: 0.912162184715271)
[2025-02-13 18:53:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:24][root][INFO] - Training Epoch: 1/2, step 336/7134 completed (loss: 0.6924837231636047, acc: 0.8652849793434143)
[2025-02-13 18:53:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:24][root][INFO] - Training Epoch: 1/2, step 337/7134 completed (loss: 0.6285609006881714, acc: 0.8198757767677307)
[2025-02-13 18:53:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:25][root][INFO] - Training Epoch: 1/2, step 338/7134 completed (loss: 0.8018808960914612, acc: 0.8376963138580322)
[2025-02-13 18:53:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:25][root][INFO] - Training Epoch: 1/2, step 339/7134 completed (loss: 0.49971336126327515, acc: 0.8716577291488647)
[2025-02-13 18:53:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:25][root][INFO] - Training Epoch: 1/2, step 340/7134 completed (loss: 0.5335501432418823, acc: 0.8888888955116272)
[2025-02-13 18:53:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:26][root][INFO] - Training Epoch: 1/2, step 341/7134 completed (loss: 0.47053542733192444, acc: 0.8775510191917419)
[2025-02-13 18:53:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:26][root][INFO] - Training Epoch: 1/2, step 342/7134 completed (loss: 0.44378533959388733, acc: 0.893081784248352)
[2025-02-13 18:53:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:27][root][INFO] - Training Epoch: 1/2, step 343/7134 completed (loss: 0.8064233064651489, acc: 0.8230769038200378)
[2025-02-13 18:53:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:27][root][INFO] - Training Epoch: 1/2, step 344/7134 completed (loss: 0.5254474878311157, acc: 0.8759689927101135)
[2025-02-13 18:53:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:27][root][INFO] - Training Epoch: 1/2, step 345/7134 completed (loss: 0.47422611713409424, acc: 0.8652482032775879)
[2025-02-13 18:53:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:28][root][INFO] - Training Epoch: 1/2, step 346/7134 completed (loss: 0.3327641487121582, acc: 0.9342105388641357)
[2025-02-13 18:53:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:28][root][INFO] - Training Epoch: 1/2, step 347/7134 completed (loss: 0.7504225373268127, acc: 0.8125)
[2025-02-13 18:53:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:29][root][INFO] - Training Epoch: 1/2, step 348/7134 completed (loss: 0.4536759853363037, acc: 0.8875739574432373)
[2025-02-13 18:53:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:29][root][INFO] - Training Epoch: 1/2, step 349/7134 completed (loss: 0.5305808782577515, acc: 0.8875739574432373)
[2025-02-13 18:53:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:29][root][INFO] - Training Epoch: 1/2, step 350/7134 completed (loss: 0.4246882498264313, acc: 0.9105263352394104)
[2025-02-13 18:53:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:30][root][INFO] - Training Epoch: 1/2, step 351/7134 completed (loss: 0.48073747754096985, acc: 0.8804348111152649)
[2025-02-13 18:53:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:30][root][INFO] - Training Epoch: 1/2, step 352/7134 completed (loss: 0.4326886236667633, acc: 0.8947368264198303)
[2025-02-13 18:53:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:31][root][INFO] - Training Epoch: 1/2, step 353/7134 completed (loss: 0.5333510041236877, acc: 0.8895705342292786)
[2025-02-13 18:53:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:31][root][INFO] - Training Epoch: 1/2, step 354/7134 completed (loss: 0.4822082221508026, acc: 0.8994975090026855)
[2025-02-13 18:53:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:31][root][INFO] - Training Epoch: 1/2, step 355/7134 completed (loss: 0.531667172908783, acc: 0.8934911489486694)
[2025-02-13 18:53:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:32][root][INFO] - Training Epoch: 1/2, step 356/7134 completed (loss: 0.27648651599884033, acc: 0.9281437397003174)
[2025-02-13 18:53:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:32][root][INFO] - Training Epoch: 1/2, step 357/7134 completed (loss: 0.39948800206184387, acc: 0.9069767594337463)
[2025-02-13 18:53:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:33][root][INFO] - Training Epoch: 1/2, step 358/7134 completed (loss: 0.4618317782878876, acc: 0.8636363744735718)
[2025-02-13 18:53:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:33][root][INFO] - Training Epoch: 1/2, step 359/7134 completed (loss: 0.5212429165840149, acc: 0.8999999761581421)
[2025-02-13 18:53:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:33][root][INFO] - Training Epoch: 1/2, step 360/7134 completed (loss: 0.3407103419303894, acc: 0.915032684803009)
[2025-02-13 18:53:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:34][root][INFO] - Training Epoch: 1/2, step 361/7134 completed (loss: 0.3788834512233734, acc: 0.8895705342292786)
[2025-02-13 18:53:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:34][root][INFO] - Training Epoch: 1/2, step 362/7134 completed (loss: 0.4187767803668976, acc: 0.905063271522522)
[2025-02-13 18:53:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:35][root][INFO] - Training Epoch: 1/2, step 363/7134 completed (loss: 0.4367346465587616, acc: 0.8852459192276001)
[2025-02-13 18:53:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:35][root][INFO] - Training Epoch: 1/2, step 364/7134 completed (loss: 0.5473805665969849, acc: 0.893203854560852)
[2025-02-13 18:53:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:35][root][INFO] - Training Epoch: 1/2, step 365/7134 completed (loss: 0.3678884506225586, acc: 0.905063271522522)
[2025-02-13 18:53:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:36][root][INFO] - Training Epoch: 1/2, step 366/7134 completed (loss: 0.26603469252586365, acc: 0.9363057613372803)
[2025-02-13 18:53:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:36][root][INFO] - Training Epoch: 1/2, step 367/7134 completed (loss: 0.29835933446884155, acc: 0.9278350472450256)
[2025-02-13 18:53:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:37][root][INFO] - Training Epoch: 1/2, step 368/7134 completed (loss: 0.17690803110599518, acc: 0.9467455744743347)
[2025-02-13 18:53:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:37][root][INFO] - Training Epoch: 1/2, step 369/7134 completed (loss: 0.49554476141929626, acc: 0.8895705342292786)
[2025-02-13 18:53:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:37][root][INFO] - Training Epoch: 1/2, step 370/7134 completed (loss: 0.48642241954803467, acc: 0.8860759735107422)
[2025-02-13 18:53:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:38][root][INFO] - Training Epoch: 1/2, step 371/7134 completed (loss: 0.3759356737136841, acc: 0.9130434989929199)
[2025-02-13 18:53:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:38][root][INFO] - Training Epoch: 1/2, step 372/7134 completed (loss: 0.5985998511314392, acc: 0.8616352081298828)
[2025-02-13 18:53:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:38][root][INFO] - Training Epoch: 1/2, step 373/7134 completed (loss: 0.3208533227443695, acc: 0.9281045794487)
[2025-02-13 18:53:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:39][root][INFO] - Training Epoch: 1/2, step 374/7134 completed (loss: 0.32929062843322754, acc: 0.912162184715271)
[2025-02-13 18:53:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:39][root][INFO] - Training Epoch: 1/2, step 375/7134 completed (loss: 0.6129036545753479, acc: 0.8723404407501221)
[2025-02-13 18:53:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:40][root][INFO] - Training Epoch: 1/2, step 376/7134 completed (loss: 0.5942814350128174, acc: 0.8444444537162781)
[2025-02-13 18:53:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:40][root][INFO] - Training Epoch: 1/2, step 377/7134 completed (loss: 0.5408604145050049, acc: 0.8715083599090576)
[2025-02-13 18:53:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:40][root][INFO] - Training Epoch: 1/2, step 378/7134 completed (loss: 0.5645188689231873, acc: 0.8474576473236084)
[2025-02-13 18:53:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:41][root][INFO] - Training Epoch: 1/2, step 379/7134 completed (loss: 0.18475304543972015, acc: 0.9509803652763367)
[2025-02-13 18:53:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:41][root][INFO] - Training Epoch: 1/2, step 380/7134 completed (loss: 0.7240212559700012, acc: 0.8041236996650696)
[2025-02-13 18:53:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:42][root][INFO] - Training Epoch: 1/2, step 381/7134 completed (loss: 0.44896847009658813, acc: 0.9105263352394104)
[2025-02-13 18:53:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:42][root][INFO] - Training Epoch: 1/2, step 382/7134 completed (loss: 0.3260273337364197, acc: 0.9128205180168152)
[2025-02-13 18:53:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:42][root][INFO] - Training Epoch: 1/2, step 383/7134 completed (loss: 0.37912073731422424, acc: 0.9069767594337463)
[2025-02-13 18:53:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:43][root][INFO] - Training Epoch: 1/2, step 384/7134 completed (loss: 0.33709844946861267, acc: 0.9281768202781677)
[2025-02-13 18:53:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:43][root][INFO] - Training Epoch: 1/2, step 385/7134 completed (loss: 0.2760612368583679, acc: 0.9215686321258545)
[2025-02-13 18:53:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:44][root][INFO] - Training Epoch: 1/2, step 386/7134 completed (loss: 0.4267629086971283, acc: 0.9099099040031433)
[2025-02-13 18:53:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:44][root][INFO] - Training Epoch: 1/2, step 387/7134 completed (loss: 0.5425376296043396, acc: 0.8520709872245789)
[2025-02-13 18:53:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:44][root][INFO] - Training Epoch: 1/2, step 388/7134 completed (loss: 0.37039488554000854, acc: 0.9090909361839294)
[2025-02-13 18:53:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:45][root][INFO] - Training Epoch: 1/2, step 389/7134 completed (loss: 0.4945102035999298, acc: 0.8971428275108337)
[2025-02-13 18:53:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:45][root][INFO] - Training Epoch: 1/2, step 390/7134 completed (loss: 0.39707934856414795, acc: 0.8918918967247009)
[2025-02-13 18:53:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:46][root][INFO] - Training Epoch: 1/2, step 391/7134 completed (loss: 0.4282582998275757, acc: 0.9066666960716248)
[2025-02-13 18:53:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:46][root][INFO] - Training Epoch: 1/2, step 392/7134 completed (loss: 0.4054214060306549, acc: 0.8779069781303406)
[2025-02-13 18:53:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:46][root][INFO] - Training Epoch: 1/2, step 393/7134 completed (loss: 0.4405688941478729, acc: 0.8877005577087402)
[2025-02-13 18:53:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:47][root][INFO] - Training Epoch: 1/2, step 394/7134 completed (loss: 0.3356594741344452, acc: 0.9083969593048096)
[2025-02-13 18:53:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:47][root][INFO] - Training Epoch: 1/2, step 395/7134 completed (loss: 0.4477035105228424, acc: 0.9042553305625916)
[2025-02-13 18:53:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:47][root][INFO] - Training Epoch: 1/2, step 396/7134 completed (loss: 0.4220294952392578, acc: 0.8879310488700867)
[2025-02-13 18:53:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:48][root][INFO] - Training Epoch: 1/2, step 397/7134 completed (loss: 0.6597163081169128, acc: 0.8804348111152649)
[2025-02-13 18:53:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:48][root][INFO] - Training Epoch: 1/2, step 398/7134 completed (loss: 0.48950064182281494, acc: 0.8989899158477783)
[2025-02-13 18:53:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:49][root][INFO] - Training Epoch: 1/2, step 399/7134 completed (loss: 0.3590879440307617, acc: 0.9240506291389465)
[2025-02-13 18:53:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:49][root][INFO] - Training Epoch: 1/2, step 400/7134 completed (loss: 0.7559072971343994, acc: 0.8291139006614685)
[2025-02-13 18:53:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:49][root][INFO] - Training Epoch: 1/2, step 401/7134 completed (loss: 0.44099706411361694, acc: 0.8970588445663452)
[2025-02-13 18:53:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:50][root][INFO] - Training Epoch: 1/2, step 402/7134 completed (loss: 0.3231601119041443, acc: 0.909547746181488)
[2025-02-13 18:53:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:50][root][INFO] - Training Epoch: 1/2, step 403/7134 completed (loss: 0.42292511463165283, acc: 0.903930127620697)
[2025-02-13 18:53:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:51][root][INFO] - Training Epoch: 1/2, step 404/7134 completed (loss: 0.47693178057670593, acc: 0.8870967626571655)
[2025-02-13 18:53:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:51][root][INFO] - Training Epoch: 1/2, step 405/7134 completed (loss: 0.675896406173706, acc: 0.819767415523529)
[2025-02-13 18:53:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:52][root][INFO] - Training Epoch: 1/2, step 406/7134 completed (loss: 0.7733315229415894, acc: 0.8432432413101196)
[2025-02-13 18:53:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:52][root][INFO] - Training Epoch: 1/2, step 407/7134 completed (loss: 0.921225368976593, acc: 0.8280254602432251)
[2025-02-13 18:53:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:52][root][INFO] - Training Epoch: 1/2, step 408/7134 completed (loss: 0.6995388269424438, acc: 0.8379888534545898)
[2025-02-13 18:53:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:53][root][INFO] - Training Epoch: 1/2, step 409/7134 completed (loss: 0.5652292370796204, acc: 0.8999999761581421)
[2025-02-13 18:53:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:53][root][INFO] - Training Epoch: 1/2, step 410/7134 completed (loss: 0.601707935333252, acc: 0.8727272748947144)
[2025-02-13 18:53:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:53][root][INFO] - Training Epoch: 1/2, step 411/7134 completed (loss: 0.4837024211883545, acc: 0.8873873949050903)
[2025-02-13 18:53:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:54][root][INFO] - Training Epoch: 1/2, step 412/7134 completed (loss: 0.4888972342014313, acc: 0.9058296084403992)
[2025-02-13 18:53:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:54][root][INFO] - Training Epoch: 1/2, step 413/7134 completed (loss: 0.6579817533493042, acc: 0.8272251486778259)
[2025-02-13 18:53:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:55][root][INFO] - Training Epoch: 1/2, step 414/7134 completed (loss: 0.5805597305297852, acc: 0.8554913401603699)
[2025-02-13 18:53:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:55][root][INFO] - Training Epoch: 1/2, step 415/7134 completed (loss: 0.2880336344242096, acc: 0.9333333373069763)
[2025-02-13 18:53:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:55][root][INFO] - Training Epoch: 1/2, step 416/7134 completed (loss: 0.3664339780807495, acc: 0.9230769276618958)
[2025-02-13 18:53:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:56][root][INFO] - Training Epoch: 1/2, step 417/7134 completed (loss: 0.6265657544136047, acc: 0.8681318759918213)
[2025-02-13 18:53:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:56][root][INFO] - Training Epoch: 1/2, step 418/7134 completed (loss: 0.3968934118747711, acc: 0.9033816456794739)
[2025-02-13 18:53:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:57][root][INFO] - Training Epoch: 1/2, step 419/7134 completed (loss: 0.3429934084415436, acc: 0.9306930899620056)
[2025-02-13 18:53:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:57][root][INFO] - Training Epoch: 1/2, step 420/7134 completed (loss: 0.44324854016304016, acc: 0.8928571343421936)
[2025-02-13 18:53:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:57][root][INFO] - Training Epoch: 1/2, step 421/7134 completed (loss: 0.4315564036369324, acc: 0.9182389974594116)
[2025-02-13 18:53:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:58][root][INFO] - Training Epoch: 1/2, step 422/7134 completed (loss: 0.3159210681915283, acc: 0.9230769276618958)
[2025-02-13 18:53:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:58][root][INFO] - Training Epoch: 1/2, step 423/7134 completed (loss: 0.19512656331062317, acc: 0.9438202381134033)
[2025-02-13 18:53:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:59][root][INFO] - Training Epoch: 1/2, step 424/7134 completed (loss: 0.42619526386260986, acc: 0.895061731338501)
[2025-02-13 18:53:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:59][root][INFO] - Training Epoch: 1/2, step 425/7134 completed (loss: 0.16571570932865143, acc: 0.9602272510528564)
[2025-02-13 18:53:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:53:59][root][INFO] - Training Epoch: 1/2, step 426/7134 completed (loss: 0.24468302726745605, acc: 0.929411768913269)
[2025-02-13 18:54:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:00][root][INFO] - Training Epoch: 1/2, step 427/7134 completed (loss: 0.34639453887939453, acc: 0.9182389974594116)
[2025-02-13 18:54:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:00][root][INFO] - Training Epoch: 1/2, step 428/7134 completed (loss: 0.42040443420410156, acc: 0.910179615020752)
[2025-02-13 18:54:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:01][root][INFO] - Training Epoch: 1/2, step 429/7134 completed (loss: 0.5444650053977966, acc: 0.8684210777282715)
[2025-02-13 18:54:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:01][root][INFO] - Training Epoch: 1/2, step 430/7134 completed (loss: 0.3155502378940582, acc: 0.9411764740943909)
[2025-02-13 18:54:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:01][root][INFO] - Training Epoch: 1/2, step 431/7134 completed (loss: 0.3010939657688141, acc: 0.9112426042556763)
[2025-02-13 18:54:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:02][root][INFO] - Training Epoch: 1/2, step 432/7134 completed (loss: 0.39152005314826965, acc: 0.9242424368858337)
[2025-02-13 18:54:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:02][root][INFO] - Training Epoch: 1/2, step 433/7134 completed (loss: 0.5148576498031616, acc: 0.8829787373542786)
[2025-02-13 18:54:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:03][root][INFO] - Training Epoch: 1/2, step 434/7134 completed (loss: 0.32102692127227783, acc: 0.9320987462997437)
[2025-02-13 18:54:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:03][root][INFO] - Training Epoch: 1/2, step 435/7134 completed (loss: 0.36086493730545044, acc: 0.8999999761581421)
[2025-02-13 18:54:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:03][root][INFO] - Training Epoch: 1/2, step 436/7134 completed (loss: 0.310928612947464, acc: 0.9305555820465088)
[2025-02-13 18:54:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:04][root][INFO] - Training Epoch: 1/2, step 437/7134 completed (loss: 0.3989722728729248, acc: 0.9295774698257446)
[2025-02-13 18:54:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:04][root][INFO] - Training Epoch: 1/2, step 438/7134 completed (loss: 0.1790303885936737, acc: 0.9527559280395508)
[2025-02-13 18:54:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:05][root][INFO] - Training Epoch: 1/2, step 439/7134 completed (loss: 0.5536051988601685, acc: 0.8563218116760254)
[2025-02-13 18:54:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:05][root][INFO] - Training Epoch: 1/2, step 440/7134 completed (loss: 0.3823382556438446, acc: 0.892307698726654)
[2025-02-13 18:54:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:05][root][INFO] - Training Epoch: 1/2, step 441/7134 completed (loss: 0.21727551519870758, acc: 0.9383561611175537)
[2025-02-13 18:54:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:06][root][INFO] - Training Epoch: 1/2, step 442/7134 completed (loss: 0.23180438578128815, acc: 0.961240291595459)
[2025-02-13 18:54:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:06][root][INFO] - Training Epoch: 1/2, step 443/7134 completed (loss: 0.1821126490831375, acc: 0.9591836929321289)
[2025-02-13 18:54:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:06][root][INFO] - Training Epoch: 1/2, step 444/7134 completed (loss: 0.10966454446315765, acc: 0.9767441749572754)
[2025-02-13 18:54:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:07][root][INFO] - Training Epoch: 1/2, step 445/7134 completed (loss: 0.4956953823566437, acc: 0.8679245114326477)
[2025-02-13 18:54:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:07][root][INFO] - Training Epoch: 1/2, step 446/7134 completed (loss: 0.202890545129776, acc: 0.9526627063751221)
[2025-02-13 18:54:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:08][root][INFO] - Training Epoch: 1/2, step 447/7134 completed (loss: 0.25280073285102844, acc: 0.9263803958892822)
[2025-02-13 18:54:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:08][root][INFO] - Training Epoch: 1/2, step 448/7134 completed (loss: 0.5554531216621399, acc: 0.8730158805847168)
[2025-02-13 18:54:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:08][root][INFO] - Training Epoch: 1/2, step 449/7134 completed (loss: 0.5139971971511841, acc: 0.8617021441459656)
[2025-02-13 18:54:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:09][root][INFO] - Training Epoch: 1/2, step 450/7134 completed (loss: 0.6039745211601257, acc: 0.8301886916160583)
[2025-02-13 18:54:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:09][root][INFO] - Training Epoch: 1/2, step 451/7134 completed (loss: 1.0403164625167847, acc: 0.7881355881690979)
[2025-02-13 18:54:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:10][root][INFO] - Training Epoch: 1/2, step 452/7134 completed (loss: 0.9481938481330872, acc: 0.7921348214149475)
[2025-02-13 18:54:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:10][root][INFO] - Training Epoch: 1/2, step 453/7134 completed (loss: 0.5702006220817566, acc: 0.8723404407501221)
[2025-02-13 18:54:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:10][root][INFO] - Training Epoch: 1/2, step 454/7134 completed (loss: 0.703788697719574, acc: 0.8396624326705933)
[2025-02-13 18:54:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:11][root][INFO] - Training Epoch: 1/2, step 455/7134 completed (loss: 0.8881493806838989, acc: 0.800000011920929)
[2025-02-13 18:54:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:11][root][INFO] - Training Epoch: 1/2, step 456/7134 completed (loss: 0.3644160032272339, acc: 0.9018405079841614)
[2025-02-13 18:54:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:11][root][INFO] - Training Epoch: 1/2, step 457/7134 completed (loss: 0.7328878045082092, acc: 0.8413792848587036)
[2025-02-13 18:54:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:12][root][INFO] - Training Epoch: 1/2, step 458/7134 completed (loss: 0.5206406712532043, acc: 0.8870967626571655)
[2025-02-13 18:54:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:12][root][INFO] - Training Epoch: 1/2, step 459/7134 completed (loss: 0.6701772212982178, acc: 0.8253012299537659)
[2025-02-13 18:54:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:13][root][INFO] - Training Epoch: 1/2, step 460/7134 completed (loss: 0.6330811977386475, acc: 0.8533333539962769)
[2025-02-13 18:54:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:13][root][INFO] - Training Epoch: 1/2, step 461/7134 completed (loss: 0.492364764213562, acc: 0.8705882430076599)
[2025-02-13 18:54:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:13][root][INFO] - Training Epoch: 1/2, step 462/7134 completed (loss: 0.5815476775169373, acc: 0.875)
[2025-02-13 18:54:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:14][root][INFO] - Training Epoch: 1/2, step 463/7134 completed (loss: 0.9901999235153198, acc: 0.7910447716712952)
[2025-02-13 18:54:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:14][root][INFO] - Training Epoch: 1/2, step 464/7134 completed (loss: 0.6898608207702637, acc: 0.8465909361839294)
[2025-02-13 18:54:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:15][root][INFO] - Training Epoch: 1/2, step 465/7134 completed (loss: 0.47597500681877136, acc: 0.9050279259681702)
[2025-02-13 18:54:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:15][root][INFO] - Training Epoch: 1/2, step 466/7134 completed (loss: 0.6974455714225769, acc: 0.8538011908531189)
[2025-02-13 18:54:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:15][root][INFO] - Training Epoch: 1/2, step 467/7134 completed (loss: 0.6912090182304382, acc: 0.8677248954772949)
[2025-02-13 18:54:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:16][root][INFO] - Training Epoch: 1/2, step 468/7134 completed (loss: 0.632904589176178, acc: 0.8733333349227905)
[2025-02-13 18:54:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:16][root][INFO] - Training Epoch: 1/2, step 469/7134 completed (loss: 0.5136768817901611, acc: 0.8888888955116272)
[2025-02-13 18:54:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:16][root][INFO] - Training Epoch: 1/2, step 470/7134 completed (loss: 0.6185029745101929, acc: 0.8108108043670654)
[2025-02-13 18:54:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:17][root][INFO] - Training Epoch: 1/2, step 471/7134 completed (loss: 0.7039857506752014, acc: 0.8461538553237915)
[2025-02-13 18:54:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:17][root][INFO] - Training Epoch: 1/2, step 472/7134 completed (loss: 0.6664640307426453, acc: 0.8533333539962769)
[2025-02-13 18:54:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:18][root][INFO] - Training Epoch: 1/2, step 473/7134 completed (loss: 0.5649852752685547, acc: 0.8560606241226196)
[2025-02-13 18:54:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:18][root][INFO] - Training Epoch: 1/2, step 474/7134 completed (loss: 0.7403106689453125, acc: 0.8484848737716675)
[2025-02-13 18:54:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:18][root][INFO] - Training Epoch: 1/2, step 475/7134 completed (loss: 0.9567469954490662, acc: 0.8041958212852478)
[2025-02-13 18:54:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:19][root][INFO] - Training Epoch: 1/2, step 476/7134 completed (loss: 0.7722261548042297, acc: 0.8187500238418579)
[2025-02-13 18:54:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:19][root][INFO] - Training Epoch: 1/2, step 477/7134 completed (loss: 0.7370620965957642, acc: 0.8288770318031311)
[2025-02-13 18:54:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:20][root][INFO] - Training Epoch: 1/2, step 478/7134 completed (loss: 0.9831591248512268, acc: 0.747826099395752)
[2025-02-13 18:54:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:20][root][INFO] - Training Epoch: 1/2, step 479/7134 completed (loss: 0.654768705368042, acc: 0.8543046116828918)
[2025-02-13 18:54:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:20][root][INFO] - Training Epoch: 1/2, step 480/7134 completed (loss: 0.7641435861587524, acc: 0.824999988079071)
[2025-02-13 18:54:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:21][root][INFO] - Training Epoch: 1/2, step 481/7134 completed (loss: 0.4630238711833954, acc: 0.9068322777748108)
[2025-02-13 18:54:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:21][root][INFO] - Training Epoch: 1/2, step 482/7134 completed (loss: 0.3772892355918884, acc: 0.9308176040649414)
[2025-02-13 18:54:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:22][root][INFO] - Training Epoch: 1/2, step 483/7134 completed (loss: 0.37761038541793823, acc: 0.9230769276618958)
[2025-02-13 18:54:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:22][root][INFO] - Training Epoch: 1/2, step 484/7134 completed (loss: 0.8079453110694885, acc: 0.8367347121238708)
[2025-02-13 18:54:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:22][root][INFO] - Training Epoch: 1/2, step 485/7134 completed (loss: 0.8556914925575256, acc: 0.8322981595993042)
[2025-02-13 18:54:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:23][root][INFO] - Training Epoch: 1/2, step 486/7134 completed (loss: 0.7991499900817871, acc: 0.8064516186714172)
[2025-02-13 18:54:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:23][root][INFO] - Training Epoch: 1/2, step 487/7134 completed (loss: 0.5149998664855957, acc: 0.8827160596847534)
[2025-02-13 18:54:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:23][root][INFO] - Training Epoch: 1/2, step 488/7134 completed (loss: 0.8210080862045288, acc: 0.8208954930305481)
[2025-02-13 18:54:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:24][root][INFO] - Training Epoch: 1/2, step 489/7134 completed (loss: 0.7107293009757996, acc: 0.8633093237876892)
[2025-02-13 18:54:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:24][root][INFO] - Training Epoch: 1/2, step 490/7134 completed (loss: 0.4800684452056885, acc: 0.8734177350997925)
[2025-02-13 18:54:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:25][root][INFO] - Training Epoch: 1/2, step 491/7134 completed (loss: 0.622185230255127, acc: 0.8541666865348816)
[2025-02-13 18:54:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:25][root][INFO] - Training Epoch: 1/2, step 492/7134 completed (loss: 0.18750634789466858, acc: 0.9612902998924255)
[2025-02-13 18:54:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:25][root][INFO] - Training Epoch: 1/2, step 493/7134 completed (loss: 0.4681604504585266, acc: 0.8875739574432373)
[2025-02-13 18:54:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:26][root][INFO] - Training Epoch: 1/2, step 494/7134 completed (loss: 0.23146431148052216, acc: 0.95333331823349)
[2025-02-13 18:54:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:26][root][INFO] - Training Epoch: 1/2, step 495/7134 completed (loss: 0.48827871680259705, acc: 0.8925619721412659)
[2025-02-13 18:54:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:26][root][INFO] - Training Epoch: 1/2, step 496/7134 completed (loss: 1.104923963546753, acc: 0.7684210538864136)
[2025-02-13 18:54:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:27][root][INFO] - Training Epoch: 1/2, step 497/7134 completed (loss: 0.4277072548866272, acc: 0.9076923131942749)
[2025-02-13 18:54:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:27][root][INFO] - Training Epoch: 1/2, step 498/7134 completed (loss: 0.4309709370136261, acc: 0.8926174640655518)
[2025-02-13 18:54:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:28][root][INFO] - Training Epoch: 1/2, step 499/7134 completed (loss: 0.36206766963005066, acc: 0.9058823585510254)
[2025-02-13 18:54:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:28][root][INFO] - Training Epoch: 1/2, step 500/7134 completed (loss: 0.49489468336105347, acc: 0.8662790656089783)
[2025-02-13 18:54:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:28][root][INFO] - Training Epoch: 1/2, step 501/7134 completed (loss: 0.47331300377845764, acc: 0.8786126971244812)
[2025-02-13 18:54:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:29][root][INFO] - Training Epoch: 1/2, step 502/7134 completed (loss: 0.2250986397266388, acc: 0.9285714030265808)
[2025-02-13 18:54:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:29][root][INFO] - Training Epoch: 1/2, step 503/7134 completed (loss: 0.4647029936313629, acc: 0.8806818127632141)
[2025-02-13 18:54:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:30][root][INFO] - Training Epoch: 1/2, step 504/7134 completed (loss: 0.3881129324436188, acc: 0.9378882050514221)
[2025-02-13 18:54:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:30][root][INFO] - Training Epoch: 1/2, step 505/7134 completed (loss: 0.18438903987407684, acc: 0.9404761791229248)
[2025-02-13 18:54:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:30][root][INFO] - Training Epoch: 1/2, step 506/7134 completed (loss: 0.4612416923046112, acc: 0.9090909361839294)
[2025-02-13 18:54:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:31][root][INFO] - Training Epoch: 1/2, step 507/7134 completed (loss: 0.3596452474594116, acc: 0.9180327653884888)
[2025-02-13 18:54:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:31][root][INFO] - Training Epoch: 1/2, step 508/7134 completed (loss: 0.3370038568973541, acc: 0.9411764740943909)
[2025-02-13 18:54:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:32][root][INFO] - Training Epoch: 1/2, step 509/7134 completed (loss: 0.37232598662376404, acc: 0.8918918967247009)
[2025-02-13 18:54:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:32][root][INFO] - Training Epoch: 1/2, step 510/7134 completed (loss: 0.6086068153381348, acc: 0.8780487775802612)
[2025-02-13 18:54:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:32][root][INFO] - Training Epoch: 1/2, step 511/7134 completed (loss: 0.4845310151576996, acc: 0.8952381014823914)
[2025-02-13 18:54:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:33][root][INFO] - Training Epoch: 1/2, step 512/7134 completed (loss: 0.7074047923088074, acc: 0.8396946787834167)
[2025-02-13 18:54:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:33][root][INFO] - Training Epoch: 1/2, step 513/7134 completed (loss: 0.5849385261535645, acc: 0.8791946172714233)
[2025-02-13 18:54:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:33][root][INFO] - Training Epoch: 1/2, step 514/7134 completed (loss: 0.8052019476890564, acc: 0.8425925970077515)
[2025-02-13 18:54:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:34][root][INFO] - Training Epoch: 1/2, step 515/7134 completed (loss: 0.44147658348083496, acc: 0.898809552192688)
[2025-02-13 18:54:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:34][root][INFO] - Training Epoch: 1/2, step 516/7134 completed (loss: 0.6267412304878235, acc: 0.8813559412956238)
[2025-02-13 18:54:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:35][root][INFO] - Training Epoch: 1/2, step 517/7134 completed (loss: 0.7008405327796936, acc: 0.8623188138008118)
[2025-02-13 18:54:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:35][root][INFO] - Training Epoch: 1/2, step 518/7134 completed (loss: 0.3705928921699524, acc: 0.9271523356437683)
[2025-02-13 18:54:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:35][root][INFO] - Training Epoch: 1/2, step 519/7134 completed (loss: 0.3306303322315216, acc: 0.9304347634315491)
[2025-02-13 18:54:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:36][root][INFO] - Training Epoch: 1/2, step 520/7134 completed (loss: 0.4453488290309906, acc: 0.8815789222717285)
[2025-02-13 18:54:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:36][root][INFO] - Training Epoch: 1/2, step 521/7134 completed (loss: 0.42183130979537964, acc: 0.8888888955116272)
[2025-02-13 18:54:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:36][root][INFO] - Training Epoch: 1/2, step 522/7134 completed (loss: 0.3770282566547394, acc: 0.898876428604126)
[2025-02-13 18:54:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:37][root][INFO] - Training Epoch: 1/2, step 523/7134 completed (loss: 0.38509494066238403, acc: 0.9127907156944275)
[2025-02-13 18:54:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:37][root][INFO] - Training Epoch: 1/2, step 524/7134 completed (loss: 0.29513880610466003, acc: 0.9435028433799744)
[2025-02-13 18:54:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:38][root][INFO] - Training Epoch: 1/2, step 525/7134 completed (loss: 0.4307849705219269, acc: 0.89570552110672)
[2025-02-13 18:54:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:38][root][INFO] - Training Epoch: 1/2, step 526/7134 completed (loss: 0.32877057790756226, acc: 0.9034090638160706)
[2025-02-13 18:54:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:38][root][INFO] - Training Epoch: 1/2, step 527/7134 completed (loss: 0.3011068105697632, acc: 0.921875)
[2025-02-13 18:54:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:39][root][INFO] - Training Epoch: 1/2, step 528/7134 completed (loss: 0.3291696012020111, acc: 0.9224137663841248)
[2025-02-13 18:54:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:39][root][INFO] - Training Epoch: 1/2, step 529/7134 completed (loss: 0.543354868888855, acc: 0.8789808750152588)
[2025-02-13 18:54:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:39][root][INFO] - Training Epoch: 1/2, step 530/7134 completed (loss: 0.3486977517604828, acc: 0.9153845906257629)
[2025-02-13 18:54:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:40][root][INFO] - Training Epoch: 1/2, step 531/7134 completed (loss: 0.630221426486969, acc: 0.8698630332946777)
[2025-02-13 18:54:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:40][root][INFO] - Training Epoch: 1/2, step 532/7134 completed (loss: 0.28437262773513794, acc: 0.8938053250312805)
[2025-02-13 18:54:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:41][root][INFO] - Training Epoch: 1/2, step 533/7134 completed (loss: 0.6678290367126465, acc: 0.8571428656578064)
[2025-02-13 18:54:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:41][root][INFO] - Training Epoch: 1/2, step 534/7134 completed (loss: 0.29591625928878784, acc: 0.9221556782722473)
[2025-02-13 18:54:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:41][root][INFO] - Training Epoch: 1/2, step 535/7134 completed (loss: 0.4136371910572052, acc: 0.8832116723060608)
[2025-02-13 18:54:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:42][root][INFO] - Training Epoch: 1/2, step 536/7134 completed (loss: 0.4216555058956146, acc: 0.8841463327407837)
[2025-02-13 18:54:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:42][root][INFO] - Training Epoch: 1/2, step 537/7134 completed (loss: 0.516456663608551, acc: 0.849711000919342)
[2025-02-13 18:54:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:42][root][INFO] - Training Epoch: 1/2, step 538/7134 completed (loss: 0.32556968927383423, acc: 0.9397590160369873)
[2025-02-13 18:54:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:43][root][INFO] - Training Epoch: 1/2, step 539/7134 completed (loss: 0.4805600643157959, acc: 0.885496199131012)
[2025-02-13 18:54:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:43][root][INFO] - Training Epoch: 1/2, step 540/7134 completed (loss: 0.4584195613861084, acc: 0.8584905862808228)
[2025-02-13 18:54:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:44][root][INFO] - Training Epoch: 1/2, step 541/7134 completed (loss: 0.6362109780311584, acc: 0.8333333134651184)
[2025-02-13 18:54:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:44][root][INFO] - Training Epoch: 1/2, step 542/7134 completed (loss: 0.828396201133728, acc: 0.8199999928474426)
[2025-02-13 18:54:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:44][root][INFO] - Training Epoch: 1/2, step 543/7134 completed (loss: 0.8717114329338074, acc: 0.7763158082962036)
[2025-02-13 18:54:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:45][root][INFO] - Training Epoch: 1/2, step 544/7134 completed (loss: 0.6013321876525879, acc: 0.8428571224212646)
[2025-02-13 18:54:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:45][root][INFO] - Training Epoch: 1/2, step 545/7134 completed (loss: 0.675937294960022, acc: 0.8421052694320679)
[2025-02-13 18:54:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:45][root][INFO] - Training Epoch: 1/2, step 546/7134 completed (loss: 0.9380962252616882, acc: 0.8055555820465088)
[2025-02-13 18:54:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:46][root][INFO] - Training Epoch: 1/2, step 547/7134 completed (loss: 0.5645514130592346, acc: 0.8543046116828918)
[2025-02-13 18:54:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:46][root][INFO] - Training Epoch: 1/2, step 548/7134 completed (loss: 0.4555306136608124, acc: 0.8881579041481018)
[2025-02-13 18:54:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:47][root][INFO] - Training Epoch: 1/2, step 549/7134 completed (loss: 1.0476890802383423, acc: 0.7785714268684387)
[2025-02-13 18:54:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:47][root][INFO] - Training Epoch: 1/2, step 550/7134 completed (loss: 0.6246393918991089, acc: 0.8188976645469666)
[2025-02-13 18:54:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:47][root][INFO] - Training Epoch: 1/2, step 551/7134 completed (loss: 0.6494056582450867, acc: 0.875)
[2025-02-13 18:54:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:48][root][INFO] - Training Epoch: 1/2, step 552/7134 completed (loss: 0.5554131269454956, acc: 0.8435373902320862)
[2025-02-13 18:54:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:48][root][INFO] - Training Epoch: 1/2, step 553/7134 completed (loss: 0.43071502447128296, acc: 0.9157894849777222)
[2025-02-13 18:54:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:49][root][INFO] - Training Epoch: 1/2, step 554/7134 completed (loss: 0.770734429359436, acc: 0.8407643437385559)
[2025-02-13 18:54:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:49][root][INFO] - Training Epoch: 1/2, step 555/7134 completed (loss: 0.8035555481910706, acc: 0.8571428656578064)
[2025-02-13 18:54:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:49][root][INFO] - Training Epoch: 1/2, step 556/7134 completed (loss: 0.3458332419395447, acc: 0.918749988079071)
[2025-02-13 18:54:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:50][root][INFO] - Training Epoch: 1/2, step 557/7134 completed (loss: 0.643258810043335, acc: 0.8652482032775879)
[2025-02-13 18:54:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:50][root][INFO] - Training Epoch: 1/2, step 558/7134 completed (loss: 0.6152824759483337, acc: 0.8653846383094788)
[2025-02-13 18:54:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:51][root][INFO] - Training Epoch: 1/2, step 559/7134 completed (loss: 0.6431461572647095, acc: 0.8590604066848755)
[2025-02-13 18:54:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:51][root][INFO] - Training Epoch: 1/2, step 560/7134 completed (loss: 0.274147629737854, acc: 0.9186992049217224)
[2025-02-13 18:54:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:52][root][INFO] - Training Epoch: 1/2, step 561/7134 completed (loss: 0.5783898830413818, acc: 0.856249988079071)
[2025-02-13 18:54:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:52][root][INFO] - Training Epoch: 1/2, step 562/7134 completed (loss: 0.38091254234313965, acc: 0.9025974273681641)
[2025-02-13 18:54:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:52][root][INFO] - Training Epoch: 1/2, step 563/7134 completed (loss: 0.7735201716423035, acc: 0.811188817024231)
[2025-02-13 18:54:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:53][root][INFO] - Training Epoch: 1/2, step 564/7134 completed (loss: 0.746268093585968, acc: 0.8187134265899658)
[2025-02-13 18:54:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:53][root][INFO] - Training Epoch: 1/2, step 565/7134 completed (loss: 0.45105159282684326, acc: 0.8976377844810486)
[2025-02-13 18:54:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:53][root][INFO] - Training Epoch: 1/2, step 566/7134 completed (loss: 0.7960574626922607, acc: 0.792792797088623)
[2025-02-13 18:54:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:54][root][INFO] - Training Epoch: 1/2, step 567/7134 completed (loss: 0.6499267816543579, acc: 0.8383233547210693)
[2025-02-13 18:54:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:54][root][INFO] - Training Epoch: 1/2, step 568/7134 completed (loss: 0.5039032101631165, acc: 0.8736842274665833)
[2025-02-13 18:54:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:55][root][INFO] - Training Epoch: 1/2, step 569/7134 completed (loss: 0.40669816732406616, acc: 0.8938547372817993)
[2025-02-13 18:54:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:55][root][INFO] - Training Epoch: 1/2, step 570/7134 completed (loss: 0.6064287424087524, acc: 0.8656716346740723)
[2025-02-13 18:54:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:55][root][INFO] - Training Epoch: 1/2, step 571/7134 completed (loss: 0.38255318999290466, acc: 0.8901098966598511)
[2025-02-13 18:54:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:56][root][INFO] - Training Epoch: 1/2, step 572/7134 completed (loss: 0.40807127952575684, acc: 0.9028571248054504)
[2025-02-13 18:54:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:56][root][INFO] - Training Epoch: 1/2, step 573/7134 completed (loss: 0.4998556971549988, acc: 0.8848484754562378)
[2025-02-13 18:54:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:56][root][INFO] - Training Epoch: 1/2, step 574/7134 completed (loss: 0.3877943158149719, acc: 0.8934911489486694)
[2025-02-13 18:54:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:57][root][INFO] - Training Epoch: 1/2, step 575/7134 completed (loss: 0.35663607716560364, acc: 0.9181286692619324)
[2025-02-13 18:54:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:57][root][INFO] - Training Epoch: 1/2, step 576/7134 completed (loss: 0.3605717122554779, acc: 0.897849440574646)
[2025-02-13 18:54:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:58][root][INFO] - Training Epoch: 1/2, step 577/7134 completed (loss: 0.3909881114959717, acc: 0.9090909361839294)
[2025-02-13 18:54:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:58][root][INFO] - Training Epoch: 1/2, step 578/7134 completed (loss: 0.3907932639122009, acc: 0.9203979969024658)
[2025-02-13 18:54:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:58][root][INFO] - Training Epoch: 1/2, step 579/7134 completed (loss: 0.5718331336975098, acc: 0.8763440847396851)
[2025-02-13 18:54:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:59][root][INFO] - Training Epoch: 1/2, step 580/7134 completed (loss: 0.8027554154396057, acc: 0.8388888835906982)
[2025-02-13 18:54:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:54:59][root][INFO] - Training Epoch: 1/2, step 581/7134 completed (loss: 0.30564209818840027, acc: 0.9539473652839661)
[2025-02-13 18:54:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:00][root][INFO] - Training Epoch: 1/2, step 582/7134 completed (loss: 0.42054176330566406, acc: 0.875)
[2025-02-13 18:55:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:00][root][INFO] - Training Epoch: 1/2, step 583/7134 completed (loss: 0.4741209149360657, acc: 0.8443113565444946)
[2025-02-13 18:55:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:00][root][INFO] - Training Epoch: 1/2, step 584/7134 completed (loss: 0.33634689450263977, acc: 0.9398906826972961)
[2025-02-13 18:55:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:01][root][INFO] - Training Epoch: 1/2, step 585/7134 completed (loss: 0.3256867229938507, acc: 0.9189189076423645)
[2025-02-13 18:55:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:01][root][INFO] - Training Epoch: 1/2, step 586/7134 completed (loss: 0.27555814385414124, acc: 0.9255319237709045)
[2025-02-13 18:55:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:02][root][INFO] - Training Epoch: 1/2, step 587/7134 completed (loss: 0.41426798701286316, acc: 0.90055251121521)
[2025-02-13 18:55:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:02][root][INFO] - Training Epoch: 1/2, step 588/7134 completed (loss: 0.3340444564819336, acc: 0.912162184715271)
[2025-02-13 18:55:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:02][root][INFO] - Training Epoch: 1/2, step 589/7134 completed (loss: 0.36341843008995056, acc: 0.9325153231620789)
[2025-02-13 18:55:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:03][root][INFO] - Training Epoch: 1/2, step 590/7134 completed (loss: 0.4150919020175934, acc: 0.9139072895050049)
[2025-02-13 18:55:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:03][root][INFO] - Training Epoch: 1/2, step 591/7134 completed (loss: 0.3010491132736206, acc: 0.9322034120559692)
[2025-02-13 18:55:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:04][root][INFO] - Training Epoch: 1/2, step 592/7134 completed (loss: 0.26425182819366455, acc: 0.9602272510528564)
[2025-02-13 18:55:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:04][root][INFO] - Training Epoch: 1/2, step 593/7134 completed (loss: 0.34819871187210083, acc: 0.9239766001701355)
[2025-02-13 18:55:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:04][root][INFO] - Training Epoch: 1/2, step 594/7134 completed (loss: 0.5051652193069458, acc: 0.910179615020752)
[2025-02-13 18:55:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:05][root][INFO] - Training Epoch: 1/2, step 595/7134 completed (loss: 0.40612366795539856, acc: 0.9086021780967712)
[2025-02-13 18:55:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:05][root][INFO] - Training Epoch: 1/2, step 596/7134 completed (loss: 0.3304993510246277, acc: 0.9166666865348816)
[2025-02-13 18:55:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:06][root][INFO] - Training Epoch: 1/2, step 597/7134 completed (loss: 0.5308336019515991, acc: 0.8826815485954285)
[2025-02-13 18:55:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:06][root][INFO] - Training Epoch: 1/2, step 598/7134 completed (loss: 0.8013810515403748, acc: 0.8369565010070801)
[2025-02-13 18:55:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:07][root][INFO] - Training Epoch: 1/2, step 599/7134 completed (loss: 0.3391043543815613, acc: 0.9301075339317322)
[2025-02-13 18:55:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:07][root][INFO] - Training Epoch: 1/2, step 600/7134 completed (loss: 0.3880992829799652, acc: 0.8907103538513184)
[2025-02-13 18:55:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:07][root][INFO] - Training Epoch: 1/2, step 601/7134 completed (loss: 0.393048495054245, acc: 0.8806818127632141)
[2025-02-13 18:55:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:08][root][INFO] - Training Epoch: 1/2, step 602/7134 completed (loss: 0.356974333524704, acc: 0.9351351261138916)
[2025-02-13 18:55:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:08][root][INFO] - Training Epoch: 1/2, step 603/7134 completed (loss: 0.46185407042503357, acc: 0.9153439402580261)
[2025-02-13 18:55:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:09][root][INFO] - Training Epoch: 1/2, step 604/7134 completed (loss: 0.5287608504295349, acc: 0.8666666746139526)
[2025-02-13 18:55:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:09][root][INFO] - Training Epoch: 1/2, step 605/7134 completed (loss: 0.4703134000301361, acc: 0.8983957171440125)
[2025-02-13 18:55:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:09][root][INFO] - Training Epoch: 1/2, step 606/7134 completed (loss: 0.35313406586647034, acc: 0.9049999713897705)
[2025-02-13 18:55:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:10][root][INFO] - Training Epoch: 1/2, step 607/7134 completed (loss: 0.41039952635765076, acc: 0.9137930870056152)
[2025-02-13 18:55:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:10][root][INFO] - Training Epoch: 1/2, step 608/7134 completed (loss: 0.4318374991416931, acc: 0.9189189076423645)
[2025-02-13 18:55:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:11][root][INFO] - Training Epoch: 1/2, step 609/7134 completed (loss: 0.5032211542129517, acc: 0.8820512890815735)
[2025-02-13 18:55:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:11][root][INFO] - Training Epoch: 1/2, step 610/7134 completed (loss: 0.3948926627635956, acc: 0.8768472671508789)
[2025-02-13 18:55:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:12][root][INFO] - Training Epoch: 1/2, step 611/7134 completed (loss: 0.390194296836853, acc: 0.9209039807319641)
[2025-02-13 18:55:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:12][root][INFO] - Training Epoch: 1/2, step 612/7134 completed (loss: 0.45061907172203064, acc: 0.89673912525177)
[2025-02-13 18:55:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:12][root][INFO] - Training Epoch: 1/2, step 613/7134 completed (loss: 0.37700939178466797, acc: 0.9242424368858337)
[2025-02-13 18:55:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:13][root][INFO] - Training Epoch: 1/2, step 614/7134 completed (loss: 0.3213646709918976, acc: 0.9347826242446899)
[2025-02-13 18:55:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:13][root][INFO] - Training Epoch: 1/2, step 615/7134 completed (loss: 0.28090423345565796, acc: 0.9263157844543457)
[2025-02-13 18:55:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:14][root][INFO] - Training Epoch: 1/2, step 616/7134 completed (loss: 0.376427561044693, acc: 0.921658992767334)
[2025-02-13 18:55:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:14][root][INFO] - Training Epoch: 1/2, step 617/7134 completed (loss: 0.31161922216415405, acc: 0.9282296895980835)
[2025-02-13 18:55:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:15][root][INFO] - Training Epoch: 1/2, step 618/7134 completed (loss: 0.367842435836792, acc: 0.903743326663971)
[2025-02-13 18:55:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:15][root][INFO] - Training Epoch: 1/2, step 619/7134 completed (loss: 0.3692621886730194, acc: 0.9098360538482666)
[2025-02-13 18:55:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:16][root][INFO] - Training Epoch: 1/2, step 620/7134 completed (loss: 0.37001362442970276, acc: 0.9246575236320496)
[2025-02-13 18:55:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:16][root][INFO] - Training Epoch: 1/2, step 621/7134 completed (loss: 0.43282729387283325, acc: 0.8799999952316284)
[2025-02-13 18:55:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:16][root][INFO] - Training Epoch: 1/2, step 622/7134 completed (loss: 0.38025379180908203, acc: 0.8947368264198303)
[2025-02-13 18:55:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:17][root][INFO] - Training Epoch: 1/2, step 623/7134 completed (loss: 0.4855211079120636, acc: 0.8992805480957031)
[2025-02-13 18:55:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:17][root][INFO] - Training Epoch: 1/2, step 624/7134 completed (loss: 0.3213280141353607, acc: 0.9416058659553528)
[2025-02-13 18:55:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:18][root][INFO] - Training Epoch: 1/2, step 625/7134 completed (loss: 0.3803445100784302, acc: 0.9125000238418579)
[2025-02-13 18:55:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:18][root][INFO] - Training Epoch: 1/2, step 626/7134 completed (loss: 0.5183857679367065, acc: 0.8880000114440918)
[2025-02-13 18:55:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:18][root][INFO] - Training Epoch: 1/2, step 627/7134 completed (loss: 0.351717084646225, acc: 0.9115646481513977)
[2025-02-13 18:55:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:19][root][INFO] - Training Epoch: 1/2, step 628/7134 completed (loss: 0.377664178609848, acc: 0.9115646481513977)
[2025-02-13 18:55:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:19][root][INFO] - Training Epoch: 1/2, step 629/7134 completed (loss: 0.5580853223800659, acc: 0.8865247964859009)
[2025-02-13 18:55:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:20][root][INFO] - Training Epoch: 1/2, step 630/7134 completed (loss: 0.6690793633460999, acc: 0.8471337556838989)
[2025-02-13 18:55:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:20][root][INFO] - Training Epoch: 1/2, step 631/7134 completed (loss: 0.3820408880710602, acc: 0.9090909361839294)
[2025-02-13 18:55:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:21][root][INFO] - Training Epoch: 1/2, step 632/7134 completed (loss: 0.3065660893917084, acc: 0.9154929518699646)
[2025-02-13 18:55:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:21][root][INFO] - Training Epoch: 1/2, step 633/7134 completed (loss: 0.31567496061325073, acc: 0.9437500238418579)
[2025-02-13 18:55:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:21][root][INFO] - Training Epoch: 1/2, step 634/7134 completed (loss: 0.38668790459632874, acc: 0.9090909361839294)
[2025-02-13 18:55:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:22][root][INFO] - Training Epoch: 1/2, step 635/7134 completed (loss: 0.49754706025123596, acc: 0.8766233921051025)
[2025-02-13 18:55:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:22][root][INFO] - Training Epoch: 1/2, step 636/7134 completed (loss: 0.4429026246070862, acc: 0.8922155499458313)
[2025-02-13 18:55:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:23][root][INFO] - Training Epoch: 1/2, step 637/7134 completed (loss: 0.3636877238750458, acc: 0.9280575513839722)
[2025-02-13 18:55:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:23][root][INFO] - Training Epoch: 1/2, step 638/7134 completed (loss: 0.5199019312858582, acc: 0.8590604066848755)
[2025-02-13 18:55:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:23][root][INFO] - Training Epoch: 1/2, step 639/7134 completed (loss: 0.3847860097885132, acc: 0.9071428775787354)
[2025-02-13 18:55:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:24][root][INFO] - Training Epoch: 1/2, step 640/7134 completed (loss: 0.32542696595191956, acc: 0.9492753744125366)
[2025-02-13 18:55:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:24][root][INFO] - Training Epoch: 1/2, step 641/7134 completed (loss: 0.263923317193985, acc: 0.9492753744125366)
[2025-02-13 18:55:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:25][root][INFO] - Training Epoch: 1/2, step 642/7134 completed (loss: 0.2992307245731354, acc: 0.9411764740943909)
[2025-02-13 18:55:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:25][root][INFO] - Training Epoch: 1/2, step 643/7134 completed (loss: 0.2684697210788727, acc: 0.9269663095474243)
[2025-02-13 18:55:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:26][root][INFO] - Training Epoch: 1/2, step 644/7134 completed (loss: 0.20588546991348267, acc: 0.953125)
[2025-02-13 18:55:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:26][root][INFO] - Training Epoch: 1/2, step 645/7134 completed (loss: 0.27317237854003906, acc: 0.9379310607910156)
[2025-02-13 18:55:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:26][root][INFO] - Training Epoch: 1/2, step 646/7134 completed (loss: 0.23088432848453522, acc: 0.9430379867553711)
[2025-02-13 18:55:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:27][root][INFO] - Training Epoch: 1/2, step 647/7134 completed (loss: 0.2988179326057434, acc: 0.9407407641410828)
[2025-02-13 18:55:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:27][root][INFO] - Training Epoch: 1/2, step 648/7134 completed (loss: 0.6900514364242554, acc: 0.8461538553237915)
[2025-02-13 18:55:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:28][root][INFO] - Training Epoch: 1/2, step 649/7134 completed (loss: 0.6469732522964478, acc: 0.8540540337562561)
[2025-02-13 18:55:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:28][root][INFO] - Training Epoch: 1/2, step 650/7134 completed (loss: 0.5873768329620361, acc: 0.8698630332946777)
[2025-02-13 18:55:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:28][root][INFO] - Training Epoch: 1/2, step 651/7134 completed (loss: 0.6030281186103821, acc: 0.8181818127632141)
[2025-02-13 18:55:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:29][root][INFO] - Training Epoch: 1/2, step 652/7134 completed (loss: 0.5633931159973145, acc: 0.8521126508712769)
[2025-02-13 18:55:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:29][root][INFO] - Training Epoch: 1/2, step 653/7134 completed (loss: 0.31487682461738586, acc: 0.9151515364646912)
[2025-02-13 18:55:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:30][root][INFO] - Training Epoch: 1/2, step 654/7134 completed (loss: 0.7213680744171143, acc: 0.8270676732063293)
[2025-02-13 18:55:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:30][root][INFO] - Training Epoch: 1/2, step 655/7134 completed (loss: 0.29071173071861267, acc: 0.931034505367279)
[2025-02-13 18:55:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:31][root][INFO] - Training Epoch: 1/2, step 656/7134 completed (loss: 0.47959384322166443, acc: 0.8989899158477783)
[2025-02-13 18:55:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:31][root][INFO] - Training Epoch: 1/2, step 657/7134 completed (loss: 0.46864795684814453, acc: 0.8802816867828369)
[2025-02-13 18:55:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:31][root][INFO] - Training Epoch: 1/2, step 658/7134 completed (loss: 0.18364916741847992, acc: 0.9491525292396545)
[2025-02-13 18:55:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:32][root][INFO] - Training Epoch: 1/2, step 659/7134 completed (loss: 0.16525140404701233, acc: 0.970059871673584)
[2025-02-13 18:55:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:32][root][INFO] - Training Epoch: 1/2, step 660/7134 completed (loss: 0.34431296586990356, acc: 0.9235293865203857)
[2025-02-13 18:55:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:33][root][INFO] - Training Epoch: 1/2, step 661/7134 completed (loss: 0.4271021783351898, acc: 0.9025423526763916)
[2025-02-13 18:55:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:33][root][INFO] - Training Epoch: 1/2, step 662/7134 completed (loss: 0.25926315784454346, acc: 0.949999988079071)
[2025-02-13 18:55:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:34][root][INFO] - Training Epoch: 1/2, step 663/7134 completed (loss: 0.40942615270614624, acc: 0.917475700378418)
[2025-02-13 18:55:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:34][root][INFO] - Training Epoch: 1/2, step 664/7134 completed (loss: 0.44399645924568176, acc: 0.881118893623352)
[2025-02-13 18:55:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:34][root][INFO] - Training Epoch: 1/2, step 665/7134 completed (loss: 0.6128455996513367, acc: 0.8579235076904297)
[2025-02-13 18:55:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:35][root][INFO] - Training Epoch: 1/2, step 666/7134 completed (loss: 0.603670060634613, acc: 0.8571428656578064)
[2025-02-13 18:55:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:35][root][INFO] - Training Epoch: 1/2, step 667/7134 completed (loss: 0.6322665810585022, acc: 0.886227548122406)
[2025-02-13 18:55:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:36][root][INFO] - Training Epoch: 1/2, step 668/7134 completed (loss: 0.2398189902305603, acc: 0.9453551769256592)
[2025-02-13 18:55:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:36][root][INFO] - Training Epoch: 1/2, step 669/7134 completed (loss: 0.2944048047065735, acc: 0.9273743033409119)
[2025-02-13 18:55:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:36][root][INFO] - Training Epoch: 1/2, step 670/7134 completed (loss: 0.27619725465774536, acc: 0.913241982460022)
[2025-02-13 18:55:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:37][root][INFO] - Training Epoch: 1/2, step 671/7134 completed (loss: 0.4374218285083771, acc: 0.8959276080131531)
[2025-02-13 18:55:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:37][root][INFO] - Training Epoch: 1/2, step 672/7134 completed (loss: 0.3477129638195038, acc: 0.918749988079071)
[2025-02-13 18:55:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:37][root][INFO] - Training Epoch: 1/2, step 673/7134 completed (loss: 0.330156534910202, acc: 0.9353233575820923)
[2025-02-13 18:55:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:38][root][INFO] - Training Epoch: 1/2, step 674/7134 completed (loss: 0.2897774279117584, acc: 0.9329268336296082)
[2025-02-13 18:55:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:38][root][INFO] - Training Epoch: 1/2, step 675/7134 completed (loss: 0.2749970555305481, acc: 0.939226508140564)
[2025-02-13 18:55:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:39][root][INFO] - Training Epoch: 1/2, step 676/7134 completed (loss: 0.22831276059150696, acc: 0.929347813129425)
[2025-02-13 18:55:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:39][root][INFO] - Training Epoch: 1/2, step 677/7134 completed (loss: 0.1855151504278183, acc: 0.9576719403266907)
[2025-02-13 18:55:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:39][root][INFO] - Training Epoch: 1/2, step 678/7134 completed (loss: 0.3656063675880432, acc: 0.9146341681480408)
[2025-02-13 18:55:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:40][root][INFO] - Training Epoch: 1/2, step 679/7134 completed (loss: 0.2982528507709503, acc: 0.9172932505607605)
[2025-02-13 18:55:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:40][root][INFO] - Training Epoch: 1/2, step 680/7134 completed (loss: 0.5022048950195312, acc: 0.8994975090026855)
[2025-02-13 18:55:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:41][root][INFO] - Training Epoch: 1/2, step 681/7134 completed (loss: 0.2820548117160797, acc: 0.9368420839309692)
[2025-02-13 18:55:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:41][root][INFO] - Training Epoch: 1/2, step 682/7134 completed (loss: 0.22786080837249756, acc: 0.9435028433799744)
[2025-02-13 18:55:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:41][root][INFO] - Training Epoch: 1/2, step 683/7134 completed (loss: 0.412930428981781, acc: 0.8972973227500916)
[2025-02-13 18:55:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:42][root][INFO] - Training Epoch: 1/2, step 684/7134 completed (loss: 0.4177553057670593, acc: 0.894444465637207)
[2025-02-13 18:55:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:42][root][INFO] - Training Epoch: 1/2, step 685/7134 completed (loss: 0.3145207464694977, acc: 0.9145728349685669)
[2025-02-13 18:55:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:43][root][INFO] - Training Epoch: 1/2, step 686/7134 completed (loss: 0.1532331258058548, acc: 0.9562841653823853)
[2025-02-13 18:55:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:43][root][INFO] - Training Epoch: 1/2, step 687/7134 completed (loss: 0.32991769909858704, acc: 0.913385808467865)
[2025-02-13 18:55:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:43][root][INFO] - Training Epoch: 1/2, step 688/7134 completed (loss: 0.15309350192546844, acc: 0.949367105960846)
[2025-02-13 18:55:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:44][root][INFO] - Training Epoch: 1/2, step 689/7134 completed (loss: 0.2656114101409912, acc: 0.9259259104728699)
[2025-02-13 18:55:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:44][root][INFO] - Training Epoch: 1/2, step 690/7134 completed (loss: 0.7665194869041443, acc: 0.85161292552948)
[2025-02-13 18:55:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:45][root][INFO] - Training Epoch: 1/2, step 691/7134 completed (loss: 0.704769492149353, acc: 0.8766233921051025)
[2025-02-13 18:55:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:45][root][INFO] - Training Epoch: 1/2, step 692/7134 completed (loss: 0.7575052380561829, acc: 0.8477157354354858)
[2025-02-13 18:55:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:46][root][INFO] - Training Epoch: 1/2, step 693/7134 completed (loss: 0.41956695914268494, acc: 0.8865247964859009)
[2025-02-13 18:55:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:46][root][INFO] - Training Epoch: 1/2, step 694/7134 completed (loss: 0.7376360297203064, acc: 0.8181818127632141)
[2025-02-13 18:55:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:46][root][INFO] - Training Epoch: 1/2, step 695/7134 completed (loss: 0.664307713508606, acc: 0.8208954930305481)
[2025-02-13 18:55:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:47][root][INFO] - Training Epoch: 1/2, step 696/7134 completed (loss: 0.7787183523178101, acc: 0.8301886916160583)
[2025-02-13 18:55:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:47][root][INFO] - Training Epoch: 1/2, step 697/7134 completed (loss: 0.464094877243042, acc: 0.885869562625885)
[2025-02-13 18:55:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:48][root][INFO] - Training Epoch: 1/2, step 698/7134 completed (loss: 0.338409960269928, acc: 0.9242424368858337)
[2025-02-13 18:55:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:48][root][INFO] - Training Epoch: 1/2, step 699/7134 completed (loss: 0.24949102103710175, acc: 0.931506872177124)
[2025-02-13 18:55:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:48][root][INFO] - Training Epoch: 1/2, step 700/7134 completed (loss: 0.5753281712532043, acc: 0.8640000224113464)
[2025-02-13 18:55:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:49][root][INFO] - Training Epoch: 1/2, step 701/7134 completed (loss: 0.6285213232040405, acc: 0.8782608509063721)
[2025-02-13 18:55:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:49][root][INFO] - Training Epoch: 1/2, step 702/7134 completed (loss: 0.872916042804718, acc: 0.8145695328712463)
[2025-02-13 18:55:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:49][root][INFO] - Training Epoch: 1/2, step 703/7134 completed (loss: 0.6342990398406982, acc: 0.8445945978164673)
[2025-02-13 18:55:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:50][root][INFO] - Training Epoch: 1/2, step 704/7134 completed (loss: 0.5171380043029785, acc: 0.9090909361839294)
[2025-02-13 18:55:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:50][root][INFO] - Training Epoch: 1/2, step 705/7134 completed (loss: 0.5542816519737244, acc: 0.8807339668273926)
[2025-02-13 18:55:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:51][root][INFO] - Training Epoch: 1/2, step 706/7134 completed (loss: 0.41535043716430664, acc: 0.8867924809455872)
[2025-02-13 18:55:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:51][root][INFO] - Training Epoch: 1/2, step 707/7134 completed (loss: 0.4508436918258667, acc: 0.896039605140686)
[2025-02-13 18:55:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:52][root][INFO] - Training Epoch: 1/2, step 708/7134 completed (loss: 0.357878178358078, acc: 0.9269663095474243)
[2025-02-13 18:55:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:52][root][INFO] - Training Epoch: 1/2, step 709/7134 completed (loss: 0.43660804629325867, acc: 0.9028571248054504)
[2025-02-13 18:55:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:52][root][INFO] - Training Epoch: 1/2, step 710/7134 completed (loss: 0.25414374470710754, acc: 0.9427083134651184)
[2025-02-13 18:55:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:53][root][INFO] - Training Epoch: 1/2, step 711/7134 completed (loss: 0.3267265260219574, acc: 0.9166666865348816)
[2025-02-13 18:55:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:53][root][INFO] - Training Epoch: 1/2, step 712/7134 completed (loss: 0.1796855330467224, acc: 0.953125)
[2025-02-13 18:55:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:53][root][INFO] - Training Epoch: 1/2, step 713/7134 completed (loss: 0.36394429206848145, acc: 0.929648220539093)
[2025-02-13 18:55:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:54][root][INFO] - Training Epoch: 1/2, step 714/7134 completed (loss: 0.42152145504951477, acc: 0.8999999761581421)
[2025-02-13 18:55:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:54][root][INFO] - Training Epoch: 1/2, step 715/7134 completed (loss: 0.13822607696056366, acc: 0.967391312122345)
[2025-02-13 18:55:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:55][root][INFO] - Training Epoch: 1/2, step 716/7134 completed (loss: 0.1840905249118805, acc: 0.9558823704719543)
[2025-02-13 18:55:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:55][root][INFO] - Training Epoch: 1/2, step 717/7134 completed (loss: 0.285926878452301, acc: 0.9251337051391602)
[2025-02-13 18:55:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:55][root][INFO] - Training Epoch: 1/2, step 718/7134 completed (loss: 0.17725355923175812, acc: 0.9470899701118469)
[2025-02-13 18:55:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:56][root][INFO] - Training Epoch: 1/2, step 719/7134 completed (loss: 0.18121038377285004, acc: 0.9716312289237976)
[2025-02-13 18:55:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:56][root][INFO] - Training Epoch: 1/2, step 720/7134 completed (loss: 0.26653432846069336, acc: 0.9459459185600281)
[2025-02-13 18:55:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:57][root][INFO] - Training Epoch: 1/2, step 721/7134 completed (loss: 0.20623885095119476, acc: 0.9578947424888611)
[2025-02-13 18:55:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:57][root][INFO] - Training Epoch: 1/2, step 722/7134 completed (loss: 0.3913675546646118, acc: 0.8737373948097229)
[2025-02-13 18:55:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:57][root][INFO] - Training Epoch: 1/2, step 723/7134 completed (loss: 0.21386756002902985, acc: 0.9510869383811951)
[2025-02-13 18:55:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:58][root][INFO] - Training Epoch: 1/2, step 724/7134 completed (loss: 0.30827754735946655, acc: 0.9308176040649414)
[2025-02-13 18:55:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:58][root][INFO] - Training Epoch: 1/2, step 725/7134 completed (loss: 0.16707707941532135, acc: 0.9523809552192688)
[2025-02-13 18:55:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:59][root][INFO] - Training Epoch: 1/2, step 726/7134 completed (loss: 0.16801854968070984, acc: 0.9553072452545166)
[2025-02-13 18:55:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:59][root][INFO] - Training Epoch: 1/2, step 727/7134 completed (loss: 0.19195055961608887, acc: 0.9653179049491882)
[2025-02-13 18:55:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:55:59][root][INFO] - Training Epoch: 1/2, step 728/7134 completed (loss: 0.149820938706398, acc: 0.9599999785423279)
[2025-02-13 18:56:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:00][root][INFO] - Training Epoch: 1/2, step 729/7134 completed (loss: 0.1789516806602478, acc: 0.9620253443717957)
[2025-02-13 18:56:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:00][root][INFO] - Training Epoch: 1/2, step 730/7134 completed (loss: 0.12200237065553665, acc: 0.9840425252914429)
[2025-02-13 18:56:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:01][root][INFO] - Training Epoch: 1/2, step 731/7134 completed (loss: 0.1111559346318245, acc: 0.9774011373519897)
[2025-02-13 18:56:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:01][root][INFO] - Training Epoch: 1/2, step 732/7134 completed (loss: 0.5097770690917969, acc: 0.868852436542511)
[2025-02-13 18:56:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:01][root][INFO] - Training Epoch: 1/2, step 733/7134 completed (loss: 0.22615256905555725, acc: 0.9538461565971375)
[2025-02-13 18:56:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:02][root][INFO] - Training Epoch: 1/2, step 734/7134 completed (loss: 0.3770381212234497, acc: 0.8709677457809448)
[2025-02-13 18:56:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:02][root][INFO] - Training Epoch: 1/2, step 735/7134 completed (loss: 0.31320685148239136, acc: 0.9295774698257446)
[2025-02-13 18:56:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:03][root][INFO] - Training Epoch: 1/2, step 736/7134 completed (loss: 0.6919922828674316, acc: 0.8415841460227966)
[2025-02-13 18:56:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:03][root][INFO] - Training Epoch: 1/2, step 737/7134 completed (loss: 0.7604069113731384, acc: 0.8728813529014587)
[2025-02-13 18:56:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:04][root][INFO] - Training Epoch: 1/2, step 738/7134 completed (loss: 0.2763703465461731, acc: 0.9245283007621765)
[2025-02-13 18:56:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:04][root][INFO] - Training Epoch: 1/2, step 739/7134 completed (loss: 0.22919785976409912, acc: 0.939130425453186)
[2025-02-13 18:56:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:04][root][INFO] - Training Epoch: 1/2, step 740/7134 completed (loss: 0.16980160772800446, acc: 0.9593495726585388)
[2025-02-13 18:56:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:05][root][INFO] - Training Epoch: 1/2, step 741/7134 completed (loss: 0.22563567757606506, acc: 0.9555555582046509)
[2025-02-13 18:56:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:05][root][INFO] - Training Epoch: 1/2, step 742/7134 completed (loss: 0.2224338799715042, acc: 0.9263157844543457)
[2025-02-13 18:56:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:06][root][INFO] - Training Epoch: 1/2, step 743/7134 completed (loss: 0.5148189663887024, acc: 0.8911564350128174)
[2025-02-13 18:56:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:06][root][INFO] - Training Epoch: 1/2, step 744/7134 completed (loss: 0.3456716239452362, acc: 0.9300000071525574)
[2025-02-13 18:56:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:06][root][INFO] - Training Epoch: 1/2, step 745/7134 completed (loss: 0.3172902464866638, acc: 0.9411764740943909)
[2025-02-13 18:56:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:07][root][INFO] - Training Epoch: 1/2, step 746/7134 completed (loss: 0.39520707726478577, acc: 0.8778625726699829)
[2025-02-13 18:56:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:07][root][INFO] - Training Epoch: 1/2, step 747/7134 completed (loss: 0.4919513463973999, acc: 0.8818897604942322)
[2025-02-13 18:56:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:08][root][INFO] - Training Epoch: 1/2, step 748/7134 completed (loss: 0.38568729162216187, acc: 0.8731343150138855)
[2025-02-13 18:56:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:08][root][INFO] - Training Epoch: 1/2, step 749/7134 completed (loss: 0.2698669135570526, acc: 0.9285714030265808)
[2025-02-13 18:56:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:08][root][INFO] - Training Epoch: 1/2, step 750/7134 completed (loss: 0.30952197313308716, acc: 0.9173553586006165)
[2025-02-13 18:56:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:09][root][INFO] - Training Epoch: 1/2, step 751/7134 completed (loss: 0.47730353474617004, acc: 0.8799999952316284)
[2025-02-13 18:56:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:09][root][INFO] - Training Epoch: 1/2, step 752/7134 completed (loss: 0.5138518214225769, acc: 0.8918918967247009)
[2025-02-13 18:56:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:09][root][INFO] - Training Epoch: 1/2, step 753/7134 completed (loss: 0.3169593811035156, acc: 0.921875)
[2025-02-13 18:56:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:10][root][INFO] - Training Epoch: 1/2, step 754/7134 completed (loss: 0.31401678919792175, acc: 0.9130434989929199)
[2025-02-13 18:56:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:10][root][INFO] - Training Epoch: 1/2, step 755/7134 completed (loss: 0.7230332493782043, acc: 0.8560000061988831)
[2025-02-13 18:56:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:11][root][INFO] - Training Epoch: 1/2, step 756/7134 completed (loss: 0.2738558053970337, acc: 0.9375)
[2025-02-13 18:56:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:11][root][INFO] - Training Epoch: 1/2, step 757/7134 completed (loss: 0.1579621434211731, acc: 0.9615384340286255)
[2025-02-13 18:56:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:11][root][INFO] - Training Epoch: 1/2, step 758/7134 completed (loss: 0.2329094558954239, acc: 0.9279279112815857)
[2025-02-13 18:56:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:12][root][INFO] - Training Epoch: 1/2, step 759/7134 completed (loss: 0.3479323983192444, acc: 0.9099099040031433)
[2025-02-13 18:56:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:12][root][INFO] - Training Epoch: 1/2, step 760/7134 completed (loss: 0.3179105222225189, acc: 0.9015151262283325)
[2025-02-13 18:56:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:13][root][INFO] - Training Epoch: 1/2, step 761/7134 completed (loss: 0.3573951721191406, acc: 0.910179615020752)
[2025-02-13 18:56:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:13][root][INFO] - Training Epoch: 1/2, step 762/7134 completed (loss: 0.36632758378982544, acc: 0.9202898740768433)
[2025-02-13 18:56:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:13][root][INFO] - Training Epoch: 1/2, step 763/7134 completed (loss: 0.4904557168483734, acc: 0.8881579041481018)
[2025-02-13 18:56:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:14][root][INFO] - Training Epoch: 1/2, step 764/7134 completed (loss: 0.2928375005722046, acc: 0.9230769276618958)
[2025-02-13 18:56:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:14][root][INFO] - Training Epoch: 1/2, step 765/7134 completed (loss: 0.16085483133792877, acc: 0.9496855139732361)
[2025-02-13 18:56:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:15][root][INFO] - Training Epoch: 1/2, step 766/7134 completed (loss: 0.2237258404493332, acc: 0.9375)
[2025-02-13 18:56:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:15][root][INFO] - Training Epoch: 1/2, step 767/7134 completed (loss: 0.3090705871582031, acc: 0.9107142686843872)
[2025-02-13 18:56:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:15][root][INFO] - Training Epoch: 1/2, step 768/7134 completed (loss: 0.37373819947242737, acc: 0.9234972596168518)
[2025-02-13 18:56:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:16][root][INFO] - Training Epoch: 1/2, step 769/7134 completed (loss: 0.24712802469730377, acc: 0.9503105878829956)
[2025-02-13 18:56:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:16][root][INFO] - Training Epoch: 1/2, step 770/7134 completed (loss: 0.3382139205932617, acc: 0.895061731338501)
[2025-02-13 18:56:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:17][root][INFO] - Training Epoch: 1/2, step 771/7134 completed (loss: 0.272546648979187, acc: 0.9124087691307068)
[2025-02-13 18:56:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:17][root][INFO] - Training Epoch: 1/2, step 772/7134 completed (loss: 0.383919894695282, acc: 0.918367326259613)
[2025-02-13 18:56:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:17][root][INFO] - Training Epoch: 1/2, step 773/7134 completed (loss: 0.46931853890419006, acc: 0.8736842274665833)
[2025-02-13 18:56:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:18][root][INFO] - Training Epoch: 1/2, step 774/7134 completed (loss: 0.3202856779098511, acc: 0.908450722694397)
[2025-02-13 18:56:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:18][root][INFO] - Training Epoch: 1/2, step 775/7134 completed (loss: 0.17357125878334045, acc: 0.957446813583374)
[2025-02-13 18:56:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:19][root][INFO] - Training Epoch: 1/2, step 776/7134 completed (loss: 0.07700493186712265, acc: 0.987261176109314)
[2025-02-13 18:56:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:19][root][INFO] - Training Epoch: 1/2, step 777/7134 completed (loss: 0.20553730428218842, acc: 0.9599999785423279)
[2025-02-13 18:56:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:19][root][INFO] - Training Epoch: 1/2, step 778/7134 completed (loss: 0.3079763650894165, acc: 0.9308176040649414)
[2025-02-13 18:56:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:20][root][INFO] - Training Epoch: 1/2, step 779/7134 completed (loss: 0.28770947456359863, acc: 0.9298245906829834)
[2025-02-13 18:56:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:20][root][INFO] - Training Epoch: 1/2, step 780/7134 completed (loss: 0.4006343185901642, acc: 0.8864864706993103)
[2025-02-13 18:56:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:21][root][INFO] - Training Epoch: 1/2, step 781/7134 completed (loss: 0.3522914946079254, acc: 0.9117646813392639)
[2025-02-13 18:56:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:21][root][INFO] - Training Epoch: 1/2, step 782/7134 completed (loss: 0.39423710107803345, acc: 0.9303797483444214)
[2025-02-13 18:56:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:22][root][INFO] - Training Epoch: 1/2, step 783/7134 completed (loss: 0.2866553068161011, acc: 0.9158878326416016)
[2025-02-13 18:56:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:22][root][INFO] - Training Epoch: 1/2, step 784/7134 completed (loss: 0.3423229157924652, acc: 0.9402984976768494)
[2025-02-13 18:56:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:22][root][INFO] - Training Epoch: 1/2, step 785/7134 completed (loss: 0.37377387285232544, acc: 0.8933333158493042)
[2025-02-13 18:56:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:23][root][INFO] - Training Epoch: 1/2, step 786/7134 completed (loss: 0.2595025897026062, acc: 0.9253731369972229)
[2025-02-13 18:56:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:23][root][INFO] - Training Epoch: 1/2, step 787/7134 completed (loss: 0.1379840075969696, acc: 0.9774436354637146)
[2025-02-13 18:56:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:24][root][INFO] - Training Epoch: 1/2, step 788/7134 completed (loss: 0.38922855257987976, acc: 0.916201114654541)
[2025-02-13 18:56:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:24][root][INFO] - Training Epoch: 1/2, step 789/7134 completed (loss: 0.3691966235637665, acc: 0.9166666865348816)
[2025-02-13 18:56:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:24][root][INFO] - Training Epoch: 1/2, step 790/7134 completed (loss: 0.43678051233291626, acc: 0.9054054021835327)
[2025-02-13 18:56:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:25][root][INFO] - Training Epoch: 1/2, step 791/7134 completed (loss: 0.48186472058296204, acc: 0.869918704032898)
[2025-02-13 18:56:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:25][root][INFO] - Training Epoch: 1/2, step 792/7134 completed (loss: 0.36442580819129944, acc: 0.9177215099334717)
[2025-02-13 18:56:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:26][root][INFO] - Training Epoch: 1/2, step 793/7134 completed (loss: 0.19519689679145813, acc: 0.956250011920929)
[2025-02-13 18:56:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:26][root][INFO] - Training Epoch: 1/2, step 794/7134 completed (loss: 0.43762296438217163, acc: 0.9137930870056152)
[2025-02-13 18:56:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:27][root][INFO] - Training Epoch: 1/2, step 795/7134 completed (loss: 0.36314108967781067, acc: 0.9113923907279968)
[2025-02-13 18:56:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:27][root][INFO] - Training Epoch: 1/2, step 796/7134 completed (loss: 0.7222170233726501, acc: 0.8285714387893677)
[2025-02-13 18:56:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:27][root][INFO] - Training Epoch: 1/2, step 797/7134 completed (loss: 0.5525734424591064, acc: 0.8976377844810486)
[2025-02-13 18:56:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:28][root][INFO] - Training Epoch: 1/2, step 798/7134 completed (loss: 0.5345263481140137, acc: 0.8888888955116272)
[2025-02-13 18:56:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:28][root][INFO] - Training Epoch: 1/2, step 799/7134 completed (loss: 0.29023829102516174, acc: 0.9090909361839294)
[2025-02-13 18:56:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:29][root][INFO] - Training Epoch: 1/2, step 800/7134 completed (loss: 0.4630768895149231, acc: 0.8814814686775208)
[2025-02-13 18:56:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:29][root][INFO] - Training Epoch: 1/2, step 801/7134 completed (loss: 0.5366915464401245, acc: 0.8580645322799683)
[2025-02-13 18:56:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:30][root][INFO] - Training Epoch: 1/2, step 802/7134 completed (loss: 0.3632902204990387, acc: 0.9007092118263245)
[2025-02-13 18:56:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:30][root][INFO] - Training Epoch: 1/2, step 803/7134 completed (loss: 0.27571815252304077, acc: 0.9420289993286133)
[2025-02-13 18:56:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:30][root][INFO] - Training Epoch: 1/2, step 804/7134 completed (loss: 0.6247665882110596, acc: 0.8527131676673889)
[2025-02-13 18:56:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:31][root][INFO] - Training Epoch: 1/2, step 805/7134 completed (loss: 0.6144954562187195, acc: 0.8391608595848083)
[2025-02-13 18:56:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:31][root][INFO] - Training Epoch: 1/2, step 806/7134 completed (loss: 0.36928480863571167, acc: 0.895061731338501)
[2025-02-13 18:56:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:32][root][INFO] - Training Epoch: 1/2, step 807/7134 completed (loss: 0.3998776376247406, acc: 0.8899999856948853)
[2025-02-13 18:56:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:32][root][INFO] - Training Epoch: 1/2, step 808/7134 completed (loss: 0.2735660970211029, acc: 0.9543147087097168)
[2025-02-13 18:56:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:33][root][INFO] - Training Epoch: 1/2, step 809/7134 completed (loss: 0.3991313874721527, acc: 0.9066666960716248)
[2025-02-13 18:56:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:33][root][INFO] - Training Epoch: 1/2, step 810/7134 completed (loss: 0.3334977924823761, acc: 0.9153439402580261)
[2025-02-13 18:56:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:33][root][INFO] - Training Epoch: 1/2, step 811/7134 completed (loss: 0.5232835412025452, acc: 0.8482758402824402)
[2025-02-13 18:56:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:34][root][INFO] - Training Epoch: 1/2, step 812/7134 completed (loss: 0.3510946035385132, acc: 0.8894736766815186)
[2025-02-13 18:56:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:34][root][INFO] - Training Epoch: 1/2, step 813/7134 completed (loss: 0.31296274065971375, acc: 0.9235293865203857)
[2025-02-13 18:56:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:34][root][INFO] - Training Epoch: 1/2, step 814/7134 completed (loss: 0.31506893038749695, acc: 0.9209039807319641)
[2025-02-13 18:56:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:35][root][INFO] - Training Epoch: 1/2, step 815/7134 completed (loss: 0.38299354910850525, acc: 0.8974359035491943)
[2025-02-13 18:56:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:35][root][INFO] - Training Epoch: 1/2, step 816/7134 completed (loss: 0.4216271936893463, acc: 0.8936170339584351)
[2025-02-13 18:56:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:36][root][INFO] - Training Epoch: 1/2, step 817/7134 completed (loss: 0.45703426003456116, acc: 0.8848921060562134)
[2025-02-13 18:56:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:36][root][INFO] - Training Epoch: 1/2, step 818/7134 completed (loss: 0.6757722496986389, acc: 0.8758170008659363)
[2025-02-13 18:56:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:37][root][INFO] - Training Epoch: 1/2, step 819/7134 completed (loss: 0.4693719446659088, acc: 0.9246575236320496)
[2025-02-13 18:56:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:37][root][INFO] - Training Epoch: 1/2, step 820/7134 completed (loss: 0.4346223771572113, acc: 0.8896104097366333)
[2025-02-13 18:56:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:37][root][INFO] - Training Epoch: 1/2, step 821/7134 completed (loss: 0.27091744542121887, acc: 0.9479768872261047)
[2025-02-13 18:56:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:38][root][INFO] - Training Epoch: 1/2, step 822/7134 completed (loss: 0.37983545660972595, acc: 0.905063271522522)
[2025-02-13 18:56:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:38][root][INFO] - Training Epoch: 1/2, step 823/7134 completed (loss: 0.24848581850528717, acc: 0.9430894255638123)
[2025-02-13 18:56:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:39][root][INFO] - Training Epoch: 1/2, step 824/7134 completed (loss: 0.5082560181617737, acc: 0.8787878751754761)
[2025-02-13 18:56:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:39][root][INFO] - Training Epoch: 1/2, step 825/7134 completed (loss: 0.5262593626976013, acc: 0.875)
[2025-02-13 18:56:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:40][root][INFO] - Training Epoch: 1/2, step 826/7134 completed (loss: 0.28776147961616516, acc: 0.9356725215911865)
[2025-02-13 18:56:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:40][root][INFO] - Training Epoch: 1/2, step 827/7134 completed (loss: 0.3500094711780548, acc: 0.9064327478408813)
[2025-02-13 18:56:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:40][root][INFO] - Training Epoch: 1/2, step 828/7134 completed (loss: 0.31406471133232117, acc: 0.9220778942108154)
[2025-02-13 18:56:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:41][root][INFO] - Training Epoch: 1/2, step 829/7134 completed (loss: 0.5335984826087952, acc: 0.9038461446762085)
[2025-02-13 18:56:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:41][root][INFO] - Training Epoch: 1/2, step 830/7134 completed (loss: 0.23561343550682068, acc: 0.9424460530281067)
[2025-02-13 18:56:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:42][root][INFO] - Training Epoch: 1/2, step 831/7134 completed (loss: 0.26931867003440857, acc: 0.9457831382751465)
[2025-02-13 18:56:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:42][root][INFO] - Training Epoch: 1/2, step 832/7134 completed (loss: 0.6499635577201843, acc: 0.8367347121238708)
[2025-02-13 18:56:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:42][root][INFO] - Training Epoch: 1/2, step 833/7134 completed (loss: 0.22902359068393707, acc: 0.9345238208770752)
[2025-02-13 18:56:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:43][root][INFO] - Training Epoch: 1/2, step 834/7134 completed (loss: 0.4069344103336334, acc: 0.8882352709770203)
[2025-02-13 18:56:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:43][root][INFO] - Training Epoch: 1/2, step 835/7134 completed (loss: 0.4183413088321686, acc: 0.8888888955116272)
[2025-02-13 18:56:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:44][root][INFO] - Training Epoch: 1/2, step 836/7134 completed (loss: 0.3722681999206543, acc: 0.8984771370887756)
[2025-02-13 18:56:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:44][root][INFO] - Training Epoch: 1/2, step 837/7134 completed (loss: 0.38739585876464844, acc: 0.9209039807319641)
[2025-02-13 18:56:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:45][root][INFO] - Training Epoch: 1/2, step 838/7134 completed (loss: 0.38763925433158875, acc: 0.9019607901573181)
[2025-02-13 18:56:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:45][root][INFO] - Training Epoch: 1/2, step 839/7134 completed (loss: 0.2649821937084198, acc: 0.9306930899620056)
[2025-02-13 18:56:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:45][root][INFO] - Training Epoch: 1/2, step 840/7134 completed (loss: 0.34859833121299744, acc: 0.910179615020752)
[2025-02-13 18:56:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:46][root][INFO] - Training Epoch: 1/2, step 841/7134 completed (loss: 0.2568765878677368, acc: 0.9473684430122375)
[2025-02-13 18:56:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:46][root][INFO] - Training Epoch: 1/2, step 842/7134 completed (loss: 0.2210891991853714, acc: 0.9470587968826294)
[2025-02-13 18:56:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:47][root][INFO] - Training Epoch: 1/2, step 843/7134 completed (loss: 0.23465576767921448, acc: 0.9298245906829834)
[2025-02-13 18:56:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:47][root][INFO] - Training Epoch: 1/2, step 844/7134 completed (loss: 0.23509955406188965, acc: 0.9246575236320496)
[2025-02-13 18:56:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:47][root][INFO] - Training Epoch: 1/2, step 845/7134 completed (loss: 0.27714434266090393, acc: 0.9407407641410828)
[2025-02-13 18:56:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:48][root][INFO] - Training Epoch: 1/2, step 846/7134 completed (loss: 0.38645246624946594, acc: 0.9246575236320496)
[2025-02-13 18:56:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:48][root][INFO] - Training Epoch: 1/2, step 847/7134 completed (loss: 0.4313400685787201, acc: 0.8922155499458313)
[2025-02-13 18:56:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:49][root][INFO] - Training Epoch: 1/2, step 848/7134 completed (loss: 0.34825000166893005, acc: 0.9122806787490845)
[2025-02-13 18:56:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:49][root][INFO] - Training Epoch: 1/2, step 849/7134 completed (loss: 0.2668244540691376, acc: 0.9207317233085632)
[2025-02-13 18:56:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:49][root][INFO] - Training Epoch: 1/2, step 850/7134 completed (loss: 0.1902182698249817, acc: 0.9664429426193237)
[2025-02-13 18:56:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:50][root][INFO] - Training Epoch: 1/2, step 851/7134 completed (loss: 0.2850135862827301, acc: 0.9127907156944275)
[2025-02-13 18:56:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:50][root][INFO] - Training Epoch: 1/2, step 852/7134 completed (loss: 0.2584342062473297, acc: 0.9476439952850342)
[2025-02-13 18:56:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:51][root][INFO] - Training Epoch: 1/2, step 853/7134 completed (loss: 0.36703553795814514, acc: 0.9319371581077576)
[2025-02-13 18:56:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:51][root][INFO] - Training Epoch: 1/2, step 854/7134 completed (loss: 0.22653551399707794, acc: 0.957317054271698)
[2025-02-13 18:56:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:51][root][INFO] - Training Epoch: 1/2, step 855/7134 completed (loss: 0.20625130832195282, acc: 0.9421965479850769)
[2025-02-13 18:56:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:52][root][INFO] - Training Epoch: 1/2, step 856/7134 completed (loss: 0.20031492412090302, acc: 0.9453551769256592)
[2025-02-13 18:56:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:52][root][INFO] - Training Epoch: 1/2, step 857/7134 completed (loss: 0.34274929761886597, acc: 0.9142857193946838)
[2025-02-13 18:56:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:53][root][INFO] - Training Epoch: 1/2, step 858/7134 completed (loss: 0.5411245822906494, acc: 0.886904776096344)
[2025-02-13 18:56:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:53][root][INFO] - Training Epoch: 1/2, step 859/7134 completed (loss: 0.5589663982391357, acc: 0.8644067645072937)
[2025-02-13 18:56:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:53][root][INFO] - Training Epoch: 1/2, step 860/7134 completed (loss: 0.5951873660087585, acc: 0.8675496578216553)
[2025-02-13 18:56:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:54][root][INFO] - Training Epoch: 1/2, step 861/7134 completed (loss: 0.3686685562133789, acc: 0.9032257795333862)
[2025-02-13 18:56:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:54][root][INFO] - Training Epoch: 1/2, step 862/7134 completed (loss: 0.524939239025116, acc: 0.8571428656578064)
[2025-02-13 18:56:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:55][root][INFO] - Training Epoch: 1/2, step 863/7134 completed (loss: 0.5063621997833252, acc: 0.874316930770874)
[2025-02-13 18:56:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:55][root][INFO] - Training Epoch: 1/2, step 864/7134 completed (loss: 0.6046728491783142, acc: 0.8809523582458496)
[2025-02-13 18:56:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:56][root][INFO] - Training Epoch: 1/2, step 865/7134 completed (loss: 0.65911465883255, acc: 0.8488371968269348)
[2025-02-13 18:56:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:56][root][INFO] - Training Epoch: 1/2, step 866/7134 completed (loss: 0.5928875803947449, acc: 0.8714285492897034)
[2025-02-13 18:56:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:57][root][INFO] - Training Epoch: 1/2, step 867/7134 completed (loss: 0.5618871450424194, acc: 0.84375)
[2025-02-13 18:56:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:57][root][INFO] - Training Epoch: 1/2, step 868/7134 completed (loss: 0.8450822234153748, acc: 0.8372092843055725)
[2025-02-13 18:56:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:57][root][INFO] - Training Epoch: 1/2, step 869/7134 completed (loss: 1.0672390460968018, acc: 0.7951807379722595)
[2025-02-13 18:56:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:58][root][INFO] - Training Epoch: 1/2, step 870/7134 completed (loss: 0.3566516041755676, acc: 0.9064327478408813)
[2025-02-13 18:56:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:58][root][INFO] - Training Epoch: 1/2, step 871/7134 completed (loss: 0.39632999897003174, acc: 0.8972602486610413)
[2025-02-13 18:56:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:58][root][INFO] - Training Epoch: 1/2, step 872/7134 completed (loss: 0.2349788397550583, acc: 0.9305555820465088)
[2025-02-13 18:56:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:59][root][INFO] - Training Epoch: 1/2, step 873/7134 completed (loss: 0.4349207878112793, acc: 0.9151515364646912)
[2025-02-13 18:56:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:56:59][root][INFO] - Training Epoch: 1/2, step 874/7134 completed (loss: 0.4668891727924347, acc: 0.8819444179534912)
[2025-02-13 18:56:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:00][root][INFO] - Training Epoch: 1/2, step 875/7134 completed (loss: 0.237291157245636, acc: 0.9568345546722412)
[2025-02-13 18:57:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:00][root][INFO] - Training Epoch: 1/2, step 876/7134 completed (loss: 0.3248301148414612, acc: 0.9210526347160339)
[2025-02-13 18:57:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:00][root][INFO] - Training Epoch: 1/2, step 877/7134 completed (loss: 0.5274816155433655, acc: 0.8895705342292786)
[2025-02-13 18:57:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:01][root][INFO] - Training Epoch: 1/2, step 878/7134 completed (loss: 0.42796140909194946, acc: 0.8999999761581421)
[2025-02-13 18:57:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:01][root][INFO] - Training Epoch: 1/2, step 879/7134 completed (loss: 0.4039175808429718, acc: 0.9108280539512634)
[2025-02-13 18:57:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:02][root][INFO] - Training Epoch: 1/2, step 880/7134 completed (loss: 0.3181956112384796, acc: 0.9263803958892822)
[2025-02-13 18:57:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:02][root][INFO] - Training Epoch: 1/2, step 881/7134 completed (loss: 0.285666823387146, acc: 0.9268292784690857)
[2025-02-13 18:57:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:02][root][INFO] - Training Epoch: 1/2, step 882/7134 completed (loss: 0.250846266746521, acc: 0.9274193644523621)
[2025-02-13 18:57:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:03][root][INFO] - Training Epoch: 1/2, step 883/7134 completed (loss: 0.47038891911506653, acc: 0.9078947305679321)
[2025-02-13 18:57:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:03][root][INFO] - Training Epoch: 1/2, step 884/7134 completed (loss: 0.37903040647506714, acc: 0.9041916131973267)
[2025-02-13 18:57:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:04][root][INFO] - Training Epoch: 1/2, step 885/7134 completed (loss: 0.17837366461753845, acc: 0.9451219439506531)
[2025-02-13 18:57:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:04][root][INFO] - Training Epoch: 1/2, step 886/7134 completed (loss: 0.3444102704524994, acc: 0.9411764740943909)
[2025-02-13 18:57:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:04][root][INFO] - Training Epoch: 1/2, step 887/7134 completed (loss: 0.24357783794403076, acc: 0.9402984976768494)
[2025-02-13 18:57:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:05][root][INFO] - Training Epoch: 1/2, step 888/7134 completed (loss: 0.32488635182380676, acc: 0.9395973086357117)
[2025-02-13 18:57:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:05][root][INFO] - Training Epoch: 1/2, step 889/7134 completed (loss: 0.3146922290325165, acc: 0.9306930899620056)
[2025-02-13 18:57:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:06][root][INFO] - Training Epoch: 1/2, step 890/7134 completed (loss: 0.6007280349731445, acc: 0.899328887462616)
[2025-02-13 18:57:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:06][root][INFO] - Training Epoch: 1/2, step 891/7134 completed (loss: 0.32410338521003723, acc: 0.9497206807136536)
[2025-02-13 18:57:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:06][root][INFO] - Training Epoch: 1/2, step 892/7134 completed (loss: 0.31510716676712036, acc: 0.9285714030265808)
[2025-02-13 18:57:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:07][root][INFO] - Training Epoch: 1/2, step 893/7134 completed (loss: 0.3899911642074585, acc: 0.9281045794487)
[2025-02-13 18:57:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:07][root][INFO] - Training Epoch: 1/2, step 894/7134 completed (loss: 0.22124440968036652, acc: 0.9604519605636597)
[2025-02-13 18:57:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:08][root][INFO] - Training Epoch: 1/2, step 895/7134 completed (loss: 0.21836870908737183, acc: 0.931506872177124)
[2025-02-13 18:57:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:08][root][INFO] - Training Epoch: 1/2, step 896/7134 completed (loss: 0.10443456470966339, acc: 0.9842519760131836)
[2025-02-13 18:57:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:08][root][INFO] - Training Epoch: 1/2, step 897/7134 completed (loss: 0.21823757886886597, acc: 0.9553072452545166)
[2025-02-13 18:57:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:09][root][INFO] - Training Epoch: 1/2, step 898/7134 completed (loss: 0.4554482698440552, acc: 0.8863636255264282)
[2025-02-13 18:57:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:09][root][INFO] - Training Epoch: 1/2, step 899/7134 completed (loss: 0.3085302710533142, acc: 0.9358288645744324)
[2025-02-13 18:57:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:10][root][INFO] - Training Epoch: 1/2, step 900/7134 completed (loss: 0.21178261935710907, acc: 0.9268292784690857)
[2025-02-13 18:57:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:10][root][INFO] - Training Epoch: 1/2, step 901/7134 completed (loss: 0.47429022192955017, acc: 0.898809552192688)
[2025-02-13 18:57:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:10][root][INFO] - Training Epoch: 1/2, step 902/7134 completed (loss: 0.3526391088962555, acc: 0.8961039185523987)
[2025-02-13 18:57:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:11][root][INFO] - Training Epoch: 1/2, step 903/7134 completed (loss: 0.15982922911643982, acc: 0.9595375657081604)
[2025-02-13 18:57:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:11][root][INFO] - Training Epoch: 1/2, step 904/7134 completed (loss: 0.22153380513191223, acc: 0.9604519605636597)
[2025-02-13 18:57:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:12][root][INFO] - Training Epoch: 1/2, step 905/7134 completed (loss: 0.15628398954868317, acc: 0.954023003578186)
[2025-02-13 18:57:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:12][root][INFO] - Training Epoch: 1/2, step 906/7134 completed (loss: 0.2817138135433197, acc: 0.9226519465446472)
[2025-02-13 18:57:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:12][root][INFO] - Training Epoch: 1/2, step 907/7134 completed (loss: 0.27824246883392334, acc: 0.9408602118492126)
[2025-02-13 18:57:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:13][root][INFO] - Training Epoch: 1/2, step 908/7134 completed (loss: 0.26714786887168884, acc: 0.9222221970558167)
[2025-02-13 18:57:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:13][root][INFO] - Training Epoch: 1/2, step 909/7134 completed (loss: 0.30857980251312256, acc: 0.9144737124443054)
[2025-02-13 18:57:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:13][root][INFO] - Training Epoch: 1/2, step 910/7134 completed (loss: 0.3315698504447937, acc: 0.9125000238418579)
[2025-02-13 18:57:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:14][root][INFO] - Training Epoch: 1/2, step 911/7134 completed (loss: 0.26236703991889954, acc: 0.948387086391449)
[2025-02-13 18:57:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:14][root][INFO] - Training Epoch: 1/2, step 912/7134 completed (loss: 0.30654945969581604, acc: 0.9157894849777222)
[2025-02-13 18:57:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:15][root][INFO] - Training Epoch: 1/2, step 913/7134 completed (loss: 0.26966598629951477, acc: 0.9505494236946106)
[2025-02-13 18:57:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:15][root][INFO] - Training Epoch: 1/2, step 914/7134 completed (loss: 0.3129749298095703, acc: 0.9272727370262146)
[2025-02-13 18:57:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:15][root][INFO] - Training Epoch: 1/2, step 915/7134 completed (loss: 0.3586316704750061, acc: 0.8999999761581421)
[2025-02-13 18:57:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:16][root][INFO] - Training Epoch: 1/2, step 916/7134 completed (loss: 0.1767449826002121, acc: 0.9457364082336426)
[2025-02-13 18:57:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:16][root][INFO] - Training Epoch: 1/2, step 917/7134 completed (loss: 0.3955274224281311, acc: 0.9285714030265808)
[2025-02-13 18:57:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:17][root][INFO] - Training Epoch: 1/2, step 918/7134 completed (loss: 0.40413063764572144, acc: 0.9072847962379456)
[2025-02-13 18:57:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:17][root][INFO] - Training Epoch: 1/2, step 919/7134 completed (loss: 0.18352247774600983, acc: 0.9545454382896423)
[2025-02-13 18:57:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:17][root][INFO] - Training Epoch: 1/2, step 920/7134 completed (loss: 0.37233686447143555, acc: 0.9127907156944275)
[2025-02-13 18:57:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:18][root][INFO] - Training Epoch: 1/2, step 921/7134 completed (loss: 0.321732759475708, acc: 0.9239766001701355)
[2025-02-13 18:57:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:18][root][INFO] - Training Epoch: 1/2, step 922/7134 completed (loss: 0.22811755537986755, acc: 0.9477611780166626)
[2025-02-13 18:57:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:18][root][INFO] - Training Epoch: 1/2, step 923/7134 completed (loss: 0.21977704763412476, acc: 0.9532163739204407)
[2025-02-13 18:57:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:19][root][INFO] - Training Epoch: 1/2, step 924/7134 completed (loss: 0.19261623919010162, acc: 0.9604519605636597)
[2025-02-13 18:57:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:19][root][INFO] - Training Epoch: 1/2, step 925/7134 completed (loss: 0.6498998403549194, acc: 0.8387096524238586)
[2025-02-13 18:57:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:20][root][INFO] - Training Epoch: 1/2, step 926/7134 completed (loss: 0.3800918459892273, acc: 0.9166666865348816)
[2025-02-13 18:57:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:20][root][INFO] - Training Epoch: 1/2, step 927/7134 completed (loss: 0.3034157156944275, acc: 0.9136690497398376)
[2025-02-13 18:57:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:21][root][INFO] - Training Epoch: 1/2, step 928/7134 completed (loss: 0.08349830657243729, acc: 0.9908257126808167)
[2025-02-13 18:57:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:21][root][INFO] - Training Epoch: 1/2, step 929/7134 completed (loss: 0.24687190353870392, acc: 0.9539473652839661)
[2025-02-13 18:57:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:21][root][INFO] - Training Epoch: 1/2, step 930/7134 completed (loss: 0.5353371500968933, acc: 0.8709677457809448)
[2025-02-13 18:57:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:22][root][INFO] - Training Epoch: 1/2, step 931/7134 completed (loss: 0.38436025381088257, acc: 0.904411792755127)
[2025-02-13 18:57:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:22][root][INFO] - Training Epoch: 1/2, step 932/7134 completed (loss: 0.24851226806640625, acc: 0.9404761791229248)
[2025-02-13 18:57:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:22][root][INFO] - Training Epoch: 1/2, step 933/7134 completed (loss: 0.39228472113609314, acc: 0.9221556782722473)
[2025-02-13 18:57:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:23][root][INFO] - Training Epoch: 1/2, step 934/7134 completed (loss: 0.24946598708629608, acc: 0.930232584476471)
[2025-02-13 18:57:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:23][root][INFO] - Training Epoch: 1/2, step 935/7134 completed (loss: 0.19638167321681976, acc: 0.9457364082336426)
[2025-02-13 18:57:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:24][root][INFO] - Training Epoch: 1/2, step 936/7134 completed (loss: 0.23623178899288177, acc: 0.9370078444480896)
[2025-02-13 18:57:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:24][root][INFO] - Training Epoch: 1/2, step 937/7134 completed (loss: 0.19824400544166565, acc: 0.9674796462059021)
[2025-02-13 18:57:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:24][root][INFO] - Training Epoch: 1/2, step 938/7134 completed (loss: 0.3460707664489746, acc: 0.9166666865348816)
[2025-02-13 18:57:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:25][root][INFO] - Training Epoch: 1/2, step 939/7134 completed (loss: 0.4433421194553375, acc: 0.9024389982223511)
[2025-02-13 18:57:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:25][root][INFO] - Training Epoch: 1/2, step 940/7134 completed (loss: 0.4140399992465973, acc: 0.913294792175293)
[2025-02-13 18:57:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:26][root][INFO] - Training Epoch: 1/2, step 941/7134 completed (loss: 0.2713061571121216, acc: 0.9382022619247437)
[2025-02-13 18:57:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:26][root][INFO] - Training Epoch: 1/2, step 942/7134 completed (loss: 0.35620400309562683, acc: 0.9127907156944275)
[2025-02-13 18:57:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:26][root][INFO] - Training Epoch: 1/2, step 943/7134 completed (loss: 0.26546379923820496, acc: 0.9263803958892822)
[2025-02-13 18:57:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:27][root][INFO] - Training Epoch: 1/2, step 944/7134 completed (loss: 0.22921451926231384, acc: 0.9554139971733093)
[2025-02-13 18:57:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:27][root][INFO] - Training Epoch: 1/2, step 945/7134 completed (loss: 0.39243558049201965, acc: 0.9271523356437683)
[2025-02-13 18:57:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:28][root][INFO] - Training Epoch: 1/2, step 946/7134 completed (loss: 0.14849837124347687, acc: 0.9805825352668762)
[2025-02-13 18:57:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:28][root][INFO] - Training Epoch: 1/2, step 947/7134 completed (loss: 0.385143905878067, acc: 0.9117646813392639)
[2025-02-13 18:57:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:28][root][INFO] - Training Epoch: 1/2, step 948/7134 completed (loss: 0.28647878766059875, acc: 0.9457831382751465)
[2025-02-13 18:57:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:29][root][INFO] - Training Epoch: 1/2, step 949/7134 completed (loss: 0.28574007749557495, acc: 0.9437500238418579)
[2025-02-13 18:57:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:29][root][INFO] - Training Epoch: 1/2, step 950/7134 completed (loss: 0.2920430898666382, acc: 0.9350649118423462)
[2025-02-13 18:57:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:30][root][INFO] - Training Epoch: 1/2, step 951/7134 completed (loss: 0.2642320692539215, acc: 0.9389312863349915)
[2025-02-13 18:57:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:30][root][INFO] - Training Epoch: 1/2, step 952/7134 completed (loss: 0.5702129602432251, acc: 0.8823529481887817)
[2025-02-13 18:57:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:30][root][INFO] - Training Epoch: 1/2, step 953/7134 completed (loss: 0.511610746383667, acc: 0.8841463327407837)
[2025-02-13 18:57:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:31][root][INFO] - Training Epoch: 1/2, step 954/7134 completed (loss: 0.4223954677581787, acc: 0.9103448390960693)
[2025-02-13 18:57:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:31][root][INFO] - Training Epoch: 1/2, step 955/7134 completed (loss: 0.4001582860946655, acc: 0.9152542352676392)
[2025-02-13 18:57:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:31][root][INFO] - Training Epoch: 1/2, step 956/7134 completed (loss: 0.4457088112831116, acc: 0.91847825050354)
[2025-02-13 18:57:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:32][root][INFO] - Training Epoch: 1/2, step 957/7134 completed (loss: 0.45956066250801086, acc: 0.9117646813392639)
[2025-02-13 18:57:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:32][root][INFO] - Training Epoch: 1/2, step 958/7134 completed (loss: 0.5963025689125061, acc: 0.858208954334259)
[2025-02-13 18:57:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:33][root][INFO] - Training Epoch: 1/2, step 959/7134 completed (loss: 0.5981389880180359, acc: 0.8402062058448792)
[2025-02-13 18:57:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:33][root][INFO] - Training Epoch: 1/2, step 960/7134 completed (loss: 0.7185642123222351, acc: 0.8432835936546326)
[2025-02-13 18:57:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:33][root][INFO] - Training Epoch: 1/2, step 961/7134 completed (loss: 0.45758819580078125, acc: 0.8791208863258362)
[2025-02-13 18:57:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:34][root][INFO] - Training Epoch: 1/2, step 962/7134 completed (loss: 0.6318312287330627, acc: 0.8421052694320679)
[2025-02-13 18:57:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:34][root][INFO] - Training Epoch: 1/2, step 963/7134 completed (loss: 0.4699278473854065, acc: 0.9127907156944275)
[2025-02-13 18:57:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:35][root][INFO] - Training Epoch: 1/2, step 964/7134 completed (loss: 0.7368762493133545, acc: 0.8475610017776489)
[2025-02-13 18:57:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:35][root][INFO] - Training Epoch: 1/2, step 965/7134 completed (loss: 0.5313835144042969, acc: 0.8823529481887817)
[2025-02-13 18:57:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:35][root][INFO] - Training Epoch: 1/2, step 966/7134 completed (loss: 0.40728989243507385, acc: 0.89570552110672)
[2025-02-13 18:57:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:36][root][INFO] - Training Epoch: 1/2, step 967/7134 completed (loss: 0.4197358787059784, acc: 0.914893627166748)
[2025-02-13 18:57:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:36][root][INFO] - Training Epoch: 1/2, step 968/7134 completed (loss: 0.5370326042175293, acc: 0.8496240377426147)
[2025-02-13 18:57:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:36][root][INFO] - Training Epoch: 1/2, step 969/7134 completed (loss: 0.6199538707733154, acc: 0.8799999952316284)
[2025-02-13 18:57:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:37][root][INFO] - Training Epoch: 1/2, step 970/7134 completed (loss: 0.5815480947494507, acc: 0.8897058963775635)
[2025-02-13 18:57:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:37][root][INFO] - Training Epoch: 1/2, step 971/7134 completed (loss: 0.5148620009422302, acc: 0.9052132964134216)
[2025-02-13 18:57:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:38][root][INFO] - Training Epoch: 1/2, step 972/7134 completed (loss: 0.5921010375022888, acc: 0.8564102649688721)
[2025-02-13 18:57:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:38][root][INFO] - Training Epoch: 1/2, step 973/7134 completed (loss: 0.5995103716850281, acc: 0.8626373410224915)
[2025-02-13 18:57:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:39][root][INFO] - Training Epoch: 1/2, step 974/7134 completed (loss: 0.49097827076911926, acc: 0.9108911156654358)
[2025-02-13 18:57:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:39][root][INFO] - Training Epoch: 1/2, step 975/7134 completed (loss: 0.4361869692802429, acc: 0.9255319237709045)
[2025-02-13 18:57:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:39][root][INFO] - Training Epoch: 1/2, step 976/7134 completed (loss: 0.31950291991233826, acc: 0.9006622433662415)
[2025-02-13 18:57:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:40][root][INFO] - Training Epoch: 1/2, step 977/7134 completed (loss: 0.7401055097579956, acc: 0.8089887499809265)
[2025-02-13 18:57:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:40][root][INFO] - Training Epoch: 1/2, step 978/7134 completed (loss: 0.3506767153739929, acc: 0.9226804375648499)
[2025-02-13 18:57:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:40][root][INFO] - Training Epoch: 1/2, step 979/7134 completed (loss: 0.624291718006134, acc: 0.8486486673355103)
[2025-02-13 18:57:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:41][root][INFO] - Training Epoch: 1/2, step 980/7134 completed (loss: 0.6124143600463867, acc: 0.8450704216957092)
[2025-02-13 18:57:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:41][root][INFO] - Training Epoch: 1/2, step 981/7134 completed (loss: 0.5372928977012634, acc: 0.8993710875511169)
[2025-02-13 18:57:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:42][root][INFO] - Training Epoch: 1/2, step 982/7134 completed (loss: 0.6226881146430969, acc: 0.8914285898208618)
[2025-02-13 18:57:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:42][root][INFO] - Training Epoch: 1/2, step 983/7134 completed (loss: 0.3866855800151825, acc: 0.9172932505607605)
[2025-02-13 18:57:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:43][root][INFO] - Training Epoch: 1/2, step 984/7134 completed (loss: 0.29558032751083374, acc: 0.9219858050346375)
[2025-02-13 18:57:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:43][root][INFO] - Training Epoch: 1/2, step 985/7134 completed (loss: 0.2726147472858429, acc: 0.949999988079071)
[2025-02-13 18:57:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:43][root][INFO] - Training Epoch: 1/2, step 986/7134 completed (loss: 0.3418259918689728, acc: 0.9441340565681458)
[2025-02-13 18:57:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:44][root][INFO] - Training Epoch: 1/2, step 987/7134 completed (loss: 0.31436967849731445, acc: 0.9194630980491638)
[2025-02-13 18:57:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:44][root][INFO] - Training Epoch: 1/2, step 988/7134 completed (loss: 0.5223240256309509, acc: 0.8926553726196289)
[2025-02-13 18:57:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:44][root][INFO] - Training Epoch: 1/2, step 989/7134 completed (loss: 0.5358715057373047, acc: 0.89570552110672)
[2025-02-13 18:57:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:45][root][INFO] - Training Epoch: 1/2, step 990/7134 completed (loss: 0.45971205830574036, acc: 0.8846153616905212)
[2025-02-13 18:57:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:45][root][INFO] - Training Epoch: 1/2, step 991/7134 completed (loss: 0.7772598266601562, acc: 0.8364779949188232)
[2025-02-13 18:57:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:46][root][INFO] - Training Epoch: 1/2, step 992/7134 completed (loss: 0.4282456636428833, acc: 0.884393036365509)
[2025-02-13 18:57:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:46][root][INFO] - Training Epoch: 1/2, step 993/7134 completed (loss: 0.32889696955680847, acc: 0.9280575513839722)
[2025-02-13 18:57:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:46][root][INFO] - Training Epoch: 1/2, step 994/7134 completed (loss: 0.41813141107559204, acc: 0.893203854560852)
[2025-02-13 18:57:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:47][root][INFO] - Training Epoch: 1/2, step 995/7134 completed (loss: 0.3637329936027527, acc: 0.9047619104385376)
[2025-02-13 18:57:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:47][root][INFO] - Training Epoch: 1/2, step 996/7134 completed (loss: 0.5037734508514404, acc: 0.895348846912384)
[2025-02-13 18:57:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:48][root][INFO] - Training Epoch: 1/2, step 997/7134 completed (loss: 0.27411168813705444, acc: 0.9352940917015076)
[2025-02-13 18:57:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:48][root][INFO] - Training Epoch: 1/2, step 998/7134 completed (loss: 0.32914674282073975, acc: 0.9221556782722473)
[2025-02-13 18:57:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:48][root][INFO] - Training Epoch: 1/2, step 999/7134 completed (loss: 0.40591248869895935, acc: 0.9076923131942749)
[2025-02-13 18:57:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:49][root][INFO] - Training Epoch: 1/2, step 1000/7134 completed (loss: 0.2367367148399353, acc: 0.970588207244873)
[2025-02-13 18:57:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:49][root][INFO] - Training Epoch: 1/2, step 1001/7134 completed (loss: 0.4416503310203552, acc: 0.9363636374473572)
[2025-02-13 18:57:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:49][root][INFO] - Training Epoch: 1/2, step 1002/7134 completed (loss: 0.26829755306243896, acc: 0.9245283007621765)
[2025-02-13 18:57:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:50][root][INFO] - Training Epoch: 1/2, step 1003/7134 completed (loss: 0.2561768889427185, acc: 0.9375)
[2025-02-13 18:57:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:50][root][INFO] - Training Epoch: 1/2, step 1004/7134 completed (loss: 0.3521239459514618, acc: 0.8938053250312805)
[2025-02-13 18:57:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:51][root][INFO] - Training Epoch: 1/2, step 1005/7134 completed (loss: 0.2789411246776581, acc: 0.9219858050346375)
[2025-02-13 18:57:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:51][root][INFO] - Training Epoch: 1/2, step 1006/7134 completed (loss: 0.39282166957855225, acc: 0.9108280539512634)
[2025-02-13 18:57:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:51][root][INFO] - Training Epoch: 1/2, step 1007/7134 completed (loss: 0.5739230513572693, acc: 0.8591549396514893)
[2025-02-13 18:57:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:52][root][INFO] - Training Epoch: 1/2, step 1008/7134 completed (loss: 0.3531551957130432, acc: 0.9259259104728699)
[2025-02-13 18:57:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:52][root][INFO] - Training Epoch: 1/2, step 1009/7134 completed (loss: 0.25866127014160156, acc: 0.930232584476471)
[2025-02-13 18:57:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:53][root][INFO] - Training Epoch: 1/2, step 1010/7134 completed (loss: 0.4405721426010132, acc: 0.8974359035491943)
[2025-02-13 18:57:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:53][root][INFO] - Training Epoch: 1/2, step 1011/7134 completed (loss: 0.22543299198150635, acc: 0.9496855139732361)
[2025-02-13 18:57:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:53][root][INFO] - Training Epoch: 1/2, step 1012/7134 completed (loss: 0.6197845935821533, acc: 0.8687499761581421)
[2025-02-13 18:57:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:54][root][INFO] - Training Epoch: 1/2, step 1013/7134 completed (loss: 0.4471977949142456, acc: 0.891566276550293)
[2025-02-13 18:57:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:54][root][INFO] - Training Epoch: 1/2, step 1014/7134 completed (loss: 0.41018861532211304, acc: 0.8936170339584351)
[2025-02-13 18:57:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:55][root][INFO] - Training Epoch: 1/2, step 1015/7134 completed (loss: 0.25555843114852905, acc: 0.9337748289108276)
[2025-02-13 18:57:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:55][root][INFO] - Training Epoch: 1/2, step 1016/7134 completed (loss: 0.30076566338539124, acc: 0.924369752407074)
[2025-02-13 18:57:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:55][root][INFO] - Training Epoch: 1/2, step 1017/7134 completed (loss: 0.3277896046638489, acc: 0.9257143139839172)
[2025-02-13 18:57:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:56][root][INFO] - Training Epoch: 1/2, step 1018/7134 completed (loss: 0.5708808898925781, acc: 0.8695651888847351)
[2025-02-13 18:57:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:56][root][INFO] - Training Epoch: 1/2, step 1019/7134 completed (loss: 0.2884020209312439, acc: 0.948051929473877)
[2025-02-13 18:57:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:57][root][INFO] - Training Epoch: 1/2, step 1020/7134 completed (loss: 0.21005506813526154, acc: 0.9363057613372803)
[2025-02-13 18:57:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:57][root][INFO] - Training Epoch: 1/2, step 1021/7134 completed (loss: 0.169621542096138, acc: 0.9673202633857727)
[2025-02-13 18:57:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:57][root][INFO] - Training Epoch: 1/2, step 1022/7134 completed (loss: 0.09843075275421143, acc: 0.9884393215179443)
[2025-02-13 18:57:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:58][root][INFO] - Training Epoch: 1/2, step 1023/7134 completed (loss: 0.23785723745822906, acc: 0.9608938694000244)
[2025-02-13 18:57:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:58][root][INFO] - Training Epoch: 1/2, step 1024/7134 completed (loss: 0.2795918583869934, acc: 0.9477611780166626)
[2025-02-13 18:57:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:59][root][INFO] - Training Epoch: 1/2, step 1025/7134 completed (loss: 0.17874042689800262, acc: 0.9503546357154846)
[2025-02-13 18:57:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:59][root][INFO] - Training Epoch: 1/2, step 1026/7134 completed (loss: 0.4056648910045624, acc: 0.9026548862457275)
[2025-02-13 18:57:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:57:59][root][INFO] - Training Epoch: 1/2, step 1027/7134 completed (loss: 0.23111845552921295, acc: 0.9647058844566345)
[2025-02-13 18:58:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:00][root][INFO] - Training Epoch: 1/2, step 1028/7134 completed (loss: 0.13019959628582, acc: 0.9748427867889404)
[2025-02-13 18:58:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:00][root][INFO] - Training Epoch: 1/2, step 1029/7134 completed (loss: 0.2045743763446808, acc: 0.9629629850387573)
[2025-02-13 18:58:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:01][root][INFO] - Training Epoch: 1/2, step 1030/7134 completed (loss: 0.0835871770977974, acc: 0.982758641242981)
[2025-02-13 18:58:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:01][root][INFO] - Training Epoch: 1/2, step 1031/7134 completed (loss: 0.42475563287734985, acc: 0.888198733329773)
[2025-02-13 18:58:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:01][root][INFO] - Training Epoch: 1/2, step 1032/7134 completed (loss: 0.2494119107723236, acc: 0.936170220375061)
[2025-02-13 18:58:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:02][root][INFO] - Training Epoch: 1/2, step 1033/7134 completed (loss: 0.2769080102443695, acc: 0.931506872177124)
[2025-02-13 18:58:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:02][root][INFO] - Training Epoch: 1/2, step 1034/7134 completed (loss: 0.6635124087333679, acc: 0.8441558480262756)
[2025-02-13 18:58:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:03][root][INFO] - Training Epoch: 1/2, step 1035/7134 completed (loss: 1.29779052734375, acc: 0.7905405163764954)
[2025-02-13 18:58:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:03][root][INFO] - Training Epoch: 1/2, step 1036/7134 completed (loss: 1.4596587419509888, acc: 0.7881355881690979)
[2025-02-13 18:58:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:03][root][INFO] - Training Epoch: 1/2, step 1037/7134 completed (loss: 0.33763495087623596, acc: 0.9255319237709045)
[2025-02-13 18:58:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:04][root][INFO] - Training Epoch: 1/2, step 1038/7134 completed (loss: 0.3969991207122803, acc: 0.9200000166893005)
[2025-02-13 18:58:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:04][root][INFO] - Training Epoch: 1/2, step 1039/7134 completed (loss: 0.3012276291847229, acc: 0.9174311757087708)
[2025-02-13 18:58:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:05][root][INFO] - Training Epoch: 1/2, step 1040/7134 completed (loss: 0.15837812423706055, acc: 0.95652174949646)
[2025-02-13 18:58:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:05][root][INFO] - Training Epoch: 1/2, step 1041/7134 completed (loss: 0.20891734957695007, acc: 0.9530201554298401)
[2025-02-13 18:58:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:05][root][INFO] - Training Epoch: 1/2, step 1042/7134 completed (loss: 0.35658204555511475, acc: 0.8882681727409363)
[2025-02-13 18:58:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:06][root][INFO] - Training Epoch: 1/2, step 1043/7134 completed (loss: 0.3154052793979645, acc: 0.9158415794372559)
[2025-02-13 18:58:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:06][root][INFO] - Training Epoch: 1/2, step 1044/7134 completed (loss: 0.1831752210855484, acc: 0.9440993666648865)
[2025-02-13 18:58:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:07][root][INFO] - Training Epoch: 1/2, step 1045/7134 completed (loss: 0.8412213325500488, acc: 0.8136646151542664)
[2025-02-13 18:58:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:07][root][INFO] - Training Epoch: 1/2, step 1046/7134 completed (loss: 0.4754892885684967, acc: 0.8646616339683533)
[2025-02-13 18:58:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:07][root][INFO] - Training Epoch: 1/2, step 1047/7134 completed (loss: 0.8427877426147461, acc: 0.800000011920929)
[2025-02-13 18:58:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:08][root][INFO] - Training Epoch: 1/2, step 1048/7134 completed (loss: 0.5086225271224976, acc: 0.8853503465652466)
[2025-02-13 18:58:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:08][root][INFO] - Training Epoch: 1/2, step 1049/7134 completed (loss: 0.4577850103378296, acc: 0.9144737124443054)
[2025-02-13 18:58:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:09][root][INFO] - Training Epoch: 1/2, step 1050/7134 completed (loss: 0.5409724116325378, acc: 0.8489583134651184)
[2025-02-13 18:58:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:09][root][INFO] - Training Epoch: 1/2, step 1051/7134 completed (loss: 0.2827332317829132, acc: 0.9314285516738892)
[2025-02-13 18:58:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:09][root][INFO] - Training Epoch: 1/2, step 1052/7134 completed (loss: 0.4954144060611725, acc: 0.891566276550293)
[2025-02-13 18:58:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:10][root][INFO] - Training Epoch: 1/2, step 1053/7134 completed (loss: 0.3475896418094635, acc: 0.9041916131973267)
[2025-02-13 18:58:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:10][root][INFO] - Training Epoch: 1/2, step 1054/7134 completed (loss: 0.2991499602794647, acc: 0.9270833134651184)
[2025-02-13 18:58:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:11][root][INFO] - Training Epoch: 1/2, step 1055/7134 completed (loss: 0.22337080538272858, acc: 0.9541984796524048)
[2025-02-13 18:58:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:11][root][INFO] - Training Epoch: 1/2, step 1056/7134 completed (loss: 0.41437187790870667, acc: 0.9069767594337463)
[2025-02-13 18:58:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:11][root][INFO] - Training Epoch: 1/2, step 1057/7134 completed (loss: 0.22670383751392365, acc: 0.9533678889274597)
[2025-02-13 18:58:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:12][root][INFO] - Training Epoch: 1/2, step 1058/7134 completed (loss: 0.23426282405853271, acc: 0.9578947424888611)
[2025-02-13 18:58:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:12][root][INFO] - Training Epoch: 1/2, step 1059/7134 completed (loss: 0.3135169446468353, acc: 0.9573459625244141)
[2025-02-13 18:58:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:13][root][INFO] - Training Epoch: 1/2, step 1060/7134 completed (loss: 0.2712496519088745, acc: 0.9487179517745972)
[2025-02-13 18:58:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:13][root][INFO] - Training Epoch: 1/2, step 1061/7134 completed (loss: 0.2902979552745819, acc: 0.9278350472450256)
[2025-02-13 18:58:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:13][root][INFO] - Training Epoch: 1/2, step 1062/7134 completed (loss: 0.2926858067512512, acc: 0.9277108311653137)
[2025-02-13 18:58:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:14][root][INFO] - Training Epoch: 1/2, step 1063/7134 completed (loss: 0.19210277497768402, acc: 0.9411764740943909)
[2025-02-13 18:58:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:14][root][INFO] - Training Epoch: 1/2, step 1064/7134 completed (loss: 0.4503839910030365, acc: 0.908108115196228)
[2025-02-13 18:58:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:15][root][INFO] - Training Epoch: 1/2, step 1065/7134 completed (loss: 0.39755576848983765, acc: 0.9226519465446472)
[2025-02-13 18:58:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:15][root][INFO] - Training Epoch: 1/2, step 1066/7134 completed (loss: 0.47138258814811707, acc: 0.885496199131012)
[2025-02-13 18:58:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:15][root][INFO] - Training Epoch: 1/2, step 1067/7134 completed (loss: 0.24765506386756897, acc: 0.9411764740943909)
[2025-02-13 18:58:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:16][root][INFO] - Training Epoch: 1/2, step 1068/7134 completed (loss: 0.3648276627063751, acc: 0.921875)
[2025-02-13 18:58:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:16][root][INFO] - Training Epoch: 1/2, step 1069/7134 completed (loss: 0.34550681710243225, acc: 0.8819444179534912)
[2025-02-13 18:58:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:17][root][INFO] - Training Epoch: 1/2, step 1070/7134 completed (loss: 0.38006412982940674, acc: 0.9452054500579834)
[2025-02-13 18:58:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:17][root][INFO] - Training Epoch: 1/2, step 1071/7134 completed (loss: 0.2690504491329193, acc: 0.9375)
[2025-02-13 18:58:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:17][root][INFO] - Training Epoch: 1/2, step 1072/7134 completed (loss: 0.11304184049367905, acc: 0.9756097793579102)
[2025-02-13 18:58:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:18][root][INFO] - Training Epoch: 1/2, step 1073/7134 completed (loss: 0.178018257021904, acc: 0.9751552939414978)
[2025-02-13 18:58:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:18][root][INFO] - Training Epoch: 1/2, step 1074/7134 completed (loss: 0.40670081973075867, acc: 0.914893627166748)
[2025-02-13 18:58:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:18][root][INFO] - Training Epoch: 1/2, step 1075/7134 completed (loss: 0.6697441339492798, acc: 0.8540145754814148)
[2025-02-13 18:58:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:19][root][INFO] - Training Epoch: 1/2, step 1076/7134 completed (loss: 0.5555413961410522, acc: 0.8563829660415649)
[2025-02-13 18:58:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:19][root][INFO] - Training Epoch: 1/2, step 1077/7134 completed (loss: 0.45585763454437256, acc: 0.9203540086746216)
[2025-02-13 18:58:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:20][root][INFO] - Training Epoch: 1/2, step 1078/7134 completed (loss: 0.6917131543159485, acc: 0.849056601524353)
[2025-02-13 18:58:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:20][root][INFO] - Training Epoch: 1/2, step 1079/7134 completed (loss: 0.46199703216552734, acc: 0.8847926259040833)
[2025-02-13 18:58:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:20][root][INFO] - Training Epoch: 1/2, step 1080/7134 completed (loss: 0.36058861017227173, acc: 0.9224806427955627)
[2025-02-13 18:58:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:21][root][INFO] - Training Epoch: 1/2, step 1081/7134 completed (loss: 0.38103920221328735, acc: 0.9337349534034729)
[2025-02-13 18:58:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:21][root][INFO] - Training Epoch: 1/2, step 1082/7134 completed (loss: 0.1475730687379837, acc: 0.9686098694801331)
[2025-02-13 18:58:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:22][root][INFO] - Training Epoch: 1/2, step 1083/7134 completed (loss: 0.36536213755607605, acc: 0.9187816977500916)
[2025-02-13 18:58:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:22][root][INFO] - Training Epoch: 1/2, step 1084/7134 completed (loss: 0.23678521811962128, acc: 0.9414893388748169)
[2025-02-13 18:58:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:22][root][INFO] - Training Epoch: 1/2, step 1085/7134 completed (loss: 0.31921201944351196, acc: 0.9245283007621765)
[2025-02-13 18:58:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:23][root][INFO] - Training Epoch: 1/2, step 1086/7134 completed (loss: 0.5518339276313782, acc: 0.8790322542190552)
[2025-02-13 18:58:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:23][root][INFO] - Training Epoch: 1/2, step 1087/7134 completed (loss: 0.23898479342460632, acc: 0.9516128897666931)
[2025-02-13 18:58:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:23][root][INFO] - Training Epoch: 1/2, step 1088/7134 completed (loss: 0.28719082474708557, acc: 0.9488636255264282)
[2025-02-13 18:58:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:24][root][INFO] - Training Epoch: 1/2, step 1089/7134 completed (loss: 0.6060683131217957, acc: 0.8711656332015991)
[2025-02-13 18:58:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:24][root][INFO] - Training Epoch: 1/2, step 1090/7134 completed (loss: 0.43782132863998413, acc: 0.898809552192688)
[2025-02-13 18:58:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:25][root][INFO] - Training Epoch: 1/2, step 1091/7134 completed (loss: 0.49345600605010986, acc: 0.8757764101028442)
[2025-02-13 18:58:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:25][root][INFO] - Training Epoch: 1/2, step 1092/7134 completed (loss: 0.2105587273836136, acc: 0.9580838084220886)
[2025-02-13 18:58:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:25][root][INFO] - Training Epoch: 1/2, step 1093/7134 completed (loss: 0.3667072653770447, acc: 0.9319728016853333)
[2025-02-13 18:58:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:26][root][INFO] - Training Epoch: 1/2, step 1094/7134 completed (loss: 0.6212396025657654, acc: 0.8796992301940918)
[2025-02-13 18:58:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:26][root][INFO] - Training Epoch: 1/2, step 1095/7134 completed (loss: 0.2876361012458801, acc: 0.9365079402923584)
[2025-02-13 18:58:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:27][root][INFO] - Training Epoch: 1/2, step 1096/7134 completed (loss: 0.3500957190990448, acc: 0.9139785170555115)
[2025-02-13 18:58:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:27][root][INFO] - Training Epoch: 1/2, step 1097/7134 completed (loss: 0.5228535532951355, acc: 0.8797814249992371)
[2025-02-13 18:58:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:27][root][INFO] - Training Epoch: 1/2, step 1098/7134 completed (loss: 0.19472844898700714, acc: 0.9611111283302307)
[2025-02-13 18:58:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:28][root][INFO] - Training Epoch: 1/2, step 1099/7134 completed (loss: 0.4011133015155792, acc: 0.90625)
[2025-02-13 18:58:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:28][root][INFO] - Training Epoch: 1/2, step 1100/7134 completed (loss: 0.34946173429489136, acc: 0.9239766001701355)
[2025-02-13 18:58:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:29][root][INFO] - Training Epoch: 1/2, step 1101/7134 completed (loss: 0.39483681321144104, acc: 0.9179487228393555)
[2025-02-13 18:58:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:29][root][INFO] - Training Epoch: 1/2, step 1102/7134 completed (loss: 0.6035017967224121, acc: 0.8333333134651184)
[2025-02-13 18:58:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:29][root][INFO] - Training Epoch: 1/2, step 1103/7134 completed (loss: 0.6689428687095642, acc: 0.8809523582458496)
[2025-02-13 18:58:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:30][root][INFO] - Training Epoch: 1/2, step 1104/7134 completed (loss: 0.3351774215698242, acc: 0.9200000166893005)
[2025-02-13 18:58:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:30][root][INFO] - Training Epoch: 1/2, step 1105/7134 completed (loss: 0.4145389199256897, acc: 0.9074074029922485)
[2025-02-13 18:58:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:31][root][INFO] - Training Epoch: 1/2, step 1106/7134 completed (loss: 0.22362154722213745, acc: 0.940397322177887)
[2025-02-13 18:58:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:31][root][INFO] - Training Epoch: 1/2, step 1107/7134 completed (loss: 0.22260107100009918, acc: 0.949999988079071)
[2025-02-13 18:58:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:31][root][INFO] - Training Epoch: 1/2, step 1108/7134 completed (loss: 0.2974122166633606, acc: 0.9171597361564636)
[2025-02-13 18:58:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:32][root][INFO] - Training Epoch: 1/2, step 1109/7134 completed (loss: 0.3647274076938629, acc: 0.9248554706573486)
[2025-02-13 18:58:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:32][root][INFO] - Training Epoch: 1/2, step 1110/7134 completed (loss: 0.3837539851665497, acc: 0.9193548560142517)
[2025-02-13 18:58:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:32][root][INFO] - Training Epoch: 1/2, step 1111/7134 completed (loss: 0.35397306084632874, acc: 0.90625)
[2025-02-13 18:58:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:33][root][INFO] - Training Epoch: 1/2, step 1112/7134 completed (loss: 0.28455930948257446, acc: 0.9440559148788452)
[2025-02-13 18:58:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:33][root][INFO] - Training Epoch: 1/2, step 1113/7134 completed (loss: 0.2886557877063751, acc: 0.9245283007621765)
[2025-02-13 18:58:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:34][root][INFO] - Training Epoch: 1/2, step 1114/7134 completed (loss: 0.36601927876472473, acc: 0.920634925365448)
[2025-02-13 18:58:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:34][root][INFO] - Training Epoch: 1/2, step 1115/7134 completed (loss: 0.3055783808231354, acc: 0.9225806593894958)
[2025-02-13 18:58:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:34][root][INFO] - Training Epoch: 1/2, step 1116/7134 completed (loss: 0.3495759963989258, acc: 0.9105691313743591)
[2025-02-13 18:58:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:35][root][INFO] - Training Epoch: 1/2, step 1117/7134 completed (loss: 0.15108127892017365, acc: 0.9473684430122375)
[2025-02-13 18:58:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:35][root][INFO] - Training Epoch: 1/2, step 1118/7134 completed (loss: 0.17235776782035828, acc: 0.9389312863349915)
[2025-02-13 18:58:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:36][root][INFO] - Training Epoch: 1/2, step 1119/7134 completed (loss: 0.22576893866062164, acc: 0.9379310607910156)
[2025-02-13 18:58:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:36][root][INFO] - Training Epoch: 1/2, step 1120/7134 completed (loss: 0.38608184456825256, acc: 0.8778625726699829)
[2025-02-13 18:58:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:36][root][INFO] - Training Epoch: 1/2, step 1121/7134 completed (loss: 0.18348485231399536, acc: 0.9677419066429138)
[2025-02-13 18:58:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:37][root][INFO] - Training Epoch: 1/2, step 1122/7134 completed (loss: 0.15750260651111603, acc: 0.9615384340286255)
[2025-02-13 18:58:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:37][root][INFO] - Training Epoch: 1/2, step 1123/7134 completed (loss: 0.21703889966011047, acc: 0.9379310607910156)
[2025-02-13 18:58:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:38][root][INFO] - Training Epoch: 1/2, step 1124/7134 completed (loss: 0.22781001031398773, acc: 0.9324324131011963)
[2025-02-13 18:58:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:38][root][INFO] - Training Epoch: 1/2, step 1125/7134 completed (loss: 0.14234556257724762, acc: 0.9583333134651184)
[2025-02-13 18:58:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:38][root][INFO] - Training Epoch: 1/2, step 1126/7134 completed (loss: 0.27628856897354126, acc: 0.9101123809814453)
[2025-02-13 18:58:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:39][root][INFO] - Training Epoch: 1/2, step 1127/7134 completed (loss: 0.1440313309431076, acc: 0.9632353186607361)
[2025-02-13 18:58:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:39][root][INFO] - Training Epoch: 1/2, step 1128/7134 completed (loss: 0.22110049426555634, acc: 0.9312977194786072)
[2025-02-13 18:58:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:40][root][INFO] - Training Epoch: 1/2, step 1129/7134 completed (loss: 0.391623318195343, acc: 0.8666666746139526)
[2025-02-13 18:58:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:40][root][INFO] - Training Epoch: 1/2, step 1130/7134 completed (loss: 0.13798387348651886, acc: 0.9784172773361206)
[2025-02-13 18:58:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:41][root][INFO] - Training Epoch: 1/2, step 1131/7134 completed (loss: 0.5291112065315247, acc: 0.9090909361839294)
[2025-02-13 18:58:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:41][root][INFO] - Training Epoch: 1/2, step 1132/7134 completed (loss: 0.6529972553253174, acc: 0.8456375598907471)
[2025-02-13 18:58:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:41][root][INFO] - Training Epoch: 1/2, step 1133/7134 completed (loss: 0.525883138179779, acc: 0.884393036365509)
[2025-02-13 18:58:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:42][root][INFO] - Training Epoch: 1/2, step 1134/7134 completed (loss: 0.5320418477058411, acc: 0.901098906993866)
[2025-02-13 18:58:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:42][root][INFO] - Training Epoch: 1/2, step 1135/7134 completed (loss: 0.37235498428344727, acc: 0.899328887462616)
[2025-02-13 18:58:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:43][root][INFO] - Training Epoch: 1/2, step 1136/7134 completed (loss: 0.5724967122077942, acc: 0.8673469424247742)
[2025-02-13 18:58:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:43][root][INFO] - Training Epoch: 1/2, step 1137/7134 completed (loss: 0.4223916828632355, acc: 0.8962264060974121)
[2025-02-13 18:58:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:43][root][INFO] - Training Epoch: 1/2, step 1138/7134 completed (loss: 0.382735937833786, acc: 0.9200000166893005)
[2025-02-13 18:58:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:44][root][INFO] - Training Epoch: 1/2, step 1139/7134 completed (loss: 0.4843101501464844, acc: 0.8736263513565063)
[2025-02-13 18:58:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:44][root][INFO] - Training Epoch: 1/2, step 1140/7134 completed (loss: 0.13744232058525085, acc: 0.9578947424888611)
[2025-02-13 18:58:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:45][root][INFO] - Training Epoch: 1/2, step 1141/7134 completed (loss: 0.1371612250804901, acc: 0.9727272987365723)
[2025-02-13 18:58:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:45][root][INFO] - Training Epoch: 1/2, step 1142/7134 completed (loss: 0.249216690659523, acc: 0.9277777671813965)
[2025-02-13 18:58:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:45][root][INFO] - Training Epoch: 1/2, step 1143/7134 completed (loss: 0.4687713384628296, acc: 0.891566276550293)
[2025-02-13 18:58:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:46][root][INFO] - Training Epoch: 1/2, step 1144/7134 completed (loss: 0.45824936032295227, acc: 0.9166666865348816)
[2025-02-13 18:58:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:46][root][INFO] - Training Epoch: 1/2, step 1145/7134 completed (loss: 0.22483032941818237, acc: 0.9385474920272827)
[2025-02-13 18:58:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:47][root][INFO] - Training Epoch: 1/2, step 1146/7134 completed (loss: 0.0706956684589386, acc: 0.9924242496490479)
[2025-02-13 18:58:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:47][root][INFO] - Training Epoch: 1/2, step 1147/7134 completed (loss: 0.13134647905826569, acc: 0.9783783555030823)
[2025-02-13 18:58:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:47][root][INFO] - Training Epoch: 1/2, step 1148/7134 completed (loss: 0.17301045358181, acc: 0.9424460530281067)
[2025-02-13 18:58:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:48][root][INFO] - Training Epoch: 1/2, step 1149/7134 completed (loss: 0.40704095363616943, acc: 0.8958333134651184)
[2025-02-13 18:58:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:48][root][INFO] - Training Epoch: 1/2, step 1150/7134 completed (loss: 0.2417072206735611, acc: 0.9411764740943909)
[2025-02-13 18:58:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:49][root][INFO] - Training Epoch: 1/2, step 1151/7134 completed (loss: 0.3373531103134155, acc: 0.9327731132507324)
[2025-02-13 18:58:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:49][root][INFO] - Training Epoch: 1/2, step 1152/7134 completed (loss: 0.20642207562923431, acc: 0.9516128897666931)
[2025-02-13 18:58:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:49][root][INFO] - Training Epoch: 1/2, step 1153/7134 completed (loss: 0.3665109872817993, acc: 0.915032684803009)
[2025-02-13 18:58:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:50][root][INFO] - Training Epoch: 1/2, step 1154/7134 completed (loss: 0.2599005699157715, acc: 0.9609375)
[2025-02-13 18:58:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:50][root][INFO] - Training Epoch: 1/2, step 1155/7134 completed (loss: 0.6635031700134277, acc: 0.8376068472862244)
[2025-02-13 18:58:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:50][root][INFO] - Training Epoch: 1/2, step 1156/7134 completed (loss: 0.22999118268489838, acc: 0.9469696879386902)
[2025-02-13 18:58:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:51][root][INFO] - Training Epoch: 1/2, step 1157/7134 completed (loss: 0.4255470037460327, acc: 0.9241379499435425)
[2025-02-13 18:58:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:51][root][INFO] - Training Epoch: 1/2, step 1158/7134 completed (loss: 0.3420119881629944, acc: 0.939130425453186)
[2025-02-13 18:58:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:52][root][INFO] - Training Epoch: 1/2, step 1159/7134 completed (loss: 0.39096421003341675, acc: 0.9281437397003174)
[2025-02-13 18:58:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:52][root][INFO] - Training Epoch: 1/2, step 1160/7134 completed (loss: 0.22057749330997467, acc: 0.9444444179534912)
[2025-02-13 18:58:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:52][root][INFO] - Training Epoch: 1/2, step 1161/7134 completed (loss: 0.15960238873958588, acc: 0.9637681245803833)
[2025-02-13 18:58:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:53][root][INFO] - Training Epoch: 1/2, step 1162/7134 completed (loss: 0.3879198133945465, acc: 0.8987341523170471)
[2025-02-13 18:58:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:53][root][INFO] - Training Epoch: 1/2, step 1163/7134 completed (loss: 0.26986244320869446, acc: 0.9241379499435425)
[2025-02-13 18:58:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:54][root][INFO] - Training Epoch: 1/2, step 1164/7134 completed (loss: 0.25753161311149597, acc: 0.9281768202781677)
[2025-02-13 18:58:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:54][root][INFO] - Training Epoch: 1/2, step 1165/7134 completed (loss: 0.21107697486877441, acc: 0.9435897469520569)
[2025-02-13 18:58:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:54][root][INFO] - Training Epoch: 1/2, step 1166/7134 completed (loss: 0.172646626830101, acc: 0.9468085169792175)
[2025-02-13 18:58:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:55][root][INFO] - Training Epoch: 1/2, step 1167/7134 completed (loss: 0.15926136076450348, acc: 0.9679999947547913)
[2025-02-13 18:58:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:55][root][INFO] - Training Epoch: 1/2, step 1168/7134 completed (loss: 0.16022202372550964, acc: 0.9735449552536011)
[2025-02-13 18:58:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:56][root][INFO] - Training Epoch: 1/2, step 1169/7134 completed (loss: 0.27516990900039673, acc: 0.936170220375061)
[2025-02-13 18:58:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:56][root][INFO] - Training Epoch: 1/2, step 1170/7134 completed (loss: 0.3605784475803375, acc: 0.909604549407959)
[2025-02-13 18:58:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:56][root][INFO] - Training Epoch: 1/2, step 1171/7134 completed (loss: 0.20618098974227905, acc: 0.9466666579246521)
[2025-02-13 18:58:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:57][root][INFO] - Training Epoch: 1/2, step 1172/7134 completed (loss: 0.22491636872291565, acc: 0.9345238208770752)
[2025-02-13 18:58:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:57][root][INFO] - Training Epoch: 1/2, step 1173/7134 completed (loss: 0.19178858399391174, acc: 0.9576719403266907)
[2025-02-13 18:58:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:58][root][INFO] - Training Epoch: 1/2, step 1174/7134 completed (loss: 0.12472166866064072, acc: 0.9693251252174377)
[2025-02-13 18:58:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:58][root][INFO] - Training Epoch: 1/2, step 1175/7134 completed (loss: 0.32802078127861023, acc: 0.9411764740943909)
[2025-02-13 18:58:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:58][root][INFO] - Training Epoch: 1/2, step 1176/7134 completed (loss: 0.28827062249183655, acc: 0.9375)
[2025-02-13 18:58:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:59][root][INFO] - Training Epoch: 1/2, step 1177/7134 completed (loss: 0.09405196458101273, acc: 0.9692307710647583)
[2025-02-13 18:58:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:59][root][INFO] - Training Epoch: 1/2, step 1178/7134 completed (loss: 0.2082480490207672, acc: 0.942307710647583)
[2025-02-13 18:58:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:58:59][root][INFO] - Training Epoch: 1/2, step 1179/7134 completed (loss: 0.1454520970582962, acc: 0.975806474685669)
[2025-02-13 18:59:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:00][root][INFO] - Training Epoch: 1/2, step 1180/7134 completed (loss: 0.20234675705432892, acc: 0.9459459185600281)
[2025-02-13 18:59:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:00][root][INFO] - Training Epoch: 1/2, step 1181/7134 completed (loss: 0.17140637338161469, acc: 0.9560439586639404)
[2025-02-13 18:59:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:01][root][INFO] - Training Epoch: 1/2, step 1182/7134 completed (loss: 0.11598387360572815, acc: 0.9727891087532043)
[2025-02-13 18:59:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:01][root][INFO] - Training Epoch: 1/2, step 1183/7134 completed (loss: 0.07791172713041306, acc: 0.9884393215179443)
[2025-02-13 18:59:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:02][root][INFO] - Training Epoch: 1/2, step 1184/7134 completed (loss: 0.23240022361278534, acc: 0.9537572264671326)
[2025-02-13 18:59:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:02][root][INFO] - Training Epoch: 1/2, step 1185/7134 completed (loss: 0.15509335696697235, acc: 0.9757575988769531)
[2025-02-13 18:59:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:02][root][INFO] - Training Epoch: 1/2, step 1186/7134 completed (loss: 0.1152268797159195, acc: 0.9728260636329651)
[2025-02-13 18:59:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:03][root][INFO] - Training Epoch: 1/2, step 1187/7134 completed (loss: 0.1759764403104782, acc: 0.9583333134651184)
[2025-02-13 18:59:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:03][root][INFO] - Training Epoch: 1/2, step 1188/7134 completed (loss: 0.0634150579571724, acc: 0.9838709831237793)
[2025-02-13 18:59:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:04][root][INFO] - Training Epoch: 1/2, step 1189/7134 completed (loss: 0.1797305941581726, acc: 0.9704142212867737)
[2025-02-13 18:59:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:04][root][INFO] - Training Epoch: 1/2, step 1190/7134 completed (loss: 0.23368746042251587, acc: 0.9555555582046509)
[2025-02-13 18:59:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:04][root][INFO] - Training Epoch: 1/2, step 1191/7134 completed (loss: 0.33225515484809875, acc: 0.9387755393981934)
[2025-02-13 18:59:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:05][root][INFO] - Training Epoch: 1/2, step 1192/7134 completed (loss: 0.1716959923505783, acc: 0.9624060392379761)
[2025-02-13 18:59:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:05][root][INFO] - Training Epoch: 1/2, step 1193/7134 completed (loss: 0.3132438659667969, acc: 0.9448819160461426)
[2025-02-13 18:59:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:06][root][INFO] - Training Epoch: 1/2, step 1194/7134 completed (loss: 0.2172907143831253, acc: 0.9731543660163879)
[2025-02-13 18:59:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:06][root][INFO] - Training Epoch: 1/2, step 1195/7134 completed (loss: 0.4229515790939331, acc: 0.9076923131942749)
[2025-02-13 18:59:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:06][root][INFO] - Training Epoch: 1/2, step 1196/7134 completed (loss: 0.3919087052345276, acc: 0.9290780425071716)
[2025-02-13 18:59:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:07][root][INFO] - Training Epoch: 1/2, step 1197/7134 completed (loss: 0.23299303650856018, acc: 0.9452054500579834)
[2025-02-13 18:59:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:07][root][INFO] - Training Epoch: 1/2, step 1198/7134 completed (loss: 0.2443769872188568, acc: 0.9624999761581421)
[2025-02-13 18:59:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:08][root][INFO] - Training Epoch: 1/2, step 1199/7134 completed (loss: 0.26664963364601135, acc: 0.948051929473877)
[2025-02-13 18:59:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:08][root][INFO] - Training Epoch: 1/2, step 1200/7134 completed (loss: 0.16029421985149384, acc: 0.9532710313796997)
[2025-02-13 18:59:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:08][root][INFO] - Training Epoch: 1/2, step 1201/7134 completed (loss: 0.6037896275520325, acc: 0.9056603908538818)
[2025-02-13 18:59:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:09][root][INFO] - Training Epoch: 1/2, step 1202/7134 completed (loss: 0.6278753876686096, acc: 0.8695651888847351)
[2025-02-13 18:59:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:09][root][INFO] - Training Epoch: 1/2, step 1203/7134 completed (loss: 0.2964317202568054, acc: 0.9266666769981384)
[2025-02-13 18:59:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:10][root][INFO] - Training Epoch: 1/2, step 1204/7134 completed (loss: 0.490939199924469, acc: 0.8602941036224365)
[2025-02-13 18:59:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:10][root][INFO] - Training Epoch: 1/2, step 1205/7134 completed (loss: 0.5582902431488037, acc: 0.8620689511299133)
[2025-02-13 18:59:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:10][root][INFO] - Training Epoch: 1/2, step 1206/7134 completed (loss: 0.46461063623428345, acc: 0.8944723606109619)
[2025-02-13 18:59:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:11][root][INFO] - Training Epoch: 1/2, step 1207/7134 completed (loss: 0.28447288274765015, acc: 0.9301075339317322)
[2025-02-13 18:59:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:11][root][INFO] - Training Epoch: 1/2, step 1208/7134 completed (loss: 0.4302446246147156, acc: 0.9160305261611938)
[2025-02-13 18:59:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:12][root][INFO] - Training Epoch: 1/2, step 1209/7134 completed (loss: 0.6442100405693054, acc: 0.8442623019218445)
[2025-02-13 18:59:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:12][root][INFO] - Training Epoch: 1/2, step 1210/7134 completed (loss: 0.5900194644927979, acc: 0.8533333539962769)
[2025-02-13 18:59:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:12][root][INFO] - Training Epoch: 1/2, step 1211/7134 completed (loss: 0.8376146554946899, acc: 0.798561155796051)
[2025-02-13 18:59:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:13][root][INFO] - Training Epoch: 1/2, step 1212/7134 completed (loss: 0.614863395690918, acc: 0.8759124279022217)
[2025-02-13 18:59:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:13][root][INFO] - Training Epoch: 1/2, step 1213/7134 completed (loss: 0.5290156006813049, acc: 0.88165682554245)
[2025-02-13 18:59:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:13][root][INFO] - Training Epoch: 1/2, step 1214/7134 completed (loss: 0.5031459331512451, acc: 0.846666693687439)
[2025-02-13 18:59:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:14][root][INFO] - Training Epoch: 1/2, step 1215/7134 completed (loss: 0.54297935962677, acc: 0.8882352709770203)
[2025-02-13 18:59:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:14][root][INFO] - Training Epoch: 1/2, step 1216/7134 completed (loss: 0.3247964382171631, acc: 0.9205297827720642)
[2025-02-13 18:59:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:15][root][INFO] - Training Epoch: 1/2, step 1217/7134 completed (loss: 0.48457586765289307, acc: 0.9144737124443054)
[2025-02-13 18:59:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:15][root][INFO] - Training Epoch: 1/2, step 1218/7134 completed (loss: 0.6019364595413208, acc: 0.862500011920929)
[2025-02-13 18:59:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:15][root][INFO] - Training Epoch: 1/2, step 1219/7134 completed (loss: 0.7430352568626404, acc: 0.846666693687439)
[2025-02-13 18:59:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:16][root][INFO] - Training Epoch: 1/2, step 1220/7134 completed (loss: 0.2799641191959381, acc: 0.9484536051750183)
[2025-02-13 18:59:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:16][root][INFO] - Training Epoch: 1/2, step 1221/7134 completed (loss: 0.44200557470321655, acc: 0.9230769276618958)
[2025-02-13 18:59:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:17][root][INFO] - Training Epoch: 1/2, step 1222/7134 completed (loss: 0.6243433356285095, acc: 0.8883495330810547)
[2025-02-13 18:59:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:17][root][INFO] - Training Epoch: 1/2, step 1223/7134 completed (loss: 0.5448598265647888, acc: 0.8579545617103577)
[2025-02-13 18:59:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:17][root][INFO] - Training Epoch: 1/2, step 1224/7134 completed (loss: 0.47955286502838135, acc: 0.9150943160057068)
[2025-02-13 18:59:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:18][root][INFO] - Training Epoch: 1/2, step 1225/7134 completed (loss: 0.46372905373573303, acc: 0.9086757898330688)
[2025-02-13 18:59:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:18][root][INFO] - Training Epoch: 1/2, step 1226/7134 completed (loss: 0.5417366027832031, acc: 0.89682537317276)
[2025-02-13 18:59:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:19][root][INFO] - Training Epoch: 1/2, step 1227/7134 completed (loss: 0.7924171686172485, acc: 0.8041236996650696)
[2025-02-13 18:59:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:19][root][INFO] - Training Epoch: 1/2, step 1228/7134 completed (loss: 0.4380433261394501, acc: 0.9011628031730652)
[2025-02-13 18:59:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:19][root][INFO] - Training Epoch: 1/2, step 1229/7134 completed (loss: 0.42106345295906067, acc: 0.9009901285171509)
[2025-02-13 18:59:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:20][root][INFO] - Training Epoch: 1/2, step 1230/7134 completed (loss: 0.6201857328414917, acc: 0.8726415038108826)
[2025-02-13 18:59:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:20][root][INFO] - Training Epoch: 1/2, step 1231/7134 completed (loss: 0.5201966762542725, acc: 0.875)
[2025-02-13 18:59:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:21][root][INFO] - Training Epoch: 1/2, step 1232/7134 completed (loss: 0.46034640073776245, acc: 0.8923766613006592)
[2025-02-13 18:59:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:21][root][INFO] - Training Epoch: 1/2, step 1233/7134 completed (loss: 0.6629045009613037, acc: 0.8597285151481628)
[2025-02-13 18:59:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:21][root][INFO] - Training Epoch: 1/2, step 1234/7134 completed (loss: 0.34853044152259827, acc: 0.9166666865348816)
[2025-02-13 18:59:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:22][root][INFO] - Training Epoch: 1/2, step 1235/7134 completed (loss: 0.3909955620765686, acc: 0.8928571343421936)
[2025-02-13 18:59:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:22][root][INFO] - Training Epoch: 1/2, step 1236/7134 completed (loss: 0.6393802762031555, acc: 0.8647058606147766)
[2025-02-13 18:59:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:23][root][INFO] - Training Epoch: 1/2, step 1237/7134 completed (loss: 0.649854838848114, acc: 0.8741722106933594)
[2025-02-13 18:59:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:23][root][INFO] - Training Epoch: 1/2, step 1238/7134 completed (loss: 1.3950873613357544, acc: 0.7255814075469971)
[2025-02-13 18:59:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:23][root][INFO] - Training Epoch: 1/2, step 1239/7134 completed (loss: 0.49451228976249695, acc: 0.8658536672592163)
[2025-02-13 18:59:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:24][root][INFO] - Training Epoch: 1/2, step 1240/7134 completed (loss: 0.5681750178337097, acc: 0.8313953280448914)
[2025-02-13 18:59:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:24][root][INFO] - Training Epoch: 1/2, step 1241/7134 completed (loss: 0.34669890999794006, acc: 0.9180327653884888)
[2025-02-13 18:59:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:25][root][INFO] - Training Epoch: 1/2, step 1242/7134 completed (loss: 0.5664266347885132, acc: 0.8691099286079407)
[2025-02-13 18:59:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:25][root][INFO] - Training Epoch: 1/2, step 1243/7134 completed (loss: 0.5284162759780884, acc: 0.8684210777282715)
[2025-02-13 18:59:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:25][root][INFO] - Training Epoch: 1/2, step 1244/7134 completed (loss: 0.5110858082771301, acc: 0.8661971688270569)
[2025-02-13 18:59:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:26][root][INFO] - Training Epoch: 1/2, step 1245/7134 completed (loss: 0.43884897232055664, acc: 0.9040403962135315)
[2025-02-13 18:59:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:26][root][INFO] - Training Epoch: 1/2, step 1246/7134 completed (loss: 0.6042972803115845, acc: 0.8842105269432068)
[2025-02-13 18:59:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:27][root][INFO] - Training Epoch: 1/2, step 1247/7134 completed (loss: 0.31019923090934753, acc: 0.9156626462936401)
[2025-02-13 18:59:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:27][root][INFO] - Training Epoch: 1/2, step 1248/7134 completed (loss: 0.5730568170547485, acc: 0.8899999856948853)
[2025-02-13 18:59:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:27][root][INFO] - Training Epoch: 1/2, step 1249/7134 completed (loss: 0.3597014546394348, acc: 0.8965517282485962)
[2025-02-13 18:59:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:28][root][INFO] - Training Epoch: 1/2, step 1250/7134 completed (loss: 0.2221529185771942, acc: 0.95333331823349)
[2025-02-13 18:59:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:28][root][INFO] - Training Epoch: 1/2, step 1251/7134 completed (loss: 0.4046942889690399, acc: 0.903030276298523)
[2025-02-13 18:59:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:28][root][INFO] - Training Epoch: 1/2, step 1252/7134 completed (loss: 0.3223516345024109, acc: 0.9448819160461426)
[2025-02-13 18:59:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:29][root][INFO] - Training Epoch: 1/2, step 1253/7134 completed (loss: 0.5298977494239807, acc: 0.8717948794364929)
[2025-02-13 18:59:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:29][root][INFO] - Training Epoch: 1/2, step 1254/7134 completed (loss: 0.4257850646972656, acc: 0.8933333158493042)
[2025-02-13 18:59:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:30][root][INFO] - Training Epoch: 1/2, step 1255/7134 completed (loss: 0.42335546016693115, acc: 0.9172413945198059)
[2025-02-13 18:59:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:30][root][INFO] - Training Epoch: 1/2, step 1256/7134 completed (loss: 0.39424505829811096, acc: 0.9007633328437805)
[2025-02-13 18:59:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:30][root][INFO] - Training Epoch: 1/2, step 1257/7134 completed (loss: 0.24179427325725555, acc: 0.9312977194786072)
[2025-02-13 18:59:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:31][root][INFO] - Training Epoch: 1/2, step 1258/7134 completed (loss: 0.5558899641036987, acc: 0.886904776096344)
[2025-02-13 18:59:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:31][root][INFO] - Training Epoch: 1/2, step 1259/7134 completed (loss: 0.5295304656028748, acc: 0.9038461446762085)
[2025-02-13 18:59:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:32][root][INFO] - Training Epoch: 1/2, step 1260/7134 completed (loss: 0.2765379548072815, acc: 0.926174521446228)
[2025-02-13 18:59:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:32][root][INFO] - Training Epoch: 1/2, step 1261/7134 completed (loss: 0.3240707814693451, acc: 0.9324324131011963)
[2025-02-13 18:59:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:32][root][INFO] - Training Epoch: 1/2, step 1262/7134 completed (loss: 0.12030313163995743, acc: 0.9767441749572754)
[2025-02-13 18:59:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:33][root][INFO] - Training Epoch: 1/2, step 1263/7134 completed (loss: 0.20406971871852875, acc: 0.9612902998924255)
[2025-02-13 18:59:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:33][root][INFO] - Training Epoch: 1/2, step 1264/7134 completed (loss: 0.3312011957168579, acc: 0.9259259104728699)
[2025-02-13 18:59:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:34][root][INFO] - Training Epoch: 1/2, step 1265/7134 completed (loss: 0.26104259490966797, acc: 0.9314285516738892)
[2025-02-13 18:59:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:34][root][INFO] - Training Epoch: 1/2, step 1266/7134 completed (loss: 0.2933286428451538, acc: 0.9236640930175781)
[2025-02-13 18:59:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:34][root][INFO] - Training Epoch: 1/2, step 1267/7134 completed (loss: 0.27873241901397705, acc: 0.9285714030265808)
[2025-02-13 18:59:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:35][root][INFO] - Training Epoch: 1/2, step 1268/7134 completed (loss: 0.4223027527332306, acc: 0.9111111164093018)
[2025-02-13 18:59:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:35][root][INFO] - Training Epoch: 1/2, step 1269/7134 completed (loss: 0.7249073386192322, acc: 0.8560606241226196)
[2025-02-13 18:59:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:35][root][INFO] - Training Epoch: 1/2, step 1270/7134 completed (loss: 0.2950461804866791, acc: 0.9041095972061157)
[2025-02-13 18:59:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:36][root][INFO] - Training Epoch: 1/2, step 1271/7134 completed (loss: 0.3763841986656189, acc: 0.9083333611488342)
[2025-02-13 18:59:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:36][root][INFO] - Training Epoch: 1/2, step 1272/7134 completed (loss: 0.20102941989898682, acc: 0.9496855139732361)
[2025-02-13 18:59:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:37][root][INFO] - Training Epoch: 1/2, step 1273/7134 completed (loss: 0.34916484355926514, acc: 0.9037036895751953)
[2025-02-13 18:59:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:37][root][INFO] - Training Epoch: 1/2, step 1274/7134 completed (loss: 0.1705818474292755, acc: 0.942148745059967)
[2025-02-13 18:59:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:37][root][INFO] - Training Epoch: 1/2, step 1275/7134 completed (loss: 0.26680460572242737, acc: 0.95652174949646)
[2025-02-13 18:59:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:38][root][INFO] - Training Epoch: 1/2, step 1276/7134 completed (loss: 0.20789626240730286, acc: 0.9696969985961914)
[2025-02-13 18:59:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:38][root][INFO] - Training Epoch: 1/2, step 1277/7134 completed (loss: 0.11572839319705963, acc: 0.98591548204422)
[2025-02-13 18:59:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:39][root][INFO] - Training Epoch: 1/2, step 1278/7134 completed (loss: 0.5342134237289429, acc: 0.8500000238418579)
[2025-02-13 18:59:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:39][root][INFO] - Training Epoch: 1/2, step 1279/7134 completed (loss: 0.38965705037117004, acc: 0.9239130616188049)
[2025-02-13 18:59:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:39][root][INFO] - Training Epoch: 1/2, step 1280/7134 completed (loss: 0.2191457748413086, acc: 0.9358288645744324)
[2025-02-13 18:59:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:40][root][INFO] - Training Epoch: 1/2, step 1281/7134 completed (loss: 0.3567682206630707, acc: 0.9015151262283325)
[2025-02-13 18:59:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:40][root][INFO] - Training Epoch: 1/2, step 1282/7134 completed (loss: 0.6507483720779419, acc: 0.8469945192337036)
[2025-02-13 18:59:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:40][root][INFO] - Training Epoch: 1/2, step 1283/7134 completed (loss: 0.37066876888275146, acc: 0.8914728760719299)
[2025-02-13 18:59:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:41][root][INFO] - Training Epoch: 1/2, step 1284/7134 completed (loss: 0.3401947617530823, acc: 0.910179615020752)
[2025-02-13 18:59:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:41][root][INFO] - Training Epoch: 1/2, step 1285/7134 completed (loss: 0.2647189497947693, acc: 0.9437500238418579)
[2025-02-13 18:59:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:42][root][INFO] - Training Epoch: 1/2, step 1286/7134 completed (loss: 0.3322451412677765, acc: 0.9207317233085632)
[2025-02-13 18:59:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:42][root][INFO] - Training Epoch: 1/2, step 1287/7134 completed (loss: 0.3306150734424591, acc: 0.8740741014480591)
[2025-02-13 18:59:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:42][root][INFO] - Training Epoch: 1/2, step 1288/7134 completed (loss: 0.33621248602867126, acc: 0.9119170904159546)
[2025-02-13 18:59:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:43][root][INFO] - Training Epoch: 1/2, step 1289/7134 completed (loss: 0.42918482422828674, acc: 0.8896551728248596)
[2025-02-13 18:59:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:43][root][INFO] - Training Epoch: 1/2, step 1290/7134 completed (loss: 0.21488609910011292, acc: 0.9520958065986633)
[2025-02-13 18:59:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:44][root][INFO] - Training Epoch: 1/2, step 1291/7134 completed (loss: 0.2838096022605896, acc: 0.9117646813392639)
[2025-02-13 18:59:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:44][root][INFO] - Training Epoch: 1/2, step 1292/7134 completed (loss: 0.29705381393432617, acc: 0.9175823926925659)
[2025-02-13 18:59:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:45][root][INFO] - Training Epoch: 1/2, step 1293/7134 completed (loss: 0.33286744356155396, acc: 0.9128205180168152)
[2025-02-13 18:59:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:45][root][INFO] - Training Epoch: 1/2, step 1294/7134 completed (loss: 0.26677992939949036, acc: 0.9398906826972961)
[2025-02-13 18:59:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:45][root][INFO] - Training Epoch: 1/2, step 1295/7134 completed (loss: 0.47423747181892395, acc: 0.9179104566574097)
[2025-02-13 18:59:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:46][root][INFO] - Training Epoch: 1/2, step 1296/7134 completed (loss: 0.3082866966724396, acc: 0.9230769276618958)
[2025-02-13 18:59:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:46][root][INFO] - Training Epoch: 1/2, step 1297/7134 completed (loss: 0.38241878151893616, acc: 0.8928571343421936)
[2025-02-13 18:59:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:46][root][INFO] - Training Epoch: 1/2, step 1298/7134 completed (loss: 0.2954643964767456, acc: 0.93388432264328)
[2025-02-13 18:59:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:47][root][INFO] - Training Epoch: 1/2, step 1299/7134 completed (loss: 0.24572093784809113, acc: 0.9397590160369873)
[2025-02-13 18:59:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:47][root][INFO] - Training Epoch: 1/2, step 1300/7134 completed (loss: 0.24948541820049286, acc: 0.9257425665855408)
[2025-02-13 18:59:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:48][root][INFO] - Training Epoch: 1/2, step 1301/7134 completed (loss: 0.17158012092113495, acc: 0.9736841917037964)
[2025-02-13 18:59:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:48][root][INFO] - Training Epoch: 1/2, step 1302/7134 completed (loss: 0.30006837844848633, acc: 0.9504950642585754)
[2025-02-13 18:59:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:48][root][INFO] - Training Epoch: 1/2, step 1303/7134 completed (loss: 0.25048625469207764, acc: 0.9399999976158142)
[2025-02-13 18:59:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:49][root][INFO] - Training Epoch: 1/2, step 1304/7134 completed (loss: 0.3230896294116974, acc: 0.9152542352676392)
[2025-02-13 18:59:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:49][root][INFO] - Training Epoch: 1/2, step 1305/7134 completed (loss: 0.3722570240497589, acc: 0.9278350472450256)
[2025-02-13 18:59:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:50][root][INFO] - Training Epoch: 1/2, step 1306/7134 completed (loss: 0.27355772256851196, acc: 0.9473684430122375)
[2025-02-13 18:59:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:50][root][INFO] - Training Epoch: 1/2, step 1307/7134 completed (loss: 0.22902047634124756, acc: 0.9496402740478516)
[2025-02-13 18:59:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:50][root][INFO] - Training Epoch: 1/2, step 1308/7134 completed (loss: 0.3103097975254059, acc: 0.9161290526390076)
[2025-02-13 18:59:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:51][root][INFO] - Training Epoch: 1/2, step 1309/7134 completed (loss: 0.34793850779533386, acc: 0.8888888955116272)
[2025-02-13 18:59:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:51][root][INFO] - Training Epoch: 1/2, step 1310/7134 completed (loss: 0.5121176242828369, acc: 0.8618784546852112)
[2025-02-13 18:59:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:52][root][INFO] - Training Epoch: 1/2, step 1311/7134 completed (loss: 0.32987162470817566, acc: 0.942307710647583)
[2025-02-13 18:59:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:52][root][INFO] - Training Epoch: 1/2, step 1312/7134 completed (loss: 0.45767438411712646, acc: 0.8944099545478821)
[2025-02-13 18:59:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:52][root][INFO] - Training Epoch: 1/2, step 1313/7134 completed (loss: 0.6224400401115417, acc: 0.8271604776382446)
[2025-02-13 18:59:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:53][root][INFO] - Training Epoch: 1/2, step 1314/7134 completed (loss: 0.22363252937793732, acc: 0.9492753744125366)
[2025-02-13 18:59:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:53][root][INFO] - Training Epoch: 1/2, step 1315/7134 completed (loss: 0.5347028374671936, acc: 0.8540540337562561)
[2025-02-13 18:59:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:54][root][INFO] - Training Epoch: 1/2, step 1316/7134 completed (loss: 0.278777539730072, acc: 0.9512194991111755)
[2025-02-13 18:59:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:54][root][INFO] - Training Epoch: 1/2, step 1317/7134 completed (loss: 0.5637633204460144, acc: 0.89552241563797)
[2025-02-13 18:59:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:54][root][INFO] - Training Epoch: 1/2, step 1318/7134 completed (loss: 0.39274969696998596, acc: 0.8791208863258362)
[2025-02-13 18:59:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:55][root][INFO] - Training Epoch: 1/2, step 1319/7134 completed (loss: 0.3204818665981293, acc: 0.9363636374473572)
[2025-02-13 18:59:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:55][root][INFO] - Training Epoch: 1/2, step 1320/7134 completed (loss: 0.46747633814811707, acc: 0.8675496578216553)
[2025-02-13 18:59:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:56][root][INFO] - Training Epoch: 1/2, step 1321/7134 completed (loss: 0.2935487926006317, acc: 0.9015544056892395)
[2025-02-13 18:59:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:56][root][INFO] - Training Epoch: 1/2, step 1322/7134 completed (loss: 0.49186062812805176, acc: 0.8911564350128174)
[2025-02-13 18:59:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:56][root][INFO] - Training Epoch: 1/2, step 1323/7134 completed (loss: 0.41238924860954285, acc: 0.9295774698257446)
[2025-02-13 18:59:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:57][root][INFO] - Training Epoch: 1/2, step 1324/7134 completed (loss: 0.34200790524482727, acc: 0.9281045794487)
[2025-02-13 18:59:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:57][root][INFO] - Training Epoch: 1/2, step 1325/7134 completed (loss: 0.3339022994041443, acc: 0.893203854560852)
[2025-02-13 18:59:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:57][root][INFO] - Training Epoch: 1/2, step 1326/7134 completed (loss: 0.49822717905044556, acc: 0.913241982460022)
[2025-02-13 18:59:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:58][root][INFO] - Training Epoch: 1/2, step 1327/7134 completed (loss: 0.27879536151885986, acc: 0.9289940595626831)
[2025-02-13 18:59:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:58][root][INFO] - Training Epoch: 1/2, step 1328/7134 completed (loss: 0.28651973605155945, acc: 0.9306930899620056)
[2025-02-13 18:59:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:59][root][INFO] - Training Epoch: 1/2, step 1329/7134 completed (loss: 0.3373945653438568, acc: 0.9100000262260437)
[2025-02-13 18:59:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:59][root][INFO] - Training Epoch: 1/2, step 1330/7134 completed (loss: 0.30635422468185425, acc: 0.9208633303642273)
[2025-02-13 18:59:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 18:59:59][root][INFO] - Training Epoch: 1/2, step 1331/7134 completed (loss: 0.13357345759868622, acc: 0.9577465057373047)
[2025-02-13 19:00:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:00][root][INFO] - Training Epoch: 1/2, step 1332/7134 completed (loss: 0.25769779086112976, acc: 0.9385964870452881)
[2025-02-13 19:00:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:00][root][INFO] - Training Epoch: 1/2, step 1333/7134 completed (loss: 0.3248867392539978, acc: 0.9342105388641357)
[2025-02-13 19:00:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:01][root][INFO] - Training Epoch: 1/2, step 1334/7134 completed (loss: 0.5446986556053162, acc: 0.8888888955116272)
[2025-02-13 19:00:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:01][root][INFO] - Training Epoch: 1/2, step 1335/7134 completed (loss: 0.4047340154647827, acc: 0.9057591557502747)
[2025-02-13 19:00:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:01][root][INFO] - Training Epoch: 1/2, step 1336/7134 completed (loss: 0.41963958740234375, acc: 0.9144384860992432)
[2025-02-13 19:00:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:02][root][INFO] - Training Epoch: 1/2, step 1337/7134 completed (loss: 0.3126200735569, acc: 0.9290322661399841)
[2025-02-13 19:00:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:02][root][INFO] - Training Epoch: 1/2, step 1338/7134 completed (loss: 0.5091176629066467, acc: 0.859375)
[2025-02-13 19:00:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:03][root][INFO] - Training Epoch: 1/2, step 1339/7134 completed (loss: 0.4482711851596832, acc: 0.8775510191917419)
[2025-02-13 19:00:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:03][root][INFO] - Training Epoch: 1/2, step 1340/7134 completed (loss: 0.5057264566421509, acc: 0.9090909361839294)
[2025-02-13 19:00:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:03][root][INFO] - Training Epoch: 1/2, step 1341/7134 completed (loss: 0.14661328494548798, acc: 0.9780219793319702)
[2025-02-13 19:00:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:04][root][INFO] - Training Epoch: 1/2, step 1342/7134 completed (loss: 0.16764730215072632, acc: 0.9668874144554138)
[2025-02-13 19:00:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:04][root][INFO] - Training Epoch: 1/2, step 1343/7134 completed (loss: 0.4489811658859253, acc: 0.8970588445663452)
[2025-02-13 19:00:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:04][root][INFO] - Training Epoch: 1/2, step 1344/7134 completed (loss: 0.2818854749202728, acc: 0.9266055226325989)
[2025-02-13 19:00:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:05][root][INFO] - Training Epoch: 1/2, step 1345/7134 completed (loss: 0.40419304370880127, acc: 0.8817204236984253)
[2025-02-13 19:00:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:05][root][INFO] - Training Epoch: 1/2, step 1346/7134 completed (loss: 0.2549591362476349, acc: 0.9453551769256592)
[2025-02-13 19:00:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:06][root][INFO] - Training Epoch: 1/2, step 1347/7134 completed (loss: 0.24565741419792175, acc: 0.9305555820465088)
[2025-02-13 19:00:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:06][root][INFO] - Training Epoch: 1/2, step 1348/7134 completed (loss: 0.1517075151205063, acc: 0.9723756909370422)
[2025-02-13 19:00:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:06][root][INFO] - Training Epoch: 1/2, step 1349/7134 completed (loss: 0.09761740267276764, acc: 0.9716312289237976)
[2025-02-13 19:00:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:07][root][INFO] - Training Epoch: 1/2, step 1350/7134 completed (loss: 0.09276842325925827, acc: 0.9940119981765747)
[2025-02-13 19:00:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:07][root][INFO] - Training Epoch: 1/2, step 1351/7134 completed (loss: 0.11775671690702438, acc: 0.9750000238418579)
[2025-02-13 19:00:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:08][root][INFO] - Training Epoch: 1/2, step 1352/7134 completed (loss: 0.1200299933552742, acc: 0.9878048896789551)
[2025-02-13 19:00:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:08][root][INFO] - Training Epoch: 1/2, step 1353/7134 completed (loss: 0.2027013599872589, acc: 0.9666666388511658)
[2025-02-13 19:00:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:08][root][INFO] - Training Epoch: 1/2, step 1354/7134 completed (loss: 0.08961965888738632, acc: 0.9733333587646484)
[2025-02-13 19:00:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:09][root][INFO] - Training Epoch: 1/2, step 1355/7134 completed (loss: 0.07616894692182541, acc: 0.9870967864990234)
[2025-02-13 19:00:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:09][root][INFO] - Training Epoch: 1/2, step 1356/7134 completed (loss: 0.2148236334323883, acc: 0.930232584476471)
[2025-02-13 19:00:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:10][root][INFO] - Training Epoch: 1/2, step 1357/7134 completed (loss: 0.10093577206134796, acc: 0.9735449552536011)
[2025-02-13 19:00:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:10][root][INFO] - Training Epoch: 1/2, step 1358/7134 completed (loss: 0.11121240258216858, acc: 0.9659090638160706)
[2025-02-13 19:00:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:10][root][INFO] - Training Epoch: 1/2, step 1359/7134 completed (loss: 0.06610332429409027, acc: 0.9834710955619812)
[2025-02-13 19:00:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:11][root][INFO] - Training Epoch: 1/2, step 1360/7134 completed (loss: 0.10797140747308731, acc: 0.976190447807312)
[2025-02-13 19:00:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:11][root][INFO] - Training Epoch: 1/2, step 1361/7134 completed (loss: 0.17545293271541595, acc: 0.9585798978805542)
[2025-02-13 19:00:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:12][root][INFO] - Training Epoch: 1/2, step 1362/7134 completed (loss: 0.24311238527297974, acc: 0.9453551769256592)
[2025-02-13 19:00:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:12][root][INFO] - Training Epoch: 1/2, step 1363/7134 completed (loss: 0.19827520847320557, acc: 0.9712643623352051)
[2025-02-13 19:00:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:12][root][INFO] - Training Epoch: 1/2, step 1364/7134 completed (loss: 0.06879226118326187, acc: 0.9823529124259949)
[2025-02-13 19:00:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:13][root][INFO] - Training Epoch: 1/2, step 1365/7134 completed (loss: 0.05529361963272095, acc: 0.9777777791023254)
[2025-02-13 19:00:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:13][root][INFO] - Training Epoch: 1/2, step 1366/7134 completed (loss: 0.07683423906564713, acc: 0.9837398529052734)
[2025-02-13 19:00:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:14][root][INFO] - Training Epoch: 1/2, step 1367/7134 completed (loss: 0.12082091718912125, acc: 0.970588207244873)
[2025-02-13 19:00:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:14][root][INFO] - Training Epoch: 1/2, step 1368/7134 completed (loss: 0.3869118094444275, acc: 0.9195402264595032)
[2025-02-13 19:00:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:14][root][INFO] - Training Epoch: 1/2, step 1369/7134 completed (loss: 0.1869923621416092, acc: 0.956204354763031)
[2025-02-13 19:00:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:15][root][INFO] - Training Epoch: 1/2, step 1370/7134 completed (loss: 0.2373637557029724, acc: 0.9438775777816772)
[2025-02-13 19:00:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:15][root][INFO] - Training Epoch: 1/2, step 1371/7134 completed (loss: 0.1693025380373001, acc: 0.9567901492118835)
[2025-02-13 19:00:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:16][root][INFO] - Training Epoch: 1/2, step 1372/7134 completed (loss: 0.3532637655735016, acc: 0.9166666865348816)
[2025-02-13 19:00:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:16][root][INFO] - Training Epoch: 1/2, step 1373/7134 completed (loss: 0.25190070271492004, acc: 0.9147287011146545)
[2025-02-13 19:00:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:16][root][INFO] - Training Epoch: 1/2, step 1374/7134 completed (loss: 0.2725520133972168, acc: 0.9166666865348816)
[2025-02-13 19:00:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:17][root][INFO] - Training Epoch: 1/2, step 1375/7134 completed (loss: 0.2691432237625122, acc: 0.9313725233078003)
[2025-02-13 19:00:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:17][root][INFO] - Training Epoch: 1/2, step 1376/7134 completed (loss: 0.5577913522720337, acc: 0.8928571343421936)
[2025-02-13 19:00:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:18][root][INFO] - Training Epoch: 1/2, step 1377/7134 completed (loss: 0.6464816927909851, acc: 0.8627451062202454)
[2025-02-13 19:00:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:18][root][INFO] - Training Epoch: 1/2, step 1378/7134 completed (loss: 0.5321964621543884, acc: 0.9038461446762085)
[2025-02-13 19:00:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:18][root][INFO] - Training Epoch: 1/2, step 1379/7134 completed (loss: 0.3864647150039673, acc: 0.8712871074676514)
[2025-02-13 19:00:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:19][root][INFO] - Training Epoch: 1/2, step 1380/7134 completed (loss: 0.3818452060222626, acc: 0.8703703880310059)
[2025-02-13 19:00:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:19][root][INFO] - Training Epoch: 1/2, step 1381/7134 completed (loss: 0.1723233312368393, acc: 0.9492753744125366)
[2025-02-13 19:00:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:20][root][INFO] - Training Epoch: 1/2, step 1382/7134 completed (loss: 0.3821941912174225, acc: 0.8999999761581421)
[2025-02-13 19:00:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:20][root][INFO] - Training Epoch: 1/2, step 1383/7134 completed (loss: 0.5670863389968872, acc: 0.8823529481887817)
[2025-02-13 19:00:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:20][root][INFO] - Training Epoch: 1/2, step 1384/7134 completed (loss: 0.5116304755210876, acc: 0.8510638475418091)
[2025-02-13 19:00:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:21][root][INFO] - Training Epoch: 1/2, step 1385/7134 completed (loss: 0.595363974571228, acc: 0.8560000061988831)
[2025-02-13 19:00:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:21][root][INFO] - Training Epoch: 1/2, step 1386/7134 completed (loss: 0.470147043466568, acc: 0.9014084339141846)
[2025-02-13 19:00:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:22][root][INFO] - Training Epoch: 1/2, step 1387/7134 completed (loss: 0.28399237990379333, acc: 0.9300699234008789)
[2025-02-13 19:00:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:22][root][INFO] - Training Epoch: 1/2, step 1388/7134 completed (loss: 0.3964832127094269, acc: 0.8960000276565552)
[2025-02-13 19:00:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:23][root][INFO] - Training Epoch: 1/2, step 1389/7134 completed (loss: 0.5479594469070435, acc: 0.858208954334259)
[2025-02-13 19:00:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:23][root][INFO] - Training Epoch: 1/2, step 1390/7134 completed (loss: 0.34869444370269775, acc: 0.9124087691307068)
[2025-02-13 19:00:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:23][root][INFO] - Training Epoch: 1/2, step 1391/7134 completed (loss: 0.625028669834137, acc: 0.845588207244873)
[2025-02-13 19:00:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:24][root][INFO] - Training Epoch: 1/2, step 1392/7134 completed (loss: 0.42215901613235474, acc: 0.8943089246749878)
[2025-02-13 19:00:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:24][root][INFO] - Training Epoch: 1/2, step 1393/7134 completed (loss: 0.4311881959438324, acc: 0.90625)
[2025-02-13 19:00:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:24][root][INFO] - Training Epoch: 1/2, step 1394/7134 completed (loss: 0.38831230998039246, acc: 0.8867924809455872)
[2025-02-13 19:00:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:25][root][INFO] - Training Epoch: 1/2, step 1395/7134 completed (loss: 0.385128378868103, acc: 0.9453125)
[2025-02-13 19:00:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:25][root][INFO] - Training Epoch: 1/2, step 1396/7134 completed (loss: 0.23357145488262177, acc: 0.9520000219345093)
[2025-02-13 19:00:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:26][root][INFO] - Training Epoch: 1/2, step 1397/7134 completed (loss: 0.7750123739242554, acc: 0.8310810923576355)
[2025-02-13 19:00:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:26][root][INFO] - Training Epoch: 1/2, step 1398/7134 completed (loss: 0.23878218233585358, acc: 0.9306930899620056)
[2025-02-13 19:00:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:26][root][INFO] - Training Epoch: 1/2, step 1399/7134 completed (loss: 0.3664349615573883, acc: 0.9064748287200928)
[2025-02-13 19:00:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:27][root][INFO] - Training Epoch: 1/2, step 1400/7134 completed (loss: 0.2507536709308624, acc: 0.9420289993286133)
[2025-02-13 19:00:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:27][root][INFO] - Training Epoch: 1/2, step 1401/7134 completed (loss: 0.22680708765983582, acc: 0.9333333373069763)
[2025-02-13 19:00:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:28][root][INFO] - Training Epoch: 1/2, step 1402/7134 completed (loss: 0.22106443345546722, acc: 0.9402984976768494)
[2025-02-13 19:00:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:28][root][INFO] - Training Epoch: 1/2, step 1403/7134 completed (loss: 0.21083322167396545, acc: 0.9467455744743347)
[2025-02-13 19:00:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:28][root][INFO] - Training Epoch: 1/2, step 1404/7134 completed (loss: 0.16962864995002747, acc: 0.9583333134651184)
[2025-02-13 19:00:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:29][root][INFO] - Training Epoch: 1/2, step 1405/7134 completed (loss: 0.21777915954589844, acc: 0.9583333134651184)
[2025-02-13 19:00:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:29][root][INFO] - Training Epoch: 1/2, step 1406/7134 completed (loss: 0.14880788326263428, acc: 0.9751552939414978)
[2025-02-13 19:00:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:30][root][INFO] - Training Epoch: 1/2, step 1407/7134 completed (loss: 0.1249435693025589, acc: 0.9825581312179565)
[2025-02-13 19:00:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:30][root][INFO] - Training Epoch: 1/2, step 1408/7134 completed (loss: 0.10769161581993103, acc: 0.9931034445762634)
[2025-02-13 19:00:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:30][root][INFO] - Training Epoch: 1/2, step 1409/7134 completed (loss: 0.07238242030143738, acc: 0.9811320900917053)
[2025-02-13 19:00:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:31][root][INFO] - Training Epoch: 1/2, step 1410/7134 completed (loss: 0.22504191100597382, acc: 0.9281768202781677)
[2025-02-13 19:00:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:31][root][INFO] - Training Epoch: 1/2, step 1411/7134 completed (loss: 0.1793498545885086, acc: 0.9599999785423279)
[2025-02-13 19:00:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:31][root][INFO] - Training Epoch: 1/2, step 1412/7134 completed (loss: 0.17917713522911072, acc: 0.9631578922271729)
[2025-02-13 19:00:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:32][root][INFO] - Training Epoch: 1/2, step 1413/7134 completed (loss: 0.1830589920282364, acc: 0.940119743347168)
[2025-02-13 19:00:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:32][root][INFO] - Training Epoch: 1/2, step 1414/7134 completed (loss: 0.11714401096105576, acc: 0.9735099077224731)
[2025-02-13 19:00:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:33][root][INFO] - Training Epoch: 1/2, step 1415/7134 completed (loss: 0.26134270429611206, acc: 0.9489051103591919)
[2025-02-13 19:00:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:33][root][INFO] - Training Epoch: 1/2, step 1416/7134 completed (loss: 0.06611234694719315, acc: 0.9830508232116699)
[2025-02-13 19:00:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:33][root][INFO] - Training Epoch: 1/2, step 1417/7134 completed (loss: 0.10248055309057236, acc: 0.9791666865348816)
[2025-02-13 19:00:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:34][root][INFO] - Training Epoch: 1/2, step 1418/7134 completed (loss: 0.11406584084033966, acc: 0.9689922332763672)
[2025-02-13 19:00:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:34][root][INFO] - Training Epoch: 1/2, step 1419/7134 completed (loss: 0.1782357096672058, acc: 0.9772727489471436)
[2025-02-13 19:00:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:34][root][INFO] - Training Epoch: 1/2, step 1420/7134 completed (loss: 0.2545722424983978, acc: 0.9263157844543457)
[2025-02-13 19:00:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:35][root][INFO] - Training Epoch: 1/2, step 1421/7134 completed (loss: 0.09958291053771973, acc: 0.9865771532058716)
[2025-02-13 19:00:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:35][root][INFO] - Training Epoch: 1/2, step 1422/7134 completed (loss: 0.4380820393562317, acc: 0.8975903391838074)
[2025-02-13 19:00:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:36][root][INFO] - Training Epoch: 1/2, step 1423/7134 completed (loss: 0.7184702754020691, acc: 0.8549618124961853)
[2025-02-13 19:00:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:36][root][INFO] - Training Epoch: 1/2, step 1424/7134 completed (loss: 0.5241036415100098, acc: 0.8617886304855347)
[2025-02-13 19:00:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:36][root][INFO] - Training Epoch: 1/2, step 1425/7134 completed (loss: 0.34690728783607483, acc: 0.8901734352111816)
[2025-02-13 19:00:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:37][root][INFO] - Training Epoch: 1/2, step 1426/7134 completed (loss: 0.36644747853279114, acc: 0.9278350472450256)
[2025-02-13 19:00:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:37][root][INFO] - Training Epoch: 1/2, step 1427/7134 completed (loss: 0.39782318472862244, acc: 0.887005627155304)
[2025-02-13 19:00:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:38][root][INFO] - Training Epoch: 1/2, step 1428/7134 completed (loss: 1.5512239933013916, acc: 0.6532257795333862)
[2025-02-13 19:00:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:38][root][INFO] - Training Epoch: 1/2, step 1429/7134 completed (loss: 0.31800365447998047, acc: 0.9074074029922485)
[2025-02-13 19:00:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:39][root][INFO] - Training Epoch: 1/2, step 1430/7134 completed (loss: 0.3201811611652374, acc: 0.9280575513839722)
[2025-02-13 19:00:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:39][root][INFO] - Training Epoch: 1/2, step 1431/7134 completed (loss: 0.2081807404756546, acc: 0.9516128897666931)
[2025-02-13 19:00:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:39][root][INFO] - Training Epoch: 1/2, step 1432/7134 completed (loss: 0.3126904368400574, acc: 0.9246231317520142)
[2025-02-13 19:00:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:40][root][INFO] - Training Epoch: 1/2, step 1433/7134 completed (loss: 0.32609617710113525, acc: 0.9313725233078003)
[2025-02-13 19:00:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:40][root][INFO] - Training Epoch: 1/2, step 1434/7134 completed (loss: 0.32569557428359985, acc: 0.89552241563797)
[2025-02-13 19:00:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:41][root][INFO] - Training Epoch: 1/2, step 1435/7134 completed (loss: 0.31630799174308777, acc: 0.9354838728904724)
[2025-02-13 19:00:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:41][root][INFO] - Training Epoch: 1/2, step 1436/7134 completed (loss: 0.3667505979537964, acc: 0.9490740895271301)
[2025-02-13 19:00:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:41][root][INFO] - Training Epoch: 1/2, step 1437/7134 completed (loss: 0.2761019766330719, acc: 0.9354838728904724)
[2025-02-13 19:00:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:42][root][INFO] - Training Epoch: 1/2, step 1438/7134 completed (loss: 0.2735987603664398, acc: 0.9515151381492615)
[2025-02-13 19:00:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:42][root][INFO] - Training Epoch: 1/2, step 1439/7134 completed (loss: 0.1946062445640564, acc: 0.9407407641410828)
[2025-02-13 19:00:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:43][root][INFO] - Training Epoch: 1/2, step 1440/7134 completed (loss: 0.27226123213768005, acc: 0.9378882050514221)
[2025-02-13 19:00:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:43][root][INFO] - Training Epoch: 1/2, step 1441/7134 completed (loss: 0.17829656600952148, acc: 0.9712643623352051)
[2025-02-13 19:00:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:43][root][INFO] - Training Epoch: 1/2, step 1442/7134 completed (loss: 0.16318444907665253, acc: 0.9821428656578064)
[2025-02-13 19:00:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:44][root][INFO] - Training Epoch: 1/2, step 1443/7134 completed (loss: 0.28998857736587524, acc: 0.9506173133850098)
[2025-02-13 19:00:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:44][root][INFO] - Training Epoch: 1/2, step 1444/7134 completed (loss: 0.3061773478984833, acc: 0.9444444179534912)
[2025-02-13 19:00:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:44][root][INFO] - Training Epoch: 1/2, step 1445/7134 completed (loss: 0.2958717942237854, acc: 0.9299362897872925)
[2025-02-13 19:00:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:45][root][INFO] - Training Epoch: 1/2, step 1446/7134 completed (loss: 0.18278726935386658, acc: 0.9545454382896423)
[2025-02-13 19:00:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:45][root][INFO] - Training Epoch: 1/2, step 1447/7134 completed (loss: 0.24537016451358795, acc: 0.9247311949729919)
[2025-02-13 19:00:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:46][root][INFO] - Training Epoch: 1/2, step 1448/7134 completed (loss: 0.8750365376472473, acc: 0.8208954930305481)
[2025-02-13 19:00:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:46][root][INFO] - Training Epoch: 1/2, step 1449/7134 completed (loss: 0.5879468321800232, acc: 0.8579235076904297)
[2025-02-13 19:00:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:46][root][INFO] - Training Epoch: 1/2, step 1450/7134 completed (loss: 0.32546740770339966, acc: 0.9197860956192017)
[2025-02-13 19:00:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:47][root][INFO] - Training Epoch: 1/2, step 1451/7134 completed (loss: 0.2800149917602539, acc: 0.9214659929275513)
[2025-02-13 19:00:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:47][root][INFO] - Training Epoch: 1/2, step 1452/7134 completed (loss: 0.30205363035202026, acc: 0.9236111044883728)
[2025-02-13 19:00:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:48][root][INFO] - Training Epoch: 1/2, step 1453/7134 completed (loss: 0.7233343720436096, acc: 0.854651153087616)
[2025-02-13 19:00:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:48][root][INFO] - Training Epoch: 1/2, step 1454/7134 completed (loss: 0.4408755600452423, acc: 0.875)
[2025-02-13 19:00:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:48][root][INFO] - Training Epoch: 1/2, step 1455/7134 completed (loss: 0.6859932541847229, acc: 0.8413792848587036)
[2025-02-13 19:00:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:49][root][INFO] - Training Epoch: 1/2, step 1456/7134 completed (loss: 0.39251580834388733, acc: 0.9008264541625977)
[2025-02-13 19:00:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:49][root][INFO] - Training Epoch: 1/2, step 1457/7134 completed (loss: 0.21428030729293823, acc: 0.9378882050514221)
[2025-02-13 19:00:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:50][root][INFO] - Training Epoch: 1/2, step 1458/7134 completed (loss: 0.2199094146490097, acc: 0.9289940595626831)
[2025-02-13 19:00:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:50][root][INFO] - Training Epoch: 1/2, step 1459/7134 completed (loss: 0.19606439769268036, acc: 0.9407407641410828)
[2025-02-13 19:00:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:50][root][INFO] - Training Epoch: 1/2, step 1460/7134 completed (loss: 0.5835028886795044, acc: 0.8934911489486694)
[2025-02-13 19:00:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:51][root][INFO] - Training Epoch: 1/2, step 1461/7134 completed (loss: 0.49531862139701843, acc: 0.8741722106933594)
[2025-02-13 19:00:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:51][root][INFO] - Training Epoch: 1/2, step 1462/7134 completed (loss: 0.35425612330436707, acc: 0.9029850959777832)
[2025-02-13 19:00:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:52][root][INFO] - Training Epoch: 1/2, step 1463/7134 completed (loss: 0.5089494585990906, acc: 0.8692810535430908)
[2025-02-13 19:00:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:52][root][INFO] - Training Epoch: 1/2, step 1464/7134 completed (loss: 0.6064788103103638, acc: 0.8674699068069458)
[2025-02-13 19:00:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:52][root][INFO] - Training Epoch: 1/2, step 1465/7134 completed (loss: 0.31860437989234924, acc: 0.9230769276618958)
[2025-02-13 19:00:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:53][root][INFO] - Training Epoch: 1/2, step 1466/7134 completed (loss: 0.1348307579755783, acc: 0.9720670580863953)
[2025-02-13 19:00:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:53][root][INFO] - Training Epoch: 1/2, step 1467/7134 completed (loss: 0.16420073807239532, acc: 0.9696969985961914)
[2025-02-13 19:00:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:54][root][INFO] - Training Epoch: 1/2, step 1468/7134 completed (loss: 0.19579118490219116, acc: 0.949438214302063)
[2025-02-13 19:00:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:54][root][INFO] - Training Epoch: 1/2, step 1469/7134 completed (loss: 0.11217164993286133, acc: 0.9772727489471436)
[2025-02-13 19:00:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:54][root][INFO] - Training Epoch: 1/2, step 1470/7134 completed (loss: 0.3185732364654541, acc: 0.9171974658966064)
[2025-02-13 19:00:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:55][root][INFO] - Training Epoch: 1/2, step 1471/7134 completed (loss: 0.490798681974411, acc: 0.8999999761581421)
[2025-02-13 19:00:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:55][root][INFO] - Training Epoch: 1/2, step 1472/7134 completed (loss: 0.18469902873039246, acc: 0.9649122953414917)
[2025-02-13 19:00:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:55][root][INFO] - Training Epoch: 1/2, step 1473/7134 completed (loss: 0.16825826466083527, acc: 0.939393937587738)
[2025-02-13 19:00:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:56][root][INFO] - Training Epoch: 1/2, step 1474/7134 completed (loss: 0.216372549533844, acc: 0.9379310607910156)
[2025-02-13 19:00:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:56][root][INFO] - Training Epoch: 1/2, step 1475/7134 completed (loss: 0.34521108865737915, acc: 0.9212121367454529)
[2025-02-13 19:00:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:57][root][INFO] - Training Epoch: 1/2, step 1476/7134 completed (loss: 0.328461229801178, acc: 0.9266666769981384)
[2025-02-13 19:00:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:57][root][INFO] - Training Epoch: 1/2, step 1477/7134 completed (loss: 0.22919900715351105, acc: 0.9241379499435425)
[2025-02-13 19:00:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:57][root][INFO] - Training Epoch: 1/2, step 1478/7134 completed (loss: 0.2298152893781662, acc: 0.9736841917037964)
[2025-02-13 19:00:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:58][root][INFO] - Training Epoch: 1/2, step 1479/7134 completed (loss: 0.12928813695907593, acc: 0.9712643623352051)
[2025-02-13 19:00:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:58][root][INFO] - Training Epoch: 1/2, step 1480/7134 completed (loss: 0.15669235587120056, acc: 0.9613259434700012)
[2025-02-13 19:00:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:58][root][INFO] - Training Epoch: 1/2, step 1481/7134 completed (loss: 0.22695420682430267, acc: 0.949999988079071)
[2025-02-13 19:00:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:59][root][INFO] - Training Epoch: 1/2, step 1482/7134 completed (loss: 0.45912471413612366, acc: 0.8875739574432373)
[2025-02-13 19:00:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:00:59][root][INFO] - Training Epoch: 1/2, step 1483/7134 completed (loss: 0.14413292706012726, acc: 0.9847328066825867)
[2025-02-13 19:00:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:00][root][INFO] - Training Epoch: 1/2, step 1484/7134 completed (loss: 0.3476736545562744, acc: 0.9397590160369873)
[2025-02-13 19:01:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:00][root][INFO] - Training Epoch: 1/2, step 1485/7134 completed (loss: 0.18141096830368042, acc: 0.9590643048286438)
[2025-02-13 19:01:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:00][root][INFO] - Training Epoch: 1/2, step 1486/7134 completed (loss: 0.21413099765777588, acc: 0.9685534834861755)
[2025-02-13 19:01:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:01][root][INFO] - Training Epoch: 1/2, step 1487/7134 completed (loss: 0.31376519799232483, acc: 0.929729700088501)
[2025-02-13 19:01:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:01][root][INFO] - Training Epoch: 1/2, step 1488/7134 completed (loss: 0.2652701139450073, acc: 0.9441860318183899)
[2025-02-13 19:01:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:02][root][INFO] - Training Epoch: 1/2, step 1489/7134 completed (loss: 0.325751394033432, acc: 0.8928571343421936)
[2025-02-13 19:01:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:02][root][INFO] - Training Epoch: 1/2, step 1490/7134 completed (loss: 0.32660728693008423, acc: 0.9292035102844238)
[2025-02-13 19:01:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:02][root][INFO] - Training Epoch: 1/2, step 1491/7134 completed (loss: 0.21940602362155914, acc: 0.9571428298950195)
[2025-02-13 19:01:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:03][root][INFO] - Training Epoch: 1/2, step 1492/7134 completed (loss: 0.29414331912994385, acc: 0.928909957408905)
[2025-02-13 19:01:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:03][root][INFO] - Training Epoch: 1/2, step 1493/7134 completed (loss: 0.2735104560852051, acc: 0.9392523169517517)
[2025-02-13 19:01:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:04][root][INFO] - Training Epoch: 1/2, step 1494/7134 completed (loss: 0.23829929530620575, acc: 0.9430052042007446)
[2025-02-13 19:01:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:04][root][INFO] - Training Epoch: 1/2, step 1495/7134 completed (loss: 0.19230614602565765, acc: 0.9317073225975037)
[2025-02-13 19:01:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:04][root][INFO] - Training Epoch: 1/2, step 1496/7134 completed (loss: 0.2698683440685272, acc: 0.956250011920929)
[2025-02-13 19:01:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:05][root][INFO] - Training Epoch: 1/2, step 1497/7134 completed (loss: 0.14498843252658844, acc: 0.9722222089767456)
[2025-02-13 19:01:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:05][root][INFO] - Training Epoch: 1/2, step 1498/7134 completed (loss: 0.2645273506641388, acc: 0.9513513445854187)
[2025-02-13 19:01:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:06][root][INFO] - Training Epoch: 1/2, step 1499/7134 completed (loss: 0.213068887591362, acc: 0.9488372206687927)
[2025-02-13 19:01:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:06][root][INFO] - Training Epoch: 1/2, step 1500/7134 completed (loss: 0.3927970230579376, acc: 0.9109588861465454)
[2025-02-13 19:01:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:06][root][INFO] - Training Epoch: 1/2, step 1501/7134 completed (loss: 0.4438351094722748, acc: 0.8875739574432373)
[2025-02-13 19:01:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:07][root][INFO] - Training Epoch: 1/2, step 1502/7134 completed (loss: 0.3445296585559845, acc: 0.918181836605072)
[2025-02-13 19:01:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:07][root][INFO] - Training Epoch: 1/2, step 1503/7134 completed (loss: 0.41550612449645996, acc: 0.8971428275108337)
[2025-02-13 19:01:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:08][root][INFO] - Training Epoch: 1/2, step 1504/7134 completed (loss: 0.20696908235549927, acc: 0.9682539701461792)
[2025-02-13 19:01:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:08][root][INFO] - Training Epoch: 1/2, step 1505/7134 completed (loss: 0.3267417252063751, acc: 0.905063271522522)
[2025-02-13 19:01:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:08][root][INFO] - Training Epoch: 1/2, step 1506/7134 completed (loss: 0.5059194564819336, acc: 0.8679245114326477)
[2025-02-13 19:01:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:09][root][INFO] - Training Epoch: 1/2, step 1507/7134 completed (loss: 0.29850754141807556, acc: 0.921875)
[2025-02-13 19:01:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:09][root][INFO] - Training Epoch: 1/2, step 1508/7134 completed (loss: 0.2677071690559387, acc: 0.9205607771873474)
[2025-02-13 19:01:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:10][root][INFO] - Training Epoch: 1/2, step 1509/7134 completed (loss: 0.21553243696689606, acc: 0.9277777671813965)
[2025-02-13 19:01:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:10][root][INFO] - Training Epoch: 1/2, step 1510/7134 completed (loss: 0.4723234474658966, acc: 0.8903225660324097)
[2025-02-13 19:01:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:10][root][INFO] - Training Epoch: 1/2, step 1511/7134 completed (loss: 0.33276888728141785, acc: 0.9217877388000488)
[2025-02-13 19:01:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:11][root][INFO] - Training Epoch: 1/2, step 1512/7134 completed (loss: 0.2281670868396759, acc: 0.9466666579246521)
[2025-02-13 19:01:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:11][root][INFO] - Training Epoch: 1/2, step 1513/7134 completed (loss: 0.09145329892635345, acc: 0.9944751262664795)
[2025-02-13 19:01:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:11][root][INFO] - Training Epoch: 1/2, step 1514/7134 completed (loss: 0.17469163239002228, acc: 0.9637305736541748)
[2025-02-13 19:01:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:12][root][INFO] - Training Epoch: 1/2, step 1515/7134 completed (loss: 0.2061758190393448, acc: 0.9729729890823364)
[2025-02-13 19:01:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:12][root][INFO] - Training Epoch: 1/2, step 1516/7134 completed (loss: 0.3284435570240021, acc: 0.925000011920929)
[2025-02-13 19:01:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:13][root][INFO] - Training Epoch: 1/2, step 1517/7134 completed (loss: 0.45564714074134827, acc: 0.9017857313156128)
[2025-02-13 19:01:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:13][root][INFO] - Training Epoch: 1/2, step 1518/7134 completed (loss: 0.2775234878063202, acc: 0.938144326210022)
[2025-02-13 19:01:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:13][root][INFO] - Training Epoch: 1/2, step 1519/7134 completed (loss: 0.23966142535209656, acc: 0.9708737730979919)
[2025-02-13 19:01:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:14][root][INFO] - Training Epoch: 1/2, step 1520/7134 completed (loss: 0.19311468303203583, acc: 0.956204354763031)
[2025-02-13 19:01:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:14][root][INFO] - Training Epoch: 1/2, step 1521/7134 completed (loss: 1.1763375997543335, acc: 0.779411792755127)
[2025-02-13 19:01:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:15][root][INFO] - Training Epoch: 1/2, step 1522/7134 completed (loss: 0.5744487047195435, acc: 0.8492063283920288)
[2025-02-13 19:01:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:15][root][INFO] - Training Epoch: 1/2, step 1523/7134 completed (loss: 1.1216075420379639, acc: 0.7960526347160339)
[2025-02-13 19:01:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:15][root][INFO] - Training Epoch: 1/2, step 1524/7134 completed (loss: 0.3338889479637146, acc: 0.931506872177124)
[2025-02-13 19:01:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:16][root][INFO] - Training Epoch: 1/2, step 1525/7134 completed (loss: 0.6929052472114563, acc: 0.8600000143051147)
[2025-02-13 19:01:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:16][root][INFO] - Training Epoch: 1/2, step 1526/7134 completed (loss: 0.8947452902793884, acc: 0.8299319744110107)
[2025-02-13 19:01:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:17][root][INFO] - Training Epoch: 1/2, step 1527/7134 completed (loss: 1.3768222332000732, acc: 0.7964601516723633)
[2025-02-13 19:01:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:17][root][INFO] - Training Epoch: 1/2, step 1528/7134 completed (loss: 0.608822762966156, acc: 0.8695651888847351)
[2025-02-13 19:01:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:17][root][INFO] - Training Epoch: 1/2, step 1529/7134 completed (loss: 0.40743109583854675, acc: 0.9095744490623474)
[2025-02-13 19:01:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:18][root][INFO] - Training Epoch: 1/2, step 1530/7134 completed (loss: 0.6598753333091736, acc: 0.8392857313156128)
[2025-02-13 19:01:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:18][root][INFO] - Training Epoch: 1/2, step 1531/7134 completed (loss: 1.1314736604690552, acc: 0.7424242496490479)
[2025-02-13 19:01:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:19][root][INFO] - Training Epoch: 1/2, step 1532/7134 completed (loss: 0.32927462458610535, acc: 0.9333333373069763)
[2025-02-13 19:01:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:19][root][INFO] - Training Epoch: 1/2, step 1533/7134 completed (loss: 0.4441570043563843, acc: 0.891566276550293)
[2025-02-13 19:01:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:19][root][INFO] - Training Epoch: 1/2, step 1534/7134 completed (loss: 0.5377120971679688, acc: 0.8848921060562134)
[2025-02-13 19:01:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:20][root][INFO] - Training Epoch: 1/2, step 1535/7134 completed (loss: 0.47927215695381165, acc: 0.8742856979370117)
[2025-02-13 19:01:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:20][root][INFO] - Training Epoch: 1/2, step 1536/7134 completed (loss: 0.336312472820282, acc: 0.9197530746459961)
[2025-02-13 19:01:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:21][root][INFO] - Training Epoch: 1/2, step 1537/7134 completed (loss: 0.429571270942688, acc: 0.9005848169326782)
[2025-02-13 19:01:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:21][root][INFO] - Training Epoch: 1/2, step 1538/7134 completed (loss: 0.5697624683380127, acc: 0.898809552192688)
[2025-02-13 19:01:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:21][root][INFO] - Training Epoch: 1/2, step 1539/7134 completed (loss: 0.3984373211860657, acc: 0.9090909361839294)
[2025-02-13 19:01:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:22][root][INFO] - Training Epoch: 1/2, step 1540/7134 completed (loss: 0.4226000905036926, acc: 0.905063271522522)
[2025-02-13 19:01:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:22][root][INFO] - Training Epoch: 1/2, step 1541/7134 completed (loss: 0.3184880018234253, acc: 0.9210526347160339)
[2025-02-13 19:01:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:23][root][INFO] - Training Epoch: 1/2, step 1542/7134 completed (loss: 0.21931825578212738, acc: 0.9269663095474243)
[2025-02-13 19:01:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:23][root][INFO] - Training Epoch: 1/2, step 1543/7134 completed (loss: 0.23634368181228638, acc: 0.9238095283508301)
[2025-02-13 19:01:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:23][root][INFO] - Training Epoch: 1/2, step 1544/7134 completed (loss: 0.2380058616399765, acc: 0.925000011920929)
[2025-02-13 19:01:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:24][root][INFO] - Training Epoch: 1/2, step 1545/7134 completed (loss: 0.2754863500595093, acc: 0.9166666865348816)
[2025-02-13 19:01:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:24][root][INFO] - Training Epoch: 1/2, step 1546/7134 completed (loss: 0.6220291256904602, acc: 0.8705882430076599)
[2025-02-13 19:01:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:25][root][INFO] - Training Epoch: 1/2, step 1547/7134 completed (loss: 0.4122004210948944, acc: 0.9012345671653748)
[2025-02-13 19:01:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:25][root][INFO] - Training Epoch: 1/2, step 1548/7134 completed (loss: 0.34449654817581177, acc: 0.9473684430122375)
[2025-02-13 19:01:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:25][root][INFO] - Training Epoch: 1/2, step 1549/7134 completed (loss: 0.16929291188716888, acc: 0.9657142758369446)
[2025-02-13 19:01:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:26][root][INFO] - Training Epoch: 1/2, step 1550/7134 completed (loss: 0.2210586816072464, acc: 0.9470587968826294)
[2025-02-13 19:01:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:26][root][INFO] - Training Epoch: 1/2, step 1551/7134 completed (loss: 0.19320020079612732, acc: 0.9470198750495911)
[2025-02-13 19:01:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:27][root][INFO] - Training Epoch: 1/2, step 1552/7134 completed (loss: 0.1749451458454132, acc: 0.9591836929321289)
[2025-02-13 19:01:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:27][root][INFO] - Training Epoch: 1/2, step 1553/7134 completed (loss: 0.1923615038394928, acc: 0.9675324559211731)
[2025-02-13 19:01:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:27][root][INFO] - Training Epoch: 1/2, step 1554/7134 completed (loss: 0.28564760088920593, acc: 0.9364162087440491)
[2025-02-13 19:01:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:28][root][INFO] - Training Epoch: 1/2, step 1555/7134 completed (loss: 0.2328691929578781, acc: 0.956250011920929)
[2025-02-13 19:01:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:28][root][INFO] - Training Epoch: 1/2, step 1556/7134 completed (loss: 0.33954283595085144, acc: 0.9152542352676392)
[2025-02-13 19:01:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:29][root][INFO] - Training Epoch: 1/2, step 1557/7134 completed (loss: 0.3091220557689667, acc: 0.9222221970558167)
[2025-02-13 19:01:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:29][root][INFO] - Training Epoch: 1/2, step 1558/7134 completed (loss: 0.35290810465812683, acc: 0.9041095972061157)
[2025-02-13 19:01:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:29][root][INFO] - Training Epoch: 1/2, step 1559/7134 completed (loss: 0.3767203092575073, acc: 0.9142857193946838)
[2025-02-13 19:01:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:30][root][INFO] - Training Epoch: 1/2, step 1560/7134 completed (loss: 0.1830340176820755, acc: 0.9644669890403748)
[2025-02-13 19:01:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:30][root][INFO] - Training Epoch: 1/2, step 1561/7134 completed (loss: 0.5223522782325745, acc: 0.8965517282485962)
[2025-02-13 19:01:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:30][root][INFO] - Training Epoch: 1/2, step 1562/7134 completed (loss: 0.3439067602157593, acc: 0.9171597361564636)
[2025-02-13 19:01:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:31][root][INFO] - Training Epoch: 1/2, step 1563/7134 completed (loss: 0.3860926032066345, acc: 0.9019607901573181)
[2025-02-13 19:01:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:31][root][INFO] - Training Epoch: 1/2, step 1564/7134 completed (loss: 0.5055980086326599, acc: 0.8564356565475464)
[2025-02-13 19:01:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:32][root][INFO] - Training Epoch: 1/2, step 1565/7134 completed (loss: 0.4116528034210205, acc: 0.8993710875511169)
[2025-02-13 19:01:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:32][root][INFO] - Training Epoch: 1/2, step 1566/7134 completed (loss: 0.3861342668533325, acc: 0.90055251121521)
[2025-02-13 19:01:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:32][root][INFO] - Training Epoch: 1/2, step 1567/7134 completed (loss: 0.5466720461845398, acc: 0.8829268217086792)
[2025-02-13 19:01:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:33][root][INFO] - Training Epoch: 1/2, step 1568/7134 completed (loss: 0.5885505676269531, acc: 0.8557692170143127)
[2025-02-13 19:01:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:33][root][INFO] - Training Epoch: 1/2, step 1569/7134 completed (loss: 0.32294103503227234, acc: 0.9090909361839294)
[2025-02-13 19:01:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:34][root][INFO] - Training Epoch: 1/2, step 1570/7134 completed (loss: 0.6596308350563049, acc: 0.8685445785522461)
[2025-02-13 19:01:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:34][root][INFO] - Training Epoch: 1/2, step 1571/7134 completed (loss: 0.38558125495910645, acc: 0.940092146396637)
[2025-02-13 19:01:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:35][root][INFO] - Training Epoch: 1/2, step 1572/7134 completed (loss: 0.3929259479045868, acc: 0.9436619877815247)
[2025-02-13 19:01:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:35][root][INFO] - Training Epoch: 1/2, step 1573/7134 completed (loss: 0.18507595360279083, acc: 0.9520958065986633)
[2025-02-13 19:01:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:35][root][INFO] - Training Epoch: 1/2, step 1574/7134 completed (loss: 0.4313662052154541, acc: 0.8923766613006592)
[2025-02-13 19:01:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:36][root][INFO] - Training Epoch: 1/2, step 1575/7134 completed (loss: 0.2765154540538788, acc: 0.9409090876579285)
[2025-02-13 19:01:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:36][root][INFO] - Training Epoch: 1/2, step 1576/7134 completed (loss: 0.24329523742198944, acc: 0.9483568072319031)
[2025-02-13 19:01:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:37][root][INFO] - Training Epoch: 1/2, step 1577/7134 completed (loss: 0.18574358522891998, acc: 0.9516907930374146)
[2025-02-13 19:01:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:37][root][INFO] - Training Epoch: 1/2, step 1578/7134 completed (loss: 0.28584492206573486, acc: 0.9204545617103577)
[2025-02-13 19:01:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:37][root][INFO] - Training Epoch: 1/2, step 1579/7134 completed (loss: 0.25772908329963684, acc: 0.932692289352417)
[2025-02-13 19:01:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:38][root][INFO] - Training Epoch: 1/2, step 1580/7134 completed (loss: 0.5972501039505005, acc: 0.8454106450080872)
[2025-02-13 19:01:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:38][root][INFO] - Training Epoch: 1/2, step 1581/7134 completed (loss: 0.7851883769035339, acc: 0.8392857313156128)
[2025-02-13 19:01:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:38][root][INFO] - Training Epoch: 1/2, step 1582/7134 completed (loss: 0.46089494228363037, acc: 0.8799999952316284)
[2025-02-13 19:01:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:39][root][INFO] - Training Epoch: 1/2, step 1583/7134 completed (loss: 0.3622027635574341, acc: 0.8914285898208618)
[2025-02-13 19:01:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:39][root][INFO] - Training Epoch: 1/2, step 1584/7134 completed (loss: 0.37549206614494324, acc: 0.9054054021835327)
[2025-02-13 19:01:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:40][root][INFO] - Training Epoch: 1/2, step 1585/7134 completed (loss: 0.2580474317073822, acc: 0.9271844625473022)
[2025-02-13 19:01:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:40][root][INFO] - Training Epoch: 1/2, step 1586/7134 completed (loss: 0.349509060382843, acc: 0.9130434989929199)
[2025-02-13 19:01:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:40][root][INFO] - Training Epoch: 1/2, step 1587/7134 completed (loss: 0.3884119987487793, acc: 0.9235668778419495)
[2025-02-13 19:01:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:41][root][INFO] - Training Epoch: 1/2, step 1588/7134 completed (loss: 0.13593830168247223, acc: 0.9745222926139832)
[2025-02-13 19:01:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:41][root][INFO] - Training Epoch: 1/2, step 1589/7134 completed (loss: 0.17535296082496643, acc: 0.9695122241973877)
[2025-02-13 19:01:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:42][root][INFO] - Training Epoch: 1/2, step 1590/7134 completed (loss: 0.2837521433830261, acc: 0.9214285612106323)
[2025-02-13 19:01:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:42][root][INFO] - Training Epoch: 1/2, step 1591/7134 completed (loss: 0.1730964481830597, acc: 0.9689922332763672)
[2025-02-13 19:01:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:42][root][INFO] - Training Epoch: 1/2, step 1592/7134 completed (loss: 0.22464175522327423, acc: 0.9675324559211731)
[2025-02-13 19:01:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:43][root][INFO] - Training Epoch: 1/2, step 1593/7134 completed (loss: 0.16329243779182434, acc: 0.9625668525695801)
[2025-02-13 19:01:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:43][root][INFO] - Training Epoch: 1/2, step 1594/7134 completed (loss: 0.13813276588916779, acc: 0.96875)
[2025-02-13 19:01:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:44][root][INFO] - Training Epoch: 1/2, step 1595/7134 completed (loss: 0.16469064354896545, acc: 0.9663865566253662)
[2025-02-13 19:01:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:44][root][INFO] - Training Epoch: 1/2, step 1596/7134 completed (loss: 0.33891671895980835, acc: 0.9305555820465088)
[2025-02-13 19:01:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:45][root][INFO] - Training Epoch: 1/2, step 1597/7134 completed (loss: 0.13954180479049683, acc: 0.9602649211883545)
[2025-02-13 19:01:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:45][root][INFO] - Training Epoch: 1/2, step 1598/7134 completed (loss: 0.32943370938301086, acc: 0.9166666865348816)
[2025-02-13 19:01:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:45][root][INFO] - Training Epoch: 1/2, step 1599/7134 completed (loss: 0.45824846625328064, acc: 0.8689655065536499)
[2025-02-13 19:01:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:46][root][INFO] - Training Epoch: 1/2, step 1600/7134 completed (loss: 0.29220569133758545, acc: 0.9387755393981934)
[2025-02-13 19:01:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:46][root][INFO] - Training Epoch: 1/2, step 1601/7134 completed (loss: 0.1799554079771042, acc: 0.9636363387107849)
[2025-02-13 19:01:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:46][root][INFO] - Training Epoch: 1/2, step 1602/7134 completed (loss: 0.2574026584625244, acc: 0.9358974099159241)
[2025-02-13 19:01:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:47][root][INFO] - Training Epoch: 1/2, step 1603/7134 completed (loss: 0.3737718164920807, acc: 0.9397590160369873)
[2025-02-13 19:01:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:47][root][INFO] - Training Epoch: 1/2, step 1604/7134 completed (loss: 0.1923656463623047, acc: 0.9515151381492615)
[2025-02-13 19:01:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:48][root][INFO] - Training Epoch: 1/2, step 1605/7134 completed (loss: 0.20350044965744019, acc: 0.9529411792755127)
[2025-02-13 19:01:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:48][root][INFO] - Training Epoch: 1/2, step 1606/7134 completed (loss: 0.16508890688419342, acc: 0.9523809552192688)
[2025-02-13 19:01:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:48][root][INFO] - Training Epoch: 1/2, step 1607/7134 completed (loss: 0.07125106453895569, acc: 0.9909090995788574)
[2025-02-13 19:01:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:49][root][INFO] - Training Epoch: 1/2, step 1608/7134 completed (loss: 0.14605176448822021, acc: 0.9685039520263672)
[2025-02-13 19:01:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:49][root][INFO] - Training Epoch: 1/2, step 1609/7134 completed (loss: 0.33248043060302734, acc: 0.9027777910232544)
[2025-02-13 19:01:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:50][root][INFO] - Training Epoch: 1/2, step 1610/7134 completed (loss: 0.22848311066627502, acc: 0.9389312863349915)
[2025-02-13 19:01:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:50][root][INFO] - Training Epoch: 1/2, step 1611/7134 completed (loss: 0.2462201714515686, acc: 0.9320987462997437)
[2025-02-13 19:01:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:50][root][INFO] - Training Epoch: 1/2, step 1612/7134 completed (loss: 0.22052527964115143, acc: 0.9518072009086609)
[2025-02-13 19:01:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:51][root][INFO] - Training Epoch: 1/2, step 1613/7134 completed (loss: 0.2266397476196289, acc: 0.9503105878829956)
[2025-02-13 19:01:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:51][root][INFO] - Training Epoch: 1/2, step 1614/7134 completed (loss: 0.2570243775844574, acc: 0.9407894611358643)
[2025-02-13 19:01:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:52][root][INFO] - Training Epoch: 1/2, step 1615/7134 completed (loss: 0.23531851172447205, acc: 0.9337349534034729)
[2025-02-13 19:01:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:52][root][INFO] - Training Epoch: 1/2, step 1616/7134 completed (loss: 0.1859370619058609, acc: 0.9612902998924255)
[2025-02-13 19:01:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:52][root][INFO] - Training Epoch: 1/2, step 1617/7134 completed (loss: 0.19129551947116852, acc: 0.9496402740478516)
[2025-02-13 19:01:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:53][root][INFO] - Training Epoch: 1/2, step 1618/7134 completed (loss: 0.4843437969684601, acc: 0.9083969593048096)
[2025-02-13 19:01:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:53][root][INFO] - Training Epoch: 1/2, step 1619/7134 completed (loss: 0.41155409812927246, acc: 0.9024389982223511)
[2025-02-13 19:01:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:54][root][INFO] - Training Epoch: 1/2, step 1620/7134 completed (loss: 0.426180899143219, acc: 0.8896104097366333)
[2025-02-13 19:01:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:54][root][INFO] - Training Epoch: 1/2, step 1621/7134 completed (loss: 0.4984488785266876, acc: 0.8888888955116272)
[2025-02-13 19:01:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:54][root][INFO] - Training Epoch: 1/2, step 1622/7134 completed (loss: 0.3404562771320343, acc: 0.915032684803009)
[2025-02-13 19:01:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:55][root][INFO] - Training Epoch: 1/2, step 1623/7134 completed (loss: 0.3410796523094177, acc: 0.9102563858032227)
[2025-02-13 19:01:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:55][root][INFO] - Training Epoch: 1/2, step 1624/7134 completed (loss: 0.2901940643787384, acc: 0.9285714030265808)
[2025-02-13 19:01:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:56][root][INFO] - Training Epoch: 1/2, step 1625/7134 completed (loss: 0.24233737587928772, acc: 0.9312499761581421)
[2025-02-13 19:01:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:56][root][INFO] - Training Epoch: 1/2, step 1626/7134 completed (loss: 0.3590467870235443, acc: 0.9484536051750183)
[2025-02-13 19:01:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:56][root][INFO] - Training Epoch: 1/2, step 1627/7134 completed (loss: 0.33793190121650696, acc: 0.9024389982223511)
[2025-02-13 19:01:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:57][root][INFO] - Training Epoch: 1/2, step 1628/7134 completed (loss: 0.18324072659015656, acc: 0.9506173133850098)
[2025-02-13 19:01:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:57][root][INFO] - Training Epoch: 1/2, step 1629/7134 completed (loss: 0.2298673391342163, acc: 0.9545454382896423)
[2025-02-13 19:01:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:58][root][INFO] - Training Epoch: 1/2, step 1630/7134 completed (loss: 0.282060831785202, acc: 0.936170220375061)
[2025-02-13 19:01:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:58][root][INFO] - Training Epoch: 1/2, step 1631/7134 completed (loss: 0.37988898158073425, acc: 0.9280575513839722)
[2025-02-13 19:01:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:58][root][INFO] - Training Epoch: 1/2, step 1632/7134 completed (loss: 0.19586995244026184, acc: 0.9626865386962891)
[2025-02-13 19:01:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:59][root][INFO] - Training Epoch: 1/2, step 1633/7134 completed (loss: 0.25421661138534546, acc: 0.9571428298950195)
[2025-02-13 19:01:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:01:59][root][INFO] - Training Epoch: 1/2, step 1634/7134 completed (loss: 0.19518618285655975, acc: 0.9496855139732361)
[2025-02-13 19:01:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:00][root][INFO] - Training Epoch: 1/2, step 1635/7134 completed (loss: 0.3164384365081787, acc: 0.9074074029922485)
[2025-02-13 19:02:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:00][root][INFO] - Training Epoch: 1/2, step 1636/7134 completed (loss: 0.5072894096374512, acc: 0.8899082541465759)
[2025-02-13 19:02:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:00][root][INFO] - Training Epoch: 1/2, step 1637/7134 completed (loss: 0.18221649527549744, acc: 0.9627329111099243)
[2025-02-13 19:02:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:01][root][INFO] - Training Epoch: 1/2, step 1638/7134 completed (loss: 0.6863889098167419, acc: 0.8582677245140076)
[2025-02-13 19:02:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:01][root][INFO] - Training Epoch: 1/2, step 1639/7134 completed (loss: 0.7561790943145752, acc: 0.8260869383811951)
[2025-02-13 19:02:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:02][root][INFO] - Training Epoch: 1/2, step 1640/7134 completed (loss: 0.3216985762119293, acc: 0.9130434989929199)
[2025-02-13 19:02:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:02][root][INFO] - Training Epoch: 1/2, step 1641/7134 completed (loss: 0.3186974823474884, acc: 0.9144737124443054)
[2025-02-13 19:02:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:02][root][INFO] - Training Epoch: 1/2, step 1642/7134 completed (loss: 0.4017663598060608, acc: 0.8742138147354126)
[2025-02-13 19:02:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:03][root][INFO] - Training Epoch: 1/2, step 1643/7134 completed (loss: 0.6469609141349792, acc: 0.8454545736312866)
[2025-02-13 19:02:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:03][root][INFO] - Training Epoch: 1/2, step 1644/7134 completed (loss: 0.4972592294216156, acc: 0.893081784248352)
[2025-02-13 19:02:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:03][root][INFO] - Training Epoch: 1/2, step 1645/7134 completed (loss: 0.3609425127506256, acc: 0.8965517282485962)
[2025-02-13 19:02:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:04][root][INFO] - Training Epoch: 1/2, step 1646/7134 completed (loss: 0.4021226465702057, acc: 0.9096774458885193)
[2025-02-13 19:02:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:04][root][INFO] - Training Epoch: 1/2, step 1647/7134 completed (loss: 0.26327258348464966, acc: 0.9624413251876831)
[2025-02-13 19:02:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:05][root][INFO] - Training Epoch: 1/2, step 1648/7134 completed (loss: 0.3755185604095459, acc: 0.93034827709198)
[2025-02-13 19:02:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:05][root][INFO] - Training Epoch: 1/2, step 1649/7134 completed (loss: 0.14394038915634155, acc: 0.9690265655517578)
[2025-02-13 19:02:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:05][root][INFO] - Training Epoch: 1/2, step 1650/7134 completed (loss: 0.20389671623706818, acc: 0.9648241400718689)
[2025-02-13 19:02:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:06][root][INFO] - Training Epoch: 1/2, step 1651/7134 completed (loss: 0.2957218885421753, acc: 0.949999988079071)
[2025-02-13 19:02:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:06][root][INFO] - Training Epoch: 1/2, step 1652/7134 completed (loss: 0.36564233899116516, acc: 0.9059829115867615)
[2025-02-13 19:02:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:06][root][INFO] - Training Epoch: 1/2, step 1653/7134 completed (loss: 0.3149281144142151, acc: 0.9289617538452148)
[2025-02-13 19:02:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:07][root][INFO] - Training Epoch: 1/2, step 1654/7134 completed (loss: 0.18699732422828674, acc: 0.9527897238731384)
[2025-02-13 19:02:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:07][root][INFO] - Training Epoch: 1/2, step 1655/7134 completed (loss: 0.2263428419828415, acc: 0.9438775777816772)
[2025-02-13 19:02:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:08][root][INFO] - Training Epoch: 1/2, step 1656/7134 completed (loss: 0.3138898015022278, acc: 0.9058296084403992)
[2025-02-13 19:02:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:08][root][INFO] - Training Epoch: 1/2, step 1657/7134 completed (loss: 0.21492062509059906, acc: 0.930232584476471)
[2025-02-13 19:02:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:09][root][INFO] - Training Epoch: 1/2, step 1658/7134 completed (loss: 0.1927109658718109, acc: 0.9502074718475342)
[2025-02-13 19:02:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:09][root][INFO] - Training Epoch: 1/2, step 1659/7134 completed (loss: 0.20744988322257996, acc: 0.9520000219345093)
[2025-02-13 19:02:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:09][root][INFO] - Training Epoch: 1/2, step 1660/7134 completed (loss: 0.26186805963516235, acc: 0.9375)
[2025-02-13 19:02:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:10][root][INFO] - Training Epoch: 1/2, step 1661/7134 completed (loss: 0.31791889667510986, acc: 0.939393937587738)
[2025-02-13 19:02:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:10][root][INFO] - Training Epoch: 1/2, step 1662/7134 completed (loss: 0.1639232337474823, acc: 0.9573459625244141)
[2025-02-13 19:02:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:10][root][INFO] - Training Epoch: 1/2, step 1663/7134 completed (loss: 0.19904901087284088, acc: 0.9531915187835693)
[2025-02-13 19:02:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:11][root][INFO] - Training Epoch: 1/2, step 1664/7134 completed (loss: 0.2208767682313919, acc: 0.9399999976158142)
[2025-02-13 19:02:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:11][root][INFO] - Training Epoch: 1/2, step 1665/7134 completed (loss: 0.20129506289958954, acc: 0.9570552110671997)
[2025-02-13 19:02:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:12][root][INFO] - Training Epoch: 1/2, step 1666/7134 completed (loss: 0.15313754975795746, acc: 0.9537814855575562)
[2025-02-13 19:02:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:12][root][INFO] - Training Epoch: 1/2, step 1667/7134 completed (loss: 0.19638711214065552, acc: 0.9343434572219849)
[2025-02-13 19:02:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:12][root][INFO] - Training Epoch: 1/2, step 1668/7134 completed (loss: 0.17094174027442932, acc: 0.9596412777900696)
[2025-02-13 19:02:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:13][root][INFO] - Training Epoch: 1/2, step 1669/7134 completed (loss: 0.24670319259166718, acc: 0.9351145029067993)
[2025-02-13 19:02:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:13][root][INFO] - Training Epoch: 1/2, step 1670/7134 completed (loss: 0.22615748643875122, acc: 0.9543726444244385)
[2025-02-13 19:02:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:14][root][INFO] - Training Epoch: 1/2, step 1671/7134 completed (loss: 0.2019464075565338, acc: 0.9438775777816772)
[2025-02-13 19:02:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:14][root][INFO] - Training Epoch: 1/2, step 1672/7134 completed (loss: 0.38723206520080566, acc: 0.9064327478408813)
[2025-02-13 19:02:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:14][root][INFO] - Training Epoch: 1/2, step 1673/7134 completed (loss: 0.30275487899780273, acc: 0.9378882050514221)
[2025-02-13 19:02:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:15][root][INFO] - Training Epoch: 1/2, step 1674/7134 completed (loss: 0.39231324195861816, acc: 0.9342105388641357)
[2025-02-13 19:02:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:15][root][INFO] - Training Epoch: 1/2, step 1675/7134 completed (loss: 0.4698764383792877, acc: 0.8767123222351074)
[2025-02-13 19:02:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:15][root][INFO] - Training Epoch: 1/2, step 1676/7134 completed (loss: 0.4531401991844177, acc: 0.8525640964508057)
[2025-02-13 19:02:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:16][root][INFO] - Training Epoch: 1/2, step 1677/7134 completed (loss: 0.4431415796279907, acc: 0.914893627166748)
[2025-02-13 19:02:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:16][root][INFO] - Training Epoch: 1/2, step 1678/7134 completed (loss: 0.3601106107234955, acc: 0.9281437397003174)
[2025-02-13 19:02:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:17][root][INFO] - Training Epoch: 1/2, step 1679/7134 completed (loss: 0.38587456941604614, acc: 0.8963414430618286)
[2025-02-13 19:02:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:17][root][INFO] - Training Epoch: 1/2, step 1680/7134 completed (loss: 0.3550548255443573, acc: 0.8999999761581421)
[2025-02-13 19:02:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:18][root][INFO] - Training Epoch: 1/2, step 1681/7134 completed (loss: 0.3368687331676483, acc: 0.9308176040649414)
[2025-02-13 19:02:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:18][root][INFO] - Training Epoch: 1/2, step 1682/7134 completed (loss: 0.3054700493812561, acc: 0.9383561611175537)
[2025-02-13 19:02:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:18][root][INFO] - Training Epoch: 1/2, step 1683/7134 completed (loss: 0.3310324251651764, acc: 0.9172413945198059)
[2025-02-13 19:02:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:19][root][INFO] - Training Epoch: 1/2, step 1684/7134 completed (loss: 0.2646787762641907, acc: 0.914893627166748)
[2025-02-13 19:02:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:19][root][INFO] - Training Epoch: 1/2, step 1685/7134 completed (loss: 0.24592536687850952, acc: 0.9515151381492615)
[2025-02-13 19:02:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:19][root][INFO] - Training Epoch: 1/2, step 1686/7134 completed (loss: 0.17073829472064972, acc: 0.9624060392379761)
[2025-02-13 19:02:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:20][root][INFO] - Training Epoch: 1/2, step 1687/7134 completed (loss: 0.1364927738904953, acc: 0.9646017551422119)
[2025-02-13 19:02:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:20][root][INFO] - Training Epoch: 1/2, step 1688/7134 completed (loss: 0.2187168449163437, acc: 0.931506872177124)
[2025-02-13 19:02:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:21][root][INFO] - Training Epoch: 1/2, step 1689/7134 completed (loss: 0.41778653860092163, acc: 0.8709677457809448)
[2025-02-13 19:02:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:21][root][INFO] - Training Epoch: 1/2, step 1690/7134 completed (loss: 0.23040062189102173, acc: 0.9292035102844238)
[2025-02-13 19:02:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:21][root][INFO] - Training Epoch: 1/2, step 1691/7134 completed (loss: 0.26035159826278687, acc: 0.936170220375061)
[2025-02-13 19:02:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:22][root][INFO] - Training Epoch: 1/2, step 1692/7134 completed (loss: 0.1893579065799713, acc: 0.9512194991111755)
[2025-02-13 19:02:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:22][root][INFO] - Training Epoch: 1/2, step 1693/7134 completed (loss: 0.23014242947101593, acc: 0.9455782175064087)
[2025-02-13 19:02:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:23][root][INFO] - Training Epoch: 1/2, step 1694/7134 completed (loss: 0.41859307885169983, acc: 0.9172413945198059)
[2025-02-13 19:02:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:23][root][INFO] - Training Epoch: 1/2, step 1695/7134 completed (loss: 0.2967331111431122, acc: 0.924369752407074)
[2025-02-13 19:02:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:23][root][INFO] - Training Epoch: 1/2, step 1696/7134 completed (loss: 0.3426688611507416, acc: 0.9172932505607605)
[2025-02-13 19:02:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:24][root][INFO] - Training Epoch: 1/2, step 1697/7134 completed (loss: 0.22599303722381592, acc: 0.9440559148788452)
[2025-02-13 19:02:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:24][root][INFO] - Training Epoch: 1/2, step 1698/7134 completed (loss: 0.1319490522146225, acc: 0.9790209531784058)
[2025-02-13 19:02:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:24][root][INFO] - Training Epoch: 1/2, step 1699/7134 completed (loss: 0.23743082582950592, acc: 0.9402984976768494)
[2025-02-13 19:02:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:25][root][INFO] - Training Epoch: 1/2, step 1700/7134 completed (loss: 0.44289302825927734, acc: 0.89552241563797)
[2025-02-13 19:02:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:25][root][INFO] - Training Epoch: 1/2, step 1701/7134 completed (loss: 0.4235908091068268, acc: 0.9066666960716248)
[2025-02-13 19:02:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:26][root][INFO] - Training Epoch: 1/2, step 1702/7134 completed (loss: 0.34716010093688965, acc: 0.9341317415237427)
[2025-02-13 19:02:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:26][root][INFO] - Training Epoch: 1/2, step 1703/7134 completed (loss: 0.5246117115020752, acc: 0.8940397500991821)
[2025-02-13 19:02:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:26][root][INFO] - Training Epoch: 1/2, step 1704/7134 completed (loss: 0.6670650839805603, acc: 0.8475610017776489)
[2025-02-13 19:02:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:27][root][INFO] - Training Epoch: 1/2, step 1705/7134 completed (loss: 0.4595021605491638, acc: 0.9039999842643738)
[2025-02-13 19:02:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:27][root][INFO] - Training Epoch: 1/2, step 1706/7134 completed (loss: 0.31125468015670776, acc: 0.9571428298950195)
[2025-02-13 19:02:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:28][root][INFO] - Training Epoch: 1/2, step 1707/7134 completed (loss: 0.4033787250518799, acc: 0.9396551847457886)
[2025-02-13 19:02:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:28][root][INFO] - Training Epoch: 1/2, step 1708/7134 completed (loss: 0.2895006835460663, acc: 0.9185185432434082)
[2025-02-13 19:02:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:28][root][INFO] - Training Epoch: 1/2, step 1709/7134 completed (loss: 0.5022834539413452, acc: 0.846666693687439)
[2025-02-13 19:02:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:29][root][INFO] - Training Epoch: 1/2, step 1710/7134 completed (loss: 0.4674873948097229, acc: 0.8648648858070374)
[2025-02-13 19:02:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:29][root][INFO] - Training Epoch: 1/2, step 1711/7134 completed (loss: 0.3469372093677521, acc: 0.9274193644523621)
[2025-02-13 19:02:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:30][root][INFO] - Training Epoch: 1/2, step 1712/7134 completed (loss: 0.2350098043680191, acc: 0.9365079402923584)
[2025-02-13 19:02:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:30][root][INFO] - Training Epoch: 1/2, step 1713/7134 completed (loss: 0.4130357503890991, acc: 0.9047619104385376)
[2025-02-13 19:02:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:31][root][INFO] - Training Epoch: 1/2, step 1714/7134 completed (loss: 0.30785974860191345, acc: 0.9290780425071716)
[2025-02-13 19:02:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:31][root][INFO] - Training Epoch: 1/2, step 1715/7134 completed (loss: 0.23096416890621185, acc: 0.9622641801834106)
[2025-02-13 19:02:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:31][root][INFO] - Training Epoch: 1/2, step 1716/7134 completed (loss: 0.516819417476654, acc: 0.9210526347160339)
[2025-02-13 19:02:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:32][root][INFO] - Training Epoch: 1/2, step 1717/7134 completed (loss: 0.2317122220993042, acc: 0.9379844665527344)
[2025-02-13 19:02:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:32][root][INFO] - Training Epoch: 1/2, step 1718/7134 completed (loss: 0.5643031001091003, acc: 0.9059829115867615)
[2025-02-13 19:02:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:33][root][INFO] - Training Epoch: 1/2, step 1719/7134 completed (loss: 0.34584227204322815, acc: 0.9349112510681152)
[2025-02-13 19:02:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:33][root][INFO] - Training Epoch: 1/2, step 1720/7134 completed (loss: 0.611860454082489, acc: 0.8461538553237915)
[2025-02-13 19:02:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:33][root][INFO] - Training Epoch: 1/2, step 1721/7134 completed (loss: 0.29504311084747314, acc: 0.90625)
[2025-02-13 19:02:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:34][root][INFO] - Training Epoch: 1/2, step 1722/7134 completed (loss: 0.24008607864379883, acc: 0.939393937587738)
[2025-02-13 19:02:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:34][root][INFO] - Training Epoch: 1/2, step 1723/7134 completed (loss: 0.5842905640602112, acc: 0.8888888955116272)
[2025-02-13 19:02:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:34][root][INFO] - Training Epoch: 1/2, step 1724/7134 completed (loss: 0.3600977957248688, acc: 0.9296875)
[2025-02-13 19:02:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:35][root][INFO] - Training Epoch: 1/2, step 1725/7134 completed (loss: 0.19104669988155365, acc: 0.9253731369972229)
[2025-02-13 19:02:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:35][root][INFO] - Training Epoch: 1/2, step 1726/7134 completed (loss: 0.13552570343017578, acc: 0.9677419066429138)
[2025-02-13 19:02:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:36][root][INFO] - Training Epoch: 1/2, step 1727/7134 completed (loss: 0.20858162641525269, acc: 0.9557521939277649)
[2025-02-13 19:02:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:36][root][INFO] - Training Epoch: 1/2, step 1728/7134 completed (loss: 0.18390031158924103, acc: 0.9615384340286255)
[2025-02-13 19:02:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:36][root][INFO] - Training Epoch: 1/2, step 1729/7134 completed (loss: 0.3388911187648773, acc: 0.9189189076423645)
[2025-02-13 19:02:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:37][root][INFO] - Training Epoch: 1/2, step 1730/7134 completed (loss: 0.23636843264102936, acc: 0.9532710313796997)
[2025-02-13 19:02:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:37][root][INFO] - Training Epoch: 1/2, step 1731/7134 completed (loss: 0.7182521820068359, acc: 0.8554216623306274)
[2025-02-13 19:02:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:38][root][INFO] - Training Epoch: 1/2, step 1732/7134 completed (loss: 0.20100809633731842, acc: 0.9351851940155029)
[2025-02-13 19:02:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:38][root][INFO] - Training Epoch: 1/2, step 1733/7134 completed (loss: 0.44419577717781067, acc: 0.8990825414657593)
[2025-02-13 19:02:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:39][root][INFO] - Training Epoch: 1/2, step 1734/7134 completed (loss: 0.3815666437149048, acc: 0.9096385836601257)
[2025-02-13 19:02:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:39][root][INFO] - Training Epoch: 1/2, step 1735/7134 completed (loss: 0.16765035688877106, acc: 0.9536423683166504)
[2025-02-13 19:02:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:39][root][INFO] - Training Epoch: 1/2, step 1736/7134 completed (loss: 0.3433006703853607, acc: 0.9113923907279968)
[2025-02-13 19:02:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:40][root][INFO] - Training Epoch: 1/2, step 1737/7134 completed (loss: 0.3080374002456665, acc: 0.9333333373069763)
[2025-02-13 19:02:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:40][root][INFO] - Training Epoch: 1/2, step 1738/7134 completed (loss: 0.11066250503063202, acc: 0.9776536226272583)
[2025-02-13 19:02:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:41][root][INFO] - Training Epoch: 1/2, step 1739/7134 completed (loss: 0.27918508648872375, acc: 0.9248120188713074)
[2025-02-13 19:02:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:41][root][INFO] - Training Epoch: 1/2, step 1740/7134 completed (loss: 0.09826405346393585, acc: 0.9797297120094299)
[2025-02-13 19:02:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:41][root][INFO] - Training Epoch: 1/2, step 1741/7134 completed (loss: 0.20392648875713348, acc: 0.95652174949646)
[2025-02-13 19:02:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:42][root][INFO] - Training Epoch: 1/2, step 1742/7134 completed (loss: 0.34618380665779114, acc: 0.9139072895050049)
[2025-02-13 19:02:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:42][root][INFO] - Training Epoch: 1/2, step 1743/7134 completed (loss: 0.3600102663040161, acc: 0.9387755393981934)
[2025-02-13 19:02:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:43][root][INFO] - Training Epoch: 1/2, step 1744/7134 completed (loss: 0.25311097502708435, acc: 0.932330846786499)
[2025-02-13 19:02:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:43][root][INFO] - Training Epoch: 1/2, step 1745/7134 completed (loss: 0.23923705518245697, acc: 0.9411764740943909)
[2025-02-13 19:02:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:43][root][INFO] - Training Epoch: 1/2, step 1746/7134 completed (loss: 0.18823640048503876, acc: 0.957446813583374)
[2025-02-13 19:02:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:44][root][INFO] - Training Epoch: 1/2, step 1747/7134 completed (loss: 0.3261244595050812, acc: 0.9268292784690857)
[2025-02-13 19:02:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:44][root][INFO] - Training Epoch: 1/2, step 1748/7134 completed (loss: 0.26361799240112305, acc: 0.9470587968826294)
[2025-02-13 19:02:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:44][root][INFO] - Training Epoch: 1/2, step 1749/7134 completed (loss: 0.3494856655597687, acc: 0.9476439952850342)
[2025-02-13 19:02:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:45][root][INFO] - Training Epoch: 1/2, step 1750/7134 completed (loss: 0.34566324949264526, acc: 0.9408283829689026)
[2025-02-13 19:02:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:45][root][INFO] - Training Epoch: 1/2, step 1751/7134 completed (loss: 0.3998267650604248, acc: 0.9166666865348816)
[2025-02-13 19:02:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:46][root][INFO] - Training Epoch: 1/2, step 1752/7134 completed (loss: 0.4593445360660553, acc: 0.9212121367454529)
[2025-02-13 19:02:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:46][root][INFO] - Training Epoch: 1/2, step 1753/7134 completed (loss: 0.3752669095993042, acc: 0.9139072895050049)
[2025-02-13 19:02:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:47][root][INFO] - Training Epoch: 1/2, step 1754/7134 completed (loss: 0.399397611618042, acc: 0.9215686321258545)
[2025-02-13 19:02:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:47][root][INFO] - Training Epoch: 1/2, step 1755/7134 completed (loss: 0.15024234354496002, acc: 0.9738562107086182)
[2025-02-13 19:02:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:47][root][INFO] - Training Epoch: 1/2, step 1756/7134 completed (loss: 0.36482563614845276, acc: 0.8999999761581421)
[2025-02-13 19:02:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:48][root][INFO] - Training Epoch: 1/2, step 1757/7134 completed (loss: 0.174592062830925, acc: 0.9586777091026306)
[2025-02-13 19:02:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:48][root][INFO] - Training Epoch: 1/2, step 1758/7134 completed (loss: 0.15141825377941132, acc: 0.970588207244873)
[2025-02-13 19:02:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:49][root][INFO] - Training Epoch: 1/2, step 1759/7134 completed (loss: 0.2059321105480194, acc: 0.9539473652839661)
[2025-02-13 19:02:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:49][root][INFO] - Training Epoch: 1/2, step 1760/7134 completed (loss: 0.12853601574897766, acc: 0.9836065769195557)
[2025-02-13 19:02:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:49][root][INFO] - Training Epoch: 1/2, step 1761/7134 completed (loss: 0.2826365828514099, acc: 0.9214285612106323)
[2025-02-13 19:02:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:50][root][INFO] - Training Epoch: 1/2, step 1762/7134 completed (loss: 0.30056703090667725, acc: 0.9320388436317444)
[2025-02-13 19:02:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:50][root][INFO] - Training Epoch: 1/2, step 1763/7134 completed (loss: 0.42064169049263, acc: 0.8698630332946777)
[2025-02-13 19:02:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:50][root][INFO] - Training Epoch: 1/2, step 1764/7134 completed (loss: 0.4217160642147064, acc: 0.8870967626571655)
[2025-02-13 19:02:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:51][root][INFO] - Training Epoch: 1/2, step 1765/7134 completed (loss: 0.3308710753917694, acc: 0.8954248428344727)
[2025-02-13 19:02:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:51][root][INFO] - Training Epoch: 1/2, step 1766/7134 completed (loss: 0.4490126669406891, acc: 0.8961039185523987)
[2025-02-13 19:02:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:52][root][INFO] - Training Epoch: 1/2, step 1767/7134 completed (loss: 0.5272923111915588, acc: 0.887005627155304)
[2025-02-13 19:02:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:52][root][INFO] - Training Epoch: 1/2, step 1768/7134 completed (loss: 0.15508587658405304, acc: 0.9642857313156128)
[2025-02-13 19:02:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:53][root][INFO] - Training Epoch: 1/2, step 1769/7134 completed (loss: 0.29484644532203674, acc: 0.9266666769981384)
[2025-02-13 19:02:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:53][root][INFO] - Training Epoch: 1/2, step 1770/7134 completed (loss: 0.2522236108779907, acc: 0.9383561611175537)
[2025-02-13 19:02:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:53][root][INFO] - Training Epoch: 1/2, step 1771/7134 completed (loss: 0.36109301447868347, acc: 0.9090909361839294)
[2025-02-13 19:02:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:54][root][INFO] - Training Epoch: 1/2, step 1772/7134 completed (loss: 0.33001601696014404, acc: 0.9142857193946838)
[2025-02-13 19:02:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:54][root][INFO] - Training Epoch: 1/2, step 1773/7134 completed (loss: 0.23166523873806, acc: 0.936170220375061)
[2025-02-13 19:02:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:55][root][INFO] - Training Epoch: 1/2, step 1774/7134 completed (loss: 0.13527625799179077, acc: 0.9621211886405945)
[2025-02-13 19:02:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:55][root][INFO] - Training Epoch: 1/2, step 1775/7134 completed (loss: 0.14685122668743134, acc: 0.9794520735740662)
[2025-02-13 19:02:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:56][root][INFO] - Training Epoch: 1/2, step 1776/7134 completed (loss: 0.22185218334197998, acc: 0.931034505367279)
[2025-02-13 19:02:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:56][root][INFO] - Training Epoch: 1/2, step 1777/7134 completed (loss: 0.0721849650144577, acc: 0.987261176109314)
[2025-02-13 19:02:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:56][root][INFO] - Training Epoch: 1/2, step 1778/7134 completed (loss: 0.34818580746650696, acc: 0.9182389974594116)
[2025-02-13 19:02:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:57][root][INFO] - Training Epoch: 1/2, step 1779/7134 completed (loss: 0.24767202138900757, acc: 0.9557521939277649)
[2025-02-13 19:02:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:57][root][INFO] - Training Epoch: 1/2, step 1780/7134 completed (loss: 0.17155329883098602, acc: 0.9580838084220886)
[2025-02-13 19:02:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:58][root][INFO] - Training Epoch: 1/2, step 1781/7134 completed (loss: 0.12922537326812744, acc: 0.9731543660163879)
[2025-02-13 19:02:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:02:58][root][INFO] - Training Epoch: 1/2, step 1782/7134 completed (loss: 0.15806886553764343, acc: 0.9615384340286255)
[2025-02-13 19:02:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:03:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:04:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:05:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:06:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:12][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.4500, device='cuda:0') eval_epoch_loss=tensor(0.3716, device='cuda:0') eval_epoch_acc=tensor(0.9162, device='cuda:0')
[2025-02-13 19:07:12][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-13 19:07:12][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-13 19:07:12][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_wavlm_llama32_1b_linear_peft_transfer_librispeech-100/asr_epoch_1_step_1783_loss_0.3715682625770569/model.pt
[2025-02-13 19:07:12][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_wavlm_llama32_1b_linear_peft_transfer_librispeech-100 directory
[2025-02-13 19:07:12][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.3715682625770569
[2025-02-13 19:07:12][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.9161514639854431
[2025-02-13 19:07:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:13][root][INFO] - Training Epoch: 1/2, step 1783/7134 completed (loss: 0.30213144421577454, acc: 0.9248554706573486)
[2025-02-13 19:07:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:13][root][INFO] - Training Epoch: 1/2, step 1784/7134 completed (loss: 0.16688436269760132, acc: 0.9871794581413269)
[2025-02-13 19:07:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:14][root][INFO] - Training Epoch: 1/2, step 1785/7134 completed (loss: 0.34097298979759216, acc: 0.9156626462936401)
[2025-02-13 19:07:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:14][root][INFO] - Training Epoch: 1/2, step 1786/7134 completed (loss: 0.1905331164598465, acc: 0.948387086391449)
[2025-02-13 19:07:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:14][root][INFO] - Training Epoch: 1/2, step 1787/7134 completed (loss: 0.3085114061832428, acc: 0.9253731369972229)
[2025-02-13 19:07:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:15][root][INFO] - Training Epoch: 1/2, step 1788/7134 completed (loss: 0.27067747712135315, acc: 0.931034505367279)
[2025-02-13 19:07:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:15][root][INFO] - Training Epoch: 1/2, step 1789/7134 completed (loss: 0.24074873328208923, acc: 0.9407894611358643)
[2025-02-13 19:07:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:16][root][INFO] - Training Epoch: 1/2, step 1790/7134 completed (loss: 0.5234473943710327, acc: 0.896774172782898)
[2025-02-13 19:07:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:16][root][INFO] - Training Epoch: 1/2, step 1791/7134 completed (loss: 0.6357429027557373, acc: 0.8650306463241577)
[2025-02-13 19:07:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:16][root][INFO] - Training Epoch: 1/2, step 1792/7134 completed (loss: 0.9629269242286682, acc: 0.826815664768219)
[2025-02-13 19:07:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:17][root][INFO] - Training Epoch: 1/2, step 1793/7134 completed (loss: 0.7836556434631348, acc: 0.8639053106307983)
[2025-02-13 19:07:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:17][root][INFO] - Training Epoch: 1/2, step 1794/7134 completed (loss: 0.5088650584220886, acc: 0.8881579041481018)
[2025-02-13 19:07:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:17][root][INFO] - Training Epoch: 1/2, step 1795/7134 completed (loss: 0.8813314437866211, acc: 0.8187500238418579)
[2025-02-13 19:07:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:18][root][INFO] - Training Epoch: 1/2, step 1796/7134 completed (loss: 0.6667386293411255, acc: 0.8418079018592834)
[2025-02-13 19:07:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:18][root][INFO] - Training Epoch: 1/2, step 1797/7134 completed (loss: 0.6121185421943665, acc: 0.8742856979370117)
[2025-02-13 19:07:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:19][root][INFO] - Training Epoch: 1/2, step 1798/7134 completed (loss: 0.5490043759346008, acc: 0.8527131676673889)
[2025-02-13 19:07:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:19][root][INFO] - Training Epoch: 1/2, step 1799/7134 completed (loss: 0.5798896551132202, acc: 0.868686854839325)
[2025-02-13 19:07:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:19][root][INFO] - Training Epoch: 1/2, step 1800/7134 completed (loss: 0.529543399810791, acc: 0.8761904835700989)
[2025-02-13 19:07:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:20][root][INFO] - Training Epoch: 1/2, step 1801/7134 completed (loss: 0.4372800588607788, acc: 0.9099099040031433)
[2025-02-13 19:07:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:20][root][INFO] - Training Epoch: 1/2, step 1802/7134 completed (loss: 0.40867045521736145, acc: 0.9095744490623474)
[2025-02-13 19:07:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:20][root][INFO] - Training Epoch: 1/2, step 1803/7134 completed (loss: 0.5073217749595642, acc: 0.8475610017776489)
[2025-02-13 19:07:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:21][root][INFO] - Training Epoch: 1/2, step 1804/7134 completed (loss: 0.20395177602767944, acc: 0.9523809552192688)
[2025-02-13 19:07:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:21][root][INFO] - Training Epoch: 1/2, step 1805/7134 completed (loss: 0.27890485525131226, acc: 0.9239766001701355)
[2025-02-13 19:07:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:22][root][INFO] - Training Epoch: 1/2, step 1806/7134 completed (loss: 0.23962664604187012, acc: 0.9508196711540222)
[2025-02-13 19:07:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:22][root][INFO] - Training Epoch: 1/2, step 1807/7134 completed (loss: 0.33321478962898254, acc: 0.9216867685317993)
[2025-02-13 19:07:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:22][root][INFO] - Training Epoch: 1/2, step 1808/7134 completed (loss: 0.17342880368232727, acc: 0.9604519605636597)
[2025-02-13 19:07:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:23][root][INFO] - Training Epoch: 1/2, step 1809/7134 completed (loss: 0.1665414422750473, acc: 0.9479768872261047)
[2025-02-13 19:07:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:23][root][INFO] - Training Epoch: 1/2, step 1810/7134 completed (loss: 0.16070589423179626, acc: 0.9615384340286255)
[2025-02-13 19:07:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:24][root][INFO] - Training Epoch: 1/2, step 1811/7134 completed (loss: 0.3261719346046448, acc: 0.9042553305625916)
[2025-02-13 19:07:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:24][root][INFO] - Training Epoch: 1/2, step 1812/7134 completed (loss: 0.3147548735141754, acc: 0.9435028433799744)
[2025-02-13 19:07:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:24][root][INFO] - Training Epoch: 1/2, step 1813/7134 completed (loss: 0.3516824543476105, acc: 0.9060773253440857)
[2025-02-13 19:07:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:25][root][INFO] - Training Epoch: 1/2, step 1814/7134 completed (loss: 0.22438278794288635, acc: 0.950276255607605)
[2025-02-13 19:07:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:25][root][INFO] - Training Epoch: 1/2, step 1815/7134 completed (loss: 0.36499080061912537, acc: 0.9044585824012756)
[2025-02-13 19:07:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:26][root][INFO] - Training Epoch: 1/2, step 1816/7134 completed (loss: 0.39177995920181274, acc: 0.8680555820465088)
[2025-02-13 19:07:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:26][root][INFO] - Training Epoch: 1/2, step 1817/7134 completed (loss: 0.23452989757061005, acc: 0.9427083134651184)
[2025-02-13 19:07:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:26][root][INFO] - Training Epoch: 1/2, step 1818/7134 completed (loss: 0.32556942105293274, acc: 0.9424083828926086)
[2025-02-13 19:07:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:27][root][INFO] - Training Epoch: 1/2, step 1819/7134 completed (loss: 0.2366437017917633, acc: 0.9534883499145508)
[2025-02-13 19:07:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:27][root][INFO] - Training Epoch: 1/2, step 1820/7134 completed (loss: 0.2252001166343689, acc: 0.9433962106704712)
[2025-02-13 19:07:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:28][root][INFO] - Training Epoch: 1/2, step 1821/7134 completed (loss: 0.19167470932006836, acc: 0.9345794320106506)
[2025-02-13 19:07:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:28][root][INFO] - Training Epoch: 1/2, step 1822/7134 completed (loss: 0.19571362435817719, acc: 0.9539473652839661)
[2025-02-13 19:07:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:28][root][INFO] - Training Epoch: 1/2, step 1823/7134 completed (loss: 0.24496492743492126, acc: 0.9534883499145508)
[2025-02-13 19:07:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:29][root][INFO] - Training Epoch: 1/2, step 1824/7134 completed (loss: 0.1336156129837036, acc: 0.9781420826911926)
[2025-02-13 19:07:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:29][root][INFO] - Training Epoch: 1/2, step 1825/7134 completed (loss: 0.17357683181762695, acc: 0.9626168012619019)
[2025-02-13 19:07:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:30][root][INFO] - Training Epoch: 1/2, step 1826/7134 completed (loss: 0.17031410336494446, acc: 0.9649122953414917)
[2025-02-13 19:07:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:30][root][INFO] - Training Epoch: 1/2, step 1827/7134 completed (loss: 0.23838849365711212, acc: 0.9620853066444397)
[2025-02-13 19:07:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:30][root][INFO] - Training Epoch: 1/2, step 1828/7134 completed (loss: 0.14975589513778687, acc: 0.9555555582046509)
[2025-02-13 19:07:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:31][root][INFO] - Training Epoch: 1/2, step 1829/7134 completed (loss: 0.18716251850128174, acc: 0.9462365508079529)
[2025-02-13 19:07:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:31][root][INFO] - Training Epoch: 1/2, step 1830/7134 completed (loss: 0.18381637334823608, acc: 0.9585492014884949)
[2025-02-13 19:07:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:32][root][INFO] - Training Epoch: 1/2, step 1831/7134 completed (loss: 0.16187599301338196, acc: 0.9696969985961914)
[2025-02-13 19:07:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:32][root][INFO] - Training Epoch: 1/2, step 1832/7134 completed (loss: 0.5305936336517334, acc: 0.8837209343910217)
[2025-02-13 19:07:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:32][root][INFO] - Training Epoch: 1/2, step 1833/7134 completed (loss: 0.9483636617660522, acc: 0.8536585569381714)
[2025-02-13 19:07:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:33][root][INFO] - Training Epoch: 1/2, step 1834/7134 completed (loss: 0.2026626318693161, acc: 0.942307710647583)
[2025-02-13 19:07:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:33][root][INFO] - Training Epoch: 1/2, step 1835/7134 completed (loss: 0.4427192509174347, acc: 0.8961039185523987)
[2025-02-13 19:07:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:34][root][INFO] - Training Epoch: 1/2, step 1836/7134 completed (loss: 0.2943285405635834, acc: 0.9304812550544739)
[2025-02-13 19:07:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:34][root][INFO] - Training Epoch: 1/2, step 1837/7134 completed (loss: 0.29466456174850464, acc: 0.9120879173278809)
[2025-02-13 19:07:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:34][root][INFO] - Training Epoch: 1/2, step 1838/7134 completed (loss: 0.2439362108707428, acc: 0.9449541568756104)
[2025-02-13 19:07:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:35][root][INFO] - Training Epoch: 1/2, step 1839/7134 completed (loss: 0.248061403632164, acc: 0.9340659379959106)
[2025-02-13 19:07:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:35][root][INFO] - Training Epoch: 1/2, step 1840/7134 completed (loss: 0.29104578495025635, acc: 0.9303797483444214)
[2025-02-13 19:07:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:36][root][INFO] - Training Epoch: 1/2, step 1841/7134 completed (loss: 0.5222323536872864, acc: 0.8727272748947144)
[2025-02-13 19:07:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:36][root][INFO] - Training Epoch: 1/2, step 1842/7134 completed (loss: 0.3444085121154785, acc: 0.9085366129875183)
[2025-02-13 19:07:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:36][root][INFO] - Training Epoch: 1/2, step 1843/7134 completed (loss: 0.35201212763786316, acc: 0.9107142686843872)
[2025-02-13 19:07:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:37][root][INFO] - Training Epoch: 1/2, step 1844/7134 completed (loss: 0.18648980557918549, acc: 0.936170220375061)
[2025-02-13 19:07:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:37][root][INFO] - Training Epoch: 1/2, step 1845/7134 completed (loss: 0.22330309450626373, acc: 0.9130434989929199)
[2025-02-13 19:07:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:38][root][INFO] - Training Epoch: 1/2, step 1846/7134 completed (loss: 0.3112567365169525, acc: 0.9166666865348816)
[2025-02-13 19:07:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:38][root][INFO] - Training Epoch: 1/2, step 1847/7134 completed (loss: 0.50889652967453, acc: 0.8636363744735718)
[2025-02-13 19:07:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:38][root][INFO] - Training Epoch: 1/2, step 1848/7134 completed (loss: 0.47530850768089294, acc: 0.8829787373542786)
[2025-02-13 19:07:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:39][root][INFO] - Training Epoch: 1/2, step 1849/7134 completed (loss: 0.3578428626060486, acc: 0.8873239159584045)
[2025-02-13 19:07:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:39][root][INFO] - Training Epoch: 1/2, step 1850/7134 completed (loss: 0.4235045313835144, acc: 0.8556700944900513)
[2025-02-13 19:07:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:40][root][INFO] - Training Epoch: 1/2, step 1851/7134 completed (loss: 0.4929271340370178, acc: 0.8769230842590332)
[2025-02-13 19:07:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:40][root][INFO] - Training Epoch: 1/2, step 1852/7134 completed (loss: 0.5728996992111206, acc: 0.887499988079071)
[2025-02-13 19:07:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:40][root][INFO] - Training Epoch: 1/2, step 1853/7134 completed (loss: 0.3450286388397217, acc: 0.9045225977897644)
[2025-02-13 19:07:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:41][root][INFO] - Training Epoch: 1/2, step 1854/7134 completed (loss: 0.39529871940612793, acc: 0.8914027214050293)
[2025-02-13 19:07:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:41][root][INFO] - Training Epoch: 1/2, step 1855/7134 completed (loss: 0.2690916657447815, acc: 0.9526066184043884)
[2025-02-13 19:07:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:42][root][INFO] - Training Epoch: 1/2, step 1856/7134 completed (loss: 0.272641658782959, acc: 0.918367326259613)
[2025-02-13 19:07:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:42][root][INFO] - Training Epoch: 1/2, step 1857/7134 completed (loss: 0.5388922691345215, acc: 0.8872548937797546)
[2025-02-13 19:07:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:42][root][INFO] - Training Epoch: 1/2, step 1858/7134 completed (loss: 0.2141215205192566, acc: 0.9438775777816772)
[2025-02-13 19:07:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:43][root][INFO] - Training Epoch: 1/2, step 1859/7134 completed (loss: 0.26573988795280457, acc: 0.9186602830886841)
[2025-02-13 19:07:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:43][root][INFO] - Training Epoch: 1/2, step 1860/7134 completed (loss: 0.2149941772222519, acc: 0.9508196711540222)
[2025-02-13 19:07:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:44][root][INFO] - Training Epoch: 1/2, step 1861/7134 completed (loss: 0.20669768750667572, acc: 0.9581151604652405)
[2025-02-13 19:07:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:44][root][INFO] - Training Epoch: 1/2, step 1862/7134 completed (loss: 0.21989287436008453, acc: 0.9523809552192688)
[2025-02-13 19:07:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:44][root][INFO] - Training Epoch: 1/2, step 1863/7134 completed (loss: 0.1865055412054062, acc: 0.9704433679580688)
[2025-02-13 19:07:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:45][root][INFO] - Training Epoch: 1/2, step 1864/7134 completed (loss: 0.8258336782455444, acc: 0.8375634551048279)
[2025-02-13 19:07:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:45][root][INFO] - Training Epoch: 1/2, step 1865/7134 completed (loss: 0.38598930835723877, acc: 0.9328858852386475)
[2025-02-13 19:07:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:45][root][INFO] - Training Epoch: 1/2, step 1866/7134 completed (loss: 0.1388854682445526, acc: 0.9647058844566345)
[2025-02-13 19:07:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:46][root][INFO] - Training Epoch: 1/2, step 1867/7134 completed (loss: 0.24817489087581635, acc: 0.9204545617103577)
[2025-02-13 19:07:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:46][root][INFO] - Training Epoch: 1/2, step 1868/7134 completed (loss: 0.5287821888923645, acc: 0.8834356069564819)
[2025-02-13 19:07:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:47][root][INFO] - Training Epoch: 1/2, step 1869/7134 completed (loss: 0.3100642263889313, acc: 0.9246575236320496)
[2025-02-13 19:07:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:47][root][INFO] - Training Epoch: 1/2, step 1870/7134 completed (loss: 0.20545640587806702, acc: 0.9459459185600281)
[2025-02-13 19:07:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:47][root][INFO] - Training Epoch: 1/2, step 1871/7134 completed (loss: 0.5500760078430176, acc: 0.9225806593894958)
[2025-02-13 19:07:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:48][root][INFO] - Training Epoch: 1/2, step 1872/7134 completed (loss: 3.3046648502349854, acc: 0.5797101259231567)
[2025-02-13 19:07:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:48][root][INFO] - Training Epoch: 1/2, step 1873/7134 completed (loss: 0.8242395520210266, acc: 0.8347107172012329)
[2025-02-13 19:07:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:49][root][INFO] - Training Epoch: 1/2, step 1874/7134 completed (loss: 0.19253651797771454, acc: 0.9712643623352051)
[2025-02-13 19:07:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:49][root][INFO] - Training Epoch: 1/2, step 1875/7134 completed (loss: 0.1898839771747589, acc: 0.9464285969734192)
[2025-02-13 19:07:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:49][root][INFO] - Training Epoch: 1/2, step 1876/7134 completed (loss: 0.705107569694519, acc: 0.845714271068573)
[2025-02-13 19:07:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:50][root][INFO] - Training Epoch: 1/2, step 1877/7134 completed (loss: 0.24098746478557587, acc: 0.9491525292396545)
[2025-02-13 19:07:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:50][root][INFO] - Training Epoch: 1/2, step 1878/7134 completed (loss: 0.20407047867774963, acc: 0.9542483687400818)
[2025-02-13 19:07:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:50][root][INFO] - Training Epoch: 1/2, step 1879/7134 completed (loss: 0.9733858704566956, acc: 0.7962962985038757)
[2025-02-13 19:07:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:51][root][INFO] - Training Epoch: 1/2, step 1880/7134 completed (loss: 0.6858013868331909, acc: 0.8943089246749878)
[2025-02-13 19:07:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:51][root][INFO] - Training Epoch: 1/2, step 1881/7134 completed (loss: 0.5172265768051147, acc: 0.9014084339141846)
[2025-02-13 19:07:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:52][root][INFO] - Training Epoch: 1/2, step 1882/7134 completed (loss: 0.550234854221344, acc: 0.8804348111152649)
[2025-02-13 19:07:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:52][root][INFO] - Training Epoch: 1/2, step 1883/7134 completed (loss: 0.5724446773529053, acc: 0.8294573426246643)
[2025-02-13 19:07:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:52][root][INFO] - Training Epoch: 1/2, step 1884/7134 completed (loss: 0.45039016008377075, acc: 0.8918918967247009)
[2025-02-13 19:07:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:53][root][INFO] - Training Epoch: 1/2, step 1885/7134 completed (loss: 0.6679260730743408, acc: 0.8521126508712769)
[2025-02-13 19:07:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:53][root][INFO] - Training Epoch: 1/2, step 1886/7134 completed (loss: 0.6786229610443115, acc: 0.8650793433189392)
[2025-02-13 19:07:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:54][root][INFO] - Training Epoch: 1/2, step 1887/7134 completed (loss: 0.4042520225048065, acc: 0.9271523356437683)
[2025-02-13 19:07:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:54][root][INFO] - Training Epoch: 1/2, step 1888/7134 completed (loss: 0.378078430891037, acc: 0.9145299196243286)
[2025-02-13 19:07:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:54][root][INFO] - Training Epoch: 1/2, step 1889/7134 completed (loss: 0.3861874043941498, acc: 0.9356725215911865)
[2025-02-13 19:07:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:55][root][INFO] - Training Epoch: 1/2, step 1890/7134 completed (loss: 0.28472772240638733, acc: 0.9271523356437683)
[2025-02-13 19:07:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:55][root][INFO] - Training Epoch: 1/2, step 1891/7134 completed (loss: 0.6320576071739197, acc: 0.854651153087616)
[2025-02-13 19:07:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:56][root][INFO] - Training Epoch: 1/2, step 1892/7134 completed (loss: 0.5468304753303528, acc: 0.8682634830474854)
[2025-02-13 19:07:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:56][root][INFO] - Training Epoch: 1/2, step 1893/7134 completed (loss: 0.4618176221847534, acc: 0.895061731338501)
[2025-02-13 19:07:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:56][root][INFO] - Training Epoch: 1/2, step 1894/7134 completed (loss: 0.4370245933532715, acc: 0.8918918967247009)
[2025-02-13 19:07:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:57][root][INFO] - Training Epoch: 1/2, step 1895/7134 completed (loss: 1.5415220260620117, acc: 0.7303370833396912)
[2025-02-13 19:07:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:57][root][INFO] - Training Epoch: 1/2, step 1896/7134 completed (loss: 0.3973788619041443, acc: 0.9090909361839294)
[2025-02-13 19:07:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:57][root][INFO] - Training Epoch: 1/2, step 1897/7134 completed (loss: 0.33656609058380127, acc: 0.9295774698257446)
[2025-02-13 19:07:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:58][root][INFO] - Training Epoch: 1/2, step 1898/7134 completed (loss: 0.4219161570072174, acc: 0.9034482836723328)
[2025-02-13 19:07:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:58][root][INFO] - Training Epoch: 1/2, step 1899/7134 completed (loss: 0.2778652310371399, acc: 0.9469696879386902)
[2025-02-13 19:07:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:59][root][INFO] - Training Epoch: 1/2, step 1900/7134 completed (loss: 0.1766253113746643, acc: 0.9779411554336548)
[2025-02-13 19:07:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:59][root][INFO] - Training Epoch: 1/2, step 1901/7134 completed (loss: 0.4256003797054291, acc: 0.9136690497398376)
[2025-02-13 19:07:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:07:59][root][INFO] - Training Epoch: 1/2, step 1902/7134 completed (loss: 0.24125608801841736, acc: 0.935251772403717)
[2025-02-13 19:08:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:00][root][INFO] - Training Epoch: 1/2, step 1903/7134 completed (loss: 0.2748047411441803, acc: 0.9607843160629272)
[2025-02-13 19:08:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:00][root][INFO] - Training Epoch: 1/2, step 1904/7134 completed (loss: 0.37409812211990356, acc: 0.9142857193946838)
[2025-02-13 19:08:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:01][root][INFO] - Training Epoch: 1/2, step 1905/7134 completed (loss: 0.3836750388145447, acc: 0.9078013896942139)
[2025-02-13 19:08:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:01][root][INFO] - Training Epoch: 1/2, step 1906/7134 completed (loss: 0.23046374320983887, acc: 0.9635036587715149)
[2025-02-13 19:08:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:01][root][INFO] - Training Epoch: 1/2, step 1907/7134 completed (loss: 0.34324249625205994, acc: 0.918367326259613)
[2025-02-13 19:08:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:02][root][INFO] - Training Epoch: 1/2, step 1908/7134 completed (loss: 0.32229384779930115, acc: 0.9252336621284485)
[2025-02-13 19:08:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:02][root][INFO] - Training Epoch: 1/2, step 1909/7134 completed (loss: 0.3688337802886963, acc: 0.8979591727256775)
[2025-02-13 19:08:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:02][root][INFO] - Training Epoch: 1/2, step 1910/7134 completed (loss: 0.20667900145053864, acc: 0.9581395387649536)
[2025-02-13 19:08:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:03][root][INFO] - Training Epoch: 1/2, step 1911/7134 completed (loss: 0.17484354972839355, acc: 0.9593908786773682)
[2025-02-13 19:08:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:03][root][INFO] - Training Epoch: 1/2, step 1912/7134 completed (loss: 0.07989310473203659, acc: 0.9878787994384766)
[2025-02-13 19:08:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:04][root][INFO] - Training Epoch: 1/2, step 1913/7134 completed (loss: 0.32638949155807495, acc: 0.9212598204612732)
[2025-02-13 19:08:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:04][root][INFO] - Training Epoch: 1/2, step 1914/7134 completed (loss: 0.33633920550346375, acc: 0.9180327653884888)
[2025-02-13 19:08:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:04][root][INFO] - Training Epoch: 1/2, step 1915/7134 completed (loss: 0.1528477817773819, acc: 0.9790209531784058)
[2025-02-13 19:08:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:05][root][INFO] - Training Epoch: 1/2, step 1916/7134 completed (loss: 0.2222236543893814, acc: 0.9450549483299255)
[2025-02-13 19:08:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:05][root][INFO] - Training Epoch: 1/2, step 1917/7134 completed (loss: 0.24323388934135437, acc: 0.9575757384300232)
[2025-02-13 19:08:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:05][root][INFO] - Training Epoch: 1/2, step 1918/7134 completed (loss: 0.4061111807823181, acc: 0.940397322177887)
[2025-02-13 19:08:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:06][root][INFO] - Training Epoch: 1/2, step 1919/7134 completed (loss: 0.13379356265068054, acc: 0.9634146094322205)
[2025-02-13 19:08:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:06][root][INFO] - Training Epoch: 1/2, step 1920/7134 completed (loss: 0.21193501353263855, acc: 0.9659863710403442)
[2025-02-13 19:08:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:07][root][INFO] - Training Epoch: 1/2, step 1921/7134 completed (loss: 0.21967342495918274, acc: 0.9707602262496948)
[2025-02-13 19:08:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:07][root][INFO] - Training Epoch: 1/2, step 1922/7134 completed (loss: 0.0977356806397438, acc: 0.9795082211494446)
[2025-02-13 19:08:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:07][root][INFO] - Training Epoch: 1/2, step 1923/7134 completed (loss: 0.24517829716205597, acc: 0.9230769276618958)
[2025-02-13 19:08:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:08][root][INFO] - Training Epoch: 1/2, step 1924/7134 completed (loss: 0.24044819176197052, acc: 0.9296875)
[2025-02-13 19:08:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:08][root][INFO] - Training Epoch: 1/2, step 1925/7134 completed (loss: 0.22664564847946167, acc: 0.9456067085266113)
[2025-02-13 19:08:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:09][root][INFO] - Training Epoch: 1/2, step 1926/7134 completed (loss: 0.12667210400104523, acc: 0.9732142686843872)
[2025-02-13 19:08:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:09][root][INFO] - Training Epoch: 1/2, step 1927/7134 completed (loss: 0.24769416451454163, acc: 0.9528301954269409)
[2025-02-13 19:08:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:09][root][INFO] - Training Epoch: 1/2, step 1928/7134 completed (loss: 0.29675570130348206, acc: 0.9166666865348816)
[2025-02-13 19:08:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:10][root][INFO] - Training Epoch: 1/2, step 1929/7134 completed (loss: 0.17297381162643433, acc: 0.9329608678817749)
[2025-02-13 19:08:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:10][root][INFO] - Training Epoch: 1/2, step 1930/7134 completed (loss: 0.48863157629966736, acc: 0.8792270421981812)
[2025-02-13 19:08:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:11][root][INFO] - Training Epoch: 1/2, step 1931/7134 completed (loss: 0.457658976316452, acc: 0.8842592835426331)
[2025-02-13 19:08:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:11][root][INFO] - Training Epoch: 1/2, step 1932/7134 completed (loss: 0.4219772517681122, acc: 0.9050279259681702)
[2025-02-13 19:08:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:11][root][INFO] - Training Epoch: 1/2, step 1933/7134 completed (loss: 0.5329580307006836, acc: 0.8693467378616333)
[2025-02-13 19:08:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:12][root][INFO] - Training Epoch: 1/2, step 1934/7134 completed (loss: 0.22537146508693695, acc: 0.9428571462631226)
[2025-02-13 19:08:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:12][root][INFO] - Training Epoch: 1/2, step 1935/7134 completed (loss: 0.6688053011894226, acc: 0.9146341681480408)
[2025-02-13 19:08:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:13][root][INFO] - Training Epoch: 1/2, step 1936/7134 completed (loss: 0.28302299976348877, acc: 0.9281045794487)
[2025-02-13 19:08:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:13][root][INFO] - Training Epoch: 1/2, step 1937/7134 completed (loss: 0.3227044641971588, acc: 0.9329268336296082)
[2025-02-13 19:08:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:13][root][INFO] - Training Epoch: 1/2, step 1938/7134 completed (loss: 0.6405254602432251, acc: 0.875)
[2025-02-13 19:08:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:14][root][INFO] - Training Epoch: 1/2, step 1939/7134 completed (loss: 0.30571311712265015, acc: 0.9224137663841248)
[2025-02-13 19:08:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:14][root][INFO] - Training Epoch: 1/2, step 1940/7134 completed (loss: 0.6133404970169067, acc: 0.832335352897644)
[2025-02-13 19:08:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:15][root][INFO] - Training Epoch: 1/2, step 1941/7134 completed (loss: 0.4810877740383148, acc: 0.8520709872245789)
[2025-02-13 19:08:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:15][root][INFO] - Training Epoch: 1/2, step 1942/7134 completed (loss: 0.3178282082080841, acc: 0.8888888955116272)
[2025-02-13 19:08:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:15][root][INFO] - Training Epoch: 1/2, step 1943/7134 completed (loss: 0.4905688166618347, acc: 0.8947368264198303)
[2025-02-13 19:08:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:16][root][INFO] - Training Epoch: 1/2, step 1944/7134 completed (loss: 0.30667707324028015, acc: 0.9180327653884888)
[2025-02-13 19:08:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:16][root][INFO] - Training Epoch: 1/2, step 1945/7134 completed (loss: 0.2981305718421936, acc: 0.9032257795333862)
[2025-02-13 19:08:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:16][root][INFO] - Training Epoch: 1/2, step 1946/7134 completed (loss: 0.31211328506469727, acc: 0.8999999761581421)
[2025-02-13 19:08:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:17][root][INFO] - Training Epoch: 1/2, step 1947/7134 completed (loss: 0.3308030366897583, acc: 0.9421965479850769)
[2025-02-13 19:08:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:17][root][INFO] - Training Epoch: 1/2, step 1948/7134 completed (loss: 0.25504767894744873, acc: 0.9384615421295166)
[2025-02-13 19:08:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:18][root][INFO] - Training Epoch: 1/2, step 1949/7134 completed (loss: 0.2479710578918457, acc: 0.9350649118423462)
[2025-02-13 19:08:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:18][root][INFO] - Training Epoch: 1/2, step 1950/7134 completed (loss: 0.22890087962150574, acc: 0.9637681245803833)
[2025-02-13 19:08:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:18][root][INFO] - Training Epoch: 1/2, step 1951/7134 completed (loss: 0.32352012395858765, acc: 0.902255654335022)
[2025-02-13 19:08:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:19][root][INFO] - Training Epoch: 1/2, step 1952/7134 completed (loss: 0.26206251978874207, acc: 0.9518072009086609)
[2025-02-13 19:08:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:19][root][INFO] - Training Epoch: 1/2, step 1953/7134 completed (loss: 0.21727627515792847, acc: 0.949367105960846)
[2025-02-13 19:08:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:20][root][INFO] - Training Epoch: 1/2, step 1954/7134 completed (loss: 0.10181983560323715, acc: 0.9803921580314636)
[2025-02-13 19:08:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:20][root][INFO] - Training Epoch: 1/2, step 1955/7134 completed (loss: 0.24423086643218994, acc: 0.9402984976768494)
[2025-02-13 19:08:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:20][root][INFO] - Training Epoch: 1/2, step 1956/7134 completed (loss: 0.1031055897474289, acc: 0.97826087474823)
[2025-02-13 19:08:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:21][root][INFO] - Training Epoch: 1/2, step 1957/7134 completed (loss: 0.2987094819545746, acc: 0.9172932505607605)
[2025-02-13 19:08:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:21][root][INFO] - Training Epoch: 1/2, step 1958/7134 completed (loss: 0.32859694957733154, acc: 0.9222221970558167)
[2025-02-13 19:08:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:22][root][INFO] - Training Epoch: 1/2, step 1959/7134 completed (loss: 0.2993927597999573, acc: 0.9285714030265808)
[2025-02-13 19:08:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:22][root][INFO] - Training Epoch: 1/2, step 1960/7134 completed (loss: 0.2472110092639923, acc: 0.949999988079071)
[2025-02-13 19:08:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:22][root][INFO] - Training Epoch: 1/2, step 1961/7134 completed (loss: 0.298920601606369, acc: 0.9113923907279968)
[2025-02-13 19:08:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:23][root][INFO] - Training Epoch: 1/2, step 1962/7134 completed (loss: 0.16694286465644836, acc: 0.9655172228813171)
[2025-02-13 19:08:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:23][root][INFO] - Training Epoch: 1/2, step 1963/7134 completed (loss: 0.4120507538318634, acc: 0.9072847962379456)
[2025-02-13 19:08:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:23][root][INFO] - Training Epoch: 1/2, step 1964/7134 completed (loss: 0.3950623869895935, acc: 0.9141414165496826)
[2025-02-13 19:08:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:24][root][INFO] - Training Epoch: 1/2, step 1965/7134 completed (loss: 0.3799934387207031, acc: 0.8932584524154663)
[2025-02-13 19:08:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:24][root][INFO] - Training Epoch: 1/2, step 1966/7134 completed (loss: 0.413698673248291, acc: 0.8980891704559326)
[2025-02-13 19:08:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:25][root][INFO] - Training Epoch: 1/2, step 1967/7134 completed (loss: 0.30469533801078796, acc: 0.9157894849777222)
[2025-02-13 19:08:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:25][root][INFO] - Training Epoch: 1/2, step 1968/7134 completed (loss: 0.31223997473716736, acc: 0.9095744490623474)
[2025-02-13 19:08:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:25][root][INFO] - Training Epoch: 1/2, step 1969/7134 completed (loss: 0.2971174418926239, acc: 0.9378530979156494)
[2025-02-13 19:08:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:26][root][INFO] - Training Epoch: 1/2, step 1970/7134 completed (loss: 0.25651785731315613, acc: 0.9408283829689026)
[2025-02-13 19:08:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:26][root][INFO] - Training Epoch: 1/2, step 1971/7134 completed (loss: 0.5058587193489075, acc: 0.9162303805351257)
[2025-02-13 19:08:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:27][root][INFO] - Training Epoch: 1/2, step 1972/7134 completed (loss: 0.8984569907188416, acc: 0.8157894611358643)
[2025-02-13 19:08:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:27][root][INFO] - Training Epoch: 1/2, step 1973/7134 completed (loss: 1.0583653450012207, acc: 0.7986111044883728)
[2025-02-13 19:08:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:27][root][INFO] - Training Epoch: 1/2, step 1974/7134 completed (loss: 0.28180307149887085, acc: 0.9278350472450256)
[2025-02-13 19:08:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:28][root][INFO] - Training Epoch: 1/2, step 1975/7134 completed (loss: 0.18900756537914276, acc: 0.9437500238418579)
[2025-02-13 19:08:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:28][root][INFO] - Training Epoch: 1/2, step 1976/7134 completed (loss: 0.39556172490119934, acc: 0.9134615659713745)
[2025-02-13 19:08:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:28][root][INFO] - Training Epoch: 1/2, step 1977/7134 completed (loss: 0.3984951674938202, acc: 0.9281437397003174)
[2025-02-13 19:08:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:29][root][INFO] - Training Epoch: 1/2, step 1978/7134 completed (loss: 0.11174330860376358, acc: 0.9883720874786377)
[2025-02-13 19:08:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:29][root][INFO] - Training Epoch: 1/2, step 1979/7134 completed (loss: 0.2540762424468994, acc: 0.9395604133605957)
[2025-02-13 19:08:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:30][root][INFO] - Training Epoch: 1/2, step 1980/7134 completed (loss: 0.20902583003044128, acc: 0.963350772857666)
[2025-02-13 19:08:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:30][root][INFO] - Training Epoch: 1/2, step 1981/7134 completed (loss: 0.2347182184457779, acc: 0.9560439586639404)
[2025-02-13 19:08:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:31][root][INFO] - Training Epoch: 1/2, step 1982/7134 completed (loss: 0.18678145110607147, acc: 0.964102566242218)
[2025-02-13 19:08:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:31][root][INFO] - Training Epoch: 1/2, step 1983/7134 completed (loss: 0.15923984348773956, acc: 0.9784946441650391)
[2025-02-13 19:08:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:31][root][INFO] - Training Epoch: 1/2, step 1984/7134 completed (loss: 0.2081737369298935, acc: 0.9534883499145508)
[2025-02-13 19:08:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:32][root][INFO] - Training Epoch: 1/2, step 1985/7134 completed (loss: 0.2959353029727936, acc: 0.9254658222198486)
[2025-02-13 19:08:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:32][root][INFO] - Training Epoch: 1/2, step 1986/7134 completed (loss: 0.11992798000574112, acc: 0.9720670580863953)
[2025-02-13 19:08:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:33][root][INFO] - Training Epoch: 1/2, step 1987/7134 completed (loss: 0.24459759891033173, acc: 0.949438214302063)
[2025-02-13 19:08:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:33][root][INFO] - Training Epoch: 1/2, step 1988/7134 completed (loss: 0.24001295864582062, acc: 0.9454545378684998)
[2025-02-13 19:08:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:33][root][INFO] - Training Epoch: 1/2, step 1989/7134 completed (loss: 0.2626342177391052, acc: 0.9109588861465454)
[2025-02-13 19:08:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:34][root][INFO] - Training Epoch: 1/2, step 1990/7134 completed (loss: 0.27206650376319885, acc: 0.9447513818740845)
[2025-02-13 19:08:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:34][root][INFO] - Training Epoch: 1/2, step 1991/7134 completed (loss: 0.1336255520582199, acc: 0.9596773982048035)
[2025-02-13 19:08:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:35][root][INFO] - Training Epoch: 1/2, step 1992/7134 completed (loss: 0.5065357685089111, acc: 0.8768116235733032)
[2025-02-13 19:08:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:35][root][INFO] - Training Epoch: 1/2, step 1993/7134 completed (loss: 0.388788640499115, acc: 0.9304347634315491)
[2025-02-13 19:08:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:35][root][INFO] - Training Epoch: 1/2, step 1994/7134 completed (loss: 0.20631763339042664, acc: 0.9534883499145508)
[2025-02-13 19:08:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:36][root][INFO] - Training Epoch: 1/2, step 1995/7134 completed (loss: 0.17247340083122253, acc: 0.9448275566101074)
[2025-02-13 19:08:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:36][root][INFO] - Training Epoch: 1/2, step 1996/7134 completed (loss: 0.14494331181049347, acc: 0.9781022071838379)
[2025-02-13 19:08:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:36][root][INFO] - Training Epoch: 1/2, step 1997/7134 completed (loss: 0.19524069130420685, acc: 0.949999988079071)
[2025-02-13 19:08:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:37][root][INFO] - Training Epoch: 1/2, step 1998/7134 completed (loss: 0.21526190638542175, acc: 0.9507042169570923)
[2025-02-13 19:08:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:37][root][INFO] - Training Epoch: 1/2, step 1999/7134 completed (loss: 0.3269716203212738, acc: 0.8951048851013184)
[2025-02-13 19:08:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:38][root][INFO] - Training Epoch: 1/2, step 2000/7134 completed (loss: 0.21179504692554474, acc: 0.9583333134651184)
[2025-02-13 19:08:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:38][root][INFO] - Training Epoch: 1/2, step 2001/7134 completed (loss: 0.1285412758588791, acc: 0.9724137783050537)
[2025-02-13 19:08:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:38][root][INFO] - Training Epoch: 1/2, step 2002/7134 completed (loss: 0.31137827038764954, acc: 0.9398496150970459)
[2025-02-13 19:08:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:39][root][INFO] - Training Epoch: 1/2, step 2003/7134 completed (loss: 0.26133453845977783, acc: 0.9561403393745422)
[2025-02-13 19:08:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:39][root][INFO] - Training Epoch: 1/2, step 2004/7134 completed (loss: 1.075817584991455, acc: 0.8278688788414001)
[2025-02-13 19:08:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:40][root][INFO] - Training Epoch: 1/2, step 2005/7134 completed (loss: 0.646508514881134, acc: 0.8979591727256775)
[2025-02-13 19:08:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:40][root][INFO] - Training Epoch: 1/2, step 2006/7134 completed (loss: 0.17897410690784454, acc: 0.9418604373931885)
[2025-02-13 19:08:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:40][root][INFO] - Training Epoch: 1/2, step 2007/7134 completed (loss: 0.18999138474464417, acc: 0.9495798349380493)
[2025-02-13 19:08:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:41][root][INFO] - Training Epoch: 1/2, step 2008/7134 completed (loss: 0.28565290570259094, acc: 0.9041095972061157)
[2025-02-13 19:08:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:41][root][INFO] - Training Epoch: 1/2, step 2009/7134 completed (loss: 0.17171336710453033, acc: 0.9629629850387573)
[2025-02-13 19:08:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:42][root][INFO] - Training Epoch: 1/2, step 2010/7134 completed (loss: 0.1665440946817398, acc: 0.956204354763031)
[2025-02-13 19:08:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:42][root][INFO] - Training Epoch: 1/2, step 2011/7134 completed (loss: 0.1388806700706482, acc: 0.9603174328804016)
[2025-02-13 19:08:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:43][root][INFO] - Training Epoch: 1/2, step 2012/7134 completed (loss: 0.17491233348846436, acc: 0.9830508232116699)
[2025-02-13 19:08:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:43][root][INFO] - Training Epoch: 1/2, step 2013/7134 completed (loss: 0.12737545371055603, acc: 0.9836065769195557)
[2025-02-13 19:08:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:43][root][INFO] - Training Epoch: 1/2, step 2014/7134 completed (loss: 0.1555360108613968, acc: 0.9567901492118835)
[2025-02-13 19:08:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:44][root][INFO] - Training Epoch: 1/2, step 2015/7134 completed (loss: 0.08337031304836273, acc: 0.981249988079071)
[2025-02-13 19:08:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:44][root][INFO] - Training Epoch: 1/2, step 2016/7134 completed (loss: 0.5713569521903992, acc: 0.884353756904602)
[2025-02-13 19:08:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:44][root][INFO] - Training Epoch: 1/2, step 2017/7134 completed (loss: 0.4724191427230835, acc: 0.887417197227478)
[2025-02-13 19:08:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:45][root][INFO] - Training Epoch: 1/2, step 2018/7134 completed (loss: 0.07534363865852356, acc: 0.9863013625144958)
[2025-02-13 19:08:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:45][root][INFO] - Training Epoch: 1/2, step 2019/7134 completed (loss: 0.1392608880996704, acc: 0.9735099077224731)
[2025-02-13 19:08:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:46][root][INFO] - Training Epoch: 1/2, step 2020/7134 completed (loss: 0.2297227382659912, acc: 0.9504132270812988)
[2025-02-13 19:08:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:46][root][INFO] - Training Epoch: 1/2, step 2021/7134 completed (loss: 0.355000764131546, acc: 0.908108115196228)
[2025-02-13 19:08:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:46][root][INFO] - Training Epoch: 1/2, step 2022/7134 completed (loss: 0.27624374628067017, acc: 0.9253731369972229)
[2025-02-13 19:08:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:47][root][INFO] - Training Epoch: 1/2, step 2023/7134 completed (loss: 0.18652582168579102, acc: 0.9509202241897583)
[2025-02-13 19:08:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:47][root][INFO] - Training Epoch: 1/2, step 2024/7134 completed (loss: 0.21680070459842682, acc: 0.9418604373931885)
[2025-02-13 19:08:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:48][root][INFO] - Training Epoch: 1/2, step 2025/7134 completed (loss: 0.28301891684532166, acc: 0.9590643048286438)
[2025-02-13 19:08:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:48][root][INFO] - Training Epoch: 1/2, step 2026/7134 completed (loss: 0.22578676044940948, acc: 0.9415204524993896)
[2025-02-13 19:08:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:48][root][INFO] - Training Epoch: 1/2, step 2027/7134 completed (loss: 0.22377952933311462, acc: 0.9473684430122375)
[2025-02-13 19:08:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:49][root][INFO] - Training Epoch: 1/2, step 2028/7134 completed (loss: 0.25411590933799744, acc: 0.9386503100395203)
[2025-02-13 19:08:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:49][root][INFO] - Training Epoch: 1/2, step 2029/7134 completed (loss: 0.32167261838912964, acc: 0.9352940917015076)
[2025-02-13 19:08:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:49][root][INFO] - Training Epoch: 1/2, step 2030/7134 completed (loss: 0.3658776879310608, acc: 0.9036144614219666)
[2025-02-13 19:08:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:50][root][INFO] - Training Epoch: 1/2, step 2031/7134 completed (loss: 0.2265302538871765, acc: 0.9558011293411255)
[2025-02-13 19:08:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:50][root][INFO] - Training Epoch: 1/2, step 2032/7134 completed (loss: 0.4006440341472626, acc: 0.9005848169326782)
[2025-02-13 19:08:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:51][root][INFO] - Training Epoch: 1/2, step 2033/7134 completed (loss: 0.09506049752235413, acc: 0.9923664331436157)
[2025-02-13 19:08:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:51][root][INFO] - Training Epoch: 1/2, step 2034/7134 completed (loss: 0.41748765110969543, acc: 0.8926174640655518)
[2025-02-13 19:08:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:51][root][INFO] - Training Epoch: 1/2, step 2035/7134 completed (loss: 0.1932159960269928, acc: 0.9516128897666931)
[2025-02-13 19:08:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:52][root][INFO] - Training Epoch: 1/2, step 2036/7134 completed (loss: 0.1961802840232849, acc: 0.9389312863349915)
[2025-02-13 19:08:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:52][root][INFO] - Training Epoch: 1/2, step 2037/7134 completed (loss: 0.18291230499744415, acc: 0.9509202241897583)
[2025-02-13 19:08:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:53][root][INFO] - Training Epoch: 1/2, step 2038/7134 completed (loss: 0.2103436291217804, acc: 0.9505494236946106)
[2025-02-13 19:08:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:53][root][INFO] - Training Epoch: 1/2, step 2039/7134 completed (loss: 0.17170721292495728, acc: 0.9489051103591919)
[2025-02-13 19:08:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:53][root][INFO] - Training Epoch: 1/2, step 2040/7134 completed (loss: 0.2652395963668823, acc: 0.9396551847457886)
[2025-02-13 19:08:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:54][root][INFO] - Training Epoch: 1/2, step 2041/7134 completed (loss: 0.1835848093032837, acc: 0.9636363387107849)
[2025-02-13 19:08:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:54][root][INFO] - Training Epoch: 1/2, step 2042/7134 completed (loss: 0.32132387161254883, acc: 0.9406779408454895)
[2025-02-13 19:08:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:54][root][INFO] - Training Epoch: 1/2, step 2043/7134 completed (loss: 0.09474875777959824, acc: 0.9798657894134521)
[2025-02-13 19:08:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:55][root][INFO] - Training Epoch: 1/2, step 2044/7134 completed (loss: 0.12951232492923737, acc: 0.9679999947547913)
[2025-02-13 19:08:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:55][root][INFO] - Training Epoch: 1/2, step 2045/7134 completed (loss: 0.313724547624588, acc: 0.925000011920929)
[2025-02-13 19:08:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:56][root][INFO] - Training Epoch: 1/2, step 2046/7134 completed (loss: 0.5586029887199402, acc: 0.8991596698760986)
[2025-02-13 19:08:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:56][root][INFO] - Training Epoch: 1/2, step 2047/7134 completed (loss: 0.4359425902366638, acc: 0.8965517282485962)
[2025-02-13 19:08:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:56][root][INFO] - Training Epoch: 1/2, step 2048/7134 completed (loss: 0.5085851550102234, acc: 0.8928571343421936)
[2025-02-13 19:08:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:57][root][INFO] - Training Epoch: 1/2, step 2049/7134 completed (loss: 0.4505551755428314, acc: 0.9272727370262146)
[2025-02-13 19:08:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:57][root][INFO] - Training Epoch: 1/2, step 2050/7134 completed (loss: 0.30595290660858154, acc: 0.931506872177124)
[2025-02-13 19:08:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:58][root][INFO] - Training Epoch: 1/2, step 2051/7134 completed (loss: 0.32472148537635803, acc: 0.9115044474601746)
[2025-02-13 19:08:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:58][root][INFO] - Training Epoch: 1/2, step 2052/7134 completed (loss: 0.4817703068256378, acc: 0.9027777910232544)
[2025-02-13 19:08:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:58][root][INFO] - Training Epoch: 1/2, step 2053/7134 completed (loss: 0.5466612577438354, acc: 0.8931297659873962)
[2025-02-13 19:08:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:59][root][INFO] - Training Epoch: 1/2, step 2054/7134 completed (loss: 0.5054937601089478, acc: 0.8910256624221802)
[2025-02-13 19:08:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:59][root][INFO] - Training Epoch: 1/2, step 2055/7134 completed (loss: 0.39941078424453735, acc: 0.8630136847496033)
[2025-02-13 19:08:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:08:59][root][INFO] - Training Epoch: 1/2, step 2056/7134 completed (loss: 0.5222744941711426, acc: 0.887005627155304)
[2025-02-13 19:09:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:00][root][INFO] - Training Epoch: 1/2, step 2057/7134 completed (loss: 0.4525608420372009, acc: 0.8764045238494873)
[2025-02-13 19:09:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:00][root][INFO] - Training Epoch: 1/2, step 2058/7134 completed (loss: 0.6428713798522949, acc: 0.849711000919342)
[2025-02-13 19:09:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:01][root][INFO] - Training Epoch: 1/2, step 2059/7134 completed (loss: 0.7439424395561218, acc: 0.8226950168609619)
[2025-02-13 19:09:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:01][root][INFO] - Training Epoch: 1/2, step 2060/7134 completed (loss: 0.3475892245769501, acc: 0.9108280539512634)
[2025-02-13 19:09:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:01][root][INFO] - Training Epoch: 1/2, step 2061/7134 completed (loss: 0.38394027948379517, acc: 0.8928571343421936)
[2025-02-13 19:09:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:02][root][INFO] - Training Epoch: 1/2, step 2062/7134 completed (loss: 0.6515777707099915, acc: 0.8671875)
[2025-02-13 19:09:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:02][root][INFO] - Training Epoch: 1/2, step 2063/7134 completed (loss: 0.2529689371585846, acc: 0.9219858050346375)
[2025-02-13 19:09:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:03][root][INFO] - Training Epoch: 1/2, step 2064/7134 completed (loss: 0.20274978876113892, acc: 0.9558823704719543)
[2025-02-13 19:09:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:03][root][INFO] - Training Epoch: 1/2, step 2065/7134 completed (loss: 0.24252693355083466, acc: 0.9428571462631226)
[2025-02-13 19:09:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:03][root][INFO] - Training Epoch: 1/2, step 2066/7134 completed (loss: 0.19496317207813263, acc: 0.9674796462059021)
[2025-02-13 19:09:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:04][root][INFO] - Training Epoch: 1/2, step 2067/7134 completed (loss: 0.3369044065475464, acc: 0.8999999761581421)
[2025-02-13 19:09:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:04][root][INFO] - Training Epoch: 1/2, step 2068/7134 completed (loss: 0.2825172543525696, acc: 0.9313725233078003)
[2025-02-13 19:09:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:05][root][INFO] - Training Epoch: 1/2, step 2069/7134 completed (loss: 0.23178353905677795, acc: 0.9395973086357117)
[2025-02-13 19:09:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:05][root][INFO] - Training Epoch: 1/2, step 2070/7134 completed (loss: 0.18229839205741882, acc: 0.9482758641242981)
[2025-02-13 19:09:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:05][root][INFO] - Training Epoch: 1/2, step 2071/7134 completed (loss: 0.11877504736185074, acc: 0.9611650705337524)
[2025-02-13 19:09:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:06][root][INFO] - Training Epoch: 1/2, step 2072/7134 completed (loss: 0.185557022690773, acc: 0.9607843160629272)
[2025-02-13 19:09:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:06][root][INFO] - Training Epoch: 1/2, step 2073/7134 completed (loss: 0.187571182847023, acc: 0.9669421315193176)
[2025-02-13 19:09:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:06][root][INFO] - Training Epoch: 1/2, step 2074/7134 completed (loss: 0.3081711232662201, acc: 0.9476439952850342)
[2025-02-13 19:09:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:07][root][INFO] - Training Epoch: 1/2, step 2075/7134 completed (loss: 0.4006509482860565, acc: 0.9056603908538818)
[2025-02-13 19:09:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:07][root][INFO] - Training Epoch: 1/2, step 2076/7134 completed (loss: 0.2909506559371948, acc: 0.9387755393981934)
[2025-02-13 19:09:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:08][root][INFO] - Training Epoch: 1/2, step 2077/7134 completed (loss: 0.3593880236148834, acc: 0.9265536665916443)
[2025-02-13 19:09:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:08][root][INFO] - Training Epoch: 1/2, step 2078/7134 completed (loss: 0.2254360020160675, acc: 0.9593908786773682)
[2025-02-13 19:09:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:08][root][INFO] - Training Epoch: 1/2, step 2079/7134 completed (loss: 0.1505679190158844, acc: 0.9638554453849792)
[2025-02-13 19:09:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:09][root][INFO] - Training Epoch: 1/2, step 2080/7134 completed (loss: 0.25354164838790894, acc: 0.9477124214172363)
[2025-02-13 19:09:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:09][root][INFO] - Training Epoch: 1/2, step 2081/7134 completed (loss: 0.25707823038101196, acc: 0.9426751732826233)
[2025-02-13 19:09:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:09][root][INFO] - Training Epoch: 1/2, step 2082/7134 completed (loss: 0.45593932271003723, acc: 0.8728813529014587)
[2025-02-13 19:09:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:10][root][INFO] - Training Epoch: 1/2, step 2083/7134 completed (loss: 0.18213900923728943, acc: 0.961240291595459)
[2025-02-13 19:09:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:10][root][INFO] - Training Epoch: 1/2, step 2084/7134 completed (loss: 0.25471577048301697, acc: 0.9333333373069763)
[2025-02-13 19:09:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:11][root][INFO] - Training Epoch: 1/2, step 2085/7134 completed (loss: 0.1458059698343277, acc: 0.9674796462059021)
[2025-02-13 19:09:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:11][root][INFO] - Training Epoch: 1/2, step 2086/7134 completed (loss: 0.1692269891500473, acc: 0.970588207244873)
[2025-02-13 19:09:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:11][root][INFO] - Training Epoch: 1/2, step 2087/7134 completed (loss: 0.08491753041744232, acc: 0.9940476417541504)
[2025-02-13 19:09:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:12][root][INFO] - Training Epoch: 1/2, step 2088/7134 completed (loss: 0.4280956983566284, acc: 0.9408283829689026)
[2025-02-13 19:09:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:12][root][INFO] - Training Epoch: 1/2, step 2089/7134 completed (loss: 0.42791861295700073, acc: 0.8690476417541504)
[2025-02-13 19:09:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:13][root][INFO] - Training Epoch: 1/2, step 2090/7134 completed (loss: 0.14741772413253784, acc: 0.9542483687400818)
[2025-02-13 19:09:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:13][root][INFO] - Training Epoch: 1/2, step 2091/7134 completed (loss: 0.11426166445016861, acc: 0.9724137783050537)
[2025-02-13 19:09:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:13][root][INFO] - Training Epoch: 1/2, step 2092/7134 completed (loss: 0.10259336978197098, acc: 0.9735099077224731)
[2025-02-13 19:09:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:14][root][INFO] - Training Epoch: 1/2, step 2093/7134 completed (loss: 0.25648069381713867, acc: 0.9473684430122375)
[2025-02-13 19:09:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:14][root][INFO] - Training Epoch: 1/2, step 2094/7134 completed (loss: 0.357425719499588, acc: 0.9179104566574097)
[2025-02-13 19:09:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:15][root][INFO] - Training Epoch: 1/2, step 2095/7134 completed (loss: 0.3002873659133911, acc: 0.9154228568077087)
[2025-02-13 19:09:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:15][root][INFO] - Training Epoch: 1/2, step 2096/7134 completed (loss: 0.2884903848171234, acc: 0.9370629191398621)
[2025-02-13 19:09:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:15][root][INFO] - Training Epoch: 1/2, step 2097/7134 completed (loss: 0.13028870522975922, acc: 0.9638554453849792)
[2025-02-13 19:09:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:16][root][INFO] - Training Epoch: 1/2, step 2098/7134 completed (loss: 0.21047630906105042, acc: 0.9539473652839661)
[2025-02-13 19:09:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:16][root][INFO] - Training Epoch: 1/2, step 2099/7134 completed (loss: 0.1981326788663864, acc: 0.9375)
[2025-02-13 19:09:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:16][root][INFO] - Training Epoch: 1/2, step 2100/7134 completed (loss: 0.2395821064710617, acc: 0.9314285516738892)
[2025-02-13 19:09:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:17][root][INFO] - Training Epoch: 1/2, step 2101/7134 completed (loss: 0.3418542146682739, acc: 0.9226190447807312)
[2025-02-13 19:09:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:17][root][INFO] - Training Epoch: 1/2, step 2102/7134 completed (loss: 0.11021382361650467, acc: 0.9567567706108093)
[2025-02-13 19:09:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:18][root][INFO] - Training Epoch: 1/2, step 2103/7134 completed (loss: 0.2521292567253113, acc: 0.9437500238418579)
[2025-02-13 19:09:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:18][root][INFO] - Training Epoch: 1/2, step 2104/7134 completed (loss: 0.12614776194095612, acc: 0.9714285731315613)
[2025-02-13 19:09:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:18][root][INFO] - Training Epoch: 1/2, step 2105/7134 completed (loss: 0.14793041348457336, acc: 0.9503546357154846)
[2025-02-13 19:09:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:19][root][INFO] - Training Epoch: 1/2, step 2106/7134 completed (loss: 0.18659238517284393, acc: 0.9605262875556946)
[2025-02-13 19:09:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:19][root][INFO] - Training Epoch: 1/2, step 2107/7134 completed (loss: 0.1701563447713852, acc: 0.950276255607605)
[2025-02-13 19:09:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:20][root][INFO] - Training Epoch: 1/2, step 2108/7134 completed (loss: 0.26969701051712036, acc: 0.9192546606063843)
[2025-02-13 19:09:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:20][root][INFO] - Training Epoch: 1/2, step 2109/7134 completed (loss: 0.35152217745780945, acc: 0.9057971239089966)
[2025-02-13 19:09:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:20][root][INFO] - Training Epoch: 1/2, step 2110/7134 completed (loss: 0.45772647857666016, acc: 0.8802395462989807)
[2025-02-13 19:09:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:21][root][INFO] - Training Epoch: 1/2, step 2111/7134 completed (loss: 0.4085136353969574, acc: 0.9430052042007446)
[2025-02-13 19:09:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:21][root][INFO] - Training Epoch: 1/2, step 2112/7134 completed (loss: 0.29784682393074036, acc: 0.9333333373069763)
[2025-02-13 19:09:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:22][root][INFO] - Training Epoch: 1/2, step 2113/7134 completed (loss: 0.2695758044719696, acc: 0.9399999976158142)
[2025-02-13 19:09:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:22][root][INFO] - Training Epoch: 1/2, step 2114/7134 completed (loss: 0.2898404598236084, acc: 0.9289940595626831)
[2025-02-13 19:09:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:22][root][INFO] - Training Epoch: 1/2, step 2115/7134 completed (loss: 0.3026575446128845, acc: 0.9195402264595032)
[2025-02-13 19:09:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:23][root][INFO] - Training Epoch: 1/2, step 2116/7134 completed (loss: 0.28731226921081543, acc: 0.9523809552192688)
[2025-02-13 19:09:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:23][root][INFO] - Training Epoch: 1/2, step 2117/7134 completed (loss: 0.31003856658935547, acc: 0.9265536665916443)
[2025-02-13 19:09:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:24][root][INFO] - Training Epoch: 1/2, step 2118/7134 completed (loss: 0.18758806586265564, acc: 0.9585798978805542)
[2025-02-13 19:09:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:24][root][INFO] - Training Epoch: 1/2, step 2119/7134 completed (loss: 0.27337753772735596, acc: 0.9421965479850769)
[2025-02-13 19:09:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:24][root][INFO] - Training Epoch: 1/2, step 2120/7134 completed (loss: 0.34840160608291626, acc: 0.9259259104728699)
[2025-02-13 19:09:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:25][root][INFO] - Training Epoch: 1/2, step 2121/7134 completed (loss: 0.33457037806510925, acc: 0.926174521446228)
[2025-02-13 19:09:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:25][root][INFO] - Training Epoch: 1/2, step 2122/7134 completed (loss: 0.25059381127357483, acc: 0.9548022747039795)
[2025-02-13 19:09:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:26][root][INFO] - Training Epoch: 1/2, step 2123/7134 completed (loss: 0.45790180563926697, acc: 0.8947368264198303)
[2025-02-13 19:09:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:26][root][INFO] - Training Epoch: 1/2, step 2124/7134 completed (loss: 0.2318902611732483, acc: 0.9324324131011963)
[2025-02-13 19:09:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:26][root][INFO] - Training Epoch: 1/2, step 2125/7134 completed (loss: 0.1964370608329773, acc: 0.9580838084220886)
[2025-02-13 19:09:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:27][root][INFO] - Training Epoch: 1/2, step 2126/7134 completed (loss: 0.31962400674819946, acc: 0.9078013896942139)
[2025-02-13 19:09:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:27][root][INFO] - Training Epoch: 1/2, step 2127/7134 completed (loss: 0.18026645481586456, acc: 0.9518072009086609)
[2025-02-13 19:09:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:27][root][INFO] - Training Epoch: 1/2, step 2128/7134 completed (loss: 0.26066601276397705, acc: 0.9555555582046509)
[2025-02-13 19:09:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:28][root][INFO] - Training Epoch: 1/2, step 2129/7134 completed (loss: 0.40299662947654724, acc: 0.9112426042556763)
[2025-02-13 19:09:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:28][root][INFO] - Training Epoch: 1/2, step 2130/7134 completed (loss: 0.394022673368454, acc: 0.8986486196517944)
[2025-02-13 19:09:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:29][root][INFO] - Training Epoch: 1/2, step 2131/7134 completed (loss: 0.14692814648151398, acc: 0.9631901979446411)
[2025-02-13 19:09:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:29][root][INFO] - Training Epoch: 1/2, step 2132/7134 completed (loss: 0.10766228288412094, acc: 0.9695122241973877)
[2025-02-13 19:09:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:29][root][INFO] - Training Epoch: 1/2, step 2133/7134 completed (loss: 0.21584993600845337, acc: 0.9473684430122375)
[2025-02-13 19:09:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:30][root][INFO] - Training Epoch: 1/2, step 2134/7134 completed (loss: 0.17567089200019836, acc: 0.959770143032074)
[2025-02-13 19:09:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:30][root][INFO] - Training Epoch: 1/2, step 2135/7134 completed (loss: 0.11093537509441376, acc: 0.9536423683166504)
[2025-02-13 19:09:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:30][root][INFO] - Training Epoch: 1/2, step 2136/7134 completed (loss: 0.1592637300491333, acc: 0.9670329689979553)
[2025-02-13 19:09:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:31][root][INFO] - Training Epoch: 1/2, step 2137/7134 completed (loss: 0.2166016697883606, acc: 0.9468085169792175)
[2025-02-13 19:09:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:31][root][INFO] - Training Epoch: 1/2, step 2138/7134 completed (loss: 0.09137599915266037, acc: 0.9820359349250793)
[2025-02-13 19:09:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:32][root][INFO] - Training Epoch: 1/2, step 2139/7134 completed (loss: 0.08896687626838684, acc: 0.9767441749572754)
[2025-02-13 19:09:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:32][root][INFO] - Training Epoch: 1/2, step 2140/7134 completed (loss: 0.533682107925415, acc: 0.8983050584793091)
[2025-02-13 19:09:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:32][root][INFO] - Training Epoch: 1/2, step 2141/7134 completed (loss: 0.5263939499855042, acc: 0.8936170339584351)
[2025-02-13 19:09:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:33][root][INFO] - Training Epoch: 1/2, step 2142/7134 completed (loss: 0.368465781211853, acc: 0.9265536665916443)
[2025-02-13 19:09:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:33][root][INFO] - Training Epoch: 1/2, step 2143/7134 completed (loss: 0.16508808732032776, acc: 0.9655172228813171)
[2025-02-13 19:09:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:34][root][INFO] - Training Epoch: 1/2, step 2144/7134 completed (loss: 0.1488790363073349, acc: 0.9659090638160706)
[2025-02-13 19:09:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:34][root][INFO] - Training Epoch: 1/2, step 2145/7134 completed (loss: 0.23395220935344696, acc: 0.9418604373931885)
[2025-02-13 19:09:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:34][root][INFO] - Training Epoch: 1/2, step 2146/7134 completed (loss: 0.09741363674402237, acc: 0.9729729890823364)
[2025-02-13 19:09:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:35][root][INFO] - Training Epoch: 1/2, step 2147/7134 completed (loss: 0.22761936485767365, acc: 0.9343434572219849)
[2025-02-13 19:09:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:35][root][INFO] - Training Epoch: 1/2, step 2148/7134 completed (loss: 0.16978321969509125, acc: 0.9545454382896423)
[2025-02-13 19:09:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:36][root][INFO] - Training Epoch: 1/2, step 2149/7134 completed (loss: 0.1321418732404709, acc: 0.9590643048286438)
[2025-02-13 19:09:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:36][root][INFO] - Training Epoch: 1/2, step 2150/7134 completed (loss: 0.20584821701049805, acc: 0.9548022747039795)
[2025-02-13 19:09:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:36][root][INFO] - Training Epoch: 1/2, step 2151/7134 completed (loss: 0.22422532737255096, acc: 0.9518072009086609)
[2025-02-13 19:09:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:37][root][INFO] - Training Epoch: 1/2, step 2152/7134 completed (loss: 0.2869061827659607, acc: 0.9419354796409607)
[2025-02-13 19:09:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:37][root][INFO] - Training Epoch: 1/2, step 2153/7134 completed (loss: 0.31023427844047546, acc: 0.9313725233078003)
[2025-02-13 19:09:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:38][root][INFO] - Training Epoch: 1/2, step 2154/7134 completed (loss: 0.2797102928161621, acc: 0.9367815852165222)
[2025-02-13 19:09:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:38][root][INFO] - Training Epoch: 1/2, step 2155/7134 completed (loss: 0.28622373938560486, acc: 0.9230769276618958)
[2025-02-13 19:09:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:38][root][INFO] - Training Epoch: 1/2, step 2156/7134 completed (loss: 0.1494000256061554, acc: 0.9632353186607361)
[2025-02-13 19:09:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:39][root][INFO] - Training Epoch: 1/2, step 2157/7134 completed (loss: 0.28627046942710876, acc: 0.9292929172515869)
[2025-02-13 19:09:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:39][root][INFO] - Training Epoch: 1/2, step 2158/7134 completed (loss: 0.27660396695137024, acc: 0.9432989954948425)
[2025-02-13 19:09:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:39][root][INFO] - Training Epoch: 1/2, step 2159/7134 completed (loss: 0.1067371517419815, acc: 0.976190447807312)
[2025-02-13 19:09:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:40][root][INFO] - Training Epoch: 1/2, step 2160/7134 completed (loss: 0.14878006279468536, acc: 0.9512194991111755)
[2025-02-13 19:09:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:40][root][INFO] - Training Epoch: 1/2, step 2161/7134 completed (loss: 0.2777242362499237, acc: 0.9333333373069763)
[2025-02-13 19:09:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:41][root][INFO] - Training Epoch: 1/2, step 2162/7134 completed (loss: 0.14443932473659515, acc: 0.9741935729980469)
[2025-02-13 19:09:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:41][root][INFO] - Training Epoch: 1/2, step 2163/7134 completed (loss: 0.1367802917957306, acc: 0.9599999785423279)
[2025-02-13 19:09:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:41][root][INFO] - Training Epoch: 1/2, step 2164/7134 completed (loss: 0.10097067058086395, acc: 0.9772727489471436)
[2025-02-13 19:09:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:42][root][INFO] - Training Epoch: 1/2, step 2165/7134 completed (loss: 0.2066473364830017, acc: 0.9634146094322205)
[2025-02-13 19:09:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:42][root][INFO] - Training Epoch: 1/2, step 2166/7134 completed (loss: 0.08904905617237091, acc: 0.9830508232116699)
[2025-02-13 19:09:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:43][root][INFO] - Training Epoch: 1/2, step 2167/7134 completed (loss: 0.24862556159496307, acc: 0.9708737730979919)
[2025-02-13 19:09:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:43][root][INFO] - Training Epoch: 1/2, step 2168/7134 completed (loss: 0.09015821665525436, acc: 0.9856114983558655)
[2025-02-13 19:09:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:43][root][INFO] - Training Epoch: 1/2, step 2169/7134 completed (loss: 0.19896137714385986, acc: 0.9597989916801453)
[2025-02-13 19:09:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:44][root][INFO] - Training Epoch: 1/2, step 2170/7134 completed (loss: 0.19148218631744385, acc: 0.9673202633857727)
[2025-02-13 19:09:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:44][root][INFO] - Training Epoch: 1/2, step 2171/7134 completed (loss: 0.31279832124710083, acc: 0.9459459185600281)
[2025-02-13 19:09:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:45][root][INFO] - Training Epoch: 1/2, step 2172/7134 completed (loss: 0.20598168671131134, acc: 0.9431279897689819)
[2025-02-13 19:09:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:45][root][INFO] - Training Epoch: 1/2, step 2173/7134 completed (loss: 0.2191932052373886, acc: 0.9589040875434875)
[2025-02-13 19:09:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:45][root][INFO] - Training Epoch: 1/2, step 2174/7134 completed (loss: 0.19771301746368408, acc: 0.9520958065986633)
[2025-02-13 19:09:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:46][root][INFO] - Training Epoch: 1/2, step 2175/7134 completed (loss: 0.20277994871139526, acc: 0.9454545378684998)
[2025-02-13 19:09:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:46][root][INFO] - Training Epoch: 1/2, step 2176/7134 completed (loss: 0.22742144763469696, acc: 0.945652186870575)
[2025-02-13 19:09:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:47][root][INFO] - Training Epoch: 1/2, step 2177/7134 completed (loss: 0.222869411110878, acc: 0.9398906826972961)
[2025-02-13 19:09:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:47][root][INFO] - Training Epoch: 1/2, step 2178/7134 completed (loss: 0.14189189672470093, acc: 0.96875)
[2025-02-13 19:09:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:47][root][INFO] - Training Epoch: 1/2, step 2179/7134 completed (loss: 0.1711438149213791, acc: 0.959770143032074)
[2025-02-13 19:09:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:48][root][INFO] - Training Epoch: 1/2, step 2180/7134 completed (loss: 0.2184067815542221, acc: 0.9505494236946106)
[2025-02-13 19:09:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:48][root][INFO] - Training Epoch: 1/2, step 2181/7134 completed (loss: 0.15708449482917786, acc: 0.9751552939414978)
[2025-02-13 19:09:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:48][root][INFO] - Training Epoch: 1/2, step 2182/7134 completed (loss: 0.33739060163497925, acc: 0.928205132484436)
[2025-02-13 19:09:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:49][root][INFO] - Training Epoch: 1/2, step 2183/7134 completed (loss: 0.22669503092765808, acc: 0.939226508140564)
[2025-02-13 19:09:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:49][root][INFO] - Training Epoch: 1/2, step 2184/7134 completed (loss: 0.17769302427768707, acc: 0.9436619877815247)
[2025-02-13 19:09:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:50][root][INFO] - Training Epoch: 1/2, step 2185/7134 completed (loss: 0.3973180651664734, acc: 0.9119496941566467)
[2025-02-13 19:09:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:50][root][INFO] - Training Epoch: 1/2, step 2186/7134 completed (loss: 0.47894832491874695, acc: 0.8866666555404663)
[2025-02-13 19:09:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:50][root][INFO] - Training Epoch: 1/2, step 2187/7134 completed (loss: 0.5055906176567078, acc: 0.8633093237876892)
[2025-02-13 19:09:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:51][root][INFO] - Training Epoch: 1/2, step 2188/7134 completed (loss: 0.1862538903951645, acc: 0.9515151381492615)
[2025-02-13 19:09:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:51][root][INFO] - Training Epoch: 1/2, step 2189/7134 completed (loss: 0.3684341311454773, acc: 0.8896104097366333)
[2025-02-13 19:09:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:52][root][INFO] - Training Epoch: 1/2, step 2190/7134 completed (loss: 0.3352316915988922, acc: 0.9273743033409119)
[2025-02-13 19:09:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:52][root][INFO] - Training Epoch: 1/2, step 2191/7134 completed (loss: 0.3022005558013916, acc: 0.9052631855010986)
[2025-02-13 19:09:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:52][root][INFO] - Training Epoch: 1/2, step 2192/7134 completed (loss: 0.15368974208831787, acc: 0.9555555582046509)
[2025-02-13 19:09:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:53][root][INFO] - Training Epoch: 1/2, step 2193/7134 completed (loss: 0.29550138115882874, acc: 0.9234972596168518)
[2025-02-13 19:09:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:53][root][INFO] - Training Epoch: 1/2, step 2194/7134 completed (loss: 0.30153754353523254, acc: 0.9210526347160339)
[2025-02-13 19:09:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:54][root][INFO] - Training Epoch: 1/2, step 2195/7134 completed (loss: 0.20118266344070435, acc: 0.9444444179534912)
[2025-02-13 19:09:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:54][root][INFO] - Training Epoch: 1/2, step 2196/7134 completed (loss: 0.19023504853248596, acc: 0.9505494236946106)
[2025-02-13 19:09:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:54][root][INFO] - Training Epoch: 1/2, step 2197/7134 completed (loss: 0.20448347926139832, acc: 0.9527559280395508)
[2025-02-13 19:09:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:55][root][INFO] - Training Epoch: 1/2, step 2198/7134 completed (loss: 0.1717807948589325, acc: 0.954023003578186)
[2025-02-13 19:09:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:55][root][INFO] - Training Epoch: 1/2, step 2199/7134 completed (loss: 0.08929242193698883, acc: 0.9810126423835754)
[2025-02-13 19:09:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:56][root][INFO] - Training Epoch: 1/2, step 2200/7134 completed (loss: 0.17987863719463348, acc: 0.9685039520263672)
[2025-02-13 19:09:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:56][root][INFO] - Training Epoch: 1/2, step 2201/7134 completed (loss: 0.15801754593849182, acc: 0.9729729890823364)
[2025-02-13 19:09:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:56][root][INFO] - Training Epoch: 1/2, step 2202/7134 completed (loss: 0.14740312099456787, acc: 0.970802903175354)
[2025-02-13 19:09:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:57][root][INFO] - Training Epoch: 1/2, step 2203/7134 completed (loss: 0.1305876523256302, acc: 0.976047933101654)
[2025-02-13 19:09:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:57][root][INFO] - Training Epoch: 1/2, step 2204/7134 completed (loss: 0.09793715924024582, acc: 0.982758641242981)
[2025-02-13 19:09:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:58][root][INFO] - Training Epoch: 1/2, step 2205/7134 completed (loss: 0.10360424220561981, acc: 0.9645389914512634)
[2025-02-13 19:09:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:58][root][INFO] - Training Epoch: 1/2, step 2206/7134 completed (loss: 0.11513461917638779, acc: 0.9731543660163879)
[2025-02-13 19:09:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:58][root][INFO] - Training Epoch: 1/2, step 2207/7134 completed (loss: 0.06839905679225922, acc: 0.9795918464660645)
[2025-02-13 19:09:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:59][root][INFO] - Training Epoch: 1/2, step 2208/7134 completed (loss: 0.14481660723686218, acc: 0.9741935729980469)
[2025-02-13 19:09:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:59][root][INFO] - Training Epoch: 1/2, step 2209/7134 completed (loss: 0.14756861329078674, acc: 0.9634146094322205)
[2025-02-13 19:09:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:09:59][root][INFO] - Training Epoch: 1/2, step 2210/7134 completed (loss: 0.21307040750980377, acc: 0.9451219439506531)
[2025-02-13 19:10:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:00][root][INFO] - Training Epoch: 1/2, step 2211/7134 completed (loss: 0.08107159286737442, acc: 0.9759036302566528)
[2025-02-13 19:10:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:00][root][INFO] - Training Epoch: 1/2, step 2212/7134 completed (loss: 0.08833321183919907, acc: 0.9828571677207947)
[2025-02-13 19:10:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:01][root][INFO] - Training Epoch: 1/2, step 2213/7134 completed (loss: 0.16106903553009033, acc: 0.9470587968826294)
[2025-02-13 19:10:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:01][root][INFO] - Training Epoch: 1/2, step 2214/7134 completed (loss: 0.20138370990753174, acc: 0.9459459185600281)
[2025-02-13 19:10:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:01][root][INFO] - Training Epoch: 1/2, step 2215/7134 completed (loss: 0.10843711346387863, acc: 0.9746835231781006)
[2025-02-13 19:10:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:02][root][INFO] - Training Epoch: 1/2, step 2216/7134 completed (loss: 0.1626444309949875, acc: 0.9766082167625427)
[2025-02-13 19:10:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:02][root][INFO] - Training Epoch: 1/2, step 2217/7134 completed (loss: 0.1026073694229126, acc: 0.9820359349250793)
[2025-02-13 19:10:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:02][root][INFO] - Training Epoch: 1/2, step 2218/7134 completed (loss: 0.19776083528995514, acc: 0.9629629850387573)
[2025-02-13 19:10:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:03][root][INFO] - Training Epoch: 1/2, step 2219/7134 completed (loss: 0.19993899762630463, acc: 0.9428571462631226)
[2025-02-13 19:10:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:03][root][INFO] - Training Epoch: 1/2, step 2220/7134 completed (loss: 0.14623582363128662, acc: 0.9777777791023254)
[2025-02-13 19:10:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:04][root][INFO] - Training Epoch: 1/2, step 2221/7134 completed (loss: 0.21857404708862305, acc: 0.9254658222198486)
[2025-02-13 19:10:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:04][root][INFO] - Training Epoch: 1/2, step 2222/7134 completed (loss: 0.3772369921207428, acc: 0.9178082346916199)
[2025-02-13 19:10:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:04][root][INFO] - Training Epoch: 1/2, step 2223/7134 completed (loss: 0.19224441051483154, acc: 0.9395973086357117)
[2025-02-13 19:10:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:05][root][INFO] - Training Epoch: 1/2, step 2224/7134 completed (loss: 0.3127579391002655, acc: 0.9240506291389465)
[2025-02-13 19:10:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:05][root][INFO] - Training Epoch: 1/2, step 2225/7134 completed (loss: 0.2574966847896576, acc: 0.9387755393981934)
[2025-02-13 19:10:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:05][root][INFO] - Training Epoch: 1/2, step 2226/7134 completed (loss: 0.155630424618721, acc: 0.9731543660163879)
[2025-02-13 19:10:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:06][root][INFO] - Training Epoch: 1/2, step 2227/7134 completed (loss: 0.1545613706111908, acc: 0.9675324559211731)
[2025-02-13 19:10:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:06][root][INFO] - Training Epoch: 1/2, step 2228/7134 completed (loss: 0.18625228106975555, acc: 0.9534883499145508)
[2025-02-13 19:10:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:07][root][INFO] - Training Epoch: 1/2, step 2229/7134 completed (loss: 0.12176638841629028, acc: 0.9708737730979919)
[2025-02-13 19:10:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:07][root][INFO] - Training Epoch: 1/2, step 2230/7134 completed (loss: 0.07722720503807068, acc: 0.9784172773361206)
[2025-02-13 19:10:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:07][root][INFO] - Training Epoch: 1/2, step 2231/7134 completed (loss: 0.30810683965682983, acc: 0.924369752407074)
[2025-02-13 19:10:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:08][root][INFO] - Training Epoch: 1/2, step 2232/7134 completed (loss: 0.20540814101696014, acc: 0.9367815852165222)
[2025-02-13 19:10:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:08][root][INFO] - Training Epoch: 1/2, step 2233/7134 completed (loss: 0.21859565377235413, acc: 0.9358974099159241)
[2025-02-13 19:10:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:08][root][INFO] - Training Epoch: 1/2, step 2234/7134 completed (loss: 0.08712726831436157, acc: 0.9740259647369385)
[2025-02-13 19:10:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:09][root][INFO] - Training Epoch: 1/2, step 2235/7134 completed (loss: 0.1960727721452713, acc: 0.9580419659614563)
[2025-02-13 19:10:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:09][root][INFO] - Training Epoch: 1/2, step 2236/7134 completed (loss: 0.16432133316993713, acc: 0.95333331823349)
[2025-02-13 19:10:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:10][root][INFO] - Training Epoch: 1/2, step 2237/7134 completed (loss: 0.21553702652454376, acc: 0.9466666579246521)
[2025-02-13 19:10:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:10][root][INFO] - Training Epoch: 1/2, step 2238/7134 completed (loss: 0.27334678173065186, acc: 0.9382715821266174)
[2025-02-13 19:10:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:11][root][INFO] - Training Epoch: 1/2, step 2239/7134 completed (loss: 0.4595275819301605, acc: 0.893048107624054)
[2025-02-13 19:10:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:11][root][INFO] - Training Epoch: 1/2, step 2240/7134 completed (loss: 0.35658228397369385, acc: 0.9034090638160706)
[2025-02-13 19:10:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:11][root][INFO] - Training Epoch: 1/2, step 2241/7134 completed (loss: 0.5620906949043274, acc: 0.84375)
[2025-02-13 19:10:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:12][root][INFO] - Training Epoch: 1/2, step 2242/7134 completed (loss: 0.6751599311828613, acc: 0.8611111044883728)
[2025-02-13 19:10:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:12][root][INFO] - Training Epoch: 1/2, step 2243/7134 completed (loss: 0.9010164141654968, acc: 0.800000011920929)
[2025-02-13 19:10:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:13][root][INFO] - Training Epoch: 1/2, step 2244/7134 completed (loss: 0.35913363099098206, acc: 0.9041095972061157)
[2025-02-13 19:10:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:13][root][INFO] - Training Epoch: 1/2, step 2245/7134 completed (loss: 0.4994817078113556, acc: 0.9084967374801636)
[2025-02-13 19:10:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:13][root][INFO] - Training Epoch: 1/2, step 2246/7134 completed (loss: 0.7917382717132568, acc: 0.8087431788444519)
[2025-02-13 19:10:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:14][root][INFO] - Training Epoch: 1/2, step 2247/7134 completed (loss: 0.6945493817329407, acc: 0.8381502628326416)
[2025-02-13 19:10:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:14][root][INFO] - Training Epoch: 1/2, step 2248/7134 completed (loss: 0.6602053046226501, acc: 0.8208092451095581)
[2025-02-13 19:10:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:14][root][INFO] - Training Epoch: 1/2, step 2249/7134 completed (loss: 0.48237863183021545, acc: 0.8839778900146484)
[2025-02-13 19:10:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:15][root][INFO] - Training Epoch: 1/2, step 2250/7134 completed (loss: 0.4064688980579376, acc: 0.8989361524581909)
[2025-02-13 19:10:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:15][root][INFO] - Training Epoch: 1/2, step 2251/7134 completed (loss: 0.2243424654006958, acc: 0.9508196711540222)
[2025-02-13 19:10:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:16][root][INFO] - Training Epoch: 1/2, step 2252/7134 completed (loss: 0.1289956122636795, acc: 0.97826087474823)
[2025-02-13 19:10:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:16][root][INFO] - Training Epoch: 1/2, step 2253/7134 completed (loss: 0.26322460174560547, acc: 0.9363057613372803)
[2025-02-13 19:10:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:16][root][INFO] - Training Epoch: 1/2, step 2254/7134 completed (loss: 0.5973037481307983, acc: 0.8709677457809448)
[2025-02-13 19:10:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:17][root][INFO] - Training Epoch: 1/2, step 2255/7134 completed (loss: 0.30264803767204285, acc: 0.9433962106704712)
[2025-02-13 19:10:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:17][root][INFO] - Training Epoch: 1/2, step 2256/7134 completed (loss: 0.5323882102966309, acc: 0.8617886304855347)
[2025-02-13 19:10:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:18][root][INFO] - Training Epoch: 1/2, step 2257/7134 completed (loss: 0.23493696749210358, acc: 0.9382715821266174)
[2025-02-13 19:10:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:18][root][INFO] - Training Epoch: 1/2, step 2258/7134 completed (loss: 0.25783273577690125, acc: 0.9379844665527344)
[2025-02-13 19:10:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:18][root][INFO] - Training Epoch: 1/2, step 2259/7134 completed (loss: 0.414733350276947, acc: 0.9059829115867615)
[2025-02-13 19:10:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:19][root][INFO] - Training Epoch: 1/2, step 2260/7134 completed (loss: 0.5232953429222107, acc: 0.8636363744735718)
[2025-02-13 19:10:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:19][root][INFO] - Training Epoch: 1/2, step 2261/7134 completed (loss: 0.8201151490211487, acc: 0.8086419701576233)
[2025-02-13 19:10:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:20][root][INFO] - Training Epoch: 1/2, step 2262/7134 completed (loss: 0.4095718562602997, acc: 0.902255654335022)
[2025-02-13 19:10:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:20][root][INFO] - Training Epoch: 1/2, step 2263/7134 completed (loss: 0.38857656717300415, acc: 0.902255654335022)
[2025-02-13 19:10:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:20][root][INFO] - Training Epoch: 1/2, step 2264/7134 completed (loss: 0.5094860792160034, acc: 0.8756756782531738)
[2025-02-13 19:10:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:21][root][INFO] - Training Epoch: 1/2, step 2265/7134 completed (loss: 0.5453317165374756, acc: 0.8831169009208679)
[2025-02-13 19:10:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:21][root][INFO] - Training Epoch: 1/2, step 2266/7134 completed (loss: 0.3436935544013977, acc: 0.9116021990776062)
[2025-02-13 19:10:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:21][root][INFO] - Training Epoch: 1/2, step 2267/7134 completed (loss: 0.24817518889904022, acc: 0.9186992049217224)
[2025-02-13 19:10:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:22][root][INFO] - Training Epoch: 1/2, step 2268/7134 completed (loss: 0.21414567530155182, acc: 0.9452054500579834)
[2025-02-13 19:10:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:22][root][INFO] - Training Epoch: 1/2, step 2269/7134 completed (loss: 0.20765984058380127, acc: 0.9329608678817749)
[2025-02-13 19:10:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:22][root][INFO] - Training Epoch: 1/2, step 2270/7134 completed (loss: 0.38641008734703064, acc: 0.921875)
[2025-02-13 19:10:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:23][root][INFO] - Training Epoch: 1/2, step 2271/7134 completed (loss: 0.47689270973205566, acc: 0.8594594597816467)
[2025-02-13 19:10:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:23][root][INFO] - Training Epoch: 1/2, step 2272/7134 completed (loss: 0.4409967362880707, acc: 0.8870967626571655)
[2025-02-13 19:10:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:24][root][INFO] - Training Epoch: 1/2, step 2273/7134 completed (loss: 0.14855456352233887, acc: 0.9444444179534912)
[2025-02-13 19:10:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:24][root][INFO] - Training Epoch: 1/2, step 2274/7134 completed (loss: 0.6682426929473877, acc: 0.8769230842590332)
[2025-02-13 19:10:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:25][root][INFO] - Training Epoch: 1/2, step 2275/7134 completed (loss: 0.28804299235343933, acc: 0.9313725233078003)
[2025-02-13 19:10:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:25][root][INFO] - Training Epoch: 1/2, step 2276/7134 completed (loss: 0.23743465542793274, acc: 0.9470899701118469)
[2025-02-13 19:10:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:25][root][INFO] - Training Epoch: 1/2, step 2277/7134 completed (loss: 0.3452012240886688, acc: 0.928205132484436)
[2025-02-13 19:10:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:26][root][INFO] - Training Epoch: 1/2, step 2278/7134 completed (loss: 0.5995148420333862, acc: 0.8838709592819214)
[2025-02-13 19:10:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:26][root][INFO] - Training Epoch: 1/2, step 2279/7134 completed (loss: 0.2255619466304779, acc: 0.9593908786773682)
[2025-02-13 19:10:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:26][root][INFO] - Training Epoch: 1/2, step 2280/7134 completed (loss: 0.3370887339115143, acc: 0.9266055226325989)
[2025-02-13 19:10:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:27][root][INFO] - Training Epoch: 1/2, step 2281/7134 completed (loss: 0.5737583637237549, acc: 0.8936170339584351)
[2025-02-13 19:10:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:27][root][INFO] - Training Epoch: 1/2, step 2282/7134 completed (loss: 0.12192114442586899, acc: 0.9578947424888611)
[2025-02-13 19:10:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:28][root][INFO] - Training Epoch: 1/2, step 2283/7134 completed (loss: 0.1947682797908783, acc: 0.9471153616905212)
[2025-02-13 19:10:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:28][root][INFO] - Training Epoch: 1/2, step 2284/7134 completed (loss: 0.2840796709060669, acc: 0.9548386931419373)
[2025-02-13 19:10:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:28][root][INFO] - Training Epoch: 1/2, step 2285/7134 completed (loss: 0.16787095367908478, acc: 0.9599999785423279)
[2025-02-13 19:10:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:29][root][INFO] - Training Epoch: 1/2, step 2286/7134 completed (loss: 0.43975701928138733, acc: 0.8908045887947083)
[2025-02-13 19:10:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:29][root][INFO] - Training Epoch: 1/2, step 2287/7134 completed (loss: 0.22662369906902313, acc: 0.9447852969169617)
[2025-02-13 19:10:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:30][root][INFO] - Training Epoch: 1/2, step 2288/7134 completed (loss: 0.1385408192873001, acc: 0.9615384340286255)
[2025-02-13 19:10:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:30][root][INFO] - Training Epoch: 1/2, step 2289/7134 completed (loss: 0.19995804131031036, acc: 0.9518716335296631)
[2025-02-13 19:10:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:30][root][INFO] - Training Epoch: 1/2, step 2290/7134 completed (loss: 0.24936001002788544, acc: 0.9354838728904724)
[2025-02-13 19:10:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:31][root][INFO] - Training Epoch: 1/2, step 2291/7134 completed (loss: 0.2906390428543091, acc: 0.9281768202781677)
[2025-02-13 19:10:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:31][root][INFO] - Training Epoch: 1/2, step 2292/7134 completed (loss: 0.39703601598739624, acc: 0.9191918969154358)
[2025-02-13 19:10:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:32][root][INFO] - Training Epoch: 1/2, step 2293/7134 completed (loss: 0.26352787017822266, acc: 0.9452054500579834)
[2025-02-13 19:10:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:32][root][INFO] - Training Epoch: 1/2, step 2294/7134 completed (loss: 0.21209296584129333, acc: 0.9425837397575378)
[2025-02-13 19:10:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:32][root][INFO] - Training Epoch: 1/2, step 2295/7134 completed (loss: 0.3558829128742218, acc: 0.9140625)
[2025-02-13 19:10:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:33][root][INFO] - Training Epoch: 1/2, step 2296/7134 completed (loss: 0.20425893366336823, acc: 0.9471153616905212)
[2025-02-13 19:10:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:33][root][INFO] - Training Epoch: 1/2, step 2297/7134 completed (loss: 0.22185437381267548, acc: 0.9575471878051758)
[2025-02-13 19:10:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:33][root][INFO] - Training Epoch: 1/2, step 2298/7134 completed (loss: 0.22780366241931915, acc: 0.9476190209388733)
[2025-02-13 19:10:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:34][root][INFO] - Training Epoch: 1/2, step 2299/7134 completed (loss: 0.25503861904144287, acc: 0.9408602118492126)
[2025-02-13 19:10:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:34][root][INFO] - Training Epoch: 1/2, step 2300/7134 completed (loss: 0.15587005019187927, acc: 0.9747474789619446)
[2025-02-13 19:10:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:35][root][INFO] - Training Epoch: 1/2, step 2301/7134 completed (loss: 0.3373722732067108, acc: 0.907608687877655)
[2025-02-13 19:10:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:35][root][INFO] - Training Epoch: 1/2, step 2302/7134 completed (loss: 0.31446027755737305, acc: 0.9128205180168152)
[2025-02-13 19:10:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:35][root][INFO] - Training Epoch: 1/2, step 2303/7134 completed (loss: 0.35283851623535156, acc: 0.8940092325210571)
[2025-02-13 19:10:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:36][root][INFO] - Training Epoch: 1/2, step 2304/7134 completed (loss: 0.44013792276382446, acc: 0.8898678421974182)
[2025-02-13 19:10:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:36][root][INFO] - Training Epoch: 1/2, step 2305/7134 completed (loss: 0.36109301447868347, acc: 0.9103773832321167)
[2025-02-13 19:10:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:37][root][INFO] - Training Epoch: 1/2, step 2306/7134 completed (loss: 0.15587227046489716, acc: 0.9488372206687927)
[2025-02-13 19:10:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:37][root][INFO] - Training Epoch: 1/2, step 2307/7134 completed (loss: 0.23685193061828613, acc: 0.9364407062530518)
[2025-02-13 19:10:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:37][root][INFO] - Training Epoch: 1/2, step 2308/7134 completed (loss: 0.24660609662532806, acc: 0.9427312612533569)
[2025-02-13 19:10:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:38][root][INFO] - Training Epoch: 1/2, step 2309/7134 completed (loss: 0.1086486354470253, acc: 0.9788359999656677)
[2025-02-13 19:10:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:38][root][INFO] - Training Epoch: 1/2, step 2310/7134 completed (loss: 0.1579793244600296, acc: 0.961904764175415)
[2025-02-13 19:10:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:39][root][INFO] - Training Epoch: 1/2, step 2311/7134 completed (loss: 0.15794378519058228, acc: 0.9556650519371033)
[2025-02-13 19:10:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:39][root][INFO] - Training Epoch: 1/2, step 2312/7134 completed (loss: 0.14492185413837433, acc: 0.9624413251876831)
[2025-02-13 19:10:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:39][root][INFO] - Training Epoch: 1/2, step 2313/7134 completed (loss: 0.26556283235549927, acc: 0.9278846383094788)
[2025-02-13 19:10:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:40][root][INFO] - Training Epoch: 1/2, step 2314/7134 completed (loss: 0.2033711075782776, acc: 0.9495412707328796)
[2025-02-13 19:10:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:40][root][INFO] - Training Epoch: 1/2, step 2315/7134 completed (loss: 0.22050407528877258, acc: 0.934883713722229)
[2025-02-13 19:10:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:41][root][INFO] - Training Epoch: 1/2, step 2316/7134 completed (loss: 0.12653569877147675, acc: 0.9682539701461792)
[2025-02-13 19:10:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:41][root][INFO] - Training Epoch: 1/2, step 2317/7134 completed (loss: 0.24423019587993622, acc: 0.9495798349380493)
[2025-02-13 19:10:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:41][root][INFO] - Training Epoch: 1/2, step 2318/7134 completed (loss: 0.10527960956096649, acc: 0.9737991094589233)
[2025-02-13 19:10:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:42][root][INFO] - Training Epoch: 1/2, step 2319/7134 completed (loss: 0.17667560279369354, acc: 0.9594594836235046)
[2025-02-13 19:10:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:42][root][INFO] - Training Epoch: 1/2, step 2320/7134 completed (loss: 0.11483363807201385, acc: 0.9692307710647583)
[2025-02-13 19:10:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:43][root][INFO] - Training Epoch: 1/2, step 2321/7134 completed (loss: 0.13571175932884216, acc: 0.9647058844566345)
[2025-02-13 19:10:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:43][root][INFO] - Training Epoch: 1/2, step 2322/7134 completed (loss: 0.29339542984962463, acc: 0.9202454090118408)
[2025-02-13 19:10:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:43][root][INFO] - Training Epoch: 1/2, step 2323/7134 completed (loss: 0.3496343493461609, acc: 0.8980891704559326)
[2025-02-13 19:10:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:44][root][INFO] - Training Epoch: 1/2, step 2324/7134 completed (loss: 0.43917593359947205, acc: 0.9518072009086609)
[2025-02-13 19:10:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:44][root][INFO] - Training Epoch: 1/2, step 2325/7134 completed (loss: 0.27036145329475403, acc: 0.9142857193946838)
[2025-02-13 19:10:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:45][root][INFO] - Training Epoch: 1/2, step 2326/7134 completed (loss: 0.17955432832241058, acc: 0.9538461565971375)
[2025-02-13 19:10:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:45][root][INFO] - Training Epoch: 1/2, step 2327/7134 completed (loss: 0.17784327268600464, acc: 0.9530201554298401)
[2025-02-13 19:10:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:46][root][INFO] - Training Epoch: 1/2, step 2328/7134 completed (loss: 0.28665387630462646, acc: 0.9328858852386475)
[2025-02-13 19:10:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:46][root][INFO] - Training Epoch: 1/2, step 2329/7134 completed (loss: 0.26788440346717834, acc: 0.9346405267715454)
[2025-02-13 19:10:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:46][root][INFO] - Training Epoch: 1/2, step 2330/7134 completed (loss: 0.08417001366615295, acc: 0.9757575988769531)
[2025-02-13 19:10:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:47][root][INFO] - Training Epoch: 1/2, step 2331/7134 completed (loss: 0.08510760217905045, acc: 0.9677419066429138)
[2025-02-13 19:10:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:47][root][INFO] - Training Epoch: 1/2, step 2332/7134 completed (loss: 0.11146204173564911, acc: 0.9757575988769531)
[2025-02-13 19:10:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:47][root][INFO] - Training Epoch: 1/2, step 2333/7134 completed (loss: 0.12740230560302734, acc: 0.9756097793579102)
[2025-02-13 19:10:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:48][root][INFO] - Training Epoch: 1/2, step 2334/7134 completed (loss: 0.2949816882610321, acc: 0.9554139971733093)
[2025-02-13 19:10:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:48][root][INFO] - Training Epoch: 1/2, step 2335/7134 completed (loss: 0.13663625717163086, acc: 0.9620253443717957)
[2025-02-13 19:10:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:49][root][INFO] - Training Epoch: 1/2, step 2336/7134 completed (loss: 0.09111552685499191, acc: 0.9798657894134521)
[2025-02-13 19:10:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:49][root][INFO] - Training Epoch: 1/2, step 2337/7134 completed (loss: 0.1007997989654541, acc: 0.9791666865348816)
[2025-02-13 19:10:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:49][root][INFO] - Training Epoch: 1/2, step 2338/7134 completed (loss: 0.12711530923843384, acc: 0.9870129823684692)
[2025-02-13 19:10:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:50][root][INFO] - Training Epoch: 1/2, step 2339/7134 completed (loss: 0.21541115641593933, acc: 0.9487179517745972)
[2025-02-13 19:10:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:50][root][INFO] - Training Epoch: 1/2, step 2340/7134 completed (loss: 0.09642181545495987, acc: 0.9740259647369385)
[2025-02-13 19:10:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:51][root][INFO] - Training Epoch: 1/2, step 2341/7134 completed (loss: 0.08609052002429962, acc: 0.9917355179786682)
[2025-02-13 19:10:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:51][root][INFO] - Training Epoch: 1/2, step 2342/7134 completed (loss: 0.0951765850186348, acc: 0.988095223903656)
[2025-02-13 19:10:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:51][root][INFO] - Training Epoch: 1/2, step 2343/7134 completed (loss: 0.11162048578262329, acc: 0.96875)
[2025-02-13 19:10:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:52][root][INFO] - Training Epoch: 1/2, step 2344/7134 completed (loss: 0.15497691929340363, acc: 0.9382715821266174)
[2025-02-13 19:10:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:52][root][INFO] - Training Epoch: 1/2, step 2345/7134 completed (loss: 0.17421044409275055, acc: 0.9425287246704102)
[2025-02-13 19:10:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:53][root][INFO] - Training Epoch: 1/2, step 2346/7134 completed (loss: 0.1254701465368271, acc: 0.9613259434700012)
[2025-02-13 19:10:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:53][root][INFO] - Training Epoch: 1/2, step 2347/7134 completed (loss: 0.1550481766462326, acc: 0.9577465057373047)
[2025-02-13 19:10:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:53][root][INFO] - Training Epoch: 1/2, step 2348/7134 completed (loss: 0.531029224395752, acc: 0.8711340427398682)
[2025-02-13 19:10:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:54][root][INFO] - Training Epoch: 1/2, step 2349/7134 completed (loss: 0.5292056202888489, acc: 0.893750011920929)
[2025-02-13 19:10:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:54][root][INFO] - Training Epoch: 1/2, step 2350/7134 completed (loss: 0.5421483516693115, acc: 0.905063271522522)
[2025-02-13 19:10:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:55][root][INFO] - Training Epoch: 1/2, step 2351/7134 completed (loss: 0.3902653753757477, acc: 0.9016393423080444)
[2025-02-13 19:10:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:55][root][INFO] - Training Epoch: 1/2, step 2352/7134 completed (loss: 0.45625439286231995, acc: 0.9096774458885193)
[2025-02-13 19:10:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:55][root][INFO] - Training Epoch: 1/2, step 2353/7134 completed (loss: 0.3392491042613983, acc: 0.9225806593894958)
[2025-02-13 19:10:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:56][root][INFO] - Training Epoch: 1/2, step 2354/7134 completed (loss: 0.23943202197551727, acc: 0.9234972596168518)
[2025-02-13 19:10:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:56][root][INFO] - Training Epoch: 1/2, step 2355/7134 completed (loss: 0.3941345512866974, acc: 0.9171270728111267)
[2025-02-13 19:10:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:57][root][INFO] - Training Epoch: 1/2, step 2356/7134 completed (loss: 0.4500104784965515, acc: 0.8840579986572266)
[2025-02-13 19:10:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:57][root][INFO] - Training Epoch: 1/2, step 2357/7134 completed (loss: 0.3388385474681854, acc: 0.89682537317276)
[2025-02-13 19:10:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:57][root][INFO] - Training Epoch: 1/2, step 2358/7134 completed (loss: 0.43838223814964294, acc: 0.8928571343421936)
[2025-02-13 19:10:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:58][root][INFO] - Training Epoch: 1/2, step 2359/7134 completed (loss: 0.3838203549385071, acc: 0.893750011920929)
[2025-02-13 19:10:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:58][root][INFO] - Training Epoch: 1/2, step 2360/7134 completed (loss: 0.2649427056312561, acc: 0.9367088675498962)
[2025-02-13 19:10:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:59][root][INFO] - Training Epoch: 1/2, step 2361/7134 completed (loss: 0.32996752858161926, acc: 0.9345238208770752)
[2025-02-13 19:10:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:59][root][INFO] - Training Epoch: 1/2, step 2362/7134 completed (loss: 0.3635794520378113, acc: 0.9369369149208069)
[2025-02-13 19:10:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:10:59][root][INFO] - Training Epoch: 1/2, step 2363/7134 completed (loss: 0.24822816252708435, acc: 0.9424083828926086)
[2025-02-13 19:10:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:00][root][INFO] - Training Epoch: 1/2, step 2364/7134 completed (loss: 0.14498941600322723, acc: 0.9803921580314636)
[2025-02-13 19:11:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:00][root][INFO] - Training Epoch: 1/2, step 2365/7134 completed (loss: 0.1775195598602295, acc: 0.9593023061752319)
[2025-02-13 19:11:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:00][root][INFO] - Training Epoch: 1/2, step 2366/7134 completed (loss: 0.14866888523101807, acc: 0.9692307710647583)
[2025-02-13 19:11:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:01][root][INFO] - Training Epoch: 1/2, step 2367/7134 completed (loss: 0.2545912563800812, acc: 0.939393937587738)
[2025-02-13 19:11:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:01][root][INFO] - Training Epoch: 1/2, step 2368/7134 completed (loss: 0.3930199444293976, acc: 0.9195979833602905)
[2025-02-13 19:11:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:02][root][INFO] - Training Epoch: 1/2, step 2369/7134 completed (loss: 0.21341237425804138, acc: 0.9669811129570007)
[2025-02-13 19:11:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:02][root][INFO] - Training Epoch: 1/2, step 2370/7134 completed (loss: 0.260569304227829, acc: 0.9333333373069763)
[2025-02-13 19:11:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:02][root][INFO] - Training Epoch: 1/2, step 2371/7134 completed (loss: 0.19179512560367584, acc: 0.9627329111099243)
[2025-02-13 19:11:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:03][root][INFO] - Training Epoch: 1/2, step 2372/7134 completed (loss: 0.1629701852798462, acc: 0.9700000286102295)
[2025-02-13 19:11:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:03][root][INFO] - Training Epoch: 1/2, step 2373/7134 completed (loss: 0.13477903604507446, acc: 0.9533678889274597)
[2025-02-13 19:11:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:03][root][INFO] - Training Epoch: 1/2, step 2374/7134 completed (loss: 0.1672469824552536, acc: 0.9668508172035217)
[2025-02-13 19:11:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:04][root][INFO] - Training Epoch: 1/2, step 2375/7134 completed (loss: 0.253925085067749, acc: 0.9272727370262146)
[2025-02-13 19:11:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:04][root][INFO] - Training Epoch: 1/2, step 2376/7134 completed (loss: 0.436847060918808, acc: 0.8758170008659363)
[2025-02-13 19:11:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:05][root][INFO] - Training Epoch: 1/2, step 2377/7134 completed (loss: 0.37512585520744324, acc: 0.8971962332725525)
[2025-02-13 19:11:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:05][root][INFO] - Training Epoch: 1/2, step 2378/7134 completed (loss: 0.47782543301582336, acc: 0.8811880946159363)
[2025-02-13 19:11:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:05][root][INFO] - Training Epoch: 1/2, step 2379/7134 completed (loss: 0.6921994686126709, acc: 0.8571428656578064)
[2025-02-13 19:11:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:06][root][INFO] - Training Epoch: 1/2, step 2380/7134 completed (loss: 0.46139347553253174, acc: 0.885869562625885)
[2025-02-13 19:11:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:06][root][INFO] - Training Epoch: 1/2, step 2381/7134 completed (loss: 0.8932796716690063, acc: 0.8012422323226929)
[2025-02-13 19:11:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:07][root][INFO] - Training Epoch: 1/2, step 2382/7134 completed (loss: 0.22928588092327118, acc: 0.9444444179534912)
[2025-02-13 19:11:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:07][root][INFO] - Training Epoch: 1/2, step 2383/7134 completed (loss: 0.2925362288951874, acc: 0.9289617538452148)
[2025-02-13 19:11:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:07][root][INFO] - Training Epoch: 1/2, step 2384/7134 completed (loss: 0.4478103816509247, acc: 0.8965517282485962)
[2025-02-13 19:11:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:08][root][INFO] - Training Epoch: 1/2, step 2385/7134 completed (loss: 0.19337685406208038, acc: 0.9505494236946106)
[2025-02-13 19:11:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:08][root][INFO] - Training Epoch: 1/2, step 2386/7134 completed (loss: 0.39107128977775574, acc: 0.920634925365448)
[2025-02-13 19:11:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:09][root][INFO] - Training Epoch: 1/2, step 2387/7134 completed (loss: 0.26194119453430176, acc: 0.9285714030265808)
[2025-02-13 19:11:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:09][root][INFO] - Training Epoch: 1/2, step 2388/7134 completed (loss: 0.373156875371933, acc: 0.9134615659713745)
[2025-02-13 19:11:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:09][root][INFO] - Training Epoch: 1/2, step 2389/7134 completed (loss: 0.37107032537460327, acc: 0.9146919250488281)
[2025-02-13 19:11:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:10][root][INFO] - Training Epoch: 1/2, step 2390/7134 completed (loss: 0.5117953419685364, acc: 0.8909952640533447)
[2025-02-13 19:11:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:10][root][INFO] - Training Epoch: 1/2, step 2391/7134 completed (loss: 0.42606738209724426, acc: 0.8810811042785645)
[2025-02-13 19:11:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:11][root][INFO] - Training Epoch: 1/2, step 2392/7134 completed (loss: 0.36350324749946594, acc: 0.9289617538452148)
[2025-02-13 19:11:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:11][root][INFO] - Training Epoch: 1/2, step 2393/7134 completed (loss: 0.33552056550979614, acc: 0.9119170904159546)
[2025-02-13 19:11:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:11][root][INFO] - Training Epoch: 1/2, step 2394/7134 completed (loss: 0.250938355922699, acc: 0.9336734414100647)
[2025-02-13 19:11:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:12][root][INFO] - Training Epoch: 1/2, step 2395/7134 completed (loss: 0.7801657319068909, acc: 0.8285714387893677)
[2025-02-13 19:11:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:12][root][INFO] - Training Epoch: 1/2, step 2396/7134 completed (loss: 0.8470140099525452, acc: 0.7828282713890076)
[2025-02-13 19:11:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:12][root][INFO] - Training Epoch: 1/2, step 2397/7134 completed (loss: 0.5625450611114502, acc: 0.8780487775802612)
[2025-02-13 19:11:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:13][root][INFO] - Training Epoch: 1/2, step 2398/7134 completed (loss: 0.1905699372291565, acc: 0.9459459185600281)
[2025-02-13 19:11:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:13][root][INFO] - Training Epoch: 1/2, step 2399/7134 completed (loss: 0.6053043007850647, acc: 0.8756476640701294)
[2025-02-13 19:11:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:14][root][INFO] - Training Epoch: 1/2, step 2400/7134 completed (loss: 0.5523884892463684, acc: 0.8693467378616333)
[2025-02-13 19:11:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:14][root][INFO] - Training Epoch: 1/2, step 2401/7134 completed (loss: 1.307163119316101, acc: 0.7067307829856873)
[2025-02-13 19:11:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:14][root][INFO] - Training Epoch: 1/2, step 2402/7134 completed (loss: 0.5401695966720581, acc: 0.8743961453437805)
[2025-02-13 19:11:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:15][root][INFO] - Training Epoch: 1/2, step 2403/7134 completed (loss: 0.3726739287376404, acc: 0.8982036113739014)
[2025-02-13 19:11:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:15][root][INFO] - Training Epoch: 1/2, step 2404/7134 completed (loss: 0.22302334010601044, acc: 0.9473684430122375)
[2025-02-13 19:11:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:15][root][INFO] - Training Epoch: 1/2, step 2405/7134 completed (loss: 0.4016892611980438, acc: 0.8917197585105896)
[2025-02-13 19:11:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:16][root][INFO] - Training Epoch: 1/2, step 2406/7134 completed (loss: 0.14141875505447388, acc: 0.9644970297813416)
[2025-02-13 19:11:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:16][root][INFO] - Training Epoch: 1/2, step 2407/7134 completed (loss: 0.1722276657819748, acc: 0.9477124214172363)
[2025-02-13 19:11:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:17][root][INFO] - Training Epoch: 1/2, step 2408/7134 completed (loss: 0.32256513833999634, acc: 0.9281768202781677)
[2025-02-13 19:11:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:17][root][INFO] - Training Epoch: 1/2, step 2409/7134 completed (loss: 0.20991119742393494, acc: 0.949999988079071)
[2025-02-13 19:11:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:17][root][INFO] - Training Epoch: 1/2, step 2410/7134 completed (loss: 0.19706515967845917, acc: 0.9507042169570923)
[2025-02-13 19:11:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:18][root][INFO] - Training Epoch: 1/2, step 2411/7134 completed (loss: 0.18053625524044037, acc: 0.950276255607605)
[2025-02-13 19:11:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:18][root][INFO] - Training Epoch: 1/2, step 2412/7134 completed (loss: 0.1320073902606964, acc: 0.9545454382896423)
[2025-02-13 19:11:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:18][root][INFO] - Training Epoch: 1/2, step 2413/7134 completed (loss: 0.1989510953426361, acc: 0.9610389471054077)
[2025-02-13 19:11:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:19][root][INFO] - Training Epoch: 1/2, step 2414/7134 completed (loss: 0.18663740158081055, acc: 0.9453551769256592)
[2025-02-13 19:11:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:19][root][INFO] - Training Epoch: 1/2, step 2415/7134 completed (loss: 0.5020537972450256, acc: 0.9146341681480408)
[2025-02-13 19:11:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:20][root][INFO] - Training Epoch: 1/2, step 2416/7134 completed (loss: 0.20386894047260284, acc: 0.9496855139732361)
[2025-02-13 19:11:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:20][root][INFO] - Training Epoch: 1/2, step 2417/7134 completed (loss: 0.2745051681995392, acc: 0.9609755873680115)
[2025-02-13 19:11:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:20][root][INFO] - Training Epoch: 1/2, step 2418/7134 completed (loss: 0.2709052264690399, acc: 0.9370629191398621)
[2025-02-13 19:11:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:21][root][INFO] - Training Epoch: 1/2, step 2419/7134 completed (loss: 0.15970273315906525, acc: 0.9693251252174377)
[2025-02-13 19:11:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:21][root][INFO] - Training Epoch: 1/2, step 2420/7134 completed (loss: 0.22189444303512573, acc: 0.9444444179534912)
[2025-02-13 19:11:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:22][root][INFO] - Training Epoch: 1/2, step 2421/7134 completed (loss: 0.1435324102640152, acc: 0.9621621370315552)
[2025-02-13 19:11:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:22][root][INFO] - Training Epoch: 1/2, step 2422/7134 completed (loss: 0.18734976649284363, acc: 0.9714285731315613)
[2025-02-13 19:11:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:22][root][INFO] - Training Epoch: 1/2, step 2423/7134 completed (loss: 0.17674347758293152, acc: 0.9464285969734192)
[2025-02-13 19:11:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:23][root][INFO] - Training Epoch: 1/2, step 2424/7134 completed (loss: 0.22126737236976624, acc: 0.9431818127632141)
[2025-02-13 19:11:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:23][root][INFO] - Training Epoch: 1/2, step 2425/7134 completed (loss: 0.22556738555431366, acc: 0.949438214302063)
[2025-02-13 19:11:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:23][root][INFO] - Training Epoch: 1/2, step 2426/7134 completed (loss: 0.19946233928203583, acc: 0.954023003578186)
[2025-02-13 19:11:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:24][root][INFO] - Training Epoch: 1/2, step 2427/7134 completed (loss: 0.1000886783003807, acc: 0.9791666865348816)
[2025-02-13 19:11:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:24][root][INFO] - Training Epoch: 1/2, step 2428/7134 completed (loss: 0.16702723503112793, acc: 0.9526315927505493)
[2025-02-13 19:11:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:25][root][INFO] - Training Epoch: 1/2, step 2429/7134 completed (loss: 0.22116586565971375, acc: 0.9367088675498962)
[2025-02-13 19:11:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:25][root][INFO] - Training Epoch: 1/2, step 2430/7134 completed (loss: 0.1884179711341858, acc: 0.9466666579246521)
[2025-02-13 19:11:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:25][root][INFO] - Training Epoch: 1/2, step 2431/7134 completed (loss: 0.17758016288280487, acc: 0.9580419659614563)
[2025-02-13 19:11:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:26][root][INFO] - Training Epoch: 1/2, step 2432/7134 completed (loss: 0.4279748201370239, acc: 0.9008264541625977)
[2025-02-13 19:11:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:26][root][INFO] - Training Epoch: 1/2, step 2433/7134 completed (loss: 0.3155520558357239, acc: 0.9424460530281067)
[2025-02-13 19:11:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:27][root][INFO] - Training Epoch: 1/2, step 2434/7134 completed (loss: 0.47207415103912354, acc: 0.9194630980491638)
[2025-02-13 19:11:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:27][root][INFO] - Training Epoch: 1/2, step 2435/7134 completed (loss: 0.3954594135284424, acc: 0.9205297827720642)
[2025-02-13 19:11:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:27][root][INFO] - Training Epoch: 1/2, step 2436/7134 completed (loss: 0.20610588788986206, acc: 0.9543147087097168)
[2025-02-13 19:11:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:28][root][INFO] - Training Epoch: 1/2, step 2437/7134 completed (loss: 0.33382081985473633, acc: 0.9295774698257446)
[2025-02-13 19:11:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:28][root][INFO] - Training Epoch: 1/2, step 2438/7134 completed (loss: 0.2937640845775604, acc: 0.9320987462997437)
[2025-02-13 19:11:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:29][root][INFO] - Training Epoch: 1/2, step 2439/7134 completed (loss: 0.2430621236562729, acc: 0.9465240836143494)
[2025-02-13 19:11:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:29][root][INFO] - Training Epoch: 1/2, step 2440/7134 completed (loss: 0.27818673849105835, acc: 0.9204545617103577)
[2025-02-13 19:11:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:29][root][INFO] - Training Epoch: 1/2, step 2441/7134 completed (loss: 0.930056095123291, acc: 0.8320000171661377)
[2025-02-13 19:11:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:30][root][INFO] - Training Epoch: 1/2, step 2442/7134 completed (loss: 0.4448995590209961, acc: 0.9112426042556763)
[2025-02-13 19:11:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:30][root][INFO] - Training Epoch: 1/2, step 2443/7134 completed (loss: 0.2973701059818268, acc: 0.9325153231620789)
[2025-02-13 19:11:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:31][root][INFO] - Training Epoch: 1/2, step 2444/7134 completed (loss: 0.27660223841667175, acc: 0.9534883499145508)
[2025-02-13 19:11:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:31][root][INFO] - Training Epoch: 1/2, step 2445/7134 completed (loss: 0.2252667248249054, acc: 0.9554139971733093)
[2025-02-13 19:11:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:32][root][INFO] - Training Epoch: 1/2, step 2446/7134 completed (loss: 0.36698421835899353, acc: 0.9025974273681641)
[2025-02-13 19:11:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:32][root][INFO] - Training Epoch: 1/2, step 2447/7134 completed (loss: 0.2012341320514679, acc: 0.9719101190567017)
[2025-02-13 19:11:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:32][root][INFO] - Training Epoch: 1/2, step 2448/7134 completed (loss: 0.21087506413459778, acc: 0.9530201554298401)
[2025-02-13 19:11:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:33][root][INFO] - Training Epoch: 1/2, step 2449/7134 completed (loss: 0.47300219535827637, acc: 0.9011628031730652)
[2025-02-13 19:11:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:33][root][INFO] - Training Epoch: 1/2, step 2450/7134 completed (loss: 0.17843617498874664, acc: 0.9529411792755127)
[2025-02-13 19:11:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:33][root][INFO] - Training Epoch: 1/2, step 2451/7134 completed (loss: 0.14781531691551208, acc: 0.9679144620895386)
[2025-02-13 19:11:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:34][root][INFO] - Training Epoch: 1/2, step 2452/7134 completed (loss: 0.1878660023212433, acc: 0.9408602118492126)
[2025-02-13 19:11:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:34][root][INFO] - Training Epoch: 1/2, step 2453/7134 completed (loss: 0.31679508090019226, acc: 0.9430052042007446)
[2025-02-13 19:11:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:35][root][INFO] - Training Epoch: 1/2, step 2454/7134 completed (loss: 0.23340828716754913, acc: 0.936170220375061)
[2025-02-13 19:11:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:35][root][INFO] - Training Epoch: 1/2, step 2455/7134 completed (loss: 0.11856700479984283, acc: 0.9754601120948792)
[2025-02-13 19:11:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:35][root][INFO] - Training Epoch: 1/2, step 2456/7134 completed (loss: 0.2804916203022003, acc: 0.9222797751426697)
[2025-02-13 19:11:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:36][root][INFO] - Training Epoch: 1/2, step 2457/7134 completed (loss: 0.41411998867988586, acc: 0.8823529481887817)
[2025-02-13 19:11:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:36][root][INFO] - Training Epoch: 1/2, step 2458/7134 completed (loss: 0.37112781405448914, acc: 0.8999999761581421)
[2025-02-13 19:11:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:37][root][INFO] - Training Epoch: 1/2, step 2459/7134 completed (loss: 0.08702414482831955, acc: 0.988304078578949)
[2025-02-13 19:11:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:37][root][INFO] - Training Epoch: 1/2, step 2460/7134 completed (loss: 0.18770645558834076, acc: 0.9505494236946106)
[2025-02-13 19:11:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:37][root][INFO] - Training Epoch: 1/2, step 2461/7134 completed (loss: 0.09223096817731857, acc: 0.9685534834861755)
[2025-02-13 19:11:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:38][root][INFO] - Training Epoch: 1/2, step 2462/7134 completed (loss: 0.3686949610710144, acc: 0.9212121367454529)
[2025-02-13 19:11:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:38][root][INFO] - Training Epoch: 1/2, step 2463/7134 completed (loss: 0.0651380866765976, acc: 0.9937499761581421)
[2025-02-13 19:11:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:39][root][INFO] - Training Epoch: 1/2, step 2464/7134 completed (loss: 0.26600074768066406, acc: 0.949999988079071)
[2025-02-13 19:11:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:39][root][INFO] - Training Epoch: 1/2, step 2465/7134 completed (loss: 0.26131439208984375, acc: 0.9441340565681458)
[2025-02-13 19:11:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:39][root][INFO] - Training Epoch: 1/2, step 2466/7134 completed (loss: 0.14971190690994263, acc: 0.946107804775238)
[2025-02-13 19:11:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:40][root][INFO] - Training Epoch: 1/2, step 2467/7134 completed (loss: 0.3632851839065552, acc: 0.9044944047927856)
[2025-02-13 19:11:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:40][root][INFO] - Training Epoch: 1/2, step 2468/7134 completed (loss: 0.19403743743896484, acc: 0.9651162624359131)
[2025-02-13 19:11:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:40][root][INFO] - Training Epoch: 1/2, step 2469/7134 completed (loss: 0.04864194616675377, acc: 0.9883720874786377)
[2025-02-13 19:11:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:41][root][INFO] - Training Epoch: 1/2, step 2470/7134 completed (loss: 0.29880401492118835, acc: 0.9503546357154846)
[2025-02-13 19:11:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:41][root][INFO] - Training Epoch: 1/2, step 2471/7134 completed (loss: 0.3039097785949707, acc: 0.9432623982429504)
[2025-02-13 19:11:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:42][root][INFO] - Training Epoch: 1/2, step 2472/7134 completed (loss: 0.1500229835510254, acc: 0.9662162065505981)
[2025-02-13 19:11:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:42][root][INFO] - Training Epoch: 1/2, step 2473/7134 completed (loss: 0.11398892104625702, acc: 0.9813084006309509)
[2025-02-13 19:11:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:42][root][INFO] - Training Epoch: 1/2, step 2474/7134 completed (loss: 0.08218095451593399, acc: 0.9818181991577148)
[2025-02-13 19:11:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:43][root][INFO] - Training Epoch: 1/2, step 2475/7134 completed (loss: 0.09669775515794754, acc: 0.9836065769195557)
[2025-02-13 19:11:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:43][root][INFO] - Training Epoch: 1/2, step 2476/7134 completed (loss: 0.13225549459457397, acc: 0.9707602262496948)
[2025-02-13 19:11:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:44][root][INFO] - Training Epoch: 1/2, step 2477/7134 completed (loss: 0.11711882054805756, acc: 0.9629629850387573)
[2025-02-13 19:11:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:44][root][INFO] - Training Epoch: 1/2, step 2478/7134 completed (loss: 0.12280350178480148, acc: 0.9664429426193237)
[2025-02-13 19:11:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:44][root][INFO] - Training Epoch: 1/2, step 2479/7134 completed (loss: 0.46156129240989685, acc: 0.9115646481513977)
[2025-02-13 19:11:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:45][root][INFO] - Training Epoch: 1/2, step 2480/7134 completed (loss: 0.12185350060462952, acc: 0.9711538553237915)
[2025-02-13 19:11:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:45][root][INFO] - Training Epoch: 1/2, step 2481/7134 completed (loss: 0.2401210516691208, acc: 0.932692289352417)
[2025-02-13 19:11:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:46][root][INFO] - Training Epoch: 1/2, step 2482/7134 completed (loss: 0.250784307718277, acc: 0.948051929473877)
[2025-02-13 19:11:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:46][root][INFO] - Training Epoch: 1/2, step 2483/7134 completed (loss: 0.13493193686008453, acc: 0.976047933101654)
[2025-02-13 19:11:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:46][root][INFO] - Training Epoch: 1/2, step 2484/7134 completed (loss: 0.17278021574020386, acc: 0.9698795080184937)
[2025-02-13 19:11:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:47][root][INFO] - Training Epoch: 1/2, step 2485/7134 completed (loss: 0.12570500373840332, acc: 0.9555555582046509)
[2025-02-13 19:11:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:47][root][INFO] - Training Epoch: 1/2, step 2486/7134 completed (loss: 0.11585859954357147, acc: 0.9726027250289917)
[2025-02-13 19:11:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:48][root][INFO] - Training Epoch: 1/2, step 2487/7134 completed (loss: 0.4119625985622406, acc: 0.9166666865348816)
[2025-02-13 19:11:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:48][root][INFO] - Training Epoch: 1/2, step 2488/7134 completed (loss: 0.2048833966255188, acc: 0.9515151381492615)
[2025-02-13 19:11:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:48][root][INFO] - Training Epoch: 1/2, step 2489/7134 completed (loss: 0.12387995421886444, acc: 0.9644970297813416)
[2025-02-13 19:11:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:49][root][INFO] - Training Epoch: 1/2, step 2490/7134 completed (loss: 0.18152284622192383, acc: 0.9675324559211731)
[2025-02-13 19:11:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:49][root][INFO] - Training Epoch: 1/2, step 2491/7134 completed (loss: 0.4340135157108307, acc: 0.9029850959777832)
[2025-02-13 19:11:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:49][root][INFO] - Training Epoch: 1/2, step 2492/7134 completed (loss: 0.2363966703414917, acc: 0.955974817276001)
[2025-02-13 19:11:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:50][root][INFO] - Training Epoch: 1/2, step 2493/7134 completed (loss: 0.20373770594596863, acc: 0.9523809552192688)
[2025-02-13 19:11:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:50][root][INFO] - Training Epoch: 1/2, step 2494/7134 completed (loss: 0.2304360717535019, acc: 0.9320987462997437)
[2025-02-13 19:11:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:51][root][INFO] - Training Epoch: 1/2, step 2495/7134 completed (loss: 0.27104490995407104, acc: 0.9352940917015076)
[2025-02-13 19:11:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:51][root][INFO] - Training Epoch: 1/2, step 2496/7134 completed (loss: 0.27998730540275574, acc: 0.9367088675498962)
[2025-02-13 19:11:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:51][root][INFO] - Training Epoch: 1/2, step 2497/7134 completed (loss: 0.14834946393966675, acc: 0.977011501789093)
[2025-02-13 19:11:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:52][root][INFO] - Training Epoch: 1/2, step 2498/7134 completed (loss: 0.17729522287845612, acc: 0.9599999785423279)
[2025-02-13 19:11:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:52][root][INFO] - Training Epoch: 1/2, step 2499/7134 completed (loss: 0.15317267179489136, acc: 0.9698795080184937)
[2025-02-13 19:11:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:52][root][INFO] - Training Epoch: 1/2, step 2500/7134 completed (loss: 0.18752798438072205, acc: 0.9662162065505981)
[2025-02-13 19:11:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:53][root][INFO] - Training Epoch: 1/2, step 2501/7134 completed (loss: 0.33004504442214966, acc: 0.915730357170105)
[2025-02-13 19:11:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:53][root][INFO] - Training Epoch: 1/2, step 2502/7134 completed (loss: 0.19903522729873657, acc: 0.9572649598121643)
[2025-02-13 19:11:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:54][root][INFO] - Training Epoch: 1/2, step 2503/7134 completed (loss: 0.12640433013439178, acc: 0.9710144996643066)
[2025-02-13 19:11:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:54][root][INFO] - Training Epoch: 1/2, step 2504/7134 completed (loss: 0.19655926525592804, acc: 0.949367105960846)
[2025-02-13 19:11:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:54][root][INFO] - Training Epoch: 1/2, step 2505/7134 completed (loss: 0.19415892660617828, acc: 0.9651162624359131)
[2025-02-13 19:11:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:55][root][INFO] - Training Epoch: 1/2, step 2506/7134 completed (loss: 0.21758107841014862, acc: 0.9508196711540222)
[2025-02-13 19:11:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:55][root][INFO] - Training Epoch: 1/2, step 2507/7134 completed (loss: 0.07369795441627502, acc: 0.9833333492279053)
[2025-02-13 19:11:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:56][root][INFO] - Training Epoch: 1/2, step 2508/7134 completed (loss: 0.1804051548242569, acc: 0.9661017060279846)
[2025-02-13 19:11:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:56][root][INFO] - Training Epoch: 1/2, step 2509/7134 completed (loss: 0.08160501718521118, acc: 0.9764705896377563)
[2025-02-13 19:11:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:56][root][INFO] - Training Epoch: 1/2, step 2510/7134 completed (loss: 0.08302170783281326, acc: 0.9785714149475098)
[2025-02-13 19:11:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:57][root][INFO] - Training Epoch: 1/2, step 2511/7134 completed (loss: 0.26163750886917114, acc: 0.9617486596107483)
[2025-02-13 19:11:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:57][root][INFO] - Training Epoch: 1/2, step 2512/7134 completed (loss: 0.27914074063301086, acc: 0.9176470637321472)
[2025-02-13 19:11:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:57][root][INFO] - Training Epoch: 1/2, step 2513/7134 completed (loss: 0.1748618185520172, acc: 0.9516128897666931)
[2025-02-13 19:11:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:58][root][INFO] - Training Epoch: 1/2, step 2514/7134 completed (loss: 0.18677207827568054, acc: 0.9426751732826233)
[2025-02-13 19:11:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:58][root][INFO] - Training Epoch: 1/2, step 2515/7134 completed (loss: 0.17803528904914856, acc: 0.9675324559211731)
[2025-02-13 19:11:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:59][root][INFO] - Training Epoch: 1/2, step 2516/7134 completed (loss: 0.1848212033510208, acc: 0.9608938694000244)
[2025-02-13 19:11:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:11:59][root][INFO] - Training Epoch: 1/2, step 2517/7134 completed (loss: 0.2260863035917282, acc: 0.9444444179534912)
[2025-02-13 19:11:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:00][root][INFO] - Training Epoch: 1/2, step 2518/7134 completed (loss: 0.15540529787540436, acc: 0.9542483687400818)
[2025-02-13 19:12:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:00][root][INFO] - Training Epoch: 1/2, step 2519/7134 completed (loss: 0.09057106077671051, acc: 0.9806451797485352)
[2025-02-13 19:12:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:00][root][INFO] - Training Epoch: 1/2, step 2520/7134 completed (loss: 0.16276037693023682, acc: 0.9671052694320679)
[2025-02-13 19:12:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:01][root][INFO] - Training Epoch: 1/2, step 2521/7134 completed (loss: 0.3502131998538971, acc: 0.9230769276618958)
[2025-02-13 19:12:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:01][root][INFO] - Training Epoch: 1/2, step 2522/7134 completed (loss: 0.1529112011194229, acc: 0.951724112033844)
[2025-02-13 19:12:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:02][root][INFO] - Training Epoch: 1/2, step 2523/7134 completed (loss: 0.17366717755794525, acc: 0.9570552110671997)
[2025-02-13 19:12:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:02][root][INFO] - Training Epoch: 1/2, step 2524/7134 completed (loss: 0.08896005153656006, acc: 0.9817073345184326)
[2025-02-13 19:12:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:02][root][INFO] - Training Epoch: 1/2, step 2525/7134 completed (loss: 0.24155624210834503, acc: 0.9383561611175537)
[2025-02-13 19:12:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:03][root][INFO] - Training Epoch: 1/2, step 2526/7134 completed (loss: 0.05277744308114052, acc: 0.9898989796638489)
[2025-02-13 19:12:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:03][root][INFO] - Training Epoch: 1/2, step 2527/7134 completed (loss: 0.1290942281484604, acc: 0.9726775884628296)
[2025-02-13 19:12:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:04][root][INFO] - Training Epoch: 1/2, step 2528/7134 completed (loss: 0.09116458147764206, acc: 0.9819276928901672)
[2025-02-13 19:12:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:04][root][INFO] - Training Epoch: 1/2, step 2529/7134 completed (loss: 0.11132675409317017, acc: 0.9702380895614624)
[2025-02-13 19:12:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:04][root][INFO] - Training Epoch: 1/2, step 2530/7134 completed (loss: 0.13801905512809753, acc: 0.966292142868042)
[2025-02-13 19:12:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:05][root][INFO] - Training Epoch: 1/2, step 2531/7134 completed (loss: 0.07191582024097443, acc: 0.9638554453849792)
[2025-02-13 19:12:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:05][root][INFO] - Training Epoch: 1/2, step 2532/7134 completed (loss: 0.10606875270605087, acc: 0.9866666793823242)
[2025-02-13 19:12:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:05][root][INFO] - Training Epoch: 1/2, step 2533/7134 completed (loss: 0.1666630655527115, acc: 0.9615384340286255)
[2025-02-13 19:12:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:06][root][INFO] - Training Epoch: 1/2, step 2534/7134 completed (loss: 0.13652180135250092, acc: 0.9664804339408875)
[2025-02-13 19:12:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:06][root][INFO] - Training Epoch: 1/2, step 2535/7134 completed (loss: 0.09132126718759537, acc: 0.9830508232116699)
[2025-02-13 19:12:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:07][root][INFO] - Training Epoch: 1/2, step 2536/7134 completed (loss: 0.2584030032157898, acc: 0.9222797751426697)
[2025-02-13 19:12:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:07][root][INFO] - Training Epoch: 1/2, step 2537/7134 completed (loss: 0.22955507040023804, acc: 0.9452054500579834)
[2025-02-13 19:12:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:07][root][INFO] - Training Epoch: 1/2, step 2538/7134 completed (loss: 0.33015376329421997, acc: 0.9080459475517273)
[2025-02-13 19:12:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:08][root][INFO] - Training Epoch: 1/2, step 2539/7134 completed (loss: 0.19887395203113556, acc: 0.9548872113227844)
[2025-02-13 19:12:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:08][root][INFO] - Training Epoch: 1/2, step 2540/7134 completed (loss: 0.20550796389579773, acc: 0.9668874144554138)
[2025-02-13 19:12:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:09][root][INFO] - Training Epoch: 1/2, step 2541/7134 completed (loss: 0.10572247207164764, acc: 0.9632353186607361)
[2025-02-13 19:12:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:09][root][INFO] - Training Epoch: 1/2, step 2542/7134 completed (loss: 0.24426543712615967, acc: 0.9437500238418579)
[2025-02-13 19:12:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:09][root][INFO] - Training Epoch: 1/2, step 2543/7134 completed (loss: 0.27265408635139465, acc: 0.9245283007621765)
[2025-02-13 19:12:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:10][root][INFO] - Training Epoch: 1/2, step 2544/7134 completed (loss: 0.1393924355506897, acc: 0.9702380895614624)
[2025-02-13 19:12:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:10][root][INFO] - Training Epoch: 1/2, step 2545/7134 completed (loss: 0.1307024359703064, acc: 0.9626865386962891)
[2025-02-13 19:12:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:10][root][INFO] - Training Epoch: 1/2, step 2546/7134 completed (loss: 0.5110296010971069, acc: 0.8826530575752258)
[2025-02-13 19:12:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:11][root][INFO] - Training Epoch: 1/2, step 2547/7134 completed (loss: 0.38293901085853577, acc: 0.9064748287200928)
[2025-02-13 19:12:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:11][root][INFO] - Training Epoch: 1/2, step 2548/7134 completed (loss: 0.17627288401126862, acc: 0.9646017551422119)
[2025-02-13 19:12:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:12][root][INFO] - Training Epoch: 1/2, step 2549/7134 completed (loss: 0.22061069309711456, acc: 0.9430894255638123)
[2025-02-13 19:12:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:12][root][INFO] - Training Epoch: 1/2, step 2550/7134 completed (loss: 0.09481551498174667, acc: 0.9589040875434875)
[2025-02-13 19:12:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:12][root][INFO] - Training Epoch: 1/2, step 2551/7134 completed (loss: 0.22815650701522827, acc: 0.9492753744125366)
[2025-02-13 19:12:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:13][root][INFO] - Training Epoch: 1/2, step 2552/7134 completed (loss: 0.14554400742053986, acc: 0.9620253443717957)
[2025-02-13 19:12:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:13][root][INFO] - Training Epoch: 1/2, step 2553/7134 completed (loss: 0.3699791133403778, acc: 0.9342105388641357)
[2025-02-13 19:12:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:14][root][INFO] - Training Epoch: 1/2, step 2554/7134 completed (loss: 0.3070603013038635, acc: 0.9448819160461426)
[2025-02-13 19:12:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:14][root][INFO] - Training Epoch: 1/2, step 2555/7134 completed (loss: 0.13968072831630707, acc: 0.9639639854431152)
[2025-02-13 19:12:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:14][root][INFO] - Training Epoch: 1/2, step 2556/7134 completed (loss: 0.15216495096683502, acc: 0.9700000286102295)
[2025-02-13 19:12:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:15][root][INFO] - Training Epoch: 1/2, step 2557/7134 completed (loss: 0.18366947770118713, acc: 0.9591836929321289)
[2025-02-13 19:12:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:15][root][INFO] - Training Epoch: 1/2, step 2558/7134 completed (loss: 0.07487864792346954, acc: 0.985401451587677)
[2025-02-13 19:12:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:16][root][INFO] - Training Epoch: 1/2, step 2559/7134 completed (loss: 0.13088181614875793, acc: 0.96875)
[2025-02-13 19:12:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:16][root][INFO] - Training Epoch: 1/2, step 2560/7134 completed (loss: 0.19492202997207642, acc: 0.9459459185600281)
[2025-02-13 19:12:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:16][root][INFO] - Training Epoch: 1/2, step 2561/7134 completed (loss: 0.18108493089675903, acc: 0.9375)
[2025-02-13 19:12:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:17][root][INFO] - Training Epoch: 1/2, step 2562/7134 completed (loss: 0.10760088264942169, acc: 0.9599999785423279)
[2025-02-13 19:12:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:17][root][INFO] - Training Epoch: 1/2, step 2563/7134 completed (loss: 0.12749677896499634, acc: 0.9822485446929932)
[2025-02-13 19:12:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:17][root][INFO] - Training Epoch: 1/2, step 2564/7134 completed (loss: 0.39847612380981445, acc: 0.9136690497398376)
[2025-02-13 19:12:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:18][root][INFO] - Training Epoch: 1/2, step 2565/7134 completed (loss: 0.14385563135147095, acc: 0.9640287756919861)
[2025-02-13 19:12:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:18][root][INFO] - Training Epoch: 1/2, step 2566/7134 completed (loss: 0.13388560712337494, acc: 0.9759036302566528)
[2025-02-13 19:12:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:19][root][INFO] - Training Epoch: 1/2, step 2567/7134 completed (loss: 0.0834403857588768, acc: 0.9849624037742615)
[2025-02-13 19:12:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:19][root][INFO] - Training Epoch: 1/2, step 2568/7134 completed (loss: 0.22986148297786713, acc: 0.9487179517745972)
[2025-02-13 19:12:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:19][root][INFO] - Training Epoch: 1/2, step 2569/7134 completed (loss: 0.06775858998298645, acc: 1.0)
[2025-02-13 19:12:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:20][root][INFO] - Training Epoch: 1/2, step 2570/7134 completed (loss: 0.15047980844974518, acc: 0.9536423683166504)
[2025-02-13 19:12:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:20][root][INFO] - Training Epoch: 1/2, step 2571/7134 completed (loss: 0.16293834149837494, acc: 0.9508196711540222)
[2025-02-13 19:12:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:21][root][INFO] - Training Epoch: 1/2, step 2572/7134 completed (loss: 0.17772753536701202, acc: 0.9659863710403442)
[2025-02-13 19:12:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:21][root][INFO] - Training Epoch: 1/2, step 2573/7134 completed (loss: 0.11122388392686844, acc: 0.9856114983558655)
[2025-02-13 19:12:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:21][root][INFO] - Training Epoch: 1/2, step 2574/7134 completed (loss: 0.14942681789398193, acc: 0.9494949579238892)
[2025-02-13 19:12:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:22][root][INFO] - Training Epoch: 1/2, step 2575/7134 completed (loss: 0.15117304027080536, acc: 0.9836065769195557)
[2025-02-13 19:12:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:22][root][INFO] - Training Epoch: 1/2, step 2576/7134 completed (loss: 0.10982848703861237, acc: 0.9837398529052734)
[2025-02-13 19:12:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:22][root][INFO] - Training Epoch: 1/2, step 2577/7134 completed (loss: 0.33726072311401367, acc: 0.9291338324546814)
[2025-02-13 19:12:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:23][root][INFO] - Training Epoch: 1/2, step 2578/7134 completed (loss: 0.6173460483551025, acc: 0.8627451062202454)
[2025-02-13 19:12:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:23][root][INFO] - Training Epoch: 1/2, step 2579/7134 completed (loss: 0.3900647461414337, acc: 0.8974359035491943)
[2025-02-13 19:12:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:24][root][INFO] - Training Epoch: 1/2, step 2580/7134 completed (loss: 0.26261523365974426, acc: 0.9306358098983765)
[2025-02-13 19:12:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:24][root][INFO] - Training Epoch: 1/2, step 2581/7134 completed (loss: 0.40553146600723267, acc: 0.909604549407959)
[2025-02-13 19:12:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:24][root][INFO] - Training Epoch: 1/2, step 2582/7134 completed (loss: 0.3705115020275116, acc: 0.903954803943634)
[2025-02-13 19:12:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:25][root][INFO] - Training Epoch: 1/2, step 2583/7134 completed (loss: 0.3967226445674896, acc: 0.9137930870056152)
[2025-02-13 19:12:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:25][root][INFO] - Training Epoch: 1/2, step 2584/7134 completed (loss: 0.5190295577049255, acc: 0.8794326186180115)
[2025-02-13 19:12:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:25][root][INFO] - Training Epoch: 1/2, step 2585/7134 completed (loss: 0.33494776487350464, acc: 0.9337748289108276)
[2025-02-13 19:12:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:26][root][INFO] - Training Epoch: 1/2, step 2586/7134 completed (loss: 0.4019985795021057, acc: 0.8846153616905212)
[2025-02-13 19:12:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:26][root][INFO] - Training Epoch: 1/2, step 2587/7134 completed (loss: 0.18010935187339783, acc: 0.949999988079071)
[2025-02-13 19:12:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:27][root][INFO] - Training Epoch: 1/2, step 2588/7134 completed (loss: 0.22134137153625488, acc: 0.9591836929321289)
[2025-02-13 19:12:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:27][root][INFO] - Training Epoch: 1/2, step 2589/7134 completed (loss: 0.47177210450172424, acc: 0.9047619104385376)
[2025-02-13 19:12:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:27][root][INFO] - Training Epoch: 1/2, step 2590/7134 completed (loss: 0.2484651356935501, acc: 0.9333333373069763)
[2025-02-13 19:12:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:28][root][INFO] - Training Epoch: 1/2, step 2591/7134 completed (loss: 0.44981563091278076, acc: 0.9064327478408813)
[2025-02-13 19:12:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:28][root][INFO] - Training Epoch: 1/2, step 2592/7134 completed (loss: 0.42930683493614197, acc: 0.916201114654541)
[2025-02-13 19:12:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:29][root][INFO] - Training Epoch: 1/2, step 2593/7134 completed (loss: 0.21773245930671692, acc: 0.9756097793579102)
[2025-02-13 19:12:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:29][root][INFO] - Training Epoch: 1/2, step 2594/7134 completed (loss: 0.5018940567970276, acc: 0.8758170008659363)
[2025-02-13 19:12:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:29][root][INFO] - Training Epoch: 1/2, step 2595/7134 completed (loss: 0.2179616540670395, acc: 0.9451219439506531)
[2025-02-13 19:12:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:30][root][INFO] - Training Epoch: 1/2, step 2596/7134 completed (loss: 0.2380077987909317, acc: 0.9352940917015076)
[2025-02-13 19:12:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:30][root][INFO] - Training Epoch: 1/2, step 2597/7134 completed (loss: 0.2855585515499115, acc: 0.903954803943634)
[2025-02-13 19:12:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:31][root][INFO] - Training Epoch: 1/2, step 2598/7134 completed (loss: 0.35234972834587097, acc: 0.8940397500991821)
[2025-02-13 19:12:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:31][root][INFO] - Training Epoch: 1/2, step 2599/7134 completed (loss: 0.05182531476020813, acc: 1.0)
[2025-02-13 19:12:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:31][root][INFO] - Training Epoch: 1/2, step 2600/7134 completed (loss: 0.4041415750980377, acc: 0.8932584524154663)
[2025-02-13 19:12:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:32][root][INFO] - Training Epoch: 1/2, step 2601/7134 completed (loss: 0.29548099637031555, acc: 0.9290322661399841)
[2025-02-13 19:12:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:32][root][INFO] - Training Epoch: 1/2, step 2602/7134 completed (loss: 0.2191692590713501, acc: 0.9285714030265808)
[2025-02-13 19:12:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:32][root][INFO] - Training Epoch: 1/2, step 2603/7134 completed (loss: 0.2580741345882416, acc: 0.9281045794487)
[2025-02-13 19:12:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:33][root][INFO] - Training Epoch: 1/2, step 2604/7134 completed (loss: 0.2586151361465454, acc: 0.9259259104728699)
[2025-02-13 19:12:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:33][root][INFO] - Training Epoch: 1/2, step 2605/7134 completed (loss: 0.4645523726940155, acc: 0.8849557638168335)
[2025-02-13 19:12:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:34][root][INFO] - Training Epoch: 1/2, step 2606/7134 completed (loss: 0.3273818790912628, acc: 0.8933333158493042)
[2025-02-13 19:12:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:34][root][INFO] - Training Epoch: 1/2, step 2607/7134 completed (loss: 0.29059627652168274, acc: 0.9438202381134033)
[2025-02-13 19:12:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:34][root][INFO] - Training Epoch: 1/2, step 2608/7134 completed (loss: 0.49060937762260437, acc: 0.9047619104385376)
[2025-02-13 19:12:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:35][root][INFO] - Training Epoch: 1/2, step 2609/7134 completed (loss: 0.4081239700317383, acc: 0.915032684803009)
[2025-02-13 19:12:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:35][root][INFO] - Training Epoch: 1/2, step 2610/7134 completed (loss: 0.23659296333789825, acc: 0.9568345546722412)
[2025-02-13 19:12:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:36][root][INFO] - Training Epoch: 1/2, step 2611/7134 completed (loss: 0.41646990180015564, acc: 0.8938547372817993)
[2025-02-13 19:12:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:36][root][INFO] - Training Epoch: 1/2, step 2612/7134 completed (loss: 0.38058802485466003, acc: 0.8949999809265137)
[2025-02-13 19:12:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:36][root][INFO] - Training Epoch: 1/2, step 2613/7134 completed (loss: 0.3059658706188202, acc: 0.9464285969734192)
[2025-02-13 19:12:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:37][root][INFO] - Training Epoch: 1/2, step 2614/7134 completed (loss: 0.5167137980461121, acc: 0.8882978558540344)
[2025-02-13 19:12:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:37][root][INFO] - Training Epoch: 1/2, step 2615/7134 completed (loss: 0.3144385516643524, acc: 0.9269663095474243)
[2025-02-13 19:12:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:38][root][INFO] - Training Epoch: 1/2, step 2616/7134 completed (loss: 0.4539269804954529, acc: 0.924369752407074)
[2025-02-13 19:12:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:38][root][INFO] - Training Epoch: 1/2, step 2617/7134 completed (loss: 0.5820751786231995, acc: 0.8477157354354858)
[2025-02-13 19:12:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:38][root][INFO] - Training Epoch: 1/2, step 2618/7134 completed (loss: 0.1916363537311554, acc: 0.9572649598121643)
[2025-02-13 19:12:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:39][root][INFO] - Training Epoch: 1/2, step 2619/7134 completed (loss: 0.2798340320587158, acc: 0.95652174949646)
[2025-02-13 19:12:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:39][root][INFO] - Training Epoch: 1/2, step 2620/7134 completed (loss: 0.29118701815605164, acc: 0.9312169551849365)
[2025-02-13 19:12:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:39][root][INFO] - Training Epoch: 1/2, step 2621/7134 completed (loss: 0.4447212815284729, acc: 0.903743326663971)
[2025-02-13 19:12:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:40][root][INFO] - Training Epoch: 1/2, step 2622/7134 completed (loss: 0.34657058119773865, acc: 0.9246575236320496)
[2025-02-13 19:12:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:40][root][INFO] - Training Epoch: 1/2, step 2623/7134 completed (loss: 0.4344072937965393, acc: 0.8807339668273926)
[2025-02-13 19:12:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:41][root][INFO] - Training Epoch: 1/2, step 2624/7134 completed (loss: 0.458045095205307, acc: 0.8619047403335571)
[2025-02-13 19:12:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:41][root][INFO] - Training Epoch: 1/2, step 2625/7134 completed (loss: 0.32225459814071655, acc: 0.9107142686843872)
[2025-02-13 19:12:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:41][root][INFO] - Training Epoch: 1/2, step 2626/7134 completed (loss: 0.28652405738830566, acc: 0.931034505367279)
[2025-02-13 19:12:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:42][root][INFO] - Training Epoch: 1/2, step 2627/7134 completed (loss: 0.2949598729610443, acc: 0.907975435256958)
[2025-02-13 19:12:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:42][root][INFO] - Training Epoch: 1/2, step 2628/7134 completed (loss: 0.370315283536911, acc: 0.9277777671813965)
[2025-02-13 19:12:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:43][root][INFO] - Training Epoch: 1/2, step 2629/7134 completed (loss: 0.22212959825992584, acc: 0.946107804775238)
[2025-02-13 19:12:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:43][root][INFO] - Training Epoch: 1/2, step 2630/7134 completed (loss: 0.304080605506897, acc: 0.9202898740768433)
[2025-02-13 19:12:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:43][root][INFO] - Training Epoch: 1/2, step 2631/7134 completed (loss: 0.3814384341239929, acc: 0.9289340376853943)
[2025-02-13 19:12:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:44][root][INFO] - Training Epoch: 1/2, step 2632/7134 completed (loss: 0.10703939944505692, acc: 0.9739130139350891)
[2025-02-13 19:12:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:44][root][INFO] - Training Epoch: 1/2, step 2633/7134 completed (loss: 0.29747486114501953, acc: 0.9395604133605957)
[2025-02-13 19:12:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:45][root][INFO] - Training Epoch: 1/2, step 2634/7134 completed (loss: 0.17537851631641388, acc: 0.9644970297813416)
[2025-02-13 19:12:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:45][root][INFO] - Training Epoch: 1/2, step 2635/7134 completed (loss: 0.5386306047439575, acc: 0.9055555462837219)
[2025-02-13 19:12:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:45][root][INFO] - Training Epoch: 1/2, step 2636/7134 completed (loss: 0.38374534249305725, acc: 0.9273743033409119)
[2025-02-13 19:12:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:46][root][INFO] - Training Epoch: 1/2, step 2637/7134 completed (loss: 0.5846276879310608, acc: 0.8527607321739197)
[2025-02-13 19:12:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:46][root][INFO] - Training Epoch: 1/2, step 2638/7134 completed (loss: 0.49368661642074585, acc: 0.8855721354484558)
[2025-02-13 19:12:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:47][root][INFO] - Training Epoch: 1/2, step 2639/7134 completed (loss: 0.7386385202407837, acc: 0.8496240377426147)
[2025-02-13 19:12:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:47][root][INFO] - Training Epoch: 1/2, step 2640/7134 completed (loss: 0.5621543526649475, acc: 0.9004974961280823)
[2025-02-13 19:12:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:47][root][INFO] - Training Epoch: 1/2, step 2641/7134 completed (loss: 0.36660492420196533, acc: 0.9085714221000671)
[2025-02-13 19:12:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:48][root][INFO] - Training Epoch: 1/2, step 2642/7134 completed (loss: 0.9131731390953064, acc: 0.8333333134651184)
[2025-02-13 19:12:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:48][root][INFO] - Training Epoch: 1/2, step 2643/7134 completed (loss: 0.5303257703781128, acc: 0.8720930218696594)
[2025-02-13 19:12:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:49][root][INFO] - Training Epoch: 1/2, step 2644/7134 completed (loss: 0.3532384932041168, acc: 0.9259259104728699)
[2025-02-13 19:12:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:49][root][INFO] - Training Epoch: 1/2, step 2645/7134 completed (loss: 0.34828296303749084, acc: 0.9314285516738892)
[2025-02-13 19:12:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:50][root][INFO] - Training Epoch: 1/2, step 2646/7134 completed (loss: 0.665154755115509, acc: 0.8447204828262329)
[2025-02-13 19:12:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:50][root][INFO] - Training Epoch: 1/2, step 2647/7134 completed (loss: 0.36729782819747925, acc: 0.91847825050354)
[2025-02-13 19:12:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:50][root][INFO] - Training Epoch: 1/2, step 2648/7134 completed (loss: 0.43467313051223755, acc: 0.905940592288971)
[2025-02-13 19:12:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:51][root][INFO] - Training Epoch: 1/2, step 2649/7134 completed (loss: 0.6852902173995972, acc: 0.8216215968132019)
[2025-02-13 19:12:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:51][root][INFO] - Training Epoch: 1/2, step 2650/7134 completed (loss: 0.6393570899963379, acc: 0.8588235378265381)
[2025-02-13 19:12:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:52][root][INFO] - Training Epoch: 1/2, step 2651/7134 completed (loss: 0.22769486904144287, acc: 0.9463414549827576)
[2025-02-13 19:12:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:52][root][INFO] - Training Epoch: 1/2, step 2652/7134 completed (loss: 0.3372488021850586, acc: 0.932584285736084)
[2025-02-13 19:12:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:52][root][INFO] - Training Epoch: 1/2, step 2653/7134 completed (loss: 0.3105521500110626, acc: 0.9215686321258545)
[2025-02-13 19:12:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:53][root][INFO] - Training Epoch: 1/2, step 2654/7134 completed (loss: 0.23401805758476257, acc: 0.9285714030265808)
[2025-02-13 19:12:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:53][root][INFO] - Training Epoch: 1/2, step 2655/7134 completed (loss: 0.2072291225194931, acc: 0.9316770434379578)
[2025-02-13 19:12:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:54][root][INFO] - Training Epoch: 1/2, step 2656/7134 completed (loss: 0.3689958453178406, acc: 0.9360465407371521)
[2025-02-13 19:12:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:54][root][INFO] - Training Epoch: 1/2, step 2657/7134 completed (loss: 0.22474487125873566, acc: 0.9177215099334717)
[2025-02-13 19:12:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:54][root][INFO] - Training Epoch: 1/2, step 2658/7134 completed (loss: 0.6976506114006042, acc: 0.8636363744735718)
[2025-02-13 19:12:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:55][root][INFO] - Training Epoch: 1/2, step 2659/7134 completed (loss: 0.2806171774864197, acc: 0.9179104566574097)
[2025-02-13 19:12:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:55][root][INFO] - Training Epoch: 1/2, step 2660/7134 completed (loss: 0.4533211588859558, acc: 0.8888888955116272)
[2025-02-13 19:12:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:56][root][INFO] - Training Epoch: 1/2, step 2661/7134 completed (loss: 0.15991419553756714, acc: 0.9526627063751221)
[2025-02-13 19:12:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:56][root][INFO] - Training Epoch: 1/2, step 2662/7134 completed (loss: 0.2095935046672821, acc: 0.9629629850387573)
[2025-02-13 19:12:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:56][root][INFO] - Training Epoch: 1/2, step 2663/7134 completed (loss: 0.24200482666492462, acc: 0.9378238320350647)
[2025-02-13 19:12:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:57][root][INFO] - Training Epoch: 1/2, step 2664/7134 completed (loss: 0.3125267028808594, acc: 0.9186046719551086)
[2025-02-13 19:12:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:57][root][INFO] - Training Epoch: 1/2, step 2665/7134 completed (loss: 0.1754923164844513, acc: 0.9457831382751465)
[2025-02-13 19:12:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:58][root][INFO] - Training Epoch: 1/2, step 2666/7134 completed (loss: 0.1078098863363266, acc: 0.9767441749572754)
[2025-02-13 19:12:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:58][root][INFO] - Training Epoch: 1/2, step 2667/7134 completed (loss: 0.09794767200946808, acc: 0.9810126423835754)
[2025-02-13 19:12:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:58][root][INFO] - Training Epoch: 1/2, step 2668/7134 completed (loss: 0.0872245654463768, acc: 0.9714285731315613)
[2025-02-13 19:12:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:59][root][INFO] - Training Epoch: 1/2, step 2669/7134 completed (loss: 0.07028597593307495, acc: 0.9850746393203735)
[2025-02-13 19:12:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:59][root][INFO] - Training Epoch: 1/2, step 2670/7134 completed (loss: 0.15697577595710754, acc: 0.954285740852356)
[2025-02-13 19:12:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:12:59][root][INFO] - Training Epoch: 1/2, step 2671/7134 completed (loss: 0.25838905572891235, acc: 0.9485714435577393)
[2025-02-13 19:13:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:00][root][INFO] - Training Epoch: 1/2, step 2672/7134 completed (loss: 0.09959883987903595, acc: 0.9698795080184937)
[2025-02-13 19:13:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:00][root][INFO] - Training Epoch: 1/2, step 2673/7134 completed (loss: 0.11932271718978882, acc: 0.9723756909370422)
[2025-02-13 19:13:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:01][root][INFO] - Training Epoch: 1/2, step 2674/7134 completed (loss: 0.1874171793460846, acc: 0.9567901492118835)
[2025-02-13 19:13:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:01][root][INFO] - Training Epoch: 1/2, step 2675/7134 completed (loss: 0.05467066168785095, acc: 0.9878787994384766)
[2025-02-13 19:13:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:01][root][INFO] - Training Epoch: 1/2, step 2676/7134 completed (loss: 0.1258750855922699, acc: 0.9715909361839294)
[2025-02-13 19:13:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:02][root][INFO] - Training Epoch: 1/2, step 2677/7134 completed (loss: 0.038807254284620285, acc: 0.9938650131225586)
[2025-02-13 19:13:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:02][root][INFO] - Training Epoch: 1/2, step 2678/7134 completed (loss: 0.05436008796095848, acc: 0.994350254535675)
[2025-02-13 19:13:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:03][root][INFO] - Training Epoch: 1/2, step 2679/7134 completed (loss: 0.07140795141458511, acc: 0.9848484992980957)
[2025-02-13 19:13:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:03][root][INFO] - Training Epoch: 1/2, step 2680/7134 completed (loss: 0.10502482205629349, acc: 0.9835164546966553)
[2025-02-13 19:13:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:03][root][INFO] - Training Epoch: 1/2, step 2681/7134 completed (loss: 0.04811635985970497, acc: 0.9810126423835754)
[2025-02-13 19:13:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:04][root][INFO] - Training Epoch: 1/2, step 2682/7134 completed (loss: 0.08481352031230927, acc: 0.9731543660163879)
[2025-02-13 19:13:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:04][root][INFO] - Training Epoch: 1/2, step 2683/7134 completed (loss: 0.06040879338979721, acc: 0.9867549538612366)
[2025-02-13 19:13:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:05][root][INFO] - Training Epoch: 1/2, step 2684/7134 completed (loss: 0.24149875342845917, acc: 0.9453125)
[2025-02-13 19:13:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:05][root][INFO] - Training Epoch: 1/2, step 2685/7134 completed (loss: 0.10729774087667465, acc: 0.9822485446929932)
[2025-02-13 19:13:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:05][root][INFO] - Training Epoch: 1/2, step 2686/7134 completed (loss: 0.32815879583358765, acc: 0.9350000023841858)
[2025-02-13 19:13:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:06][root][INFO] - Training Epoch: 1/2, step 2687/7134 completed (loss: 0.2513226568698883, acc: 0.9411764740943909)
[2025-02-13 19:13:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:06][root][INFO] - Training Epoch: 1/2, step 2688/7134 completed (loss: 0.26577267050743103, acc: 0.9473684430122375)
[2025-02-13 19:13:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:06][root][INFO] - Training Epoch: 1/2, step 2689/7134 completed (loss: 0.22702357172966003, acc: 0.934959352016449)
[2025-02-13 19:13:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:07][root][INFO] - Training Epoch: 1/2, step 2690/7134 completed (loss: 0.2550140619277954, acc: 0.9208633303642273)
[2025-02-13 19:13:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:07][root][INFO] - Training Epoch: 1/2, step 2691/7134 completed (loss: 0.48705920577049255, acc: 0.8938053250312805)
[2025-02-13 19:13:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:08][root][INFO] - Training Epoch: 1/2, step 2692/7134 completed (loss: 0.5996126532554626, acc: 0.8677685856819153)
[2025-02-13 19:13:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:08][root][INFO] - Training Epoch: 1/2, step 2693/7134 completed (loss: 0.521766722202301, acc: 0.8849557638168335)
[2025-02-13 19:13:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:08][root][INFO] - Training Epoch: 1/2, step 2694/7134 completed (loss: 0.4180413484573364, acc: 0.9139072895050049)
[2025-02-13 19:13:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:09][root][INFO] - Training Epoch: 1/2, step 2695/7134 completed (loss: 0.21649940311908722, acc: 0.9629629850387573)
[2025-02-13 19:13:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:09][root][INFO] - Training Epoch: 1/2, step 2696/7134 completed (loss: 0.2930053472518921, acc: 0.9428571462631226)
[2025-02-13 19:13:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:09][root][INFO] - Training Epoch: 1/2, step 2697/7134 completed (loss: 0.42605358362197876, acc: 0.9090909361839294)
[2025-02-13 19:13:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:10][root][INFO] - Training Epoch: 1/2, step 2698/7134 completed (loss: 0.3119067847728729, acc: 0.9251700639724731)
[2025-02-13 19:13:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:10][root][INFO] - Training Epoch: 1/2, step 2699/7134 completed (loss: 0.1641235649585724, acc: 0.982758641242981)
[2025-02-13 19:13:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:11][root][INFO] - Training Epoch: 1/2, step 2700/7134 completed (loss: 0.28468549251556396, acc: 0.9142857193946838)
[2025-02-13 19:13:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:11][root][INFO] - Training Epoch: 1/2, step 2701/7134 completed (loss: 0.31862661242485046, acc: 0.9548872113227844)
[2025-02-13 19:13:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:11][root][INFO] - Training Epoch: 1/2, step 2702/7134 completed (loss: 0.5127450227737427, acc: 0.8703703880310059)
[2025-02-13 19:13:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:12][root][INFO] - Training Epoch: 1/2, step 2703/7134 completed (loss: 0.12210782617330551, acc: 0.9599999785423279)
[2025-02-13 19:13:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:12][root][INFO] - Training Epoch: 1/2, step 2704/7134 completed (loss: 0.3051852881908417, acc: 0.9345238208770752)
[2025-02-13 19:13:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:12][root][INFO] - Training Epoch: 1/2, step 2705/7134 completed (loss: 0.47862550616264343, acc: 0.9014084339141846)
[2025-02-13 19:13:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:13][root][INFO] - Training Epoch: 1/2, step 2706/7134 completed (loss: 0.14849701523780823, acc: 0.9794520735740662)
[2025-02-13 19:13:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:13][root][INFO] - Training Epoch: 1/2, step 2707/7134 completed (loss: 0.32046154141426086, acc: 0.925000011920929)
[2025-02-13 19:13:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:14][root][INFO] - Training Epoch: 1/2, step 2708/7134 completed (loss: 0.3079240322113037, acc: 0.9200000166893005)
[2025-02-13 19:13:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:14][root][INFO] - Training Epoch: 1/2, step 2709/7134 completed (loss: 0.1718180924654007, acc: 0.9507042169570923)
[2025-02-13 19:13:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:14][root][INFO] - Training Epoch: 1/2, step 2710/7134 completed (loss: 0.41709938645362854, acc: 0.8500000238418579)
[2025-02-13 19:13:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:15][root][INFO] - Training Epoch: 1/2, step 2711/7134 completed (loss: 0.15638640522956848, acc: 0.9793814420700073)
[2025-02-13 19:13:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:15][root][INFO] - Training Epoch: 1/2, step 2712/7134 completed (loss: 0.3103269636631012, acc: 0.9255319237709045)
[2025-02-13 19:13:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:16][root][INFO] - Training Epoch: 1/2, step 2713/7134 completed (loss: 0.2697046995162964, acc: 0.949999988079071)
[2025-02-13 19:13:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:16][root][INFO] - Training Epoch: 1/2, step 2714/7134 completed (loss: 0.09002725034952164, acc: 0.9852941036224365)
[2025-02-13 19:13:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:16][root][INFO] - Training Epoch: 1/2, step 2715/7134 completed (loss: 0.19698160886764526, acc: 0.9241379499435425)
[2025-02-13 19:13:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:17][root][INFO] - Training Epoch: 1/2, step 2716/7134 completed (loss: 0.13620789349079132, acc: 0.9709302186965942)
[2025-02-13 19:13:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:17][root][INFO] - Training Epoch: 1/2, step 2717/7134 completed (loss: 0.184361070394516, acc: 0.9487179517745972)
[2025-02-13 19:13:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:18][root][INFO] - Training Epoch: 1/2, step 2718/7134 completed (loss: 0.4973711371421814, acc: 0.9236111044883728)
[2025-02-13 19:13:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:18][root][INFO] - Training Epoch: 1/2, step 2719/7134 completed (loss: 0.32852867245674133, acc: 0.9424460530281067)
[2025-02-13 19:13:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:18][root][INFO] - Training Epoch: 1/2, step 2720/7134 completed (loss: 0.4234132468700409, acc: 0.9124087691307068)
[2025-02-13 19:13:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:19][root][INFO] - Training Epoch: 1/2, step 2721/7134 completed (loss: 0.2233542948961258, acc: 0.9735099077224731)
[2025-02-13 19:13:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:19][root][INFO] - Training Epoch: 1/2, step 2722/7134 completed (loss: 0.25659167766571045, acc: 0.9255319237709045)
[2025-02-13 19:13:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:19][root][INFO] - Training Epoch: 1/2, step 2723/7134 completed (loss: 0.48389214277267456, acc: 0.9281437397003174)
[2025-02-13 19:13:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:20][root][INFO] - Training Epoch: 1/2, step 2724/7134 completed (loss: 0.08595407754182816, acc: 0.9887005686759949)
[2025-02-13 19:13:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:20][root][INFO] - Training Epoch: 1/2, step 2725/7134 completed (loss: 0.24572893977165222, acc: 0.9328358173370361)
[2025-02-13 19:13:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:21][root][INFO] - Training Epoch: 1/2, step 2726/7134 completed (loss: 0.30334189534187317, acc: 0.934959352016449)
[2025-02-13 19:13:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:21][root][INFO] - Training Epoch: 1/2, step 2727/7134 completed (loss: 0.11674083024263382, acc: 0.9824561476707458)
[2025-02-13 19:13:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:21][root][INFO] - Training Epoch: 1/2, step 2728/7134 completed (loss: 1.5480237007141113, acc: 0.7037037014961243)
[2025-02-13 19:13:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:22][root][INFO] - Training Epoch: 1/2, step 2729/7134 completed (loss: 0.983811616897583, acc: 0.8142856955528259)
[2025-02-13 19:13:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:22][root][INFO] - Training Epoch: 1/2, step 2730/7134 completed (loss: 0.6865178942680359, acc: 0.8318583965301514)
[2025-02-13 19:13:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:23][root][INFO] - Training Epoch: 1/2, step 2731/7134 completed (loss: 0.7463924288749695, acc: 0.844660222530365)
[2025-02-13 19:13:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:23][root][INFO] - Training Epoch: 1/2, step 2732/7134 completed (loss: 0.648747444152832, acc: 0.8627451062202454)
[2025-02-13 19:13:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:23][root][INFO] - Training Epoch: 1/2, step 2733/7134 completed (loss: 0.46864286065101624, acc: 0.9111111164093018)
[2025-02-13 19:13:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:24][root][INFO] - Training Epoch: 1/2, step 2734/7134 completed (loss: 0.5352543592453003, acc: 0.8677685856819153)
[2025-02-13 19:13:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:24][root][INFO] - Training Epoch: 1/2, step 2735/7134 completed (loss: 0.3415229618549347, acc: 0.9451219439506531)
[2025-02-13 19:13:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:24][root][INFO] - Training Epoch: 1/2, step 2736/7134 completed (loss: 0.6253803968429565, acc: 0.8616352081298828)
[2025-02-13 19:13:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:25][root][INFO] - Training Epoch: 1/2, step 2737/7134 completed (loss: 0.5419325232505798, acc: 0.8928571343421936)
[2025-02-13 19:13:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:25][root][INFO] - Training Epoch: 1/2, step 2738/7134 completed (loss: 0.434123694896698, acc: 0.916167676448822)
[2025-02-13 19:13:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:26][root][INFO] - Training Epoch: 1/2, step 2739/7134 completed (loss: 0.5270330309867859, acc: 0.8623188138008118)
[2025-02-13 19:13:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:26][root][INFO] - Training Epoch: 1/2, step 2740/7134 completed (loss: 0.26696300506591797, acc: 0.9251700639724731)
[2025-02-13 19:13:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:26][root][INFO] - Training Epoch: 1/2, step 2741/7134 completed (loss: 0.27967751026153564, acc: 0.9396551847457886)
[2025-02-13 19:13:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:27][root][INFO] - Training Epoch: 1/2, step 2742/7134 completed (loss: 0.2887280583381653, acc: 0.924369752407074)
[2025-02-13 19:13:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:27][root][INFO] - Training Epoch: 1/2, step 2743/7134 completed (loss: 0.26323169469833374, acc: 0.914893627166748)
[2025-02-13 19:13:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:28][root][INFO] - Training Epoch: 1/2, step 2744/7134 completed (loss: 0.17631162703037262, acc: 0.9354838728904724)
[2025-02-13 19:13:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:28][root][INFO] - Training Epoch: 1/2, step 2745/7134 completed (loss: 0.739047110080719, acc: 0.8214285969734192)
[2025-02-13 19:13:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:28][root][INFO] - Training Epoch: 1/2, step 2746/7134 completed (loss: 0.4573013484477997, acc: 0.875)
[2025-02-13 19:13:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:29][root][INFO] - Training Epoch: 1/2, step 2747/7134 completed (loss: 0.5961209535598755, acc: 0.8981481194496155)
[2025-02-13 19:13:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:29][root][INFO] - Training Epoch: 1/2, step 2748/7134 completed (loss: 0.22450050711631775, acc: 0.948051929473877)
[2025-02-13 19:13:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:30][root][INFO] - Training Epoch: 1/2, step 2749/7134 completed (loss: 0.43566784262657166, acc: 0.9175257682800293)
[2025-02-13 19:13:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:30][root][INFO] - Training Epoch: 1/2, step 2750/7134 completed (loss: 0.265071302652359, acc: 0.9518072009086609)
[2025-02-13 19:13:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:30][root][INFO] - Training Epoch: 1/2, step 2751/7134 completed (loss: 0.525519073009491, acc: 0.8738738894462585)
[2025-02-13 19:13:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:31][root][INFO] - Training Epoch: 1/2, step 2752/7134 completed (loss: 0.34833353757858276, acc: 0.9124087691307068)
[2025-02-13 19:13:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:31][root][INFO] - Training Epoch: 1/2, step 2753/7134 completed (loss: 0.3870839774608612, acc: 0.8907563090324402)
[2025-02-13 19:13:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:32][root][INFO] - Training Epoch: 1/2, step 2754/7134 completed (loss: 0.37242260575294495, acc: 0.9083333611488342)
[2025-02-13 19:13:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:32][root][INFO] - Training Epoch: 1/2, step 2755/7134 completed (loss: 0.29368939995765686, acc: 0.9296875)
[2025-02-13 19:13:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:32][root][INFO] - Training Epoch: 1/2, step 2756/7134 completed (loss: 0.405528724193573, acc: 0.9256756901741028)
[2025-02-13 19:13:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:33][root][INFO] - Training Epoch: 1/2, step 2757/7134 completed (loss: 0.431537926197052, acc: 0.8804348111152649)
[2025-02-13 19:13:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:33][root][INFO] - Training Epoch: 1/2, step 2758/7134 completed (loss: 0.37272587418556213, acc: 0.9262295365333557)
[2025-02-13 19:13:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:33][root][INFO] - Training Epoch: 1/2, step 2759/7134 completed (loss: 0.25405895709991455, acc: 0.9285714030265808)
[2025-02-13 19:13:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:34][root][INFO] - Training Epoch: 1/2, step 2760/7134 completed (loss: 0.2777411937713623, acc: 0.9222221970558167)
[2025-02-13 19:13:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:34][root][INFO] - Training Epoch: 1/2, step 2761/7134 completed (loss: 0.5406571626663208, acc: 0.8684210777282715)
[2025-02-13 19:13:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:35][root][INFO] - Training Epoch: 1/2, step 2762/7134 completed (loss: 0.4310997724533081, acc: 0.9014778137207031)
[2025-02-13 19:13:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:35][root][INFO] - Training Epoch: 1/2, step 2763/7134 completed (loss: 0.23335406184196472, acc: 0.9344262480735779)
[2025-02-13 19:13:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:35][root][INFO] - Training Epoch: 1/2, step 2764/7134 completed (loss: 0.34264981746673584, acc: 0.921875)
[2025-02-13 19:13:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:36][root][INFO] - Training Epoch: 1/2, step 2765/7134 completed (loss: 0.3338364064693451, acc: 0.9235293865203857)
[2025-02-13 19:13:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:36][root][INFO] - Training Epoch: 1/2, step 2766/7134 completed (loss: 0.12609758973121643, acc: 0.9808917045593262)
[2025-02-13 19:13:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:37][root][INFO] - Training Epoch: 1/2, step 2767/7134 completed (loss: 0.22744612395763397, acc: 0.9700000286102295)
[2025-02-13 19:13:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:37][root][INFO] - Training Epoch: 1/2, step 2768/7134 completed (loss: 0.2685423791408539, acc: 0.9397590160369873)
[2025-02-13 19:13:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:37][root][INFO] - Training Epoch: 1/2, step 2769/7134 completed (loss: 0.46280360221862793, acc: 0.9263157844543457)
[2025-02-13 19:13:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:38][root][INFO] - Training Epoch: 1/2, step 2770/7134 completed (loss: 0.2156876027584076, acc: 0.9444444179534912)
[2025-02-13 19:13:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:38][root][INFO] - Training Epoch: 1/2, step 2771/7134 completed (loss: 0.5253968834877014, acc: 0.8862559199333191)
[2025-02-13 19:13:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:39][root][INFO] - Training Epoch: 1/2, step 2772/7134 completed (loss: 0.34844276309013367, acc: 0.9399999976158142)
[2025-02-13 19:13:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:39][root][INFO] - Training Epoch: 1/2, step 2773/7134 completed (loss: 0.33441534638404846, acc: 0.9396985173225403)
[2025-02-13 19:13:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:39][root][INFO] - Training Epoch: 1/2, step 2774/7134 completed (loss: 0.44372138381004333, acc: 0.9219512343406677)
[2025-02-13 19:13:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:40][root][INFO] - Training Epoch: 1/2, step 2775/7134 completed (loss: 0.2911050021648407, acc: 0.9234972596168518)
[2025-02-13 19:13:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:40][root][INFO] - Training Epoch: 1/2, step 2776/7134 completed (loss: 0.1991928219795227, acc: 0.9523809552192688)
[2025-02-13 19:13:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:40][root][INFO] - Training Epoch: 1/2, step 2777/7134 completed (loss: 0.1480233073234558, acc: 0.9657142758369446)
[2025-02-13 19:13:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:41][root][INFO] - Training Epoch: 1/2, step 2778/7134 completed (loss: 0.45971807837486267, acc: 0.9040403962135315)
[2025-02-13 19:13:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:41][root][INFO] - Training Epoch: 1/2, step 2779/7134 completed (loss: 0.2905290424823761, acc: 0.9269406199455261)
[2025-02-13 19:13:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:42][root][INFO] - Training Epoch: 1/2, step 2780/7134 completed (loss: 0.35967695713043213, acc: 0.9115044474601746)
[2025-02-13 19:13:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:42][root][INFO] - Training Epoch: 1/2, step 2781/7134 completed (loss: 0.27408522367477417, acc: 0.9130434989929199)
[2025-02-13 19:13:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:42][root][INFO] - Training Epoch: 1/2, step 2782/7134 completed (loss: 0.14067427814006805, acc: 0.96875)
[2025-02-13 19:13:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:43][root][INFO] - Training Epoch: 1/2, step 2783/7134 completed (loss: 0.16760961711406708, acc: 0.9727891087532043)
[2025-02-13 19:13:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:43][root][INFO] - Training Epoch: 1/2, step 2784/7134 completed (loss: 0.17867588996887207, acc: 0.9613526463508606)
[2025-02-13 19:13:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:43][root][INFO] - Training Epoch: 1/2, step 2785/7134 completed (loss: 0.20006166398525238, acc: 0.9514563083648682)
[2025-02-13 19:13:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:44][root][INFO] - Training Epoch: 1/2, step 2786/7134 completed (loss: 0.3376573324203491, acc: 0.9111111164093018)
[2025-02-13 19:13:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:44][root][INFO] - Training Epoch: 1/2, step 2787/7134 completed (loss: 0.22276146709918976, acc: 0.9421965479850769)
[2025-02-13 19:13:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:45][root][INFO] - Training Epoch: 1/2, step 2788/7134 completed (loss: 0.43670421838760376, acc: 0.8789808750152588)
[2025-02-13 19:13:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:45][root][INFO] - Training Epoch: 1/2, step 2789/7134 completed (loss: 1.8324265480041504, acc: 0.561904788017273)
[2025-02-13 19:13:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:45][root][INFO] - Training Epoch: 1/2, step 2790/7134 completed (loss: 1.4313806295394897, acc: 0.675000011920929)
[2025-02-13 19:13:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:46][root][INFO] - Training Epoch: 1/2, step 2791/7134 completed (loss: 0.5195672512054443, acc: 0.8741722106933594)
[2025-02-13 19:13:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:46][root][INFO] - Training Epoch: 1/2, step 2792/7134 completed (loss: 0.23651054501533508, acc: 0.9360465407371521)
[2025-02-13 19:13:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:47][root][INFO] - Training Epoch: 1/2, step 2793/7134 completed (loss: 0.21699386835098267, acc: 0.9520958065986633)
[2025-02-13 19:13:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:47][root][INFO] - Training Epoch: 1/2, step 2794/7134 completed (loss: 0.17343725264072418, acc: 0.939393937587738)
[2025-02-13 19:13:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:47][root][INFO] - Training Epoch: 1/2, step 2795/7134 completed (loss: 0.5782942771911621, acc: 0.8703703880310059)
[2025-02-13 19:13:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:48][root][INFO] - Training Epoch: 1/2, step 2796/7134 completed (loss: 0.3766981363296509, acc: 0.8943089246749878)
[2025-02-13 19:13:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:48][root][INFO] - Training Epoch: 1/2, step 2797/7134 completed (loss: 0.6669228076934814, acc: 0.8536585569381714)
[2025-02-13 19:13:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:49][root][INFO] - Training Epoch: 1/2, step 2798/7134 completed (loss: 0.4517778158187866, acc: 0.8916666507720947)
[2025-02-13 19:13:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:49][root][INFO] - Training Epoch: 1/2, step 2799/7134 completed (loss: 0.3491816222667694, acc: 0.921875)
[2025-02-13 19:13:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:49][root][INFO] - Training Epoch: 1/2, step 2800/7134 completed (loss: 0.33263155817985535, acc: 0.8978102207183838)
[2025-02-13 19:13:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:50][root][INFO] - Training Epoch: 1/2, step 2801/7134 completed (loss: 0.4061649441719055, acc: 0.8961748480796814)
[2025-02-13 19:13:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:50][root][INFO] - Training Epoch: 1/2, step 2802/7134 completed (loss: 0.4132550358772278, acc: 0.9045225977897644)
[2025-02-13 19:13:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:50][root][INFO] - Training Epoch: 1/2, step 2803/7134 completed (loss: 0.14305812120437622, acc: 0.9629629850387573)
[2025-02-13 19:13:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:51][root][INFO] - Training Epoch: 1/2, step 2804/7134 completed (loss: 0.18691828846931458, acc: 0.9653179049491882)
[2025-02-13 19:13:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:51][root][INFO] - Training Epoch: 1/2, step 2805/7134 completed (loss: 0.27634909749031067, acc: 0.9476743936538696)
[2025-02-13 19:13:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:52][root][INFO] - Training Epoch: 1/2, step 2806/7134 completed (loss: 0.25027069449424744, acc: 0.9338235259056091)
[2025-02-13 19:13:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:52][root][INFO] - Training Epoch: 1/2, step 2807/7134 completed (loss: 0.2265656441450119, acc: 0.9611650705337524)
[2025-02-13 19:13:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:52][root][INFO] - Training Epoch: 1/2, step 2808/7134 completed (loss: 0.38393285870552063, acc: 0.9195979833602905)
[2025-02-13 19:13:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:53][root][INFO] - Training Epoch: 1/2, step 2809/7134 completed (loss: 0.28915634751319885, acc: 0.9291338324546814)
[2025-02-13 19:13:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:53][root][INFO] - Training Epoch: 1/2, step 2810/7134 completed (loss: 0.24172785878181458, acc: 0.9562841653823853)
[2025-02-13 19:13:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:54][root][INFO] - Training Epoch: 1/2, step 2811/7134 completed (loss: 0.10955080389976501, acc: 0.9668508172035217)
[2025-02-13 19:13:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:54][root][INFO] - Training Epoch: 1/2, step 2812/7134 completed (loss: 0.08268088847398758, acc: 0.9801324605941772)
[2025-02-13 19:13:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:54][root][INFO] - Training Epoch: 1/2, step 2813/7134 completed (loss: 0.25943371653556824, acc: 0.9482758641242981)
[2025-02-13 19:13:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:55][root][INFO] - Training Epoch: 1/2, step 2814/7134 completed (loss: 0.25347939133644104, acc: 0.9476743936538696)
[2025-02-13 19:13:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:55][root][INFO] - Training Epoch: 1/2, step 2815/7134 completed (loss: 0.3330243229866028, acc: 0.9505494236946106)
[2025-02-13 19:13:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:56][root][INFO] - Training Epoch: 1/2, step 2816/7134 completed (loss: 0.22986635565757751, acc: 0.9346405267715454)
[2025-02-13 19:13:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:56][root][INFO] - Training Epoch: 1/2, step 2817/7134 completed (loss: 0.2565135955810547, acc: 0.9441624283790588)
[2025-02-13 19:13:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:56][root][INFO] - Training Epoch: 1/2, step 2818/7134 completed (loss: 0.200368732213974, acc: 0.957446813583374)
[2025-02-13 19:13:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:57][root][INFO] - Training Epoch: 1/2, step 2819/7134 completed (loss: 0.14164723455905914, acc: 0.9520547986030579)
[2025-02-13 19:13:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:57][root][INFO] - Training Epoch: 1/2, step 2820/7134 completed (loss: 0.46847590804100037, acc: 0.8895705342292786)
[2025-02-13 19:13:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:57][root][INFO] - Training Epoch: 1/2, step 2821/7134 completed (loss: 0.4600365161895752, acc: 0.9067357778549194)
[2025-02-13 19:13:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:58][root][INFO] - Training Epoch: 1/2, step 2822/7134 completed (loss: 0.5024681091308594, acc: 0.9243243336677551)
[2025-02-13 19:13:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:58][root][INFO] - Training Epoch: 1/2, step 2823/7134 completed (loss: 0.37550127506256104, acc: 0.9192546606063843)
[2025-02-13 19:13:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:59][root][INFO] - Training Epoch: 1/2, step 2824/7134 completed (loss: 0.8597309589385986, acc: 0.8113207817077637)
[2025-02-13 19:13:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:59][root][INFO] - Training Epoch: 1/2, step 2825/7134 completed (loss: 0.23318222165107727, acc: 0.9444444179534912)
[2025-02-13 19:13:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:13:59][root][INFO] - Training Epoch: 1/2, step 2826/7134 completed (loss: 0.38468512892723083, acc: 0.9022988677024841)
[2025-02-13 19:14:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:00][root][INFO] - Training Epoch: 1/2, step 2827/7134 completed (loss: 0.2643072307109833, acc: 0.9377990365028381)
[2025-02-13 19:14:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:00][root][INFO] - Training Epoch: 1/2, step 2828/7134 completed (loss: 0.2705850899219513, acc: 0.9303797483444214)
[2025-02-13 19:14:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:01][root][INFO] - Training Epoch: 1/2, step 2829/7134 completed (loss: 0.2695736885070801, acc: 0.9464285969734192)
[2025-02-13 19:14:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:01][root][INFO] - Training Epoch: 1/2, step 2830/7134 completed (loss: 0.09259558469057083, acc: 0.9701492786407471)
[2025-02-13 19:14:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:01][root][INFO] - Training Epoch: 1/2, step 2831/7134 completed (loss: 0.2170349359512329, acc: 0.9459459185600281)
[2025-02-13 19:14:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:02][root][INFO] - Training Epoch: 1/2, step 2832/7134 completed (loss: 0.3003520667552948, acc: 0.9301075339317322)
[2025-02-13 19:14:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:02][root][INFO] - Training Epoch: 1/2, step 2833/7134 completed (loss: 0.27989107370376587, acc: 0.9470899701118469)
[2025-02-13 19:14:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:03][root][INFO] - Training Epoch: 1/2, step 2834/7134 completed (loss: 0.30367353558540344, acc: 0.9333333373069763)
[2025-02-13 19:14:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:03][root][INFO] - Training Epoch: 1/2, step 2835/7134 completed (loss: 0.15751546621322632, acc: 0.9407894611358643)
[2025-02-13 19:14:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:03][root][INFO] - Training Epoch: 1/2, step 2836/7134 completed (loss: 0.3651960492134094, acc: 0.9300000071525574)
[2025-02-13 19:14:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:04][root][INFO] - Training Epoch: 1/2, step 2837/7134 completed (loss: 0.1970522403717041, acc: 0.9532163739204407)
[2025-02-13 19:14:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:04][root][INFO] - Training Epoch: 1/2, step 2838/7134 completed (loss: 0.2512798309326172, acc: 0.9507389068603516)
[2025-02-13 19:14:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:05][root][INFO] - Training Epoch: 1/2, step 2839/7134 completed (loss: 0.3374517261981964, acc: 0.9253731369972229)
[2025-02-13 19:14:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:05][root][INFO] - Training Epoch: 1/2, step 2840/7134 completed (loss: 0.19635115563869476, acc: 0.9529411792755127)
[2025-02-13 19:14:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:05][root][INFO] - Training Epoch: 1/2, step 2841/7134 completed (loss: 0.25210899114608765, acc: 0.9433962106704712)
[2025-02-13 19:14:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:06][root][INFO] - Training Epoch: 1/2, step 2842/7134 completed (loss: 0.3508130609989166, acc: 0.9166666865348816)
[2025-02-13 19:14:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:06][root][INFO] - Training Epoch: 1/2, step 2843/7134 completed (loss: 0.2131357043981552, acc: 0.9433962106704712)
[2025-02-13 19:14:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:07][root][INFO] - Training Epoch: 1/2, step 2844/7134 completed (loss: 0.5008739233016968, acc: 0.885869562625885)
[2025-02-13 19:14:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:07][root][INFO] - Training Epoch: 1/2, step 2845/7134 completed (loss: 0.27203166484832764, acc: 0.9256756901741028)
[2025-02-13 19:14:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:07][root][INFO] - Training Epoch: 1/2, step 2846/7134 completed (loss: 0.22703945636749268, acc: 0.9346405267715454)
[2025-02-13 19:14:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:08][root][INFO] - Training Epoch: 1/2, step 2847/7134 completed (loss: 0.17774711549282074, acc: 0.9745222926139832)
[2025-02-13 19:14:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:08][root][INFO] - Training Epoch: 1/2, step 2848/7134 completed (loss: 0.28667131066322327, acc: 0.9082568883895874)
[2025-02-13 19:14:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:09][root][INFO] - Training Epoch: 1/2, step 2849/7134 completed (loss: 0.3006226122379303, acc: 0.9314285516738892)
[2025-02-13 19:14:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:09][root][INFO] - Training Epoch: 1/2, step 2850/7134 completed (loss: 0.5511978268623352, acc: 0.8632478713989258)
[2025-02-13 19:14:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:09][root][INFO] - Training Epoch: 1/2, step 2851/7134 completed (loss: 0.34313833713531494, acc: 0.9111111164093018)
[2025-02-13 19:14:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:10][root][INFO] - Training Epoch: 1/2, step 2852/7134 completed (loss: 0.319560706615448, acc: 0.934959352016449)
[2025-02-13 19:14:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:10][root][INFO] - Training Epoch: 1/2, step 2853/7134 completed (loss: 0.16928181052207947, acc: 0.9591836929321289)
[2025-02-13 19:14:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:11][root][INFO] - Training Epoch: 1/2, step 2854/7134 completed (loss: 0.5254523158073425, acc: 0.9047619104385376)
[2025-02-13 19:14:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:11][root][INFO] - Training Epoch: 1/2, step 2855/7134 completed (loss: 0.1992189884185791, acc: 0.9527027010917664)
[2025-02-13 19:14:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:11][root][INFO] - Training Epoch: 1/2, step 2856/7134 completed (loss: 0.1802331954240799, acc: 0.9813664555549622)
[2025-02-13 19:14:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:12][root][INFO] - Training Epoch: 1/2, step 2857/7134 completed (loss: 0.27741315960884094, acc: 0.9520958065986633)
[2025-02-13 19:14:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:12][root][INFO] - Training Epoch: 1/2, step 2858/7134 completed (loss: 0.1521945744752884, acc: 0.9545454382896423)
[2025-02-13 19:14:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:13][root][INFO] - Training Epoch: 1/2, step 2859/7134 completed (loss: 0.5015799403190613, acc: 0.9047619104385376)
[2025-02-13 19:14:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:13][root][INFO] - Training Epoch: 1/2, step 2860/7134 completed (loss: 0.33158236742019653, acc: 0.9140625)
[2025-02-13 19:14:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:13][root][INFO] - Training Epoch: 1/2, step 2861/7134 completed (loss: 0.1778733730316162, acc: 0.9464285969734192)
[2025-02-13 19:14:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:14][root][INFO] - Training Epoch: 1/2, step 2862/7134 completed (loss: 0.16987545788288116, acc: 0.9624060392379761)
[2025-02-13 19:14:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:14][root][INFO] - Training Epoch: 1/2, step 2863/7134 completed (loss: 0.1641441434621811, acc: 0.950276255607605)
[2025-02-13 19:14:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:14][root][INFO] - Training Epoch: 1/2, step 2864/7134 completed (loss: 0.18340006470680237, acc: 0.9684210419654846)
[2025-02-13 19:14:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:15][root][INFO] - Training Epoch: 1/2, step 2865/7134 completed (loss: 0.20985636115074158, acc: 0.9304347634315491)
[2025-02-13 19:14:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:15][root][INFO] - Training Epoch: 1/2, step 2866/7134 completed (loss: 0.17392051219940186, acc: 0.9677419066429138)
[2025-02-13 19:14:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:16][root][INFO] - Training Epoch: 1/2, step 2867/7134 completed (loss: 0.1322382539510727, acc: 0.9753086566925049)
[2025-02-13 19:14:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:16][root][INFO] - Training Epoch: 1/2, step 2868/7134 completed (loss: 0.20288102328777313, acc: 0.9411764740943909)
[2025-02-13 19:14:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:16][root][INFO] - Training Epoch: 1/2, step 2869/7134 completed (loss: 0.25578275322914124, acc: 0.9655172228813171)
[2025-02-13 19:14:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:17][root][INFO] - Training Epoch: 1/2, step 2870/7134 completed (loss: 0.6043803691864014, acc: 0.918181836605072)
[2025-02-13 19:14:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:17][root][INFO] - Training Epoch: 1/2, step 2871/7134 completed (loss: 0.224842831492424, acc: 0.9448275566101074)
[2025-02-13 19:14:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:17][root][INFO] - Training Epoch: 1/2, step 2872/7134 completed (loss: 0.14609962701797485, acc: 0.9734513163566589)
[2025-02-13 19:14:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:18][root][INFO] - Training Epoch: 1/2, step 2873/7134 completed (loss: 0.2523263990879059, acc: 0.9457364082336426)
[2025-02-13 19:14:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:18][root][INFO] - Training Epoch: 1/2, step 2874/7134 completed (loss: 0.12519249320030212, acc: 0.9583333134651184)
[2025-02-13 19:14:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:19][root][INFO] - Training Epoch: 1/2, step 2875/7134 completed (loss: 0.25134390592575073, acc: 0.9285714030265808)
[2025-02-13 19:14:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:19][root][INFO] - Training Epoch: 1/2, step 2876/7134 completed (loss: 0.3973556160926819, acc: 0.9056603908538818)
[2025-02-13 19:14:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:19][root][INFO] - Training Epoch: 1/2, step 2877/7134 completed (loss: 0.15089371800422668, acc: 0.9658119678497314)
[2025-02-13 19:14:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:20][root][INFO] - Training Epoch: 1/2, step 2878/7134 completed (loss: 0.14724916219711304, acc: 0.9736841917037964)
[2025-02-13 19:14:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:20][root][INFO] - Training Epoch: 1/2, step 2879/7134 completed (loss: 0.1561093032360077, acc: 0.9591836929321289)
[2025-02-13 19:14:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:21][root][INFO] - Training Epoch: 1/2, step 2880/7134 completed (loss: 0.09352920204401016, acc: 0.9794520735740662)
[2025-02-13 19:14:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:21][root][INFO] - Training Epoch: 1/2, step 2881/7134 completed (loss: 0.2663162052631378, acc: 0.9285714030265808)
[2025-02-13 19:14:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:22][root][INFO] - Training Epoch: 1/2, step 2882/7134 completed (loss: 0.4052645266056061, acc: 0.8959537744522095)
[2025-02-13 19:14:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:22][root][INFO] - Training Epoch: 1/2, step 2883/7134 completed (loss: 0.4052324891090393, acc: 0.916167676448822)
[2025-02-13 19:14:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:22][root][INFO] - Training Epoch: 1/2, step 2884/7134 completed (loss: 0.3980882167816162, acc: 0.939393937587738)
[2025-02-13 19:14:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:23][root][INFO] - Training Epoch: 1/2, step 2885/7134 completed (loss: 0.5950989723205566, acc: 0.8727272748947144)
[2025-02-13 19:14:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:23][root][INFO] - Training Epoch: 1/2, step 2886/7134 completed (loss: 0.44129031896591187, acc: 0.9058823585510254)
[2025-02-13 19:14:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:23][root][INFO] - Training Epoch: 1/2, step 2887/7134 completed (loss: 0.39629510045051575, acc: 0.9170305728912354)
[2025-02-13 19:14:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:24][root][INFO] - Training Epoch: 1/2, step 2888/7134 completed (loss: 0.3731203079223633, acc: 0.9105263352394104)
[2025-02-13 19:14:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:24][root][INFO] - Training Epoch: 1/2, step 2889/7134 completed (loss: 0.44185978174209595, acc: 0.8994413614273071)
[2025-02-13 19:14:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:25][root][INFO] - Training Epoch: 1/2, step 2890/7134 completed (loss: 0.24681468307971954, acc: 0.9384615421295166)
[2025-02-13 19:14:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:25][root][INFO] - Training Epoch: 1/2, step 2891/7134 completed (loss: 0.3399149179458618, acc: 0.9259259104728699)
[2025-02-13 19:14:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:25][root][INFO] - Training Epoch: 1/2, step 2892/7134 completed (loss: 0.3925624191761017, acc: 0.8994082808494568)
[2025-02-13 19:14:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:26][root][INFO] - Training Epoch: 1/2, step 2893/7134 completed (loss: 0.35596203804016113, acc: 0.8852459192276001)
[2025-02-13 19:14:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:26][root][INFO] - Training Epoch: 1/2, step 2894/7134 completed (loss: 0.40199410915374756, acc: 0.9037036895751953)
[2025-02-13 19:14:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:27][root][INFO] - Training Epoch: 1/2, step 2895/7134 completed (loss: 0.11973430961370468, acc: 0.9745762944221497)
[2025-02-13 19:14:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:27][root][INFO] - Training Epoch: 1/2, step 2896/7134 completed (loss: 0.19878429174423218, acc: 0.940119743347168)
[2025-02-13 19:14:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:27][root][INFO] - Training Epoch: 1/2, step 2897/7134 completed (loss: 0.47175726294517517, acc: 0.8872180581092834)
[2025-02-13 19:14:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:28][root][INFO] - Training Epoch: 1/2, step 2898/7134 completed (loss: 0.18912287056446075, acc: 0.967391312122345)
[2025-02-13 19:14:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:28][root][INFO] - Training Epoch: 1/2, step 2899/7134 completed (loss: 0.20806781947612762, acc: 0.9520958065986633)
[2025-02-13 19:14:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:28][root][INFO] - Training Epoch: 1/2, step 2900/7134 completed (loss: 0.23627537488937378, acc: 0.957446813583374)
[2025-02-13 19:14:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:29][root][INFO] - Training Epoch: 1/2, step 2901/7134 completed (loss: 0.24650682508945465, acc: 0.9662162065505981)
[2025-02-13 19:14:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:29][root][INFO] - Training Epoch: 1/2, step 2902/7134 completed (loss: 0.19445064663887024, acc: 0.9487179517745972)
[2025-02-13 19:14:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:30][root][INFO] - Training Epoch: 1/2, step 2903/7134 completed (loss: 0.17611242830753326, acc: 0.9606741666793823)
[2025-02-13 19:14:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:30][root][INFO] - Training Epoch: 1/2, step 2904/7134 completed (loss: 0.14675579965114594, acc: 0.9513513445854187)
[2025-02-13 19:14:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:30][root][INFO] - Training Epoch: 1/2, step 2905/7134 completed (loss: 0.2793191373348236, acc: 0.9465240836143494)
[2025-02-13 19:14:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:31][root][INFO] - Training Epoch: 1/2, step 2906/7134 completed (loss: 0.16579732298851013, acc: 0.9790576100349426)
[2025-02-13 19:14:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:31][root][INFO] - Training Epoch: 1/2, step 2907/7134 completed (loss: 0.21100609004497528, acc: 0.940119743347168)
[2025-02-13 19:14:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:32][root][INFO] - Training Epoch: 1/2, step 2908/7134 completed (loss: 0.21781134605407715, acc: 0.954023003578186)
[2025-02-13 19:14:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:32][root][INFO] - Training Epoch: 1/2, step 2909/7134 completed (loss: 0.1939079612493515, acc: 0.9726775884628296)
[2025-02-13 19:14:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:32][root][INFO] - Training Epoch: 1/2, step 2910/7134 completed (loss: 0.2214914858341217, acc: 0.9465240836143494)
[2025-02-13 19:14:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:33][root][INFO] - Training Epoch: 1/2, step 2911/7134 completed (loss: 0.18037058413028717, acc: 0.9580419659614563)
[2025-02-13 19:14:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:33][root][INFO] - Training Epoch: 1/2, step 2912/7134 completed (loss: 0.18611983954906464, acc: 0.9452054500579834)
[2025-02-13 19:14:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:34][root][INFO] - Training Epoch: 1/2, step 2913/7134 completed (loss: 0.5025227069854736, acc: 0.8776978254318237)
[2025-02-13 19:14:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:34][root][INFO] - Training Epoch: 1/2, step 2914/7134 completed (loss: 0.23882876336574554, acc: 0.9272727370262146)
[2025-02-13 19:14:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:34][root][INFO] - Training Epoch: 1/2, step 2915/7134 completed (loss: 0.46605268120765686, acc: 0.9103448390960693)
[2025-02-13 19:14:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:35][root][INFO] - Training Epoch: 1/2, step 2916/7134 completed (loss: 0.24300913512706757, acc: 0.9298245906829834)
[2025-02-13 19:14:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:35][root][INFO] - Training Epoch: 1/2, step 2917/7134 completed (loss: 0.19776344299316406, acc: 0.9561403393745422)
[2025-02-13 19:14:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:36][root][INFO] - Training Epoch: 1/2, step 2918/7134 completed (loss: 0.24997450411319733, acc: 0.9181286692619324)
[2025-02-13 19:14:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:36][root][INFO] - Training Epoch: 1/2, step 2919/7134 completed (loss: 0.24172690510749817, acc: 0.9337349534034729)
[2025-02-13 19:14:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:36][root][INFO] - Training Epoch: 1/2, step 2920/7134 completed (loss: 0.1934998482465744, acc: 0.9371069073677063)
[2025-02-13 19:14:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:37][root][INFO] - Training Epoch: 1/2, step 2921/7134 completed (loss: 0.22914338111877441, acc: 0.9642857313156128)
[2025-02-13 19:14:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:37][root][INFO] - Training Epoch: 1/2, step 2922/7134 completed (loss: 0.45225369930267334, acc: 0.9202454090118408)
[2025-02-13 19:14:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:38][root][INFO] - Training Epoch: 1/2, step 2923/7134 completed (loss: 0.7751361131668091, acc: 0.8661971688270569)
[2025-02-13 19:14:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:38][root][INFO] - Training Epoch: 1/2, step 2924/7134 completed (loss: 0.27927911281585693, acc: 0.9451219439506531)
[2025-02-13 19:14:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:38][root][INFO] - Training Epoch: 1/2, step 2925/7134 completed (loss: 0.22355054318904877, acc: 0.970588207244873)
[2025-02-13 19:14:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:39][root][INFO] - Training Epoch: 1/2, step 2926/7134 completed (loss: 0.6806032061576843, acc: 0.8994975090026855)
[2025-02-13 19:14:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:39][root][INFO] - Training Epoch: 1/2, step 2927/7134 completed (loss: 0.1957177221775055, acc: 0.9710982441902161)
[2025-02-13 19:14:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:40][root][INFO] - Training Epoch: 1/2, step 2928/7134 completed (loss: 0.4253428876399994, acc: 0.9027777910232544)
[2025-02-13 19:14:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:40][root][INFO] - Training Epoch: 1/2, step 2929/7134 completed (loss: 0.15393602848052979, acc: 0.9583333134651184)
[2025-02-13 19:14:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:40][root][INFO] - Training Epoch: 1/2, step 2930/7134 completed (loss: 0.21544204652309418, acc: 0.9455782175064087)
[2025-02-13 19:14:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:41][root][INFO] - Training Epoch: 1/2, step 2931/7134 completed (loss: 0.5402212142944336, acc: 0.8717948794364929)
[2025-02-13 19:14:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:41][root][INFO] - Training Epoch: 1/2, step 2932/7134 completed (loss: 0.26876816153526306, acc: 0.9144384860992432)
[2025-02-13 19:14:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:42][root][INFO] - Training Epoch: 1/2, step 2933/7134 completed (loss: 0.19755816459655762, acc: 0.9489051103591919)
[2025-02-13 19:14:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:42][root][INFO] - Training Epoch: 1/2, step 2934/7134 completed (loss: 0.16869719326496124, acc: 0.9642857313156128)
[2025-02-13 19:14:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:43][root][INFO] - Training Epoch: 1/2, step 2935/7134 completed (loss: 0.1322992593050003, acc: 0.9735449552536011)
[2025-02-13 19:14:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:43][root][INFO] - Training Epoch: 1/2, step 2936/7134 completed (loss: 0.3214484751224518, acc: 0.9336283206939697)
[2025-02-13 19:14:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:43][root][INFO] - Training Epoch: 1/2, step 2937/7134 completed (loss: 0.2474837601184845, acc: 0.9338235259056091)
[2025-02-13 19:14:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:44][root][INFO] - Training Epoch: 1/2, step 2938/7134 completed (loss: 0.30259403586387634, acc: 0.9344262480735779)
[2025-02-13 19:14:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:44][root][INFO] - Training Epoch: 1/2, step 2939/7134 completed (loss: 0.21958377957344055, acc: 0.940397322177887)
[2025-02-13 19:14:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:45][root][INFO] - Training Epoch: 1/2, step 2940/7134 completed (loss: 0.25232985615730286, acc: 0.9453551769256592)
[2025-02-13 19:14:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:45][root][INFO] - Training Epoch: 1/2, step 2941/7134 completed (loss: 0.3268018364906311, acc: 0.9213483333587646)
[2025-02-13 19:14:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:45][root][INFO] - Training Epoch: 1/2, step 2942/7134 completed (loss: 0.1836043894290924, acc: 0.9454545378684998)
[2025-02-13 19:14:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:46][root][INFO] - Training Epoch: 1/2, step 2943/7134 completed (loss: 0.3010134994983673, acc: 0.9054054021835327)
[2025-02-13 19:14:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:46][root][INFO] - Training Epoch: 1/2, step 2944/7134 completed (loss: 0.22853431105613708, acc: 0.9397590160369873)
[2025-02-13 19:14:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:47][root][INFO] - Training Epoch: 1/2, step 2945/7134 completed (loss: 0.3214209973812103, acc: 0.9395973086357117)
[2025-02-13 19:14:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:47][root][INFO] - Training Epoch: 1/2, step 2946/7134 completed (loss: 0.32254841923713684, acc: 0.9197530746459961)
[2025-02-13 19:14:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:47][root][INFO] - Training Epoch: 1/2, step 2947/7134 completed (loss: 0.3222264349460602, acc: 0.918749988079071)
[2025-02-13 19:14:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:48][root][INFO] - Training Epoch: 1/2, step 2948/7134 completed (loss: 0.2515937089920044, acc: 0.9166666865348816)
[2025-02-13 19:14:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:48][root][INFO] - Training Epoch: 1/2, step 2949/7134 completed (loss: 0.13698799908161163, acc: 0.9673202633857727)
[2025-02-13 19:14:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:49][root][INFO] - Training Epoch: 1/2, step 2950/7134 completed (loss: 0.2619315981864929, acc: 0.9385474920272827)
[2025-02-13 19:14:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:49][root][INFO] - Training Epoch: 1/2, step 2951/7134 completed (loss: 0.16550523042678833, acc: 0.95652174949646)
[2025-02-13 19:14:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:49][root][INFO] - Training Epoch: 1/2, step 2952/7134 completed (loss: 0.1544286608695984, acc: 0.9490445852279663)
[2025-02-13 19:14:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:50][root][INFO] - Training Epoch: 1/2, step 2953/7134 completed (loss: 0.17896266281604767, acc: 0.9591836929321289)
[2025-02-13 19:14:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:50][root][INFO] - Training Epoch: 1/2, step 2954/7134 completed (loss: 0.2508990168571472, acc: 0.9278350472450256)
[2025-02-13 19:14:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:51][root][INFO] - Training Epoch: 1/2, step 2955/7134 completed (loss: 0.2514478862285614, acc: 0.9515151381492615)
[2025-02-13 19:14:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:51][root][INFO] - Training Epoch: 1/2, step 2956/7134 completed (loss: 0.1351431906223297, acc: 0.9570552110671997)
[2025-02-13 19:14:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:51][root][INFO] - Training Epoch: 1/2, step 2957/7134 completed (loss: 0.16627225279808044, acc: 0.9655172228813171)
[2025-02-13 19:14:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:52][root][INFO] - Training Epoch: 1/2, step 2958/7134 completed (loss: 0.0976661890745163, acc: 0.9722222089767456)
[2025-02-13 19:14:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:52][root][INFO] - Training Epoch: 1/2, step 2959/7134 completed (loss: 0.12072274833917618, acc: 0.9620253443717957)
[2025-02-13 19:14:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:53][root][INFO] - Training Epoch: 1/2, step 2960/7134 completed (loss: 0.2085256278514862, acc: 0.9340659379959106)
[2025-02-13 19:14:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:53][root][INFO] - Training Epoch: 1/2, step 2961/7134 completed (loss: 0.25145477056503296, acc: 0.9520958065986633)
[2025-02-13 19:14:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:54][root][INFO] - Training Epoch: 1/2, step 2962/7134 completed (loss: 0.14351435005664825, acc: 0.9702380895614624)
[2025-02-13 19:14:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:54][root][INFO] - Training Epoch: 1/2, step 2963/7134 completed (loss: 0.20148040354251862, acc: 0.9385474920272827)
[2025-02-13 19:14:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:55][root][INFO] - Training Epoch: 1/2, step 2964/7134 completed (loss: 0.0709557756781578, acc: 0.9866666793823242)
[2025-02-13 19:14:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:55][root][INFO] - Training Epoch: 1/2, step 2965/7134 completed (loss: 0.2183011919260025, acc: 0.9545454382896423)
[2025-02-13 19:14:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:55][root][INFO] - Training Epoch: 1/2, step 2966/7134 completed (loss: 0.09995108842849731, acc: 0.9743589758872986)
[2025-02-13 19:14:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:56][root][INFO] - Training Epoch: 1/2, step 2967/7134 completed (loss: 0.4678648114204407, acc: 0.8907103538513184)
[2025-02-13 19:14:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:56][root][INFO] - Training Epoch: 1/2, step 2968/7134 completed (loss: 0.6902749538421631, acc: 0.862500011920929)
[2025-02-13 19:14:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:57][root][INFO] - Training Epoch: 1/2, step 2969/7134 completed (loss: 1.431002140045166, acc: 0.7815533876419067)
[2025-02-13 19:14:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:57][root][INFO] - Training Epoch: 1/2, step 2970/7134 completed (loss: 1.4800323247909546, acc: 0.7388059496879578)
[2025-02-13 19:14:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:57][root][INFO] - Training Epoch: 1/2, step 2971/7134 completed (loss: 0.33201009035110474, acc: 0.9281768202781677)
[2025-02-13 19:14:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:58][root][INFO] - Training Epoch: 1/2, step 2972/7134 completed (loss: 0.4563407301902771, acc: 0.8799999952316284)
[2025-02-13 19:14:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:58][root][INFO] - Training Epoch: 1/2, step 2973/7134 completed (loss: 0.23468706011772156, acc: 0.9454545378684998)
[2025-02-13 19:14:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:59][root][INFO] - Training Epoch: 1/2, step 2974/7134 completed (loss: 0.32994338870048523, acc: 0.9239766001701355)
[2025-02-13 19:14:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:59][root][INFO] - Training Epoch: 1/2, step 2975/7134 completed (loss: 0.2860020697116852, acc: 0.9069767594337463)
[2025-02-13 19:14:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:14:59][root][INFO] - Training Epoch: 1/2, step 2976/7134 completed (loss: 0.25190141797065735, acc: 0.9386792182922363)
[2025-02-13 19:14:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:00][root][INFO] - Training Epoch: 1/2, step 2977/7134 completed (loss: 0.22238948941230774, acc: 0.9448275566101074)
[2025-02-13 19:15:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:00][root][INFO] - Training Epoch: 1/2, step 2978/7134 completed (loss: 0.3236532509326935, acc: 0.9277108311653137)
[2025-02-13 19:15:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:01][root][INFO] - Training Epoch: 1/2, step 2979/7134 completed (loss: 0.27340763807296753, acc: 0.931034505367279)
[2025-02-13 19:15:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:01][root][INFO] - Training Epoch: 1/2, step 2980/7134 completed (loss: 0.39019230008125305, acc: 0.8914285898208618)
[2025-02-13 19:15:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:01][root][INFO] - Training Epoch: 1/2, step 2981/7134 completed (loss: 0.44797366857528687, acc: 0.931034505367279)
[2025-02-13 19:15:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:02][root][INFO] - Training Epoch: 1/2, step 2982/7134 completed (loss: 0.5477514266967773, acc: 0.8626373410224915)
[2025-02-13 19:15:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:02][root][INFO] - Training Epoch: 1/2, step 2983/7134 completed (loss: 0.36239445209503174, acc: 0.907975435256958)
[2025-02-13 19:15:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:03][root][INFO] - Training Epoch: 1/2, step 2984/7134 completed (loss: 0.41272562742233276, acc: 0.8901098966598511)
[2025-02-13 19:15:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:03][root][INFO] - Training Epoch: 1/2, step 2985/7134 completed (loss: 0.2044403851032257, acc: 0.9512194991111755)
[2025-02-13 19:15:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:03][root][INFO] - Training Epoch: 1/2, step 2986/7134 completed (loss: 0.20684832334518433, acc: 0.9432989954948425)
[2025-02-13 19:15:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:04][root][INFO] - Training Epoch: 1/2, step 2987/7134 completed (loss: 0.23650430142879486, acc: 0.967391312122345)
[2025-02-13 19:15:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:04][root][INFO] - Training Epoch: 1/2, step 2988/7134 completed (loss: 0.2262326180934906, acc: 0.9459459185600281)
[2025-02-13 19:15:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:04][root][INFO] - Training Epoch: 1/2, step 2989/7134 completed (loss: 0.2690623700618744, acc: 0.916167676448822)
[2025-02-13 19:15:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:05][root][INFO] - Training Epoch: 1/2, step 2990/7134 completed (loss: 0.5027658343315125, acc: 0.8918918967247009)
[2025-02-13 19:15:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:05][root][INFO] - Training Epoch: 1/2, step 2991/7134 completed (loss: 1.00448739528656, acc: 0.765625)
[2025-02-13 19:15:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:05][root][INFO] - Training Epoch: 1/2, step 2992/7134 completed (loss: 0.5136799812316895, acc: 0.8993710875511169)
[2025-02-13 19:15:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:06][root][INFO] - Training Epoch: 1/2, step 2993/7134 completed (loss: 0.29291602969169617, acc: 0.9246231317520142)
[2025-02-13 19:15:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:06][root][INFO] - Training Epoch: 1/2, step 2994/7134 completed (loss: 0.21734006702899933, acc: 0.9603174328804016)
[2025-02-13 19:15:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:07][root][INFO] - Training Epoch: 1/2, step 2995/7134 completed (loss: 0.27996572852134705, acc: 0.9563106894493103)
[2025-02-13 19:15:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:07][root][INFO] - Training Epoch: 1/2, step 2996/7134 completed (loss: 0.2138487696647644, acc: 0.9278350472450256)
[2025-02-13 19:15:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:07][root][INFO] - Training Epoch: 1/2, step 2997/7134 completed (loss: 0.13428165018558502, acc: 0.9669811129570007)
[2025-02-13 19:15:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:08][root][INFO] - Training Epoch: 1/2, step 2998/7134 completed (loss: 0.2465287148952484, acc: 0.9523809552192688)
[2025-02-13 19:15:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:08][root][INFO] - Training Epoch: 1/2, step 2999/7134 completed (loss: 0.1365181803703308, acc: 0.96875)
[2025-02-13 19:15:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:08][root][INFO] - Training Epoch: 1/2, step 3000/7134 completed (loss: 0.42988255620002747, acc: 0.8888888955116272)
[2025-02-13 19:15:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:09][root][INFO] - Training Epoch: 1/2, step 3001/7134 completed (loss: 0.22243143618106842, acc: 0.9441624283790588)
[2025-02-13 19:15:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:09][root][INFO] - Training Epoch: 1/2, step 3002/7134 completed (loss: 0.3275151550769806, acc: 0.9354838728904724)
[2025-02-13 19:15:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:10][root][INFO] - Training Epoch: 1/2, step 3003/7134 completed (loss: 0.06214883178472519, acc: 0.989847719669342)
[2025-02-13 19:15:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:10][root][INFO] - Training Epoch: 1/2, step 3004/7134 completed (loss: 0.07209626585245132, acc: 0.9851484894752502)
[2025-02-13 19:15:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:10][root][INFO] - Training Epoch: 1/2, step 3005/7134 completed (loss: 0.05787869170308113, acc: 0.984455943107605)
[2025-02-13 19:15:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:11][root][INFO] - Training Epoch: 1/2, step 3006/7134 completed (loss: 0.07785216718912125, acc: 0.9828571677207947)
[2025-02-13 19:15:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:11][root][INFO] - Training Epoch: 1/2, step 3007/7134 completed (loss: 0.19875772297382355, acc: 0.9534883499145508)
[2025-02-13 19:15:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:12][root][INFO] - Training Epoch: 1/2, step 3008/7134 completed (loss: 0.2976669669151306, acc: 0.918367326259613)
[2025-02-13 19:15:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:12][root][INFO] - Training Epoch: 1/2, step 3009/7134 completed (loss: 0.37682709097862244, acc: 0.9404761791229248)
[2025-02-13 19:15:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:12][root][INFO] - Training Epoch: 1/2, step 3010/7134 completed (loss: 0.17322960495948792, acc: 0.9714285731315613)
[2025-02-13 19:15:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:13][root][INFO] - Training Epoch: 1/2, step 3011/7134 completed (loss: 0.7060134410858154, acc: 0.8484848737716675)
[2025-02-13 19:15:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:13][root][INFO] - Training Epoch: 1/2, step 3012/7134 completed (loss: 0.4386743903160095, acc: 0.8992805480957031)
[2025-02-13 19:15:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:14][root][INFO] - Training Epoch: 1/2, step 3013/7134 completed (loss: 0.06305374205112457, acc: 0.9883720874786377)
[2025-02-13 19:15:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:14][root][INFO] - Training Epoch: 1/2, step 3014/7134 completed (loss: 0.16947400569915771, acc: 0.9683544039726257)
[2025-02-13 19:15:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:14][root][INFO] - Training Epoch: 1/2, step 3015/7134 completed (loss: 0.1098145991563797, acc: 0.9788732528686523)
[2025-02-13 19:15:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:15][root][INFO] - Training Epoch: 1/2, step 3016/7134 completed (loss: 0.14186891913414001, acc: 0.965753436088562)
[2025-02-13 19:15:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:15][root][INFO] - Training Epoch: 1/2, step 3017/7134 completed (loss: 0.10401961952447891, acc: 0.9752066135406494)
[2025-02-13 19:15:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:16][root][INFO] - Training Epoch: 1/2, step 3018/7134 completed (loss: 0.13210032880306244, acc: 0.9541984796524048)
[2025-02-13 19:15:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:16][root][INFO] - Training Epoch: 1/2, step 3019/7134 completed (loss: 0.17812618613243103, acc: 0.9485294222831726)
[2025-02-13 19:15:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:16][root][INFO] - Training Epoch: 1/2, step 3020/7134 completed (loss: 0.23277831077575684, acc: 0.9640287756919861)
[2025-02-13 19:15:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:17][root][INFO] - Training Epoch: 1/2, step 3021/7134 completed (loss: 0.25625553727149963, acc: 0.9469696879386902)
[2025-02-13 19:15:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:17][root][INFO] - Training Epoch: 1/2, step 3022/7134 completed (loss: 0.1685478389263153, acc: 0.9473684430122375)
[2025-02-13 19:15:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:17][root][INFO] - Training Epoch: 1/2, step 3023/7134 completed (loss: 0.3295133709907532, acc: 0.9076923131942749)
[2025-02-13 19:15:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:18][root][INFO] - Training Epoch: 1/2, step 3024/7134 completed (loss: 0.1454952508211136, acc: 0.9666666388511658)
[2025-02-13 19:15:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:18][root][INFO] - Training Epoch: 1/2, step 3025/7134 completed (loss: 0.2703210413455963, acc: 0.9461538195610046)
[2025-02-13 19:15:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:19][root][INFO] - Training Epoch: 1/2, step 3026/7134 completed (loss: 0.20511315762996674, acc: 0.9590163826942444)
[2025-02-13 19:15:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:19][root][INFO] - Training Epoch: 1/2, step 3027/7134 completed (loss: 0.2720150351524353, acc: 0.9375)
[2025-02-13 19:15:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:19][root][INFO] - Training Epoch: 1/2, step 3028/7134 completed (loss: 0.08528152108192444, acc: 1.0)
[2025-02-13 19:15:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:20][root][INFO] - Training Epoch: 1/2, step 3029/7134 completed (loss: 0.2679920196533203, acc: 0.9452054500579834)
[2025-02-13 19:15:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:20][root][INFO] - Training Epoch: 1/2, step 3030/7134 completed (loss: 0.13073280453681946, acc: 0.9729729890823364)
[2025-02-13 19:15:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:21][root][INFO] - Training Epoch: 1/2, step 3031/7134 completed (loss: 0.12368360161781311, acc: 0.9595959782600403)
[2025-02-13 19:15:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:21][root][INFO] - Training Epoch: 1/2, step 3032/7134 completed (loss: 0.2482791692018509, acc: 0.9354838728904724)
[2025-02-13 19:15:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:21][root][INFO] - Training Epoch: 1/2, step 3033/7134 completed (loss: 0.22565367817878723, acc: 0.9609375)
[2025-02-13 19:15:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:22][root][INFO] - Training Epoch: 1/2, step 3034/7134 completed (loss: 0.2212357223033905, acc: 0.9459459185600281)
[2025-02-13 19:15:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:22][root][INFO] - Training Epoch: 1/2, step 3035/7134 completed (loss: 0.14429965615272522, acc: 0.9455782175064087)
[2025-02-13 19:15:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:22][root][INFO] - Training Epoch: 1/2, step 3036/7134 completed (loss: 0.24017927050590515, acc: 0.9359999895095825)
[2025-02-13 19:15:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:23][root][INFO] - Training Epoch: 1/2, step 3037/7134 completed (loss: 0.19401608407497406, acc: 0.9530201554298401)
[2025-02-13 19:15:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:23][root][INFO] - Training Epoch: 1/2, step 3038/7134 completed (loss: 0.23654384911060333, acc: 0.9477611780166626)
[2025-02-13 19:15:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:24][root][INFO] - Training Epoch: 1/2, step 3039/7134 completed (loss: 0.21703483164310455, acc: 0.9662162065505981)
[2025-02-13 19:15:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:24][root][INFO] - Training Epoch: 1/2, step 3040/7134 completed (loss: 0.2562730610370636, acc: 0.930232584476471)
[2025-02-13 19:15:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:24][root][INFO] - Training Epoch: 1/2, step 3041/7134 completed (loss: 0.3204946219921112, acc: 0.9428571462631226)
[2025-02-13 19:15:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:25][root][INFO] - Training Epoch: 1/2, step 3042/7134 completed (loss: 0.1712290495634079, acc: 0.9615384340286255)
[2025-02-13 19:15:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:25][root][INFO] - Training Epoch: 1/2, step 3043/7134 completed (loss: 0.20434488356113434, acc: 0.9464285969734192)
[2025-02-13 19:15:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:26][root][INFO] - Training Epoch: 1/2, step 3044/7134 completed (loss: 0.18517781794071198, acc: 0.9548872113227844)
[2025-02-13 19:15:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:26][root][INFO] - Training Epoch: 1/2, step 3045/7134 completed (loss: 0.10601247102022171, acc: 0.9689922332763672)
[2025-02-13 19:15:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:27][root][INFO] - Training Epoch: 1/2, step 3046/7134 completed (loss: 0.17352394759655, acc: 0.9526627063751221)
[2025-02-13 19:15:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:27][root][INFO] - Training Epoch: 1/2, step 3047/7134 completed (loss: 0.21337701380252838, acc: 0.9491525292396545)
[2025-02-13 19:15:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:27][root][INFO] - Training Epoch: 1/2, step 3048/7134 completed (loss: 0.3376866281032562, acc: 0.9052631855010986)
[2025-02-13 19:15:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:28][root][INFO] - Training Epoch: 1/2, step 3049/7134 completed (loss: 0.38249117136001587, acc: 0.8999999761581421)
[2025-02-13 19:15:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:28][root][INFO] - Training Epoch: 1/2, step 3050/7134 completed (loss: 0.1993262618780136, acc: 0.9340101480484009)
[2025-02-13 19:15:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:29][root][INFO] - Training Epoch: 1/2, step 3051/7134 completed (loss: 0.2799726724624634, acc: 0.9047619104385376)
[2025-02-13 19:15:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:29][root][INFO] - Training Epoch: 1/2, step 3052/7134 completed (loss: 0.24093219637870789, acc: 0.9351351261138916)
[2025-02-13 19:15:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:29][root][INFO] - Training Epoch: 1/2, step 3053/7134 completed (loss: 0.16240893304347992, acc: 0.9601989984512329)
[2025-02-13 19:15:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:30][root][INFO] - Training Epoch: 1/2, step 3054/7134 completed (loss: 0.12491786479949951, acc: 0.978723406791687)
[2025-02-13 19:15:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:30][root][INFO] - Training Epoch: 1/2, step 3055/7134 completed (loss: 0.173619344830513, acc: 0.9324324131011963)
[2025-02-13 19:15:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:30][root][INFO] - Training Epoch: 1/2, step 3056/7134 completed (loss: 0.23630119860172272, acc: 0.9513513445854187)
[2025-02-13 19:15:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:31][root][INFO] - Training Epoch: 1/2, step 3057/7134 completed (loss: 0.1640787422657013, acc: 0.9386503100395203)
[2025-02-13 19:15:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:31][root][INFO] - Training Epoch: 1/2, step 3058/7134 completed (loss: 0.19126486778259277, acc: 0.9473684430122375)
[2025-02-13 19:15:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:32][root][INFO] - Training Epoch: 1/2, step 3059/7134 completed (loss: 0.06147577613592148, acc: 1.0)
[2025-02-13 19:15:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:32][root][INFO] - Training Epoch: 1/2, step 3060/7134 completed (loss: 0.2247231900691986, acc: 0.9371727705001831)
[2025-02-13 19:15:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:32][root][INFO] - Training Epoch: 1/2, step 3061/7134 completed (loss: 0.10952216386795044, acc: 0.9750000238418579)
[2025-02-13 19:15:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:33][root][INFO] - Training Epoch: 1/2, step 3062/7134 completed (loss: 0.09497936069965363, acc: 0.9811320900917053)
[2025-02-13 19:15:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:33][root][INFO] - Training Epoch: 1/2, step 3063/7134 completed (loss: 0.19078531861305237, acc: 0.9488636255264282)
[2025-02-13 19:15:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:33][root][INFO] - Training Epoch: 1/2, step 3064/7134 completed (loss: 0.10272620618343353, acc: 0.976331353187561)
[2025-02-13 19:15:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:34][root][INFO] - Training Epoch: 1/2, step 3065/7134 completed (loss: 0.05240092799067497, acc: 0.9887640476226807)
[2025-02-13 19:15:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:34][root][INFO] - Training Epoch: 1/2, step 3066/7134 completed (loss: 0.105318084359169, acc: 0.9735099077224731)
[2025-02-13 19:15:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:35][root][INFO] - Training Epoch: 1/2, step 3067/7134 completed (loss: 0.08284887671470642, acc: 0.9632353186607361)
[2025-02-13 19:15:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:35][root][INFO] - Training Epoch: 1/2, step 3068/7134 completed (loss: 0.2344500720500946, acc: 0.9441340565681458)
[2025-02-13 19:15:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:35][root][INFO] - Training Epoch: 1/2, step 3069/7134 completed (loss: 0.046174176037311554, acc: 0.9897959232330322)
[2025-02-13 19:15:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:36][root][INFO] - Training Epoch: 1/2, step 3070/7134 completed (loss: 0.03737109899520874, acc: 1.0)
[2025-02-13 19:15:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:36][root][INFO] - Training Epoch: 1/2, step 3071/7134 completed (loss: 0.10225053131580353, acc: 0.9833333492279053)
[2025-02-13 19:15:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:37][root][INFO] - Training Epoch: 1/2, step 3072/7134 completed (loss: 0.07341229915618896, acc: 0.98591548204422)
[2025-02-13 19:15:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:37][root][INFO] - Training Epoch: 1/2, step 3073/7134 completed (loss: 0.06677243113517761, acc: 0.9887005686759949)
[2025-02-13 19:15:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:37][root][INFO] - Training Epoch: 1/2, step 3074/7134 completed (loss: 0.05356394499540329, acc: 0.9867549538612366)
[2025-02-13 19:15:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:38][root][INFO] - Training Epoch: 1/2, step 3075/7134 completed (loss: 0.1386907398700714, acc: 0.9629629850387573)
[2025-02-13 19:15:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:38][root][INFO] - Training Epoch: 1/2, step 3076/7134 completed (loss: 0.11002807319164276, acc: 0.9784946441650391)
[2025-02-13 19:15:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:39][root][INFO] - Training Epoch: 1/2, step 3077/7134 completed (loss: 0.27085256576538086, acc: 0.9319371581077576)
[2025-02-13 19:15:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:39][root][INFO] - Training Epoch: 1/2, step 3078/7134 completed (loss: 0.1718783974647522, acc: 0.9427083134651184)
[2025-02-13 19:15:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:39][root][INFO] - Training Epoch: 1/2, step 3079/7134 completed (loss: 0.2783172130584717, acc: 0.9470899701118469)
[2025-02-13 19:15:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:40][root][INFO] - Training Epoch: 1/2, step 3080/7134 completed (loss: 0.3254573345184326, acc: 0.9212962985038757)
[2025-02-13 19:15:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:40][root][INFO] - Training Epoch: 1/2, step 3081/7134 completed (loss: 0.33357566595077515, acc: 0.9108911156654358)
[2025-02-13 19:15:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:41][root][INFO] - Training Epoch: 1/2, step 3082/7134 completed (loss: 0.40324556827545166, acc: 0.89552241563797)
[2025-02-13 19:15:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:41][root][INFO] - Training Epoch: 1/2, step 3083/7134 completed (loss: 0.21016445755958557, acc: 0.9351351261138916)
[2025-02-13 19:15:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:41][root][INFO] - Training Epoch: 1/2, step 3084/7134 completed (loss: 0.3149701952934265, acc: 0.9109947681427002)
[2025-02-13 19:15:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:42][root][INFO] - Training Epoch: 1/2, step 3085/7134 completed (loss: 0.34622859954833984, acc: 0.9225806593894958)
[2025-02-13 19:15:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:42][root][INFO] - Training Epoch: 1/2, step 3086/7134 completed (loss: 0.2640463709831238, acc: 0.9543378949165344)
[2025-02-13 19:15:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:43][root][INFO] - Training Epoch: 1/2, step 3087/7134 completed (loss: 0.14885325729846954, acc: 0.9467455744743347)
[2025-02-13 19:15:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:43][root][INFO] - Training Epoch: 1/2, step 3088/7134 completed (loss: 0.22404880821704865, acc: 0.9392523169517517)
[2025-02-13 19:15:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:43][root][INFO] - Training Epoch: 1/2, step 3089/7134 completed (loss: 0.18829724192619324, acc: 0.9627906680107117)
[2025-02-13 19:15:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:44][root][INFO] - Training Epoch: 1/2, step 3090/7134 completed (loss: 0.5563310384750366, acc: 0.887005627155304)
[2025-02-13 19:15:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:44][root][INFO] - Training Epoch: 1/2, step 3091/7134 completed (loss: 0.7532892823219299, acc: 0.8111587762832642)
[2025-02-13 19:15:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:45][root][INFO] - Training Epoch: 1/2, step 3092/7134 completed (loss: 0.5331760048866272, acc: 0.885496199131012)
[2025-02-13 19:15:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:45][root][INFO] - Training Epoch: 1/2, step 3093/7134 completed (loss: 0.28658536076545715, acc: 0.9436619877815247)
[2025-02-13 19:15:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:45][root][INFO] - Training Epoch: 1/2, step 3094/7134 completed (loss: 0.12919148802757263, acc: 0.9884393215179443)
[2025-02-13 19:15:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:46][root][INFO] - Training Epoch: 1/2, step 3095/7134 completed (loss: 0.255993127822876, acc: 0.949999988079071)
[2025-02-13 19:15:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:46][root][INFO] - Training Epoch: 1/2, step 3096/7134 completed (loss: 0.34076398611068726, acc: 0.926701545715332)
[2025-02-13 19:15:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:47][root][INFO] - Training Epoch: 1/2, step 3097/7134 completed (loss: 0.29433032870292664, acc: 0.9390243887901306)
[2025-02-13 19:15:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:47][root][INFO] - Training Epoch: 1/2, step 3098/7134 completed (loss: 0.24998997151851654, acc: 0.9281045794487)
[2025-02-13 19:15:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:47][root][INFO] - Training Epoch: 1/2, step 3099/7134 completed (loss: 0.20707820355892181, acc: 0.9343065619468689)
[2025-02-13 19:15:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:48][root][INFO] - Training Epoch: 1/2, step 3100/7134 completed (loss: 0.2626459002494812, acc: 0.9417040348052979)
[2025-02-13 19:15:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:48][root][INFO] - Training Epoch: 1/2, step 3101/7134 completed (loss: 0.18109248578548431, acc: 0.9441624283790588)
[2025-02-13 19:15:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:49][root][INFO] - Training Epoch: 1/2, step 3102/7134 completed (loss: 0.14596900343894958, acc: 0.9570552110671997)
[2025-02-13 19:15:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:49][root][INFO] - Training Epoch: 1/2, step 3103/7134 completed (loss: 0.18732458353042603, acc: 0.9568965435028076)
[2025-02-13 19:15:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:49][root][INFO] - Training Epoch: 1/2, step 3104/7134 completed (loss: 0.08505858480930328, acc: 0.9767441749572754)
[2025-02-13 19:15:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:50][root][INFO] - Training Epoch: 1/2, step 3105/7134 completed (loss: 0.2813916802406311, acc: 0.939130425453186)
[2025-02-13 19:15:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:50][root][INFO] - Training Epoch: 1/2, step 3106/7134 completed (loss: 0.20491541922092438, acc: 0.9530201554298401)
[2025-02-13 19:15:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:51][root][INFO] - Training Epoch: 1/2, step 3107/7134 completed (loss: 0.13187743723392487, acc: 0.9418604373931885)
[2025-02-13 19:15:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:51][root][INFO] - Training Epoch: 1/2, step 3108/7134 completed (loss: 0.17881187796592712, acc: 0.9659863710403442)
[2025-02-13 19:15:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:51][root][INFO] - Training Epoch: 1/2, step 3109/7134 completed (loss: 0.26436471939086914, acc: 0.9384615421295166)
[2025-02-13 19:15:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:52][root][INFO] - Training Epoch: 1/2, step 3110/7134 completed (loss: 0.1898878514766693, acc: 0.9514563083648682)
[2025-02-13 19:15:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:52][root][INFO] - Training Epoch: 1/2, step 3111/7134 completed (loss: 0.5591998100280762, acc: 0.8623188138008118)
[2025-02-13 19:15:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:53][root][INFO] - Training Epoch: 1/2, step 3112/7134 completed (loss: 0.45069581270217896, acc: 0.9124087691307068)
[2025-02-13 19:15:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:53][root][INFO] - Training Epoch: 1/2, step 3113/7134 completed (loss: 0.23958934843540192, acc: 0.9329268336296082)
[2025-02-13 19:15:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:53][root][INFO] - Training Epoch: 1/2, step 3114/7134 completed (loss: 0.20705540478229523, acc: 0.9548872113227844)
[2025-02-13 19:15:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:54][root][INFO] - Training Epoch: 1/2, step 3115/7134 completed (loss: 0.23041339218616486, acc: 0.9605262875556946)
[2025-02-13 19:15:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:54][root][INFO] - Training Epoch: 1/2, step 3116/7134 completed (loss: 0.32404401898384094, acc: 0.9378530979156494)
[2025-02-13 19:15:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:55][root][INFO] - Training Epoch: 1/2, step 3117/7134 completed (loss: 0.3442545533180237, acc: 0.9382022619247437)
[2025-02-13 19:15:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:55][root][INFO] - Training Epoch: 1/2, step 3118/7134 completed (loss: 0.2644387483596802, acc: 0.9613259434700012)
[2025-02-13 19:15:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:55][root][INFO] - Training Epoch: 1/2, step 3119/7134 completed (loss: 0.3974323272705078, acc: 0.9312169551849365)
[2025-02-13 19:15:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:56][root][INFO] - Training Epoch: 1/2, step 3120/7134 completed (loss: 0.3817041516304016, acc: 0.9012345671653748)
[2025-02-13 19:15:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:56][root][INFO] - Training Epoch: 1/2, step 3121/7134 completed (loss: 0.20352211594581604, acc: 0.9318181872367859)
[2025-02-13 19:15:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:57][root][INFO] - Training Epoch: 1/2, step 3122/7134 completed (loss: 0.2857055962085724, acc: 0.9368420839309692)
[2025-02-13 19:15:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:57][root][INFO] - Training Epoch: 1/2, step 3123/7134 completed (loss: 0.3622182011604309, acc: 0.9189189076423645)
[2025-02-13 19:15:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:58][root][INFO] - Training Epoch: 1/2, step 3124/7134 completed (loss: 0.38109350204467773, acc: 0.9277777671813965)
[2025-02-13 19:15:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:58][root][INFO] - Training Epoch: 1/2, step 3125/7134 completed (loss: 0.14610348641872406, acc: 0.9659863710403442)
[2025-02-13 19:15:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:58][root][INFO] - Training Epoch: 1/2, step 3126/7134 completed (loss: 0.45256757736206055, acc: 0.8994413614273071)
[2025-02-13 19:15:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:59][root][INFO] - Training Epoch: 1/2, step 3127/7134 completed (loss: 0.20232446491718292, acc: 0.9389671087265015)
[2025-02-13 19:15:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:15:59][root][INFO] - Training Epoch: 1/2, step 3128/7134 completed (loss: 0.22399096190929413, acc: 0.9518716335296631)
[2025-02-13 19:15:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:00][root][INFO] - Training Epoch: 1/2, step 3129/7134 completed (loss: 0.21675100922584534, acc: 0.9431818127632141)
[2025-02-13 19:16:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:00][root][INFO] - Training Epoch: 1/2, step 3130/7134 completed (loss: 0.11608991026878357, acc: 0.9695431590080261)
[2025-02-13 19:16:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:00][root][INFO] - Training Epoch: 1/2, step 3131/7134 completed (loss: 0.31171128153800964, acc: 0.9152542352676392)
[2025-02-13 19:16:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:01][root][INFO] - Training Epoch: 1/2, step 3132/7134 completed (loss: 0.14295949041843414, acc: 0.9646017551422119)
[2025-02-13 19:16:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:01][root][INFO] - Training Epoch: 1/2, step 3133/7134 completed (loss: 0.3699702322483063, acc: 0.9375)
[2025-02-13 19:16:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:01][root][INFO] - Training Epoch: 1/2, step 3134/7134 completed (loss: 0.22640573978424072, acc: 0.921875)
[2025-02-13 19:16:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:02][root][INFO] - Training Epoch: 1/2, step 3135/7134 completed (loss: 0.23151087760925293, acc: 0.9278350472450256)
[2025-02-13 19:16:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:02][root][INFO] - Training Epoch: 1/2, step 3136/7134 completed (loss: 0.17793133854866028, acc: 0.9465649127960205)
[2025-02-13 19:16:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:03][root][INFO] - Training Epoch: 1/2, step 3137/7134 completed (loss: 0.35066089034080505, acc: 0.9007092118263245)
[2025-02-13 19:16:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:03][root][INFO] - Training Epoch: 1/2, step 3138/7134 completed (loss: 0.32275453209877014, acc: 0.9333333373069763)
[2025-02-13 19:16:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:03][root][INFO] - Training Epoch: 1/2, step 3139/7134 completed (loss: 0.40621376037597656, acc: 0.8992805480957031)
[2025-02-13 19:16:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:04][root][INFO] - Training Epoch: 1/2, step 3140/7134 completed (loss: 0.2218288779258728, acc: 0.9726027250289917)
[2025-02-13 19:16:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:04][root][INFO] - Training Epoch: 1/2, step 3141/7134 completed (loss: 0.29206860065460205, acc: 0.9391891956329346)
[2025-02-13 19:16:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:05][root][INFO] - Training Epoch: 1/2, step 3142/7134 completed (loss: 0.14045463502407074, acc: 0.9580838084220886)
[2025-02-13 19:16:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:05][root][INFO] - Training Epoch: 1/2, step 3143/7134 completed (loss: 0.26238778233528137, acc: 0.9225806593894958)
[2025-02-13 19:16:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:05][root][INFO] - Training Epoch: 1/2, step 3144/7134 completed (loss: 0.3748268187046051, acc: 0.9166666865348816)
[2025-02-13 19:16:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:06][root][INFO] - Training Epoch: 1/2, step 3145/7134 completed (loss: 0.3614925146102905, acc: 0.8918918967247009)
[2025-02-13 19:16:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:06][root][INFO] - Training Epoch: 1/2, step 3146/7134 completed (loss: 0.3505077064037323, acc: 0.9242424368858337)
[2025-02-13 19:16:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:07][root][INFO] - Training Epoch: 1/2, step 3147/7134 completed (loss: 0.1861170530319214, acc: 0.965753436088562)
[2025-02-13 19:16:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:07][root][INFO] - Training Epoch: 1/2, step 3148/7134 completed (loss: 0.15129996836185455, acc: 0.9608938694000244)
[2025-02-13 19:16:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:07][root][INFO] - Training Epoch: 1/2, step 3149/7134 completed (loss: 0.1608261913061142, acc: 0.9741935729980469)
[2025-02-13 19:16:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:08][root][INFO] - Training Epoch: 1/2, step 3150/7134 completed (loss: 1.4066410064697266, acc: 0.7714285850524902)
[2025-02-13 19:16:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:08][root][INFO] - Training Epoch: 1/2, step 3151/7134 completed (loss: 0.5098738670349121, acc: 0.885496199131012)
[2025-02-13 19:16:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:09][root][INFO] - Training Epoch: 1/2, step 3152/7134 completed (loss: 0.27007579803466797, acc: 0.9366196990013123)
[2025-02-13 19:16:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:09][root][INFO] - Training Epoch: 1/2, step 3153/7134 completed (loss: 0.33701395988464355, acc: 0.9438202381134033)
[2025-02-13 19:16:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:09][root][INFO] - Training Epoch: 1/2, step 3154/7134 completed (loss: 0.3920533061027527, acc: 0.9172932505607605)
[2025-02-13 19:16:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:10][root][INFO] - Training Epoch: 1/2, step 3155/7134 completed (loss: 0.6550225615501404, acc: 0.8571428656578064)
[2025-02-13 19:16:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:10][root][INFO] - Training Epoch: 1/2, step 3156/7134 completed (loss: 0.46904826164245605, acc: 0.895348846912384)
[2025-02-13 19:16:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:10][root][INFO] - Training Epoch: 1/2, step 3157/7134 completed (loss: 0.27952805161476135, acc: 0.947826087474823)
[2025-02-13 19:16:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:11][root][INFO] - Training Epoch: 1/2, step 3158/7134 completed (loss: 0.20605231821537018, acc: 0.9432623982429504)
[2025-02-13 19:16:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:11][root][INFO] - Training Epoch: 1/2, step 3159/7134 completed (loss: 0.34376609325408936, acc: 0.9484536051750183)
[2025-02-13 19:16:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:12][root][INFO] - Training Epoch: 1/2, step 3160/7134 completed (loss: 0.1790267825126648, acc: 0.942307710647583)
[2025-02-13 19:16:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:12][root][INFO] - Training Epoch: 1/2, step 3161/7134 completed (loss: 0.27859199047088623, acc: 0.9490445852279663)
[2025-02-13 19:16:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:12][root][INFO] - Training Epoch: 1/2, step 3162/7134 completed (loss: 0.1520589292049408, acc: 0.9794520735740662)
[2025-02-13 19:16:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:13][root][INFO] - Training Epoch: 1/2, step 3163/7134 completed (loss: 0.18050776422023773, acc: 0.9590163826942444)
[2025-02-13 19:16:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:13][root][INFO] - Training Epoch: 1/2, step 3164/7134 completed (loss: 0.3340129554271698, acc: 0.9420289993286133)
[2025-02-13 19:16:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:13][root][INFO] - Training Epoch: 1/2, step 3165/7134 completed (loss: 0.268206924200058, acc: 0.9300699234008789)
[2025-02-13 19:16:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:14][root][INFO] - Training Epoch: 1/2, step 3166/7134 completed (loss: 0.5577508211135864, acc: 0.8860759735107422)
[2025-02-13 19:16:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:14][root][INFO] - Training Epoch: 1/2, step 3167/7134 completed (loss: 0.6830682158470154, acc: 0.8703703880310059)
[2025-02-13 19:16:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:15][root][INFO] - Training Epoch: 1/2, step 3168/7134 completed (loss: 0.4358164966106415, acc: 0.8954248428344727)
[2025-02-13 19:16:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:15][root][INFO] - Training Epoch: 1/2, step 3169/7134 completed (loss: 0.4863758087158203, acc: 0.8770492076873779)
[2025-02-13 19:16:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:15][root][INFO] - Training Epoch: 1/2, step 3170/7134 completed (loss: 0.2716607451438904, acc: 0.9264705777168274)
[2025-02-13 19:16:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:16][root][INFO] - Training Epoch: 1/2, step 3171/7134 completed (loss: 0.24002358317375183, acc: 0.9395604133605957)
[2025-02-13 19:16:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:16][root][INFO] - Training Epoch: 1/2, step 3172/7134 completed (loss: 0.15912069380283356, acc: 0.9603174328804016)
[2025-02-13 19:16:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:16][root][INFO] - Training Epoch: 1/2, step 3173/7134 completed (loss: 0.21926532685756683, acc: 0.9494949579238892)
[2025-02-13 19:16:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:17][root][INFO] - Training Epoch: 1/2, step 3174/7134 completed (loss: 0.38290849328041077, acc: 0.8975903391838074)
[2025-02-13 19:16:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:17][root][INFO] - Training Epoch: 1/2, step 3175/7134 completed (loss: 0.3348093330860138, acc: 0.930232584476471)
[2025-02-13 19:16:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:18][root][INFO] - Training Epoch: 1/2, step 3176/7134 completed (loss: 0.5706505179405212, acc: 0.8895348906517029)
[2025-02-13 19:16:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:18][root][INFO] - Training Epoch: 1/2, step 3177/7134 completed (loss: 0.5059153437614441, acc: 0.9074074029922485)
[2025-02-13 19:16:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:18][root][INFO] - Training Epoch: 1/2, step 3178/7134 completed (loss: 0.2813495993614197, acc: 0.9333333373069763)
[2025-02-13 19:16:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:19][root][INFO] - Training Epoch: 1/2, step 3179/7134 completed (loss: 0.5186835527420044, acc: 0.8928571343421936)
[2025-02-13 19:16:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:19][root][INFO] - Training Epoch: 1/2, step 3180/7134 completed (loss: 0.25471872091293335, acc: 0.9320987462997437)
[2025-02-13 19:16:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:20][root][INFO] - Training Epoch: 1/2, step 3181/7134 completed (loss: 0.4545767307281494, acc: 0.8784530162811279)
[2025-02-13 19:16:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:20][root][INFO] - Training Epoch: 1/2, step 3182/7134 completed (loss: 0.2008262276649475, acc: 0.9451219439506531)
[2025-02-13 19:16:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:20][root][INFO] - Training Epoch: 1/2, step 3183/7134 completed (loss: 0.3514220714569092, acc: 0.9124087691307068)
[2025-02-13 19:16:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:21][root][INFO] - Training Epoch: 1/2, step 3184/7134 completed (loss: 0.30857306718826294, acc: 0.925000011920929)
[2025-02-13 19:16:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:21][root][INFO] - Training Epoch: 1/2, step 3185/7134 completed (loss: 0.14933426678180695, acc: 0.9685534834861755)
[2025-02-13 19:16:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:22][root][INFO] - Training Epoch: 1/2, step 3186/7134 completed (loss: 0.19924914836883545, acc: 0.9615384340286255)
[2025-02-13 19:16:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:22][root][INFO] - Training Epoch: 1/2, step 3187/7134 completed (loss: 0.20492464303970337, acc: 0.9357143044471741)
[2025-02-13 19:16:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:22][root][INFO] - Training Epoch: 1/2, step 3188/7134 completed (loss: 0.1947837471961975, acc: 0.9441340565681458)
[2025-02-13 19:16:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:23][root][INFO] - Training Epoch: 1/2, step 3189/7134 completed (loss: 0.31527334451675415, acc: 0.9320987462997437)
[2025-02-13 19:16:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:23][root][INFO] - Training Epoch: 1/2, step 3190/7134 completed (loss: 0.2540850043296814, acc: 0.9437500238418579)
[2025-02-13 19:16:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:23][root][INFO] - Training Epoch: 1/2, step 3191/7134 completed (loss: 0.12455321848392487, acc: 0.9813664555549622)
[2025-02-13 19:16:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:24][root][INFO] - Training Epoch: 1/2, step 3192/7134 completed (loss: 0.4925279915332794, acc: 0.8905109763145447)
[2025-02-13 19:16:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:24][root][INFO] - Training Epoch: 1/2, step 3193/7134 completed (loss: 0.3943944573402405, acc: 0.9027026891708374)
[2025-02-13 19:16:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:24][root][INFO] - Training Epoch: 1/2, step 3194/7134 completed (loss: 0.223429337143898, acc: 0.9333333373069763)
[2025-02-13 19:16:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:25][root][INFO] - Training Epoch: 1/2, step 3195/7134 completed (loss: 0.511345624923706, acc: 0.888198733329773)
[2025-02-13 19:16:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:25][root][INFO] - Training Epoch: 1/2, step 3196/7134 completed (loss: 0.2148292511701584, acc: 0.9347826242446899)
[2025-02-13 19:16:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:26][root][INFO] - Training Epoch: 1/2, step 3197/7134 completed (loss: 0.21637631952762604, acc: 0.940119743347168)
[2025-02-13 19:16:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:26][root][INFO] - Training Epoch: 1/2, step 3198/7134 completed (loss: 0.24980787932872772, acc: 0.9312499761581421)
[2025-02-13 19:16:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:26][root][INFO] - Training Epoch: 1/2, step 3199/7134 completed (loss: 0.16777054965496063, acc: 0.9593023061752319)
[2025-02-13 19:16:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:27][root][INFO] - Training Epoch: 1/2, step 3200/7134 completed (loss: 0.19359911978244781, acc: 0.9645389914512634)
[2025-02-13 19:16:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:27][root][INFO] - Training Epoch: 1/2, step 3201/7134 completed (loss: 0.42943769693374634, acc: 0.8976377844810486)
[2025-02-13 19:16:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:28][root][INFO] - Training Epoch: 1/2, step 3202/7134 completed (loss: 0.2231014519929886, acc: 0.9438202381134033)
[2025-02-13 19:16:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:28][root][INFO] - Training Epoch: 1/2, step 3203/7134 completed (loss: 0.28970956802368164, acc: 0.8999999761581421)
[2025-02-13 19:16:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:28][root][INFO] - Training Epoch: 1/2, step 3204/7134 completed (loss: 0.2948688566684723, acc: 0.9383561611175537)
[2025-02-13 19:16:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:29][root][INFO] - Training Epoch: 1/2, step 3205/7134 completed (loss: 0.10024455934762955, acc: 0.9716312289237976)
[2025-02-13 19:16:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:29][root][INFO] - Training Epoch: 1/2, step 3206/7134 completed (loss: 0.1838502436876297, acc: 0.9636363387107849)
[2025-02-13 19:16:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:30][root][INFO] - Training Epoch: 1/2, step 3207/7134 completed (loss: 0.2821539342403412, acc: 0.9386503100395203)
[2025-02-13 19:16:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:30][root][INFO] - Training Epoch: 1/2, step 3208/7134 completed (loss: 0.19870539009571075, acc: 0.9496402740478516)
[2025-02-13 19:16:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:30][root][INFO] - Training Epoch: 1/2, step 3209/7134 completed (loss: 0.21787965297698975, acc: 0.9375)
[2025-02-13 19:16:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:31][root][INFO] - Training Epoch: 1/2, step 3210/7134 completed (loss: 0.1037287563085556, acc: 0.9612902998924255)
[2025-02-13 19:16:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:31][root][INFO] - Training Epoch: 1/2, step 3211/7134 completed (loss: 0.20699484646320343, acc: 0.9663865566253662)
[2025-02-13 19:16:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:32][root][INFO] - Training Epoch: 1/2, step 3212/7134 completed (loss: 0.1827208250761032, acc: 0.9532163739204407)
[2025-02-13 19:16:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:32][root][INFO] - Training Epoch: 1/2, step 3213/7134 completed (loss: 0.2308150976896286, acc: 0.9404761791229248)
[2025-02-13 19:16:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:32][root][INFO] - Training Epoch: 1/2, step 3214/7134 completed (loss: 0.2026856690645218, acc: 0.9532163739204407)
[2025-02-13 19:16:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:33][root][INFO] - Training Epoch: 1/2, step 3215/7134 completed (loss: 0.31758061051368713, acc: 0.9289940595626831)
[2025-02-13 19:16:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:33][root][INFO] - Training Epoch: 1/2, step 3216/7134 completed (loss: 0.12582965195178986, acc: 0.9691358208656311)
[2025-02-13 19:16:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:34][root][INFO] - Training Epoch: 1/2, step 3217/7134 completed (loss: 0.16375456750392914, acc: 0.9719101190567017)
[2025-02-13 19:16:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:34][root][INFO] - Training Epoch: 1/2, step 3218/7134 completed (loss: 0.16501401364803314, acc: 0.948387086391449)
[2025-02-13 19:16:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:34][root][INFO] - Training Epoch: 1/2, step 3219/7134 completed (loss: 0.2263820767402649, acc: 0.9404761791229248)
[2025-02-13 19:16:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:35][root][INFO] - Training Epoch: 1/2, step 3220/7134 completed (loss: 0.22261032462120056, acc: 0.9435028433799744)
[2025-02-13 19:16:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:35][root][INFO] - Training Epoch: 1/2, step 3221/7134 completed (loss: 0.21633252501487732, acc: 0.9354838728904724)
[2025-02-13 19:16:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:35][root][INFO] - Training Epoch: 1/2, step 3222/7134 completed (loss: 0.4435745179653168, acc: 0.8905472755432129)
[2025-02-13 19:16:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:36][root][INFO] - Training Epoch: 1/2, step 3223/7134 completed (loss: 0.38729602098464966, acc: 0.9141104221343994)
[2025-02-13 19:16:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:36][root][INFO] - Training Epoch: 1/2, step 3224/7134 completed (loss: 0.4214390516281128, acc: 0.9017341136932373)
[2025-02-13 19:16:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:37][root][INFO] - Training Epoch: 1/2, step 3225/7134 completed (loss: 0.17954690754413605, acc: 0.949999988079071)
[2025-02-13 19:16:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:37][root][INFO] - Training Epoch: 1/2, step 3226/7134 completed (loss: 0.30534154176712036, acc: 0.9205297827720642)
[2025-02-13 19:16:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:37][root][INFO] - Training Epoch: 1/2, step 3227/7134 completed (loss: 0.23101165890693665, acc: 0.9547325372695923)
[2025-02-13 19:16:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:38][root][INFO] - Training Epoch: 1/2, step 3228/7134 completed (loss: 0.1798166036605835, acc: 0.9444444179534912)
[2025-02-13 19:16:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:38][root][INFO] - Training Epoch: 1/2, step 3229/7134 completed (loss: 0.26339006423950195, acc: 0.9343434572219849)
[2025-02-13 19:16:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:39][root][INFO] - Training Epoch: 1/2, step 3230/7134 completed (loss: 0.17863427102565765, acc: 0.9624999761581421)
[2025-02-13 19:16:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:39][root][INFO] - Training Epoch: 1/2, step 3231/7134 completed (loss: 0.34728556871414185, acc: 0.9285714030265808)
[2025-02-13 19:16:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:39][root][INFO] - Training Epoch: 1/2, step 3232/7134 completed (loss: 0.1383323222398758, acc: 0.9629629850387573)
[2025-02-13 19:16:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:40][root][INFO] - Training Epoch: 1/2, step 3233/7134 completed (loss: 0.2998999059200287, acc: 0.9291338324546814)
[2025-02-13 19:16:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:40][root][INFO] - Training Epoch: 1/2, step 3234/7134 completed (loss: 0.10622207075357437, acc: 0.9811320900917053)
[2025-02-13 19:16:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:41][root][INFO] - Training Epoch: 1/2, step 3235/7134 completed (loss: 0.1853199154138565, acc: 0.936170220375061)
[2025-02-13 19:16:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:41][root][INFO] - Training Epoch: 1/2, step 3236/7134 completed (loss: 0.3105987310409546, acc: 0.9435028433799744)
[2025-02-13 19:16:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:41][root][INFO] - Training Epoch: 1/2, step 3237/7134 completed (loss: 0.1423211246728897, acc: 0.9545454382896423)
[2025-02-13 19:16:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:42][root][INFO] - Training Epoch: 1/2, step 3238/7134 completed (loss: 0.2647465467453003, acc: 0.9222221970558167)
[2025-02-13 19:16:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:42][root][INFO] - Training Epoch: 1/2, step 3239/7134 completed (loss: 0.14141353964805603, acc: 0.9634146094322205)
[2025-02-13 19:16:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:43][root][INFO] - Training Epoch: 1/2, step 3240/7134 completed (loss: 0.08111323416233063, acc: 0.9888268113136292)
[2025-02-13 19:16:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:43][root][INFO] - Training Epoch: 1/2, step 3241/7134 completed (loss: 0.11711825430393219, acc: 0.9588235020637512)
[2025-02-13 19:16:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:43][root][INFO] - Training Epoch: 1/2, step 3242/7134 completed (loss: 0.09410810470581055, acc: 0.9740259647369385)
[2025-02-13 19:16:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:44][root][INFO] - Training Epoch: 1/2, step 3243/7134 completed (loss: 0.10797677934169769, acc: 0.9732620120048523)
[2025-02-13 19:16:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:44][root][INFO] - Training Epoch: 1/2, step 3244/7134 completed (loss: 0.22717690467834473, acc: 0.9595375657081604)
[2025-02-13 19:16:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:44][root][INFO] - Training Epoch: 1/2, step 3245/7134 completed (loss: 0.14615897834300995, acc: 0.9743589758872986)
[2025-02-13 19:16:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:45][root][INFO] - Training Epoch: 1/2, step 3246/7134 completed (loss: 0.14188680052757263, acc: 0.9487179517745972)
[2025-02-13 19:16:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:45][root][INFO] - Training Epoch: 1/2, step 3247/7134 completed (loss: 0.149371936917305, acc: 0.9551281929016113)
[2025-02-13 19:16:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:46][root][INFO] - Training Epoch: 1/2, step 3248/7134 completed (loss: 0.08983303606510162, acc: 0.9781022071838379)
[2025-02-13 19:16:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:46][root][INFO] - Training Epoch: 1/2, step 3249/7134 completed (loss: 0.12105712294578552, acc: 0.9704142212867737)
[2025-02-13 19:16:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:46][root][INFO] - Training Epoch: 1/2, step 3250/7134 completed (loss: 0.235164076089859, acc: 0.940119743347168)
[2025-02-13 19:16:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:47][root][INFO] - Training Epoch: 1/2, step 3251/7134 completed (loss: 0.2004382312297821, acc: 0.9551281929016113)
[2025-02-13 19:16:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:47][root][INFO] - Training Epoch: 1/2, step 3252/7134 completed (loss: 0.07403307408094406, acc: 0.9879518151283264)
[2025-02-13 19:16:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:48][root][INFO] - Training Epoch: 1/2, step 3253/7134 completed (loss: 0.19783589243888855, acc: 0.9539473652839661)
[2025-02-13 19:16:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:48][root][INFO] - Training Epoch: 1/2, step 3254/7134 completed (loss: 0.19188448786735535, acc: 0.9545454382896423)
[2025-02-13 19:16:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:48][root][INFO] - Training Epoch: 1/2, step 3255/7134 completed (loss: 0.18494220077991486, acc: 0.9829545617103577)
[2025-02-13 19:16:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:49][root][INFO] - Training Epoch: 1/2, step 3256/7134 completed (loss: 0.13209831714630127, acc: 0.9875776171684265)
[2025-02-13 19:16:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:49][root][INFO] - Training Epoch: 1/2, step 3257/7134 completed (loss: 0.37343570590019226, acc: 0.939393937587738)
[2025-02-13 19:16:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:49][root][INFO] - Training Epoch: 1/2, step 3258/7134 completed (loss: 0.43275541067123413, acc: 0.9105691313743591)
[2025-02-13 19:16:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:50][root][INFO] - Training Epoch: 1/2, step 3259/7134 completed (loss: 0.2773624360561371, acc: 0.9316770434379578)
[2025-02-13 19:16:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:50][root][INFO] - Training Epoch: 1/2, step 3260/7134 completed (loss: 0.4755418300628662, acc: 0.9103448390960693)
[2025-02-13 19:16:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:51][root][INFO] - Training Epoch: 1/2, step 3261/7134 completed (loss: 0.33514729142189026, acc: 0.9197080135345459)
[2025-02-13 19:16:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:51][root][INFO] - Training Epoch: 1/2, step 3262/7134 completed (loss: 0.2962090075016022, acc: 0.9135802388191223)
[2025-02-13 19:16:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:51][root][INFO] - Training Epoch: 1/2, step 3263/7134 completed (loss: 0.40228068828582764, acc: 0.9171597361564636)
[2025-02-13 19:16:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:52][root][INFO] - Training Epoch: 1/2, step 3264/7134 completed (loss: 0.24844679236412048, acc: 0.9451219439506531)
[2025-02-13 19:16:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:52][root][INFO] - Training Epoch: 1/2, step 3265/7134 completed (loss: 0.33547961711883545, acc: 0.9108280539512634)
[2025-02-13 19:16:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:53][root][INFO] - Training Epoch: 1/2, step 3266/7134 completed (loss: 0.3817566931247711, acc: 0.931034505367279)
[2025-02-13 19:16:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:53][root][INFO] - Training Epoch: 1/2, step 3267/7134 completed (loss: 0.2536067068576813, acc: 0.939393937587738)
[2025-02-13 19:16:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:53][root][INFO] - Training Epoch: 1/2, step 3268/7134 completed (loss: 0.18474504351615906, acc: 0.9589040875434875)
[2025-02-13 19:16:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:54][root][INFO] - Training Epoch: 1/2, step 3269/7134 completed (loss: 0.2358929067850113, acc: 0.9521276354789734)
[2025-02-13 19:16:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:54][root][INFO] - Training Epoch: 1/2, step 3270/7134 completed (loss: 0.11104533076286316, acc: 0.9729729890823364)
[2025-02-13 19:16:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:54][root][INFO] - Training Epoch: 1/2, step 3271/7134 completed (loss: 0.16175439953804016, acc: 0.9718309640884399)
[2025-02-13 19:16:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:55][root][INFO] - Training Epoch: 1/2, step 3272/7134 completed (loss: 0.1743529587984085, acc: 0.9666666388511658)
[2025-02-13 19:16:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:55][root][INFO] - Training Epoch: 1/2, step 3273/7134 completed (loss: 0.1308068335056305, acc: 0.9657142758369446)
[2025-02-13 19:16:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:56][root][INFO] - Training Epoch: 1/2, step 3274/7134 completed (loss: 0.20869289338588715, acc: 0.9440993666648865)
[2025-02-13 19:16:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:56][root][INFO] - Training Epoch: 1/2, step 3275/7134 completed (loss: 0.8056884407997131, acc: 0.826347291469574)
[2025-02-13 19:16:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:56][root][INFO] - Training Epoch: 1/2, step 3276/7134 completed (loss: 0.23182441294193268, acc: 0.9542483687400818)
[2025-02-13 19:16:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:57][root][INFO] - Training Epoch: 1/2, step 3277/7134 completed (loss: 0.30180132389068604, acc: 0.9424460530281067)
[2025-02-13 19:16:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:57][root][INFO] - Training Epoch: 1/2, step 3278/7134 completed (loss: 0.268794983625412, acc: 0.9112903475761414)
[2025-02-13 19:16:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:58][root][INFO] - Training Epoch: 1/2, step 3279/7134 completed (loss: 0.371468186378479, acc: 0.9235293865203857)
[2025-02-13 19:16:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:58][root][INFO] - Training Epoch: 1/2, step 3280/7134 completed (loss: 0.6068097949028015, acc: 0.8557692170143127)
[2025-02-13 19:16:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:58][root][INFO] - Training Epoch: 1/2, step 3281/7134 completed (loss: 0.4294489324092865, acc: 0.9251700639724731)
[2025-02-13 19:16:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:59][root][INFO] - Training Epoch: 1/2, step 3282/7134 completed (loss: 0.15562699735164642, acc: 0.9618320465087891)
[2025-02-13 19:16:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:16:59][root][INFO] - Training Epoch: 1/2, step 3283/7134 completed (loss: 0.24530838429927826, acc: 0.9411764740943909)
[2025-02-13 19:16:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:00][root][INFO] - Training Epoch: 1/2, step 3284/7134 completed (loss: 0.3090716004371643, acc: 0.931034505367279)
[2025-02-13 19:17:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:00][root][INFO] - Training Epoch: 1/2, step 3285/7134 completed (loss: 0.3885546326637268, acc: 0.8823529481887817)
[2025-02-13 19:17:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:01][root][INFO] - Training Epoch: 1/2, step 3286/7134 completed (loss: 0.22822944819927216, acc: 0.9461538195610046)
[2025-02-13 19:17:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:01][root][INFO] - Training Epoch: 1/2, step 3287/7134 completed (loss: 0.20147301256656647, acc: 0.9407407641410828)
[2025-02-13 19:17:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:01][root][INFO] - Training Epoch: 1/2, step 3288/7134 completed (loss: 0.2667243182659149, acc: 0.9568345546722412)
[2025-02-13 19:17:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:02][root][INFO] - Training Epoch: 1/2, step 3289/7134 completed (loss: 0.2628232538700104, acc: 0.9485294222831726)
[2025-02-13 19:17:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:02][root][INFO] - Training Epoch: 1/2, step 3290/7134 completed (loss: 0.3053087592124939, acc: 0.9327731132507324)
[2025-02-13 19:17:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:03][root][INFO] - Training Epoch: 1/2, step 3291/7134 completed (loss: 0.341795414686203, acc: 0.9207921028137207)
[2025-02-13 19:17:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:03][root][INFO] - Training Epoch: 1/2, step 3292/7134 completed (loss: 0.3690204322338104, acc: 0.9411764740943909)
[2025-02-13 19:17:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:03][root][INFO] - Training Epoch: 1/2, step 3293/7134 completed (loss: 0.23450958728790283, acc: 0.9166666865348816)
[2025-02-13 19:17:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:04][root][INFO] - Training Epoch: 1/2, step 3294/7134 completed (loss: 0.4762672185897827, acc: 0.9166666865348816)
[2025-02-13 19:17:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:04][root][INFO] - Training Epoch: 1/2, step 3295/7134 completed (loss: 0.06874258816242218, acc: 0.9838709831237793)
[2025-02-13 19:17:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:05][root][INFO] - Training Epoch: 1/2, step 3296/7134 completed (loss: 0.32545205950737, acc: 0.8846153616905212)
[2025-02-13 19:17:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:05][root][INFO] - Training Epoch: 1/2, step 3297/7134 completed (loss: 0.31240618228912354, acc: 0.9056603908538818)
[2025-02-13 19:17:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:05][root][INFO] - Training Epoch: 1/2, step 3298/7134 completed (loss: 0.3656843304634094, acc: 0.9473684430122375)
[2025-02-13 19:17:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:06][root][INFO] - Training Epoch: 1/2, step 3299/7134 completed (loss: 0.21190699934959412, acc: 0.9370078444480896)
[2025-02-13 19:17:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:06][root][INFO] - Training Epoch: 1/2, step 3300/7134 completed (loss: 0.32712894678115845, acc: 0.9172932505607605)
[2025-02-13 19:17:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:07][root][INFO] - Training Epoch: 1/2, step 3301/7134 completed (loss: 0.16119399666786194, acc: 0.9514563083648682)
[2025-02-13 19:17:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:07][root][INFO] - Training Epoch: 1/2, step 3302/7134 completed (loss: 0.3223085403442383, acc: 0.9300699234008789)
[2025-02-13 19:17:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:07][root][INFO] - Training Epoch: 1/2, step 3303/7134 completed (loss: 0.26441729068756104, acc: 0.9306930899620056)
[2025-02-13 19:17:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:08][root][INFO] - Training Epoch: 1/2, step 3304/7134 completed (loss: 0.15921002626419067, acc: 0.9541284441947937)
[2025-02-13 19:17:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:08][root][INFO] - Training Epoch: 1/2, step 3305/7134 completed (loss: 0.28188377618789673, acc: 0.9371069073677063)
[2025-02-13 19:17:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:08][root][INFO] - Training Epoch: 1/2, step 3306/7134 completed (loss: 0.17873765528202057, acc: 0.9425287246704102)
[2025-02-13 19:17:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:09][root][INFO] - Training Epoch: 1/2, step 3307/7134 completed (loss: 0.21508702635765076, acc: 0.9555555582046509)
[2025-02-13 19:17:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:09][root][INFO] - Training Epoch: 1/2, step 3308/7134 completed (loss: 0.32213106751441956, acc: 0.9117646813392639)
[2025-02-13 19:17:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:10][root][INFO] - Training Epoch: 1/2, step 3309/7134 completed (loss: 0.330041766166687, acc: 0.9230769276618958)
[2025-02-13 19:17:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:10][root][INFO] - Training Epoch: 1/2, step 3310/7134 completed (loss: 0.1942702829837799, acc: 0.9363636374473572)
[2025-02-13 19:17:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:10][root][INFO] - Training Epoch: 1/2, step 3311/7134 completed (loss: 0.23389695584774017, acc: 0.9207921028137207)
[2025-02-13 19:17:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:11][root][INFO] - Training Epoch: 1/2, step 3312/7134 completed (loss: 0.28719526529312134, acc: 0.9172932505607605)
[2025-02-13 19:17:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:11][root][INFO] - Training Epoch: 1/2, step 3313/7134 completed (loss: 0.37946876883506775, acc: 0.9081632494926453)
[2025-02-13 19:17:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:12][root][INFO] - Training Epoch: 1/2, step 3314/7134 completed (loss: 0.2694874107837677, acc: 0.9541284441947937)
[2025-02-13 19:17:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:12][root][INFO] - Training Epoch: 1/2, step 3315/7134 completed (loss: 0.34561580419540405, acc: 0.9230769276618958)
[2025-02-13 19:17:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:12][root][INFO] - Training Epoch: 1/2, step 3316/7134 completed (loss: 0.36068257689476013, acc: 0.9253731369972229)
[2025-02-13 19:17:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:13][root][INFO] - Training Epoch: 1/2, step 3317/7134 completed (loss: 0.5337938070297241, acc: 0.8842975497245789)
[2025-02-13 19:17:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:13][root][INFO] - Training Epoch: 1/2, step 3318/7134 completed (loss: 0.36484140157699585, acc: 0.9322034120559692)
[2025-02-13 19:17:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:14][root][INFO] - Training Epoch: 1/2, step 3319/7134 completed (loss: 0.3021049201488495, acc: 0.9275362491607666)
[2025-02-13 19:17:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:14][root][INFO] - Training Epoch: 1/2, step 3320/7134 completed (loss: 0.2910747826099396, acc: 0.9322034120559692)
[2025-02-13 19:17:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:14][root][INFO] - Training Epoch: 1/2, step 3321/7134 completed (loss: 0.40416258573532104, acc: 0.9357143044471741)
[2025-02-13 19:17:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:15][root][INFO] - Training Epoch: 1/2, step 3322/7134 completed (loss: 0.3362251818180084, acc: 0.9210526347160339)
[2025-02-13 19:17:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:15][root][INFO] - Training Epoch: 1/2, step 3323/7134 completed (loss: 0.28531017899513245, acc: 0.9005848169326782)
[2025-02-13 19:17:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:15][root][INFO] - Training Epoch: 1/2, step 3324/7134 completed (loss: 0.4266388714313507, acc: 0.8704662919044495)
[2025-02-13 19:17:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:16][root][INFO] - Training Epoch: 1/2, step 3325/7134 completed (loss: 0.26271751523017883, acc: 0.9580838084220886)
[2025-02-13 19:17:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:16][root][INFO] - Training Epoch: 1/2, step 3326/7134 completed (loss: 0.4428706765174866, acc: 0.9090909361839294)
[2025-02-13 19:17:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:17][root][INFO] - Training Epoch: 1/2, step 3327/7134 completed (loss: 0.2708773910999298, acc: 0.9116021990776062)
[2025-02-13 19:17:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:17][root][INFO] - Training Epoch: 1/2, step 3328/7134 completed (loss: 0.17436224222183228, acc: 0.978723406791687)
[2025-02-13 19:17:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:18][root][INFO] - Training Epoch: 1/2, step 3329/7134 completed (loss: 0.25976070761680603, acc: 0.9166666865348816)
[2025-02-13 19:17:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:18][root][INFO] - Training Epoch: 1/2, step 3330/7134 completed (loss: 0.21365799009799957, acc: 0.9589743614196777)
[2025-02-13 19:17:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:18][root][INFO] - Training Epoch: 1/2, step 3331/7134 completed (loss: 0.6374332904815674, acc: 0.8461538553237915)
[2025-02-13 19:17:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:19][root][INFO] - Training Epoch: 1/2, step 3332/7134 completed (loss: 0.4011193811893463, acc: 0.9266666769981384)
[2025-02-13 19:17:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:19][root][INFO] - Training Epoch: 1/2, step 3333/7134 completed (loss: 0.4116337299346924, acc: 0.886956512928009)
[2025-02-13 19:17:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:19][root][INFO] - Training Epoch: 1/2, step 3334/7134 completed (loss: 0.2480187565088272, acc: 0.9510869383811951)
[2025-02-13 19:17:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:20][root][INFO] - Training Epoch: 1/2, step 3335/7134 completed (loss: 0.11578484624624252, acc: 0.9666666388511658)
[2025-02-13 19:17:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:20][root][INFO] - Training Epoch: 1/2, step 3336/7134 completed (loss: 0.1105007454752922, acc: 0.970059871673584)
[2025-02-13 19:17:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:21][root][INFO] - Training Epoch: 1/2, step 3337/7134 completed (loss: 0.22671134769916534, acc: 0.9663865566253662)
[2025-02-13 19:17:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:21][root][INFO] - Training Epoch: 1/2, step 3338/7134 completed (loss: 0.11713691800832748, acc: 0.9637305736541748)
[2025-02-13 19:17:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:21][root][INFO] - Training Epoch: 1/2, step 3339/7134 completed (loss: 0.269868940114975, acc: 0.9227052927017212)
[2025-02-13 19:17:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:22][root][INFO] - Training Epoch: 1/2, step 3340/7134 completed (loss: 0.4268821179866791, acc: 0.9047619104385376)
[2025-02-13 19:17:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:22][root][INFO] - Training Epoch: 1/2, step 3341/7134 completed (loss: 0.2649502754211426, acc: 0.9580838084220886)
[2025-02-13 19:17:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:23][root][INFO] - Training Epoch: 1/2, step 3342/7134 completed (loss: 0.15557730197906494, acc: 0.965753436088562)
[2025-02-13 19:17:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:23][root][INFO] - Training Epoch: 1/2, step 3343/7134 completed (loss: 0.178696408867836, acc: 0.9568965435028076)
[2025-02-13 19:17:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:23][root][INFO] - Training Epoch: 1/2, step 3344/7134 completed (loss: 0.2059641033411026, acc: 0.9338235259056091)
[2025-02-13 19:17:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:24][root][INFO] - Training Epoch: 1/2, step 3345/7134 completed (loss: 0.22718507051467896, acc: 0.9580419659614563)
[2025-02-13 19:17:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:24][root][INFO] - Training Epoch: 1/2, step 3346/7134 completed (loss: 0.1408052146434784, acc: 0.9634146094322205)
[2025-02-13 19:17:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:25][root][INFO] - Training Epoch: 1/2, step 3347/7134 completed (loss: 0.45310041308403015, acc: 0.9084967374801636)
[2025-02-13 19:17:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:25][root][INFO] - Training Epoch: 1/2, step 3348/7134 completed (loss: 0.19522660970687866, acc: 0.9683544039726257)
[2025-02-13 19:17:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:25][root][INFO] - Training Epoch: 1/2, step 3349/7134 completed (loss: 0.2347433716058731, acc: 0.9457364082336426)
[2025-02-13 19:17:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:26][root][INFO] - Training Epoch: 1/2, step 3350/7134 completed (loss: 0.14201310276985168, acc: 0.9669421315193176)
[2025-02-13 19:17:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:26][root][INFO] - Training Epoch: 1/2, step 3351/7134 completed (loss: 0.4018881320953369, acc: 0.8936170339584351)
[2025-02-13 19:17:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:27][root][INFO] - Training Epoch: 1/2, step 3352/7134 completed (loss: 0.14037474989891052, acc: 0.970370352268219)
[2025-02-13 19:17:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:27][root][INFO] - Training Epoch: 1/2, step 3353/7134 completed (loss: 0.23371157050132751, acc: 0.9512194991111755)
[2025-02-13 19:17:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:27][root][INFO] - Training Epoch: 1/2, step 3354/7134 completed (loss: 0.17746101319789886, acc: 0.9620253443717957)
[2025-02-13 19:17:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:28][root][INFO] - Training Epoch: 1/2, step 3355/7134 completed (loss: 0.0745646059513092, acc: 0.9817073345184326)
[2025-02-13 19:17:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:28][root][INFO] - Training Epoch: 1/2, step 3356/7134 completed (loss: 0.04472897946834564, acc: 1.0)
[2025-02-13 19:17:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:28][root][INFO] - Training Epoch: 1/2, step 3357/7134 completed (loss: 0.15012259781360626, acc: 0.946107804775238)
[2025-02-13 19:17:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:29][root][INFO] - Training Epoch: 1/2, step 3358/7134 completed (loss: 0.5097295045852661, acc: 0.8970588445663452)
[2025-02-13 19:17:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:29][root][INFO] - Training Epoch: 1/2, step 3359/7134 completed (loss: 0.4309841990470886, acc: 0.9178082346916199)
[2025-02-13 19:17:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:30][root][INFO] - Training Epoch: 1/2, step 3360/7134 completed (loss: 0.3767283260822296, acc: 0.939130425453186)
[2025-02-13 19:17:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:30][root][INFO] - Training Epoch: 1/2, step 3361/7134 completed (loss: 0.5172470211982727, acc: 0.9017857313156128)
[2025-02-13 19:17:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:30][root][INFO] - Training Epoch: 1/2, step 3362/7134 completed (loss: 0.3855120539665222, acc: 0.8999999761581421)
[2025-02-13 19:17:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:31][root][INFO] - Training Epoch: 1/2, step 3363/7134 completed (loss: 0.17320454120635986, acc: 0.9719626307487488)
[2025-02-13 19:17:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:31][root][INFO] - Training Epoch: 1/2, step 3364/7134 completed (loss: 0.542376697063446, acc: 0.8758170008659363)
[2025-02-13 19:17:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:31][root][INFO] - Training Epoch: 1/2, step 3365/7134 completed (loss: 0.2758813798427582, acc: 0.9115044474601746)
[2025-02-13 19:17:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:32][root][INFO] - Training Epoch: 1/2, step 3366/7134 completed (loss: 0.18769234418869019, acc: 0.9722222089767456)
[2025-02-13 19:17:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:32][root][INFO] - Training Epoch: 1/2, step 3367/7134 completed (loss: 0.25224190950393677, acc: 0.9251337051391602)
[2025-02-13 19:17:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:33][root][INFO] - Training Epoch: 1/2, step 3368/7134 completed (loss: 0.28131169080734253, acc: 0.9192546606063843)
[2025-02-13 19:17:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:33][root][INFO] - Training Epoch: 1/2, step 3369/7134 completed (loss: 0.234153151512146, acc: 0.9351351261138916)
[2025-02-13 19:17:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:33][root][INFO] - Training Epoch: 1/2, step 3370/7134 completed (loss: 0.17457567155361176, acc: 0.9509202241897583)
[2025-02-13 19:17:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:34][root][INFO] - Training Epoch: 1/2, step 3371/7134 completed (loss: 0.22942869365215302, acc: 0.9444444179534912)
[2025-02-13 19:17:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:34][root][INFO] - Training Epoch: 1/2, step 3372/7134 completed (loss: 0.2599688172340393, acc: 0.9387755393981934)
[2025-02-13 19:17:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:35][root][INFO] - Training Epoch: 1/2, step 3373/7134 completed (loss: 0.12913104891777039, acc: 0.9738562107086182)
[2025-02-13 19:17:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:35][root][INFO] - Training Epoch: 1/2, step 3374/7134 completed (loss: 0.08605773001909256, acc: 0.9885714054107666)
[2025-02-13 19:17:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:35][root][INFO] - Training Epoch: 1/2, step 3375/7134 completed (loss: 0.19934822618961334, acc: 0.946107804775238)
[2025-02-13 19:17:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:36][root][INFO] - Training Epoch: 1/2, step 3376/7134 completed (loss: 0.1265619546175003, acc: 0.9788359999656677)
[2025-02-13 19:17:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:36][root][INFO] - Training Epoch: 1/2, step 3377/7134 completed (loss: 0.12974657118320465, acc: 0.979899525642395)
[2025-02-13 19:17:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:36][root][INFO] - Training Epoch: 1/2, step 3378/7134 completed (loss: 0.06931423395872116, acc: 0.9934210777282715)
[2025-02-13 19:17:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:37][root][INFO] - Training Epoch: 1/2, step 3379/7134 completed (loss: 0.25632593035697937, acc: 0.9054054021835327)
[2025-02-13 19:17:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:37][root][INFO] - Training Epoch: 1/2, step 3380/7134 completed (loss: 0.1903802752494812, acc: 0.9492753744125366)
[2025-02-13 19:17:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:38][root][INFO] - Training Epoch: 1/2, step 3381/7134 completed (loss: 0.1692539006471634, acc: 0.9634146094322205)
[2025-02-13 19:17:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:38][root][INFO] - Training Epoch: 1/2, step 3382/7134 completed (loss: 0.15883700549602509, acc: 0.9453125)
[2025-02-13 19:17:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:38][root][INFO] - Training Epoch: 1/2, step 3383/7134 completed (loss: 0.15786202251911163, acc: 0.9720279574394226)
[2025-02-13 19:17:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:39][root][INFO] - Training Epoch: 1/2, step 3384/7134 completed (loss: 0.27047011256217957, acc: 0.9270073175430298)
[2025-02-13 19:17:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:39][root][INFO] - Training Epoch: 1/2, step 3385/7134 completed (loss: 0.2129366397857666, acc: 0.9444444179534912)
[2025-02-13 19:17:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:40][root][INFO] - Training Epoch: 1/2, step 3386/7134 completed (loss: 0.1120477169752121, acc: 0.9704142212867737)
[2025-02-13 19:17:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:40][root][INFO] - Training Epoch: 1/2, step 3387/7134 completed (loss: 0.16966676712036133, acc: 0.9476743936538696)
[2025-02-13 19:17:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:40][root][INFO] - Training Epoch: 1/2, step 3388/7134 completed (loss: 0.1300249546766281, acc: 0.9743589758872986)
[2025-02-13 19:17:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:41][root][INFO] - Training Epoch: 1/2, step 3389/7134 completed (loss: 0.19377568364143372, acc: 0.9617486596107483)
[2025-02-13 19:17:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:41][root][INFO] - Training Epoch: 1/2, step 3390/7134 completed (loss: 0.18766823410987854, acc: 0.9488636255264282)
[2025-02-13 19:17:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:41][root][INFO] - Training Epoch: 1/2, step 3391/7134 completed (loss: 0.16758912801742554, acc: 0.9695122241973877)
[2025-02-13 19:17:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:42][root][INFO] - Training Epoch: 1/2, step 3392/7134 completed (loss: 0.14294816553592682, acc: 0.9770992398262024)
[2025-02-13 19:17:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:42][root][INFO] - Training Epoch: 1/2, step 3393/7134 completed (loss: 0.2893710434436798, acc: 0.9300699234008789)
[2025-02-13 19:17:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:43][root][INFO] - Training Epoch: 1/2, step 3394/7134 completed (loss: 0.26195043325424194, acc: 0.9214285612106323)
[2025-02-13 19:17:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:43][root][INFO] - Training Epoch: 1/2, step 3395/7134 completed (loss: 0.3649540841579437, acc: 0.9144737124443054)
[2025-02-13 19:17:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:43][root][INFO] - Training Epoch: 1/2, step 3396/7134 completed (loss: 0.18414467573165894, acc: 0.9711538553237915)
[2025-02-13 19:17:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:44][root][INFO] - Training Epoch: 1/2, step 3397/7134 completed (loss: 0.11390763521194458, acc: 0.9701492786407471)
[2025-02-13 19:17:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:44][root][INFO] - Training Epoch: 1/2, step 3398/7134 completed (loss: 0.1806824654340744, acc: 0.9420289993286133)
[2025-02-13 19:17:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:44][root][INFO] - Training Epoch: 1/2, step 3399/7134 completed (loss: 0.5205768346786499, acc: 0.8958333134651184)
[2025-02-13 19:17:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:45][root][INFO] - Training Epoch: 1/2, step 3400/7134 completed (loss: 0.3898909091949463, acc: 0.8976377844810486)
[2025-02-13 19:17:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:45][root][INFO] - Training Epoch: 1/2, step 3401/7134 completed (loss: 0.26705557107925415, acc: 0.9251700639724731)
[2025-02-13 19:17:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:46][root][INFO] - Training Epoch: 1/2, step 3402/7134 completed (loss: 0.6948410272598267, acc: 0.866310179233551)
[2025-02-13 19:17:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:46][root][INFO] - Training Epoch: 1/2, step 3403/7134 completed (loss: 0.4883194863796234, acc: 0.8939393758773804)
[2025-02-13 19:17:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:46][root][INFO] - Training Epoch: 1/2, step 3404/7134 completed (loss: 0.12704366445541382, acc: 0.9622641801834106)
[2025-02-13 19:17:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:47][root][INFO] - Training Epoch: 1/2, step 3405/7134 completed (loss: 0.4538402259349823, acc: 0.9117646813392639)
[2025-02-13 19:17:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:47][root][INFO] - Training Epoch: 1/2, step 3406/7134 completed (loss: 0.40516340732574463, acc: 0.8899999856948853)
[2025-02-13 19:17:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:47][root][INFO] - Training Epoch: 1/2, step 3407/7134 completed (loss: 0.2786836624145508, acc: 0.9202454090118408)
[2025-02-13 19:17:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:48][root][INFO] - Training Epoch: 1/2, step 3408/7134 completed (loss: 0.22252769768238068, acc: 0.9339622855186462)
[2025-02-13 19:17:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:48][root][INFO] - Training Epoch: 1/2, step 3409/7134 completed (loss: 0.1897411197423935, acc: 0.9482758641242981)
[2025-02-13 19:17:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:49][root][INFO] - Training Epoch: 1/2, step 3410/7134 completed (loss: 0.3509749472141266, acc: 0.9268292784690857)
[2025-02-13 19:17:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:49][root][INFO] - Training Epoch: 1/2, step 3411/7134 completed (loss: 0.18992358446121216, acc: 0.9505494236946106)
[2025-02-13 19:17:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:49][root][INFO] - Training Epoch: 1/2, step 3412/7134 completed (loss: 0.4353574812412262, acc: 0.9083333611488342)
[2025-02-13 19:17:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:50][root][INFO] - Training Epoch: 1/2, step 3413/7134 completed (loss: 0.23672373592853546, acc: 0.9658119678497314)
[2025-02-13 19:17:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:50][root][INFO] - Training Epoch: 1/2, step 3414/7134 completed (loss: 0.24021989107131958, acc: 0.9305555820465088)
[2025-02-13 19:17:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:50][root][INFO] - Training Epoch: 1/2, step 3415/7134 completed (loss: 0.11522384732961655, acc: 0.9642857313156128)
[2025-02-13 19:17:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:51][root][INFO] - Training Epoch: 1/2, step 3416/7134 completed (loss: 0.24031922221183777, acc: 0.9453551769256592)
[2025-02-13 19:17:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:51][root][INFO] - Training Epoch: 1/2, step 3417/7134 completed (loss: 0.1701018512248993, acc: 0.9640718698501587)
[2025-02-13 19:17:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:52][root][INFO] - Training Epoch: 1/2, step 3418/7134 completed (loss: 0.30847108364105225, acc: 0.9325153231620789)
[2025-02-13 19:17:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:52][root][INFO] - Training Epoch: 1/2, step 3419/7134 completed (loss: 0.12019553780555725, acc: 0.9578313231468201)
[2025-02-13 19:17:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:53][root][INFO] - Training Epoch: 1/2, step 3420/7134 completed (loss: 0.05911332741379738, acc: 0.9933775067329407)
[2025-02-13 19:17:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:53][root][INFO] - Training Epoch: 1/2, step 3421/7134 completed (loss: 0.28161218762397766, acc: 0.9551281929016113)
[2025-02-13 19:17:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:53][root][INFO] - Training Epoch: 1/2, step 3422/7134 completed (loss: 0.07638544589281082, acc: 0.970588207244873)
[2025-02-13 19:17:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:54][root][INFO] - Training Epoch: 1/2, step 3423/7134 completed (loss: 0.04857971891760826, acc: 0.9934210777282715)
[2025-02-13 19:17:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:54][root][INFO] - Training Epoch: 1/2, step 3424/7134 completed (loss: 0.41060012578964233, acc: 0.9133333563804626)
[2025-02-13 19:17:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:54][root][INFO] - Training Epoch: 1/2, step 3425/7134 completed (loss: 0.37632256746292114, acc: 0.9368420839309692)
[2025-02-13 19:17:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:55][root][INFO] - Training Epoch: 1/2, step 3426/7134 completed (loss: 0.22509600222110748, acc: 0.9642857313156128)
[2025-02-13 19:17:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:55][root][INFO] - Training Epoch: 1/2, step 3427/7134 completed (loss: 0.21072746813297272, acc: 0.9675324559211731)
[2025-02-13 19:17:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:56][root][INFO] - Training Epoch: 1/2, step 3428/7134 completed (loss: 0.16906453669071198, acc: 0.970588207244873)
[2025-02-13 19:17:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:56][root][INFO] - Training Epoch: 1/2, step 3429/7134 completed (loss: 0.22051389515399933, acc: 0.9507042169570923)
[2025-02-13 19:17:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:56][root][INFO] - Training Epoch: 1/2, step 3430/7134 completed (loss: 0.3318261206150055, acc: 0.9520547986030579)
[2025-02-13 19:17:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:57][root][INFO] - Training Epoch: 1/2, step 3431/7134 completed (loss: 0.22996243834495544, acc: 0.9420289993286133)
[2025-02-13 19:17:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:57][root][INFO] - Training Epoch: 1/2, step 3432/7134 completed (loss: 0.17859488725662231, acc: 0.9631901979446411)
[2025-02-13 19:17:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:58][root][INFO] - Training Epoch: 1/2, step 3433/7134 completed (loss: 0.35048505663871765, acc: 0.9117646813392639)
[2025-02-13 19:17:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:58][root][INFO] - Training Epoch: 1/2, step 3434/7134 completed (loss: 0.2673552334308624, acc: 0.9357143044471741)
[2025-02-13 19:17:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:58][root][INFO] - Training Epoch: 1/2, step 3435/7134 completed (loss: 0.2771560847759247, acc: 0.931034505367279)
[2025-02-13 19:17:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:59][root][INFO] - Training Epoch: 1/2, step 3436/7134 completed (loss: 0.24579675495624542, acc: 0.9197080135345459)
[2025-02-13 19:17:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:59][root][INFO] - Training Epoch: 1/2, step 3437/7134 completed (loss: 0.35234683752059937, acc: 0.9173553586006165)
[2025-02-13 19:17:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:17:59][root][INFO] - Training Epoch: 1/2, step 3438/7134 completed (loss: 0.2757020890712738, acc: 0.9385964870452881)
[2025-02-13 19:18:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:00][root][INFO] - Training Epoch: 1/2, step 3439/7134 completed (loss: 0.39246344566345215, acc: 0.9285714030265808)
[2025-02-13 19:18:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:00][root][INFO] - Training Epoch: 1/2, step 3440/7134 completed (loss: 0.4471818208694458, acc: 0.8793103694915771)
[2025-02-13 19:18:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:01][root][INFO] - Training Epoch: 1/2, step 3441/7134 completed (loss: 0.34987884759902954, acc: 0.9069767594337463)
[2025-02-13 19:18:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:01][root][INFO] - Training Epoch: 1/2, step 3442/7134 completed (loss: 0.20308515429496765, acc: 0.9432623982429504)
[2025-02-13 19:18:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:01][root][INFO] - Training Epoch: 1/2, step 3443/7134 completed (loss: 0.4378640651702881, acc: 0.9101123809814453)
[2025-02-13 19:18:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:02][root][INFO] - Training Epoch: 1/2, step 3444/7134 completed (loss: 0.2271714210510254, acc: 0.9363057613372803)
[2025-02-13 19:18:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:02][root][INFO] - Training Epoch: 1/2, step 3445/7134 completed (loss: 0.3705374300479889, acc: 0.9172932505607605)
[2025-02-13 19:18:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:02][root][INFO] - Training Epoch: 1/2, step 3446/7134 completed (loss: 0.4195536673069, acc: 0.8933333158493042)
[2025-02-13 19:18:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:03][root][INFO] - Training Epoch: 1/2, step 3447/7134 completed (loss: 0.47896063327789307, acc: 0.8888888955116272)
[2025-02-13 19:18:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:03][root][INFO] - Training Epoch: 1/2, step 3448/7134 completed (loss: 0.3792705535888672, acc: 0.913294792175293)
[2025-02-13 19:18:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:04][root][INFO] - Training Epoch: 1/2, step 3449/7134 completed (loss: 0.32665833830833435, acc: 0.9207921028137207)
[2025-02-13 19:18:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:04][root][INFO] - Training Epoch: 1/2, step 3450/7134 completed (loss: 0.3572379946708679, acc: 0.9019607901573181)
[2025-02-13 19:18:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:04][root][INFO] - Training Epoch: 1/2, step 3451/7134 completed (loss: 0.4200299382209778, acc: 0.9007092118263245)
[2025-02-13 19:18:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:05][root][INFO] - Training Epoch: 1/2, step 3452/7134 completed (loss: 0.5862116813659668, acc: 0.8861788511276245)
[2025-02-13 19:18:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:05][root][INFO] - Training Epoch: 1/2, step 3453/7134 completed (loss: 0.32578176259994507, acc: 0.9305555820465088)
[2025-02-13 19:18:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:06][root][INFO] - Training Epoch: 1/2, step 3454/7134 completed (loss: 0.2911983132362366, acc: 0.9337349534034729)
[2025-02-13 19:18:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:06][root][INFO] - Training Epoch: 1/2, step 3455/7134 completed (loss: 0.09795038402080536, acc: 0.9751552939414978)
[2025-02-13 19:18:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:06][root][INFO] - Training Epoch: 1/2, step 3456/7134 completed (loss: 0.11768363416194916, acc: 0.979899525642395)
[2025-02-13 19:18:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:07][root][INFO] - Training Epoch: 1/2, step 3457/7134 completed (loss: 0.11506348103284836, acc: 0.9647887349128723)
[2025-02-13 19:18:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:07][root][INFO] - Training Epoch: 1/2, step 3458/7134 completed (loss: 0.3442811667919159, acc: 0.9340659379959106)
[2025-02-13 19:18:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:08][root][INFO] - Training Epoch: 1/2, step 3459/7134 completed (loss: 0.2629489302635193, acc: 0.9433962106704712)
[2025-02-13 19:18:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:08][root][INFO] - Training Epoch: 1/2, step 3460/7134 completed (loss: 1.1707916259765625, acc: 0.7676767706871033)
[2025-02-13 19:18:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:08][root][INFO] - Training Epoch: 1/2, step 3461/7134 completed (loss: 1.1289777755737305, acc: 0.7615384459495544)
[2025-02-13 19:18:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:09][root][INFO] - Training Epoch: 1/2, step 3462/7134 completed (loss: 0.46304625272750854, acc: 0.9005848169326782)
[2025-02-13 19:18:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:09][root][INFO] - Training Epoch: 1/2, step 3463/7134 completed (loss: 0.10988367348909378, acc: 0.9814814925193787)
[2025-02-13 19:18:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:09][root][INFO] - Training Epoch: 1/2, step 3464/7134 completed (loss: 0.22523798048496246, acc: 0.932584285736084)
[2025-02-13 19:18:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:10][root][INFO] - Training Epoch: 1/2, step 3465/7134 completed (loss: 0.4227263033390045, acc: 0.8888888955116272)
[2025-02-13 19:18:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:10][root][INFO] - Training Epoch: 1/2, step 3466/7134 completed (loss: 0.30481860041618347, acc: 0.9320987462997437)
[2025-02-13 19:18:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:11][root][INFO] - Training Epoch: 1/2, step 3467/7134 completed (loss: 0.135429248213768, acc: 0.9576271176338196)
[2025-02-13 19:18:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:11][root][INFO] - Training Epoch: 1/2, step 3468/7134 completed (loss: 0.09247420728206635, acc: 0.9736841917037964)
[2025-02-13 19:18:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:11][root][INFO] - Training Epoch: 1/2, step 3469/7134 completed (loss: 0.19538980722427368, acc: 0.9398496150970459)
[2025-02-13 19:18:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:12][root][INFO] - Training Epoch: 1/2, step 3470/7134 completed (loss: 0.0556589812040329, acc: 1.0)
[2025-02-13 19:18:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:12][root][INFO] - Training Epoch: 1/2, step 3471/7134 completed (loss: 0.2519867718219757, acc: 0.9333333373069763)
[2025-02-13 19:18:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:12][root][INFO] - Training Epoch: 1/2, step 3472/7134 completed (loss: 0.10551978647708893, acc: 0.9681528806686401)
[2025-02-13 19:18:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:13][root][INFO] - Training Epoch: 1/2, step 3473/7134 completed (loss: 0.13176943361759186, acc: 0.9605262875556946)
[2025-02-13 19:18:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:13][root][INFO] - Training Epoch: 1/2, step 3474/7134 completed (loss: 0.24645009636878967, acc: 0.9347826242446899)
[2025-02-13 19:18:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:14][root][INFO] - Training Epoch: 1/2, step 3475/7134 completed (loss: 0.1500004082918167, acc: 0.9545454382896423)
[2025-02-13 19:18:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:14][root][INFO] - Training Epoch: 1/2, step 3476/7134 completed (loss: 0.4045344293117523, acc: 0.931506872177124)
[2025-02-13 19:18:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:14][root][INFO] - Training Epoch: 1/2, step 3477/7134 completed (loss: 0.2303318828344345, acc: 0.9662162065505981)
[2025-02-13 19:18:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:15][root][INFO] - Training Epoch: 1/2, step 3478/7134 completed (loss: 0.09105872362852097, acc: 0.9820359349250793)
[2025-02-13 19:18:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:15][root][INFO] - Training Epoch: 1/2, step 3479/7134 completed (loss: 0.03164124861359596, acc: 1.0)
[2025-02-13 19:18:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:16][root][INFO] - Training Epoch: 1/2, step 3480/7134 completed (loss: 0.2212272435426712, acc: 0.9395604133605957)
[2025-02-13 19:18:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:16][root][INFO] - Training Epoch: 1/2, step 3481/7134 completed (loss: 0.076960489153862, acc: 0.9924812316894531)
[2025-02-13 19:18:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:16][root][INFO] - Training Epoch: 1/2, step 3482/7134 completed (loss: 0.034532107412815094, acc: 0.9921875)
[2025-02-13 19:18:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:17][root][INFO] - Training Epoch: 1/2, step 3483/7134 completed (loss: 0.058406658470630646, acc: 0.9935483932495117)
[2025-02-13 19:18:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:17][root][INFO] - Training Epoch: 1/2, step 3484/7134 completed (loss: 0.23637042939662933, acc: 0.9642857313156128)
[2025-02-13 19:18:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:18][root][INFO] - Training Epoch: 1/2, step 3485/7134 completed (loss: 0.2216198593378067, acc: 0.9454545378684998)
[2025-02-13 19:18:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:18][root][INFO] - Training Epoch: 1/2, step 3486/7134 completed (loss: 0.34702807664871216, acc: 0.9230769276618958)
[2025-02-13 19:18:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:18][root][INFO] - Training Epoch: 1/2, step 3487/7134 completed (loss: 0.3617246747016907, acc: 0.9096385836601257)
[2025-02-13 19:18:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:19][root][INFO] - Training Epoch: 1/2, step 3488/7134 completed (loss: 0.19399355351924896, acc: 0.9642857313156128)
[2025-02-13 19:18:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:19][root][INFO] - Training Epoch: 1/2, step 3489/7134 completed (loss: 0.3714594542980194, acc: 0.8814814686775208)
[2025-02-13 19:18:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:19][root][INFO] - Training Epoch: 1/2, step 3490/7134 completed (loss: 0.26482391357421875, acc: 0.9440559148788452)
[2025-02-13 19:18:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:20][root][INFO] - Training Epoch: 1/2, step 3491/7134 completed (loss: 0.25354859232902527, acc: 0.9363057613372803)
[2025-02-13 19:18:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:20][root][INFO] - Training Epoch: 1/2, step 3492/7134 completed (loss: 0.18989697098731995, acc: 0.9724770784378052)
[2025-02-13 19:18:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:21][root][INFO] - Training Epoch: 1/2, step 3493/7134 completed (loss: 0.23424817621707916, acc: 0.9259259104728699)
[2025-02-13 19:18:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:21][root][INFO] - Training Epoch: 1/2, step 3494/7134 completed (loss: 0.23176522552967072, acc: 0.9391891956329346)
[2025-02-13 19:18:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:21][root][INFO] - Training Epoch: 1/2, step 3495/7134 completed (loss: 0.09724175930023193, acc: 0.9793814420700073)
[2025-02-13 19:18:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:22][root][INFO] - Training Epoch: 1/2, step 3496/7134 completed (loss: 0.21282391250133514, acc: 0.9607843160629272)
[2025-02-13 19:18:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:22][root][INFO] - Training Epoch: 1/2, step 3497/7134 completed (loss: 0.2277524471282959, acc: 0.949367105960846)
[2025-02-13 19:18:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:22][root][INFO] - Training Epoch: 1/2, step 3498/7134 completed (loss: 0.35803741216659546, acc: 0.905063271522522)
[2025-02-13 19:18:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:23][root][INFO] - Training Epoch: 1/2, step 3499/7134 completed (loss: 0.3203011453151703, acc: 0.9281045794487)
[2025-02-13 19:18:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:23][root][INFO] - Training Epoch: 1/2, step 3500/7134 completed (loss: 0.3131674826145172, acc: 0.9356725215911865)
[2025-02-13 19:18:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:24][root][INFO] - Training Epoch: 1/2, step 3501/7134 completed (loss: 0.31034189462661743, acc: 0.9242424368858337)
[2025-02-13 19:18:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:24][root][INFO] - Training Epoch: 1/2, step 3502/7134 completed (loss: 0.46688950061798096, acc: 0.8783783912658691)
[2025-02-13 19:18:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:24][root][INFO] - Training Epoch: 1/2, step 3503/7134 completed (loss: 0.387722373008728, acc: 0.9136690497398376)
[2025-02-13 19:18:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:25][root][INFO] - Training Epoch: 1/2, step 3504/7134 completed (loss: 0.09753740578889847, acc: 0.9733333587646484)
[2025-02-13 19:18:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:25][root][INFO] - Training Epoch: 1/2, step 3505/7134 completed (loss: 0.18380741775035858, acc: 0.9523809552192688)
[2025-02-13 19:18:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:26][root][INFO] - Training Epoch: 1/2, step 3506/7134 completed (loss: 0.18820372223854065, acc: 0.961240291595459)
[2025-02-13 19:18:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:26][root][INFO] - Training Epoch: 1/2, step 3507/7134 completed (loss: 0.10264690220355988, acc: 0.9791666865348816)
[2025-02-13 19:18:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:26][root][INFO] - Training Epoch: 1/2, step 3508/7134 completed (loss: 0.10210530459880829, acc: 0.9801324605941772)
[2025-02-13 19:18:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:27][root][INFO] - Training Epoch: 1/2, step 3509/7134 completed (loss: 0.293044775724411, acc: 0.9447852969169617)
[2025-02-13 19:18:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:27][root][INFO] - Training Epoch: 1/2, step 3510/7134 completed (loss: 0.2319067269563675, acc: 0.9473684430122375)
[2025-02-13 19:18:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:28][root][INFO] - Training Epoch: 1/2, step 3511/7134 completed (loss: 0.07086256891489029, acc: 0.9779411554336548)
[2025-02-13 19:18:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:28][root][INFO] - Training Epoch: 1/2, step 3512/7134 completed (loss: 0.40725353360176086, acc: 0.9294871687889099)
[2025-02-13 19:18:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:29][root][INFO] - Training Epoch: 1/2, step 3513/7134 completed (loss: 0.11982474476099014, acc: 0.9710144996643066)
[2025-02-13 19:18:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:29][root][INFO] - Training Epoch: 1/2, step 3514/7134 completed (loss: 0.1292581856250763, acc: 0.9760765433311462)
[2025-02-13 19:18:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:29][root][INFO] - Training Epoch: 1/2, step 3515/7134 completed (loss: 0.08292435109615326, acc: 0.9774011373519897)
[2025-02-13 19:18:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:30][root][INFO] - Training Epoch: 1/2, step 3516/7134 completed (loss: 0.10096731781959534, acc: 0.9736841917037964)
[2025-02-13 19:18:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:30][root][INFO] - Training Epoch: 1/2, step 3517/7134 completed (loss: 0.13360974192619324, acc: 0.9595959782600403)
[2025-02-13 19:18:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:30][root][INFO] - Training Epoch: 1/2, step 3518/7134 completed (loss: 0.16627027094364166, acc: 0.9629629850387573)
[2025-02-13 19:18:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:31][root][INFO] - Training Epoch: 1/2, step 3519/7134 completed (loss: 0.11984912306070328, acc: 0.9688888788223267)
[2025-02-13 19:18:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:31][root][INFO] - Training Epoch: 1/2, step 3520/7134 completed (loss: 0.2173348218202591, acc: 0.9545454382896423)
[2025-02-13 19:18:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:32][root][INFO] - Training Epoch: 1/2, step 3521/7134 completed (loss: 0.07585397362709045, acc: 0.9820359349250793)
[2025-02-13 19:18:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:32][root][INFO] - Training Epoch: 1/2, step 3522/7134 completed (loss: 0.11574988812208176, acc: 0.9666666388511658)
[2025-02-13 19:18:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:32][root][INFO] - Training Epoch: 1/2, step 3523/7134 completed (loss: 0.03588775172829628, acc: 0.9867549538612366)
[2025-02-13 19:18:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:33][root][INFO] - Training Epoch: 1/2, step 3524/7134 completed (loss: 0.14002425968647003, acc: 0.9725274443626404)
[2025-02-13 19:18:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:33][root][INFO] - Training Epoch: 1/2, step 3525/7134 completed (loss: 0.1165611669421196, acc: 0.9729729890823364)
[2025-02-13 19:18:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:34][root][INFO] - Training Epoch: 1/2, step 3526/7134 completed (loss: 0.16140130162239075, acc: 0.9595959782600403)
[2025-02-13 19:18:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:34][root][INFO] - Training Epoch: 1/2, step 3527/7134 completed (loss: 0.16843873262405396, acc: 0.9627659320831299)
[2025-02-13 19:18:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:34][root][INFO] - Training Epoch: 1/2, step 3528/7134 completed (loss: 0.03250115364789963, acc: 1.0)
[2025-02-13 19:18:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:35][root][INFO] - Training Epoch: 1/2, step 3529/7134 completed (loss: 0.16928286850452423, acc: 0.9545454382896423)
[2025-02-13 19:18:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:35][root][INFO] - Training Epoch: 1/2, step 3530/7134 completed (loss: 0.3363868296146393, acc: 0.8982300758361816)
[2025-02-13 19:18:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:36][root][INFO] - Training Epoch: 1/2, step 3531/7134 completed (loss: 0.17819061875343323, acc: 0.949999988079071)
[2025-02-13 19:18:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:36][root][INFO] - Training Epoch: 1/2, step 3532/7134 completed (loss: 0.14683951437473297, acc: 0.9594594836235046)
[2025-02-13 19:18:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:36][root][INFO] - Training Epoch: 1/2, step 3533/7134 completed (loss: 0.15163001418113708, acc: 0.9688888788223267)
[2025-02-13 19:18:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:37][root][INFO] - Training Epoch: 1/2, step 3534/7134 completed (loss: 0.1642727553844452, acc: 0.9532163739204407)
[2025-02-13 19:18:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:37][root][INFO] - Training Epoch: 1/2, step 3535/7134 completed (loss: 0.1813320517539978, acc: 0.9568345546722412)
[2025-02-13 19:18:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:38][root][INFO] - Training Epoch: 1/2, step 3536/7134 completed (loss: 0.2526543438434601, acc: 0.9320987462997437)
[2025-02-13 19:18:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:38][root][INFO] - Training Epoch: 1/2, step 3537/7134 completed (loss: 0.1217178925871849, acc: 0.96875)
[2025-02-13 19:18:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:38][root][INFO] - Training Epoch: 1/2, step 3538/7134 completed (loss: 0.1210697740316391, acc: 0.9747899174690247)
[2025-02-13 19:18:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:39][root][INFO] - Training Epoch: 1/2, step 3539/7134 completed (loss: 0.2486991286277771, acc: 0.931034505367279)
[2025-02-13 19:18:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:39][root][INFO] - Training Epoch: 1/2, step 3540/7134 completed (loss: 0.17318196594715118, acc: 0.9692307710647583)
[2025-02-13 19:18:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:39][root][INFO] - Training Epoch: 1/2, step 3541/7134 completed (loss: 0.17051109671592712, acc: 0.9599999785423279)
[2025-02-13 19:18:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:40][root][INFO] - Training Epoch: 1/2, step 3542/7134 completed (loss: 0.11454011499881744, acc: 0.9726027250289917)
[2025-02-13 19:18:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:40][root][INFO] - Training Epoch: 1/2, step 3543/7134 completed (loss: 0.2481904923915863, acc: 0.9366196990013123)
[2025-02-13 19:18:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:41][root][INFO] - Training Epoch: 1/2, step 3544/7134 completed (loss: 0.3159242272377014, acc: 0.9271523356437683)
[2025-02-13 19:18:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:41][root][INFO] - Training Epoch: 1/2, step 3545/7134 completed (loss: 0.1312483251094818, acc: 0.9583333134651184)
[2025-02-13 19:18:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:41][root][INFO] - Training Epoch: 1/2, step 3546/7134 completed (loss: 0.3484342098236084, acc: 0.9280575513839722)
[2025-02-13 19:18:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:42][root][INFO] - Training Epoch: 1/2, step 3547/7134 completed (loss: 0.3952271640300751, acc: 0.9152542352676392)
[2025-02-13 19:18:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:42][root][INFO] - Training Epoch: 1/2, step 3548/7134 completed (loss: 0.27591076493263245, acc: 0.9266055226325989)
[2025-02-13 19:18:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:43][root][INFO] - Training Epoch: 1/2, step 3549/7134 completed (loss: 0.2735922932624817, acc: 0.9624060392379761)
[2025-02-13 19:18:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:43][root][INFO] - Training Epoch: 1/2, step 3550/7134 completed (loss: 0.2418563961982727, acc: 0.956204354763031)
[2025-02-13 19:18:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:43][root][INFO] - Training Epoch: 1/2, step 3551/7134 completed (loss: 0.43107226490974426, acc: 0.8911564350128174)
[2025-02-13 19:18:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:44][root][INFO] - Training Epoch: 1/2, step 3552/7134 completed (loss: 0.31312838196754456, acc: 0.913385808467865)
[2025-02-13 19:18:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:44][root][INFO] - Training Epoch: 1/2, step 3553/7134 completed (loss: 0.25218310952186584, acc: 0.9264705777168274)
[2025-02-13 19:18:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:45][root][INFO] - Training Epoch: 1/2, step 3554/7134 completed (loss: 0.4541679918766022, acc: 0.8895705342292786)
[2025-02-13 19:18:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:45][root][INFO] - Training Epoch: 1/2, step 3555/7134 completed (loss: 0.4906417727470398, acc: 0.8903225660324097)
[2025-02-13 19:18:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:45][root][INFO] - Training Epoch: 1/2, step 3556/7134 completed (loss: 0.28533247113227844, acc: 0.9090909361839294)
[2025-02-13 19:18:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:46][root][INFO] - Training Epoch: 1/2, step 3557/7134 completed (loss: 0.28436753153800964, acc: 0.9078013896942139)
[2025-02-13 19:18:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:46][root][INFO] - Training Epoch: 1/2, step 3558/7134 completed (loss: 0.46891292929649353, acc: 0.8799999952316284)
[2025-02-13 19:18:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:47][root][INFO] - Training Epoch: 1/2, step 3559/7134 completed (loss: 0.3456951677799225, acc: 0.9166666865348816)
[2025-02-13 19:18:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:47][root][INFO] - Training Epoch: 1/2, step 3560/7134 completed (loss: 0.2151043862104416, acc: 0.9562841653823853)
[2025-02-13 19:18:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:47][root][INFO] - Training Epoch: 1/2, step 3561/7134 completed (loss: 0.2517808973789215, acc: 0.9491525292396545)
[2025-02-13 19:18:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:48][root][INFO] - Training Epoch: 1/2, step 3562/7134 completed (loss: 0.2603868246078491, acc: 0.9558823704719543)
[2025-02-13 19:18:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:48][root][INFO] - Training Epoch: 1/2, step 3563/7134 completed (loss: 0.3165440559387207, acc: 0.9290780425071716)
[2025-02-13 19:18:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:48][root][INFO] - Training Epoch: 1/2, step 3564/7134 completed (loss: 0.4614911675453186, acc: 0.8809523582458496)
[2025-02-13 19:18:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:49][root][INFO] - Training Epoch: 1/2, step 3565/7134 completed (loss: 0.4433231055736542, acc: 0.8823529481887817)
[2025-02-13 19:18:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:18:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:19:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:20:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:21:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:53][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.3415, device='cuda:0') eval_epoch_loss=tensor(0.2938, device='cuda:0') eval_epoch_acc=tensor(0.9299, device='cuda:0')
[2025-02-13 19:22:53][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-13 19:22:53][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-13 19:22:54][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_wavlm_llama32_1b_linear_peft_transfer_librispeech-100/asr_epoch_1_step_3566_loss_0.2937663793563843/model.pt
[2025-02-13 19:22:54][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_wavlm_llama32_1b_linear_peft_transfer_librispeech-100 directory
[2025-02-13 19:22:54][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.2937663793563843
[2025-02-13 19:22:54][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.929926872253418
[2025-02-13 19:22:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:54][root][INFO] - Training Epoch: 1/2, step 3566/7134 completed (loss: 0.45324188470840454, acc: 0.8823529481887817)
[2025-02-13 19:22:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:54][root][INFO] - Training Epoch: 1/2, step 3567/7134 completed (loss: 0.22354291379451752, acc: 0.949438214302063)
[2025-02-13 19:22:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:55][root][INFO] - Training Epoch: 1/2, step 3568/7134 completed (loss: 0.38358765840530396, acc: 0.914893627166748)
[2025-02-13 19:22:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:55][root][INFO] - Training Epoch: 1/2, step 3569/7134 completed (loss: 0.29842355847358704, acc: 0.9333333373069763)
[2025-02-13 19:22:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:56][root][INFO] - Training Epoch: 1/2, step 3570/7134 completed (loss: 0.5469440221786499, acc: 0.845588207244873)
[2025-02-13 19:22:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:56][root][INFO] - Training Epoch: 1/2, step 3571/7134 completed (loss: 0.4838346540927887, acc: 0.882022500038147)
[2025-02-13 19:22:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:56][root][INFO] - Training Epoch: 1/2, step 3572/7134 completed (loss: 0.1649400144815445, acc: 0.9642857313156128)
[2025-02-13 19:22:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:57][root][INFO] - Training Epoch: 1/2, step 3573/7134 completed (loss: 0.2562585473060608, acc: 0.9539473652839661)
[2025-02-13 19:22:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:57][root][INFO] - Training Epoch: 1/2, step 3574/7134 completed (loss: 0.5055402517318726, acc: 0.9173553586006165)
[2025-02-13 19:22:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:58][root][INFO] - Training Epoch: 1/2, step 3575/7134 completed (loss: 0.6230276226997375, acc: 0.8881579041481018)
[2025-02-13 19:22:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:58][root][INFO] - Training Epoch: 1/2, step 3576/7134 completed (loss: 0.34974175691604614, acc: 0.9179104566574097)
[2025-02-13 19:22:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:58][root][INFO] - Training Epoch: 1/2, step 3577/7134 completed (loss: 0.3015594780445099, acc: 0.9123711585998535)
[2025-02-13 19:22:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:59][root][INFO] - Training Epoch: 1/2, step 3578/7134 completed (loss: 0.1944332867860794, acc: 0.9638554453849792)
[2025-02-13 19:22:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:22:59][root][INFO] - Training Epoch: 1/2, step 3579/7134 completed (loss: 0.3676125705242157, acc: 0.930232584476471)
[2025-02-13 19:22:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:00][root][INFO] - Training Epoch: 1/2, step 3580/7134 completed (loss: 0.5650973916053772, acc: 0.849711000919342)
[2025-02-13 19:23:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:00][root][INFO] - Training Epoch: 1/2, step 3581/7134 completed (loss: 0.41096165776252747, acc: 0.8647058606147766)
[2025-02-13 19:23:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:00][root][INFO] - Training Epoch: 1/2, step 3582/7134 completed (loss: 0.4283583462238312, acc: 0.8888888955116272)
[2025-02-13 19:23:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:01][root][INFO] - Training Epoch: 1/2, step 3583/7134 completed (loss: 0.40797293186187744, acc: 0.886227548122406)
[2025-02-13 19:23:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:01][root][INFO] - Training Epoch: 1/2, step 3584/7134 completed (loss: 0.539626955986023, acc: 0.8829787373542786)
[2025-02-13 19:23:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:01][root][INFO] - Training Epoch: 1/2, step 3585/7134 completed (loss: 0.51700359582901, acc: 0.9141104221343994)
[2025-02-13 19:23:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:02][root][INFO] - Training Epoch: 1/2, step 3586/7134 completed (loss: 0.3572048544883728, acc: 0.9171974658966064)
[2025-02-13 19:23:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:02][root][INFO] - Training Epoch: 1/2, step 3587/7134 completed (loss: 0.3303460478782654, acc: 0.9300699234008789)
[2025-02-13 19:23:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:03][root][INFO] - Training Epoch: 1/2, step 3588/7134 completed (loss: 0.2504907250404358, acc: 0.9306358098983765)
[2025-02-13 19:23:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:03][root][INFO] - Training Epoch: 1/2, step 3589/7134 completed (loss: 0.38635796308517456, acc: 0.9344262480735779)
[2025-02-13 19:23:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:03][root][INFO] - Training Epoch: 1/2, step 3590/7134 completed (loss: 0.11461829394102097, acc: 0.9777777791023254)
[2025-02-13 19:23:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:04][root][INFO] - Training Epoch: 1/2, step 3591/7134 completed (loss: 0.20639535784721375, acc: 0.9649122953414917)
[2025-02-13 19:23:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:04][root][INFO] - Training Epoch: 1/2, step 3592/7134 completed (loss: 0.0716731920838356, acc: 0.9857142567634583)
[2025-02-13 19:23:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:05][root][INFO] - Training Epoch: 1/2, step 3593/7134 completed (loss: 0.4113750755786896, acc: 0.9366196990013123)
[2025-02-13 19:23:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:05][root][INFO] - Training Epoch: 1/2, step 3594/7134 completed (loss: 0.177161306142807, acc: 0.940397322177887)
[2025-02-13 19:23:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:06][root][INFO] - Training Epoch: 1/2, step 3595/7134 completed (loss: 0.2972283661365509, acc: 0.9542483687400818)
[2025-02-13 19:23:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:06][root][INFO] - Training Epoch: 1/2, step 3596/7134 completed (loss: 0.3864153325557709, acc: 0.9117646813392639)
[2025-02-13 19:23:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:06][root][INFO] - Training Epoch: 1/2, step 3597/7134 completed (loss: 0.1834639459848404, acc: 0.9539473652839661)
[2025-02-13 19:23:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:07][root][INFO] - Training Epoch: 1/2, step 3598/7134 completed (loss: 0.36400049924850464, acc: 0.9241706132888794)
[2025-02-13 19:23:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:07][root][INFO] - Training Epoch: 1/2, step 3599/7134 completed (loss: 0.40016260743141174, acc: 0.9135135412216187)
[2025-02-13 19:23:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:07][root][INFO] - Training Epoch: 1/2, step 3600/7134 completed (loss: 0.37759339809417725, acc: 0.9222797751426697)
[2025-02-13 19:23:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:08][root][INFO] - Training Epoch: 1/2, step 3601/7134 completed (loss: 0.7871689796447754, acc: 0.8258426785469055)
[2025-02-13 19:23:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:08][root][INFO] - Training Epoch: 1/2, step 3602/7134 completed (loss: 0.4666866958141327, acc: 0.8883720636367798)
[2025-02-13 19:23:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:09][root][INFO] - Training Epoch: 1/2, step 3603/7134 completed (loss: 0.44094139337539673, acc: 0.9119496941566467)
[2025-02-13 19:23:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:09][root][INFO] - Training Epoch: 1/2, step 3604/7134 completed (loss: 0.4250505566596985, acc: 0.913385808467865)
[2025-02-13 19:23:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:09][root][INFO] - Training Epoch: 1/2, step 3605/7134 completed (loss: 0.34768056869506836, acc: 0.9237667918205261)
[2025-02-13 19:23:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:10][root][INFO] - Training Epoch: 1/2, step 3606/7134 completed (loss: 0.7558403611183167, acc: 0.8424657583236694)
[2025-02-13 19:23:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:10][root][INFO] - Training Epoch: 1/2, step 3607/7134 completed (loss: 0.2709842026233673, acc: 0.931506872177124)
[2025-02-13 19:23:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:11][root][INFO] - Training Epoch: 1/2, step 3608/7134 completed (loss: 0.36916905641555786, acc: 0.9223300814628601)
[2025-02-13 19:23:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:11][root][INFO] - Training Epoch: 1/2, step 3609/7134 completed (loss: 0.24076122045516968, acc: 0.9368420839309692)
[2025-02-13 19:23:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:11][root][INFO] - Training Epoch: 1/2, step 3610/7134 completed (loss: 0.4038344919681549, acc: 0.8956043720245361)
[2025-02-13 19:23:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:12][root][INFO] - Training Epoch: 1/2, step 3611/7134 completed (loss: 0.31960731744766235, acc: 0.9371428489685059)
[2025-02-13 19:23:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:12][root][INFO] - Training Epoch: 1/2, step 3612/7134 completed (loss: 0.5335208773612976, acc: 0.8836206793785095)
[2025-02-13 19:23:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:13][root][INFO] - Training Epoch: 1/2, step 3613/7134 completed (loss: 0.25687605142593384, acc: 0.9289617538452148)
[2025-02-13 19:23:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:13][root][INFO] - Training Epoch: 1/2, step 3614/7134 completed (loss: 0.19595162570476532, acc: 0.9680851101875305)
[2025-02-13 19:23:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:13][root][INFO] - Training Epoch: 1/2, step 3615/7134 completed (loss: 0.22662299871444702, acc: 0.9459459185600281)
[2025-02-13 19:23:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:14][root][INFO] - Training Epoch: 1/2, step 3616/7134 completed (loss: 0.42813554406166077, acc: 0.8757764101028442)
[2025-02-13 19:23:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:14][root][INFO] - Training Epoch: 1/2, step 3617/7134 completed (loss: 0.2260385900735855, acc: 0.9405405521392822)
[2025-02-13 19:23:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:15][root][INFO] - Training Epoch: 1/2, step 3618/7134 completed (loss: 0.4109354317188263, acc: 0.9018405079841614)
[2025-02-13 19:23:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:15][root][INFO] - Training Epoch: 1/2, step 3619/7134 completed (loss: 0.5245577692985535, acc: 0.8689956068992615)
[2025-02-13 19:23:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:15][root][INFO] - Training Epoch: 1/2, step 3620/7134 completed (loss: 0.27793771028518677, acc: 0.940119743347168)
[2025-02-13 19:23:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:16][root][INFO] - Training Epoch: 1/2, step 3621/7134 completed (loss: 0.26731106638908386, acc: 0.9235668778419495)
[2025-02-13 19:23:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:16][root][INFO] - Training Epoch: 1/2, step 3622/7134 completed (loss: 0.1617729663848877, acc: 0.9610389471054077)
[2025-02-13 19:23:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:16][root][INFO] - Training Epoch: 1/2, step 3623/7134 completed (loss: 0.20317746698856354, acc: 0.9464285969734192)
[2025-02-13 19:23:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:17][root][INFO] - Training Epoch: 1/2, step 3624/7134 completed (loss: 0.27980026602745056, acc: 0.935251772403717)
[2025-02-13 19:23:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:17][root][INFO] - Training Epoch: 1/2, step 3625/7134 completed (loss: 0.8145503997802734, acc: 0.8151260614395142)
[2025-02-13 19:23:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:18][root][INFO] - Training Epoch: 1/2, step 3626/7134 completed (loss: 1.0389173030853271, acc: 0.7865168452262878)
[2025-02-13 19:23:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:18][root][INFO] - Training Epoch: 1/2, step 3627/7134 completed (loss: 0.5160561203956604, acc: 0.8584905862808228)
[2025-02-13 19:23:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:18][root][INFO] - Training Epoch: 1/2, step 3628/7134 completed (loss: 0.559061586856842, acc: 0.8285714387893677)
[2025-02-13 19:23:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:19][root][INFO] - Training Epoch: 1/2, step 3629/7134 completed (loss: 0.47460854053497314, acc: 0.9193548560142517)
[2025-02-13 19:23:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:19][root][INFO] - Training Epoch: 1/2, step 3630/7134 completed (loss: 0.37231138348579407, acc: 0.9032257795333862)
[2025-02-13 19:23:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:20][root][INFO] - Training Epoch: 1/2, step 3631/7134 completed (loss: 0.4282782971858978, acc: 0.9285714030265808)
[2025-02-13 19:23:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:20][root][INFO] - Training Epoch: 1/2, step 3632/7134 completed (loss: 0.2386917620897293, acc: 0.9590163826942444)
[2025-02-13 19:23:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:20][root][INFO] - Training Epoch: 1/2, step 3633/7134 completed (loss: 0.32581931352615356, acc: 0.9171974658966064)
[2025-02-13 19:23:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:21][root][INFO] - Training Epoch: 1/2, step 3634/7134 completed (loss: 0.4523031413555145, acc: 0.8971962332725525)
[2025-02-13 19:23:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:21][root][INFO] - Training Epoch: 1/2, step 3635/7134 completed (loss: 0.3367154002189636, acc: 0.9051724076271057)
[2025-02-13 19:23:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:22][root][INFO] - Training Epoch: 1/2, step 3636/7134 completed (loss: 0.40223968029022217, acc: 0.8867924809455872)
[2025-02-13 19:23:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:22][root][INFO] - Training Epoch: 1/2, step 3637/7134 completed (loss: 0.40444421768188477, acc: 0.8909090757369995)
[2025-02-13 19:23:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:22][root][INFO] - Training Epoch: 1/2, step 3638/7134 completed (loss: 0.25222569704055786, acc: 0.9340659379959106)
[2025-02-13 19:23:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:23][root][INFO] - Training Epoch: 1/2, step 3639/7134 completed (loss: 0.269974946975708, acc: 0.957446813583374)
[2025-02-13 19:23:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:23][root][INFO] - Training Epoch: 1/2, step 3640/7134 completed (loss: 0.27197903394699097, acc: 0.93388432264328)
[2025-02-13 19:23:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:24][root][INFO] - Training Epoch: 1/2, step 3641/7134 completed (loss: 0.12326721847057343, acc: 0.9838709831237793)
[2025-02-13 19:23:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:24][root][INFO] - Training Epoch: 1/2, step 3642/7134 completed (loss: 0.1300112009048462, acc: 0.9731543660163879)
[2025-02-13 19:23:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:24][root][INFO] - Training Epoch: 1/2, step 3643/7134 completed (loss: 0.2726532816886902, acc: 0.9140625)
[2025-02-13 19:23:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:25][root][INFO] - Training Epoch: 1/2, step 3644/7134 completed (loss: 0.32543009519577026, acc: 0.9189189076423645)
[2025-02-13 19:23:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:25][root][INFO] - Training Epoch: 1/2, step 3645/7134 completed (loss: 0.583175778388977, acc: 0.890625)
[2025-02-13 19:23:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:26][root][INFO] - Training Epoch: 1/2, step 3646/7134 completed (loss: 0.5801931619644165, acc: 0.8636363744735718)
[2025-02-13 19:23:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:26][root][INFO] - Training Epoch: 1/2, step 3647/7134 completed (loss: 0.19304892420768738, acc: 0.966292142868042)
[2025-02-13 19:23:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:26][root][INFO] - Training Epoch: 1/2, step 3648/7134 completed (loss: 0.38643595576286316, acc: 0.9053254723548889)
[2025-02-13 19:23:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:27][root][INFO] - Training Epoch: 1/2, step 3649/7134 completed (loss: 0.427059143781662, acc: 0.9328858852386475)
[2025-02-13 19:23:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:27][root][INFO] - Training Epoch: 1/2, step 3650/7134 completed (loss: 0.2782580554485321, acc: 0.9171597361564636)
[2025-02-13 19:23:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:28][root][INFO] - Training Epoch: 1/2, step 3651/7134 completed (loss: 0.3553006649017334, acc: 0.9156626462936401)
[2025-02-13 19:23:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:28][root][INFO] - Training Epoch: 1/2, step 3652/7134 completed (loss: 0.3467431664466858, acc: 0.9251700639724731)
[2025-02-13 19:23:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:28][root][INFO] - Training Epoch: 1/2, step 3653/7134 completed (loss: 0.5987990498542786, acc: 0.8466257452964783)
[2025-02-13 19:23:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:29][root][INFO] - Training Epoch: 1/2, step 3654/7134 completed (loss: 0.0640764981508255, acc: 0.984000027179718)
[2025-02-13 19:23:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:29][root][INFO] - Training Epoch: 1/2, step 3655/7134 completed (loss: 0.46187007427215576, acc: 0.9041916131973267)
[2025-02-13 19:23:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:29][root][INFO] - Training Epoch: 1/2, step 3656/7134 completed (loss: 0.4835146963596344, acc: 0.8985507488250732)
[2025-02-13 19:23:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:30][root][INFO] - Training Epoch: 1/2, step 3657/7134 completed (loss: 0.18660202622413635, acc: 0.9440559148788452)
[2025-02-13 19:23:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:30][root][INFO] - Training Epoch: 1/2, step 3658/7134 completed (loss: 0.30372560024261475, acc: 0.9375)
[2025-02-13 19:23:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:31][root][INFO] - Training Epoch: 1/2, step 3659/7134 completed (loss: 0.3571104407310486, acc: 0.918367326259613)
[2025-02-13 19:23:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:31][root][INFO] - Training Epoch: 1/2, step 3660/7134 completed (loss: 0.5463446974754333, acc: 0.8395061492919922)
[2025-02-13 19:23:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:31][root][INFO] - Training Epoch: 1/2, step 3661/7134 completed (loss: 0.31617096066474915, acc: 0.8999999761581421)
[2025-02-13 19:23:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:32][root][INFO] - Training Epoch: 1/2, step 3662/7134 completed (loss: 0.5256189107894897, acc: 0.8757764101028442)
[2025-02-13 19:23:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:32][root][INFO] - Training Epoch: 1/2, step 3663/7134 completed (loss: 0.3803945779800415, acc: 0.90625)
[2025-02-13 19:23:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:33][root][INFO] - Training Epoch: 1/2, step 3664/7134 completed (loss: 0.5709106922149658, acc: 0.8441558480262756)
[2025-02-13 19:23:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:33][root][INFO] - Training Epoch: 1/2, step 3665/7134 completed (loss: 0.27980831265449524, acc: 0.912162184715271)
[2025-02-13 19:23:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:33][root][INFO] - Training Epoch: 1/2, step 3666/7134 completed (loss: 0.3770414888858795, acc: 0.8972602486610413)
[2025-02-13 19:23:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:34][root][INFO] - Training Epoch: 1/2, step 3667/7134 completed (loss: 0.7792654037475586, acc: 0.8062015771865845)
[2025-02-13 19:23:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:34][root][INFO] - Training Epoch: 1/2, step 3668/7134 completed (loss: 0.49135488271713257, acc: 0.8758620619773865)
[2025-02-13 19:23:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:34][root][INFO] - Training Epoch: 1/2, step 3669/7134 completed (loss: 0.14743998646736145, acc: 0.9586777091026306)
[2025-02-13 19:23:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:35][root][INFO] - Training Epoch: 1/2, step 3670/7134 completed (loss: 0.16252142190933228, acc: 0.9457831382751465)
[2025-02-13 19:23:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:35][root][INFO] - Training Epoch: 1/2, step 3671/7134 completed (loss: 0.2142190784215927, acc: 0.9367088675498962)
[2025-02-13 19:23:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:36][root][INFO] - Training Epoch: 1/2, step 3672/7134 completed (loss: 0.17492936551570892, acc: 0.939226508140564)
[2025-02-13 19:23:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:36][root][INFO] - Training Epoch: 1/2, step 3673/7134 completed (loss: 0.2539253532886505, acc: 0.9379310607910156)
[2025-02-13 19:23:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:36][root][INFO] - Training Epoch: 1/2, step 3674/7134 completed (loss: 0.22884507477283478, acc: 0.9527027010917664)
[2025-02-13 19:23:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:37][root][INFO] - Training Epoch: 1/2, step 3675/7134 completed (loss: 0.13837504386901855, acc: 0.9613259434700012)
[2025-02-13 19:23:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:37][root][INFO] - Training Epoch: 1/2, step 3676/7134 completed (loss: 0.24804997444152832, acc: 0.9230769276618958)
[2025-02-13 19:23:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:37][root][INFO] - Training Epoch: 1/2, step 3677/7134 completed (loss: 0.31761273741722107, acc: 0.9204545617103577)
[2025-02-13 19:23:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:38][root][INFO] - Training Epoch: 1/2, step 3678/7134 completed (loss: 0.4656447768211365, acc: 0.9240506291389465)
[2025-02-13 19:23:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:38][root][INFO] - Training Epoch: 1/2, step 3679/7134 completed (loss: 0.22504229843616486, acc: 0.9615384340286255)
[2025-02-13 19:23:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:39][root][INFO] - Training Epoch: 1/2, step 3680/7134 completed (loss: 0.30098238587379456, acc: 0.9266666769981384)
[2025-02-13 19:23:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:39][root][INFO] - Training Epoch: 1/2, step 3681/7134 completed (loss: 0.2106839418411255, acc: 0.9437500238418579)
[2025-02-13 19:23:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:40][root][INFO] - Training Epoch: 1/2, step 3682/7134 completed (loss: 0.1359042376279831, acc: 0.976190447807312)
[2025-02-13 19:23:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:40][root][INFO] - Training Epoch: 1/2, step 3683/7134 completed (loss: 0.11270938068628311, acc: 0.9632353186607361)
[2025-02-13 19:23:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:40][root][INFO] - Training Epoch: 1/2, step 3684/7134 completed (loss: 0.07898728549480438, acc: 0.9868420958518982)
[2025-02-13 19:23:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:41][root][INFO] - Training Epoch: 1/2, step 3685/7134 completed (loss: 0.19869354367256165, acc: 0.934959352016449)
[2025-02-13 19:23:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:41][root][INFO] - Training Epoch: 1/2, step 3686/7134 completed (loss: 0.1681460589170456, acc: 0.9647887349128723)
[2025-02-13 19:23:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:42][root][INFO] - Training Epoch: 1/2, step 3687/7134 completed (loss: 0.16979025304317474, acc: 0.9378882050514221)
[2025-02-13 19:23:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:42][root][INFO] - Training Epoch: 1/2, step 3688/7134 completed (loss: 0.10517771542072296, acc: 0.9833333492279053)
[2025-02-13 19:23:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:42][root][INFO] - Training Epoch: 1/2, step 3689/7134 completed (loss: 0.1776602566242218, acc: 0.957446813583374)
[2025-02-13 19:23:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:43][root][INFO] - Training Epoch: 1/2, step 3690/7134 completed (loss: 0.14158105850219727, acc: 0.9556962251663208)
[2025-02-13 19:23:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:43][root][INFO] - Training Epoch: 1/2, step 3691/7134 completed (loss: 0.14549313485622406, acc: 0.9504950642585754)
[2025-02-13 19:23:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:44][root][INFO] - Training Epoch: 1/2, step 3692/7134 completed (loss: 0.08621420711278915, acc: 0.987730085849762)
[2025-02-13 19:23:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:44][root][INFO] - Training Epoch: 1/2, step 3693/7134 completed (loss: 0.09627760946750641, acc: 0.9848484992980957)
[2025-02-13 19:23:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:44][root][INFO] - Training Epoch: 1/2, step 3694/7134 completed (loss: 0.14008773863315582, acc: 0.9624999761581421)
[2025-02-13 19:23:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:45][root][INFO] - Training Epoch: 1/2, step 3695/7134 completed (loss: 0.18273165822029114, acc: 0.9617834687232971)
[2025-02-13 19:23:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:45][root][INFO] - Training Epoch: 1/2, step 3696/7134 completed (loss: 0.19875623285770416, acc: 0.9655172228813171)
[2025-02-13 19:23:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:45][root][INFO] - Training Epoch: 1/2, step 3697/7134 completed (loss: 0.08049595355987549, acc: 0.9817073345184326)
[2025-02-13 19:23:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:46][root][INFO] - Training Epoch: 1/2, step 3698/7134 completed (loss: 0.10294884443283081, acc: 0.9724137783050537)
[2025-02-13 19:23:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:46][root][INFO] - Training Epoch: 1/2, step 3699/7134 completed (loss: 0.13303278386592865, acc: 0.9685039520263672)
[2025-02-13 19:23:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:47][root][INFO] - Training Epoch: 1/2, step 3700/7134 completed (loss: 0.1472744196653366, acc: 0.9685534834861755)
[2025-02-13 19:23:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:47][root][INFO] - Training Epoch: 1/2, step 3701/7134 completed (loss: 0.16001670062541962, acc: 0.956204354763031)
[2025-02-13 19:23:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:47][root][INFO] - Training Epoch: 1/2, step 3702/7134 completed (loss: 0.135213240981102, acc: 0.9818181991577148)
[2025-02-13 19:23:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:48][root][INFO] - Training Epoch: 1/2, step 3703/7134 completed (loss: 0.17434769868850708, acc: 0.9488636255264282)
[2025-02-13 19:23:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:48][root][INFO] - Training Epoch: 1/2, step 3704/7134 completed (loss: 0.10532697290182114, acc: 0.9801980257034302)
[2025-02-13 19:23:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:49][root][INFO] - Training Epoch: 1/2, step 3705/7134 completed (loss: 0.29382097721099854, acc: 0.9447852969169617)
[2025-02-13 19:23:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:49][root][INFO] - Training Epoch: 1/2, step 3706/7134 completed (loss: 0.4335033595561981, acc: 0.8910890817642212)
[2025-02-13 19:23:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:49][root][INFO] - Training Epoch: 1/2, step 3707/7134 completed (loss: 0.13152152299880981, acc: 0.9800000190734863)
[2025-02-13 19:23:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:50][root][INFO] - Training Epoch: 1/2, step 3708/7134 completed (loss: 0.34604573249816895, acc: 0.9230769276618958)
[2025-02-13 19:23:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:50][root][INFO] - Training Epoch: 1/2, step 3709/7134 completed (loss: 0.15256132185459137, acc: 0.9583333134651184)
[2025-02-13 19:23:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:51][root][INFO] - Training Epoch: 1/2, step 3710/7134 completed (loss: 0.23544611036777496, acc: 0.9259259104728699)
[2025-02-13 19:23:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:51][root][INFO] - Training Epoch: 1/2, step 3711/7134 completed (loss: 0.235380157828331, acc: 0.9395604133605957)
[2025-02-13 19:23:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:51][root][INFO] - Training Epoch: 1/2, step 3712/7134 completed (loss: 0.348554402589798, acc: 0.91847825050354)
[2025-02-13 19:23:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:52][root][INFO] - Training Epoch: 1/2, step 3713/7134 completed (loss: 0.18377186357975006, acc: 0.9424460530281067)
[2025-02-13 19:23:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:52][root][INFO] - Training Epoch: 1/2, step 3714/7134 completed (loss: 0.24451926350593567, acc: 0.9378238320350647)
[2025-02-13 19:23:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:52][root][INFO] - Training Epoch: 1/2, step 3715/7134 completed (loss: 0.16303257644176483, acc: 0.9680851101875305)
[2025-02-13 19:23:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:53][root][INFO] - Training Epoch: 1/2, step 3716/7134 completed (loss: 0.38477206230163574, acc: 0.9340101480484009)
[2025-02-13 19:23:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:53][root][INFO] - Training Epoch: 1/2, step 3717/7134 completed (loss: 0.22213609516620636, acc: 0.9512194991111755)
[2025-02-13 19:23:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:54][root][INFO] - Training Epoch: 1/2, step 3718/7134 completed (loss: 0.24172662198543549, acc: 0.9635416865348816)
[2025-02-13 19:23:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:54][root][INFO] - Training Epoch: 1/2, step 3719/7134 completed (loss: 0.3390548825263977, acc: 0.9181286692619324)
[2025-02-13 19:23:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:55][root][INFO] - Training Epoch: 1/2, step 3720/7134 completed (loss: 0.11494828760623932, acc: 0.9620253443717957)
[2025-02-13 19:23:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:55][root][INFO] - Training Epoch: 1/2, step 3721/7134 completed (loss: 0.22229786217212677, acc: 0.9421965479850769)
[2025-02-13 19:23:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:55][root][INFO] - Training Epoch: 1/2, step 3722/7134 completed (loss: 0.2134874314069748, acc: 0.9378238320350647)
[2025-02-13 19:23:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:56][root][INFO] - Training Epoch: 1/2, step 3723/7134 completed (loss: 0.2177785038948059, acc: 0.9548386931419373)
[2025-02-13 19:23:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:56][root][INFO] - Training Epoch: 1/2, step 3724/7134 completed (loss: 0.12225719541311264, acc: 0.9759036302566528)
[2025-02-13 19:23:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:56][root][INFO] - Training Epoch: 1/2, step 3725/7134 completed (loss: 0.38700833916664124, acc: 0.9390243887901306)
[2025-02-13 19:23:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:57][root][INFO] - Training Epoch: 1/2, step 3726/7134 completed (loss: 0.1759386658668518, acc: 0.9547738432884216)
[2025-02-13 19:23:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:57][root][INFO] - Training Epoch: 1/2, step 3727/7134 completed (loss: 0.1616794466972351, acc: 0.9595375657081604)
[2025-02-13 19:23:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:58][root][INFO] - Training Epoch: 1/2, step 3728/7134 completed (loss: 0.134517639875412, acc: 0.9556650519371033)
[2025-02-13 19:23:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:58][root][INFO] - Training Epoch: 1/2, step 3729/7134 completed (loss: 0.1715978980064392, acc: 0.9622641801834106)
[2025-02-13 19:23:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:58][root][INFO] - Training Epoch: 1/2, step 3730/7134 completed (loss: 0.2385365068912506, acc: 0.9333333373069763)
[2025-02-13 19:23:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:59][root][INFO] - Training Epoch: 1/2, step 3731/7134 completed (loss: 0.09859409183263779, acc: 0.959770143032074)
[2025-02-13 19:23:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:23:59][root][INFO] - Training Epoch: 1/2, step 3732/7134 completed (loss: 0.12461192160844803, acc: 0.9743589758872986)
[2025-02-13 19:23:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:00][root][INFO] - Training Epoch: 1/2, step 3733/7134 completed (loss: 0.28789475560188293, acc: 0.9378882050514221)
[2025-02-13 19:24:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:00][root][INFO] - Training Epoch: 1/2, step 3734/7134 completed (loss: 0.40359073877334595, acc: 0.9358974099159241)
[2025-02-13 19:24:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:00][root][INFO] - Training Epoch: 1/2, step 3735/7134 completed (loss: 0.25678691267967224, acc: 0.9415584206581116)
[2025-02-13 19:24:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:01][root][INFO] - Training Epoch: 1/2, step 3736/7134 completed (loss: 0.22147901356220245, acc: 0.9338235259056091)
[2025-02-13 19:24:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:01][root][INFO] - Training Epoch: 1/2, step 3737/7134 completed (loss: 0.17079684138298035, acc: 0.9515151381492615)
[2025-02-13 19:24:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:01][root][INFO] - Training Epoch: 1/2, step 3738/7134 completed (loss: 0.10192704945802689, acc: 0.9861111044883728)
[2025-02-13 19:24:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:02][root][INFO] - Training Epoch: 1/2, step 3739/7134 completed (loss: 0.21726445853710175, acc: 0.9468085169792175)
[2025-02-13 19:24:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:02][root][INFO] - Training Epoch: 1/2, step 3740/7134 completed (loss: 0.1742117553949356, acc: 0.9426751732826233)
[2025-02-13 19:24:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:03][root][INFO] - Training Epoch: 1/2, step 3741/7134 completed (loss: 0.21841531991958618, acc: 0.9655172228813171)
[2025-02-13 19:24:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:03][root][INFO] - Training Epoch: 1/2, step 3742/7134 completed (loss: 0.2240712195634842, acc: 0.9591836929321289)
[2025-02-13 19:24:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:03][root][INFO] - Training Epoch: 1/2, step 3743/7134 completed (loss: 0.3471224009990692, acc: 0.9176470637321472)
[2025-02-13 19:24:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:04][root][INFO] - Training Epoch: 1/2, step 3744/7134 completed (loss: 0.2839092016220093, acc: 0.9418604373931885)
[2025-02-13 19:24:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:04][root][INFO] - Training Epoch: 1/2, step 3745/7134 completed (loss: 0.40760573744773865, acc: 0.9333333373069763)
[2025-02-13 19:24:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:05][root][INFO] - Training Epoch: 1/2, step 3746/7134 completed (loss: 0.22700513899326324, acc: 0.9491525292396545)
[2025-02-13 19:24:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:05][root][INFO] - Training Epoch: 1/2, step 3747/7134 completed (loss: 0.1913810670375824, acc: 0.9722222089767456)
[2025-02-13 19:24:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:05][root][INFO] - Training Epoch: 1/2, step 3748/7134 completed (loss: 0.23580001294612885, acc: 0.9399999976158142)
[2025-02-13 19:24:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:06][root][INFO] - Training Epoch: 1/2, step 3749/7134 completed (loss: 0.4171231687068939, acc: 0.898809552192688)
[2025-02-13 19:24:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:06][root][INFO] - Training Epoch: 1/2, step 3750/7134 completed (loss: 0.1083076000213623, acc: 0.9693251252174377)
[2025-02-13 19:24:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:06][root][INFO] - Training Epoch: 1/2, step 3751/7134 completed (loss: 0.27761563658714294, acc: 0.9337349534034729)
[2025-02-13 19:24:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:07][root][INFO] - Training Epoch: 1/2, step 3752/7134 completed (loss: 0.28249165415763855, acc: 0.9371069073677063)
[2025-02-13 19:24:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:07][root][INFO] - Training Epoch: 1/2, step 3753/7134 completed (loss: 0.40865135192871094, acc: 0.9018405079841614)
[2025-02-13 19:24:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:08][root][INFO] - Training Epoch: 1/2, step 3754/7134 completed (loss: 0.10315766930580139, acc: 0.9677419066429138)
[2025-02-13 19:24:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:08][root][INFO] - Training Epoch: 1/2, step 3755/7134 completed (loss: 0.25635141134262085, acc: 0.9173553586006165)
[2025-02-13 19:24:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:08][root][INFO] - Training Epoch: 1/2, step 3756/7134 completed (loss: 0.2899028956890106, acc: 0.9190751314163208)
[2025-02-13 19:24:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:09][root][INFO] - Training Epoch: 1/2, step 3757/7134 completed (loss: 0.3817969560623169, acc: 0.9285714030265808)
[2025-02-13 19:24:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:09][root][INFO] - Training Epoch: 1/2, step 3758/7134 completed (loss: 0.2104189693927765, acc: 0.9448819160461426)
[2025-02-13 19:24:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:09][root][INFO] - Training Epoch: 1/2, step 3759/7134 completed (loss: 0.26234591007232666, acc: 0.9160839319229126)
[2025-02-13 19:24:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:10][root][INFO] - Training Epoch: 1/2, step 3760/7134 completed (loss: 0.31817716360092163, acc: 0.9146341681480408)
[2025-02-13 19:24:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:10][root][INFO] - Training Epoch: 1/2, step 3761/7134 completed (loss: 0.1326856166124344, acc: 0.9772727489471436)
[2025-02-13 19:24:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:11][root][INFO] - Training Epoch: 1/2, step 3762/7134 completed (loss: 0.1914581060409546, acc: 0.9407894611358643)
[2025-02-13 19:24:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:11][root][INFO] - Training Epoch: 1/2, step 3763/7134 completed (loss: 0.13929204642772675, acc: 0.9642857313156128)
[2025-02-13 19:24:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:11][root][INFO] - Training Epoch: 1/2, step 3764/7134 completed (loss: 0.3559722900390625, acc: 0.9281437397003174)
[2025-02-13 19:24:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:12][root][INFO] - Training Epoch: 1/2, step 3765/7134 completed (loss: 0.17500054836273193, acc: 0.9715909361839294)
[2025-02-13 19:24:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:12][root][INFO] - Training Epoch: 1/2, step 3766/7134 completed (loss: 0.20295263826847076, acc: 0.9463087320327759)
[2025-02-13 19:24:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:13][root][INFO] - Training Epoch: 1/2, step 3767/7134 completed (loss: 0.17973601818084717, acc: 0.9441340565681458)
[2025-02-13 19:24:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:13][root][INFO] - Training Epoch: 1/2, step 3768/7134 completed (loss: 0.22677375376224518, acc: 0.9519230723381042)
[2025-02-13 19:24:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:13][root][INFO] - Training Epoch: 1/2, step 3769/7134 completed (loss: 0.288163423538208, acc: 0.929411768913269)
[2025-02-13 19:24:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:14][root][INFO] - Training Epoch: 1/2, step 3770/7134 completed (loss: 0.16648606956005096, acc: 0.9597315192222595)
[2025-02-13 19:24:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:14][root][INFO] - Training Epoch: 1/2, step 3771/7134 completed (loss: 0.10285995155572891, acc: 0.9868420958518982)
[2025-02-13 19:24:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:15][root][INFO] - Training Epoch: 1/2, step 3772/7134 completed (loss: 0.39193153381347656, acc: 0.9142857193946838)
[2025-02-13 19:24:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:15][root][INFO] - Training Epoch: 1/2, step 3773/7134 completed (loss: 0.19099082052707672, acc: 0.9468085169792175)
[2025-02-13 19:24:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:15][root][INFO] - Training Epoch: 1/2, step 3774/7134 completed (loss: 0.28548464179039, acc: 0.9395604133605957)
[2025-02-13 19:24:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:16][root][INFO] - Training Epoch: 1/2, step 3775/7134 completed (loss: 0.27635684609413147, acc: 0.9254658222198486)
[2025-02-13 19:24:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:16][root][INFO] - Training Epoch: 1/2, step 3776/7134 completed (loss: 0.3056429624557495, acc: 0.9102563858032227)
[2025-02-13 19:24:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:16][root][INFO] - Training Epoch: 1/2, step 3777/7134 completed (loss: 0.1396215409040451, acc: 0.9588235020637512)
[2025-02-13 19:24:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:17][root][INFO] - Training Epoch: 1/2, step 3778/7134 completed (loss: 0.2715821862220764, acc: 0.9360465407371521)
[2025-02-13 19:24:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:17][root][INFO] - Training Epoch: 1/2, step 3779/7134 completed (loss: 0.1958087980747223, acc: 0.9437500238418579)
[2025-02-13 19:24:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:18][root][INFO] - Training Epoch: 1/2, step 3780/7134 completed (loss: 0.1473788321018219, acc: 0.9575757384300232)
[2025-02-13 19:24:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:18][root][INFO] - Training Epoch: 1/2, step 3781/7134 completed (loss: 0.2944445312023163, acc: 0.9316770434379578)
[2025-02-13 19:24:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:18][root][INFO] - Training Epoch: 1/2, step 3782/7134 completed (loss: 0.1870502531528473, acc: 0.9308176040649414)
[2025-02-13 19:24:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:19][root][INFO] - Training Epoch: 1/2, step 3783/7134 completed (loss: 0.24955934286117554, acc: 0.9467455744743347)
[2025-02-13 19:24:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:19][root][INFO] - Training Epoch: 1/2, step 3784/7134 completed (loss: 0.2659772038459778, acc: 0.9328858852386475)
[2025-02-13 19:24:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:20][root][INFO] - Training Epoch: 1/2, step 3785/7134 completed (loss: 0.17763717472553253, acc: 0.9645389914512634)
[2025-02-13 19:24:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:20][root][INFO] - Training Epoch: 1/2, step 3786/7134 completed (loss: 0.108951136469841, acc: 0.9821428656578064)
[2025-02-13 19:24:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:20][root][INFO] - Training Epoch: 1/2, step 3787/7134 completed (loss: 0.19714175164699554, acc: 0.9384615421295166)
[2025-02-13 19:24:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:21][root][INFO] - Training Epoch: 1/2, step 3788/7134 completed (loss: 0.27047616243362427, acc: 0.9447852969169617)
[2025-02-13 19:24:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:21][root][INFO] - Training Epoch: 1/2, step 3789/7134 completed (loss: 0.3220245838165283, acc: 0.9235293865203857)
[2025-02-13 19:24:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:22][root][INFO] - Training Epoch: 1/2, step 3790/7134 completed (loss: 0.487817645072937, acc: 0.8941176533699036)
[2025-02-13 19:24:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:22][root][INFO] - Training Epoch: 1/2, step 3791/7134 completed (loss: 0.1806982010602951, acc: 0.9421965479850769)
[2025-02-13 19:24:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:22][root][INFO] - Training Epoch: 1/2, step 3792/7134 completed (loss: 0.20252200961112976, acc: 0.9624060392379761)
[2025-02-13 19:24:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:23][root][INFO] - Training Epoch: 1/2, step 3793/7134 completed (loss: 0.3710205554962158, acc: 0.9252873659133911)
[2025-02-13 19:24:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:23][root][INFO] - Training Epoch: 1/2, step 3794/7134 completed (loss: 0.4232015907764435, acc: 0.9111111164093018)
[2025-02-13 19:24:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:24][root][INFO] - Training Epoch: 1/2, step 3795/7134 completed (loss: 0.271967351436615, acc: 0.9222221970558167)
[2025-02-13 19:24:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:24][root][INFO] - Training Epoch: 1/2, step 3796/7134 completed (loss: 0.22774766385555267, acc: 0.9415204524993896)
[2025-02-13 19:24:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:24][root][INFO] - Training Epoch: 1/2, step 3797/7134 completed (loss: 0.1686231642961502, acc: 0.9575757384300232)
[2025-02-13 19:24:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:25][root][INFO] - Training Epoch: 1/2, step 3798/7134 completed (loss: 0.4723062217235565, acc: 0.893081784248352)
[2025-02-13 19:24:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:25][root][INFO] - Training Epoch: 1/2, step 3799/7134 completed (loss: 0.4533890187740326, acc: 0.887005627155304)
[2025-02-13 19:24:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:26][root][INFO] - Training Epoch: 1/2, step 3800/7134 completed (loss: 0.37531593441963196, acc: 0.8758620619773865)
[2025-02-13 19:24:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:26][root][INFO] - Training Epoch: 1/2, step 3801/7134 completed (loss: 0.2280341535806656, acc: 0.9337016344070435)
[2025-02-13 19:24:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:26][root][INFO] - Training Epoch: 1/2, step 3802/7134 completed (loss: 0.2552729845046997, acc: 0.9270833134651184)
[2025-02-13 19:24:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:27][root][INFO] - Training Epoch: 1/2, step 3803/7134 completed (loss: 0.2549939751625061, acc: 0.9363057613372803)
[2025-02-13 19:24:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:27][root][INFO] - Training Epoch: 1/2, step 3804/7134 completed (loss: 0.3516911268234253, acc: 0.8943662047386169)
[2025-02-13 19:24:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:28][root][INFO] - Training Epoch: 1/2, step 3805/7134 completed (loss: 0.280813068151474, acc: 0.9257143139839172)
[2025-02-13 19:24:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:28][root][INFO] - Training Epoch: 1/2, step 3806/7134 completed (loss: 0.25372767448425293, acc: 0.9444444179534912)
[2025-02-13 19:24:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:28][root][INFO] - Training Epoch: 1/2, step 3807/7134 completed (loss: 0.10578428208827972, acc: 0.9794520735740662)
[2025-02-13 19:24:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:29][root][INFO] - Training Epoch: 1/2, step 3808/7134 completed (loss: 0.2806417942047119, acc: 0.929347813129425)
[2025-02-13 19:24:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:29][root][INFO] - Training Epoch: 1/2, step 3809/7134 completed (loss: 0.24169644713401794, acc: 0.9593023061752319)
[2025-02-13 19:24:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:29][root][INFO] - Training Epoch: 1/2, step 3810/7134 completed (loss: 0.2718946635723114, acc: 0.9415584206581116)
[2025-02-13 19:24:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:30][root][INFO] - Training Epoch: 1/2, step 3811/7134 completed (loss: 0.39360901713371277, acc: 0.916167676448822)
[2025-02-13 19:24:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:30][root][INFO] - Training Epoch: 1/2, step 3812/7134 completed (loss: 0.6327592730522156, acc: 0.8775510191917419)
[2025-02-13 19:24:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:31][root][INFO] - Training Epoch: 1/2, step 3813/7134 completed (loss: 0.5367523431777954, acc: 0.8918918967247009)
[2025-02-13 19:24:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:31][root][INFO] - Training Epoch: 1/2, step 3814/7134 completed (loss: 0.14676949381828308, acc: 0.9657142758369446)
[2025-02-13 19:24:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:31][root][INFO] - Training Epoch: 1/2, step 3815/7134 completed (loss: 0.2396387755870819, acc: 0.9407894611358643)
[2025-02-13 19:24:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:32][root][INFO] - Training Epoch: 1/2, step 3816/7134 completed (loss: 0.20009003579616547, acc: 0.9552238583564758)
[2025-02-13 19:24:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:32][root][INFO] - Training Epoch: 1/2, step 3817/7134 completed (loss: 0.3313179910182953, acc: 0.9438202381134033)
[2025-02-13 19:24:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:33][root][INFO] - Training Epoch: 1/2, step 3818/7134 completed (loss: 0.39016395807266235, acc: 0.9035087823867798)
[2025-02-13 19:24:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:33][root][INFO] - Training Epoch: 1/2, step 3819/7134 completed (loss: 0.3925097584724426, acc: 0.9285714030265808)
[2025-02-13 19:24:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:33][root][INFO] - Training Epoch: 1/2, step 3820/7134 completed (loss: 0.32065677642822266, acc: 0.9074074029922485)
[2025-02-13 19:24:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:34][root][INFO] - Training Epoch: 1/2, step 3821/7134 completed (loss: 0.22117607295513153, acc: 0.9520547986030579)
[2025-02-13 19:24:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:34][root][INFO] - Training Epoch: 1/2, step 3822/7134 completed (loss: 0.221415713429451, acc: 0.9433962106704712)
[2025-02-13 19:24:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:35][root][INFO] - Training Epoch: 1/2, step 3823/7134 completed (loss: 0.09206610172986984, acc: 0.9932885766029358)
[2025-02-13 19:24:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:35][root][INFO] - Training Epoch: 1/2, step 3824/7134 completed (loss: 0.2761133015155792, acc: 0.9453125)
[2025-02-13 19:24:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:35][root][INFO] - Training Epoch: 1/2, step 3825/7134 completed (loss: 0.23012734949588776, acc: 0.9351851940155029)
[2025-02-13 19:24:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:36][root][INFO] - Training Epoch: 1/2, step 3826/7134 completed (loss: 0.19166292250156403, acc: 0.9577465057373047)
[2025-02-13 19:24:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:36][root][INFO] - Training Epoch: 1/2, step 3827/7134 completed (loss: 0.12078636139631271, acc: 0.9745222926139832)
[2025-02-13 19:24:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:36][root][INFO] - Training Epoch: 1/2, step 3828/7134 completed (loss: 0.18354985117912292, acc: 0.9375)
[2025-02-13 19:24:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:37][root][INFO] - Training Epoch: 1/2, step 3829/7134 completed (loss: 0.2669447064399719, acc: 0.921875)
[2025-02-13 19:24:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:37][root][INFO] - Training Epoch: 1/2, step 3830/7134 completed (loss: 0.22444064915180206, acc: 0.9481481313705444)
[2025-02-13 19:24:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:38][root][INFO] - Training Epoch: 1/2, step 3831/7134 completed (loss: 0.26239651441574097, acc: 0.9407894611358643)
[2025-02-13 19:24:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:38][root][INFO] - Training Epoch: 1/2, step 3832/7134 completed (loss: 0.15223746001720428, acc: 0.9732142686843872)
[2025-02-13 19:24:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:38][root][INFO] - Training Epoch: 1/2, step 3833/7134 completed (loss: 0.09958112239837646, acc: 0.9781420826911926)
[2025-02-13 19:24:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:39][root][INFO] - Training Epoch: 1/2, step 3834/7134 completed (loss: 0.019035473465919495, acc: 1.0)
[2025-02-13 19:24:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:39][root][INFO] - Training Epoch: 1/2, step 3835/7134 completed (loss: 0.08399349451065063, acc: 0.984375)
[2025-02-13 19:24:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:40][root][INFO] - Training Epoch: 1/2, step 3836/7134 completed (loss: 0.36371034383773804, acc: 0.9629629850387573)
[2025-02-13 19:24:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:40][root][INFO] - Training Epoch: 1/2, step 3837/7134 completed (loss: 0.20492379367351532, acc: 0.9681528806686401)
[2025-02-13 19:24:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:40][root][INFO] - Training Epoch: 1/2, step 3838/7134 completed (loss: 0.15410688519477844, acc: 0.966292142868042)
[2025-02-13 19:24:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:41][root][INFO] - Training Epoch: 1/2, step 3839/7134 completed (loss: 0.10906361788511276, acc: 0.9689440727233887)
[2025-02-13 19:24:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:41][root][INFO] - Training Epoch: 1/2, step 3840/7134 completed (loss: 0.10928814113140106, acc: 0.9818181991577148)
[2025-02-13 19:24:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:41][root][INFO] - Training Epoch: 1/2, step 3841/7134 completed (loss: 0.19545386731624603, acc: 0.9648241400718689)
[2025-02-13 19:24:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:42][root][INFO] - Training Epoch: 1/2, step 3842/7134 completed (loss: 0.05742081254720688, acc: 0.9896907210350037)
[2025-02-13 19:24:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:42][root][INFO] - Training Epoch: 1/2, step 3843/7134 completed (loss: 0.25143131613731384, acc: 0.9639639854431152)
[2025-02-13 19:24:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:43][root][INFO] - Training Epoch: 1/2, step 3844/7134 completed (loss: 0.31802111864089966, acc: 0.9109947681427002)
[2025-02-13 19:24:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:43][root][INFO] - Training Epoch: 1/2, step 3845/7134 completed (loss: 0.237221360206604, acc: 0.9714285731315613)
[2025-02-13 19:24:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:43][root][INFO] - Training Epoch: 1/2, step 3846/7134 completed (loss: 0.10127653181552887, acc: 0.985401451587677)
[2025-02-13 19:24:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:44][root][INFO] - Training Epoch: 1/2, step 3847/7134 completed (loss: 0.15733753144741058, acc: 0.9716312289237976)
[2025-02-13 19:24:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:44][root][INFO] - Training Epoch: 1/2, step 3848/7134 completed (loss: 0.15423880517482758, acc: 0.9715909361839294)
[2025-02-13 19:24:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:44][root][INFO] - Training Epoch: 1/2, step 3849/7134 completed (loss: 0.2582018971443176, acc: 0.9481481313705444)
[2025-02-13 19:24:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:45][root][INFO] - Training Epoch: 1/2, step 3850/7134 completed (loss: 0.27315810322761536, acc: 0.9398906826972961)
[2025-02-13 19:24:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:45][root][INFO] - Training Epoch: 1/2, step 3851/7134 completed (loss: 0.28180840611457825, acc: 0.9226190447807312)
[2025-02-13 19:24:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:46][root][INFO] - Training Epoch: 1/2, step 3852/7134 completed (loss: 0.1038428395986557, acc: 0.9784172773361206)
[2025-02-13 19:24:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:46][root][INFO] - Training Epoch: 1/2, step 3853/7134 completed (loss: 0.3645537495613098, acc: 0.8938547372817993)
[2025-02-13 19:24:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:46][root][INFO] - Training Epoch: 1/2, step 3854/7134 completed (loss: 0.1740015745162964, acc: 0.9426751732826233)
[2025-02-13 19:24:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:47][root][INFO] - Training Epoch: 1/2, step 3855/7134 completed (loss: 0.22763493657112122, acc: 0.9281437397003174)
[2025-02-13 19:24:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:47][root][INFO] - Training Epoch: 1/2, step 3856/7134 completed (loss: 0.17406733334064484, acc: 0.9426751732826233)
[2025-02-13 19:24:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:47][root][INFO] - Training Epoch: 1/2, step 3857/7134 completed (loss: 0.21960529685020447, acc: 0.9459459185600281)
[2025-02-13 19:24:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:48][root][INFO] - Training Epoch: 1/2, step 3858/7134 completed (loss: 0.4491350054740906, acc: 0.918749988079071)
[2025-02-13 19:24:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:48][root][INFO] - Training Epoch: 1/2, step 3859/7134 completed (loss: 0.26336392760276794, acc: 0.9322034120559692)
[2025-02-13 19:24:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:49][root][INFO] - Training Epoch: 1/2, step 3860/7134 completed (loss: 0.17623764276504517, acc: 0.9528796076774597)
[2025-02-13 19:24:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:49][root][INFO] - Training Epoch: 1/2, step 3861/7134 completed (loss: 0.30618858337402344, acc: 0.9215686321258545)
[2025-02-13 19:24:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:49][root][INFO] - Training Epoch: 1/2, step 3862/7134 completed (loss: 0.1785142719745636, acc: 0.9692307710647583)
[2025-02-13 19:24:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:50][root][INFO] - Training Epoch: 1/2, step 3863/7134 completed (loss: 0.08433995395898819, acc: 0.9709302186965942)
[2025-02-13 19:24:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:50][root][INFO] - Training Epoch: 1/2, step 3864/7134 completed (loss: 0.17893430590629578, acc: 0.9610389471054077)
[2025-02-13 19:24:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:51][root][INFO] - Training Epoch: 1/2, step 3865/7134 completed (loss: 0.3153001666069031, acc: 0.9196428656578064)
[2025-02-13 19:24:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:51][root][INFO] - Training Epoch: 1/2, step 3866/7134 completed (loss: 0.250460147857666, acc: 0.9449999928474426)
[2025-02-13 19:24:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:51][root][INFO] - Training Epoch: 1/2, step 3867/7134 completed (loss: 0.1620778888463974, acc: 0.9727272987365723)
[2025-02-13 19:24:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:52][root][INFO] - Training Epoch: 1/2, step 3868/7134 completed (loss: 0.1610962301492691, acc: 0.9485294222831726)
[2025-02-13 19:24:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:52][root][INFO] - Training Epoch: 1/2, step 3869/7134 completed (loss: 0.26828497648239136, acc: 0.9370629191398621)
[2025-02-13 19:24:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:53][root][INFO] - Training Epoch: 1/2, step 3870/7134 completed (loss: 0.40078553557395935, acc: 0.9215686321258545)
[2025-02-13 19:24:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:53][root][INFO] - Training Epoch: 1/2, step 3871/7134 completed (loss: 0.22856763005256653, acc: 0.9425287246704102)
[2025-02-13 19:24:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:53][root][INFO] - Training Epoch: 1/2, step 3872/7134 completed (loss: 0.2756047546863556, acc: 0.9343434572219849)
[2025-02-13 19:24:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:54][root][INFO] - Training Epoch: 1/2, step 3873/7134 completed (loss: 0.17682160437107086, acc: 0.9575757384300232)
[2025-02-13 19:24:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:54][root][INFO] - Training Epoch: 1/2, step 3874/7134 completed (loss: 0.15823064744472504, acc: 0.9707602262496948)
[2025-02-13 19:24:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:54][root][INFO] - Training Epoch: 1/2, step 3875/7134 completed (loss: 0.2602349519729614, acc: 0.949367105960846)
[2025-02-13 19:24:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:55][root][INFO] - Training Epoch: 1/2, step 3876/7134 completed (loss: 0.1299927979707718, acc: 0.9679999947547913)
[2025-02-13 19:24:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:55][root][INFO] - Training Epoch: 1/2, step 3877/7134 completed (loss: 0.26338574290275574, acc: 0.9360465407371521)
[2025-02-13 19:24:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:56][root][INFO] - Training Epoch: 1/2, step 3878/7134 completed (loss: 0.7802248001098633, acc: 0.8372092843055725)
[2025-02-13 19:24:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:56][root][INFO] - Training Epoch: 1/2, step 3879/7134 completed (loss: 0.24567563831806183, acc: 0.9253731369972229)
[2025-02-13 19:24:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:56][root][INFO] - Training Epoch: 1/2, step 3880/7134 completed (loss: 0.34908148646354675, acc: 0.9358974099159241)
[2025-02-13 19:24:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:57][root][INFO] - Training Epoch: 1/2, step 3881/7134 completed (loss: 0.5041243433952332, acc: 0.893048107624054)
[2025-02-13 19:24:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:57][root][INFO] - Training Epoch: 1/2, step 3882/7134 completed (loss: 0.41044554114341736, acc: 0.9226804375648499)
[2025-02-13 19:24:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:57][root][INFO] - Training Epoch: 1/2, step 3883/7134 completed (loss: 0.2766564190387726, acc: 0.9477124214172363)
[2025-02-13 19:24:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:58][root][INFO] - Training Epoch: 1/2, step 3884/7134 completed (loss: 0.19079452753067017, acc: 0.9599999785423279)
[2025-02-13 19:24:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:58][root][INFO] - Training Epoch: 1/2, step 3885/7134 completed (loss: 0.10542438179254532, acc: 0.9808917045593262)
[2025-02-13 19:24:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:59][root][INFO] - Training Epoch: 1/2, step 3886/7134 completed (loss: 0.271157830953598, acc: 0.9365079402923584)
[2025-02-13 19:24:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:24:59][root][INFO] - Training Epoch: 1/2, step 3887/7134 completed (loss: 0.28314533829689026, acc: 0.9433962106704712)
[2025-02-13 19:24:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:00][root][INFO] - Training Epoch: 1/2, step 3888/7134 completed (loss: 0.2536819279193878, acc: 0.9418604373931885)
[2025-02-13 19:25:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:00][root][INFO] - Training Epoch: 1/2, step 3889/7134 completed (loss: 0.10917281359434128, acc: 0.9830508232116699)
[2025-02-13 19:25:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:00][root][INFO] - Training Epoch: 1/2, step 3890/7134 completed (loss: 0.18390437960624695, acc: 0.9484536051750183)
[2025-02-13 19:25:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:01][root][INFO] - Training Epoch: 1/2, step 3891/7134 completed (loss: 0.13354413211345673, acc: 0.9595375657081604)
[2025-02-13 19:25:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:01][root][INFO] - Training Epoch: 1/2, step 3892/7134 completed (loss: 0.25699084997177124, acc: 0.9466666579246521)
[2025-02-13 19:25:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:01][root][INFO] - Training Epoch: 1/2, step 3893/7134 completed (loss: 0.3279366195201874, acc: 0.9329897165298462)
[2025-02-13 19:25:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:02][root][INFO] - Training Epoch: 1/2, step 3894/7134 completed (loss: 0.14888447523117065, acc: 0.9620253443717957)
[2025-02-13 19:25:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:02][root][INFO] - Training Epoch: 1/2, step 3895/7134 completed (loss: 0.2462766468524933, acc: 0.9281768202781677)
[2025-02-13 19:25:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:03][root][INFO] - Training Epoch: 1/2, step 3896/7134 completed (loss: 0.1882323920726776, acc: 0.9573459625244141)
[2025-02-13 19:25:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:03][root][INFO] - Training Epoch: 1/2, step 3897/7134 completed (loss: 0.4220682680606842, acc: 0.9418604373931885)
[2025-02-13 19:25:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:03][root][INFO] - Training Epoch: 1/2, step 3898/7134 completed (loss: 0.20843854546546936, acc: 0.954023003578186)
[2025-02-13 19:25:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:04][root][INFO] - Training Epoch: 1/2, step 3899/7134 completed (loss: 0.17972023785114288, acc: 0.9537572264671326)
[2025-02-13 19:25:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:04][root][INFO] - Training Epoch: 1/2, step 3900/7134 completed (loss: 0.1826026886701584, acc: 0.9484536051750183)
[2025-02-13 19:25:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:05][root][INFO] - Training Epoch: 1/2, step 3901/7134 completed (loss: 0.5067224502563477, acc: 0.9096385836601257)
[2025-02-13 19:25:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:05][root][INFO] - Training Epoch: 1/2, step 3902/7134 completed (loss: 0.4128837585449219, acc: 0.875)
[2025-02-13 19:25:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:05][root][INFO] - Training Epoch: 1/2, step 3903/7134 completed (loss: 0.6865478754043579, acc: 0.8290155529975891)
[2025-02-13 19:25:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:06][root][INFO] - Training Epoch: 1/2, step 3904/7134 completed (loss: 0.3938139081001282, acc: 0.9005848169326782)
[2025-02-13 19:25:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:06][root][INFO] - Training Epoch: 1/2, step 3905/7134 completed (loss: 0.12339279055595398, acc: 0.9611111283302307)
[2025-02-13 19:25:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:06][root][INFO] - Training Epoch: 1/2, step 3906/7134 completed (loss: 0.1974058449268341, acc: 0.932584285736084)
[2025-02-13 19:25:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:07][root][INFO] - Training Epoch: 1/2, step 3907/7134 completed (loss: 0.29709044098854065, acc: 0.9278350472450256)
[2025-02-13 19:25:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:07][root][INFO] - Training Epoch: 1/2, step 3908/7134 completed (loss: 0.395129531621933, acc: 0.9075144529342651)
[2025-02-13 19:25:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:08][root][INFO] - Training Epoch: 1/2, step 3909/7134 completed (loss: 0.2098199427127838, acc: 0.9371069073677063)
[2025-02-13 19:25:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:08][root][INFO] - Training Epoch: 1/2, step 3910/7134 completed (loss: 0.17344699800014496, acc: 0.9545454382896423)
[2025-02-13 19:25:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:08][root][INFO] - Training Epoch: 1/2, step 3911/7134 completed (loss: 0.15443843603134155, acc: 0.9685534834861755)
[2025-02-13 19:25:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:09][root][INFO] - Training Epoch: 1/2, step 3912/7134 completed (loss: 0.06310240924358368, acc: 0.9823529124259949)
[2025-02-13 19:25:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:09][root][INFO] - Training Epoch: 1/2, step 3913/7134 completed (loss: 0.11257658898830414, acc: 0.9747474789619446)
[2025-02-13 19:25:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:10][root][INFO] - Training Epoch: 1/2, step 3914/7134 completed (loss: 0.1159389540553093, acc: 0.9811320900917053)
[2025-02-13 19:25:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:10][root][INFO] - Training Epoch: 1/2, step 3915/7134 completed (loss: 0.07891697436571121, acc: 0.982758641242981)
[2025-02-13 19:25:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:10][root][INFO] - Training Epoch: 1/2, step 3916/7134 completed (loss: 0.12722954154014587, acc: 0.9693877696990967)
[2025-02-13 19:25:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:11][root][INFO] - Training Epoch: 1/2, step 3917/7134 completed (loss: 0.10953038185834885, acc: 0.9767441749572754)
[2025-02-13 19:25:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:11][root][INFO] - Training Epoch: 1/2, step 3918/7134 completed (loss: 0.13723649084568024, acc: 0.961904764175415)
[2025-02-13 19:25:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:12][root][INFO] - Training Epoch: 1/2, step 3919/7134 completed (loss: 0.38994958996772766, acc: 0.9408283829689026)
[2025-02-13 19:25:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:12][root][INFO] - Training Epoch: 1/2, step 3920/7134 completed (loss: 0.4088214337825775, acc: 0.8982036113739014)
[2025-02-13 19:25:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:12][root][INFO] - Training Epoch: 1/2, step 3921/7134 completed (loss: 0.304463654756546, acc: 0.9154929518699646)
[2025-02-13 19:25:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:13][root][INFO] - Training Epoch: 1/2, step 3922/7134 completed (loss: 0.3809656798839569, acc: 0.9155844449996948)
[2025-02-13 19:25:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:13][root][INFO] - Training Epoch: 1/2, step 3923/7134 completed (loss: 0.5604163408279419, acc: 0.8928571343421936)
[2025-02-13 19:25:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:14][root][INFO] - Training Epoch: 1/2, step 3924/7134 completed (loss: 0.2664780020713806, acc: 0.9484536051750183)
[2025-02-13 19:25:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:14][root][INFO] - Training Epoch: 1/2, step 3925/7134 completed (loss: 0.24900934100151062, acc: 0.9337016344070435)
[2025-02-13 19:25:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:14][root][INFO] - Training Epoch: 1/2, step 3926/7134 completed (loss: 0.23376919329166412, acc: 0.9430379867553711)
[2025-02-13 19:25:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:15][root][INFO] - Training Epoch: 1/2, step 3927/7134 completed (loss: 0.29688259959220886, acc: 0.9515151381492615)
[2025-02-13 19:25:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:15][root][INFO] - Training Epoch: 1/2, step 3928/7134 completed (loss: 0.4376402199268341, acc: 0.910179615020752)
[2025-02-13 19:25:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:15][root][INFO] - Training Epoch: 1/2, step 3929/7134 completed (loss: 0.14172756671905518, acc: 0.9610389471054077)
[2025-02-13 19:25:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:16][root][INFO] - Training Epoch: 1/2, step 3930/7134 completed (loss: 0.4265102744102478, acc: 0.8813559412956238)
[2025-02-13 19:25:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:16][root][INFO] - Training Epoch: 1/2, step 3931/7134 completed (loss: 0.18393857777118683, acc: 0.9683544039726257)
[2025-02-13 19:25:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:17][root][INFO] - Training Epoch: 1/2, step 3932/7134 completed (loss: 0.14753060042858124, acc: 0.9636363387107849)
[2025-02-13 19:25:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:17][root][INFO] - Training Epoch: 1/2, step 3933/7134 completed (loss: 0.05420134589076042, acc: 0.9857142567634583)
[2025-02-13 19:25:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:17][root][INFO] - Training Epoch: 1/2, step 3934/7134 completed (loss: 0.1496421992778778, acc: 0.9537037014961243)
[2025-02-13 19:25:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:18][root][INFO] - Training Epoch: 1/2, step 3935/7134 completed (loss: 0.1052326112985611, acc: 0.9726027250289917)
[2025-02-13 19:25:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:18][root][INFO] - Training Epoch: 1/2, step 3936/7134 completed (loss: 0.23515896499156952, acc: 0.9435483813285828)
[2025-02-13 19:25:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:19][root][INFO] - Training Epoch: 1/2, step 3937/7134 completed (loss: 0.37965306639671326, acc: 0.9322034120559692)
[2025-02-13 19:25:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:19][root][INFO] - Training Epoch: 1/2, step 3938/7134 completed (loss: 0.29950380325317383, acc: 0.9459459185600281)
[2025-02-13 19:25:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:19][root][INFO] - Training Epoch: 1/2, step 3939/7134 completed (loss: 0.3231481909751892, acc: 0.9185185432434082)
[2025-02-13 19:25:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:20][root][INFO] - Training Epoch: 1/2, step 3940/7134 completed (loss: 0.20779068768024445, acc: 0.95652174949646)
[2025-02-13 19:25:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:20][root][INFO] - Training Epoch: 1/2, step 3941/7134 completed (loss: 0.25170591473579407, acc: 0.9390243887901306)
[2025-02-13 19:25:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:20][root][INFO] - Training Epoch: 1/2, step 3942/7134 completed (loss: 0.16998988389968872, acc: 0.9440000057220459)
[2025-02-13 19:25:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:21][root][INFO] - Training Epoch: 1/2, step 3943/7134 completed (loss: 0.2602730691432953, acc: 0.9577465057373047)
[2025-02-13 19:25:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:21][root][INFO] - Training Epoch: 1/2, step 3944/7134 completed (loss: 0.2893469035625458, acc: 0.9552238583564758)
[2025-02-13 19:25:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:22][root][INFO] - Training Epoch: 1/2, step 3945/7134 completed (loss: 0.11088044941425323, acc: 0.9802631735801697)
[2025-02-13 19:25:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:22][root][INFO] - Training Epoch: 1/2, step 3946/7134 completed (loss: 0.1364438533782959, acc: 0.9464285969734192)
[2025-02-13 19:25:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:22][root][INFO] - Training Epoch: 1/2, step 3947/7134 completed (loss: 0.0664696991443634, acc: 0.9780219793319702)
[2025-02-13 19:25:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:23][root][INFO] - Training Epoch: 1/2, step 3948/7134 completed (loss: 0.03342847898602486, acc: 1.0)
[2025-02-13 19:25:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:23][root][INFO] - Training Epoch: 1/2, step 3949/7134 completed (loss: 0.16590645909309387, acc: 0.9609375)
[2025-02-13 19:25:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:24][root][INFO] - Training Epoch: 1/2, step 3950/7134 completed (loss: 0.23170189559459686, acc: 0.9736841917037964)
[2025-02-13 19:25:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:24][root][INFO] - Training Epoch: 1/2, step 3951/7134 completed (loss: 0.08288004994392395, acc: 0.969072163105011)
[2025-02-13 19:25:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:24][root][INFO] - Training Epoch: 1/2, step 3952/7134 completed (loss: 0.17338956892490387, acc: 0.9591836929321289)
[2025-02-13 19:25:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:25][root][INFO] - Training Epoch: 1/2, step 3953/7134 completed (loss: 0.08400893211364746, acc: 0.97826087474823)
[2025-02-13 19:25:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:25][root][INFO] - Training Epoch: 1/2, step 3954/7134 completed (loss: 0.11789857596158981, acc: 0.9587628841400146)
[2025-02-13 19:25:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:25][root][INFO] - Training Epoch: 1/2, step 3955/7134 completed (loss: 0.11338072270154953, acc: 0.9842519760131836)
[2025-02-13 19:25:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:26][root][INFO] - Training Epoch: 1/2, step 3956/7134 completed (loss: 0.257440984249115, acc: 0.9411764740943909)
[2025-02-13 19:25:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:26][root][INFO] - Training Epoch: 1/2, step 3957/7134 completed (loss: 0.13484355807304382, acc: 0.9829059839248657)
[2025-02-13 19:25:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:27][root][INFO] - Training Epoch: 1/2, step 3958/7134 completed (loss: 0.49517813324928284, acc: 0.9047619104385376)
[2025-02-13 19:25:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:27][root][INFO] - Training Epoch: 1/2, step 3959/7134 completed (loss: 0.2462051659822464, acc: 0.931034505367279)
[2025-02-13 19:25:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:27][root][INFO] - Training Epoch: 1/2, step 3960/7134 completed (loss: 0.26187655329704285, acc: 0.9497206807136536)
[2025-02-13 19:25:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:28][root][INFO] - Training Epoch: 1/2, step 3961/7134 completed (loss: 0.11022008210420609, acc: 0.9691358208656311)
[2025-02-13 19:25:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:28][root][INFO] - Training Epoch: 1/2, step 3962/7134 completed (loss: 0.14598946273326874, acc: 0.9597315192222595)
[2025-02-13 19:25:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:29][root][INFO] - Training Epoch: 1/2, step 3963/7134 completed (loss: 0.3746488690376282, acc: 0.91847825050354)
[2025-02-13 19:25:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:29][root][INFO] - Training Epoch: 1/2, step 3964/7134 completed (loss: 0.3468836843967438, acc: 0.9307692050933838)
[2025-02-13 19:25:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:29][root][INFO] - Training Epoch: 1/2, step 3965/7134 completed (loss: 0.18860729038715363, acc: 0.9575757384300232)
[2025-02-13 19:25:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:30][root][INFO] - Training Epoch: 1/2, step 3966/7134 completed (loss: 0.20791099965572357, acc: 0.9453125)
[2025-02-13 19:25:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:30][root][INFO] - Training Epoch: 1/2, step 3967/7134 completed (loss: 0.4126659035682678, acc: 0.9057971239089966)
[2025-02-13 19:25:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:30][root][INFO] - Training Epoch: 1/2, step 3968/7134 completed (loss: 0.07867449522018433, acc: 0.9832402467727661)
[2025-02-13 19:25:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:31][root][INFO] - Training Epoch: 1/2, step 3969/7134 completed (loss: 0.39328381419181824, acc: 0.8867924809455872)
[2025-02-13 19:25:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:31][root][INFO] - Training Epoch: 1/2, step 3970/7134 completed (loss: 0.2958543002605438, acc: 0.9479768872261047)
[2025-02-13 19:25:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:32][root][INFO] - Training Epoch: 1/2, step 3971/7134 completed (loss: 0.6743161678314209, acc: 0.8820512890815735)
[2025-02-13 19:25:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:32][root][INFO] - Training Epoch: 1/2, step 3972/7134 completed (loss: 0.8995411992073059, acc: 0.8557692170143127)
[2025-02-13 19:25:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:32][root][INFO] - Training Epoch: 1/2, step 3973/7134 completed (loss: 0.37996360659599304, acc: 0.908450722694397)
[2025-02-13 19:25:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:33][root][INFO] - Training Epoch: 1/2, step 3974/7134 completed (loss: 0.4775030016899109, acc: 0.8769230842590332)
[2025-02-13 19:25:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:33][root][INFO] - Training Epoch: 1/2, step 3975/7134 completed (loss: 0.34939563274383545, acc: 0.9085714221000671)
[2025-02-13 19:25:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:33][root][INFO] - Training Epoch: 1/2, step 3976/7134 completed (loss: 0.6362678408622742, acc: 0.891566276550293)
[2025-02-13 19:25:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:34][root][INFO] - Training Epoch: 1/2, step 3977/7134 completed (loss: 0.19561710953712463, acc: 0.9347826242446899)
[2025-02-13 19:25:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:34][root][INFO] - Training Epoch: 1/2, step 3978/7134 completed (loss: 0.18514646589756012, acc: 0.9502487778663635)
[2025-02-13 19:25:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:35][root][INFO] - Training Epoch: 1/2, step 3979/7134 completed (loss: 0.2030038833618164, acc: 0.9416058659553528)
[2025-02-13 19:25:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:35][root][INFO] - Training Epoch: 1/2, step 3980/7134 completed (loss: 0.12849153578281403, acc: 0.9677419066429138)
[2025-02-13 19:25:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:35][root][INFO] - Training Epoch: 1/2, step 3981/7134 completed (loss: 0.274535596370697, acc: 0.9337349534034729)
[2025-02-13 19:25:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:36][root][INFO] - Training Epoch: 1/2, step 3982/7134 completed (loss: 0.4791802763938904, acc: 0.9011628031730652)
[2025-02-13 19:25:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:36][root][INFO] - Training Epoch: 1/2, step 3983/7134 completed (loss: 1.3605973720550537, acc: 0.7674418687820435)
[2025-02-13 19:25:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:37][root][INFO] - Training Epoch: 1/2, step 3984/7134 completed (loss: 0.5701797604560852, acc: 0.8829787373542786)
[2025-02-13 19:25:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:37][root][INFO] - Training Epoch: 1/2, step 3985/7134 completed (loss: 0.18520064651966095, acc: 0.9538461565971375)
[2025-02-13 19:25:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:37][root][INFO] - Training Epoch: 1/2, step 3986/7134 completed (loss: 0.7550525069236755, acc: 0.8409090638160706)
[2025-02-13 19:25:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:38][root][INFO] - Training Epoch: 1/2, step 3987/7134 completed (loss: 0.31097981333732605, acc: 0.8888888955116272)
[2025-02-13 19:25:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:38][root][INFO] - Training Epoch: 1/2, step 3988/7134 completed (loss: 0.45201465487480164, acc: 0.9043062329292297)
[2025-02-13 19:25:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:39][root][INFO] - Training Epoch: 1/2, step 3989/7134 completed (loss: 0.599020779132843, acc: 0.8760330677032471)
[2025-02-13 19:25:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:39][root][INFO] - Training Epoch: 1/2, step 3990/7134 completed (loss: 0.36873993277549744, acc: 0.9139072895050049)
[2025-02-13 19:25:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:39][root][INFO] - Training Epoch: 1/2, step 3991/7134 completed (loss: 0.2673282027244568, acc: 0.9466666579246521)
[2025-02-13 19:25:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:40][root][INFO] - Training Epoch: 1/2, step 3992/7134 completed (loss: 0.3389061391353607, acc: 0.9120879173278809)
[2025-02-13 19:25:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:40][root][INFO] - Training Epoch: 1/2, step 3993/7134 completed (loss: 0.39201679825782776, acc: 0.9166666865348816)
[2025-02-13 19:25:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:40][root][INFO] - Training Epoch: 1/2, step 3994/7134 completed (loss: 0.2931268513202667, acc: 0.9313725233078003)
[2025-02-13 19:25:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:41][root][INFO] - Training Epoch: 1/2, step 3995/7134 completed (loss: 0.6867056488990784, acc: 0.8449612259864807)
[2025-02-13 19:25:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:41][root][INFO] - Training Epoch: 1/2, step 3996/7134 completed (loss: 0.3004346191883087, acc: 0.93388432264328)
[2025-02-13 19:25:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:42][root][INFO] - Training Epoch: 1/2, step 3997/7134 completed (loss: 0.4741077423095703, acc: 0.8965517282485962)
[2025-02-13 19:25:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:42][root][INFO] - Training Epoch: 1/2, step 3998/7134 completed (loss: 0.3422148823738098, acc: 0.8846153616905212)
[2025-02-13 19:25:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:42][root][INFO] - Training Epoch: 1/2, step 3999/7134 completed (loss: 0.26019540429115295, acc: 0.9435483813285828)
[2025-02-13 19:25:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:43][root][INFO] - Training Epoch: 1/2, step 4000/7134 completed (loss: 0.23869238793849945, acc: 0.9430894255638123)
[2025-02-13 19:25:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:43][root][INFO] - Training Epoch: 1/2, step 4001/7134 completed (loss: 0.5986670255661011, acc: 0.8467742204666138)
[2025-02-13 19:25:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:44][root][INFO] - Training Epoch: 1/2, step 4002/7134 completed (loss: 0.5801234245300293, acc: 0.8691588640213013)
[2025-02-13 19:25:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:44][root][INFO] - Training Epoch: 1/2, step 4003/7134 completed (loss: 0.474659264087677, acc: 0.8947368264198303)
[2025-02-13 19:25:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:44][root][INFO] - Training Epoch: 1/2, step 4004/7134 completed (loss: 0.32269200682640076, acc: 0.9100000262260437)
[2025-02-13 19:25:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:45][root][INFO] - Training Epoch: 1/2, step 4005/7134 completed (loss: 0.11595981568098068, acc: 0.9738562107086182)
[2025-02-13 19:25:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:45][root][INFO] - Training Epoch: 1/2, step 4006/7134 completed (loss: 0.1510043889284134, acc: 0.9594594836235046)
[2025-02-13 19:25:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:45][root][INFO] - Training Epoch: 1/2, step 4007/7134 completed (loss: 0.14591874182224274, acc: 0.9642857313156128)
[2025-02-13 19:25:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:46][root][INFO] - Training Epoch: 1/2, step 4008/7134 completed (loss: 0.1337987184524536, acc: 0.9686098694801331)
[2025-02-13 19:25:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:46][root][INFO] - Training Epoch: 1/2, step 4009/7134 completed (loss: 0.08283039927482605, acc: 0.9832402467727661)
[2025-02-13 19:25:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:46][root][INFO] - Training Epoch: 1/2, step 4010/7134 completed (loss: 0.12037558108568192, acc: 0.9760765433311462)
[2025-02-13 19:25:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:47][root][INFO] - Training Epoch: 1/2, step 4011/7134 completed (loss: 0.1917935460805893, acc: 0.9485714435577393)
[2025-02-13 19:25:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:47][root][INFO] - Training Epoch: 1/2, step 4012/7134 completed (loss: 0.12216760963201523, acc: 0.9578313231468201)
[2025-02-13 19:25:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:48][root][INFO] - Training Epoch: 1/2, step 4013/7134 completed (loss: 0.05519125610589981, acc: 0.9860140085220337)
[2025-02-13 19:25:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:48][root][INFO] - Training Epoch: 1/2, step 4014/7134 completed (loss: 0.1695992797613144, acc: 0.9599999785423279)
[2025-02-13 19:25:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:48][root][INFO] - Training Epoch: 1/2, step 4015/7134 completed (loss: 0.07447342574596405, acc: 0.9862068891525269)
[2025-02-13 19:25:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:49][root][INFO] - Training Epoch: 1/2, step 4016/7134 completed (loss: 0.3454643189907074, acc: 0.9430052042007446)
[2025-02-13 19:25:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:49][root][INFO] - Training Epoch: 1/2, step 4017/7134 completed (loss: 0.19507618248462677, acc: 0.9587628841400146)
[2025-02-13 19:25:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:50][root][INFO] - Training Epoch: 1/2, step 4018/7134 completed (loss: 0.28284752368927, acc: 0.9227052927017212)
[2025-02-13 19:25:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:50][root][INFO] - Training Epoch: 1/2, step 4019/7134 completed (loss: 0.17962908744812012, acc: 0.949999988079071)
[2025-02-13 19:25:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:50][root][INFO] - Training Epoch: 1/2, step 4020/7134 completed (loss: 0.2619675397872925, acc: 0.9204545617103577)
[2025-02-13 19:25:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:51][root][INFO] - Training Epoch: 1/2, step 4021/7134 completed (loss: 0.16288506984710693, acc: 0.9411764740943909)
[2025-02-13 19:25:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:51][root][INFO] - Training Epoch: 1/2, step 4022/7134 completed (loss: 0.19491346180438995, acc: 0.942307710647583)
[2025-02-13 19:25:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:52][root][INFO] - Training Epoch: 1/2, step 4023/7134 completed (loss: 0.2504326105117798, acc: 0.9428571462631226)
[2025-02-13 19:25:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:52][root][INFO] - Training Epoch: 1/2, step 4024/7134 completed (loss: 0.3020636737346649, acc: 0.9022988677024841)
[2025-02-13 19:25:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:52][root][INFO] - Training Epoch: 1/2, step 4025/7134 completed (loss: 0.22703927755355835, acc: 0.9371727705001831)
[2025-02-13 19:25:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:53][root][INFO] - Training Epoch: 1/2, step 4026/7134 completed (loss: 0.39529216289520264, acc: 0.9192546606063843)
[2025-02-13 19:25:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:53][root][INFO] - Training Epoch: 1/2, step 4027/7134 completed (loss: 0.24174636602401733, acc: 0.9438202381134033)
[2025-02-13 19:25:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:53][root][INFO] - Training Epoch: 1/2, step 4028/7134 completed (loss: 0.3484988808631897, acc: 0.9055117964744568)
[2025-02-13 19:25:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:54][root][INFO] - Training Epoch: 1/2, step 4029/7134 completed (loss: 0.14230549335479736, acc: 0.9783783555030823)
[2025-02-13 19:25:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:54][root][INFO] - Training Epoch: 1/2, step 4030/7134 completed (loss: 0.12604127824306488, acc: 0.9615384340286255)
[2025-02-13 19:25:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:54][root][INFO] - Training Epoch: 1/2, step 4031/7134 completed (loss: 0.050164494663476944, acc: 0.9937888383865356)
[2025-02-13 19:25:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:55][root][INFO] - Training Epoch: 1/2, step 4032/7134 completed (loss: 0.27212005853652954, acc: 0.9534883499145508)
[2025-02-13 19:25:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:55][root][INFO] - Training Epoch: 1/2, step 4033/7134 completed (loss: 0.11428346484899521, acc: 0.9786096215248108)
[2025-02-13 19:25:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:56][root][INFO] - Training Epoch: 1/2, step 4034/7134 completed (loss: 0.08668208867311478, acc: 0.9846938848495483)
[2025-02-13 19:25:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:56][root][INFO] - Training Epoch: 1/2, step 4035/7134 completed (loss: 0.3818593919277191, acc: 0.9219512343406677)
[2025-02-13 19:25:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:56][root][INFO] - Training Epoch: 1/2, step 4036/7134 completed (loss: 0.2401326298713684, acc: 0.9307692050933838)
[2025-02-13 19:25:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:57][root][INFO] - Training Epoch: 1/2, step 4037/7134 completed (loss: 0.1872899979352951, acc: 0.9635416865348816)
[2025-02-13 19:25:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:57][root][INFO] - Training Epoch: 1/2, step 4038/7134 completed (loss: 0.25969111919403076, acc: 0.9431818127632141)
[2025-02-13 19:25:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:58][root][INFO] - Training Epoch: 1/2, step 4039/7134 completed (loss: 0.3201926648616791, acc: 0.9230769276618958)
[2025-02-13 19:25:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:58][root][INFO] - Training Epoch: 1/2, step 4040/7134 completed (loss: 0.1308417171239853, acc: 0.9562841653823853)
[2025-02-13 19:25:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:58][root][INFO] - Training Epoch: 1/2, step 4041/7134 completed (loss: 0.1552535593509674, acc: 0.9595375657081604)
[2025-02-13 19:25:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:59][root][INFO] - Training Epoch: 1/2, step 4042/7134 completed (loss: 0.1860511153936386, acc: 0.9797297120094299)
[2025-02-13 19:25:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:59][root][INFO] - Training Epoch: 1/2, step 4043/7134 completed (loss: 0.2680135667324066, acc: 0.9367088675498962)
[2025-02-13 19:25:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:25:59][root][INFO] - Training Epoch: 1/2, step 4044/7134 completed (loss: 0.1777278631925583, acc: 0.9670329689979553)
[2025-02-13 19:26:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:00][root][INFO] - Training Epoch: 1/2, step 4045/7134 completed (loss: 0.20008961856365204, acc: 0.9371428489685059)
[2025-02-13 19:26:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:00][root][INFO] - Training Epoch: 1/2, step 4046/7134 completed (loss: 0.24079065024852753, acc: 0.9298245906829834)
[2025-02-13 19:26:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:01][root][INFO] - Training Epoch: 1/2, step 4047/7134 completed (loss: 0.21982476115226746, acc: 0.9672130942344666)
[2025-02-13 19:26:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:01][root][INFO] - Training Epoch: 1/2, step 4048/7134 completed (loss: 0.09915539622306824, acc: 0.9644669890403748)
[2025-02-13 19:26:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:01][root][INFO] - Training Epoch: 1/2, step 4049/7134 completed (loss: 0.2592954635620117, acc: 0.9502487778663635)
[2025-02-13 19:26:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:02][root][INFO] - Training Epoch: 1/2, step 4050/7134 completed (loss: 0.242082878947258, acc: 0.9312499761581421)
[2025-02-13 19:26:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:02][root][INFO] - Training Epoch: 1/2, step 4051/7134 completed (loss: 0.27514225244522095, acc: 0.9329608678817749)
[2025-02-13 19:26:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:03][root][INFO] - Training Epoch: 1/2, step 4052/7134 completed (loss: 0.2530156672000885, acc: 0.9322034120559692)
[2025-02-13 19:26:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:03][root][INFO] - Training Epoch: 1/2, step 4053/7134 completed (loss: 0.25949549674987793, acc: 0.9226190447807312)
[2025-02-13 19:26:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:03][root][INFO] - Training Epoch: 1/2, step 4054/7134 completed (loss: 0.4182790219783783, acc: 0.895652174949646)
[2025-02-13 19:26:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:04][root][INFO] - Training Epoch: 1/2, step 4055/7134 completed (loss: 0.20624154806137085, acc: 0.9468085169792175)
[2025-02-13 19:26:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:04][root][INFO] - Training Epoch: 1/2, step 4056/7134 completed (loss: 0.17337150871753693, acc: 0.9526315927505493)
[2025-02-13 19:26:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:05][root][INFO] - Training Epoch: 1/2, step 4057/7134 completed (loss: 0.198772132396698, acc: 0.9411764740943909)
[2025-02-13 19:26:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:05][root][INFO] - Training Epoch: 1/2, step 4058/7134 completed (loss: 0.35440781712532043, acc: 0.9015151262283325)
[2025-02-13 19:26:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:05][root][INFO] - Training Epoch: 1/2, step 4059/7134 completed (loss: 0.05848650261759758, acc: 1.0)
[2025-02-13 19:26:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:06][root][INFO] - Training Epoch: 1/2, step 4060/7134 completed (loss: 0.1618005335330963, acc: 0.9662162065505981)
[2025-02-13 19:26:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:06][root][INFO] - Training Epoch: 1/2, step 4061/7134 completed (loss: 0.03930845111608505, acc: 1.0)
[2025-02-13 19:26:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:07][root][INFO] - Training Epoch: 1/2, step 4062/7134 completed (loss: 0.06279820203781128, acc: 0.9934210777282715)
[2025-02-13 19:26:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:07][root][INFO] - Training Epoch: 1/2, step 4063/7134 completed (loss: 0.07430512458086014, acc: 0.9849624037742615)
[2025-02-13 19:26:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:07][root][INFO] - Training Epoch: 1/2, step 4064/7134 completed (loss: 0.06186065077781677, acc: 0.9934640526771545)
[2025-02-13 19:26:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:08][root][INFO] - Training Epoch: 1/2, step 4065/7134 completed (loss: 0.06303048878908157, acc: 0.9870967864990234)
[2025-02-13 19:26:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:08][root][INFO] - Training Epoch: 1/2, step 4066/7134 completed (loss: 0.07058203965425491, acc: 0.9934640526771545)
[2025-02-13 19:26:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:09][root][INFO] - Training Epoch: 1/2, step 4067/7134 completed (loss: 0.18295811116695404, acc: 0.9383561611175537)
[2025-02-13 19:26:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:09][root][INFO] - Training Epoch: 1/2, step 4068/7134 completed (loss: 0.37242016196250916, acc: 0.8760330677032471)
[2025-02-13 19:26:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:09][root][INFO] - Training Epoch: 1/2, step 4069/7134 completed (loss: 0.14017078280448914, acc: 0.9647887349128723)
[2025-02-13 19:26:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:10][root][INFO] - Training Epoch: 1/2, step 4070/7134 completed (loss: 0.16623272001743317, acc: 0.9470198750495911)
[2025-02-13 19:26:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:10][root][INFO] - Training Epoch: 1/2, step 4071/7134 completed (loss: 0.09324344247579575, acc: 0.9794520735740662)
[2025-02-13 19:26:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:11][root][INFO] - Training Epoch: 1/2, step 4072/7134 completed (loss: 0.13371004164218903, acc: 0.9695122241973877)
[2025-02-13 19:26:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:11][root][INFO] - Training Epoch: 1/2, step 4073/7134 completed (loss: 0.04243386164307594, acc: 0.9939758777618408)
[2025-02-13 19:26:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:11][root][INFO] - Training Epoch: 1/2, step 4074/7134 completed (loss: 0.10864189267158508, acc: 0.9668874144554138)
[2025-02-13 19:26:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:12][root][INFO] - Training Epoch: 1/2, step 4075/7134 completed (loss: 0.05818933621048927, acc: 0.9800000190734863)
[2025-02-13 19:26:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:12][root][INFO] - Training Epoch: 1/2, step 4076/7134 completed (loss: 0.04777495563030243, acc: 1.0)
[2025-02-13 19:26:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:12][root][INFO] - Training Epoch: 1/2, step 4077/7134 completed (loss: 0.1738305687904358, acc: 0.9455782175064087)
[2025-02-13 19:26:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:13][root][INFO] - Training Epoch: 1/2, step 4078/7134 completed (loss: 0.1480259895324707, acc: 0.9685534834861755)
[2025-02-13 19:26:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:13][root][INFO] - Training Epoch: 1/2, step 4079/7134 completed (loss: 0.07490401715040207, acc: 0.9923664331436157)
[2025-02-13 19:26:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:14][root][INFO] - Training Epoch: 1/2, step 4080/7134 completed (loss: 0.0729609876871109, acc: 0.9793103337287903)
[2025-02-13 19:26:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:14][root][INFO] - Training Epoch: 1/2, step 4081/7134 completed (loss: 0.09662687033414841, acc: 0.9652777910232544)
[2025-02-13 19:26:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:14][root][INFO] - Training Epoch: 1/2, step 4082/7134 completed (loss: 0.5103718042373657, acc: 0.9078947305679321)
[2025-02-13 19:26:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:15][root][INFO] - Training Epoch: 1/2, step 4083/7134 completed (loss: 0.4068264365196228, acc: 0.9005848169326782)
[2025-02-13 19:26:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:15][root][INFO] - Training Epoch: 1/2, step 4084/7134 completed (loss: 0.438217431306839, acc: 0.8791946172714233)
[2025-02-13 19:26:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:16][root][INFO] - Training Epoch: 1/2, step 4085/7134 completed (loss: 0.18778996169567108, acc: 0.9368420839309692)
[2025-02-13 19:26:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:16][root][INFO] - Training Epoch: 1/2, step 4086/7134 completed (loss: 0.36067014932632446, acc: 0.9230769276618958)
[2025-02-13 19:26:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:16][root][INFO] - Training Epoch: 1/2, step 4087/7134 completed (loss: 0.1677553355693817, acc: 0.9892473220825195)
[2025-02-13 19:26:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:17][root][INFO] - Training Epoch: 1/2, step 4088/7134 completed (loss: 0.39209166169166565, acc: 0.893401026725769)
[2025-02-13 19:26:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:17][root][INFO] - Training Epoch: 1/2, step 4089/7134 completed (loss: 0.5551822185516357, acc: 0.8939393758773804)
[2025-02-13 19:26:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:17][root][INFO] - Training Epoch: 1/2, step 4090/7134 completed (loss: 0.5858906507492065, acc: 0.8899521827697754)
[2025-02-13 19:26:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:18][root][INFO] - Training Epoch: 1/2, step 4091/7134 completed (loss: 0.7126353979110718, acc: 0.8539325594902039)
[2025-02-13 19:26:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:18][root][INFO] - Training Epoch: 1/2, step 4092/7134 completed (loss: 0.24875900149345398, acc: 0.9476439952850342)
[2025-02-13 19:26:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:19][root][INFO] - Training Epoch: 1/2, step 4093/7134 completed (loss: 0.21940992772579193, acc: 0.9520000219345093)
[2025-02-13 19:26:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:19][root][INFO] - Training Epoch: 1/2, step 4094/7134 completed (loss: 0.37809517979621887, acc: 0.8902438879013062)
[2025-02-13 19:26:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:19][root][INFO] - Training Epoch: 1/2, step 4095/7134 completed (loss: 0.20170697569847107, acc: 0.9554139971733093)
[2025-02-13 19:26:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:20][root][INFO] - Training Epoch: 1/2, step 4096/7134 completed (loss: 0.4078978896141052, acc: 0.9350649118423462)
[2025-02-13 19:26:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:20][root][INFO] - Training Epoch: 1/2, step 4097/7134 completed (loss: 0.43009141087532043, acc: 0.9133333563804626)
[2025-02-13 19:26:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:21][root][INFO] - Training Epoch: 1/2, step 4098/7134 completed (loss: 0.1497022807598114, acc: 0.9655172228813171)
[2025-02-13 19:26:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:21][root][INFO] - Training Epoch: 1/2, step 4099/7134 completed (loss: 0.211152583360672, acc: 0.939393937587738)
[2025-02-13 19:26:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:21][root][INFO] - Training Epoch: 1/2, step 4100/7134 completed (loss: 0.07193905860185623, acc: 0.9803921580314636)
[2025-02-13 19:26:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:22][root][INFO] - Training Epoch: 1/2, step 4101/7134 completed (loss: 0.23103375732898712, acc: 0.9440000057220459)
[2025-02-13 19:26:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:22][root][INFO] - Training Epoch: 1/2, step 4102/7134 completed (loss: 0.280180960893631, acc: 0.9593495726585388)
[2025-02-13 19:26:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:23][root][INFO] - Training Epoch: 1/2, step 4103/7134 completed (loss: 0.11837747693061829, acc: 0.9673202633857727)
[2025-02-13 19:26:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:23][root][INFO] - Training Epoch: 1/2, step 4104/7134 completed (loss: 0.20567816495895386, acc: 0.9275362491607666)
[2025-02-13 19:26:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:23][root][INFO] - Training Epoch: 1/2, step 4105/7134 completed (loss: 0.15636877715587616, acc: 0.9599999785423279)
[2025-02-13 19:26:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:24][root][INFO] - Training Epoch: 1/2, step 4106/7134 completed (loss: 0.16698531806468964, acc: 0.9387755393981934)
[2025-02-13 19:26:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:24][root][INFO] - Training Epoch: 1/2, step 4107/7134 completed (loss: 0.17204660177230835, acc: 0.9632353186607361)
[2025-02-13 19:26:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:24][root][INFO] - Training Epoch: 1/2, step 4108/7134 completed (loss: 0.11742127686738968, acc: 0.9795918464660645)
[2025-02-13 19:26:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:25][root][INFO] - Training Epoch: 1/2, step 4109/7134 completed (loss: 0.0953030213713646, acc: 0.9863945841789246)
[2025-02-13 19:26:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:25][root][INFO] - Training Epoch: 1/2, step 4110/7134 completed (loss: 0.10249330848455429, acc: 0.9689922332763672)
[2025-02-13 19:26:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:26][root][INFO] - Training Epoch: 1/2, step 4111/7134 completed (loss: 0.15062446892261505, acc: 0.951724112033844)
[2025-02-13 19:26:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:26][root][INFO] - Training Epoch: 1/2, step 4112/7134 completed (loss: 0.13977734744548798, acc: 0.9534883499145508)
[2025-02-13 19:26:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:26][root][INFO] - Training Epoch: 1/2, step 4113/7134 completed (loss: 0.09033247083425522, acc: 0.9650349617004395)
[2025-02-13 19:26:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:27][root][INFO] - Training Epoch: 1/2, step 4114/7134 completed (loss: 0.6218286156654358, acc: 0.8695651888847351)
[2025-02-13 19:26:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:27][root][INFO] - Training Epoch: 1/2, step 4115/7134 completed (loss: 0.49075156450271606, acc: 0.9152542352676392)
[2025-02-13 19:26:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:27][root][INFO] - Training Epoch: 1/2, step 4116/7134 completed (loss: 0.1277010142803192, acc: 0.9694656729698181)
[2025-02-13 19:26:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:28][root][INFO] - Training Epoch: 1/2, step 4117/7134 completed (loss: 0.172092467546463, acc: 0.9513888955116272)
[2025-02-13 19:26:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:28][root][INFO] - Training Epoch: 1/2, step 4118/7134 completed (loss: 0.09867242723703384, acc: 0.9791666865348816)
[2025-02-13 19:26:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:29][root][INFO] - Training Epoch: 1/2, step 4119/7134 completed (loss: 0.30352431535720825, acc: 0.9256756901741028)
[2025-02-13 19:26:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:29][root][INFO] - Training Epoch: 1/2, step 4120/7134 completed (loss: 0.14163987338542938, acc: 0.9774436354637146)
[2025-02-13 19:26:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:29][root][INFO] - Training Epoch: 1/2, step 4121/7134 completed (loss: 0.10675971955060959, acc: 0.9695122241973877)
[2025-02-13 19:26:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:30][root][INFO] - Training Epoch: 1/2, step 4122/7134 completed (loss: 0.0460677333176136, acc: 0.9915966391563416)
[2025-02-13 19:26:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:30][root][INFO] - Training Epoch: 1/2, step 4123/7134 completed (loss: 0.09614257514476776, acc: 0.9675324559211731)
[2025-02-13 19:26:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:30][root][INFO] - Training Epoch: 1/2, step 4124/7134 completed (loss: 0.12903085350990295, acc: 0.9624999761581421)
[2025-02-13 19:26:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:31][root][INFO] - Training Epoch: 1/2, step 4125/7134 completed (loss: 0.10221055150032043, acc: 0.9748427867889404)
[2025-02-13 19:26:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:31][root][INFO] - Training Epoch: 1/2, step 4126/7134 completed (loss: 0.10518883913755417, acc: 0.9671052694320679)
[2025-02-13 19:26:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:32][root][INFO] - Training Epoch: 1/2, step 4127/7134 completed (loss: 0.10590248554944992, acc: 0.9731543660163879)
[2025-02-13 19:26:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:32][root][INFO] - Training Epoch: 1/2, step 4128/7134 completed (loss: 0.08983351290225983, acc: 0.9849624037742615)
[2025-02-13 19:26:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:32][root][INFO] - Training Epoch: 1/2, step 4129/7134 completed (loss: 0.08953624963760376, acc: 0.9798657894134521)
[2025-02-13 19:26:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:33][root][INFO] - Training Epoch: 1/2, step 4130/7134 completed (loss: 0.11348149180412292, acc: 0.982758641242981)
[2025-02-13 19:26:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:33][root][INFO] - Training Epoch: 1/2, step 4131/7134 completed (loss: 0.2763786315917969, acc: 0.9481481313705444)
[2025-02-13 19:26:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:34][root][INFO] - Training Epoch: 1/2, step 4132/7134 completed (loss: 0.14457890391349792, acc: 0.9537037014961243)
[2025-02-13 19:26:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:34][root][INFO] - Training Epoch: 1/2, step 4133/7134 completed (loss: 0.36881980299949646, acc: 0.9370629191398621)
[2025-02-13 19:26:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:34][root][INFO] - Training Epoch: 1/2, step 4134/7134 completed (loss: 0.7332924604415894, acc: 0.8636363744735718)
[2025-02-13 19:26:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:35][root][INFO] - Training Epoch: 1/2, step 4135/7134 completed (loss: 0.09503655880689621, acc: 0.9785714149475098)
[2025-02-13 19:26:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:35][root][INFO] - Training Epoch: 1/2, step 4136/7134 completed (loss: 0.12303372472524643, acc: 0.9624060392379761)
[2025-02-13 19:26:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:35][root][INFO] - Training Epoch: 1/2, step 4137/7134 completed (loss: 0.11317767202854156, acc: 0.970588207244873)
[2025-02-13 19:26:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:36][root][INFO] - Training Epoch: 1/2, step 4138/7134 completed (loss: 0.40660545229911804, acc: 0.9281045794487)
[2025-02-13 19:26:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:36][root][INFO] - Training Epoch: 1/2, step 4139/7134 completed (loss: 0.20424781739711761, acc: 0.9714285731315613)
[2025-02-13 19:26:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:37][root][INFO] - Training Epoch: 1/2, step 4140/7134 completed (loss: 0.4157988131046295, acc: 0.8813559412956238)
[2025-02-13 19:26:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:37][root][INFO] - Training Epoch: 1/2, step 4141/7134 completed (loss: 0.278427392244339, acc: 0.9294871687889099)
[2025-02-13 19:26:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:37][root][INFO] - Training Epoch: 1/2, step 4142/7134 completed (loss: 0.32272252440452576, acc: 0.925000011920929)
[2025-02-13 19:26:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:38][root][INFO] - Training Epoch: 1/2, step 4143/7134 completed (loss: 0.22677403688430786, acc: 0.9519650936126709)
[2025-02-13 19:26:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:38][root][INFO] - Training Epoch: 1/2, step 4144/7134 completed (loss: 0.43319085240364075, acc: 0.8820512890815735)
[2025-02-13 19:26:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:39][root][INFO] - Training Epoch: 1/2, step 4145/7134 completed (loss: 0.366649329662323, acc: 0.9152542352676392)
[2025-02-13 19:26:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:39][root][INFO] - Training Epoch: 1/2, step 4146/7134 completed (loss: 0.7223586440086365, acc: 0.8303571343421936)
[2025-02-13 19:26:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:39][root][INFO] - Training Epoch: 1/2, step 4147/7134 completed (loss: 0.8799853324890137, acc: 0.8474576473236084)
[2025-02-13 19:26:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:40][root][INFO] - Training Epoch: 1/2, step 4148/7134 completed (loss: 0.8997530341148376, acc: 0.7986577153205872)
[2025-02-13 19:26:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:40][root][INFO] - Training Epoch: 1/2, step 4149/7134 completed (loss: 0.644973874092102, acc: 0.8372092843055725)
[2025-02-13 19:26:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:41][root][INFO] - Training Epoch: 1/2, step 4150/7134 completed (loss: 0.9501487016677856, acc: 0.752136766910553)
[2025-02-13 19:26:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:41][root][INFO] - Training Epoch: 1/2, step 4151/7134 completed (loss: 0.4916830062866211, acc: 0.8842975497245789)
[2025-02-13 19:26:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:41][root][INFO] - Training Epoch: 1/2, step 4152/7134 completed (loss: 0.3219872713088989, acc: 0.9115646481513977)
[2025-02-13 19:26:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:42][root][INFO] - Training Epoch: 1/2, step 4153/7134 completed (loss: 0.36715492606163025, acc: 0.9428571462631226)
[2025-02-13 19:26:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:42][root][INFO] - Training Epoch: 1/2, step 4154/7134 completed (loss: 0.2549435496330261, acc: 0.9330143332481384)
[2025-02-13 19:26:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:43][root][INFO] - Training Epoch: 1/2, step 4155/7134 completed (loss: 0.33852627873420715, acc: 0.9194630980491638)
[2025-02-13 19:26:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:43][root][INFO] - Training Epoch: 1/2, step 4156/7134 completed (loss: 0.3917572796344757, acc: 0.8888888955116272)
[2025-02-13 19:26:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:43][root][INFO] - Training Epoch: 1/2, step 4157/7134 completed (loss: 0.30121317505836487, acc: 0.9159663915634155)
[2025-02-13 19:26:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:44][root][INFO] - Training Epoch: 1/2, step 4158/7134 completed (loss: 0.6375593543052673, acc: 0.8571428656578064)
[2025-02-13 19:26:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:44][root][INFO] - Training Epoch: 1/2, step 4159/7134 completed (loss: 0.29939061403274536, acc: 0.9172932505607605)
[2025-02-13 19:26:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:44][root][INFO] - Training Epoch: 1/2, step 4160/7134 completed (loss: 0.2965659201145172, acc: 0.9075630307197571)
[2025-02-13 19:26:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:45][root][INFO] - Training Epoch: 1/2, step 4161/7134 completed (loss: 0.1206267848610878, acc: 0.982758641242981)
[2025-02-13 19:26:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:45][root][INFO] - Training Epoch: 1/2, step 4162/7134 completed (loss: 0.21670284867286682, acc: 0.9520958065986633)
[2025-02-13 19:26:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:45][root][INFO] - Training Epoch: 1/2, step 4163/7134 completed (loss: 0.29932931065559387, acc: 0.9395973086357117)
[2025-02-13 19:26:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:46][root][INFO] - Training Epoch: 1/2, step 4164/7134 completed (loss: 0.2603376507759094, acc: 0.9378882050514221)
[2025-02-13 19:26:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:46][root][INFO] - Training Epoch: 1/2, step 4165/7134 completed (loss: 0.2570231556892395, acc: 0.9193548560142517)
[2025-02-13 19:26:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:47][root][INFO] - Training Epoch: 1/2, step 4166/7134 completed (loss: 0.3256242871284485, acc: 0.9102563858032227)
[2025-02-13 19:26:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:47][root][INFO] - Training Epoch: 1/2, step 4167/7134 completed (loss: 0.19555756449699402, acc: 0.9593495726585388)
[2025-02-13 19:26:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:47][root][INFO] - Training Epoch: 1/2, step 4168/7134 completed (loss: 0.21680551767349243, acc: 0.9248120188713074)
[2025-02-13 19:26:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:48][root][INFO] - Training Epoch: 1/2, step 4169/7134 completed (loss: 0.2946338355541229, acc: 0.9539170265197754)
[2025-02-13 19:26:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:48][root][INFO] - Training Epoch: 1/2, step 4170/7134 completed (loss: 0.38586702942848206, acc: 0.8952381014823914)
[2025-02-13 19:26:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:49][root][INFO] - Training Epoch: 1/2, step 4171/7134 completed (loss: 0.11370739340782166, acc: 0.9802955389022827)
[2025-02-13 19:26:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:49][root][INFO] - Training Epoch: 1/2, step 4172/7134 completed (loss: 0.29215577244758606, acc: 0.9222221970558167)
[2025-02-13 19:26:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:50][root][INFO] - Training Epoch: 1/2, step 4173/7134 completed (loss: 0.2512216866016388, acc: 0.9444444179534912)
[2025-02-13 19:26:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:50][root][INFO] - Training Epoch: 1/2, step 4174/7134 completed (loss: 0.03811930492520332, acc: 0.9920634627342224)
[2025-02-13 19:26:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:50][root][INFO] - Training Epoch: 1/2, step 4175/7134 completed (loss: 0.15886659920215607, acc: 0.9436619877815247)
[2025-02-13 19:26:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:51][root][INFO] - Training Epoch: 1/2, step 4176/7134 completed (loss: 0.11431042850017548, acc: 0.9677419066429138)
[2025-02-13 19:26:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:51][root][INFO] - Training Epoch: 1/2, step 4177/7134 completed (loss: 0.10379865020513535, acc: 0.9707602262496948)
[2025-02-13 19:26:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:51][root][INFO] - Training Epoch: 1/2, step 4178/7134 completed (loss: 0.1590358316898346, acc: 0.9736841917037964)
[2025-02-13 19:26:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:52][root][INFO] - Training Epoch: 1/2, step 4179/7134 completed (loss: 0.044702835381031036, acc: 1.0)
[2025-02-13 19:26:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:52][root][INFO] - Training Epoch: 1/2, step 4180/7134 completed (loss: 0.0956743136048317, acc: 0.981249988079071)
[2025-02-13 19:26:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:53][root][INFO] - Training Epoch: 1/2, step 4181/7134 completed (loss: 0.03216351568698883, acc: 0.9927536249160767)
[2025-02-13 19:26:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:53][root][INFO] - Training Epoch: 1/2, step 4182/7134 completed (loss: 0.051134321838617325, acc: 0.9935064911842346)
[2025-02-13 19:26:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:53][root][INFO] - Training Epoch: 1/2, step 4183/7134 completed (loss: 0.17154991626739502, acc: 0.9570552110671997)
[2025-02-13 19:26:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:54][root][INFO] - Training Epoch: 1/2, step 4184/7134 completed (loss: 0.11086183786392212, acc: 0.970588207244873)
[2025-02-13 19:26:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:54][root][INFO] - Training Epoch: 1/2, step 4185/7134 completed (loss: 0.08047179877758026, acc: 0.9809523820877075)
[2025-02-13 19:26:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:54][root][INFO] - Training Epoch: 1/2, step 4186/7134 completed (loss: 0.19009822607040405, acc: 0.969072163105011)
[2025-02-13 19:26:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:55][root][INFO] - Training Epoch: 1/2, step 4187/7134 completed (loss: 0.04516042023897171, acc: 0.9919354915618896)
[2025-02-13 19:26:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:55][root][INFO] - Training Epoch: 1/2, step 4188/7134 completed (loss: 0.06020566448569298, acc: 0.9736841917037964)
[2025-02-13 19:26:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:56][root][INFO] - Training Epoch: 1/2, step 4189/7134 completed (loss: 0.1211736872792244, acc: 0.9820359349250793)
[2025-02-13 19:26:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:56][root][INFO] - Training Epoch: 1/2, step 4190/7134 completed (loss: 0.10011137276887894, acc: 0.9707602262496948)
[2025-02-13 19:26:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:56][root][INFO] - Training Epoch: 1/2, step 4191/7134 completed (loss: 0.1273711621761322, acc: 0.9615384340286255)
[2025-02-13 19:26:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:57][root][INFO] - Training Epoch: 1/2, step 4192/7134 completed (loss: 0.06632623076438904, acc: 0.9794520735740662)
[2025-02-13 19:26:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:57][root][INFO] - Training Epoch: 1/2, step 4193/7134 completed (loss: 0.12335643917322159, acc: 0.9776536226272583)
[2025-02-13 19:26:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:58][root][INFO] - Training Epoch: 1/2, step 4194/7134 completed (loss: 0.04473734647035599, acc: 0.9935897588729858)
[2025-02-13 19:26:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:58][root][INFO] - Training Epoch: 1/2, step 4195/7134 completed (loss: 0.2644871175289154, acc: 0.9485714435577393)
[2025-02-13 19:26:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:58][root][INFO] - Training Epoch: 1/2, step 4196/7134 completed (loss: 0.08568522334098816, acc: 0.9784172773361206)
[2025-02-13 19:26:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:59][root][INFO] - Training Epoch: 1/2, step 4197/7134 completed (loss: 0.11468885093927383, acc: 0.9728260636329651)
[2025-02-13 19:26:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:59][root][INFO] - Training Epoch: 1/2, step 4198/7134 completed (loss: 0.11484409123659134, acc: 0.9885714054107666)
[2025-02-13 19:26:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:26:59][root][INFO] - Training Epoch: 1/2, step 4199/7134 completed (loss: 0.3258669972419739, acc: 0.9366196990013123)
[2025-02-13 19:27:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:00][root][INFO] - Training Epoch: 1/2, step 4200/7134 completed (loss: 0.29156574606895447, acc: 0.9379310607910156)
[2025-02-13 19:27:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:00][root][INFO] - Training Epoch: 1/2, step 4201/7134 completed (loss: 0.09140899032354355, acc: 0.9716312289237976)
[2025-02-13 19:27:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:01][root][INFO] - Training Epoch: 1/2, step 4202/7134 completed (loss: 0.19049720466136932, acc: 0.9398496150970459)
[2025-02-13 19:27:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:01][root][INFO] - Training Epoch: 1/2, step 4203/7134 completed (loss: 0.14878813922405243, acc: 0.9683544039726257)
[2025-02-13 19:27:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:01][root][INFO] - Training Epoch: 1/2, step 4204/7134 completed (loss: 0.23943418264389038, acc: 0.9599999785423279)
[2025-02-13 19:27:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:02][root][INFO] - Training Epoch: 1/2, step 4205/7134 completed (loss: 0.16460566222667694, acc: 0.970588207244873)
[2025-02-13 19:27:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:02][root][INFO] - Training Epoch: 1/2, step 4206/7134 completed (loss: 0.19893260300159454, acc: 0.9623655676841736)
[2025-02-13 19:27:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:03][root][INFO] - Training Epoch: 1/2, step 4207/7134 completed (loss: 0.17860041558742523, acc: 0.9748427867889404)
[2025-02-13 19:27:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:03][root][INFO] - Training Epoch: 1/2, step 4208/7134 completed (loss: 0.23211251199245453, acc: 0.9655172228813171)
[2025-02-13 19:27:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:03][root][INFO] - Training Epoch: 1/2, step 4209/7134 completed (loss: 0.15071608126163483, acc: 0.9668508172035217)
[2025-02-13 19:27:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:04][root][INFO] - Training Epoch: 1/2, step 4210/7134 completed (loss: 0.08899121731519699, acc: 0.9736841917037964)
[2025-02-13 19:27:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:04][root][INFO] - Training Epoch: 1/2, step 4211/7134 completed (loss: 0.11640854924917221, acc: 0.9672130942344666)
[2025-02-13 19:27:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:05][root][INFO] - Training Epoch: 1/2, step 4212/7134 completed (loss: 0.1297348439693451, acc: 0.9741935729980469)
[2025-02-13 19:27:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:05][root][INFO] - Training Epoch: 1/2, step 4213/7134 completed (loss: 0.10128670930862427, acc: 0.977011501789093)
[2025-02-13 19:27:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:05][root][INFO] - Training Epoch: 1/2, step 4214/7134 completed (loss: 0.27777713537216187, acc: 0.9356725215911865)
[2025-02-13 19:27:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:06][root][INFO] - Training Epoch: 1/2, step 4215/7134 completed (loss: 0.11490561813116074, acc: 0.9842519760131836)
[2025-02-13 19:27:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:06][root][INFO] - Training Epoch: 1/2, step 4216/7134 completed (loss: 0.20788569748401642, acc: 0.9846153855323792)
[2025-02-13 19:27:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:07][root][INFO] - Training Epoch: 1/2, step 4217/7134 completed (loss: 0.05064398795366287, acc: 0.9833333492279053)
[2025-02-13 19:27:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:07][root][INFO] - Training Epoch: 1/2, step 4218/7134 completed (loss: 0.19609464704990387, acc: 0.9776119589805603)
[2025-02-13 19:27:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:07][root][INFO] - Training Epoch: 1/2, step 4219/7134 completed (loss: 0.12183192372322083, acc: 0.9855072498321533)
[2025-02-13 19:27:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:08][root][INFO] - Training Epoch: 1/2, step 4220/7134 completed (loss: 0.2912690043449402, acc: 0.9545454382896423)
[2025-02-13 19:27:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:08][root][INFO] - Training Epoch: 1/2, step 4221/7134 completed (loss: 0.1949777603149414, acc: 0.9391891956329346)
[2025-02-13 19:27:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:08][root][INFO] - Training Epoch: 1/2, step 4222/7134 completed (loss: 0.5147839784622192, acc: 0.9122806787490845)
[2025-02-13 19:27:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:09][root][INFO] - Training Epoch: 1/2, step 4223/7134 completed (loss: 0.4401923716068268, acc: 0.8944723606109619)
[2025-02-13 19:27:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:09][root][INFO] - Training Epoch: 1/2, step 4224/7134 completed (loss: 0.3493461310863495, acc: 0.9435483813285828)
[2025-02-13 19:27:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:10][root][INFO] - Training Epoch: 1/2, step 4225/7134 completed (loss: 0.36180025339126587, acc: 0.8846153616905212)
[2025-02-13 19:27:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:10][root][INFO] - Training Epoch: 1/2, step 4226/7134 completed (loss: 0.38210824131965637, acc: 0.9187816977500916)
[2025-02-13 19:27:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:10][root][INFO] - Training Epoch: 1/2, step 4227/7134 completed (loss: 0.6421083211898804, acc: 0.8584905862808228)
[2025-02-13 19:27:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:11][root][INFO] - Training Epoch: 1/2, step 4228/7134 completed (loss: 0.6169283390045166, acc: 0.8525640964508057)
[2025-02-13 19:27:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:11][root][INFO] - Training Epoch: 1/2, step 4229/7134 completed (loss: 0.4459015130996704, acc: 0.9039999842643738)
[2025-02-13 19:27:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:11][root][INFO] - Training Epoch: 1/2, step 4230/7134 completed (loss: 0.41335803270339966, acc: 0.895348846912384)
[2025-02-13 19:27:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:12][root][INFO] - Training Epoch: 1/2, step 4231/7134 completed (loss: 0.6980040669441223, acc: 0.8926174640655518)
[2025-02-13 19:27:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:12][root][INFO] - Training Epoch: 1/2, step 4232/7134 completed (loss: 0.4024898409843445, acc: 0.8999999761581421)
[2025-02-13 19:27:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:13][root][INFO] - Training Epoch: 1/2, step 4233/7134 completed (loss: 0.36440664529800415, acc: 0.9290780425071716)
[2025-02-13 19:27:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:13][root][INFO] - Training Epoch: 1/2, step 4234/7134 completed (loss: 0.14772644639015198, acc: 0.9716312289237976)
[2025-02-13 19:27:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:13][root][INFO] - Training Epoch: 1/2, step 4235/7134 completed (loss: 0.13553766906261444, acc: 0.9710144996643066)
[2025-02-13 19:27:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:14][root][INFO] - Training Epoch: 1/2, step 4236/7134 completed (loss: 0.11882371455430984, acc: 0.9677419066429138)
[2025-02-13 19:27:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:14][root][INFO] - Training Epoch: 1/2, step 4237/7134 completed (loss: 0.13171060383319855, acc: 0.9719626307487488)
[2025-02-13 19:27:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:14][root][INFO] - Training Epoch: 1/2, step 4238/7134 completed (loss: 0.24432162940502167, acc: 0.9461538195610046)
[2025-02-13 19:27:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:15][root][INFO] - Training Epoch: 1/2, step 4239/7134 completed (loss: 0.1250600963830948, acc: 0.9760000109672546)
[2025-02-13 19:27:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:15][root][INFO] - Training Epoch: 1/2, step 4240/7134 completed (loss: 0.2560807466506958, acc: 0.9577465057373047)
[2025-02-13 19:27:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:16][root][INFO] - Training Epoch: 1/2, step 4241/7134 completed (loss: 0.07193075865507126, acc: 0.9900000095367432)
[2025-02-13 19:27:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:16][root][INFO] - Training Epoch: 1/2, step 4242/7134 completed (loss: 0.2567124664783478, acc: 0.9285714030265808)
[2025-02-13 19:27:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:16][root][INFO] - Training Epoch: 1/2, step 4243/7134 completed (loss: 0.24751746654510498, acc: 0.93388432264328)
[2025-02-13 19:27:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:17][root][INFO] - Training Epoch: 1/2, step 4244/7134 completed (loss: 0.09812388569116592, acc: 0.9905660152435303)
[2025-02-13 19:27:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:17][root][INFO] - Training Epoch: 1/2, step 4245/7134 completed (loss: 0.13983796536922455, acc: 0.9684210419654846)
[2025-02-13 19:27:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:18][root][INFO] - Training Epoch: 1/2, step 4246/7134 completed (loss: 0.13463911414146423, acc: 0.9794520735740662)
[2025-02-13 19:27:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:18][root][INFO] - Training Epoch: 1/2, step 4247/7134 completed (loss: 0.11581923067569733, acc: 0.9760000109672546)
[2025-02-13 19:27:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:18][root][INFO] - Training Epoch: 1/2, step 4248/7134 completed (loss: 0.24126240611076355, acc: 0.9520547986030579)
[2025-02-13 19:27:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:19][root][INFO] - Training Epoch: 1/2, step 4249/7134 completed (loss: 0.32106155157089233, acc: 0.9411764740943909)
[2025-02-13 19:27:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:19][root][INFO] - Training Epoch: 1/2, step 4250/7134 completed (loss: 0.39451783895492554, acc: 0.9454545378684998)
[2025-02-13 19:27:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:19][root][INFO] - Training Epoch: 1/2, step 4251/7134 completed (loss: 0.11858238279819489, acc: 0.9774436354637146)
[2025-02-13 19:27:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:20][root][INFO] - Training Epoch: 1/2, step 4252/7134 completed (loss: 0.1515703946352005, acc: 0.9596773982048035)
[2025-02-13 19:27:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:20][root][INFO] - Training Epoch: 1/2, step 4253/7134 completed (loss: 0.16243302822113037, acc: 0.9652174115180969)
[2025-02-13 19:27:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:21][root][INFO] - Training Epoch: 1/2, step 4254/7134 completed (loss: 0.1073274314403534, acc: 0.9886363744735718)
[2025-02-13 19:27:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:21][root][INFO] - Training Epoch: 1/2, step 4255/7134 completed (loss: 0.30902111530303955, acc: 0.9307692050933838)
[2025-02-13 19:27:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:21][root][INFO] - Training Epoch: 1/2, step 4256/7134 completed (loss: 0.14665572345256805, acc: 0.9658119678497314)
[2025-02-13 19:27:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:22][root][INFO] - Training Epoch: 1/2, step 4257/7134 completed (loss: 0.13404279947280884, acc: 0.9696969985961914)
[2025-02-13 19:27:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:22][root][INFO] - Training Epoch: 1/2, step 4258/7134 completed (loss: 0.0584569089114666, acc: 1.0)
[2025-02-13 19:27:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:23][root][INFO] - Training Epoch: 1/2, step 4259/7134 completed (loss: 0.24664881825447083, acc: 0.949999988079071)
[2025-02-13 19:27:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:23][root][INFO] - Training Epoch: 1/2, step 4260/7134 completed (loss: 0.12159093469381332, acc: 0.9541284441947937)
[2025-02-13 19:27:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:23][root][INFO] - Training Epoch: 1/2, step 4261/7134 completed (loss: 0.16025763750076294, acc: 0.947826087474823)
[2025-02-13 19:27:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:24][root][INFO] - Training Epoch: 1/2, step 4262/7134 completed (loss: 0.2177819311618805, acc: 0.9370629191398621)
[2025-02-13 19:27:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:24][root][INFO] - Training Epoch: 1/2, step 4263/7134 completed (loss: 0.1631743609905243, acc: 0.9629629850387573)
[2025-02-13 19:27:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:24][root][INFO] - Training Epoch: 1/2, step 4264/7134 completed (loss: 0.2799030542373657, acc: 0.932584285736084)
[2025-02-13 19:27:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:25][root][INFO] - Training Epoch: 1/2, step 4265/7134 completed (loss: 0.39307186007499695, acc: 0.9244186282157898)
[2025-02-13 19:27:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:25][root][INFO] - Training Epoch: 1/2, step 4266/7134 completed (loss: 0.1452520340681076, acc: 0.9581151604652405)
[2025-02-13 19:27:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:26][root][INFO] - Training Epoch: 1/2, step 4267/7134 completed (loss: 0.1678282469511032, acc: 0.939393937587738)
[2025-02-13 19:27:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:26][root][INFO] - Training Epoch: 1/2, step 4268/7134 completed (loss: 0.2394605278968811, acc: 0.9512194991111755)
[2025-02-13 19:27:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:26][root][INFO] - Training Epoch: 1/2, step 4269/7134 completed (loss: 0.31315502524375916, acc: 0.9248554706573486)
[2025-02-13 19:27:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:27][root][INFO] - Training Epoch: 1/2, step 4270/7134 completed (loss: 0.1813926249742508, acc: 0.9583333134651184)
[2025-02-13 19:27:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:27][root][INFO] - Training Epoch: 1/2, step 4271/7134 completed (loss: 0.16648723185062408, acc: 0.9405405521392822)
[2025-02-13 19:27:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:28][root][INFO] - Training Epoch: 1/2, step 4272/7134 completed (loss: 0.18758408725261688, acc: 0.9402984976768494)
[2025-02-13 19:27:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:28][root][INFO] - Training Epoch: 1/2, step 4273/7134 completed (loss: 0.1036258339881897, acc: 0.9735449552536011)
[2025-02-13 19:27:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:28][root][INFO] - Training Epoch: 1/2, step 4274/7134 completed (loss: 0.2838501036167145, acc: 0.9313725233078003)
[2025-02-13 19:27:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:29][root][INFO] - Training Epoch: 1/2, step 4275/7134 completed (loss: 0.1457793414592743, acc: 0.9608938694000244)
[2025-02-13 19:27:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:29][root][INFO] - Training Epoch: 1/2, step 4276/7134 completed (loss: 0.20923717319965363, acc: 0.9621621370315552)
[2025-02-13 19:27:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:29][root][INFO] - Training Epoch: 1/2, step 4277/7134 completed (loss: 0.07419510930776596, acc: 0.9718309640884399)
[2025-02-13 19:27:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:30][root][INFO] - Training Epoch: 1/2, step 4278/7134 completed (loss: 0.11683868616819382, acc: 0.9744898080825806)
[2025-02-13 19:27:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:30][root][INFO] - Training Epoch: 1/2, step 4279/7134 completed (loss: 0.11683988571166992, acc: 0.9751552939414978)
[2025-02-13 19:27:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:31][root][INFO] - Training Epoch: 1/2, step 4280/7134 completed (loss: 0.04149322584271431, acc: 0.9929078221321106)
[2025-02-13 19:27:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:31][root][INFO] - Training Epoch: 1/2, step 4281/7134 completed (loss: 0.1630522906780243, acc: 0.9731183052062988)
[2025-02-13 19:27:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:31][root][INFO] - Training Epoch: 1/2, step 4282/7134 completed (loss: 0.11764874309301376, acc: 0.9482758641242981)
[2025-02-13 19:27:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:32][root][INFO] - Training Epoch: 1/2, step 4283/7134 completed (loss: 0.08609797060489655, acc: 0.9846938848495483)
[2025-02-13 19:27:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:32][root][INFO] - Training Epoch: 1/2, step 4284/7134 completed (loss: 0.16962112486362457, acc: 0.9689922332763672)
[2025-02-13 19:27:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:33][root][INFO] - Training Epoch: 1/2, step 4285/7134 completed (loss: 0.08088751137256622, acc: 0.9922480583190918)
[2025-02-13 19:27:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:33][root][INFO] - Training Epoch: 1/2, step 4286/7134 completed (loss: 0.1979246437549591, acc: 0.9579831957817078)
[2025-02-13 19:27:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:33][root][INFO] - Training Epoch: 1/2, step 4287/7134 completed (loss: 0.16820493340492249, acc: 0.967391312122345)
[2025-02-13 19:27:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:34][root][INFO] - Training Epoch: 1/2, step 4288/7134 completed (loss: 0.1368677318096161, acc: 0.9588235020637512)
[2025-02-13 19:27:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:34][root][INFO] - Training Epoch: 1/2, step 4289/7134 completed (loss: 0.18290871381759644, acc: 0.9693251252174377)
[2025-02-13 19:27:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:35][root][INFO] - Training Epoch: 1/2, step 4290/7134 completed (loss: 0.1413884162902832, acc: 0.9537572264671326)
[2025-02-13 19:27:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:35][root][INFO] - Training Epoch: 1/2, step 4291/7134 completed (loss: 0.08923034369945526, acc: 0.9826589822769165)
[2025-02-13 19:27:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:35][root][INFO] - Training Epoch: 1/2, step 4292/7134 completed (loss: 0.19358232617378235, acc: 0.9612902998924255)
[2025-02-13 19:27:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:36][root][INFO] - Training Epoch: 1/2, step 4293/7134 completed (loss: 0.21707113087177277, acc: 0.9583333134651184)
[2025-02-13 19:27:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:36][root][INFO] - Training Epoch: 1/2, step 4294/7134 completed (loss: 0.29424265027046204, acc: 0.9485714435577393)
[2025-02-13 19:27:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:36][root][INFO] - Training Epoch: 1/2, step 4295/7134 completed (loss: 0.21902760863304138, acc: 0.9571428298950195)
[2025-02-13 19:27:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:37][root][INFO] - Training Epoch: 1/2, step 4296/7134 completed (loss: 0.10666549205780029, acc: 0.9801980257034302)
[2025-02-13 19:27:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:37][root][INFO] - Training Epoch: 1/2, step 4297/7134 completed (loss: 0.08980871737003326, acc: 0.9714285731315613)
[2025-02-13 19:27:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:38][root][INFO] - Training Epoch: 1/2, step 4298/7134 completed (loss: 0.18920421600341797, acc: 0.9602272510528564)
[2025-02-13 19:27:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:38][root][INFO] - Training Epoch: 1/2, step 4299/7134 completed (loss: 0.1732787936925888, acc: 0.9462365508079529)
[2025-02-13 19:27:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:38][root][INFO] - Training Epoch: 1/2, step 4300/7134 completed (loss: 0.2940041720867157, acc: 0.9327731132507324)
[2025-02-13 19:27:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:39][root][INFO] - Training Epoch: 1/2, step 4301/7134 completed (loss: 0.12710653245449066, acc: 0.9718309640884399)
[2025-02-13 19:27:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:39][root][INFO] - Training Epoch: 1/2, step 4302/7134 completed (loss: 0.14287370443344116, acc: 0.9745222926139832)
[2025-02-13 19:27:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:40][root][INFO] - Training Epoch: 1/2, step 4303/7134 completed (loss: 0.21089740097522736, acc: 0.9455445408821106)
[2025-02-13 19:27:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:40][root][INFO] - Training Epoch: 1/2, step 4304/7134 completed (loss: 0.16190168261528015, acc: 0.9585798978805542)
[2025-02-13 19:27:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:40][root][INFO] - Training Epoch: 1/2, step 4305/7134 completed (loss: 0.1324261575937271, acc: 0.9700000286102295)
[2025-02-13 19:27:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:41][root][INFO] - Training Epoch: 1/2, step 4306/7134 completed (loss: 0.21808892488479614, acc: 0.9528796076774597)
[2025-02-13 19:27:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:41][root][INFO] - Training Epoch: 1/2, step 4307/7134 completed (loss: 0.14355354011058807, acc: 0.9542483687400818)
[2025-02-13 19:27:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:41][root][INFO] - Training Epoch: 1/2, step 4308/7134 completed (loss: 0.1467103511095047, acc: 0.9518716335296631)
[2025-02-13 19:27:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:42][root][INFO] - Training Epoch: 1/2, step 4309/7134 completed (loss: 0.17756912112236023, acc: 0.9545454382896423)
[2025-02-13 19:27:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:42][root][INFO] - Training Epoch: 1/2, step 4310/7134 completed (loss: 0.17311517894268036, acc: 0.9593908786773682)
[2025-02-13 19:27:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:43][root][INFO] - Training Epoch: 1/2, step 4311/7134 completed (loss: 0.1120779886841774, acc: 0.978723406791687)
[2025-02-13 19:27:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:43][root][INFO] - Training Epoch: 1/2, step 4312/7134 completed (loss: 0.13071687519550323, acc: 0.9631901979446411)
[2025-02-13 19:27:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:43][root][INFO] - Training Epoch: 1/2, step 4313/7134 completed (loss: 0.08452703803777695, acc: 0.9878787994384766)
[2025-02-13 19:27:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:44][root][INFO] - Training Epoch: 1/2, step 4314/7134 completed (loss: 0.17440052330493927, acc: 0.9738562107086182)
[2025-02-13 19:27:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:44][root][INFO] - Training Epoch: 1/2, step 4315/7134 completed (loss: 0.16382372379302979, acc: 0.9583333134651184)
[2025-02-13 19:27:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:44][root][INFO] - Training Epoch: 1/2, step 4316/7134 completed (loss: 0.17060595750808716, acc: 0.9640718698501587)
[2025-02-13 19:27:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:45][root][INFO] - Training Epoch: 1/2, step 4317/7134 completed (loss: 0.12130139768123627, acc: 0.9709302186965942)
[2025-02-13 19:27:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:45][root][INFO] - Training Epoch: 1/2, step 4318/7134 completed (loss: 0.15193413197994232, acc: 0.9642857313156128)
[2025-02-13 19:27:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:46][root][INFO] - Training Epoch: 1/2, step 4319/7134 completed (loss: 0.09443888068199158, acc: 0.9802955389022827)
[2025-02-13 19:27:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:46][root][INFO] - Training Epoch: 1/2, step 4320/7134 completed (loss: 0.1616833359003067, acc: 0.9615384340286255)
[2025-02-13 19:27:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:46][root][INFO] - Training Epoch: 1/2, step 4321/7134 completed (loss: 0.48400795459747314, acc: 0.9108911156654358)
[2025-02-13 19:27:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:47][root][INFO] - Training Epoch: 1/2, step 4322/7134 completed (loss: 0.278936505317688, acc: 0.9182389974594116)
[2025-02-13 19:27:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:47][root][INFO] - Training Epoch: 1/2, step 4323/7134 completed (loss: 0.2072819024324417, acc: 0.9248120188713074)
[2025-02-13 19:27:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:48][root][INFO] - Training Epoch: 1/2, step 4324/7134 completed (loss: 0.1454349309206009, acc: 0.95652174949646)
[2025-02-13 19:27:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:48][root][INFO] - Training Epoch: 1/2, step 4325/7134 completed (loss: 0.2702285349369049, acc: 0.9383561611175537)
[2025-02-13 19:27:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:48][root][INFO] - Training Epoch: 1/2, step 4326/7134 completed (loss: 0.28443560004234314, acc: 0.9242424368858337)
[2025-02-13 19:27:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:49][root][INFO] - Training Epoch: 1/2, step 4327/7134 completed (loss: 0.12027395516633987, acc: 0.9635036587715149)
[2025-02-13 19:27:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:49][root][INFO] - Training Epoch: 1/2, step 4328/7134 completed (loss: 0.13784009218215942, acc: 0.96875)
[2025-02-13 19:27:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:49][root][INFO] - Training Epoch: 1/2, step 4329/7134 completed (loss: 0.1976737082004547, acc: 0.9356725215911865)
[2025-02-13 19:27:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:50][root][INFO] - Training Epoch: 1/2, step 4330/7134 completed (loss: 0.13769814372062683, acc: 0.954954981803894)
[2025-02-13 19:27:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:50][root][INFO] - Training Epoch: 1/2, step 4331/7134 completed (loss: 0.12492374330759048, acc: 0.9629629850387573)
[2025-02-13 19:27:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:51][root][INFO] - Training Epoch: 1/2, step 4332/7134 completed (loss: 0.23980574309825897, acc: 0.9365079402923584)
[2025-02-13 19:27:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:51][root][INFO] - Training Epoch: 1/2, step 4333/7134 completed (loss: 0.09039244055747986, acc: 0.981249988079071)
[2025-02-13 19:27:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:51][root][INFO] - Training Epoch: 1/2, step 4334/7134 completed (loss: 0.17868950963020325, acc: 0.9693251252174377)
[2025-02-13 19:27:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:52][root][INFO] - Training Epoch: 1/2, step 4335/7134 completed (loss: 0.16883274912834167, acc: 0.9669421315193176)
[2025-02-13 19:27:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:52][root][INFO] - Training Epoch: 1/2, step 4336/7134 completed (loss: 0.037445563822984695, acc: 0.9928057789802551)
[2025-02-13 19:27:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:53][root][INFO] - Training Epoch: 1/2, step 4337/7134 completed (loss: 0.11274977028369904, acc: 0.9650349617004395)
[2025-02-13 19:27:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:53][root][INFO] - Training Epoch: 1/2, step 4338/7134 completed (loss: 0.15057772397994995, acc: 0.9636363387107849)
[2025-02-13 19:27:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:53][root][INFO] - Training Epoch: 1/2, step 4339/7134 completed (loss: 0.1412748545408249, acc: 0.970588207244873)
[2025-02-13 19:27:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:54][root][INFO] - Training Epoch: 1/2, step 4340/7134 completed (loss: 0.15922409296035767, acc: 0.9655172228813171)
[2025-02-13 19:27:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:54][root][INFO] - Training Epoch: 1/2, step 4341/7134 completed (loss: 0.19504354894161224, acc: 0.9523809552192688)
[2025-02-13 19:27:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:55][root][INFO] - Training Epoch: 1/2, step 4342/7134 completed (loss: 0.2334185242652893, acc: 0.9337748289108276)
[2025-02-13 19:27:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:55][root][INFO] - Training Epoch: 1/2, step 4343/7134 completed (loss: 0.24753016233444214, acc: 0.9407407641410828)
[2025-02-13 19:27:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:56][root][INFO] - Training Epoch: 1/2, step 4344/7134 completed (loss: 0.20641161501407623, acc: 0.947826087474823)
[2025-02-13 19:27:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:56][root][INFO] - Training Epoch: 1/2, step 4345/7134 completed (loss: 0.127071350812912, acc: 0.9639639854431152)
[2025-02-13 19:27:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:56][root][INFO] - Training Epoch: 1/2, step 4346/7134 completed (loss: 0.12437861412763596, acc: 0.9819819927215576)
[2025-02-13 19:27:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:57][root][INFO] - Training Epoch: 1/2, step 4347/7134 completed (loss: 0.2396974265575409, acc: 0.9408283829689026)
[2025-02-13 19:27:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:57][root][INFO] - Training Epoch: 1/2, step 4348/7134 completed (loss: 0.13055646419525146, acc: 0.9795918464660645)
[2025-02-13 19:27:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:57][root][INFO] - Training Epoch: 1/2, step 4349/7134 completed (loss: 0.1832038313150406, acc: 0.9583333134651184)
[2025-02-13 19:27:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:58][root][INFO] - Training Epoch: 1/2, step 4350/7134 completed (loss: 0.1619105339050293, acc: 0.9719101190567017)
[2025-02-13 19:27:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:58][root][INFO] - Training Epoch: 1/2, step 4351/7134 completed (loss: 0.19710494577884674, acc: 0.942307710647583)
[2025-02-13 19:27:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:59][root][INFO] - Training Epoch: 1/2, step 4352/7134 completed (loss: 0.29972347617149353, acc: 0.9130434989929199)
[2025-02-13 19:27:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:59][root][INFO] - Training Epoch: 1/2, step 4353/7134 completed (loss: 0.233892560005188, acc: 0.946107804775238)
[2025-02-13 19:27:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:27:59][root][INFO] - Training Epoch: 1/2, step 4354/7134 completed (loss: 0.20625829696655273, acc: 0.9570552110671997)
[2025-02-13 19:27:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:00][root][INFO] - Training Epoch: 1/2, step 4355/7134 completed (loss: 0.3022736608982086, acc: 0.9254658222198486)
[2025-02-13 19:28:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:00][root][INFO] - Training Epoch: 1/2, step 4356/7134 completed (loss: 0.14306458830833435, acc: 0.9825581312179565)
[2025-02-13 19:28:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:00][root][INFO] - Training Epoch: 1/2, step 4357/7134 completed (loss: 0.5177239179611206, acc: 0.8724831938743591)
[2025-02-13 19:28:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:01][root][INFO] - Training Epoch: 1/2, step 4358/7134 completed (loss: 0.2787995934486389, acc: 0.9390243887901306)
[2025-02-13 19:28:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:01][root][INFO] - Training Epoch: 1/2, step 4359/7134 completed (loss: 0.12690487504005432, acc: 0.9682539701461792)
[2025-02-13 19:28:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:02][root][INFO] - Training Epoch: 1/2, step 4360/7134 completed (loss: 0.14492012560367584, acc: 0.9490445852279663)
[2025-02-13 19:28:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:02][root][INFO] - Training Epoch: 1/2, step 4361/7134 completed (loss: 0.18563838303089142, acc: 0.9470198750495911)
[2025-02-13 19:28:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:02][root][INFO] - Training Epoch: 1/2, step 4362/7134 completed (loss: 0.19227366149425507, acc: 0.9619565010070801)
[2025-02-13 19:28:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:03][root][INFO] - Training Epoch: 1/2, step 4363/7134 completed (loss: 0.16321977972984314, acc: 0.9772727489471436)
[2025-02-13 19:28:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:03][root][INFO] - Training Epoch: 1/2, step 4364/7134 completed (loss: 0.1559179276227951, acc: 0.9689922332763672)
[2025-02-13 19:28:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:03][root][INFO] - Training Epoch: 1/2, step 4365/7134 completed (loss: 0.11925815045833588, acc: 0.9640287756919861)
[2025-02-13 19:28:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:04][root][INFO] - Training Epoch: 1/2, step 4366/7134 completed (loss: 0.21771368384361267, acc: 0.9476743936538696)
[2025-02-13 19:28:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:04][root][INFO] - Training Epoch: 1/2, step 4367/7134 completed (loss: 0.2080184817314148, acc: 0.926174521446228)
[2025-02-13 19:28:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:05][root][INFO] - Training Epoch: 1/2, step 4368/7134 completed (loss: 0.1398163139820099, acc: 0.9880239367485046)
[2025-02-13 19:28:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:05][root][INFO] - Training Epoch: 1/2, step 4369/7134 completed (loss: 0.17870956659317017, acc: 0.9512194991111755)
[2025-02-13 19:28:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:05][root][INFO] - Training Epoch: 1/2, step 4370/7134 completed (loss: 0.2889852225780487, acc: 0.9418604373931885)
[2025-02-13 19:28:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:06][root][INFO] - Training Epoch: 1/2, step 4371/7134 completed (loss: 0.1782761514186859, acc: 0.9512194991111755)
[2025-02-13 19:28:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:06][root][INFO] - Training Epoch: 1/2, step 4372/7134 completed (loss: 0.1344434916973114, acc: 0.9590163826942444)
[2025-02-13 19:28:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:07][root][INFO] - Training Epoch: 1/2, step 4373/7134 completed (loss: 0.24169480800628662, acc: 0.9447513818740845)
[2025-02-13 19:28:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:07][root][INFO] - Training Epoch: 1/2, step 4374/7134 completed (loss: 0.2716403901576996, acc: 0.9207317233085632)
[2025-02-13 19:28:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:07][root][INFO] - Training Epoch: 1/2, step 4375/7134 completed (loss: 0.15382498502731323, acc: 0.9583333134651184)
[2025-02-13 19:28:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:08][root][INFO] - Training Epoch: 1/2, step 4376/7134 completed (loss: 0.17285169661045074, acc: 0.9375)
[2025-02-13 19:28:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:08][root][INFO] - Training Epoch: 1/2, step 4377/7134 completed (loss: 0.2834380269050598, acc: 0.9647058844566345)
[2025-02-13 19:28:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:09][root][INFO] - Training Epoch: 1/2, step 4378/7134 completed (loss: 0.2941634953022003, acc: 0.9248120188713074)
[2025-02-13 19:28:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:09][root][INFO] - Training Epoch: 1/2, step 4379/7134 completed (loss: 0.4493103325366974, acc: 0.875)
[2025-02-13 19:28:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:09][root][INFO] - Training Epoch: 1/2, step 4380/7134 completed (loss: 0.2345062792301178, acc: 0.9510489702224731)
[2025-02-13 19:28:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:10][root][INFO] - Training Epoch: 1/2, step 4381/7134 completed (loss: 0.29464617371559143, acc: 0.9520958065986633)
[2025-02-13 19:28:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:10][root][INFO] - Training Epoch: 1/2, step 4382/7134 completed (loss: 0.3272069990634918, acc: 0.9419354796409607)
[2025-02-13 19:28:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:10][root][INFO] - Training Epoch: 1/2, step 4383/7134 completed (loss: 0.411634236574173, acc: 0.913705587387085)
[2025-02-13 19:28:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:11][root][INFO] - Training Epoch: 1/2, step 4384/7134 completed (loss: 0.14611853659152985, acc: 0.9715909361839294)
[2025-02-13 19:28:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:11][root][INFO] - Training Epoch: 1/2, step 4385/7134 completed (loss: 0.34590983390808105, acc: 0.9015544056892395)
[2025-02-13 19:28:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:12][root][INFO] - Training Epoch: 1/2, step 4386/7134 completed (loss: 0.2846527397632599, acc: 0.9368932247161865)
[2025-02-13 19:28:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:12][root][INFO] - Training Epoch: 1/2, step 4387/7134 completed (loss: 0.32558658719062805, acc: 0.9176470637321472)
[2025-02-13 19:28:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:12][root][INFO] - Training Epoch: 1/2, step 4388/7134 completed (loss: 0.457040011882782, acc: 0.9025974273681641)
[2025-02-13 19:28:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:13][root][INFO] - Training Epoch: 1/2, step 4389/7134 completed (loss: 0.45104390382766724, acc: 0.9130434989929199)
[2025-02-13 19:28:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:13][root][INFO] - Training Epoch: 1/2, step 4390/7134 completed (loss: 0.49913835525512695, acc: 0.8895705342292786)
[2025-02-13 19:28:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:13][root][INFO] - Training Epoch: 1/2, step 4391/7134 completed (loss: 0.5252177715301514, acc: 0.8767123222351074)
[2025-02-13 19:28:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:14][root][INFO] - Training Epoch: 1/2, step 4392/7134 completed (loss: 0.18866625428199768, acc: 0.9576719403266907)
[2025-02-13 19:28:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:14][root][INFO] - Training Epoch: 1/2, step 4393/7134 completed (loss: 0.21492840349674225, acc: 0.9399999976158142)
[2025-02-13 19:28:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:15][root][INFO] - Training Epoch: 1/2, step 4394/7134 completed (loss: 0.24799910187721252, acc: 0.9463087320327759)
[2025-02-13 19:28:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:15][root][INFO] - Training Epoch: 1/2, step 4395/7134 completed (loss: 0.3107714354991913, acc: 0.9254658222198486)
[2025-02-13 19:28:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:15][root][INFO] - Training Epoch: 1/2, step 4396/7134 completed (loss: 0.488033264875412, acc: 0.8994709253311157)
[2025-02-13 19:28:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:16][root][INFO] - Training Epoch: 1/2, step 4397/7134 completed (loss: 1.1015671491622925, acc: 0.8108108043670654)
[2025-02-13 19:28:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:16][root][INFO] - Training Epoch: 1/2, step 4398/7134 completed (loss: 0.33379513025283813, acc: 0.9555555582046509)
[2025-02-13 19:28:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:16][root][INFO] - Training Epoch: 1/2, step 4399/7134 completed (loss: 0.2655041217803955, acc: 0.9305555820465088)
[2025-02-13 19:28:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:17][root][INFO] - Training Epoch: 1/2, step 4400/7134 completed (loss: 0.2695920169353485, acc: 0.9631578922271729)
[2025-02-13 19:28:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:17][root][INFO] - Training Epoch: 1/2, step 4401/7134 completed (loss: 0.3196752965450287, acc: 0.9290322661399841)
[2025-02-13 19:28:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:18][root][INFO] - Training Epoch: 1/2, step 4402/7134 completed (loss: 0.6230182647705078, acc: 0.8765432238578796)
[2025-02-13 19:28:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:18][root][INFO] - Training Epoch: 1/2, step 4403/7134 completed (loss: 0.6478314399719238, acc: 0.8636363744735718)
[2025-02-13 19:28:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:18][root][INFO] - Training Epoch: 1/2, step 4404/7134 completed (loss: 0.5054857134819031, acc: 0.8917526006698608)
[2025-02-13 19:28:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:19][root][INFO] - Training Epoch: 1/2, step 4405/7134 completed (loss: 0.2966112494468689, acc: 0.9356435537338257)
[2025-02-13 19:28:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:19][root][INFO] - Training Epoch: 1/2, step 4406/7134 completed (loss: 0.16716250777244568, acc: 0.9609755873680115)
[2025-02-13 19:28:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:20][root][INFO] - Training Epoch: 1/2, step 4407/7134 completed (loss: 0.4005962312221527, acc: 0.9234693646430969)
[2025-02-13 19:28:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:20][root][INFO] - Training Epoch: 1/2, step 4408/7134 completed (loss: 0.4530966877937317, acc: 0.8990384340286255)
[2025-02-13 19:28:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:20][root][INFO] - Training Epoch: 1/2, step 4409/7134 completed (loss: 0.21930640935897827, acc: 0.9583333134651184)
[2025-02-13 19:28:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:21][root][INFO] - Training Epoch: 1/2, step 4410/7134 completed (loss: 0.347421795129776, acc: 0.9160839319229126)
[2025-02-13 19:28:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:21][root][INFO] - Training Epoch: 1/2, step 4411/7134 completed (loss: 0.5118361115455627, acc: 0.897849440574646)
[2025-02-13 19:28:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:22][root][INFO] - Training Epoch: 1/2, step 4412/7134 completed (loss: 0.8561610579490662, acc: 0.8190954923629761)
[2025-02-13 19:28:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:22][root][INFO] - Training Epoch: 1/2, step 4413/7134 completed (loss: 0.540142297744751, acc: 0.8756476640701294)
[2025-02-13 19:28:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:22][root][INFO] - Training Epoch: 1/2, step 4414/7134 completed (loss: 0.28160184621810913, acc: 0.9354838728904724)
[2025-02-13 19:28:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:23][root][INFO] - Training Epoch: 1/2, step 4415/7134 completed (loss: 0.26686763763427734, acc: 0.9269663095474243)
[2025-02-13 19:28:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:23][root][INFO] - Training Epoch: 1/2, step 4416/7134 completed (loss: 0.21976247429847717, acc: 0.9274611473083496)
[2025-02-13 19:28:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:23][root][INFO] - Training Epoch: 1/2, step 4417/7134 completed (loss: 0.46557486057281494, acc: 0.8989899158477783)
[2025-02-13 19:28:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:24][root][INFO] - Training Epoch: 1/2, step 4418/7134 completed (loss: 0.2808477580547333, acc: 0.945652186870575)
[2025-02-13 19:28:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:24][root][INFO] - Training Epoch: 1/2, step 4419/7134 completed (loss: 0.33665162324905396, acc: 0.9121951460838318)
[2025-02-13 19:28:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:25][root][INFO] - Training Epoch: 1/2, step 4420/7134 completed (loss: 0.4111003279685974, acc: 0.8963730335235596)
[2025-02-13 19:28:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:25][root][INFO] - Training Epoch: 1/2, step 4421/7134 completed (loss: 0.2694483697414398, acc: 0.9403669834136963)
[2025-02-13 19:28:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:25][root][INFO] - Training Epoch: 1/2, step 4422/7134 completed (loss: 0.30669599771499634, acc: 0.9041916131973267)
[2025-02-13 19:28:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:26][root][INFO] - Training Epoch: 1/2, step 4423/7134 completed (loss: 0.2616730332374573, acc: 0.9553571343421936)
[2025-02-13 19:28:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:26][root][INFO] - Training Epoch: 1/2, step 4424/7134 completed (loss: 0.1657341867685318, acc: 0.9659090638160706)
[2025-02-13 19:28:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:27][root][INFO] - Training Epoch: 1/2, step 4425/7134 completed (loss: 0.10316256433725357, acc: 0.9702970385551453)
[2025-02-13 19:28:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:27][root][INFO] - Training Epoch: 1/2, step 4426/7134 completed (loss: 0.30505210161209106, acc: 0.9279999732971191)
[2025-02-13 19:28:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:27][root][INFO] - Training Epoch: 1/2, step 4427/7134 completed (loss: 0.6190149784088135, acc: 0.8793103694915771)
[2025-02-13 19:28:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:28][root][INFO] - Training Epoch: 1/2, step 4428/7134 completed (loss: 0.17428286373615265, acc: 0.9537572264671326)
[2025-02-13 19:28:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:28][root][INFO] - Training Epoch: 1/2, step 4429/7134 completed (loss: 0.3860979378223419, acc: 0.8952381014823914)
[2025-02-13 19:28:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:28][root][INFO] - Training Epoch: 1/2, step 4430/7134 completed (loss: 0.343136191368103, acc: 0.915730357170105)
[2025-02-13 19:28:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:29][root][INFO] - Training Epoch: 1/2, step 4431/7134 completed (loss: 0.2682516574859619, acc: 0.9476743936538696)
[2025-02-13 19:28:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:29][root][INFO] - Training Epoch: 1/2, step 4432/7134 completed (loss: 0.15353231132030487, acc: 0.9554139971733093)
[2025-02-13 19:28:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:30][root][INFO] - Training Epoch: 1/2, step 4433/7134 completed (loss: 0.2092771828174591, acc: 0.9428571462631226)
[2025-02-13 19:28:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:30][root][INFO] - Training Epoch: 1/2, step 4434/7134 completed (loss: 0.33315563201904297, acc: 0.9200000166893005)
[2025-02-13 19:28:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:30][root][INFO] - Training Epoch: 1/2, step 4435/7134 completed (loss: 0.2051641345024109, acc: 0.9583333134651184)
[2025-02-13 19:28:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:31][root][INFO] - Training Epoch: 1/2, step 4436/7134 completed (loss: 0.25689443945884705, acc: 0.9289940595626831)
[2025-02-13 19:28:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:31][root][INFO] - Training Epoch: 1/2, step 4437/7134 completed (loss: 0.0732584297657013, acc: 0.9890710115432739)
[2025-02-13 19:28:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:32][root][INFO] - Training Epoch: 1/2, step 4438/7134 completed (loss: 0.22270376980304718, acc: 0.9776119589805603)
[2025-02-13 19:28:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:32][root][INFO] - Training Epoch: 1/2, step 4439/7134 completed (loss: 0.06327351182699203, acc: 0.978723406791687)
[2025-02-13 19:28:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:32][root][INFO] - Training Epoch: 1/2, step 4440/7134 completed (loss: 0.1563967764377594, acc: 0.9578313231468201)
[2025-02-13 19:28:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:33][root][INFO] - Training Epoch: 1/2, step 4441/7134 completed (loss: 0.11970820277929306, acc: 0.9622641801834106)
[2025-02-13 19:28:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:33][root][INFO] - Training Epoch: 1/2, step 4442/7134 completed (loss: 0.3295057415962219, acc: 0.9382022619247437)
[2025-02-13 19:28:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:34][root][INFO] - Training Epoch: 1/2, step 4443/7134 completed (loss: 0.27465707063674927, acc: 0.9352940917015076)
[2025-02-13 19:28:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:34][root][INFO] - Training Epoch: 1/2, step 4444/7134 completed (loss: 0.18166568875312805, acc: 0.9416058659553528)
[2025-02-13 19:28:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:34][root][INFO] - Training Epoch: 1/2, step 4445/7134 completed (loss: 0.4496293365955353, acc: 0.8875739574432373)
[2025-02-13 19:28:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:35][root][INFO] - Training Epoch: 1/2, step 4446/7134 completed (loss: 0.12368224561214447, acc: 0.9767441749572754)
[2025-02-13 19:28:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:35][root][INFO] - Training Epoch: 1/2, step 4447/7134 completed (loss: 0.11315788328647614, acc: 0.9712643623352051)
[2025-02-13 19:28:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:36][root][INFO] - Training Epoch: 1/2, step 4448/7134 completed (loss: 0.33635368943214417, acc: 0.9457364082336426)
[2025-02-13 19:28:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:36][root][INFO] - Training Epoch: 1/2, step 4449/7134 completed (loss: 0.2813176214694977, acc: 0.9406779408454895)
[2025-02-13 19:28:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:36][root][INFO] - Training Epoch: 1/2, step 4450/7134 completed (loss: 0.1765655130147934, acc: 0.9414893388748169)
[2025-02-13 19:28:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:37][root][INFO] - Training Epoch: 1/2, step 4451/7134 completed (loss: 0.4715941548347473, acc: 0.9108280539512634)
[2025-02-13 19:28:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:37][root][INFO] - Training Epoch: 1/2, step 4452/7134 completed (loss: 0.26079633831977844, acc: 0.9481481313705444)
[2025-02-13 19:28:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:37][root][INFO] - Training Epoch: 1/2, step 4453/7134 completed (loss: 0.1924888789653778, acc: 0.9477124214172363)
[2025-02-13 19:28:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:38][root][INFO] - Training Epoch: 1/2, step 4454/7134 completed (loss: 0.4897446632385254, acc: 0.9192546606063843)
[2025-02-13 19:28:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:38][root][INFO] - Training Epoch: 1/2, step 4455/7134 completed (loss: 0.34006616473197937, acc: 0.9629629850387573)
[2025-02-13 19:28:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:39][root][INFO] - Training Epoch: 1/2, step 4456/7134 completed (loss: 0.08148514479398727, acc: 0.9818181991577148)
[2025-02-13 19:28:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:39][root][INFO] - Training Epoch: 1/2, step 4457/7134 completed (loss: 0.057902026921510696, acc: 0.9870129823684692)
[2025-02-13 19:28:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:39][root][INFO] - Training Epoch: 1/2, step 4458/7134 completed (loss: 0.06686394661664963, acc: 0.9860140085220337)
[2025-02-13 19:28:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:40][root][INFO] - Training Epoch: 1/2, step 4459/7134 completed (loss: 0.15599201619625092, acc: 0.9651162624359131)
[2025-02-13 19:28:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:40][root][INFO] - Training Epoch: 1/2, step 4460/7134 completed (loss: 0.1516374796628952, acc: 0.9704433679580688)
[2025-02-13 19:28:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:41][root][INFO] - Training Epoch: 1/2, step 4461/7134 completed (loss: 0.15307192504405975, acc: 0.9635416865348816)
[2025-02-13 19:28:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:41][root][INFO] - Training Epoch: 1/2, step 4462/7134 completed (loss: 0.14125683903694153, acc: 0.954285740852356)
[2025-02-13 19:28:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:41][root][INFO] - Training Epoch: 1/2, step 4463/7134 completed (loss: 0.08891142904758453, acc: 0.9735449552536011)
[2025-02-13 19:28:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:42][root][INFO] - Training Epoch: 1/2, step 4464/7134 completed (loss: 0.18922674655914307, acc: 0.9383561611175537)
[2025-02-13 19:28:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:42][root][INFO] - Training Epoch: 1/2, step 4465/7134 completed (loss: 0.12866869568824768, acc: 0.9595959782600403)
[2025-02-13 19:28:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:43][root][INFO] - Training Epoch: 1/2, step 4466/7134 completed (loss: 0.13002179563045502, acc: 0.9646464586257935)
[2025-02-13 19:28:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:43][root][INFO] - Training Epoch: 1/2, step 4467/7134 completed (loss: 0.10353057831525803, acc: 0.9732620120048523)
[2025-02-13 19:28:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:43][root][INFO] - Training Epoch: 1/2, step 4468/7134 completed (loss: 0.07883314043283463, acc: 0.9893048405647278)
[2025-02-13 19:28:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:44][root][INFO] - Training Epoch: 1/2, step 4469/7134 completed (loss: 0.12026955187320709, acc: 0.9702970385551453)
[2025-02-13 19:28:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:44][root][INFO] - Training Epoch: 1/2, step 4470/7134 completed (loss: 0.06573016941547394, acc: 0.9903846383094788)
[2025-02-13 19:28:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:45][root][INFO] - Training Epoch: 1/2, step 4471/7134 completed (loss: 0.05881034955382347, acc: 0.9898989796638489)
[2025-02-13 19:28:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:45][root][INFO] - Training Epoch: 1/2, step 4472/7134 completed (loss: 0.07156344503164291, acc: 0.9833333492279053)
[2025-02-13 19:28:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:45][root][INFO] - Training Epoch: 1/2, step 4473/7134 completed (loss: 0.2116374373435974, acc: 0.9467455744743347)
[2025-02-13 19:28:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:46][root][INFO] - Training Epoch: 1/2, step 4474/7134 completed (loss: 0.22778289020061493, acc: 0.9440993666648865)
[2025-02-13 19:28:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:46][root][INFO] - Training Epoch: 1/2, step 4475/7134 completed (loss: 0.35898858308792114, acc: 0.8989361524581909)
[2025-02-13 19:28:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:46][root][INFO] - Training Epoch: 1/2, step 4476/7134 completed (loss: 0.34122100472450256, acc: 0.9285714030265808)
[2025-02-13 19:28:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:47][root][INFO] - Training Epoch: 1/2, step 4477/7134 completed (loss: 0.07519363611936569, acc: 0.9897959232330322)
[2025-02-13 19:28:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:47][root][INFO] - Training Epoch: 1/2, step 4478/7134 completed (loss: 0.13090725243091583, acc: 0.9581151604652405)
[2025-02-13 19:28:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:48][root][INFO] - Training Epoch: 1/2, step 4479/7134 completed (loss: 0.15549036860466003, acc: 0.9666666388511658)
[2025-02-13 19:28:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:48][root][INFO] - Training Epoch: 1/2, step 4480/7134 completed (loss: 0.235243558883667, acc: 0.9504950642585754)
[2025-02-13 19:28:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:48][root][INFO] - Training Epoch: 1/2, step 4481/7134 completed (loss: 0.1472427397966385, acc: 0.9750000238418579)
[2025-02-13 19:28:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:49][root][INFO] - Training Epoch: 1/2, step 4482/7134 completed (loss: 0.09623714536428452, acc: 0.9801980257034302)
[2025-02-13 19:28:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:49][root][INFO] - Training Epoch: 1/2, step 4483/7134 completed (loss: 0.26872843503952026, acc: 0.9192546606063843)
[2025-02-13 19:28:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:50][root][INFO] - Training Epoch: 1/2, step 4484/7134 completed (loss: 0.16645583510398865, acc: 0.9488636255264282)
[2025-02-13 19:28:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:50][root][INFO] - Training Epoch: 1/2, step 4485/7134 completed (loss: 0.050793759524822235, acc: 0.9934640526771545)
[2025-02-13 19:28:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:50][root][INFO] - Training Epoch: 1/2, step 4486/7134 completed (loss: 0.09656523168087006, acc: 0.9627329111099243)
[2025-02-13 19:28:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:51][root][INFO] - Training Epoch: 1/2, step 4487/7134 completed (loss: 0.09186962991952896, acc: 0.9815950989723206)
[2025-02-13 19:28:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:51][root][INFO] - Training Epoch: 1/2, step 4488/7134 completed (loss: 0.16669684648513794, acc: 0.961240291595459)
[2025-02-13 19:28:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:51][root][INFO] - Training Epoch: 1/2, step 4489/7134 completed (loss: 0.06557115167379379, acc: 0.9793103337287903)
[2025-02-13 19:28:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:52][root][INFO] - Training Epoch: 1/2, step 4490/7134 completed (loss: 0.13689205050468445, acc: 0.9679999947547913)
[2025-02-13 19:28:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:52][root][INFO] - Training Epoch: 1/2, step 4491/7134 completed (loss: 0.18708065152168274, acc: 0.9694656729698181)
[2025-02-13 19:28:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:53][root][INFO] - Training Epoch: 1/2, step 4492/7134 completed (loss: 0.13523873686790466, acc: 0.969924807548523)
[2025-02-13 19:28:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:53][root][INFO] - Training Epoch: 1/2, step 4493/7134 completed (loss: 0.30452635884284973, acc: 0.9418604373931885)
[2025-02-13 19:28:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:53][root][INFO] - Training Epoch: 1/2, step 4494/7134 completed (loss: 0.13493353128433228, acc: 0.9719101190567017)
[2025-02-13 19:28:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:54][root][INFO] - Training Epoch: 1/2, step 4495/7134 completed (loss: 0.09135060757398605, acc: 0.9793814420700073)
[2025-02-13 19:28:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:54][root][INFO] - Training Epoch: 1/2, step 4496/7134 completed (loss: 0.07293828576803207, acc: 0.9824561476707458)
[2025-02-13 19:28:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:55][root][INFO] - Training Epoch: 1/2, step 4497/7134 completed (loss: 0.059661343693733215, acc: 0.9788359999656677)
[2025-02-13 19:28:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:55][root][INFO] - Training Epoch: 1/2, step 4498/7134 completed (loss: 0.05706406012177467, acc: 0.9884393215179443)
[2025-02-13 19:28:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:55][root][INFO] - Training Epoch: 1/2, step 4499/7134 completed (loss: 0.09516502916812897, acc: 0.978723406791687)
[2025-02-13 19:28:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:56][root][INFO] - Training Epoch: 1/2, step 4500/7134 completed (loss: 0.0837993249297142, acc: 0.9839572310447693)
[2025-02-13 19:28:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:56][root][INFO] - Training Epoch: 1/2, step 4501/7134 completed (loss: 0.05291169136762619, acc: 0.988950252532959)
[2025-02-13 19:28:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:56][root][INFO] - Training Epoch: 1/2, step 4502/7134 completed (loss: 0.05891093239188194, acc: 0.9941176176071167)
[2025-02-13 19:28:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:57][root][INFO] - Training Epoch: 1/2, step 4503/7134 completed (loss: 0.041087497025728226, acc: 0.9937106966972351)
[2025-02-13 19:28:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:57][root][INFO] - Training Epoch: 1/2, step 4504/7134 completed (loss: 0.24548499286174774, acc: 0.9316770434379578)
[2025-02-13 19:28:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:58][root][INFO] - Training Epoch: 1/2, step 4505/7134 completed (loss: 0.20328418910503387, acc: 0.9513513445854187)
[2025-02-13 19:28:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:58][root][INFO] - Training Epoch: 1/2, step 4506/7134 completed (loss: 0.19002000987529755, acc: 0.9447852969169617)
[2025-02-13 19:28:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:58][root][INFO] - Training Epoch: 1/2, step 4507/7134 completed (loss: 0.09133721888065338, acc: 0.9935064911842346)
[2025-02-13 19:28:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:59][root][INFO] - Training Epoch: 1/2, step 4508/7134 completed (loss: 0.1814240664243698, acc: 0.9503105878829956)
[2025-02-13 19:28:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:28:59][root][INFO] - Training Epoch: 1/2, step 4509/7134 completed (loss: 0.15717241168022156, acc: 0.9575757384300232)
[2025-02-13 19:28:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:00][root][INFO] - Training Epoch: 1/2, step 4510/7134 completed (loss: 0.28807276487350464, acc: 0.9226190447807312)
[2025-02-13 19:29:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:00][root][INFO] - Training Epoch: 1/2, step 4511/7134 completed (loss: 0.2770652174949646, acc: 0.9318181872367859)
[2025-02-13 19:29:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:00][root][INFO] - Training Epoch: 1/2, step 4512/7134 completed (loss: 0.36774885654449463, acc: 0.8888888955116272)
[2025-02-13 19:29:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:01][root][INFO] - Training Epoch: 1/2, step 4513/7134 completed (loss: 0.20963755249977112, acc: 0.9473684430122375)
[2025-02-13 19:29:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:01][root][INFO] - Training Epoch: 1/2, step 4514/7134 completed (loss: 0.3879024386405945, acc: 0.9248554706573486)
[2025-02-13 19:29:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:01][root][INFO] - Training Epoch: 1/2, step 4515/7134 completed (loss: 0.2567618191242218, acc: 0.9407894611358643)
[2025-02-13 19:29:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:02][root][INFO] - Training Epoch: 1/2, step 4516/7134 completed (loss: 0.17269957065582275, acc: 0.9572192430496216)
[2025-02-13 19:29:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:02][root][INFO] - Training Epoch: 1/2, step 4517/7134 completed (loss: 0.2130034863948822, acc: 0.9350000023841858)
[2025-02-13 19:29:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:02][root][INFO] - Training Epoch: 1/2, step 4518/7134 completed (loss: 0.16220761835575104, acc: 0.9639175534248352)
[2025-02-13 19:29:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:03][root][INFO] - Training Epoch: 1/2, step 4519/7134 completed (loss: 0.13658301532268524, acc: 0.96517413854599)
[2025-02-13 19:29:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:03][root][INFO] - Training Epoch: 1/2, step 4520/7134 completed (loss: 0.12789444625377655, acc: 0.9679144620895386)
[2025-02-13 19:29:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:04][root][INFO] - Training Epoch: 1/2, step 4521/7134 completed (loss: 0.22207212448120117, acc: 0.9449999928474426)
[2025-02-13 19:29:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:04][root][INFO] - Training Epoch: 1/2, step 4522/7134 completed (loss: 0.16376131772994995, acc: 0.9620853066444397)
[2025-02-13 19:29:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:04][root][INFO] - Training Epoch: 1/2, step 4523/7134 completed (loss: 0.09335105866193771, acc: 0.9718309640884399)
[2025-02-13 19:29:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:05][root][INFO] - Training Epoch: 1/2, step 4524/7134 completed (loss: 0.17832373082637787, acc: 0.9528796076774597)
[2025-02-13 19:29:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:05][root][INFO] - Training Epoch: 1/2, step 4525/7134 completed (loss: 0.23484353721141815, acc: 0.935960590839386)
[2025-02-13 19:29:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:06][root][INFO] - Training Epoch: 1/2, step 4526/7134 completed (loss: 0.2786993384361267, acc: 0.9340659379959106)
[2025-02-13 19:29:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:06][root][INFO] - Training Epoch: 1/2, step 4527/7134 completed (loss: 0.22436630725860596, acc: 0.9641255736351013)
[2025-02-13 19:29:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:06][root][INFO] - Training Epoch: 1/2, step 4528/7134 completed (loss: 0.2608798146247864, acc: 0.9408866763114929)
[2025-02-13 19:29:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:07][root][INFO] - Training Epoch: 1/2, step 4529/7134 completed (loss: 0.2746627926826477, acc: 0.9319371581077576)
[2025-02-13 19:29:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:07][root][INFO] - Training Epoch: 1/2, step 4530/7134 completed (loss: 0.10704915225505829, acc: 0.9756097793579102)
[2025-02-13 19:29:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:08][root][INFO] - Training Epoch: 1/2, step 4531/7134 completed (loss: 0.09800969064235687, acc: 0.9609375)
[2025-02-13 19:29:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:08][root][INFO] - Training Epoch: 1/2, step 4532/7134 completed (loss: 0.1854490488767624, acc: 0.9620253443717957)
[2025-02-13 19:29:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:08][root][INFO] - Training Epoch: 1/2, step 4533/7134 completed (loss: 0.08497127890586853, acc: 0.9861111044883728)
[2025-02-13 19:29:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:09][root][INFO] - Training Epoch: 1/2, step 4534/7134 completed (loss: 0.1335059553384781, acc: 0.9644669890403748)
[2025-02-13 19:29:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:09][root][INFO] - Training Epoch: 1/2, step 4535/7134 completed (loss: 0.10954582691192627, acc: 0.9807692170143127)
[2025-02-13 19:29:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:10][root][INFO] - Training Epoch: 1/2, step 4536/7134 completed (loss: 0.0836290717124939, acc: 0.9799196720123291)
[2025-02-13 19:29:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:10][root][INFO] - Training Epoch: 1/2, step 4537/7134 completed (loss: 0.06914078444242477, acc: 0.9728506803512573)
[2025-02-13 19:29:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:10][root][INFO] - Training Epoch: 1/2, step 4538/7134 completed (loss: 0.16208453476428986, acc: 0.966183602809906)
[2025-02-13 19:29:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:11][root][INFO] - Training Epoch: 1/2, step 4539/7134 completed (loss: 0.16248348355293274, acc: 0.9671361446380615)
[2025-02-13 19:29:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:11][root][INFO] - Training Epoch: 1/2, step 4540/7134 completed (loss: 0.2270832061767578, acc: 0.9404761791229248)
[2025-02-13 19:29:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:11][root][INFO] - Training Epoch: 1/2, step 4541/7134 completed (loss: 0.27461767196655273, acc: 0.9426229596138)
[2025-02-13 19:29:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:12][root][INFO] - Training Epoch: 1/2, step 4542/7134 completed (loss: 0.33304470777511597, acc: 0.9035087823867798)
[2025-02-13 19:29:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:12][root][INFO] - Training Epoch: 1/2, step 4543/7134 completed (loss: 0.44914737343788147, acc: 0.8896104097366333)
[2025-02-13 19:29:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:13][root][INFO] - Training Epoch: 1/2, step 4544/7134 completed (loss: 0.1985005885362625, acc: 0.9391891956329346)
[2025-02-13 19:29:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:13][root][INFO] - Training Epoch: 1/2, step 4545/7134 completed (loss: 0.16596783697605133, acc: 0.9351851940155029)
[2025-02-13 19:29:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:13][root][INFO] - Training Epoch: 1/2, step 4546/7134 completed (loss: 0.20134200155735016, acc: 0.9685039520263672)
[2025-02-13 19:29:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:14][root][INFO] - Training Epoch: 1/2, step 4547/7134 completed (loss: 0.21975865960121155, acc: 0.9237288236618042)
[2025-02-13 19:29:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:14][root][INFO] - Training Epoch: 1/2, step 4548/7134 completed (loss: 0.2231599986553192, acc: 0.960629940032959)
[2025-02-13 19:29:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:15][root][INFO] - Training Epoch: 1/2, step 4549/7134 completed (loss: 0.3418143391609192, acc: 0.9210526347160339)
[2025-02-13 19:29:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:15][root][INFO] - Training Epoch: 1/2, step 4550/7134 completed (loss: 0.3220187723636627, acc: 0.9473684430122375)
[2025-02-13 19:29:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:15][root][INFO] - Training Epoch: 1/2, step 4551/7134 completed (loss: 0.2702867090702057, acc: 0.9459459185600281)
[2025-02-13 19:29:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:16][root][INFO] - Training Epoch: 1/2, step 4552/7134 completed (loss: 0.1688200682401657, acc: 0.9473684430122375)
[2025-02-13 19:29:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:16][root][INFO] - Training Epoch: 1/2, step 4553/7134 completed (loss: 0.16001656651496887, acc: 0.9545454382896423)
[2025-02-13 19:29:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:17][root][INFO] - Training Epoch: 1/2, step 4554/7134 completed (loss: 0.21125130355358124, acc: 0.9343065619468689)
[2025-02-13 19:29:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:17][root][INFO] - Training Epoch: 1/2, step 4555/7134 completed (loss: 0.30803945660591125, acc: 0.9270833134651184)
[2025-02-13 19:29:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:17][root][INFO] - Training Epoch: 1/2, step 4556/7134 completed (loss: 0.4004627764225006, acc: 0.9032257795333862)
[2025-02-13 19:29:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:18][root][INFO] - Training Epoch: 1/2, step 4557/7134 completed (loss: 0.13687506318092346, acc: 0.9552238583564758)
[2025-02-13 19:29:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:18][root][INFO] - Training Epoch: 1/2, step 4558/7134 completed (loss: 0.22899985313415527, acc: 0.9457364082336426)
[2025-02-13 19:29:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:19][root][INFO] - Training Epoch: 1/2, step 4559/7134 completed (loss: 0.33522069454193115, acc: 0.9318181872367859)
[2025-02-13 19:29:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:19][root][INFO] - Training Epoch: 1/2, step 4560/7134 completed (loss: 0.18504598736763, acc: 0.9354838728904724)
[2025-02-13 19:29:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:19][root][INFO] - Training Epoch: 1/2, step 4561/7134 completed (loss: 0.1982022374868393, acc: 0.9508196711540222)
[2025-02-13 19:29:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:20][root][INFO] - Training Epoch: 1/2, step 4562/7134 completed (loss: 0.25427567958831787, acc: 0.934959352016449)
[2025-02-13 19:29:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:20][root][INFO] - Training Epoch: 1/2, step 4563/7134 completed (loss: 0.15832248330116272, acc: 0.957446813583374)
[2025-02-13 19:29:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:20][root][INFO] - Training Epoch: 1/2, step 4564/7134 completed (loss: 0.29954054951667786, acc: 0.948051929473877)
[2025-02-13 19:29:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:21][root][INFO] - Training Epoch: 1/2, step 4565/7134 completed (loss: 0.2508920729160309, acc: 0.9380530714988708)
[2025-02-13 19:29:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:21][root][INFO] - Training Epoch: 1/2, step 4566/7134 completed (loss: 0.2595829665660858, acc: 0.932330846786499)
[2025-02-13 19:29:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:22][root][INFO] - Training Epoch: 1/2, step 4567/7134 completed (loss: 0.055218808352947235, acc: 0.9909909963607788)
[2025-02-13 19:29:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:22][root][INFO] - Training Epoch: 1/2, step 4568/7134 completed (loss: 0.3182615041732788, acc: 0.9263157844543457)
[2025-02-13 19:29:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:22][root][INFO] - Training Epoch: 1/2, step 4569/7134 completed (loss: 0.4148119390010834, acc: 0.9380530714988708)
[2025-02-13 19:29:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:23][root][INFO] - Training Epoch: 1/2, step 4570/7134 completed (loss: 0.1871803104877472, acc: 0.9545454382896423)
[2025-02-13 19:29:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:23][root][INFO] - Training Epoch: 1/2, step 4571/7134 completed (loss: 0.1943848431110382, acc: 0.9482758641242981)
[2025-02-13 19:29:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:23][root][INFO] - Training Epoch: 1/2, step 4572/7134 completed (loss: 0.17579704523086548, acc: 0.9407894611358643)
[2025-02-13 19:29:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:24][root][INFO] - Training Epoch: 1/2, step 4573/7134 completed (loss: 0.16412286460399628, acc: 0.9615384340286255)
[2025-02-13 19:29:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:24][root][INFO] - Training Epoch: 1/2, step 4574/7134 completed (loss: 0.08875724673271179, acc: 0.9696969985961914)
[2025-02-13 19:29:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:25][root][INFO] - Training Epoch: 1/2, step 4575/7134 completed (loss: 0.11390108615159988, acc: 0.9723756909370422)
[2025-02-13 19:29:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:25][root][INFO] - Training Epoch: 1/2, step 4576/7134 completed (loss: 0.08098097890615463, acc: 0.9885714054107666)
[2025-02-13 19:29:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:25][root][INFO] - Training Epoch: 1/2, step 4577/7134 completed (loss: 0.23486514389514923, acc: 0.9418604373931885)
[2025-02-13 19:29:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:26][root][INFO] - Training Epoch: 1/2, step 4578/7134 completed (loss: 0.16896580159664154, acc: 0.939393937587738)
[2025-02-13 19:29:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:26][root][INFO] - Training Epoch: 1/2, step 4579/7134 completed (loss: 0.22107045352458954, acc: 0.9620253443717957)
[2025-02-13 19:29:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:27][root][INFO] - Training Epoch: 1/2, step 4580/7134 completed (loss: 0.10912029445171356, acc: 0.9810126423835754)
[2025-02-13 19:29:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:27][root][INFO] - Training Epoch: 1/2, step 4581/7134 completed (loss: 0.0674515962600708, acc: 0.9790209531784058)
[2025-02-13 19:29:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:27][root][INFO] - Training Epoch: 1/2, step 4582/7134 completed (loss: 0.1328536868095398, acc: 0.9722222089767456)
[2025-02-13 19:29:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:28][root][INFO] - Training Epoch: 1/2, step 4583/7134 completed (loss: 0.13195887207984924, acc: 0.9466666579246521)
[2025-02-13 19:29:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:28][root][INFO] - Training Epoch: 1/2, step 4584/7134 completed (loss: 0.10929828882217407, acc: 0.9724137783050537)
[2025-02-13 19:29:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:28][root][INFO] - Training Epoch: 1/2, step 4585/7134 completed (loss: 0.033059075474739075, acc: 0.9935897588729858)
[2025-02-13 19:29:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:29][root][INFO] - Training Epoch: 1/2, step 4586/7134 completed (loss: 0.17513802647590637, acc: 0.9526627063751221)
[2025-02-13 19:29:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:29][root][INFO] - Training Epoch: 1/2, step 4587/7134 completed (loss: 0.3403358459472656, acc: 0.9418604373931885)
[2025-02-13 19:29:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:30][root][INFO] - Training Epoch: 1/2, step 4588/7134 completed (loss: 0.17470550537109375, acc: 0.956250011920929)
[2025-02-13 19:29:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:30][root][INFO] - Training Epoch: 1/2, step 4589/7134 completed (loss: 0.21621616184711456, acc: 0.9195402264595032)
[2025-02-13 19:29:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:30][root][INFO] - Training Epoch: 1/2, step 4590/7134 completed (loss: 0.20981791615486145, acc: 0.9464285969734192)
[2025-02-13 19:29:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:31][root][INFO] - Training Epoch: 1/2, step 4591/7134 completed (loss: 0.2326303869485855, acc: 0.9375)
[2025-02-13 19:29:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:31][root][INFO] - Training Epoch: 1/2, step 4592/7134 completed (loss: 0.23865725100040436, acc: 0.9390243887901306)
[2025-02-13 19:29:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:32][root][INFO] - Training Epoch: 1/2, step 4593/7134 completed (loss: 0.4310286045074463, acc: 0.8947368264198303)
[2025-02-13 19:29:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:32][root][INFO] - Training Epoch: 1/2, step 4594/7134 completed (loss: 0.27036190032958984, acc: 0.920634925365448)
[2025-02-13 19:29:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:32][root][INFO] - Training Epoch: 1/2, step 4595/7134 completed (loss: 0.1746223121881485, acc: 0.9556962251663208)
[2025-02-13 19:29:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:33][root][INFO] - Training Epoch: 1/2, step 4596/7134 completed (loss: 0.21672388911247253, acc: 0.9437500238418579)
[2025-02-13 19:29:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:33][root][INFO] - Training Epoch: 1/2, step 4597/7134 completed (loss: 0.16475434601306915, acc: 0.9487179517745972)
[2025-02-13 19:29:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:34][root][INFO] - Training Epoch: 1/2, step 4598/7134 completed (loss: 0.35560616850852966, acc: 0.9135135412216187)
[2025-02-13 19:29:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:34][root][INFO] - Training Epoch: 1/2, step 4599/7134 completed (loss: 0.09753711521625519, acc: 0.97826087474823)
[2025-02-13 19:29:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:34][root][INFO] - Training Epoch: 1/2, step 4600/7134 completed (loss: 0.23750115931034088, acc: 0.9371069073677063)
[2025-02-13 19:29:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:35][root][INFO] - Training Epoch: 1/2, step 4601/7134 completed (loss: 0.14491957426071167, acc: 0.9604519605636597)
[2025-02-13 19:29:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:35][root][INFO] - Training Epoch: 1/2, step 4602/7134 completed (loss: 0.1961887776851654, acc: 0.949999988079071)
[2025-02-13 19:29:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:36][root][INFO] - Training Epoch: 1/2, step 4603/7134 completed (loss: 0.2397920936346054, acc: 0.9476743936538696)
[2025-02-13 19:29:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:36][root][INFO] - Training Epoch: 1/2, step 4604/7134 completed (loss: 0.20056940615177155, acc: 0.9370629191398621)
[2025-02-13 19:29:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:36][root][INFO] - Training Epoch: 1/2, step 4605/7134 completed (loss: 0.19507871568202972, acc: 0.9454545378684998)
[2025-02-13 19:29:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:37][root][INFO] - Training Epoch: 1/2, step 4606/7134 completed (loss: 0.1703653186559677, acc: 0.9685534834861755)
[2025-02-13 19:29:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:37][root][INFO] - Training Epoch: 1/2, step 4607/7134 completed (loss: 0.1900988519191742, acc: 0.9432623982429504)
[2025-02-13 19:29:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:37][root][INFO] - Training Epoch: 1/2, step 4608/7134 completed (loss: 0.1519654244184494, acc: 0.9649122953414917)
[2025-02-13 19:29:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:38][root][INFO] - Training Epoch: 1/2, step 4609/7134 completed (loss: 0.22319285571575165, acc: 0.9389312863349915)
[2025-02-13 19:29:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:38][root][INFO] - Training Epoch: 1/2, step 4610/7134 completed (loss: 0.0742335170507431, acc: 0.9832402467727661)
[2025-02-13 19:29:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:39][root][INFO] - Training Epoch: 1/2, step 4611/7134 completed (loss: 0.1000288799405098, acc: 0.9724137783050537)
[2025-02-13 19:29:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:39][root][INFO] - Training Epoch: 1/2, step 4612/7134 completed (loss: 0.17046073079109192, acc: 0.9555555582046509)
[2025-02-13 19:29:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:39][root][INFO] - Training Epoch: 1/2, step 4613/7134 completed (loss: 0.14289651811122894, acc: 0.9814814925193787)
[2025-02-13 19:29:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:40][root][INFO] - Training Epoch: 1/2, step 4614/7134 completed (loss: 0.14273233711719513, acc: 0.9538461565971375)
[2025-02-13 19:29:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:40][root][INFO] - Training Epoch: 1/2, step 4615/7134 completed (loss: 0.3274437189102173, acc: 0.910179615020752)
[2025-02-13 19:29:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:40][root][INFO] - Training Epoch: 1/2, step 4616/7134 completed (loss: 0.27889809012413025, acc: 0.9186046719551086)
[2025-02-13 19:29:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:41][root][INFO] - Training Epoch: 1/2, step 4617/7134 completed (loss: 0.37175774574279785, acc: 0.9341317415237427)
[2025-02-13 19:29:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:41][root][INFO] - Training Epoch: 1/2, step 4618/7134 completed (loss: 0.3325962722301483, acc: 0.8914728760719299)
[2025-02-13 19:29:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:42][root][INFO] - Training Epoch: 1/2, step 4619/7134 completed (loss: 0.3367140591144562, acc: 0.9375)
[2025-02-13 19:29:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:42][root][INFO] - Training Epoch: 1/2, step 4620/7134 completed (loss: 0.09318535774946213, acc: 0.9647058844566345)
[2025-02-13 19:29:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:42][root][INFO] - Training Epoch: 1/2, step 4621/7134 completed (loss: 0.14277805387973785, acc: 0.9817073345184326)
[2025-02-13 19:29:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:43][root][INFO] - Training Epoch: 1/2, step 4622/7134 completed (loss: 0.1205366849899292, acc: 0.9666666388511658)
[2025-02-13 19:29:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:43][root][INFO] - Training Epoch: 1/2, step 4623/7134 completed (loss: 0.20566533505916595, acc: 0.9529411792755127)
[2025-02-13 19:29:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:44][root][INFO] - Training Epoch: 1/2, step 4624/7134 completed (loss: 0.3478150963783264, acc: 0.9352940917015076)
[2025-02-13 19:29:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:44][root][INFO] - Training Epoch: 1/2, step 4625/7134 completed (loss: 0.5174468159675598, acc: 0.8466257452964783)
[2025-02-13 19:29:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:44][root][INFO] - Training Epoch: 1/2, step 4626/7134 completed (loss: 0.4022173583507538, acc: 0.9021739363670349)
[2025-02-13 19:29:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:45][root][INFO] - Training Epoch: 1/2, step 4627/7134 completed (loss: 0.336069256067276, acc: 0.9419354796409607)
[2025-02-13 19:29:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:45][root][INFO] - Training Epoch: 1/2, step 4628/7134 completed (loss: 0.2873832583427429, acc: 0.9329268336296082)
[2025-02-13 19:29:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:46][root][INFO] - Training Epoch: 1/2, step 4629/7134 completed (loss: 0.21118497848510742, acc: 0.9350649118423462)
[2025-02-13 19:29:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:46][root][INFO] - Training Epoch: 1/2, step 4630/7134 completed (loss: 0.19851286709308624, acc: 0.9555555582046509)
[2025-02-13 19:29:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:47][root][INFO] - Training Epoch: 1/2, step 4631/7134 completed (loss: 0.19005168974399567, acc: 0.9674418568611145)
[2025-02-13 19:29:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:47][root][INFO] - Training Epoch: 1/2, step 4632/7134 completed (loss: 0.2384936511516571, acc: 0.9457831382751465)
[2025-02-13 19:29:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:47][root][INFO] - Training Epoch: 1/2, step 4633/7134 completed (loss: 0.32960274815559387, acc: 0.9086757898330688)
[2025-02-13 19:29:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:48][root][INFO] - Training Epoch: 1/2, step 4634/7134 completed (loss: 0.15376128256320953, acc: 0.970370352268219)
[2025-02-13 19:29:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:48][root][INFO] - Training Epoch: 1/2, step 4635/7134 completed (loss: 0.2674013674259186, acc: 0.9634146094322205)
[2025-02-13 19:29:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:48][root][INFO] - Training Epoch: 1/2, step 4636/7134 completed (loss: 0.062042880803346634, acc: 0.9893617033958435)
[2025-02-13 19:29:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:49][root][INFO] - Training Epoch: 1/2, step 4637/7134 completed (loss: 0.22650806605815887, acc: 0.9603174328804016)
[2025-02-13 19:29:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:49][root][INFO] - Training Epoch: 1/2, step 4638/7134 completed (loss: 0.2623903751373291, acc: 0.9653179049491882)
[2025-02-13 19:29:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:50][root][INFO] - Training Epoch: 1/2, step 4639/7134 completed (loss: 0.18610689043998718, acc: 0.9621621370315552)
[2025-02-13 19:29:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:50][root][INFO] - Training Epoch: 1/2, step 4640/7134 completed (loss: 0.24700027704238892, acc: 0.9365079402923584)
[2025-02-13 19:29:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:50][root][INFO] - Training Epoch: 1/2, step 4641/7134 completed (loss: 0.11995482444763184, acc: 0.9722222089767456)
[2025-02-13 19:29:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:51][root][INFO] - Training Epoch: 1/2, step 4642/7134 completed (loss: 0.08492439985275269, acc: 0.9661017060279846)
[2025-02-13 19:29:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:51][root][INFO] - Training Epoch: 1/2, step 4643/7134 completed (loss: 0.07424554228782654, acc: 0.9804878234863281)
[2025-02-13 19:29:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:52][root][INFO] - Training Epoch: 1/2, step 4644/7134 completed (loss: 0.10266439616680145, acc: 0.9802955389022827)
[2025-02-13 19:29:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:52][root][INFO] - Training Epoch: 1/2, step 4645/7134 completed (loss: 0.06062862277030945, acc: 0.9756097793579102)
[2025-02-13 19:29:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:52][root][INFO] - Training Epoch: 1/2, step 4646/7134 completed (loss: 0.15961873531341553, acc: 0.9571428298950195)
[2025-02-13 19:29:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:53][root][INFO] - Training Epoch: 1/2, step 4647/7134 completed (loss: 0.2513049840927124, acc: 0.9508196711540222)
[2025-02-13 19:29:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:53][root][INFO] - Training Epoch: 1/2, step 4648/7134 completed (loss: 0.2171243280172348, acc: 0.9490445852279663)
[2025-02-13 19:29:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:54][root][INFO] - Training Epoch: 1/2, step 4649/7134 completed (loss: 0.1955973356962204, acc: 0.9512194991111755)
[2025-02-13 19:29:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:54][root][INFO] - Training Epoch: 1/2, step 4650/7134 completed (loss: 0.29099467396736145, acc: 0.912162184715271)
[2025-02-13 19:29:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:54][root][INFO] - Training Epoch: 1/2, step 4651/7134 completed (loss: 0.29797711968421936, acc: 0.9329608678817749)
[2025-02-13 19:29:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:55][root][INFO] - Training Epoch: 1/2, step 4652/7134 completed (loss: 0.15057483315467834, acc: 0.9567901492118835)
[2025-02-13 19:29:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:55][root][INFO] - Training Epoch: 1/2, step 4653/7134 completed (loss: 0.15556590259075165, acc: 0.9613526463508606)
[2025-02-13 19:29:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:56][root][INFO] - Training Epoch: 1/2, step 4654/7134 completed (loss: 0.32408252358436584, acc: 0.9189189076423645)
[2025-02-13 19:29:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:56][root][INFO] - Training Epoch: 1/2, step 4655/7134 completed (loss: 0.23745177686214447, acc: 0.9485714435577393)
[2025-02-13 19:29:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:56][root][INFO] - Training Epoch: 1/2, step 4656/7134 completed (loss: 0.22395415604114532, acc: 0.9237288236618042)
[2025-02-13 19:29:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:57][root][INFO] - Training Epoch: 1/2, step 4657/7134 completed (loss: 0.22255903482437134, acc: 0.931034505367279)
[2025-02-13 19:29:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:57][root][INFO] - Training Epoch: 1/2, step 4658/7134 completed (loss: 0.22629807889461517, acc: 0.9204545617103577)
[2025-02-13 19:29:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:57][root][INFO] - Training Epoch: 1/2, step 4659/7134 completed (loss: 0.350793719291687, acc: 0.9100528955459595)
[2025-02-13 19:29:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:58][root][INFO] - Training Epoch: 1/2, step 4660/7134 completed (loss: 0.3063306212425232, acc: 0.9230769276618958)
[2025-02-13 19:29:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:58][root][INFO] - Training Epoch: 1/2, step 4661/7134 completed (loss: 0.18458107113838196, acc: 0.9447852969169617)
[2025-02-13 19:29:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:59][root][INFO] - Training Epoch: 1/2, step 4662/7134 completed (loss: 0.4694764018058777, acc: 0.9219858050346375)
[2025-02-13 19:29:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:59][root][INFO] - Training Epoch: 1/2, step 4663/7134 completed (loss: 0.2839357852935791, acc: 0.9289617538452148)
[2025-02-13 19:29:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:29:59][root][INFO] - Training Epoch: 1/2, step 4664/7134 completed (loss: 0.2169065922498703, acc: 0.9567567706108093)
[2025-02-13 19:29:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:00][root][INFO] - Training Epoch: 1/2, step 4665/7134 completed (loss: 0.19351640343666077, acc: 0.9615384340286255)
[2025-02-13 19:30:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:00][root][INFO] - Training Epoch: 1/2, step 4666/7134 completed (loss: 0.41458505392074585, acc: 0.9222797751426697)
[2025-02-13 19:30:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:01][root][INFO] - Training Epoch: 1/2, step 4667/7134 completed (loss: 0.29455044865608215, acc: 0.9256756901741028)
[2025-02-13 19:30:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:01][root][INFO] - Training Epoch: 1/2, step 4668/7134 completed (loss: 0.17282889783382416, acc: 0.9530201554298401)
[2025-02-13 19:30:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:01][root][INFO] - Training Epoch: 1/2, step 4669/7134 completed (loss: 0.17816898226737976, acc: 0.9491525292396545)
[2025-02-13 19:30:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:02][root][INFO] - Training Epoch: 1/2, step 4670/7134 completed (loss: 0.2140054702758789, acc: 0.9477611780166626)
[2025-02-13 19:30:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:02][root][INFO] - Training Epoch: 1/2, step 4671/7134 completed (loss: 0.1466357260942459, acc: 0.9398906826972961)
[2025-02-13 19:30:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:02][root][INFO] - Training Epoch: 1/2, step 4672/7134 completed (loss: 0.15083251893520355, acc: 0.9586206674575806)
[2025-02-13 19:30:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:03][root][INFO] - Training Epoch: 1/2, step 4673/7134 completed (loss: 0.21445052325725555, acc: 0.9433962106704712)
[2025-02-13 19:30:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:03][root][INFO] - Training Epoch: 1/2, step 4674/7134 completed (loss: 0.1194726750254631, acc: 0.9610389471054077)
[2025-02-13 19:30:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:04][root][INFO] - Training Epoch: 1/2, step 4675/7134 completed (loss: 0.19809316098690033, acc: 0.964102566242218)
[2025-02-13 19:30:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:04][root][INFO] - Training Epoch: 1/2, step 4676/7134 completed (loss: 0.10265348106622696, acc: 0.9805194735527039)
[2025-02-13 19:30:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:04][root][INFO] - Training Epoch: 1/2, step 4677/7134 completed (loss: 0.11349496245384216, acc: 0.9663865566253662)
[2025-02-13 19:30:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:05][root][INFO] - Training Epoch: 1/2, step 4678/7134 completed (loss: 0.30833175778388977, acc: 0.946107804775238)
[2025-02-13 19:30:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:05][root][INFO] - Training Epoch: 1/2, step 4679/7134 completed (loss: 0.14567585289478302, acc: 0.9768785834312439)
[2025-02-13 19:30:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:06][root][INFO] - Training Epoch: 1/2, step 4680/7134 completed (loss: 0.26352250576019287, acc: 0.9272727370262146)
[2025-02-13 19:30:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:06][root][INFO] - Training Epoch: 1/2, step 4681/7134 completed (loss: 0.27565377950668335, acc: 0.9350649118423462)
[2025-02-13 19:30:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:06][root][INFO] - Training Epoch: 1/2, step 4682/7134 completed (loss: 0.18531714379787445, acc: 0.9613259434700012)
[2025-02-13 19:30:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:07][root][INFO] - Training Epoch: 1/2, step 4683/7134 completed (loss: 0.23264305293560028, acc: 0.9399999976158142)
[2025-02-13 19:30:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:07][root][INFO] - Training Epoch: 1/2, step 4684/7134 completed (loss: 0.18650756776332855, acc: 0.9639175534248352)
[2025-02-13 19:30:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:07][root][INFO] - Training Epoch: 1/2, step 4685/7134 completed (loss: 0.11277482658624649, acc: 0.9802955389022827)
[2025-02-13 19:30:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:08][root][INFO] - Training Epoch: 1/2, step 4686/7134 completed (loss: 0.14161288738250732, acc: 0.9682539701461792)
[2025-02-13 19:30:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:08][root][INFO] - Training Epoch: 1/2, step 4687/7134 completed (loss: 0.7405053973197937, acc: 0.8478260636329651)
[2025-02-13 19:30:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:09][root][INFO] - Training Epoch: 1/2, step 4688/7134 completed (loss: 0.9177177548408508, acc: 0.8515625)
[2025-02-13 19:30:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:09][root][INFO] - Training Epoch: 1/2, step 4689/7134 completed (loss: 0.46890610456466675, acc: 0.9022988677024841)
[2025-02-13 19:30:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:09][root][INFO] - Training Epoch: 1/2, step 4690/7134 completed (loss: 0.5223393440246582, acc: 0.9210526347160339)
[2025-02-13 19:30:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:10][root][INFO] - Training Epoch: 1/2, step 4691/7134 completed (loss: 0.20020583271980286, acc: 0.9692307710647583)
[2025-02-13 19:30:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:10][root][INFO] - Training Epoch: 1/2, step 4692/7134 completed (loss: 0.9061702489852905, acc: 0.8095238208770752)
[2025-02-13 19:30:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:11][root][INFO] - Training Epoch: 1/2, step 4693/7134 completed (loss: 0.7872459888458252, acc: 0.8399999737739563)
[2025-02-13 19:30:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:11][root][INFO] - Training Epoch: 1/2, step 4694/7134 completed (loss: 0.5712527632713318, acc: 0.8888888955116272)
[2025-02-13 19:30:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:11][root][INFO] - Training Epoch: 1/2, step 4695/7134 completed (loss: 0.40059396624565125, acc: 0.9133333563804626)
[2025-02-13 19:30:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:12][root][INFO] - Training Epoch: 1/2, step 4696/7134 completed (loss: 0.13517753779888153, acc: 0.9636363387107849)
[2025-02-13 19:30:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:12][root][INFO] - Training Epoch: 1/2, step 4697/7134 completed (loss: 0.13357263803482056, acc: 0.976190447807312)
[2025-02-13 19:30:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:13][root][INFO] - Training Epoch: 1/2, step 4698/7134 completed (loss: 0.4705609977245331, acc: 0.8999999761581421)
[2025-02-13 19:30:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:13][root][INFO] - Training Epoch: 1/2, step 4699/7134 completed (loss: 0.4468323886394501, acc: 0.9136690497398376)
[2025-02-13 19:30:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:13][root][INFO] - Training Epoch: 1/2, step 4700/7134 completed (loss: 0.28854691982269287, acc: 0.9349112510681152)
[2025-02-13 19:30:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:14][root][INFO] - Training Epoch: 1/2, step 4701/7134 completed (loss: 0.6420618295669556, acc: 0.8500000238418579)
[2025-02-13 19:30:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:14][root][INFO] - Training Epoch: 1/2, step 4702/7134 completed (loss: 0.22992543876171112, acc: 0.9354838728904724)
[2025-02-13 19:30:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:14][root][INFO] - Training Epoch: 1/2, step 4703/7134 completed (loss: 0.1271827071905136, acc: 0.9764705896377563)
[2025-02-13 19:30:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:15][root][INFO] - Training Epoch: 1/2, step 4704/7134 completed (loss: 0.16697603464126587, acc: 0.9435897469520569)
[2025-02-13 19:30:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:15][root][INFO] - Training Epoch: 1/2, step 4705/7134 completed (loss: 0.31213000416755676, acc: 0.9162561297416687)
[2025-02-13 19:30:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:16][root][INFO] - Training Epoch: 1/2, step 4706/7134 completed (loss: 0.3557714819908142, acc: 0.9230769276618958)
[2025-02-13 19:30:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:16][root][INFO] - Training Epoch: 1/2, step 4707/7134 completed (loss: 0.775547981262207, acc: 0.8616352081298828)
[2025-02-13 19:30:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:16][root][INFO] - Training Epoch: 1/2, step 4708/7134 completed (loss: 0.2568477392196655, acc: 0.9497487545013428)
[2025-02-13 19:30:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:17][root][INFO] - Training Epoch: 1/2, step 4709/7134 completed (loss: 0.12835998833179474, acc: 0.9729729890823364)
[2025-02-13 19:30:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:17][root][INFO] - Training Epoch: 1/2, step 4710/7134 completed (loss: 0.23260226845741272, acc: 0.9333333373069763)
[2025-02-13 19:30:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:18][root][INFO] - Training Epoch: 1/2, step 4711/7134 completed (loss: 0.1996341347694397, acc: 0.9324324131011963)
[2025-02-13 19:30:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:18][root][INFO] - Training Epoch: 1/2, step 4712/7134 completed (loss: 0.4179958403110504, acc: 0.8823529481887817)
[2025-02-13 19:30:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:18][root][INFO] - Training Epoch: 1/2, step 4713/7134 completed (loss: 0.19892434775829315, acc: 0.9477611780166626)
[2025-02-13 19:30:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:19][root][INFO] - Training Epoch: 1/2, step 4714/7134 completed (loss: 0.31540074944496155, acc: 0.9389312863349915)
[2025-02-13 19:30:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:19][root][INFO] - Training Epoch: 1/2, step 4715/7134 completed (loss: 0.4874922037124634, acc: 0.925000011920929)
[2025-02-13 19:30:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:20][root][INFO] - Training Epoch: 1/2, step 4716/7134 completed (loss: 0.2031223475933075, acc: 0.9677419066429138)
[2025-02-13 19:30:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:20][root][INFO] - Training Epoch: 1/2, step 4717/7134 completed (loss: 0.12579354643821716, acc: 0.9599999785423279)
[2025-02-13 19:30:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:20][root][INFO] - Training Epoch: 1/2, step 4718/7134 completed (loss: 0.08727636933326721, acc: 0.9784172773361206)
[2025-02-13 19:30:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:21][root][INFO] - Training Epoch: 1/2, step 4719/7134 completed (loss: 0.1531822830438614, acc: 0.9644970297813416)
[2025-02-13 19:30:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:21][root][INFO] - Training Epoch: 1/2, step 4720/7134 completed (loss: 0.09613718837499619, acc: 0.9750000238418579)
[2025-02-13 19:30:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:21][root][INFO] - Training Epoch: 1/2, step 4721/7134 completed (loss: 0.05861286446452141, acc: 0.9876543283462524)
[2025-02-13 19:30:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:22][root][INFO] - Training Epoch: 1/2, step 4722/7134 completed (loss: 0.36608943343162537, acc: 0.9308176040649414)
[2025-02-13 19:30:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:22][root][INFO] - Training Epoch: 1/2, step 4723/7134 completed (loss: 0.15206994116306305, acc: 0.9617834687232971)
[2025-02-13 19:30:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:23][root][INFO] - Training Epoch: 1/2, step 4724/7134 completed (loss: 0.1765485256910324, acc: 0.9634146094322205)
[2025-02-13 19:30:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:23][root][INFO] - Training Epoch: 1/2, step 4725/7134 completed (loss: 0.2803606390953064, acc: 0.9186046719551086)
[2025-02-13 19:30:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:23][root][INFO] - Training Epoch: 1/2, step 4726/7134 completed (loss: 0.3712726831436157, acc: 0.8793103694915771)
[2025-02-13 19:30:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:24][root][INFO] - Training Epoch: 1/2, step 4727/7134 completed (loss: 0.16901297867298126, acc: 0.9801324605941772)
[2025-02-13 19:30:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:24][root][INFO] - Training Epoch: 1/2, step 4728/7134 completed (loss: 0.25025811791419983, acc: 0.887417197227478)
[2025-02-13 19:30:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:24][root][INFO] - Training Epoch: 1/2, step 4729/7134 completed (loss: 0.16037540137767792, acc: 0.9653179049491882)
[2025-02-13 19:30:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:25][root][INFO] - Training Epoch: 1/2, step 4730/7134 completed (loss: 0.2156570851802826, acc: 0.9532163739204407)
[2025-02-13 19:30:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:25][root][INFO] - Training Epoch: 1/2, step 4731/7134 completed (loss: 0.3451739251613617, acc: 0.931034505367279)
[2025-02-13 19:30:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:26][root][INFO] - Training Epoch: 1/2, step 4732/7134 completed (loss: 0.25072190165519714, acc: 0.9338235259056091)
[2025-02-13 19:30:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:26][root][INFO] - Training Epoch: 1/2, step 4733/7134 completed (loss: 0.26325416564941406, acc: 0.95652174949646)
[2025-02-13 19:30:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:26][root][INFO] - Training Epoch: 1/2, step 4734/7134 completed (loss: 0.5332346558570862, acc: 0.893081784248352)
[2025-02-13 19:30:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:27][root][INFO] - Training Epoch: 1/2, step 4735/7134 completed (loss: 0.2743891775608063, acc: 0.9308176040649414)
[2025-02-13 19:30:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:27][root][INFO] - Training Epoch: 1/2, step 4736/7134 completed (loss: 0.37266501784324646, acc: 0.9166666865348816)
[2025-02-13 19:30:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:28][root][INFO] - Training Epoch: 1/2, step 4737/7134 completed (loss: 0.29100480675697327, acc: 0.9103448390960693)
[2025-02-13 19:30:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:28][root][INFO] - Training Epoch: 1/2, step 4738/7134 completed (loss: 0.23088490962982178, acc: 0.949999988079071)
[2025-02-13 19:30:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:28][root][INFO] - Training Epoch: 1/2, step 4739/7134 completed (loss: 0.4621327519416809, acc: 0.8876404762268066)
[2025-02-13 19:30:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:29][root][INFO] - Training Epoch: 1/2, step 4740/7134 completed (loss: 0.4222502112388611, acc: 0.9055555462837219)
[2025-02-13 19:30:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:29][root][INFO] - Training Epoch: 1/2, step 4741/7134 completed (loss: 0.28799745440483093, acc: 0.9071428775787354)
[2025-02-13 19:30:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:29][root][INFO] - Training Epoch: 1/2, step 4742/7134 completed (loss: 0.5821194052696228, acc: 0.8787878751754761)
[2025-02-13 19:30:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:30][root][INFO] - Training Epoch: 1/2, step 4743/7134 completed (loss: 0.3074316084384918, acc: 0.9078947305679321)
[2025-02-13 19:30:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:30][root][INFO] - Training Epoch: 1/2, step 4744/7134 completed (loss: 0.13129036128520966, acc: 0.9816513657569885)
[2025-02-13 19:30:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:31][root][INFO] - Training Epoch: 1/2, step 4745/7134 completed (loss: 0.31157827377319336, acc: 0.9305555820465088)
[2025-02-13 19:30:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:31][root][INFO] - Training Epoch: 1/2, step 4746/7134 completed (loss: 0.17175012826919556, acc: 0.961240291595459)
[2025-02-13 19:30:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:31][root][INFO] - Training Epoch: 1/2, step 4747/7134 completed (loss: 0.15543943643569946, acc: 0.9534883499145508)
[2025-02-13 19:30:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:32][root][INFO] - Training Epoch: 1/2, step 4748/7134 completed (loss: 0.3039223253726959, acc: 0.9230769276618958)
[2025-02-13 19:30:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:32][root][INFO] - Training Epoch: 1/2, step 4749/7134 completed (loss: 0.17031607031822205, acc: 0.9360465407371521)
[2025-02-13 19:30:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:33][root][INFO] - Training Epoch: 1/2, step 4750/7134 completed (loss: 0.22770896553993225, acc: 0.9430894255638123)
[2025-02-13 19:30:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:33][root][INFO] - Training Epoch: 1/2, step 4751/7134 completed (loss: 0.13026653230190277, acc: 0.9634146094322205)
[2025-02-13 19:30:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:33][root][INFO] - Training Epoch: 1/2, step 4752/7134 completed (loss: 0.29816004633903503, acc: 0.9254658222198486)
[2025-02-13 19:30:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:34][root][INFO] - Training Epoch: 1/2, step 4753/7134 completed (loss: 0.33820950984954834, acc: 0.9230769276618958)
[2025-02-13 19:30:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:34][root][INFO] - Training Epoch: 1/2, step 4754/7134 completed (loss: 0.06242544576525688, acc: 0.9870129823684692)
[2025-02-13 19:30:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:35][root][INFO] - Training Epoch: 1/2, step 4755/7134 completed (loss: 0.30485448241233826, acc: 0.9395604133605957)
[2025-02-13 19:30:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:35][root][INFO] - Training Epoch: 1/2, step 4756/7134 completed (loss: 0.35600197315216064, acc: 0.9435483813285828)
[2025-02-13 19:30:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:35][root][INFO] - Training Epoch: 1/2, step 4757/7134 completed (loss: 0.14467264711856842, acc: 0.9700000286102295)
[2025-02-13 19:30:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:36][root][INFO] - Training Epoch: 1/2, step 4758/7134 completed (loss: 0.11600539833307266, acc: 0.9779005646705627)
[2025-02-13 19:30:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:36][root][INFO] - Training Epoch: 1/2, step 4759/7134 completed (loss: 0.07903243601322174, acc: 0.978723406791687)
[2025-02-13 19:30:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:37][root][INFO] - Training Epoch: 1/2, step 4760/7134 completed (loss: 0.1508202850818634, acc: 0.9411764740943909)
[2025-02-13 19:30:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:37][root][INFO] - Training Epoch: 1/2, step 4761/7134 completed (loss: 0.15823635458946228, acc: 0.9695122241973877)
[2025-02-13 19:30:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:37][root][INFO] - Training Epoch: 1/2, step 4762/7134 completed (loss: 0.4615178406238556, acc: 0.9175823926925659)
[2025-02-13 19:30:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:38][root][INFO] - Training Epoch: 1/2, step 4763/7134 completed (loss: 0.16020916402339935, acc: 0.9668874144554138)
[2025-02-13 19:30:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:38][root][INFO] - Training Epoch: 1/2, step 4764/7134 completed (loss: 0.20912306010723114, acc: 0.969072163105011)
[2025-02-13 19:30:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:38][root][INFO] - Training Epoch: 1/2, step 4765/7134 completed (loss: 0.3325052857398987, acc: 0.9599999785423279)
[2025-02-13 19:30:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:39][root][INFO] - Training Epoch: 1/2, step 4766/7134 completed (loss: 0.16548609733581543, acc: 0.9666666388511658)
[2025-02-13 19:30:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:39][root][INFO] - Training Epoch: 1/2, step 4767/7134 completed (loss: 0.1557852327823639, acc: 0.9693877696990967)
[2025-02-13 19:30:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:39][root][INFO] - Training Epoch: 1/2, step 4768/7134 completed (loss: 0.11220750212669373, acc: 0.970588207244873)
[2025-02-13 19:30:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:40][root][INFO] - Training Epoch: 1/2, step 4769/7134 completed (loss: 0.24272072315216064, acc: 0.9539473652839661)
[2025-02-13 19:30:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:40][root][INFO] - Training Epoch: 1/2, step 4770/7134 completed (loss: 0.08837681263685226, acc: 0.9679999947547913)
[2025-02-13 19:30:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:41][root][INFO] - Training Epoch: 1/2, step 4771/7134 completed (loss: 0.39105600118637085, acc: 0.9202454090118408)
[2025-02-13 19:30:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:41][root][INFO] - Training Epoch: 1/2, step 4772/7134 completed (loss: 0.2876809537410736, acc: 0.9419354796409607)
[2025-02-13 19:30:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:41][root][INFO] - Training Epoch: 1/2, step 4773/7134 completed (loss: 0.11517376452684402, acc: 0.9679487347602844)
[2025-02-13 19:30:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:42][root][INFO] - Training Epoch: 1/2, step 4774/7134 completed (loss: 0.18301735818386078, acc: 0.976331353187561)
[2025-02-13 19:30:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:42][root][INFO] - Training Epoch: 1/2, step 4775/7134 completed (loss: 0.23564592003822327, acc: 0.955974817276001)
[2025-02-13 19:30:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:42][root][INFO] - Training Epoch: 1/2, step 4776/7134 completed (loss: 0.3753313720226288, acc: 0.908450722694397)
[2025-02-13 19:30:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:43][root][INFO] - Training Epoch: 1/2, step 4777/7134 completed (loss: 0.23687751591205597, acc: 0.9520547986030579)
[2025-02-13 19:30:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:43][root][INFO] - Training Epoch: 1/2, step 4778/7134 completed (loss: 0.2611640393733978, acc: 0.9285714030265808)
[2025-02-13 19:30:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:44][root][INFO] - Training Epoch: 1/2, step 4779/7134 completed (loss: 0.1068878322839737, acc: 0.9777777791023254)
[2025-02-13 19:30:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:44][root][INFO] - Training Epoch: 1/2, step 4780/7134 completed (loss: 0.06222565472126007, acc: 0.9743589758872986)
[2025-02-13 19:30:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:44][root][INFO] - Training Epoch: 1/2, step 4781/7134 completed (loss: 0.08705565333366394, acc: 0.9838709831237793)
[2025-02-13 19:30:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:45][root][INFO] - Training Epoch: 1/2, step 4782/7134 completed (loss: 0.18115010857582092, acc: 0.96875)
[2025-02-13 19:30:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:45][root][INFO] - Training Epoch: 1/2, step 4783/7134 completed (loss: 0.4541175365447998, acc: 0.8985507488250732)
[2025-02-13 19:30:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:46][root][INFO] - Training Epoch: 1/2, step 4784/7134 completed (loss: 0.1386292576789856, acc: 0.9777777791023254)
[2025-02-13 19:30:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:46][root][INFO] - Training Epoch: 1/2, step 4785/7134 completed (loss: 0.11564850062131882, acc: 0.9850746393203735)
[2025-02-13 19:30:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:46][root][INFO] - Training Epoch: 1/2, step 4786/7134 completed (loss: 0.12894687056541443, acc: 0.9691358208656311)
[2025-02-13 19:30:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:47][root][INFO] - Training Epoch: 1/2, step 4787/7134 completed (loss: 0.05873497202992439, acc: 0.9937106966972351)
[2025-02-13 19:30:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:47][root][INFO] - Training Epoch: 1/2, step 4788/7134 completed (loss: 0.025247227400541306, acc: 0.9923076629638672)
[2025-02-13 19:30:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:48][root][INFO] - Training Epoch: 1/2, step 4789/7134 completed (loss: 0.23744189739227295, acc: 0.9551281929016113)
[2025-02-13 19:30:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:48][root][INFO] - Training Epoch: 1/2, step 4790/7134 completed (loss: 0.14779417216777802, acc: 0.966292142868042)
[2025-02-13 19:30:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:48][root][INFO] - Training Epoch: 1/2, step 4791/7134 completed (loss: 0.05475075915455818, acc: 0.9850746393203735)
[2025-02-13 19:30:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:49][root][INFO] - Training Epoch: 1/2, step 4792/7134 completed (loss: 0.04283144697546959, acc: 1.0)
[2025-02-13 19:30:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:49][root][INFO] - Training Epoch: 1/2, step 4793/7134 completed (loss: 0.38320192694664, acc: 0.9052631855010986)
[2025-02-13 19:30:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:49][root][INFO] - Training Epoch: 1/2, step 4794/7134 completed (loss: 0.34845778346061707, acc: 0.9219858050346375)
[2025-02-13 19:30:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:50][root][INFO] - Training Epoch: 1/2, step 4795/7134 completed (loss: 0.16229043900966644, acc: 0.9447852969169617)
[2025-02-13 19:30:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:50][root][INFO] - Training Epoch: 1/2, step 4796/7134 completed (loss: 0.25982072949409485, acc: 0.9407894611358643)
[2025-02-13 19:30:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:51][root][INFO] - Training Epoch: 1/2, step 4797/7134 completed (loss: 0.10254698246717453, acc: 0.9781420826911926)
[2025-02-13 19:30:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:51][root][INFO] - Training Epoch: 1/2, step 4798/7134 completed (loss: 0.16999787092208862, acc: 0.970059871673584)
[2025-02-13 19:30:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:51][root][INFO] - Training Epoch: 1/2, step 4799/7134 completed (loss: 0.25478097796440125, acc: 0.9375)
[2025-02-13 19:30:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:52][root][INFO] - Training Epoch: 1/2, step 4800/7134 completed (loss: 0.14623667299747467, acc: 0.9671052694320679)
[2025-02-13 19:30:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:52][root][INFO] - Training Epoch: 1/2, step 4801/7134 completed (loss: 0.09705314040184021, acc: 0.9744898080825806)
[2025-02-13 19:30:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:53][root][INFO] - Training Epoch: 1/2, step 4802/7134 completed (loss: 0.27346140146255493, acc: 0.9175257682800293)
[2025-02-13 19:30:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:53][root][INFO] - Training Epoch: 1/2, step 4803/7134 completed (loss: 0.2160937488079071, acc: 0.9454545378684998)
[2025-02-13 19:30:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:53][root][INFO] - Training Epoch: 1/2, step 4804/7134 completed (loss: 0.30054181814193726, acc: 0.9156626462936401)
[2025-02-13 19:30:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:54][root][INFO] - Training Epoch: 1/2, step 4805/7134 completed (loss: 0.25984716415405273, acc: 0.9735099077224731)
[2025-02-13 19:30:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:54][root][INFO] - Training Epoch: 1/2, step 4806/7134 completed (loss: 0.20558270812034607, acc: 0.9510489702224731)
[2025-02-13 19:30:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:54][root][INFO] - Training Epoch: 1/2, step 4807/7134 completed (loss: 0.21802303194999695, acc: 0.949367105960846)
[2025-02-13 19:30:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:55][root][INFO] - Training Epoch: 1/2, step 4808/7134 completed (loss: 0.17346103489398956, acc: 0.939393937587738)
[2025-02-13 19:30:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:55][root][INFO] - Training Epoch: 1/2, step 4809/7134 completed (loss: 0.13037484884262085, acc: 0.9724137783050537)
[2025-02-13 19:30:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:56][root][INFO] - Training Epoch: 1/2, step 4810/7134 completed (loss: 0.19229911267757416, acc: 0.9470587968826294)
[2025-02-13 19:30:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:56][root][INFO] - Training Epoch: 1/2, step 4811/7134 completed (loss: 0.195580393075943, acc: 0.9508196711540222)
[2025-02-13 19:30:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:56][root][INFO] - Training Epoch: 1/2, step 4812/7134 completed (loss: 0.2332918643951416, acc: 0.9661017060279846)
[2025-02-13 19:30:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:57][root][INFO] - Training Epoch: 1/2, step 4813/7134 completed (loss: 0.13513028621673584, acc: 0.96875)
[2025-02-13 19:30:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:57][root][INFO] - Training Epoch: 1/2, step 4814/7134 completed (loss: 0.15288355946540833, acc: 0.9239130616188049)
[2025-02-13 19:30:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:57][root][INFO] - Training Epoch: 1/2, step 4815/7134 completed (loss: 0.025458991527557373, acc: 1.0)
[2025-02-13 19:30:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:58][root][INFO] - Training Epoch: 1/2, step 4816/7134 completed (loss: 0.0446988008916378, acc: 0.9899497628211975)
[2025-02-13 19:30:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:58][root][INFO] - Training Epoch: 1/2, step 4817/7134 completed (loss: 0.10317748785018921, acc: 0.9797979593276978)
[2025-02-13 19:30:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:59][root][INFO] - Training Epoch: 1/2, step 4818/7134 completed (loss: 0.10624702274799347, acc: 0.9727891087532043)
[2025-02-13 19:30:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:59][root][INFO] - Training Epoch: 1/2, step 4819/7134 completed (loss: 0.049862392246723175, acc: 0.9916666746139526)
[2025-02-13 19:30:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:30:59][root][INFO] - Training Epoch: 1/2, step 4820/7134 completed (loss: 0.07088223844766617, acc: 0.9786096215248108)
[2025-02-13 19:31:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:00][root][INFO] - Training Epoch: 1/2, step 4821/7134 completed (loss: 0.07249104231595993, acc: 0.9884393215179443)
[2025-02-13 19:31:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:00][root][INFO] - Training Epoch: 1/2, step 4822/7134 completed (loss: 0.07637622207403183, acc: 0.9852941036224365)
[2025-02-13 19:31:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:01][root][INFO] - Training Epoch: 1/2, step 4823/7134 completed (loss: 0.26618313789367676, acc: 0.9308176040649414)
[2025-02-13 19:31:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:01][root][INFO] - Training Epoch: 1/2, step 4824/7134 completed (loss: 0.3106459975242615, acc: 0.9322034120559692)
[2025-02-13 19:31:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:01][root][INFO] - Training Epoch: 1/2, step 4825/7134 completed (loss: 0.06783362478017807, acc: 0.9852941036224365)
[2025-02-13 19:31:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:02][root][INFO] - Training Epoch: 1/2, step 4826/7134 completed (loss: 0.1435573399066925, acc: 0.9504132270812988)
[2025-02-13 19:31:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:02][root][INFO] - Training Epoch: 1/2, step 4827/7134 completed (loss: 0.34385010600090027, acc: 0.9548386931419373)
[2025-02-13 19:31:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:02][root][INFO] - Training Epoch: 1/2, step 4828/7134 completed (loss: 0.08833485841751099, acc: 0.9693877696990967)
[2025-02-13 19:31:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:03][root][INFO] - Training Epoch: 1/2, step 4829/7134 completed (loss: 0.051637981086969376, acc: 0.9900990128517151)
[2025-02-13 19:31:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:03][root][INFO] - Training Epoch: 1/2, step 4830/7134 completed (loss: 0.08392166346311569, acc: 0.9696969985961914)
[2025-02-13 19:31:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:04][root][INFO] - Training Epoch: 1/2, step 4831/7134 completed (loss: 0.02580355852842331, acc: 1.0)
[2025-02-13 19:31:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:04][root][INFO] - Training Epoch: 1/2, step 4832/7134 completed (loss: 0.0619790181517601, acc: 0.9803921580314636)
[2025-02-13 19:31:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:04][root][INFO] - Training Epoch: 1/2, step 4833/7134 completed (loss: 0.09463559836149216, acc: 0.9644669890403748)
[2025-02-13 19:31:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:05][root][INFO] - Training Epoch: 1/2, step 4834/7134 completed (loss: 0.11434280127286911, acc: 0.9720930457115173)
[2025-02-13 19:31:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:05][root][INFO] - Training Epoch: 1/2, step 4835/7134 completed (loss: 0.10798650979995728, acc: 0.964102566242218)
[2025-02-13 19:31:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:06][root][INFO] - Training Epoch: 1/2, step 4836/7134 completed (loss: 0.06809123605489731, acc: 0.9938271641731262)
[2025-02-13 19:31:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:06][root][INFO] - Training Epoch: 1/2, step 4837/7134 completed (loss: 0.3865707814693451, acc: 0.9214285612106323)
[2025-02-13 19:31:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:06][root][INFO] - Training Epoch: 1/2, step 4838/7134 completed (loss: 0.36597493290901184, acc: 0.8992805480957031)
[2025-02-13 19:31:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:07][root][INFO] - Training Epoch: 1/2, step 4839/7134 completed (loss: 0.27671369910240173, acc: 0.9224806427955627)
[2025-02-13 19:31:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:07][root][INFO] - Training Epoch: 1/2, step 4840/7134 completed (loss: 0.18636967241764069, acc: 0.9342105388641357)
[2025-02-13 19:31:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:07][root][INFO] - Training Epoch: 1/2, step 4841/7134 completed (loss: 0.12223716825246811, acc: 0.9583333134651184)
[2025-02-13 19:31:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:08][root][INFO] - Training Epoch: 1/2, step 4842/7134 completed (loss: 0.14111168682575226, acc: 0.9722222089767456)
[2025-02-13 19:31:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:08][root][INFO] - Training Epoch: 1/2, step 4843/7134 completed (loss: 0.2727178633213043, acc: 0.9308176040649414)
[2025-02-13 19:31:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:09][root][INFO] - Training Epoch: 1/2, step 4844/7134 completed (loss: 0.17797401547431946, acc: 0.9384615421295166)
[2025-02-13 19:31:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:09][root][INFO] - Training Epoch: 1/2, step 4845/7134 completed (loss: 0.3061130940914154, acc: 0.9375)
[2025-02-13 19:31:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:10][root][INFO] - Training Epoch: 1/2, step 4846/7134 completed (loss: 0.2667553424835205, acc: 0.9375)
[2025-02-13 19:31:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:10][root][INFO] - Training Epoch: 1/2, step 4847/7134 completed (loss: 0.13938499987125397, acc: 0.9704142212867737)
[2025-02-13 19:31:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:10][root][INFO] - Training Epoch: 1/2, step 4848/7134 completed (loss: 0.19981791079044342, acc: 0.9629629850387573)
[2025-02-13 19:31:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:11][root][INFO] - Training Epoch: 1/2, step 4849/7134 completed (loss: 0.17300166189670563, acc: 0.965753436088562)
[2025-02-13 19:31:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:11][root][INFO] - Training Epoch: 1/2, step 4850/7134 completed (loss: 0.23449666798114777, acc: 0.9441340565681458)
[2025-02-13 19:31:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:12][root][INFO] - Training Epoch: 1/2, step 4851/7134 completed (loss: 0.2169150859117508, acc: 0.9459459185600281)
[2025-02-13 19:31:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:12][root][INFO] - Training Epoch: 1/2, step 4852/7134 completed (loss: 0.35325369238853455, acc: 0.9069767594337463)
[2025-02-13 19:31:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:12][root][INFO] - Training Epoch: 1/2, step 4853/7134 completed (loss: 0.5571279525756836, acc: 0.8682634830474854)
[2025-02-13 19:31:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:13][root][INFO] - Training Epoch: 1/2, step 4854/7134 completed (loss: 0.12886296212673187, acc: 0.9798657894134521)
[2025-02-13 19:31:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:13][root][INFO] - Training Epoch: 1/2, step 4855/7134 completed (loss: 0.19431224465370178, acc: 0.954023003578186)
[2025-02-13 19:31:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:14][root][INFO] - Training Epoch: 1/2, step 4856/7134 completed (loss: 0.14436770975589752, acc: 0.9642857313156128)
[2025-02-13 19:31:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:14][root][INFO] - Training Epoch: 1/2, step 4857/7134 completed (loss: 0.14108583331108093, acc: 0.9567901492118835)
[2025-02-13 19:31:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:14][root][INFO] - Training Epoch: 1/2, step 4858/7134 completed (loss: 0.2973460555076599, acc: 0.9135802388191223)
[2025-02-13 19:31:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:15][root][INFO] - Training Epoch: 1/2, step 4859/7134 completed (loss: 0.1376160830259323, acc: 0.9702380895614624)
[2025-02-13 19:31:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:15][root][INFO] - Training Epoch: 1/2, step 4860/7134 completed (loss: 0.17264918982982635, acc: 0.9707602262496948)
[2025-02-13 19:31:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:15][root][INFO] - Training Epoch: 1/2, step 4861/7134 completed (loss: 0.15888433158397675, acc: 0.9822485446929932)
[2025-02-13 19:31:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:16][root][INFO] - Training Epoch: 1/2, step 4862/7134 completed (loss: 0.17063260078430176, acc: 0.9671052694320679)
[2025-02-13 19:31:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:16][root][INFO] - Training Epoch: 1/2, step 4863/7134 completed (loss: 0.19735924899578094, acc: 0.961240291595459)
[2025-02-13 19:31:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:17][root][INFO] - Training Epoch: 1/2, step 4864/7134 completed (loss: 0.2030791938304901, acc: 0.9617834687232971)
[2025-02-13 19:31:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:17][root][INFO] - Training Epoch: 1/2, step 4865/7134 completed (loss: 0.253905326128006, acc: 0.9301075339317322)
[2025-02-13 19:31:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:17][root][INFO] - Training Epoch: 1/2, step 4866/7134 completed (loss: 0.187447652220726, acc: 0.9418604373931885)
[2025-02-13 19:31:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:18][root][INFO] - Training Epoch: 1/2, step 4867/7134 completed (loss: 0.19615550339221954, acc: 0.9497206807136536)
[2025-02-13 19:31:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:18][root][INFO] - Training Epoch: 1/2, step 4868/7134 completed (loss: 0.36018067598342896, acc: 0.9327731132507324)
[2025-02-13 19:31:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:19][root][INFO] - Training Epoch: 1/2, step 4869/7134 completed (loss: 0.21575967967510223, acc: 0.9285714030265808)
[2025-02-13 19:31:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:19][root][INFO] - Training Epoch: 1/2, step 4870/7134 completed (loss: 0.10080718249082565, acc: 0.9783783555030823)
[2025-02-13 19:31:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:19][root][INFO] - Training Epoch: 1/2, step 4871/7134 completed (loss: 0.11911063641309738, acc: 0.9671052694320679)
[2025-02-13 19:31:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:20][root][INFO] - Training Epoch: 1/2, step 4872/7134 completed (loss: 0.039133280515670776, acc: 0.9940476417541504)
[2025-02-13 19:31:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:20][root][INFO] - Training Epoch: 1/2, step 4873/7134 completed (loss: 0.12350182235240936, acc: 0.9726775884628296)
[2025-02-13 19:31:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:20][root][INFO] - Training Epoch: 1/2, step 4874/7134 completed (loss: 0.18334320187568665, acc: 0.9658536314964294)
[2025-02-13 19:31:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:21][root][INFO] - Training Epoch: 1/2, step 4875/7134 completed (loss: 0.10705570131540298, acc: 0.9813664555549622)
[2025-02-13 19:31:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:21][root][INFO] - Training Epoch: 1/2, step 4876/7134 completed (loss: 0.1940232366323471, acc: 0.9351851940155029)
[2025-02-13 19:31:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:22][root][INFO] - Training Epoch: 1/2, step 4877/7134 completed (loss: 0.22604522109031677, acc: 0.964102566242218)
[2025-02-13 19:31:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:22][root][INFO] - Training Epoch: 1/2, step 4878/7134 completed (loss: 0.1567421406507492, acc: 0.9740259647369385)
[2025-02-13 19:31:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:22][root][INFO] - Training Epoch: 1/2, step 4879/7134 completed (loss: 0.17537179589271545, acc: 0.9551281929016113)
[2025-02-13 19:31:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:23][root][INFO] - Training Epoch: 1/2, step 4880/7134 completed (loss: 0.07917491346597672, acc: 0.9882352948188782)
[2025-02-13 19:31:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:23][root][INFO] - Training Epoch: 1/2, step 4881/7134 completed (loss: 0.2428627461194992, acc: 0.9463087320327759)
[2025-02-13 19:31:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:24][root][INFO] - Training Epoch: 1/2, step 4882/7134 completed (loss: 0.17881256341934204, acc: 0.9548386931419373)
[2025-02-13 19:31:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:24][root][INFO] - Training Epoch: 1/2, step 4883/7134 completed (loss: 0.12158051878213882, acc: 0.9666666388511658)
[2025-02-13 19:31:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:24][root][INFO] - Training Epoch: 1/2, step 4884/7134 completed (loss: 0.09887564182281494, acc: 0.9607843160629272)
[2025-02-13 19:31:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:25][root][INFO] - Training Epoch: 1/2, step 4885/7134 completed (loss: 0.10505198687314987, acc: 0.9677419066429138)
[2025-02-13 19:31:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:25][root][INFO] - Training Epoch: 1/2, step 4886/7134 completed (loss: 0.2092342972755432, acc: 0.9519230723381042)
[2025-02-13 19:31:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:25][root][INFO] - Training Epoch: 1/2, step 4887/7134 completed (loss: 0.07456953078508377, acc: 0.9842932224273682)
[2025-02-13 19:31:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:26][root][INFO] - Training Epoch: 1/2, step 4888/7134 completed (loss: 0.10249795764684677, acc: 0.9780219793319702)
[2025-02-13 19:31:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:26][root][INFO] - Training Epoch: 1/2, step 4889/7134 completed (loss: 0.04989136382937431, acc: 0.9893048405647278)
[2025-02-13 19:31:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:27][root][INFO] - Training Epoch: 1/2, step 4890/7134 completed (loss: 0.18220491707324982, acc: 0.9622641801834106)
[2025-02-13 19:31:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:27][root][INFO] - Training Epoch: 1/2, step 4891/7134 completed (loss: 0.19993560016155243, acc: 0.954081654548645)
[2025-02-13 19:31:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:27][root][INFO] - Training Epoch: 1/2, step 4892/7134 completed (loss: 0.15211020410060883, acc: 0.9675324559211731)
[2025-02-13 19:31:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:28][root][INFO] - Training Epoch: 1/2, step 4893/7134 completed (loss: 0.7432698011398315, acc: 0.8448275923728943)
[2025-02-13 19:31:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:28][root][INFO] - Training Epoch: 1/2, step 4894/7134 completed (loss: 0.13331551849842072, acc: 0.9588235020637512)
[2025-02-13 19:31:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:29][root][INFO] - Training Epoch: 1/2, step 4895/7134 completed (loss: 0.07319750636816025, acc: 0.9744898080825806)
[2025-02-13 19:31:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:29][root][INFO] - Training Epoch: 1/2, step 4896/7134 completed (loss: 0.12614649534225464, acc: 0.9664804339408875)
[2025-02-13 19:31:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:29][root][INFO] - Training Epoch: 1/2, step 4897/7134 completed (loss: 0.09288642555475235, acc: 0.9852216839790344)
[2025-02-13 19:31:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:30][root][INFO] - Training Epoch: 1/2, step 4898/7134 completed (loss: 0.07961194217205048, acc: 0.9892473220825195)
[2025-02-13 19:31:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:30][root][INFO] - Training Epoch: 1/2, step 4899/7134 completed (loss: 0.13282646238803864, acc: 0.9756097793579102)
[2025-02-13 19:31:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:31][root][INFO] - Training Epoch: 1/2, step 4900/7134 completed (loss: 0.11972946673631668, acc: 0.9839572310447693)
[2025-02-13 19:31:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:31][root][INFO] - Training Epoch: 1/2, step 4901/7134 completed (loss: 0.09479431062936783, acc: 0.9695122241973877)
[2025-02-13 19:31:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:31][root][INFO] - Training Epoch: 1/2, step 4902/7134 completed (loss: 0.0715930387377739, acc: 0.9794871807098389)
[2025-02-13 19:31:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:32][root][INFO] - Training Epoch: 1/2, step 4903/7134 completed (loss: 0.1006767749786377, acc: 0.9781022071838379)
[2025-02-13 19:31:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:32][root][INFO] - Training Epoch: 1/2, step 4904/7134 completed (loss: 0.03905363380908966, acc: 0.9941176176071167)
[2025-02-13 19:31:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:32][root][INFO] - Training Epoch: 1/2, step 4905/7134 completed (loss: 0.060785554349422455, acc: 0.9803921580314636)
[2025-02-13 19:31:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:33][root][INFO] - Training Epoch: 1/2, step 4906/7134 completed (loss: 0.03541512414813042, acc: 1.0)
[2025-02-13 19:31:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:33][root][INFO] - Training Epoch: 1/2, step 4907/7134 completed (loss: 0.09185989201068878, acc: 0.9863945841789246)
[2025-02-13 19:31:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:34][root][INFO] - Training Epoch: 1/2, step 4908/7134 completed (loss: 0.05355241149663925, acc: 0.9870129823684692)
[2025-02-13 19:31:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:34][root][INFO] - Training Epoch: 1/2, step 4909/7134 completed (loss: 0.0367126502096653, acc: 0.9932885766029358)
[2025-02-13 19:31:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:34][root][INFO] - Training Epoch: 1/2, step 4910/7134 completed (loss: 0.16502277553081512, acc: 0.9677419066429138)
[2025-02-13 19:31:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:35][root][INFO] - Training Epoch: 1/2, step 4911/7134 completed (loss: 0.07468216866254807, acc: 0.9839572310447693)
[2025-02-13 19:31:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:35][root][INFO] - Training Epoch: 1/2, step 4912/7134 completed (loss: 0.06309828907251358, acc: 0.9850746393203735)
[2025-02-13 19:31:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:35][root][INFO] - Training Epoch: 1/2, step 4913/7134 completed (loss: 0.16073094308376312, acc: 0.9510489702224731)
[2025-02-13 19:31:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:36][root][INFO] - Training Epoch: 1/2, step 4914/7134 completed (loss: 0.10952683538198471, acc: 0.9806451797485352)
[2025-02-13 19:31:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:36][root][INFO] - Training Epoch: 1/2, step 4915/7134 completed (loss: 0.015052271075546741, acc: 1.0)
[2025-02-13 19:31:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:37][root][INFO] - Training Epoch: 1/2, step 4916/7134 completed (loss: 0.2703624367713928, acc: 0.9347826242446899)
[2025-02-13 19:31:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:37][root][INFO] - Training Epoch: 1/2, step 4917/7134 completed (loss: 0.20430317521095276, acc: 0.9554139971733093)
[2025-02-13 19:31:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:37][root][INFO] - Training Epoch: 1/2, step 4918/7134 completed (loss: 0.04141104221343994, acc: 0.9938271641731262)
[2025-02-13 19:31:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:38][root][INFO] - Training Epoch: 1/2, step 4919/7134 completed (loss: 0.13886788487434387, acc: 0.9830508232116699)
[2025-02-13 19:31:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:38][root][INFO] - Training Epoch: 1/2, step 4920/7134 completed (loss: 0.2556559443473816, acc: 0.9440993666648865)
[2025-02-13 19:31:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:39][root][INFO] - Training Epoch: 1/2, step 4921/7134 completed (loss: 0.1971503347158432, acc: 0.9343065619468689)
[2025-02-13 19:31:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:39][root][INFO] - Training Epoch: 1/2, step 4922/7134 completed (loss: 0.11191871762275696, acc: 0.9813664555549622)
[2025-02-13 19:31:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:39][root][INFO] - Training Epoch: 1/2, step 4923/7134 completed (loss: 0.08468521386384964, acc: 0.97826087474823)
[2025-02-13 19:31:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:40][root][INFO] - Training Epoch: 1/2, step 4924/7134 completed (loss: 0.09602658450603485, acc: 0.9805825352668762)
[2025-02-13 19:31:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:40][root][INFO] - Training Epoch: 1/2, step 4925/7134 completed (loss: 0.15188299119472504, acc: 0.9463087320327759)
[2025-02-13 19:31:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:41][root][INFO] - Training Epoch: 1/2, step 4926/7134 completed (loss: 0.14614862203598022, acc: 0.9731543660163879)
[2025-02-13 19:31:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:41][root][INFO] - Training Epoch: 1/2, step 4927/7134 completed (loss: 0.07256778329610825, acc: 0.9870967864990234)
[2025-02-13 19:31:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:41][root][INFO] - Training Epoch: 1/2, step 4928/7134 completed (loss: 0.05282524973154068, acc: 0.9894737005233765)
[2025-02-13 19:31:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:42][root][INFO] - Training Epoch: 1/2, step 4929/7134 completed (loss: 0.08243061602115631, acc: 0.9868420958518982)
[2025-02-13 19:31:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:42][root][INFO] - Training Epoch: 1/2, step 4930/7134 completed (loss: 0.13475194573402405, acc: 0.9797297120094299)
[2025-02-13 19:31:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:42][root][INFO] - Training Epoch: 1/2, step 4931/7134 completed (loss: 0.13392196595668793, acc: 0.9693251252174377)
[2025-02-13 19:31:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:43][root][INFO] - Training Epoch: 1/2, step 4932/7134 completed (loss: 0.11239992827177048, acc: 0.970588207244873)
[2025-02-13 19:31:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:43][root][INFO] - Training Epoch: 1/2, step 4933/7134 completed (loss: 0.2099192887544632, acc: 0.9371727705001831)
[2025-02-13 19:31:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:44][root][INFO] - Training Epoch: 1/2, step 4934/7134 completed (loss: 0.061773210763931274, acc: 0.9882352948188782)
[2025-02-13 19:31:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:44][root][INFO] - Training Epoch: 1/2, step 4935/7134 completed (loss: 0.11185692995786667, acc: 0.9588235020637512)
[2025-02-13 19:31:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:44][root][INFO] - Training Epoch: 1/2, step 4936/7134 completed (loss: 0.05239550396800041, acc: 0.9863945841789246)
[2025-02-13 19:31:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:45][root][INFO] - Training Epoch: 1/2, step 4937/7134 completed (loss: 0.15411266684532166, acc: 0.9860140085220337)
[2025-02-13 19:31:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:45][root][INFO] - Training Epoch: 1/2, step 4938/7134 completed (loss: 0.04181747883558273, acc: 0.9938271641731262)
[2025-02-13 19:31:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:46][root][INFO] - Training Epoch: 1/2, step 4939/7134 completed (loss: 0.19439104199409485, acc: 0.949999988079071)
[2025-02-13 19:31:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:46][root][INFO] - Training Epoch: 1/2, step 4940/7134 completed (loss: 0.1983267217874527, acc: 0.9457364082336426)
[2025-02-13 19:31:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:46][root][INFO] - Training Epoch: 1/2, step 4941/7134 completed (loss: 0.1863396316766739, acc: 0.9520958065986633)
[2025-02-13 19:31:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:47][root][INFO] - Training Epoch: 1/2, step 4942/7134 completed (loss: 0.13068154454231262, acc: 0.9556962251663208)
[2025-02-13 19:31:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:47][root][INFO] - Training Epoch: 1/2, step 4943/7134 completed (loss: 0.20007693767547607, acc: 0.9437500238418579)
[2025-02-13 19:31:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:47][root][INFO] - Training Epoch: 1/2, step 4944/7134 completed (loss: 0.3344041705131531, acc: 0.9327731132507324)
[2025-02-13 19:31:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:48][root][INFO] - Training Epoch: 1/2, step 4945/7134 completed (loss: 0.33375313878059387, acc: 0.9318181872367859)
[2025-02-13 19:31:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:48][root][INFO] - Training Epoch: 1/2, step 4946/7134 completed (loss: 0.26340070366859436, acc: 0.9375)
[2025-02-13 19:31:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:49][root][INFO] - Training Epoch: 1/2, step 4947/7134 completed (loss: 0.2447417676448822, acc: 0.9304347634315491)
[2025-02-13 19:31:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:49][root][INFO] - Training Epoch: 1/2, step 4948/7134 completed (loss: 0.20716509222984314, acc: 0.9333333373069763)
[2025-02-13 19:31:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:49][root][INFO] - Training Epoch: 1/2, step 4949/7134 completed (loss: 0.0775415375828743, acc: 0.9876543283462524)
[2025-02-13 19:31:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:50][root][INFO] - Training Epoch: 1/2, step 4950/7134 completed (loss: 0.06132647395133972, acc: 0.9931972622871399)
[2025-02-13 19:31:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:50][root][INFO] - Training Epoch: 1/2, step 4951/7134 completed (loss: 0.11955507844686508, acc: 0.966292142868042)
[2025-02-13 19:31:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:51][root][INFO] - Training Epoch: 1/2, step 4952/7134 completed (loss: 0.07880806922912598, acc: 0.9662162065505981)
[2025-02-13 19:31:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:51][root][INFO] - Training Epoch: 1/2, step 4953/7134 completed (loss: 0.10313005745410919, acc: 0.9819819927215576)
[2025-02-13 19:31:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:51][root][INFO] - Training Epoch: 1/2, step 4954/7134 completed (loss: 0.17836138606071472, acc: 0.9387755393981934)
[2025-02-13 19:31:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:52][root][INFO] - Training Epoch: 1/2, step 4955/7134 completed (loss: 0.15271605551242828, acc: 0.9583333134651184)
[2025-02-13 19:31:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:52][root][INFO] - Training Epoch: 1/2, step 4956/7134 completed (loss: 0.32483020424842834, acc: 0.9529411792755127)
[2025-02-13 19:31:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:53][root][INFO] - Training Epoch: 1/2, step 4957/7134 completed (loss: 0.20261971652507782, acc: 0.957446813583374)
[2025-02-13 19:31:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:53][root][INFO] - Training Epoch: 1/2, step 4958/7134 completed (loss: 0.1403619796037674, acc: 0.9433962106704712)
[2025-02-13 19:31:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:53][root][INFO] - Training Epoch: 1/2, step 4959/7134 completed (loss: 0.1954975575208664, acc: 0.9578313231468201)
[2025-02-13 19:31:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:54][root][INFO] - Training Epoch: 1/2, step 4960/7134 completed (loss: 0.19345980882644653, acc: 0.9632353186607361)
[2025-02-13 19:31:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:54][root][INFO] - Training Epoch: 1/2, step 4961/7134 completed (loss: 0.15681329369544983, acc: 0.96875)
[2025-02-13 19:31:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:55][root][INFO] - Training Epoch: 1/2, step 4962/7134 completed (loss: 0.10538260638713837, acc: 0.9756097793579102)
[2025-02-13 19:31:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:55][root][INFO] - Training Epoch: 1/2, step 4963/7134 completed (loss: 0.07693013548851013, acc: 0.9677419066429138)
[2025-02-13 19:31:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:55][root][INFO] - Training Epoch: 1/2, step 4964/7134 completed (loss: 0.21494948863983154, acc: 0.9496402740478516)
[2025-02-13 19:31:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:56][root][INFO] - Training Epoch: 1/2, step 4965/7134 completed (loss: 0.046911053359508514, acc: 0.991304337978363)
[2025-02-13 19:31:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:56][root][INFO] - Training Epoch: 1/2, step 4966/7134 completed (loss: 0.23959071934223175, acc: 0.9340659379959106)
[2025-02-13 19:31:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:56][root][INFO] - Training Epoch: 1/2, step 4967/7134 completed (loss: 0.11482540518045425, acc: 0.9844961166381836)
[2025-02-13 19:31:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:57][root][INFO] - Training Epoch: 1/2, step 4968/7134 completed (loss: 0.4000066816806793, acc: 0.8652482032775879)
[2025-02-13 19:31:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:57][root][INFO] - Training Epoch: 1/2, step 4969/7134 completed (loss: 0.1423666775226593, acc: 0.951724112033844)
[2025-02-13 19:31:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:58][root][INFO] - Training Epoch: 1/2, step 4970/7134 completed (loss: 0.13040436804294586, acc: 0.9793103337287903)
[2025-02-13 19:31:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:58][root][INFO] - Training Epoch: 1/2, step 4971/7134 completed (loss: 0.09429992735385895, acc: 0.9735099077224731)
[2025-02-13 19:31:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:58][root][INFO] - Training Epoch: 1/2, step 4972/7134 completed (loss: 0.2585410475730896, acc: 0.9437500238418579)
[2025-02-13 19:31:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:59][root][INFO] - Training Epoch: 1/2, step 4973/7134 completed (loss: 0.13127364218235016, acc: 0.9696969985961914)
[2025-02-13 19:31:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:59][root][INFO] - Training Epoch: 1/2, step 4974/7134 completed (loss: 0.1454416960477829, acc: 0.9621211886405945)
[2025-02-13 19:31:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:31:59][root][INFO] - Training Epoch: 1/2, step 4975/7134 completed (loss: 0.40867361426353455, acc: 0.9103448390960693)
[2025-02-13 19:32:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:00][root][INFO] - Training Epoch: 1/2, step 4976/7134 completed (loss: 0.1867101788520813, acc: 0.9684210419654846)
[2025-02-13 19:32:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:00][root][INFO] - Training Epoch: 1/2, step 4977/7134 completed (loss: 0.3073328733444214, acc: 0.9417475461959839)
[2025-02-13 19:32:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:01][root][INFO] - Training Epoch: 1/2, step 4978/7134 completed (loss: 0.10408040881156921, acc: 0.977011501789093)
[2025-02-13 19:32:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:01][root][INFO] - Training Epoch: 1/2, step 4979/7134 completed (loss: 0.3778955638408661, acc: 0.9230769276618958)
[2025-02-13 19:32:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:01][root][INFO] - Training Epoch: 1/2, step 4980/7134 completed (loss: 0.25309139490127563, acc: 0.9357143044471741)
[2025-02-13 19:32:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:02][root][INFO] - Training Epoch: 1/2, step 4981/7134 completed (loss: 0.19256894290447235, acc: 0.9357143044471741)
[2025-02-13 19:32:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:02][root][INFO] - Training Epoch: 1/2, step 4982/7134 completed (loss: 0.23450955748558044, acc: 0.954954981803894)
[2025-02-13 19:32:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:02][root][INFO] - Training Epoch: 1/2, step 4983/7134 completed (loss: 0.1860538125038147, acc: 0.9640287756919861)
[2025-02-13 19:32:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:03][root][INFO] - Training Epoch: 1/2, step 4984/7134 completed (loss: 0.3495774269104004, acc: 0.9245283007621765)
[2025-02-13 19:32:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:03][root][INFO] - Training Epoch: 1/2, step 4985/7134 completed (loss: 0.28492268919944763, acc: 0.9459459185600281)
[2025-02-13 19:32:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:04][root][INFO] - Training Epoch: 1/2, step 4986/7134 completed (loss: 0.11500059813261032, acc: 0.9790209531784058)
[2025-02-13 19:32:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:04][root][INFO] - Training Epoch: 1/2, step 4987/7134 completed (loss: 0.08869647979736328, acc: 0.9716981053352356)
[2025-02-13 19:32:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:04][root][INFO] - Training Epoch: 1/2, step 4988/7134 completed (loss: 0.0959387868642807, acc: 0.982758641242981)
[2025-02-13 19:32:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:05][root][INFO] - Training Epoch: 1/2, step 4989/7134 completed (loss: 0.1374814510345459, acc: 0.9647058844566345)
[2025-02-13 19:32:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:05][root][INFO] - Training Epoch: 1/2, step 4990/7134 completed (loss: 0.11354978382587433, acc: 0.9743589758872986)
[2025-02-13 19:32:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:06][root][INFO] - Training Epoch: 1/2, step 4991/7134 completed (loss: 0.062283508479595184, acc: 0.9924812316894531)
[2025-02-13 19:32:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:06][root][INFO] - Training Epoch: 1/2, step 4992/7134 completed (loss: 0.17907162010669708, acc: 0.9741379022598267)
[2025-02-13 19:32:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:06][root][INFO] - Training Epoch: 1/2, step 4993/7134 completed (loss: 0.04388829693198204, acc: 1.0)
[2025-02-13 19:32:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:07][root][INFO] - Training Epoch: 1/2, step 4994/7134 completed (loss: 0.06610926240682602, acc: 0.9867549538612366)
[2025-02-13 19:32:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:07][root][INFO] - Training Epoch: 1/2, step 4995/7134 completed (loss: 0.18773400783538818, acc: 0.9814814925193787)
[2025-02-13 19:32:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:07][root][INFO] - Training Epoch: 1/2, step 4996/7134 completed (loss: 0.08231721818447113, acc: 0.9916666746139526)
[2025-02-13 19:32:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:08][root][INFO] - Training Epoch: 1/2, step 4997/7134 completed (loss: 0.10091499984264374, acc: 0.9694656729698181)
[2025-02-13 19:32:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:08][root][INFO] - Training Epoch: 1/2, step 4998/7134 completed (loss: 0.17712821066379547, acc: 0.9466666579246521)
[2025-02-13 19:32:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:09][root][INFO] - Training Epoch: 1/2, step 4999/7134 completed (loss: 0.13261516392230988, acc: 0.9763779640197754)
[2025-02-13 19:32:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:09][root][INFO] - Training Epoch: 1/2, step 5000/7134 completed (loss: 0.11214887350797653, acc: 0.9587628841400146)
[2025-02-13 19:32:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:09][root][INFO] - Training Epoch: 1/2, step 5001/7134 completed (loss: 0.1974838674068451, acc: 0.9659863710403442)
[2025-02-13 19:32:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:10][root][INFO] - Training Epoch: 1/2, step 5002/7134 completed (loss: 0.08541149646043777, acc: 0.9734513163566589)
[2025-02-13 19:32:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:10][root][INFO] - Training Epoch: 1/2, step 5003/7134 completed (loss: 0.06721855700016022, acc: 0.9777777791023254)
[2025-02-13 19:32:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:11][root][INFO] - Training Epoch: 1/2, step 5004/7134 completed (loss: 0.03547004610300064, acc: 1.0)
[2025-02-13 19:32:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:11][root][INFO] - Training Epoch: 1/2, step 5005/7134 completed (loss: 0.18911583721637726, acc: 0.9670329689979553)
[2025-02-13 19:32:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:11][root][INFO] - Training Epoch: 1/2, step 5006/7134 completed (loss: 0.1052747294306755, acc: 0.9671052694320679)
[2025-02-13 19:32:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:12][root][INFO] - Training Epoch: 1/2, step 5007/7134 completed (loss: 0.07184607535600662, acc: 0.9876543283462524)
[2025-02-13 19:32:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:12][root][INFO] - Training Epoch: 1/2, step 5008/7134 completed (loss: 0.2723807096481323, acc: 0.9583333134651184)
[2025-02-13 19:32:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:12][root][INFO] - Training Epoch: 1/2, step 5009/7134 completed (loss: 0.2945787012577057, acc: 0.9175257682800293)
[2025-02-13 19:32:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:13][root][INFO] - Training Epoch: 1/2, step 5010/7134 completed (loss: 0.15797318518161774, acc: 0.9487179517745972)
[2025-02-13 19:32:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:13][root][INFO] - Training Epoch: 1/2, step 5011/7134 completed (loss: 0.14765208959579468, acc: 0.9655172228813171)
[2025-02-13 19:32:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:14][root][INFO] - Training Epoch: 1/2, step 5012/7134 completed (loss: 0.053688496351242065, acc: 1.0)
[2025-02-13 19:32:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:14][root][INFO] - Training Epoch: 1/2, step 5013/7134 completed (loss: 0.06902103126049042, acc: 0.9829059839248657)
[2025-02-13 19:32:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:14][root][INFO] - Training Epoch: 1/2, step 5014/7134 completed (loss: 0.1514529585838318, acc: 0.9807692170143127)
[2025-02-13 19:32:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:15][root][INFO] - Training Epoch: 1/2, step 5015/7134 completed (loss: 0.20887687802314758, acc: 0.9509202241897583)
[2025-02-13 19:32:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:15][root][INFO] - Training Epoch: 1/2, step 5016/7134 completed (loss: 0.3528474271297455, acc: 0.9389312863349915)
[2025-02-13 19:32:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:16][root][INFO] - Training Epoch: 1/2, step 5017/7134 completed (loss: 0.12457691878080368, acc: 0.9772727489471436)
[2025-02-13 19:32:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:16][root][INFO] - Training Epoch: 1/2, step 5018/7134 completed (loss: 0.2762162983417511, acc: 0.9248120188713074)
[2025-02-13 19:32:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:16][root][INFO] - Training Epoch: 1/2, step 5019/7134 completed (loss: 0.45426514744758606, acc: 0.909604549407959)
[2025-02-13 19:32:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:17][root][INFO] - Training Epoch: 1/2, step 5020/7134 completed (loss: 0.396151602268219, acc: 0.9047619104385376)
[2025-02-13 19:32:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:17][root][INFO] - Training Epoch: 1/2, step 5021/7134 completed (loss: 0.14826428890228271, acc: 0.9809523820877075)
[2025-02-13 19:32:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:18][root][INFO] - Training Epoch: 1/2, step 5022/7134 completed (loss: 0.08591951429843903, acc: 0.9927536249160767)
[2025-02-13 19:32:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:18][root][INFO] - Training Epoch: 1/2, step 5023/7134 completed (loss: 0.12808802723884583, acc: 0.9677419066429138)
[2025-02-13 19:32:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:18][root][INFO] - Training Epoch: 1/2, step 5024/7134 completed (loss: 0.07060060650110245, acc: 0.9819276928901672)
[2025-02-13 19:32:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:19][root][INFO] - Training Epoch: 1/2, step 5025/7134 completed (loss: 0.18461713194847107, acc: 0.9615384340286255)
[2025-02-13 19:32:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:19][root][INFO] - Training Epoch: 1/2, step 5026/7134 completed (loss: 0.12449855357408524, acc: 0.9694656729698181)
[2025-02-13 19:32:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:19][root][INFO] - Training Epoch: 1/2, step 5027/7134 completed (loss: 0.09711367636919022, acc: 0.9767441749572754)
[2025-02-13 19:32:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:20][root][INFO] - Training Epoch: 1/2, step 5028/7134 completed (loss: 0.11341801285743713, acc: 0.9718309640884399)
[2025-02-13 19:32:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:20][root][INFO] - Training Epoch: 1/2, step 5029/7134 completed (loss: 0.08968909084796906, acc: 0.9769230484962463)
[2025-02-13 19:32:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:21][root][INFO] - Training Epoch: 1/2, step 5030/7134 completed (loss: 0.14764131605625153, acc: 0.9640718698501587)
[2025-02-13 19:32:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:21][root][INFO] - Training Epoch: 1/2, step 5031/7134 completed (loss: 0.3831365406513214, acc: 0.9420289993286133)
[2025-02-13 19:32:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:21][root][INFO] - Training Epoch: 1/2, step 5032/7134 completed (loss: 0.08764441311359406, acc: 0.98591548204422)
[2025-02-13 19:32:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:22][root][INFO] - Training Epoch: 1/2, step 5033/7134 completed (loss: 0.05029461905360222, acc: 0.98591548204422)
[2025-02-13 19:32:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:22][root][INFO] - Training Epoch: 1/2, step 5034/7134 completed (loss: 0.0647122785449028, acc: 0.9924812316894531)
[2025-02-13 19:32:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:23][root][INFO] - Training Epoch: 1/2, step 5035/7134 completed (loss: 0.01424302440136671, acc: 1.0)
[2025-02-13 19:32:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:23][root][INFO] - Training Epoch: 1/2, step 5036/7134 completed (loss: 0.22234898805618286, acc: 0.9615384340286255)
[2025-02-13 19:32:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:23][root][INFO] - Training Epoch: 1/2, step 5037/7134 completed (loss: 0.124130979180336, acc: 0.9647058844566345)
[2025-02-13 19:32:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:24][root][INFO] - Training Epoch: 1/2, step 5038/7134 completed (loss: 0.5798291563987732, acc: 0.895652174949646)
[2025-02-13 19:32:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:24][root][INFO] - Training Epoch: 1/2, step 5039/7134 completed (loss: 0.1520785242319107, acc: 0.9645389914512634)
[2025-02-13 19:32:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:24][root][INFO] - Training Epoch: 1/2, step 5040/7134 completed (loss: 0.1661217212677002, acc: 0.9750000238418579)
[2025-02-13 19:32:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:25][root][INFO] - Training Epoch: 1/2, step 5041/7134 completed (loss: 0.041228149086236954, acc: 0.9941860437393188)
[2025-02-13 19:32:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:25][root][INFO] - Training Epoch: 1/2, step 5042/7134 completed (loss: 0.3499131202697754, acc: 0.9417475461959839)
[2025-02-13 19:32:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:26][root][INFO] - Training Epoch: 1/2, step 5043/7134 completed (loss: 0.21338345110416412, acc: 0.9203540086746216)
[2025-02-13 19:32:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:26][root][INFO] - Training Epoch: 1/2, step 5044/7134 completed (loss: 0.13835081458091736, acc: 0.9617834687232971)
[2025-02-13 19:32:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:26][root][INFO] - Training Epoch: 1/2, step 5045/7134 completed (loss: 0.2294570654630661, acc: 0.9513513445854187)
[2025-02-13 19:32:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:27][root][INFO] - Training Epoch: 1/2, step 5046/7134 completed (loss: 0.29894837737083435, acc: 0.9516128897666931)
[2025-02-13 19:32:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:27][root][INFO] - Training Epoch: 1/2, step 5047/7134 completed (loss: 0.18248283863067627, acc: 0.9489051103591919)
[2025-02-13 19:32:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:27][root][INFO] - Training Epoch: 1/2, step 5048/7134 completed (loss: 0.22202925384044647, acc: 0.9642857313156128)
[2025-02-13 19:32:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:28][root][INFO] - Training Epoch: 1/2, step 5049/7134 completed (loss: 0.263519823551178, acc: 0.9235668778419495)
[2025-02-13 19:32:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:28][root][INFO] - Training Epoch: 1/2, step 5050/7134 completed (loss: 0.29882416129112244, acc: 0.96875)
[2025-02-13 19:32:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:29][root][INFO] - Training Epoch: 1/2, step 5051/7134 completed (loss: 0.24996371567249298, acc: 0.9387755393981934)
[2025-02-13 19:32:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:29][root][INFO] - Training Epoch: 1/2, step 5052/7134 completed (loss: 0.21804305911064148, acc: 0.942307710647583)
[2025-02-13 19:32:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:29][root][INFO] - Training Epoch: 1/2, step 5053/7134 completed (loss: 0.36871030926704407, acc: 0.925000011920929)
[2025-02-13 19:32:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:30][root][INFO] - Training Epoch: 1/2, step 5054/7134 completed (loss: 0.23155906796455383, acc: 0.9259259104728699)
[2025-02-13 19:32:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:30][root][INFO] - Training Epoch: 1/2, step 5055/7134 completed (loss: 0.2864018976688385, acc: 0.9292035102844238)
[2025-02-13 19:32:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:31][root][INFO] - Training Epoch: 1/2, step 5056/7134 completed (loss: 0.23053810000419617, acc: 0.9395973086357117)
[2025-02-13 19:32:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:31][root][INFO] - Training Epoch: 1/2, step 5057/7134 completed (loss: 0.3188660740852356, acc: 0.9363057613372803)
[2025-02-13 19:32:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:31][root][INFO] - Training Epoch: 1/2, step 5058/7134 completed (loss: 0.12509936094284058, acc: 0.9634146094322205)
[2025-02-13 19:32:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:32][root][INFO] - Training Epoch: 1/2, step 5059/7134 completed (loss: 0.14664383232593536, acc: 0.9580838084220886)
[2025-02-13 19:32:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:32][root][INFO] - Training Epoch: 1/2, step 5060/7134 completed (loss: 0.170597642660141, acc: 0.9733333587646484)
[2025-02-13 19:32:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:33][root][INFO] - Training Epoch: 1/2, step 5061/7134 completed (loss: 0.294178307056427, acc: 0.9238095283508301)
[2025-02-13 19:32:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:33][root][INFO] - Training Epoch: 1/2, step 5062/7134 completed (loss: 0.20023313164710999, acc: 0.9407407641410828)
[2025-02-13 19:32:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:33][root][INFO] - Training Epoch: 1/2, step 5063/7134 completed (loss: 0.1533571183681488, acc: 0.9580419659614563)
[2025-02-13 19:32:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:34][root][INFO] - Training Epoch: 1/2, step 5064/7134 completed (loss: 0.11415358632802963, acc: 0.9642857313156128)
[2025-02-13 19:32:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:34][root][INFO] - Training Epoch: 1/2, step 5065/7134 completed (loss: 0.2071666568517685, acc: 0.9496855139732361)
[2025-02-13 19:32:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:35][root][INFO] - Training Epoch: 1/2, step 5066/7134 completed (loss: 0.17867009341716766, acc: 0.9655172228813171)
[2025-02-13 19:32:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:35][root][INFO] - Training Epoch: 1/2, step 5067/7134 completed (loss: 0.07738002389669418, acc: 0.9918032884597778)
[2025-02-13 19:32:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:35][root][INFO] - Training Epoch: 1/2, step 5068/7134 completed (loss: 0.21733559668064117, acc: 0.9366196990013123)
[2025-02-13 19:32:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:36][root][INFO] - Training Epoch: 1/2, step 5069/7134 completed (loss: 0.1083933487534523, acc: 0.9797297120094299)
[2025-02-13 19:32:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:36][root][INFO] - Training Epoch: 1/2, step 5070/7134 completed (loss: 0.04029393568634987, acc: 0.9878048896789551)
[2025-02-13 19:32:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:36][root][INFO] - Training Epoch: 1/2, step 5071/7134 completed (loss: 0.1851808726787567, acc: 0.9588235020637512)
[2025-02-13 19:32:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:37][root][INFO] - Training Epoch: 1/2, step 5072/7134 completed (loss: 0.17983411252498627, acc: 0.9450549483299255)
[2025-02-13 19:32:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:37][root][INFO] - Training Epoch: 1/2, step 5073/7134 completed (loss: 0.1610325574874878, acc: 0.9465649127960205)
[2025-02-13 19:32:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:38][root][INFO] - Training Epoch: 1/2, step 5074/7134 completed (loss: 0.16636402904987335, acc: 0.9457364082336426)
[2025-02-13 19:32:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:38][root][INFO] - Training Epoch: 1/2, step 5075/7134 completed (loss: 0.18503132462501526, acc: 0.939130425453186)
[2025-02-13 19:32:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:38][root][INFO] - Training Epoch: 1/2, step 5076/7134 completed (loss: 0.17418649792671204, acc: 0.9569892287254333)
[2025-02-13 19:32:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:39][root][INFO] - Training Epoch: 1/2, step 5077/7134 completed (loss: 0.12446563690900803, acc: 0.9632353186607361)
[2025-02-13 19:32:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:39][root][INFO] - Training Epoch: 1/2, step 5078/7134 completed (loss: 0.35043323040008545, acc: 0.925000011920929)
[2025-02-13 19:32:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:40][root][INFO] - Training Epoch: 1/2, step 5079/7134 completed (loss: 0.14879511296749115, acc: 0.949999988079071)
[2025-02-13 19:32:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:40][root][INFO] - Training Epoch: 1/2, step 5080/7134 completed (loss: 0.08851303160190582, acc: 0.9656862616539001)
[2025-02-13 19:32:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:40][root][INFO] - Training Epoch: 1/2, step 5081/7134 completed (loss: 0.10753325372934341, acc: 0.9653179049491882)
[2025-02-13 19:32:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:41][root][INFO] - Training Epoch: 1/2, step 5082/7134 completed (loss: 0.12928597629070282, acc: 0.9655172228813171)
[2025-02-13 19:32:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:41][root][INFO] - Training Epoch: 1/2, step 5083/7134 completed (loss: 0.19142544269561768, acc: 0.9585798978805542)
[2025-02-13 19:32:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:41][root][INFO] - Training Epoch: 1/2, step 5084/7134 completed (loss: 0.10239089280366898, acc: 0.9653465151786804)
[2025-02-13 19:32:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:42][root][INFO] - Training Epoch: 1/2, step 5085/7134 completed (loss: 0.1548854559659958, acc: 0.9484536051750183)
[2025-02-13 19:32:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:42][root][INFO] - Training Epoch: 1/2, step 5086/7134 completed (loss: 0.1256304532289505, acc: 0.9613259434700012)
[2025-02-13 19:32:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:43][root][INFO] - Training Epoch: 1/2, step 5087/7134 completed (loss: 0.07964435964822769, acc: 0.9783783555030823)
[2025-02-13 19:32:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:43][root][INFO] - Training Epoch: 1/2, step 5088/7134 completed (loss: 0.02983754128217697, acc: 1.0)
[2025-02-13 19:32:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:43][root][INFO] - Training Epoch: 1/2, step 5089/7134 completed (loss: 0.10179910808801651, acc: 0.9846938848495483)
[2025-02-13 19:32:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:44][root][INFO] - Training Epoch: 1/2, step 5090/7134 completed (loss: 0.074999138712883, acc: 0.9805825352668762)
[2025-02-13 19:32:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:44][root][INFO] - Training Epoch: 1/2, step 5091/7134 completed (loss: 0.13588660955429077, acc: 0.9670329689979553)
[2025-02-13 19:32:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:45][root][INFO] - Training Epoch: 1/2, step 5092/7134 completed (loss: 0.11475528031587601, acc: 0.97826087474823)
[2025-02-13 19:32:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:45][root][INFO] - Training Epoch: 1/2, step 5093/7134 completed (loss: 0.13950587809085846, acc: 0.9602272510528564)
[2025-02-13 19:32:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:45][root][INFO] - Training Epoch: 1/2, step 5094/7134 completed (loss: 0.08690577000379562, acc: 0.9923076629638672)
[2025-02-13 19:32:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:46][root][INFO] - Training Epoch: 1/2, step 5095/7134 completed (loss: 0.36641445755958557, acc: 0.9044585824012756)
[2025-02-13 19:32:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:46][root][INFO] - Training Epoch: 1/2, step 5096/7134 completed (loss: 0.18788129091262817, acc: 0.9627659320831299)
[2025-02-13 19:32:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:46][root][INFO] - Training Epoch: 1/2, step 5097/7134 completed (loss: 0.13069717586040497, acc: 0.9625668525695801)
[2025-02-13 19:32:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:47][root][INFO] - Training Epoch: 1/2, step 5098/7134 completed (loss: 0.22653095424175262, acc: 0.9438775777816772)
[2025-02-13 19:32:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:47][root][INFO] - Training Epoch: 1/2, step 5099/7134 completed (loss: 0.15791670978069305, acc: 0.9523809552192688)
[2025-02-13 19:32:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:48][root][INFO] - Training Epoch: 1/2, step 5100/7134 completed (loss: 0.14346538484096527, acc: 0.9558823704719543)
[2025-02-13 19:32:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:48][root][INFO] - Training Epoch: 1/2, step 5101/7134 completed (loss: 0.08725661784410477, acc: 0.9842932224273682)
[2025-02-13 19:32:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:48][root][INFO] - Training Epoch: 1/2, step 5102/7134 completed (loss: 0.18214255571365356, acc: 0.9512194991111755)
[2025-02-13 19:32:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:49][root][INFO] - Training Epoch: 1/2, step 5103/7134 completed (loss: 0.15275144577026367, acc: 0.961904764175415)
[2025-02-13 19:32:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:49][root][INFO] - Training Epoch: 1/2, step 5104/7134 completed (loss: 0.13054785132408142, acc: 0.9736841917037964)
[2025-02-13 19:32:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:50][root][INFO] - Training Epoch: 1/2, step 5105/7134 completed (loss: 0.21374092996120453, acc: 0.9470198750495911)
[2025-02-13 19:32:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:50][root][INFO] - Training Epoch: 1/2, step 5106/7134 completed (loss: 0.34790557622909546, acc: 0.931506872177124)
[2025-02-13 19:32:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:50][root][INFO] - Training Epoch: 1/2, step 5107/7134 completed (loss: 0.39787012338638306, acc: 0.918367326259613)
[2025-02-13 19:32:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:51][root][INFO] - Training Epoch: 1/2, step 5108/7134 completed (loss: 0.5051538944244385, acc: 0.8794326186180115)
[2025-02-13 19:32:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:51][root][INFO] - Training Epoch: 1/2, step 5109/7134 completed (loss: 0.5071943998336792, acc: 0.8894472122192383)
[2025-02-13 19:32:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:52][root][INFO] - Training Epoch: 1/2, step 5110/7134 completed (loss: 0.280307799577713, acc: 0.9395604133605957)
[2025-02-13 19:32:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:52][root][INFO] - Training Epoch: 1/2, step 5111/7134 completed (loss: 0.23678885400295258, acc: 0.9367815852165222)
[2025-02-13 19:32:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:52][root][INFO] - Training Epoch: 1/2, step 5112/7134 completed (loss: 0.2975583076477051, acc: 0.9243243336677551)
[2025-02-13 19:32:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:53][root][INFO] - Training Epoch: 1/2, step 5113/7134 completed (loss: 0.2277785986661911, acc: 0.9470198750495911)
[2025-02-13 19:32:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:53][root][INFO] - Training Epoch: 1/2, step 5114/7134 completed (loss: 0.2375389039516449, acc: 0.9337748289108276)
[2025-02-13 19:32:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:54][root][INFO] - Training Epoch: 1/2, step 5115/7134 completed (loss: 0.1308591365814209, acc: 0.9523809552192688)
[2025-02-13 19:32:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:54][root][INFO] - Training Epoch: 1/2, step 5116/7134 completed (loss: 0.13608191907405853, acc: 0.9576719403266907)
[2025-02-13 19:32:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:54][root][INFO] - Training Epoch: 1/2, step 5117/7134 completed (loss: 0.11906246840953827, acc: 0.9779005646705627)
[2025-02-13 19:32:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:55][root][INFO] - Training Epoch: 1/2, step 5118/7134 completed (loss: 0.18333111703395844, acc: 0.9457831382751465)
[2025-02-13 19:32:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:55][root][INFO] - Training Epoch: 1/2, step 5119/7134 completed (loss: 0.13787966966629028, acc: 0.982758641242981)
[2025-02-13 19:32:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:55][root][INFO] - Training Epoch: 1/2, step 5120/7134 completed (loss: 0.1962788701057434, acc: 0.9576719403266907)
[2025-02-13 19:32:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:56][root][INFO] - Training Epoch: 1/2, step 5121/7134 completed (loss: 0.13230888545513153, acc: 0.9757575988769531)
[2025-02-13 19:32:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:56][root][INFO] - Training Epoch: 1/2, step 5122/7134 completed (loss: 0.26757222414016724, acc: 0.9418604373931885)
[2025-02-13 19:32:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:57][root][INFO] - Training Epoch: 1/2, step 5123/7134 completed (loss: 0.11488749831914902, acc: 0.9752475023269653)
[2025-02-13 19:32:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:57][root][INFO] - Training Epoch: 1/2, step 5124/7134 completed (loss: 0.2440836876630783, acc: 0.935960590839386)
[2025-02-13 19:32:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:57][root][INFO] - Training Epoch: 1/2, step 5125/7134 completed (loss: 0.09576544910669327, acc: 0.9803921580314636)
[2025-02-13 19:32:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:58][root][INFO] - Training Epoch: 1/2, step 5126/7134 completed (loss: 0.1799992471933365, acc: 0.9379310607910156)
[2025-02-13 19:32:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:58][root][INFO] - Training Epoch: 1/2, step 5127/7134 completed (loss: 0.38830965757369995, acc: 0.9252873659133911)
[2025-02-13 19:32:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:58][root][INFO] - Training Epoch: 1/2, step 5128/7134 completed (loss: 0.12206947058439255, acc: 0.9784482717514038)
[2025-02-13 19:32:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:59][root][INFO] - Training Epoch: 1/2, step 5129/7134 completed (loss: 0.31311431527137756, acc: 0.9457364082336426)
[2025-02-13 19:32:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:32:59][root][INFO] - Training Epoch: 1/2, step 5130/7134 completed (loss: 0.1460759937763214, acc: 0.9675324559211731)
[2025-02-13 19:32:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:00][root][INFO] - Training Epoch: 1/2, step 5131/7134 completed (loss: 0.34800249338150024, acc: 0.9294871687889099)
[2025-02-13 19:33:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:00][root][INFO] - Training Epoch: 1/2, step 5132/7134 completed (loss: 0.24681782722473145, acc: 0.9375)
[2025-02-13 19:33:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:00][root][INFO] - Training Epoch: 1/2, step 5133/7134 completed (loss: 0.2864055633544922, acc: 0.9268292784690857)
[2025-02-13 19:33:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:01][root][INFO] - Training Epoch: 1/2, step 5134/7134 completed (loss: 0.07295700907707214, acc: 0.9855072498321533)
[2025-02-13 19:33:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:01][root][INFO] - Training Epoch: 1/2, step 5135/7134 completed (loss: 0.22199556231498718, acc: 0.9768785834312439)
[2025-02-13 19:33:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:01][root][INFO] - Training Epoch: 1/2, step 5136/7134 completed (loss: 0.15347926318645477, acc: 0.9623655676841736)
[2025-02-13 19:33:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:02][root][INFO] - Training Epoch: 1/2, step 5137/7134 completed (loss: 0.37068429589271545, acc: 0.9347826242446899)
[2025-02-13 19:33:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:02][root][INFO] - Training Epoch: 1/2, step 5138/7134 completed (loss: 0.3058421313762665, acc: 0.936170220375061)
[2025-02-13 19:33:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:03][root][INFO] - Training Epoch: 1/2, step 5139/7134 completed (loss: 0.18301044404506683, acc: 0.9588235020637512)
[2025-02-13 19:33:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:03][root][INFO] - Training Epoch: 1/2, step 5140/7134 completed (loss: 0.22333122789859772, acc: 0.9541666507720947)
[2025-02-13 19:33:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:03][root][INFO] - Training Epoch: 1/2, step 5141/7134 completed (loss: 0.22541002929210663, acc: 0.9672130942344666)
[2025-02-13 19:33:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:04][root][INFO] - Training Epoch: 1/2, step 5142/7134 completed (loss: 0.09580957889556885, acc: 0.9850746393203735)
[2025-02-13 19:33:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:04][root][INFO] - Training Epoch: 1/2, step 5143/7134 completed (loss: 0.16030889749526978, acc: 0.9502487778663635)
[2025-02-13 19:33:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:05][root][INFO] - Training Epoch: 1/2, step 5144/7134 completed (loss: 0.11908909678459167, acc: 0.9682539701461792)
[2025-02-13 19:33:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:05][root][INFO] - Training Epoch: 1/2, step 5145/7134 completed (loss: 0.12389383465051651, acc: 0.96875)
[2025-02-13 19:33:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:05][root][INFO] - Training Epoch: 1/2, step 5146/7134 completed (loss: 0.24386373162269592, acc: 0.9453551769256592)
[2025-02-13 19:33:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:06][root][INFO] - Training Epoch: 1/2, step 5147/7134 completed (loss: 0.10006802529096603, acc: 0.9679487347602844)
[2025-02-13 19:33:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:06][root][INFO] - Training Epoch: 1/2, step 5148/7134 completed (loss: 0.2986641824245453, acc: 0.9375)
[2025-02-13 19:33:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:07][root][INFO] - Training Epoch: 1/2, step 5149/7134 completed (loss: 0.14775343239307404, acc: 0.959276020526886)
[2025-02-13 19:33:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:07][root][INFO] - Training Epoch: 1/2, step 5150/7134 completed (loss: 0.2472531646490097, acc: 0.9341317415237427)
[2025-02-13 19:33:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:07][root][INFO] - Training Epoch: 1/2, step 5151/7134 completed (loss: 0.18987871706485748, acc: 0.9599999785423279)
[2025-02-13 19:33:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:08][root][INFO] - Training Epoch: 1/2, step 5152/7134 completed (loss: 0.13209445774555206, acc: 0.9647058844566345)
[2025-02-13 19:33:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:08][root][INFO] - Training Epoch: 1/2, step 5153/7134 completed (loss: 0.14805428683757782, acc: 0.9673202633857727)
[2025-02-13 19:33:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:09][root][INFO] - Training Epoch: 1/2, step 5154/7134 completed (loss: 0.07754631340503693, acc: 0.9750000238418579)
[2025-02-13 19:33:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:09][root][INFO] - Training Epoch: 1/2, step 5155/7134 completed (loss: 0.23248660564422607, acc: 0.9489051103591919)
[2025-02-13 19:33:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:09][root][INFO] - Training Epoch: 1/2, step 5156/7134 completed (loss: 0.1525457203388214, acc: 0.9768518805503845)
[2025-02-13 19:33:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:10][root][INFO] - Training Epoch: 1/2, step 5157/7134 completed (loss: 0.20086072385311127, acc: 0.9488636255264282)
[2025-02-13 19:33:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:10][root][INFO] - Training Epoch: 1/2, step 5158/7134 completed (loss: 0.336478590965271, acc: 0.8974359035491943)
[2025-02-13 19:33:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:10][root][INFO] - Training Epoch: 1/2, step 5159/7134 completed (loss: 0.2859056890010834, acc: 0.9242424368858337)
[2025-02-13 19:33:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:11][root][INFO] - Training Epoch: 1/2, step 5160/7134 completed (loss: 0.2620031237602234, acc: 0.9222221970558167)
[2025-02-13 19:33:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:11][root][INFO] - Training Epoch: 1/2, step 5161/7134 completed (loss: 0.4227740466594696, acc: 0.9086021780967712)
[2025-02-13 19:33:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:12][root][INFO] - Training Epoch: 1/2, step 5162/7134 completed (loss: 0.29824694991111755, acc: 0.9257143139839172)
[2025-02-13 19:33:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:12][root][INFO] - Training Epoch: 1/2, step 5163/7134 completed (loss: 0.2239713817834854, acc: 0.9569892287254333)
[2025-02-13 19:33:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:12][root][INFO] - Training Epoch: 1/2, step 5164/7134 completed (loss: 0.10459187626838684, acc: 0.9806451797485352)
[2025-02-13 19:33:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:13][root][INFO] - Training Epoch: 1/2, step 5165/7134 completed (loss: 0.17957843840122223, acc: 0.9575757384300232)
[2025-02-13 19:33:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:13][root][INFO] - Training Epoch: 1/2, step 5166/7134 completed (loss: 0.2109660804271698, acc: 0.9259259104728699)
[2025-02-13 19:33:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:14][root][INFO] - Training Epoch: 1/2, step 5167/7134 completed (loss: 0.28187182545661926, acc: 0.939226508140564)
[2025-02-13 19:33:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:14][root][INFO] - Training Epoch: 1/2, step 5168/7134 completed (loss: 0.3729008436203003, acc: 0.9244186282157898)
[2025-02-13 19:33:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:14][root][INFO] - Training Epoch: 1/2, step 5169/7134 completed (loss: 0.27679678797721863, acc: 0.9451219439506531)
[2025-02-13 19:33:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:15][root][INFO] - Training Epoch: 1/2, step 5170/7134 completed (loss: 0.42948949337005615, acc: 0.9067357778549194)
[2025-02-13 19:33:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:15][root][INFO] - Training Epoch: 1/2, step 5171/7134 completed (loss: 0.21890580654144287, acc: 0.9430894255638123)
[2025-02-13 19:33:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:15][root][INFO] - Training Epoch: 1/2, step 5172/7134 completed (loss: 0.5906054377555847, acc: 0.8658536672592163)
[2025-02-13 19:33:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:16][root][INFO] - Training Epoch: 1/2, step 5173/7134 completed (loss: 0.18183815479278564, acc: 0.9570552110671997)
[2025-02-13 19:33:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:16][root][INFO] - Training Epoch: 1/2, step 5174/7134 completed (loss: 0.15336784720420837, acc: 0.9595375657081604)
[2025-02-13 19:33:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:17][root][INFO] - Training Epoch: 1/2, step 5175/7134 completed (loss: 0.203853577375412, acc: 0.946107804775238)
[2025-02-13 19:33:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:17][root][INFO] - Training Epoch: 1/2, step 5176/7134 completed (loss: 0.1692247837781906, acc: 0.9473684430122375)
[2025-02-13 19:33:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:17][root][INFO] - Training Epoch: 1/2, step 5177/7134 completed (loss: 0.3099916875362396, acc: 0.9466666579246521)
[2025-02-13 19:33:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:18][root][INFO] - Training Epoch: 1/2, step 5178/7134 completed (loss: 0.3887769281864166, acc: 0.9266666769981384)
[2025-02-13 19:33:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:18][root][INFO] - Training Epoch: 1/2, step 5179/7134 completed (loss: 0.27939772605895996, acc: 0.9285714030265808)
[2025-02-13 19:33:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:19][root][INFO] - Training Epoch: 1/2, step 5180/7134 completed (loss: 0.3141483962535858, acc: 0.9078013896942139)
[2025-02-13 19:33:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:19][root][INFO] - Training Epoch: 1/2, step 5181/7134 completed (loss: 0.6008960008621216, acc: 0.898876428604126)
[2025-02-13 19:33:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:19][root][INFO] - Training Epoch: 1/2, step 5182/7134 completed (loss: 0.8550878763198853, acc: 0.8267716765403748)
[2025-02-13 19:33:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:20][root][INFO] - Training Epoch: 1/2, step 5183/7134 completed (loss: 0.29717475175857544, acc: 0.9558823704719543)
[2025-02-13 19:33:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:20][root][INFO] - Training Epoch: 1/2, step 5184/7134 completed (loss: 0.12555530667304993, acc: 0.9711538553237915)
[2025-02-13 19:33:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:21][root][INFO] - Training Epoch: 1/2, step 5185/7134 completed (loss: 0.19380329549312592, acc: 0.9702970385551453)
[2025-02-13 19:33:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:21][root][INFO] - Training Epoch: 1/2, step 5186/7134 completed (loss: 0.19132007658481598, acc: 0.960629940032959)
[2025-02-13 19:33:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:21][root][INFO] - Training Epoch: 1/2, step 5187/7134 completed (loss: 0.07179256528615952, acc: 0.9919999837875366)
[2025-02-13 19:33:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:22][root][INFO] - Training Epoch: 1/2, step 5188/7134 completed (loss: 0.3126317858695984, acc: 0.9130434989929199)
[2025-02-13 19:33:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:22][root][INFO] - Training Epoch: 1/2, step 5189/7134 completed (loss: 0.26086950302124023, acc: 0.9603960514068604)
[2025-02-13 19:33:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:22][root][INFO] - Training Epoch: 1/2, step 5190/7134 completed (loss: 0.10949191451072693, acc: 0.9727272987365723)
[2025-02-13 19:33:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:23][root][INFO] - Training Epoch: 1/2, step 5191/7134 completed (loss: 0.3462370038032532, acc: 0.9135802388191223)
[2025-02-13 19:33:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:23][root][INFO] - Training Epoch: 1/2, step 5192/7134 completed (loss: 0.21733002364635468, acc: 0.9696969985961914)
[2025-02-13 19:33:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:24][root][INFO] - Training Epoch: 1/2, step 5193/7134 completed (loss: 0.11620991677045822, acc: 0.9750000238418579)
[2025-02-13 19:33:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:24][root][INFO] - Training Epoch: 1/2, step 5194/7134 completed (loss: 0.3449900448322296, acc: 0.9108280539512634)
[2025-02-13 19:33:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:24][root][INFO] - Training Epoch: 1/2, step 5195/7134 completed (loss: 0.37412458658218384, acc: 0.9074074029922485)
[2025-02-13 19:33:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:25][root][INFO] - Training Epoch: 1/2, step 5196/7134 completed (loss: 0.34386155009269714, acc: 0.949999988079071)
[2025-02-13 19:33:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:25][root][INFO] - Training Epoch: 1/2, step 5197/7134 completed (loss: 0.18791262805461884, acc: 0.9365079402923584)
[2025-02-13 19:33:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:26][root][INFO] - Training Epoch: 1/2, step 5198/7134 completed (loss: 0.23598742485046387, acc: 0.9426229596138)
[2025-02-13 19:33:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:26][root][INFO] - Training Epoch: 1/2, step 5199/7134 completed (loss: 0.4083813428878784, acc: 0.8837209343910217)
[2025-02-13 19:33:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:26][root][INFO] - Training Epoch: 1/2, step 5200/7134 completed (loss: 0.16583940386772156, acc: 0.9444444179534912)
[2025-02-13 19:33:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:27][root][INFO] - Training Epoch: 1/2, step 5201/7134 completed (loss: 0.33371254801750183, acc: 0.9275362491607666)
[2025-02-13 19:33:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:27][root][INFO] - Training Epoch: 1/2, step 5202/7134 completed (loss: 0.27587538957595825, acc: 0.9375)
[2025-02-13 19:33:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:27][root][INFO] - Training Epoch: 1/2, step 5203/7134 completed (loss: 0.11352285742759705, acc: 0.9725274443626404)
[2025-02-13 19:33:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:28][root][INFO] - Training Epoch: 1/2, step 5204/7134 completed (loss: 0.06864036619663239, acc: 1.0)
[2025-02-13 19:33:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:28][root][INFO] - Training Epoch: 1/2, step 5205/7134 completed (loss: 0.16959567368030548, acc: 0.9638554453849792)
[2025-02-13 19:33:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:29][root][INFO] - Training Epoch: 1/2, step 5206/7134 completed (loss: 0.138326495885849, acc: 0.9766082167625427)
[2025-02-13 19:33:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:29][root][INFO] - Training Epoch: 1/2, step 5207/7134 completed (loss: 0.6758878827095032, acc: 0.8809523582458496)
[2025-02-13 19:33:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:29][root][INFO] - Training Epoch: 1/2, step 5208/7134 completed (loss: 0.19054915010929108, acc: 0.959770143032074)
[2025-02-13 19:33:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:30][root][INFO] - Training Epoch: 1/2, step 5209/7134 completed (loss: 0.23252247273921967, acc: 0.932330846786499)
[2025-02-13 19:33:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:30][root][INFO] - Training Epoch: 1/2, step 5210/7134 completed (loss: 0.1072167158126831, acc: 0.9704142212867737)
[2025-02-13 19:33:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:31][root][INFO] - Training Epoch: 1/2, step 5211/7134 completed (loss: 0.170606330037117, acc: 0.9603174328804016)
[2025-02-13 19:33:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:31][root][INFO] - Training Epoch: 1/2, step 5212/7134 completed (loss: 0.3095455765724182, acc: 0.9220778942108154)
[2025-02-13 19:33:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:31][root][INFO] - Training Epoch: 1/2, step 5213/7134 completed (loss: 0.20975901186466217, acc: 0.9629629850387573)
[2025-02-13 19:33:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:32][root][INFO] - Training Epoch: 1/2, step 5214/7134 completed (loss: 0.4113743305206299, acc: 0.9159663915634155)
[2025-02-13 19:33:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:32][root][INFO] - Training Epoch: 1/2, step 5215/7134 completed (loss: 0.44258710741996765, acc: 0.9108911156654358)
[2025-02-13 19:33:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:32][root][INFO] - Training Epoch: 1/2, step 5216/7134 completed (loss: 0.310278058052063, acc: 0.9027777910232544)
[2025-02-13 19:33:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:33][root][INFO] - Training Epoch: 1/2, step 5217/7134 completed (loss: 0.41698697209358215, acc: 0.9021739363670349)
[2025-02-13 19:33:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:33][root][INFO] - Training Epoch: 1/2, step 5218/7134 completed (loss: 0.22902077436447144, acc: 0.9405940771102905)
[2025-02-13 19:33:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:34][root][INFO] - Training Epoch: 1/2, step 5219/7134 completed (loss: 0.22415149211883545, acc: 0.9537037014961243)
[2025-02-13 19:33:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:34][root][INFO] - Training Epoch: 1/2, step 5220/7134 completed (loss: 0.1869070678949356, acc: 0.931506872177124)
[2025-02-13 19:33:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:34][root][INFO] - Training Epoch: 1/2, step 5221/7134 completed (loss: 0.15502993762493134, acc: 0.9753086566925049)
[2025-02-13 19:33:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:35][root][INFO] - Training Epoch: 1/2, step 5222/7134 completed (loss: 0.17624062299728394, acc: 0.9404761791229248)
[2025-02-13 19:33:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:35][root][INFO] - Training Epoch: 1/2, step 5223/7134 completed (loss: 0.139150932431221, acc: 0.9646017551422119)
[2025-02-13 19:33:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:36][root][INFO] - Training Epoch: 1/2, step 5224/7134 completed (loss: 0.22741059958934784, acc: 0.9407407641410828)
[2025-02-13 19:33:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:36][root][INFO] - Training Epoch: 1/2, step 5225/7134 completed (loss: 0.19767561554908752, acc: 0.9671052694320679)
[2025-02-13 19:33:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:36][root][INFO] - Training Epoch: 1/2, step 5226/7134 completed (loss: 0.25088801980018616, acc: 0.9551281929016113)
[2025-02-13 19:33:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:37][root][INFO] - Training Epoch: 1/2, step 5227/7134 completed (loss: 0.20410308241844177, acc: 0.9435897469520569)
[2025-02-13 19:33:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:37][root][INFO] - Training Epoch: 1/2, step 5228/7134 completed (loss: 0.11101555079221725, acc: 0.9714285731315613)
[2025-02-13 19:33:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:37][root][INFO] - Training Epoch: 1/2, step 5229/7134 completed (loss: 0.05803285911679268, acc: 1.0)
[2025-02-13 19:33:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:38][root][INFO] - Training Epoch: 1/2, step 5230/7134 completed (loss: 0.06324706971645355, acc: 1.0)
[2025-02-13 19:33:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:38][root][INFO] - Training Epoch: 1/2, step 5231/7134 completed (loss: 0.10344278067350388, acc: 0.9793814420700073)
[2025-02-13 19:33:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:39][root][INFO] - Training Epoch: 1/2, step 5232/7134 completed (loss: 0.10597110539674759, acc: 0.9717513918876648)
[2025-02-13 19:33:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:39][root][INFO] - Training Epoch: 1/2, step 5233/7134 completed (loss: 0.13450872898101807, acc: 0.9770641922950745)
[2025-02-13 19:33:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:39][root][INFO] - Training Epoch: 1/2, step 5234/7134 completed (loss: 0.11094452440738678, acc: 0.9736841917037964)
[2025-02-13 19:33:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:40][root][INFO] - Training Epoch: 1/2, step 5235/7134 completed (loss: 0.07343762367963791, acc: 0.9797297120094299)
[2025-02-13 19:33:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:40][root][INFO] - Training Epoch: 1/2, step 5236/7134 completed (loss: 0.06280911713838577, acc: 0.985401451587677)
[2025-02-13 19:33:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:41][root][INFO] - Training Epoch: 1/2, step 5237/7134 completed (loss: 0.05850612744688988, acc: 0.9929577708244324)
[2025-02-13 19:33:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:41][root][INFO] - Training Epoch: 1/2, step 5238/7134 completed (loss: 0.10844870656728745, acc: 0.9776119589805603)
[2025-02-13 19:33:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:41][root][INFO] - Training Epoch: 1/2, step 5239/7134 completed (loss: 0.18947875499725342, acc: 0.9327731132507324)
[2025-02-13 19:33:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:42][root][INFO] - Training Epoch: 1/2, step 5240/7134 completed (loss: 0.13212651014328003, acc: 0.9651162624359131)
[2025-02-13 19:33:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:42][root][INFO] - Training Epoch: 1/2, step 5241/7134 completed (loss: 0.23562905192375183, acc: 0.9741935729980469)
[2025-02-13 19:33:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:43][root][INFO] - Training Epoch: 1/2, step 5242/7134 completed (loss: 0.09257223457098007, acc: 0.9802631735801697)
[2025-02-13 19:33:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:43][root][INFO] - Training Epoch: 1/2, step 5243/7134 completed (loss: 0.034768763929605484, acc: 0.9928571581840515)
[2025-02-13 19:33:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:43][root][INFO] - Training Epoch: 1/2, step 5244/7134 completed (loss: 0.18459685146808624, acc: 0.9593023061752319)
[2025-02-13 19:33:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:44][root][INFO] - Training Epoch: 1/2, step 5245/7134 completed (loss: 0.1299269050359726, acc: 0.9603174328804016)
[2025-02-13 19:33:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:44][root][INFO] - Training Epoch: 1/2, step 5246/7134 completed (loss: 0.1264985054731369, acc: 0.9620253443717957)
[2025-02-13 19:33:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:45][root][INFO] - Training Epoch: 1/2, step 5247/7134 completed (loss: 0.14415454864501953, acc: 0.9555555582046509)
[2025-02-13 19:33:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:45][root][INFO] - Training Epoch: 1/2, step 5248/7134 completed (loss: 0.13465489447116852, acc: 0.9770992398262024)
[2025-02-13 19:33:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:45][root][INFO] - Training Epoch: 1/2, step 5249/7134 completed (loss: 0.10313498228788376, acc: 0.9745222926139832)
[2025-02-13 19:33:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:46][root][INFO] - Training Epoch: 1/2, step 5250/7134 completed (loss: 0.127994105219841, acc: 0.967391312122345)
[2025-02-13 19:33:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:46][root][INFO] - Training Epoch: 1/2, step 5251/7134 completed (loss: 0.12777237594127655, acc: 0.96875)
[2025-02-13 19:33:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:46][root][INFO] - Training Epoch: 1/2, step 5252/7134 completed (loss: 0.24395382404327393, acc: 0.9380530714988708)
[2025-02-13 19:33:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:47][root][INFO] - Training Epoch: 1/2, step 5253/7134 completed (loss: 0.1028827354311943, acc: 0.9791666865348816)
[2025-02-13 19:33:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:47][root][INFO] - Training Epoch: 1/2, step 5254/7134 completed (loss: 0.3185853362083435, acc: 0.9300699234008789)
[2025-02-13 19:33:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:48][root][INFO] - Training Epoch: 1/2, step 5255/7134 completed (loss: 0.17465896904468536, acc: 0.954023003578186)
[2025-02-13 19:33:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:48][root][INFO] - Training Epoch: 1/2, step 5256/7134 completed (loss: 0.27703940868377686, acc: 0.95333331823349)
[2025-02-13 19:33:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:48][root][INFO] - Training Epoch: 1/2, step 5257/7134 completed (loss: 0.36085787415504456, acc: 0.9281437397003174)
[2025-02-13 19:33:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:49][root][INFO] - Training Epoch: 1/2, step 5258/7134 completed (loss: 0.2576969861984253, acc: 0.9312169551849365)
[2025-02-13 19:33:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:49][root][INFO] - Training Epoch: 1/2, step 5259/7134 completed (loss: 0.2949904501438141, acc: 0.9424460530281067)
[2025-02-13 19:33:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:50][root][INFO] - Training Epoch: 1/2, step 5260/7134 completed (loss: 0.16269004344940186, acc: 0.9682539701461792)
[2025-02-13 19:33:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:50][root][INFO] - Training Epoch: 1/2, step 5261/7134 completed (loss: 0.3061390817165375, acc: 0.9230769276618958)
[2025-02-13 19:33:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:50][root][INFO] - Training Epoch: 1/2, step 5262/7134 completed (loss: 0.17330165207386017, acc: 0.9578313231468201)
[2025-02-13 19:33:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:51][root][INFO] - Training Epoch: 1/2, step 5263/7134 completed (loss: 0.16704264283180237, acc: 0.9436619877815247)
[2025-02-13 19:33:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:51][root][INFO] - Training Epoch: 1/2, step 5264/7134 completed (loss: 0.17748285830020905, acc: 0.9489796161651611)
[2025-02-13 19:33:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:52][root][INFO] - Training Epoch: 1/2, step 5265/7134 completed (loss: 0.26648223400115967, acc: 0.9453551769256592)
[2025-02-13 19:33:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:52][root][INFO] - Training Epoch: 1/2, step 5266/7134 completed (loss: 0.06903520971536636, acc: 0.9876543283462524)
[2025-02-13 19:33:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:52][root][INFO] - Training Epoch: 1/2, step 5267/7134 completed (loss: 0.4251071810722351, acc: 0.8888888955116272)
[2025-02-13 19:33:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:53][root][INFO] - Training Epoch: 1/2, step 5268/7134 completed (loss: 0.27165889739990234, acc: 0.930232584476471)
[2025-02-13 19:33:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:53][root][INFO] - Training Epoch: 1/2, step 5269/7134 completed (loss: 0.2616373896598816, acc: 0.9571428298950195)
[2025-02-13 19:33:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:54][root][INFO] - Training Epoch: 1/2, step 5270/7134 completed (loss: 0.22921060025691986, acc: 0.959770143032074)
[2025-02-13 19:33:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:54][root][INFO] - Training Epoch: 1/2, step 5271/7134 completed (loss: 0.21867074072360992, acc: 0.9424460530281067)
[2025-02-13 19:33:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:54][root][INFO] - Training Epoch: 1/2, step 5272/7134 completed (loss: 0.1525188535451889, acc: 0.9595375657081604)
[2025-02-13 19:33:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:55][root][INFO] - Training Epoch: 1/2, step 5273/7134 completed (loss: 0.24458906054496765, acc: 0.9375)
[2025-02-13 19:33:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:55][root][INFO] - Training Epoch: 1/2, step 5274/7134 completed (loss: 0.2523992955684662, acc: 0.942307710647583)
[2025-02-13 19:33:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:55][root][INFO] - Training Epoch: 1/2, step 5275/7134 completed (loss: 0.10475027561187744, acc: 0.9837398529052734)
[2025-02-13 19:33:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:56][root][INFO] - Training Epoch: 1/2, step 5276/7134 completed (loss: 0.2741677761077881, acc: 0.9407894611358643)
[2025-02-13 19:33:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:56][root][INFO] - Training Epoch: 1/2, step 5277/7134 completed (loss: 0.09629581868648529, acc: 0.9808917045593262)
[2025-02-13 19:33:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:57][root][INFO] - Training Epoch: 1/2, step 5278/7134 completed (loss: 0.39848849177360535, acc: 0.9212121367454529)
[2025-02-13 19:33:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:57][root][INFO] - Training Epoch: 1/2, step 5279/7134 completed (loss: 0.05489229038357735, acc: 0.9918699264526367)
[2025-02-13 19:33:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:57][root][INFO] - Training Epoch: 1/2, step 5280/7134 completed (loss: 0.15428677201271057, acc: 0.949999988079071)
[2025-02-13 19:33:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:58][root][INFO] - Training Epoch: 1/2, step 5281/7134 completed (loss: 0.2064625322818756, acc: 0.9492753744125366)
[2025-02-13 19:33:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:58][root][INFO] - Training Epoch: 1/2, step 5282/7134 completed (loss: 0.37760061025619507, acc: 0.9418604373931885)
[2025-02-13 19:33:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:58][root][INFO] - Training Epoch: 1/2, step 5283/7134 completed (loss: 0.15288816392421722, acc: 0.9562841653823853)
[2025-02-13 19:33:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:59][root][INFO] - Training Epoch: 1/2, step 5284/7134 completed (loss: 0.19572623074054718, acc: 0.9750000238418579)
[2025-02-13 19:33:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:59][root][INFO] - Training Epoch: 1/2, step 5285/7134 completed (loss: 0.24884375929832458, acc: 0.9408602118492126)
[2025-02-13 19:33:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:33:59][root][INFO] - Training Epoch: 1/2, step 5286/7134 completed (loss: 0.17097477614879608, acc: 0.9798657894134521)
[2025-02-13 19:34:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:00][root][INFO] - Training Epoch: 1/2, step 5287/7134 completed (loss: 0.2593388557434082, acc: 0.9444444179534912)
[2025-02-13 19:34:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:00][root][INFO] - Training Epoch: 1/2, step 5288/7134 completed (loss: 0.17290781438350677, acc: 0.96875)
[2025-02-13 19:34:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:01][root][INFO] - Training Epoch: 1/2, step 5289/7134 completed (loss: 0.16004455089569092, acc: 0.9602272510528564)
[2025-02-13 19:34:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:01][root][INFO] - Training Epoch: 1/2, step 5290/7134 completed (loss: 0.2568422853946686, acc: 0.9312169551849365)
[2025-02-13 19:34:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:01][root][INFO] - Training Epoch: 1/2, step 5291/7134 completed (loss: 0.08208305388689041, acc: 0.9729729890823364)
[2025-02-13 19:34:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:02][root][INFO] - Training Epoch: 1/2, step 5292/7134 completed (loss: 0.08718682080507278, acc: 0.9887005686759949)
[2025-02-13 19:34:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:02][root][INFO] - Training Epoch: 1/2, step 5293/7134 completed (loss: 0.12403416633605957, acc: 0.9617834687232971)
[2025-02-13 19:34:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:02][root][INFO] - Training Epoch: 1/2, step 5294/7134 completed (loss: 0.1726725846529007, acc: 0.9467455744743347)
[2025-02-13 19:34:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:03][root][INFO] - Training Epoch: 1/2, step 5295/7134 completed (loss: 0.11224395781755447, acc: 0.961240291595459)
[2025-02-13 19:34:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:03][root][INFO] - Training Epoch: 1/2, step 5296/7134 completed (loss: 0.12236259877681732, acc: 0.9661017060279846)
[2025-02-13 19:34:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:04][root][INFO] - Training Epoch: 1/2, step 5297/7134 completed (loss: 0.14404384791851044, acc: 0.9578313231468201)
[2025-02-13 19:34:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:04][root][INFO] - Training Epoch: 1/2, step 5298/7134 completed (loss: 0.22092723846435547, acc: 0.9432623982429504)
[2025-02-13 19:34:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:04][root][INFO] - Training Epoch: 1/2, step 5299/7134 completed (loss: 0.07884841412305832, acc: 0.9794520735740662)
[2025-02-13 19:34:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:05][root][INFO] - Training Epoch: 1/2, step 5300/7134 completed (loss: 0.0588284395635128, acc: 0.9759036302566528)
[2025-02-13 19:34:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:05][root][INFO] - Training Epoch: 1/2, step 5301/7134 completed (loss: 0.09332746267318726, acc: 0.9878048896789551)
[2025-02-13 19:34:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:06][root][INFO] - Training Epoch: 1/2, step 5302/7134 completed (loss: 0.1812090277671814, acc: 0.9608938694000244)
[2025-02-13 19:34:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:06][root][INFO] - Training Epoch: 1/2, step 5303/7134 completed (loss: 0.47627368569374084, acc: 0.8787878751754761)
[2025-02-13 19:34:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:06][root][INFO] - Training Epoch: 1/2, step 5304/7134 completed (loss: 0.5428644418716431, acc: 0.8835616707801819)
[2025-02-13 19:34:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:07][root][INFO] - Training Epoch: 1/2, step 5305/7134 completed (loss: 0.24733392894268036, acc: 0.9312499761581421)
[2025-02-13 19:34:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:07][root][INFO] - Training Epoch: 1/2, step 5306/7134 completed (loss: 0.1668236255645752, acc: 0.9555555582046509)
[2025-02-13 19:34:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:07][root][INFO] - Training Epoch: 1/2, step 5307/7134 completed (loss: 0.4110414683818817, acc: 0.9298245906829834)
[2025-02-13 19:34:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:08][root][INFO] - Training Epoch: 1/2, step 5308/7134 completed (loss: 0.2050396203994751, acc: 0.9567901492118835)
[2025-02-13 19:34:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:08][root][INFO] - Training Epoch: 1/2, step 5309/7134 completed (loss: 0.5424007773399353, acc: 0.8881118893623352)
[2025-02-13 19:34:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:09][root][INFO] - Training Epoch: 1/2, step 5310/7134 completed (loss: 0.4091617465019226, acc: 0.8863636255264282)
[2025-02-13 19:34:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:09][root][INFO] - Training Epoch: 1/2, step 5311/7134 completed (loss: 0.388816773891449, acc: 0.9006211161613464)
[2025-02-13 19:34:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:09][root][INFO] - Training Epoch: 1/2, step 5312/7134 completed (loss: 0.17311203479766846, acc: 0.9603174328804016)
[2025-02-13 19:34:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:10][root][INFO] - Training Epoch: 1/2, step 5313/7134 completed (loss: 0.2543792128562927, acc: 0.9508196711540222)
[2025-02-13 19:34:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:10][root][INFO] - Training Epoch: 1/2, step 5314/7134 completed (loss: 0.2562081813812256, acc: 0.9214285612106323)
[2025-02-13 19:34:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:11][root][INFO] - Training Epoch: 1/2, step 5315/7134 completed (loss: 0.11973731219768524, acc: 0.971222996711731)
[2025-02-13 19:34:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:11][root][INFO] - Training Epoch: 1/2, step 5316/7134 completed (loss: 0.14854256808757782, acc: 0.95333331823349)
[2025-02-13 19:34:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:11][root][INFO] - Training Epoch: 1/2, step 5317/7134 completed (loss: 0.1880628913640976, acc: 0.9523809552192688)
[2025-02-13 19:34:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:12][root][INFO] - Training Epoch: 1/2, step 5318/7134 completed (loss: 0.05715586245059967, acc: 0.987261176109314)
[2025-02-13 19:34:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:12][root][INFO] - Training Epoch: 1/2, step 5319/7134 completed (loss: 0.20685438811779022, acc: 0.9473684430122375)
[2025-02-13 19:34:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:13][root][INFO] - Training Epoch: 1/2, step 5320/7134 completed (loss: 0.35854241251945496, acc: 0.9182389974594116)
[2025-02-13 19:34:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:13][root][INFO] - Training Epoch: 1/2, step 5321/7134 completed (loss: 0.2170885056257248, acc: 0.9415584206581116)
[2025-02-13 19:34:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:13][root][INFO] - Training Epoch: 1/2, step 5322/7134 completed (loss: 0.13118745386600494, acc: 0.9466666579246521)
[2025-02-13 19:34:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:14][root][INFO] - Training Epoch: 1/2, step 5323/7134 completed (loss: 0.24516573548316956, acc: 0.9127516746520996)
[2025-02-13 19:34:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:14][root][INFO] - Training Epoch: 1/2, step 5324/7134 completed (loss: 0.11602339893579483, acc: 0.9736841917037964)
[2025-02-13 19:34:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:15][root][INFO] - Training Epoch: 1/2, step 5325/7134 completed (loss: 0.15328221023082733, acc: 0.9637681245803833)
[2025-02-13 19:34:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:15][root][INFO] - Training Epoch: 1/2, step 5326/7134 completed (loss: 0.2826118767261505, acc: 0.931034505367279)
[2025-02-13 19:34:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:15][root][INFO] - Training Epoch: 1/2, step 5327/7134 completed (loss: 0.16718249022960663, acc: 0.948387086391449)
[2025-02-13 19:34:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:16][root][INFO] - Training Epoch: 1/2, step 5328/7134 completed (loss: 0.14737868309020996, acc: 0.969924807548523)
[2025-02-13 19:34:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:16][root][INFO] - Training Epoch: 1/2, step 5329/7134 completed (loss: 0.177859827876091, acc: 0.9599999785423279)
[2025-02-13 19:34:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:16][root][INFO] - Training Epoch: 1/2, step 5330/7134 completed (loss: 0.29444870352745056, acc: 0.9367088675498962)
[2025-02-13 19:34:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:17][root][INFO] - Training Epoch: 1/2, step 5331/7134 completed (loss: 0.2944471836090088, acc: 0.9111111164093018)
[2025-02-13 19:34:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:17][root][INFO] - Training Epoch: 1/2, step 5332/7134 completed (loss: 0.1888635903596878, acc: 0.9520547986030579)
[2025-02-13 19:34:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:18][root][INFO] - Training Epoch: 1/2, step 5333/7134 completed (loss: 0.08091197162866592, acc: 0.9868420958518982)
[2025-02-13 19:34:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:18][root][INFO] - Training Epoch: 1/2, step 5334/7134 completed (loss: 0.2790188193321228, acc: 0.9214285612106323)
[2025-02-13 19:34:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:18][root][INFO] - Training Epoch: 1/2, step 5335/7134 completed (loss: 0.14284265041351318, acc: 0.9798657894134521)
[2025-02-13 19:34:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:19][root][INFO] - Training Epoch: 1/2, step 5336/7134 completed (loss: 0.2386535257101059, acc: 0.9436619877815247)
[2025-02-13 19:34:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:19][root][INFO] - Training Epoch: 1/2, step 5337/7134 completed (loss: 0.20836330950260162, acc: 0.9439252614974976)
[2025-02-13 19:34:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:20][root][INFO] - Training Epoch: 1/2, step 5338/7134 completed (loss: 0.1490689367055893, acc: 0.9478672742843628)
[2025-02-13 19:34:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:20][root][INFO] - Training Epoch: 1/2, step 5339/7134 completed (loss: 0.18507462739944458, acc: 0.9505494236946106)
[2025-02-13 19:34:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:20][root][INFO] - Training Epoch: 1/2, step 5340/7134 completed (loss: 0.3263917565345764, acc: 0.9127516746520996)
[2025-02-13 19:34:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:21][root][INFO] - Training Epoch: 1/2, step 5341/7134 completed (loss: 0.2107086181640625, acc: 0.949999988079071)
[2025-02-13 19:34:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:21][root][INFO] - Training Epoch: 1/2, step 5342/7134 completed (loss: 0.5185458660125732, acc: 0.8894736766815186)
[2025-02-13 19:34:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:21][root][INFO] - Training Epoch: 1/2, step 5343/7134 completed (loss: 0.27869051694869995, acc: 0.929729700088501)
[2025-02-13 19:34:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:22][root][INFO] - Training Epoch: 1/2, step 5344/7134 completed (loss: 0.2627750635147095, acc: 0.9210526347160339)
[2025-02-13 19:34:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:22][root][INFO] - Training Epoch: 1/2, step 5345/7134 completed (loss: 0.25801101326942444, acc: 0.9242424368858337)
[2025-02-13 19:34:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:23][root][INFO] - Training Epoch: 1/2, step 5346/7134 completed (loss: 0.1837797909975052, acc: 0.9587156176567078)
[2025-02-13 19:34:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:23][root][INFO] - Training Epoch: 1/2, step 5347/7134 completed (loss: 0.1849544644355774, acc: 0.9462365508079529)
[2025-02-13 19:34:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:23][root][INFO] - Training Epoch: 1/2, step 5348/7134 completed (loss: 0.23306125402450562, acc: 0.9337349534034729)
[2025-02-13 19:34:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:34:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:35:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:36:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:37:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:33][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.3111, device='cuda:0') eval_epoch_loss=tensor(0.2708, device='cuda:0') eval_epoch_acc=tensor(0.9377, device='cuda:0')
[2025-02-13 19:38:33][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-13 19:38:33][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-13 19:38:33][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_wavlm_llama32_1b_linear_peft_transfer_librispeech-100/asr_epoch_1_step_5349_loss_0.2708382308483124/model.pt
[2025-02-13 19:38:33][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_wavlm_llama32_1b_linear_peft_transfer_librispeech-100 directory
[2025-02-13 19:38:33][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.2708382308483124
[2025-02-13 19:38:33][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.9377016425132751
[2025-02-13 19:38:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:34][root][INFO] - Training Epoch: 1/2, step 5349/7134 completed (loss: 0.42448508739471436, acc: 0.8914285898208618)
[2025-02-13 19:38:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:34][root][INFO] - Training Epoch: 1/2, step 5350/7134 completed (loss: 0.14349909126758575, acc: 0.9529411792755127)
[2025-02-13 19:38:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:35][root][INFO] - Training Epoch: 1/2, step 5351/7134 completed (loss: 0.2601259648799896, acc: 0.9405405521392822)
[2025-02-13 19:38:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:35][root][INFO] - Training Epoch: 1/2, step 5352/7134 completed (loss: 0.2274029701948166, acc: 0.946107804775238)
[2025-02-13 19:38:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:35][root][INFO] - Training Epoch: 1/2, step 5353/7134 completed (loss: 0.24481192231178284, acc: 0.939393937587738)
[2025-02-13 19:38:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:36][root][INFO] - Training Epoch: 1/2, step 5354/7134 completed (loss: 0.2111603021621704, acc: 0.9490740895271301)
[2025-02-13 19:38:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:36][root][INFO] - Training Epoch: 1/2, step 5355/7134 completed (loss: 0.25990644097328186, acc: 0.953125)
[2025-02-13 19:38:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:36][root][INFO] - Training Epoch: 1/2, step 5356/7134 completed (loss: 0.267905592918396, acc: 0.9308510422706604)
[2025-02-13 19:38:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:37][root][INFO] - Training Epoch: 1/2, step 5357/7134 completed (loss: 0.16129857301712036, acc: 0.9432623982429504)
[2025-02-13 19:38:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:37][root][INFO] - Training Epoch: 1/2, step 5358/7134 completed (loss: 0.5433454513549805, acc: 0.8681318759918213)
[2025-02-13 19:38:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:38][root][INFO] - Training Epoch: 1/2, step 5359/7134 completed (loss: 0.14764390885829926, acc: 0.9753694534301758)
[2025-02-13 19:38:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:38][root][INFO] - Training Epoch: 1/2, step 5360/7134 completed (loss: 0.14564532041549683, acc: 0.9655172228813171)
[2025-02-13 19:38:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:39][root][INFO] - Training Epoch: 1/2, step 5361/7134 completed (loss: 0.3427368700504303, acc: 0.9238578677177429)
[2025-02-13 19:38:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:39][root][INFO] - Training Epoch: 1/2, step 5362/7134 completed (loss: 0.28174033761024475, acc: 0.9391891956329346)
[2025-02-13 19:38:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:39][root][INFO] - Training Epoch: 1/2, step 5363/7134 completed (loss: 0.2532947361469269, acc: 0.9390243887901306)
[2025-02-13 19:38:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:40][root][INFO] - Training Epoch: 1/2, step 5364/7134 completed (loss: 0.18210065364837646, acc: 0.9537572264671326)
[2025-02-13 19:38:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:40][root][INFO] - Training Epoch: 1/2, step 5365/7134 completed (loss: 0.21504688262939453, acc: 0.9318181872367859)
[2025-02-13 19:38:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:40][root][INFO] - Training Epoch: 1/2, step 5366/7134 completed (loss: 0.295502632856369, acc: 0.9701492786407471)
[2025-02-13 19:38:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:41][root][INFO] - Training Epoch: 1/2, step 5367/7134 completed (loss: 0.24786031246185303, acc: 0.9402173757553101)
[2025-02-13 19:38:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:41][root][INFO] - Training Epoch: 1/2, step 5368/7134 completed (loss: 0.15126517415046692, acc: 0.9534883499145508)
[2025-02-13 19:38:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:42][root][INFO] - Training Epoch: 1/2, step 5369/7134 completed (loss: 0.26867353916168213, acc: 0.9491525292396545)
[2025-02-13 19:38:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:42][root][INFO] - Training Epoch: 1/2, step 5370/7134 completed (loss: 0.27563369274139404, acc: 0.9415584206581116)
[2025-02-13 19:38:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:42][root][INFO] - Training Epoch: 1/2, step 5371/7134 completed (loss: 0.40497708320617676, acc: 0.9200000166893005)
[2025-02-13 19:38:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:43][root][INFO] - Training Epoch: 1/2, step 5372/7134 completed (loss: 0.11315524578094482, acc: 0.9622641801834106)
[2025-02-13 19:38:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:43][root][INFO] - Training Epoch: 1/2, step 5373/7134 completed (loss: 0.1576322764158249, acc: 0.976190447807312)
[2025-02-13 19:38:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:44][root][INFO] - Training Epoch: 1/2, step 5374/7134 completed (loss: 0.11640386283397675, acc: 0.976190447807312)
[2025-02-13 19:38:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:44][root][INFO] - Training Epoch: 1/2, step 5375/7134 completed (loss: 0.10213744640350342, acc: 0.9844961166381836)
[2025-02-13 19:38:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:44][root][INFO] - Training Epoch: 1/2, step 5376/7134 completed (loss: 0.08414994180202484, acc: 0.9797297120094299)
[2025-02-13 19:38:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:45][root][INFO] - Training Epoch: 1/2, step 5377/7134 completed (loss: 0.0850379467010498, acc: 0.9876543283462524)
[2025-02-13 19:38:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:45][root][INFO] - Training Epoch: 1/2, step 5378/7134 completed (loss: 0.056725919246673584, acc: 0.9947368502616882)
[2025-02-13 19:38:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:45][root][INFO] - Training Epoch: 1/2, step 5379/7134 completed (loss: 0.07799556106328964, acc: 0.984375)
[2025-02-13 19:38:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:46][root][INFO] - Training Epoch: 1/2, step 5380/7134 completed (loss: 0.10794582217931747, acc: 0.9851852059364319)
[2025-02-13 19:38:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:46][root][INFO] - Training Epoch: 1/2, step 5381/7134 completed (loss: 0.10509859770536423, acc: 0.9661017060279846)
[2025-02-13 19:38:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:47][root][INFO] - Training Epoch: 1/2, step 5382/7134 completed (loss: 0.07807514071464539, acc: 0.9710144996643066)
[2025-02-13 19:38:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:47][root][INFO] - Training Epoch: 1/2, step 5383/7134 completed (loss: 0.09075913578271866, acc: 0.975806474685669)
[2025-02-13 19:38:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:48][root][INFO] - Training Epoch: 1/2, step 5384/7134 completed (loss: 0.100165955722332, acc: 0.989847719669342)
[2025-02-13 19:38:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:48][root][INFO] - Training Epoch: 1/2, step 5385/7134 completed (loss: 0.12363526225090027, acc: 0.9767441749572754)
[2025-02-13 19:38:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:48][root][INFO] - Training Epoch: 1/2, step 5386/7134 completed (loss: 0.08261473476886749, acc: 0.9890109896659851)
[2025-02-13 19:38:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:49][root][INFO] - Training Epoch: 1/2, step 5387/7134 completed (loss: 0.1554015725851059, acc: 0.9836065769195557)
[2025-02-13 19:38:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:49][root][INFO] - Training Epoch: 1/2, step 5388/7134 completed (loss: 0.06721503287553787, acc: 0.9888268113136292)
[2025-02-13 19:38:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:50][root][INFO] - Training Epoch: 1/2, step 5389/7134 completed (loss: 0.07876282930374146, acc: 0.9795918464660645)
[2025-02-13 19:38:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:50][root][INFO] - Training Epoch: 1/2, step 5390/7134 completed (loss: 0.21274514496326447, acc: 0.9622641801834106)
[2025-02-13 19:38:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:50][root][INFO] - Training Epoch: 1/2, step 5391/7134 completed (loss: 0.21162927150726318, acc: 0.9424460530281067)
[2025-02-13 19:38:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:51][root][INFO] - Training Epoch: 1/2, step 5392/7134 completed (loss: 0.23043641448020935, acc: 0.9488636255264282)
[2025-02-13 19:38:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:51][root][INFO] - Training Epoch: 1/2, step 5393/7134 completed (loss: 0.21757546067237854, acc: 0.9333333373069763)
[2025-02-13 19:38:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:52][root][INFO] - Training Epoch: 1/2, step 5394/7134 completed (loss: 0.4308054745197296, acc: 0.875)
[2025-02-13 19:38:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:52][root][INFO] - Training Epoch: 1/2, step 5395/7134 completed (loss: 0.06990452855825424, acc: 0.9932432174682617)
[2025-02-13 19:38:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:53][root][INFO] - Training Epoch: 1/2, step 5396/7134 completed (loss: 0.1314421147108078, acc: 0.9736841917037964)
[2025-02-13 19:38:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:53][root][INFO] - Training Epoch: 1/2, step 5397/7134 completed (loss: 0.10219988971948624, acc: 0.9716312289237976)
[2025-02-13 19:38:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:54][root][INFO] - Training Epoch: 1/2, step 5398/7134 completed (loss: 0.1563108265399933, acc: 0.9698795080184937)
[2025-02-13 19:38:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:54][root][INFO] - Training Epoch: 1/2, step 5399/7134 completed (loss: 0.1458316296339035, acc: 0.9485714435577393)
[2025-02-13 19:38:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:54][root][INFO] - Training Epoch: 1/2, step 5400/7134 completed (loss: 0.3085123300552368, acc: 0.9379310607910156)
[2025-02-13 19:38:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:55][root][INFO] - Training Epoch: 1/2, step 5401/7134 completed (loss: 0.18801186978816986, acc: 0.9652777910232544)
[2025-02-13 19:38:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:55][root][INFO] - Training Epoch: 1/2, step 5402/7134 completed (loss: 0.15425582230091095, acc: 0.9520547986030579)
[2025-02-13 19:38:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:55][root][INFO] - Training Epoch: 1/2, step 5403/7134 completed (loss: 0.26166418194770813, acc: 0.9262295365333557)
[2025-02-13 19:38:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:56][root][INFO] - Training Epoch: 1/2, step 5404/7134 completed (loss: 0.3489205837249756, acc: 0.9194630980491638)
[2025-02-13 19:38:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:56][root][INFO] - Training Epoch: 1/2, step 5405/7134 completed (loss: 0.13613516092300415, acc: 0.9826086759567261)
[2025-02-13 19:38:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:57][root][INFO] - Training Epoch: 1/2, step 5406/7134 completed (loss: 0.2465466856956482, acc: 0.9279279112815857)
[2025-02-13 19:38:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:57][root][INFO] - Training Epoch: 1/2, step 5407/7134 completed (loss: 0.1546383798122406, acc: 0.9780219793319702)
[2025-02-13 19:38:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:58][root][INFO] - Training Epoch: 1/2, step 5408/7134 completed (loss: 0.3423722982406616, acc: 0.9078013896942139)
[2025-02-13 19:38:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:58][root][INFO] - Training Epoch: 1/2, step 5409/7134 completed (loss: 0.33026123046875, acc: 0.9440559148788452)
[2025-02-13 19:38:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:58][root][INFO] - Training Epoch: 1/2, step 5410/7134 completed (loss: 0.0902426466345787, acc: 0.9819276928901672)
[2025-02-13 19:38:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:59][root][INFO] - Training Epoch: 1/2, step 5411/7134 completed (loss: 0.21313828229904175, acc: 0.9507042169570923)
[2025-02-13 19:38:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:59][root][INFO] - Training Epoch: 1/2, step 5412/7134 completed (loss: 0.3555903732776642, acc: 0.9557521939277649)
[2025-02-13 19:38:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:38:59][root][INFO] - Training Epoch: 1/2, step 5413/7134 completed (loss: 0.32501399517059326, acc: 0.9027777910232544)
[2025-02-13 19:39:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:00][root][INFO] - Training Epoch: 1/2, step 5414/7134 completed (loss: 0.5574327111244202, acc: 0.8622754216194153)
[2025-02-13 19:39:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:00][root][INFO] - Training Epoch: 1/2, step 5415/7134 completed (loss: 0.3256462514400482, acc: 0.9285714030265808)
[2025-02-13 19:39:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:01][root][INFO] - Training Epoch: 1/2, step 5416/7134 completed (loss: 0.34101977944374084, acc: 0.9090909361839294)
[2025-02-13 19:39:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:01][root][INFO] - Training Epoch: 1/2, step 5417/7134 completed (loss: 0.28480905294418335, acc: 0.9102563858032227)
[2025-02-13 19:39:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:02][root][INFO] - Training Epoch: 1/2, step 5418/7134 completed (loss: 0.37724217772483826, acc: 0.9097222089767456)
[2025-02-13 19:39:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:02][root][INFO] - Training Epoch: 1/2, step 5419/7134 completed (loss: 0.7540991306304932, acc: 0.8175182342529297)
[2025-02-13 19:39:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:02][root][INFO] - Training Epoch: 1/2, step 5420/7134 completed (loss: 0.6094194054603577, acc: 0.8421052694320679)
[2025-02-13 19:39:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:03][root][INFO] - Training Epoch: 1/2, step 5421/7134 completed (loss: 0.5671136975288391, acc: 0.8687499761581421)
[2025-02-13 19:39:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:03][root][INFO] - Training Epoch: 1/2, step 5422/7134 completed (loss: 0.2298279106616974, acc: 0.9513888955116272)
[2025-02-13 19:39:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:03][root][INFO] - Training Epoch: 1/2, step 5423/7134 completed (loss: 0.20849406719207764, acc: 0.949999988079071)
[2025-02-13 19:39:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:04][root][INFO] - Training Epoch: 1/2, step 5424/7134 completed (loss: 0.2785990536212921, acc: 0.9307692050933838)
[2025-02-13 19:39:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:04][root][INFO] - Training Epoch: 1/2, step 5425/7134 completed (loss: 0.18697981536388397, acc: 0.9453125)
[2025-02-13 19:39:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:05][root][INFO] - Training Epoch: 1/2, step 5426/7134 completed (loss: 0.2109973281621933, acc: 0.9754098653793335)
[2025-02-13 19:39:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:05][root][INFO] - Training Epoch: 1/2, step 5427/7134 completed (loss: 0.14911456406116486, acc: 0.9477124214172363)
[2025-02-13 19:39:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:05][root][INFO] - Training Epoch: 1/2, step 5428/7134 completed (loss: 0.1965101957321167, acc: 0.9448275566101074)
[2025-02-13 19:39:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:06][root][INFO] - Training Epoch: 1/2, step 5429/7134 completed (loss: 0.27990421652793884, acc: 0.926174521446228)
[2025-02-13 19:39:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:06][root][INFO] - Training Epoch: 1/2, step 5430/7134 completed (loss: 0.26052212715148926, acc: 0.9545454382896423)
[2025-02-13 19:39:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:07][root][INFO] - Training Epoch: 1/2, step 5431/7134 completed (loss: 0.26868483424186707, acc: 0.9172932505607605)
[2025-02-13 19:39:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:07][root][INFO] - Training Epoch: 1/2, step 5432/7134 completed (loss: 0.150973379611969, acc: 0.9453125)
[2025-02-13 19:39:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:07][root][INFO] - Training Epoch: 1/2, step 5433/7134 completed (loss: 0.2673752009868622, acc: 0.924369752407074)
[2025-02-13 19:39:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:08][root][INFO] - Training Epoch: 1/2, step 5434/7134 completed (loss: 0.283704549074173, acc: 0.9596773982048035)
[2025-02-13 19:39:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:08][root][INFO] - Training Epoch: 1/2, step 5435/7134 completed (loss: 0.21692052483558655, acc: 0.9520000219345093)
[2025-02-13 19:39:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:09][root][INFO] - Training Epoch: 1/2, step 5436/7134 completed (loss: 0.18244245648384094, acc: 0.9677419066429138)
[2025-02-13 19:39:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:09][root][INFO] - Training Epoch: 1/2, step 5437/7134 completed (loss: 0.21267150342464447, acc: 0.9548872113227844)
[2025-02-13 19:39:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:09][root][INFO] - Training Epoch: 1/2, step 5438/7134 completed (loss: 0.2799150049686432, acc: 0.9111111164093018)
[2025-02-13 19:39:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:10][root][INFO] - Training Epoch: 1/2, step 5439/7134 completed (loss: 0.43315577507019043, acc: 0.8787878751754761)
[2025-02-13 19:39:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:10][root][INFO] - Training Epoch: 1/2, step 5440/7134 completed (loss: 0.27878430485725403, acc: 0.9166666865348816)
[2025-02-13 19:39:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:11][root][INFO] - Training Epoch: 1/2, step 5441/7134 completed (loss: 0.33643460273742676, acc: 0.908108115196228)
[2025-02-13 19:39:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:11][root][INFO] - Training Epoch: 1/2, step 5442/7134 completed (loss: 0.16150815784931183, acc: 0.9583333134651184)
[2025-02-13 19:39:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:11][root][INFO] - Training Epoch: 1/2, step 5443/7134 completed (loss: 0.11650611460208893, acc: 0.9817073345184326)
[2025-02-13 19:39:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:12][root][INFO] - Training Epoch: 1/2, step 5444/7134 completed (loss: 0.1707010418176651, acc: 0.9485294222831726)
[2025-02-13 19:39:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:12][root][INFO] - Training Epoch: 1/2, step 5445/7134 completed (loss: 0.25223618745803833, acc: 0.9338235259056091)
[2025-02-13 19:39:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:12][root][INFO] - Training Epoch: 1/2, step 5446/7134 completed (loss: 0.13153792917728424, acc: 0.9509202241897583)
[2025-02-13 19:39:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:13][root][INFO] - Training Epoch: 1/2, step 5447/7134 completed (loss: 0.20923489332199097, acc: 0.9383561611175537)
[2025-02-13 19:39:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:13][root][INFO] - Training Epoch: 1/2, step 5448/7134 completed (loss: 0.2370183765888214, acc: 0.9450549483299255)
[2025-02-13 19:39:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:14][root][INFO] - Training Epoch: 1/2, step 5449/7134 completed (loss: 0.2772113084793091, acc: 0.9344262480735779)
[2025-02-13 19:39:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:14][root][INFO] - Training Epoch: 1/2, step 5450/7134 completed (loss: 0.2183680236339569, acc: 0.9555555582046509)
[2025-02-13 19:39:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:14][root][INFO] - Training Epoch: 1/2, step 5451/7134 completed (loss: 0.1099839061498642, acc: 0.976331353187561)
[2025-02-13 19:39:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:15][root][INFO] - Training Epoch: 1/2, step 5452/7134 completed (loss: 0.09765543788671494, acc: 0.9822485446929932)
[2025-02-13 19:39:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:15][root][INFO] - Training Epoch: 1/2, step 5453/7134 completed (loss: 0.2793828248977661, acc: 0.949999988079071)
[2025-02-13 19:39:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:15][root][INFO] - Training Epoch: 1/2, step 5454/7134 completed (loss: 0.19235500693321228, acc: 0.967391312122345)
[2025-02-13 19:39:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:16][root][INFO] - Training Epoch: 1/2, step 5455/7134 completed (loss: 0.19146877527236938, acc: 0.9572192430496216)
[2025-02-13 19:39:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:16][root][INFO] - Training Epoch: 1/2, step 5456/7134 completed (loss: 0.1969859004020691, acc: 0.9408602118492126)
[2025-02-13 19:39:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:17][root][INFO] - Training Epoch: 1/2, step 5457/7134 completed (loss: 0.206441268324852, acc: 0.9425287246704102)
[2025-02-13 19:39:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:17][root][INFO] - Training Epoch: 1/2, step 5458/7134 completed (loss: 0.13684777915477753, acc: 0.9558011293411255)
[2025-02-13 19:39:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:17][root][INFO] - Training Epoch: 1/2, step 5459/7134 completed (loss: 0.050804611295461655, acc: 0.9815950989723206)
[2025-02-13 19:39:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:18][root][INFO] - Training Epoch: 1/2, step 5460/7134 completed (loss: 0.12245754152536392, acc: 0.9526627063751221)
[2025-02-13 19:39:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:18][root][INFO] - Training Epoch: 1/2, step 5461/7134 completed (loss: 0.3455384373664856, acc: 0.9174311757087708)
[2025-02-13 19:39:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:19][root][INFO] - Training Epoch: 1/2, step 5462/7134 completed (loss: 0.09344805777072906, acc: 0.9754601120948792)
[2025-02-13 19:39:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:19][root][INFO] - Training Epoch: 1/2, step 5463/7134 completed (loss: 0.258769154548645, acc: 0.9320987462997437)
[2025-02-13 19:39:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:19][root][INFO] - Training Epoch: 1/2, step 5464/7134 completed (loss: 0.2873660624027252, acc: 0.9234972596168518)
[2025-02-13 19:39:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:20][root][INFO] - Training Epoch: 1/2, step 5465/7134 completed (loss: 0.11009404808282852, acc: 0.987500011920929)
[2025-02-13 19:39:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:20][root][INFO] - Training Epoch: 1/2, step 5466/7134 completed (loss: 0.12181242555379868, acc: 0.9502487778663635)
[2025-02-13 19:39:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:20][root][INFO] - Training Epoch: 1/2, step 5467/7134 completed (loss: 0.08959642797708511, acc: 0.9751243591308594)
[2025-02-13 19:39:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:21][root][INFO] - Training Epoch: 1/2, step 5468/7134 completed (loss: 0.0855356752872467, acc: 0.9738219976425171)
[2025-02-13 19:39:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:21][root][INFO] - Training Epoch: 1/2, step 5469/7134 completed (loss: 0.2051302045583725, acc: 0.9417989253997803)
[2025-02-13 19:39:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:22][root][INFO] - Training Epoch: 1/2, step 5470/7134 completed (loss: 0.06151224672794342, acc: 0.9800994992256165)
[2025-02-13 19:39:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:22][root][INFO] - Training Epoch: 1/2, step 5471/7134 completed (loss: 0.0599510632455349, acc: 0.9941176176071167)
[2025-02-13 19:39:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:22][root][INFO] - Training Epoch: 1/2, step 5472/7134 completed (loss: 0.10933871567249298, acc: 0.9888268113136292)
[2025-02-13 19:39:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:23][root][INFO] - Training Epoch: 1/2, step 5473/7134 completed (loss: 0.05683675408363342, acc: 0.9898989796638489)
[2025-02-13 19:39:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:23][root][INFO] - Training Epoch: 1/2, step 5474/7134 completed (loss: 0.13111767172813416, acc: 0.9818181991577148)
[2025-02-13 19:39:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:24][root][INFO] - Training Epoch: 1/2, step 5475/7134 completed (loss: 0.08001081645488739, acc: 0.9814814925193787)
[2025-02-13 19:39:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:24][root][INFO] - Training Epoch: 1/2, step 5476/7134 completed (loss: 0.046270180493593216, acc: 0.9870129823684692)
[2025-02-13 19:39:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:24][root][INFO] - Training Epoch: 1/2, step 5477/7134 completed (loss: 0.061900537461042404, acc: 0.9795918464660645)
[2025-02-13 19:39:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:25][root][INFO] - Training Epoch: 1/2, step 5478/7134 completed (loss: 0.14385537803173065, acc: 0.970370352268219)
[2025-02-13 19:39:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:25][root][INFO] - Training Epoch: 1/2, step 5479/7134 completed (loss: 0.336087167263031, acc: 0.912162184715271)
[2025-02-13 19:39:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:26][root][INFO] - Training Epoch: 1/2, step 5480/7134 completed (loss: 0.22126011550426483, acc: 0.9345238208770752)
[2025-02-13 19:39:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:26][root][INFO] - Training Epoch: 1/2, step 5481/7134 completed (loss: 0.17380771040916443, acc: 0.9672130942344666)
[2025-02-13 19:39:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:27][root][INFO] - Training Epoch: 1/2, step 5482/7134 completed (loss: 0.30821189284324646, acc: 0.9432989954948425)
[2025-02-13 19:39:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:27][root][INFO] - Training Epoch: 1/2, step 5483/7134 completed (loss: 0.236319899559021, acc: 0.903030276298523)
[2025-02-13 19:39:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:28][root][INFO] - Training Epoch: 1/2, step 5484/7134 completed (loss: 0.22852057218551636, acc: 0.9197530746459961)
[2025-02-13 19:39:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:28][root][INFO] - Training Epoch: 1/2, step 5485/7134 completed (loss: 0.336797833442688, acc: 0.9112903475761414)
[2025-02-13 19:39:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:28][root][INFO] - Training Epoch: 1/2, step 5486/7134 completed (loss: 0.41457951068878174, acc: 0.8863636255264282)
[2025-02-13 19:39:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:29][root][INFO] - Training Epoch: 1/2, step 5487/7134 completed (loss: 0.19293312728405, acc: 0.9455782175064087)
[2025-02-13 19:39:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:29][root][INFO] - Training Epoch: 1/2, step 5488/7134 completed (loss: 0.3038322627544403, acc: 0.910179615020752)
[2025-02-13 19:39:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:30][root][INFO] - Training Epoch: 1/2, step 5489/7134 completed (loss: 0.5716214776039124, acc: 0.8658536672592163)
[2025-02-13 19:39:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:30][root][INFO] - Training Epoch: 1/2, step 5490/7134 completed (loss: 0.19023415446281433, acc: 0.939393937587738)
[2025-02-13 19:39:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:30][root][INFO] - Training Epoch: 1/2, step 5491/7134 completed (loss: 0.16544973850250244, acc: 0.9590643048286438)
[2025-02-13 19:39:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:31][root][INFO] - Training Epoch: 1/2, step 5492/7134 completed (loss: 0.3422669768333435, acc: 0.9328358173370361)
[2025-02-13 19:39:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:31][root][INFO] - Training Epoch: 1/2, step 5493/7134 completed (loss: 0.34420791268348694, acc: 0.90625)
[2025-02-13 19:39:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:32][root][INFO] - Training Epoch: 1/2, step 5494/7134 completed (loss: 0.18708369135856628, acc: 0.9685534834861755)
[2025-02-13 19:39:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:32][root][INFO] - Training Epoch: 1/2, step 5495/7134 completed (loss: 0.22206076979637146, acc: 0.932330846786499)
[2025-02-13 19:39:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:32][root][INFO] - Training Epoch: 1/2, step 5496/7134 completed (loss: 0.35682961344718933, acc: 0.9365079402923584)
[2025-02-13 19:39:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:33][root][INFO] - Training Epoch: 1/2, step 5497/7134 completed (loss: 0.32455170154571533, acc: 0.9117646813392639)
[2025-02-13 19:39:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:33][root][INFO] - Training Epoch: 1/2, step 5498/7134 completed (loss: 0.3026478588581085, acc: 0.9308510422706604)
[2025-02-13 19:39:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:34][root][INFO] - Training Epoch: 1/2, step 5499/7134 completed (loss: 0.3067719042301178, acc: 0.9009009003639221)
[2025-02-13 19:39:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:34][root][INFO] - Training Epoch: 1/2, step 5500/7134 completed (loss: 0.26773178577423096, acc: 0.9430052042007446)
[2025-02-13 19:39:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:34][root][INFO] - Training Epoch: 1/2, step 5501/7134 completed (loss: 0.2442900538444519, acc: 0.9444444179534912)
[2025-02-13 19:39:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:35][root][INFO] - Training Epoch: 1/2, step 5502/7134 completed (loss: 0.1719958484172821, acc: 0.9604519605636597)
[2025-02-13 19:39:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:35][root][INFO] - Training Epoch: 1/2, step 5503/7134 completed (loss: 0.5383158922195435, acc: 0.892307698726654)
[2025-02-13 19:39:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:35][root][INFO] - Training Epoch: 1/2, step 5504/7134 completed (loss: 0.3249674141407013, acc: 0.9036144614219666)
[2025-02-13 19:39:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:36][root][INFO] - Training Epoch: 1/2, step 5505/7134 completed (loss: 0.4681874215602875, acc: 0.8636363744735718)
[2025-02-13 19:39:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:36][root][INFO] - Training Epoch: 1/2, step 5506/7134 completed (loss: 0.35506191849708557, acc: 0.9040403962135315)
[2025-02-13 19:39:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:37][root][INFO] - Training Epoch: 1/2, step 5507/7134 completed (loss: 0.26265037059783936, acc: 0.9181286692619324)
[2025-02-13 19:39:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:37][root][INFO] - Training Epoch: 1/2, step 5508/7134 completed (loss: 0.30277058482170105, acc: 0.9425837397575378)
[2025-02-13 19:39:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:37][root][INFO] - Training Epoch: 1/2, step 5509/7134 completed (loss: 0.31623557209968567, acc: 0.900473952293396)
[2025-02-13 19:39:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:38][root][INFO] - Training Epoch: 1/2, step 5510/7134 completed (loss: 0.28203466534614563, acc: 0.9247311949729919)
[2025-02-13 19:39:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:38][root][INFO] - Training Epoch: 1/2, step 5511/7134 completed (loss: 0.34755024313926697, acc: 0.9126213788986206)
[2025-02-13 19:39:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:39][root][INFO] - Training Epoch: 1/2, step 5512/7134 completed (loss: 0.36882826685905457, acc: 0.9175823926925659)
[2025-02-13 19:39:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:39][root][INFO] - Training Epoch: 1/2, step 5513/7134 completed (loss: 0.3473077714443207, acc: 0.8875739574432373)
[2025-02-13 19:39:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:39][root][INFO] - Training Epoch: 1/2, step 5514/7134 completed (loss: 0.4046972393989563, acc: 0.8935185074806213)
[2025-02-13 19:39:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:40][root][INFO] - Training Epoch: 1/2, step 5515/7134 completed (loss: 0.2990523874759674, acc: 0.9285714030265808)
[2025-02-13 19:39:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:40][root][INFO] - Training Epoch: 1/2, step 5516/7134 completed (loss: 0.1576480269432068, acc: 0.9655172228813171)
[2025-02-13 19:39:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:40][root][INFO] - Training Epoch: 1/2, step 5517/7134 completed (loss: 0.18958090245723724, acc: 0.9553072452545166)
[2025-02-13 19:39:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:41][root][INFO] - Training Epoch: 1/2, step 5518/7134 completed (loss: 0.26091697812080383, acc: 0.9333333373069763)
[2025-02-13 19:39:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:41][root][INFO] - Training Epoch: 1/2, step 5519/7134 completed (loss: 0.17347507178783417, acc: 0.9629629850387573)
[2025-02-13 19:39:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:42][root][INFO] - Training Epoch: 1/2, step 5520/7134 completed (loss: 0.19637858867645264, acc: 0.9431279897689819)
[2025-02-13 19:39:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:42][root][INFO] - Training Epoch: 1/2, step 5521/7134 completed (loss: 0.2717262804508209, acc: 0.9162303805351257)
[2025-02-13 19:39:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:42][root][INFO] - Training Epoch: 1/2, step 5522/7134 completed (loss: 0.3803979754447937, acc: 0.8795811533927917)
[2025-02-13 19:39:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:43][root][INFO] - Training Epoch: 1/2, step 5523/7134 completed (loss: 0.32406747341156006, acc: 0.9230769276618958)
[2025-02-13 19:39:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:43][root][INFO] - Training Epoch: 1/2, step 5524/7134 completed (loss: 0.33874621987342834, acc: 0.8860759735107422)
[2025-02-13 19:39:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:44][root][INFO] - Training Epoch: 1/2, step 5525/7134 completed (loss: 0.19745701551437378, acc: 0.9451219439506531)
[2025-02-13 19:39:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:44][root][INFO] - Training Epoch: 1/2, step 5526/7134 completed (loss: 0.16714191436767578, acc: 0.9503105878829956)
[2025-02-13 19:39:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:44][root][INFO] - Training Epoch: 1/2, step 5527/7134 completed (loss: 0.17140847444534302, acc: 0.9650349617004395)
[2025-02-13 19:39:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:45][root][INFO] - Training Epoch: 1/2, step 5528/7134 completed (loss: 0.19356034696102142, acc: 0.9444444179534912)
[2025-02-13 19:39:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:45][root][INFO] - Training Epoch: 1/2, step 5529/7134 completed (loss: 0.18383580446243286, acc: 0.9590908885002136)
[2025-02-13 19:39:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:46][root][INFO] - Training Epoch: 1/2, step 5530/7134 completed (loss: 0.16017669439315796, acc: 0.9659863710403442)
[2025-02-13 19:39:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:46][root][INFO] - Training Epoch: 1/2, step 5531/7134 completed (loss: 0.1388210952281952, acc: 0.9638554453849792)
[2025-02-13 19:39:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:46][root][INFO] - Training Epoch: 1/2, step 5532/7134 completed (loss: 0.2925698757171631, acc: 0.9378530979156494)
[2025-02-13 19:39:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:47][root][INFO] - Training Epoch: 1/2, step 5533/7134 completed (loss: 0.07452470809221268, acc: 0.9888888597488403)
[2025-02-13 19:39:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:47][root][INFO] - Training Epoch: 1/2, step 5534/7134 completed (loss: 0.28691181540489197, acc: 0.932584285736084)
[2025-02-13 19:39:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:48][root][INFO] - Training Epoch: 1/2, step 5535/7134 completed (loss: 0.17821291089057922, acc: 0.9740932583808899)
[2025-02-13 19:39:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:48][root][INFO] - Training Epoch: 1/2, step 5536/7134 completed (loss: 0.1503438651561737, acc: 0.9672130942344666)
[2025-02-13 19:39:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:49][root][INFO] - Training Epoch: 1/2, step 5537/7134 completed (loss: 0.26261791586875916, acc: 0.9482758641242981)
[2025-02-13 19:39:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:49][root][INFO] - Training Epoch: 1/2, step 5538/7134 completed (loss: 0.18775606155395508, acc: 0.9738219976425171)
[2025-02-13 19:39:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:50][root][INFO] - Training Epoch: 1/2, step 5539/7134 completed (loss: 0.16917754709720612, acc: 0.9672130942344666)
[2025-02-13 19:39:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:50][root][INFO] - Training Epoch: 1/2, step 5540/7134 completed (loss: 0.247815802693367, acc: 0.9337349534034729)
[2025-02-13 19:39:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:50][root][INFO] - Training Epoch: 1/2, step 5541/7134 completed (loss: 0.1273401826620102, acc: 0.9534883499145508)
[2025-02-13 19:39:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:51][root][INFO] - Training Epoch: 1/2, step 5542/7134 completed (loss: 0.07697396725416183, acc: 0.9806451797485352)
[2025-02-13 19:39:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:51][root][INFO] - Training Epoch: 1/2, step 5543/7134 completed (loss: 0.15252292156219482, acc: 0.9839572310447693)
[2025-02-13 19:39:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:52][root][INFO] - Training Epoch: 1/2, step 5544/7134 completed (loss: 0.07889025658369064, acc: 0.9890710115432739)
[2025-02-13 19:39:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:52][root][INFO] - Training Epoch: 1/2, step 5545/7134 completed (loss: 0.16929404437541962, acc: 0.9583333134651184)
[2025-02-13 19:39:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:53][root][INFO] - Training Epoch: 1/2, step 5546/7134 completed (loss: 0.08942343294620514, acc: 0.9880239367485046)
[2025-02-13 19:39:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:53][root][INFO] - Training Epoch: 1/2, step 5547/7134 completed (loss: 0.1466776430606842, acc: 0.9723756909370422)
[2025-02-13 19:39:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:53][root][INFO] - Training Epoch: 1/2, step 5548/7134 completed (loss: 0.1453738659620285, acc: 0.970588207244873)
[2025-02-13 19:39:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:54][root][INFO] - Training Epoch: 1/2, step 5549/7134 completed (loss: 0.16393066942691803, acc: 0.9731543660163879)
[2025-02-13 19:39:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:54][root][INFO] - Training Epoch: 1/2, step 5550/7134 completed (loss: 0.11381924897432327, acc: 0.983146071434021)
[2025-02-13 19:39:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:55][root][INFO] - Training Epoch: 1/2, step 5551/7134 completed (loss: 0.44273272156715393, acc: 0.9290322661399841)
[2025-02-13 19:39:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:55][root][INFO] - Training Epoch: 1/2, step 5552/7134 completed (loss: 0.10537785291671753, acc: 0.9693251252174377)
[2025-02-13 19:39:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:55][root][INFO] - Training Epoch: 1/2, step 5553/7134 completed (loss: 0.2095150649547577, acc: 0.9585798978805542)
[2025-02-13 19:39:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:56][root][INFO] - Training Epoch: 1/2, step 5554/7134 completed (loss: 0.10104541480541229, acc: 0.9808917045593262)
[2025-02-13 19:39:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:56][root][INFO] - Training Epoch: 1/2, step 5555/7134 completed (loss: 0.2208775281906128, acc: 0.9464285969734192)
[2025-02-13 19:39:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:57][root][INFO] - Training Epoch: 1/2, step 5556/7134 completed (loss: 0.3005477786064148, acc: 0.9385474920272827)
[2025-02-13 19:39:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:57][root][INFO] - Training Epoch: 1/2, step 5557/7134 completed (loss: 0.19488832354545593, acc: 0.95652174949646)
[2025-02-13 19:39:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:57][root][INFO] - Training Epoch: 1/2, step 5558/7134 completed (loss: 0.16281874477863312, acc: 0.9611111283302307)
[2025-02-13 19:39:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:58][root][INFO] - Training Epoch: 1/2, step 5559/7134 completed (loss: 0.2782856523990631, acc: 0.9553072452545166)
[2025-02-13 19:39:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:58][root][INFO] - Training Epoch: 1/2, step 5560/7134 completed (loss: 0.3522748351097107, acc: 0.9111111164093018)
[2025-02-13 19:39:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:59][root][INFO] - Training Epoch: 1/2, step 5561/7134 completed (loss: 0.295261412858963, acc: 0.9590163826942444)
[2025-02-13 19:39:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:59][root][INFO] - Training Epoch: 1/2, step 5562/7134 completed (loss: 0.29179951548576355, acc: 0.935251772403717)
[2025-02-13 19:39:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:39:59][root][INFO] - Training Epoch: 1/2, step 5563/7134 completed (loss: 0.13649755716323853, acc: 0.9652777910232544)
[2025-02-13 19:39:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:00][root][INFO] - Training Epoch: 1/2, step 5564/7134 completed (loss: 0.1290658712387085, acc: 0.96875)
[2025-02-13 19:40:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:00][root][INFO] - Training Epoch: 1/2, step 5565/7134 completed (loss: 0.054678644984960556, acc: 0.9866666793823242)
[2025-02-13 19:40:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:01][root][INFO] - Training Epoch: 1/2, step 5566/7134 completed (loss: 0.06921415030956268, acc: 0.989847719669342)
[2025-02-13 19:40:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:01][root][INFO] - Training Epoch: 1/2, step 5567/7134 completed (loss: 0.06723500788211823, acc: 0.9850746393203735)
[2025-02-13 19:40:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:01][root][INFO] - Training Epoch: 1/2, step 5568/7134 completed (loss: 0.06717096269130707, acc: 0.9890109896659851)
[2025-02-13 19:40:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:02][root][INFO] - Training Epoch: 1/2, step 5569/7134 completed (loss: 0.051929280161857605, acc: 0.9947916865348816)
[2025-02-13 19:40:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:02][root][INFO] - Training Epoch: 1/2, step 5570/7134 completed (loss: 0.07820289582014084, acc: 0.9728260636329651)
[2025-02-13 19:40:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:03][root][INFO] - Training Epoch: 1/2, step 5571/7134 completed (loss: 0.10320315510034561, acc: 0.9810126423835754)
[2025-02-13 19:40:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:03][root][INFO] - Training Epoch: 1/2, step 5572/7134 completed (loss: 0.15774564445018768, acc: 0.9693251252174377)
[2025-02-13 19:40:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:03][root][INFO] - Training Epoch: 1/2, step 5573/7134 completed (loss: 0.10533208400011063, acc: 0.9803921580314636)
[2025-02-13 19:40:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:04][root][INFO] - Training Epoch: 1/2, step 5574/7134 completed (loss: 0.10753685981035233, acc: 0.9791666865348816)
[2025-02-13 19:40:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:04][root][INFO] - Training Epoch: 1/2, step 5575/7134 completed (loss: 0.22930091619491577, acc: 0.9599999785423279)
[2025-02-13 19:40:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:05][root][INFO] - Training Epoch: 1/2, step 5576/7134 completed (loss: 0.07216598838567734, acc: 0.9940476417541504)
[2025-02-13 19:40:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:05][root][INFO] - Training Epoch: 1/2, step 5577/7134 completed (loss: 0.2843111455440521, acc: 0.9333333373069763)
[2025-02-13 19:40:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:05][root][INFO] - Training Epoch: 1/2, step 5578/7134 completed (loss: 0.2715720534324646, acc: 0.9303797483444214)
[2025-02-13 19:40:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:06][root][INFO] - Training Epoch: 1/2, step 5579/7134 completed (loss: 0.22824199497699738, acc: 0.9386503100395203)
[2025-02-13 19:40:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:06][root][INFO] - Training Epoch: 1/2, step 5580/7134 completed (loss: 0.09222768247127533, acc: 0.9833333492279053)
[2025-02-13 19:40:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:07][root][INFO] - Training Epoch: 1/2, step 5581/7134 completed (loss: 0.15401121973991394, acc: 0.9696969985961914)
[2025-02-13 19:40:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:07][root][INFO] - Training Epoch: 1/2, step 5582/7134 completed (loss: 0.12315832078456879, acc: 0.9723756909370422)
[2025-02-13 19:40:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:07][root][INFO] - Training Epoch: 1/2, step 5583/7134 completed (loss: 0.08718711882829666, acc: 0.9748743772506714)
[2025-02-13 19:40:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:08][root][INFO] - Training Epoch: 1/2, step 5584/7134 completed (loss: 0.16070245206356049, acc: 0.9556962251663208)
[2025-02-13 19:40:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:08][root][INFO] - Training Epoch: 1/2, step 5585/7134 completed (loss: 0.16337822377681732, acc: 0.9609755873680115)
[2025-02-13 19:40:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:09][root][INFO] - Training Epoch: 1/2, step 5586/7134 completed (loss: 0.06266815960407257, acc: 1.0)
[2025-02-13 19:40:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:09][root][INFO] - Training Epoch: 1/2, step 5587/7134 completed (loss: 0.11207632720470428, acc: 0.9714285731315613)
[2025-02-13 19:40:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:09][root][INFO] - Training Epoch: 1/2, step 5588/7134 completed (loss: 0.06369554251432419, acc: 0.9938271641731262)
[2025-02-13 19:40:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:10][root][INFO] - Training Epoch: 1/2, step 5589/7134 completed (loss: 0.22438029944896698, acc: 0.9503105878829956)
[2025-02-13 19:40:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:10][root][INFO] - Training Epoch: 1/2, step 5590/7134 completed (loss: 0.16152897477149963, acc: 0.9595375657081604)
[2025-02-13 19:40:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:11][root][INFO] - Training Epoch: 1/2, step 5591/7134 completed (loss: 0.07272114604711533, acc: 0.9826589822769165)
[2025-02-13 19:40:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:11][root][INFO] - Training Epoch: 1/2, step 5592/7134 completed (loss: 0.09233979880809784, acc: 0.9941860437393188)
[2025-02-13 19:40:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:12][root][INFO] - Training Epoch: 1/2, step 5593/7134 completed (loss: 0.13421465456485748, acc: 0.9619565010070801)
[2025-02-13 19:40:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:12][root][INFO] - Training Epoch: 1/2, step 5594/7134 completed (loss: 0.08411381393671036, acc: 0.9772727489471436)
[2025-02-13 19:40:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:12][root][INFO] - Training Epoch: 1/2, step 5595/7134 completed (loss: 0.11387834697961807, acc: 0.9772727489471436)
[2025-02-13 19:40:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:13][root][INFO] - Training Epoch: 1/2, step 5596/7134 completed (loss: 0.07087439298629761, acc: 0.9820359349250793)
[2025-02-13 19:40:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:13][root][INFO] - Training Epoch: 1/2, step 5597/7134 completed (loss: 0.15320947766304016, acc: 0.9638554453849792)
[2025-02-13 19:40:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:14][root][INFO] - Training Epoch: 1/2, step 5598/7134 completed (loss: 0.16533297300338745, acc: 0.9653179049491882)
[2025-02-13 19:40:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:14][root][INFO] - Training Epoch: 1/2, step 5599/7134 completed (loss: 0.2473776638507843, acc: 0.9455782175064087)
[2025-02-13 19:40:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:14][root][INFO] - Training Epoch: 1/2, step 5600/7134 completed (loss: 0.13485093414783478, acc: 0.9642857313156128)
[2025-02-13 19:40:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:15][root][INFO] - Training Epoch: 1/2, step 5601/7134 completed (loss: 0.11840871721506119, acc: 0.9618320465087891)
[2025-02-13 19:40:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:15][root][INFO] - Training Epoch: 1/2, step 5602/7134 completed (loss: 0.2512681484222412, acc: 0.9295774698257446)
[2025-02-13 19:40:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:16][root][INFO] - Training Epoch: 1/2, step 5603/7134 completed (loss: 0.227101668715477, acc: 0.9426229596138)
[2025-02-13 19:40:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:16][root][INFO] - Training Epoch: 1/2, step 5604/7134 completed (loss: 0.1808471828699112, acc: 0.9593495726585388)
[2025-02-13 19:40:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:16][root][INFO] - Training Epoch: 1/2, step 5605/7134 completed (loss: 0.19565895199775696, acc: 0.9448819160461426)
[2025-02-13 19:40:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:17][root][INFO] - Training Epoch: 1/2, step 5606/7134 completed (loss: 0.2751028835773468, acc: 0.9532710313796997)
[2025-02-13 19:40:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:17][root][INFO] - Training Epoch: 1/2, step 5607/7134 completed (loss: 0.08021552860736847, acc: 0.9784172773361206)
[2025-02-13 19:40:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:18][root][INFO] - Training Epoch: 1/2, step 5608/7134 completed (loss: 0.14097820222377777, acc: 0.9767441749572754)
[2025-02-13 19:40:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:18][root][INFO] - Training Epoch: 1/2, step 5609/7134 completed (loss: 0.21080507338047028, acc: 0.9416666626930237)
[2025-02-13 19:40:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:19][root][INFO] - Training Epoch: 1/2, step 5610/7134 completed (loss: 0.06637288630008698, acc: 0.9818181991577148)
[2025-02-13 19:40:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:19][root][INFO] - Training Epoch: 1/2, step 5611/7134 completed (loss: 0.16524365544319153, acc: 0.9626865386962891)
[2025-02-13 19:40:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:19][root][INFO] - Training Epoch: 1/2, step 5612/7134 completed (loss: 0.24966011941432953, acc: 0.9436619877815247)
[2025-02-13 19:40:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:20][root][INFO] - Training Epoch: 1/2, step 5613/7134 completed (loss: 0.23968885838985443, acc: 0.9285714030265808)
[2025-02-13 19:40:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:20][root][INFO] - Training Epoch: 1/2, step 5614/7134 completed (loss: 0.2565244436264038, acc: 0.9463087320327759)
[2025-02-13 19:40:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:21][root][INFO] - Training Epoch: 1/2, step 5615/7134 completed (loss: 0.12171825766563416, acc: 0.9809523820877075)
[2025-02-13 19:40:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:21][root][INFO] - Training Epoch: 1/2, step 5616/7134 completed (loss: 0.09319397062063217, acc: 0.9851852059364319)
[2025-02-13 19:40:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:21][root][INFO] - Training Epoch: 1/2, step 5617/7134 completed (loss: 0.11289755254983902, acc: 0.9689922332763672)
[2025-02-13 19:40:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:22][root][INFO] - Training Epoch: 1/2, step 5618/7134 completed (loss: 0.18536114692687988, acc: 0.9338235259056091)
[2025-02-13 19:40:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:22][root][INFO] - Training Epoch: 1/2, step 5619/7134 completed (loss: 0.1918015033006668, acc: 0.9663865566253662)
[2025-02-13 19:40:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:23][root][INFO] - Training Epoch: 1/2, step 5620/7134 completed (loss: 0.13394705951213837, acc: 0.9652174115180969)
[2025-02-13 19:40:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:23][root][INFO] - Training Epoch: 1/2, step 5621/7134 completed (loss: 0.15129241347312927, acc: 0.9419354796409607)
[2025-02-13 19:40:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:24][root][INFO] - Training Epoch: 1/2, step 5622/7134 completed (loss: 0.20617151260375977, acc: 0.9572192430496216)
[2025-02-13 19:40:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:24][root][INFO] - Training Epoch: 1/2, step 5623/7134 completed (loss: 0.23886793851852417, acc: 0.9444444179534912)
[2025-02-13 19:40:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:24][root][INFO] - Training Epoch: 1/2, step 5624/7134 completed (loss: 0.13557882606983185, acc: 0.9738562107086182)
[2025-02-13 19:40:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:25][root][INFO] - Training Epoch: 1/2, step 5625/7134 completed (loss: 0.1105601117014885, acc: 0.970588207244873)
[2025-02-13 19:40:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:25][root][INFO] - Training Epoch: 1/2, step 5626/7134 completed (loss: 0.11809060722589493, acc: 0.9655172228813171)
[2025-02-13 19:40:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:26][root][INFO] - Training Epoch: 1/2, step 5627/7134 completed (loss: 0.4704347550868988, acc: 0.8999999761581421)
[2025-02-13 19:40:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:26][root][INFO] - Training Epoch: 1/2, step 5628/7134 completed (loss: 0.1894114762544632, acc: 0.9642857313156128)
[2025-02-13 19:40:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:26][root][INFO] - Training Epoch: 1/2, step 5629/7134 completed (loss: 0.07552804797887802, acc: 0.9836065769195557)
[2025-02-13 19:40:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:27][root][INFO] - Training Epoch: 1/2, step 5630/7134 completed (loss: 0.25361618399620056, acc: 0.9607843160629272)
[2025-02-13 19:40:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:27][root][INFO] - Training Epoch: 1/2, step 5631/7134 completed (loss: 0.16122981905937195, acc: 0.9430894255638123)
[2025-02-13 19:40:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:28][root][INFO] - Training Epoch: 1/2, step 5632/7134 completed (loss: 0.17731262743473053, acc: 0.9711538553237915)
[2025-02-13 19:40:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:28][root][INFO] - Training Epoch: 1/2, step 5633/7134 completed (loss: 0.11452214419841766, acc: 0.9821428656578064)
[2025-02-13 19:40:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:28][root][INFO] - Training Epoch: 1/2, step 5634/7134 completed (loss: 0.140997514128685, acc: 0.9646017551422119)
[2025-02-13 19:40:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:29][root][INFO] - Training Epoch: 1/2, step 5635/7134 completed (loss: 0.10380801558494568, acc: 0.9609375)
[2025-02-13 19:40:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:29][root][INFO] - Training Epoch: 1/2, step 5636/7134 completed (loss: 0.13465939462184906, acc: 0.9615384340286255)
[2025-02-13 19:40:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:30][root][INFO] - Training Epoch: 1/2, step 5637/7134 completed (loss: 0.15242727100849152, acc: 0.9545454382896423)
[2025-02-13 19:40:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:30][root][INFO] - Training Epoch: 1/2, step 5638/7134 completed (loss: 0.0962371751666069, acc: 0.9736841917037964)
[2025-02-13 19:40:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:30][root][INFO] - Training Epoch: 1/2, step 5639/7134 completed (loss: 0.14428545534610748, acc: 0.9545454382896423)
[2025-02-13 19:40:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:31][root][INFO] - Training Epoch: 1/2, step 5640/7134 completed (loss: 0.12024323642253876, acc: 0.978723406791687)
[2025-02-13 19:40:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:31][root][INFO] - Training Epoch: 1/2, step 5641/7134 completed (loss: 0.18707290291786194, acc: 0.966292142868042)
[2025-02-13 19:40:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:32][root][INFO] - Training Epoch: 1/2, step 5642/7134 completed (loss: 0.20306271314620972, acc: 0.9510489702224731)
[2025-02-13 19:40:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:32][root][INFO] - Training Epoch: 1/2, step 5643/7134 completed (loss: 0.24019823968410492, acc: 0.9368420839309692)
[2025-02-13 19:40:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:32][root][INFO] - Training Epoch: 1/2, step 5644/7134 completed (loss: 0.15773612260818481, acc: 0.9809523820877075)
[2025-02-13 19:40:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:33][root][INFO] - Training Epoch: 1/2, step 5645/7134 completed (loss: 0.12265376001596451, acc: 0.9652777910232544)
[2025-02-13 19:40:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:33][root][INFO] - Training Epoch: 1/2, step 5646/7134 completed (loss: 0.15786020457744598, acc: 0.9357143044471741)
[2025-02-13 19:40:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:34][root][INFO] - Training Epoch: 1/2, step 5647/7134 completed (loss: 0.1825462281703949, acc: 0.9513888955116272)
[2025-02-13 19:40:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:34][root][INFO] - Training Epoch: 1/2, step 5648/7134 completed (loss: 0.22682543098926544, acc: 0.9466666579246521)
[2025-02-13 19:40:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:34][root][INFO] - Training Epoch: 1/2, step 5649/7134 completed (loss: 0.29924994707107544, acc: 0.9054054021835327)
[2025-02-13 19:40:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:35][root][INFO] - Training Epoch: 1/2, step 5650/7134 completed (loss: 0.2178449034690857, acc: 0.9274193644523621)
[2025-02-13 19:40:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:35][root][INFO] - Training Epoch: 1/2, step 5651/7134 completed (loss: 0.12670141458511353, acc: 0.9727891087532043)
[2025-02-13 19:40:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:35][root][INFO] - Training Epoch: 1/2, step 5652/7134 completed (loss: 0.3210049867630005, acc: 0.9208633303642273)
[2025-02-13 19:40:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:36][root][INFO] - Training Epoch: 1/2, step 5653/7134 completed (loss: 0.2764189541339874, acc: 0.8961039185523987)
[2025-02-13 19:40:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:36][root][INFO] - Training Epoch: 1/2, step 5654/7134 completed (loss: 0.10472548753023148, acc: 0.96875)
[2025-02-13 19:40:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:37][root][INFO] - Training Epoch: 1/2, step 5655/7134 completed (loss: 0.207218736410141, acc: 0.9439252614974976)
[2025-02-13 19:40:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:37][root][INFO] - Training Epoch: 1/2, step 5656/7134 completed (loss: 0.16713224351406097, acc: 0.9508196711540222)
[2025-02-13 19:40:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:37][root][INFO] - Training Epoch: 1/2, step 5657/7134 completed (loss: 0.36442551016807556, acc: 0.9072847962379456)
[2025-02-13 19:40:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:38][root][INFO] - Training Epoch: 1/2, step 5658/7134 completed (loss: 0.21329087018966675, acc: 0.9477124214172363)
[2025-02-13 19:40:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:38][root][INFO] - Training Epoch: 1/2, step 5659/7134 completed (loss: 0.1419752836227417, acc: 0.949999988079071)
[2025-02-13 19:40:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:39][root][INFO] - Training Epoch: 1/2, step 5660/7134 completed (loss: 0.16850745677947998, acc: 0.953125)
[2025-02-13 19:40:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:39][root][INFO] - Training Epoch: 1/2, step 5661/7134 completed (loss: 0.14550702273845673, acc: 0.9663865566253662)
[2025-02-13 19:40:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:39][root][INFO] - Training Epoch: 1/2, step 5662/7134 completed (loss: 0.14992870390415192, acc: 0.9568345546722412)
[2025-02-13 19:40:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:40][root][INFO] - Training Epoch: 1/2, step 5663/7134 completed (loss: 0.06693437695503235, acc: 0.9855072498321533)
[2025-02-13 19:40:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:40][root][INFO] - Training Epoch: 1/2, step 5664/7134 completed (loss: 0.3201638460159302, acc: 0.9383561611175537)
[2025-02-13 19:40:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:41][root][INFO] - Training Epoch: 1/2, step 5665/7134 completed (loss: 0.10336898267269135, acc: 0.9795918464660645)
[2025-02-13 19:40:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:41][root][INFO] - Training Epoch: 1/2, step 5666/7134 completed (loss: 0.18501797318458557, acc: 0.9738562107086182)
[2025-02-13 19:40:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:41][root][INFO] - Training Epoch: 1/2, step 5667/7134 completed (loss: 0.20517022907733917, acc: 0.9674796462059021)
[2025-02-13 19:40:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:42][root][INFO] - Training Epoch: 1/2, step 5668/7134 completed (loss: 0.23536482453346252, acc: 0.9555555582046509)
[2025-02-13 19:40:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:42][root][INFO] - Training Epoch: 1/2, step 5669/7134 completed (loss: 0.18441033363342285, acc: 0.9490445852279663)
[2025-02-13 19:40:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:43][root][INFO] - Training Epoch: 1/2, step 5670/7134 completed (loss: 0.08901463449001312, acc: 0.9841269850730896)
[2025-02-13 19:40:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:43][root][INFO] - Training Epoch: 1/2, step 5671/7134 completed (loss: 0.1794627159833908, acc: 0.966292142868042)
[2025-02-13 19:40:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:43][root][INFO] - Training Epoch: 1/2, step 5672/7134 completed (loss: 0.1364159882068634, acc: 0.9777777791023254)
[2025-02-13 19:40:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:44][root][INFO] - Training Epoch: 1/2, step 5673/7134 completed (loss: 0.14326190948486328, acc: 0.965753436088562)
[2025-02-13 19:40:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:44][root][INFO] - Training Epoch: 1/2, step 5674/7134 completed (loss: 0.20976829528808594, acc: 0.9455782175064087)
[2025-02-13 19:40:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:45][root][INFO] - Training Epoch: 1/2, step 5675/7134 completed (loss: 0.3437223434448242, acc: 0.9066666960716248)
[2025-02-13 19:40:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:45][root][INFO] - Training Epoch: 1/2, step 5676/7134 completed (loss: 0.38105836510658264, acc: 0.8793103694915771)
[2025-02-13 19:40:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:46][root][INFO] - Training Epoch: 1/2, step 5677/7134 completed (loss: 0.4729146957397461, acc: 0.8901098966598511)
[2025-02-13 19:40:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:46][root][INFO] - Training Epoch: 1/2, step 5678/7134 completed (loss: 0.1800239235162735, acc: 0.9506173133850098)
[2025-02-13 19:40:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:46][root][INFO] - Training Epoch: 1/2, step 5679/7134 completed (loss: 0.518086850643158, acc: 0.892405092716217)
[2025-02-13 19:40:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:47][root][INFO] - Training Epoch: 1/2, step 5680/7134 completed (loss: 0.5560427904129028, acc: 0.8695651888847351)
[2025-02-13 19:40:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:47][root][INFO] - Training Epoch: 1/2, step 5681/7134 completed (loss: 0.40873658657073975, acc: 0.903030276298523)
[2025-02-13 19:40:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:47][root][INFO] - Training Epoch: 1/2, step 5682/7134 completed (loss: 0.2635428309440613, acc: 0.9337349534034729)
[2025-02-13 19:40:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:48][root][INFO] - Training Epoch: 1/2, step 5683/7134 completed (loss: 0.26665207743644714, acc: 0.9444444179534912)
[2025-02-13 19:40:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:48][root][INFO] - Training Epoch: 1/2, step 5684/7134 completed (loss: 0.5534080862998962, acc: 0.9024389982223511)
[2025-02-13 19:40:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:49][root][INFO] - Training Epoch: 1/2, step 5685/7134 completed (loss: 0.37276971340179443, acc: 0.9055117964744568)
[2025-02-13 19:40:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:49][root][INFO] - Training Epoch: 1/2, step 5686/7134 completed (loss: 0.21736016869544983, acc: 0.9424460530281067)
[2025-02-13 19:40:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:49][root][INFO] - Training Epoch: 1/2, step 5687/7134 completed (loss: 0.36350953578948975, acc: 0.9090909361839294)
[2025-02-13 19:40:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:50][root][INFO] - Training Epoch: 1/2, step 5688/7134 completed (loss: 0.3801926076412201, acc: 0.9111111164093018)
[2025-02-13 19:40:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:50][root][INFO] - Training Epoch: 1/2, step 5689/7134 completed (loss: 0.20324918627738953, acc: 0.9634146094322205)
[2025-02-13 19:40:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:51][root][INFO] - Training Epoch: 1/2, step 5690/7134 completed (loss: 0.18172989785671234, acc: 0.9278350472450256)
[2025-02-13 19:40:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:51][root][INFO] - Training Epoch: 1/2, step 5691/7134 completed (loss: 0.2990894317626953, acc: 0.9289617538452148)
[2025-02-13 19:40:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:51][root][INFO] - Training Epoch: 1/2, step 5692/7134 completed (loss: 0.20857268571853638, acc: 0.9428571462631226)
[2025-02-13 19:40:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:52][root][INFO] - Training Epoch: 1/2, step 5693/7134 completed (loss: 0.04375534504652023, acc: 0.9811320900917053)
[2025-02-13 19:40:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:52][root][INFO] - Training Epoch: 1/2, step 5694/7134 completed (loss: 0.1151251345872879, acc: 0.9709302186965942)
[2025-02-13 19:40:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:52][root][INFO] - Training Epoch: 1/2, step 5695/7134 completed (loss: 0.10768492519855499, acc: 0.9693251252174377)
[2025-02-13 19:40:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:53][root][INFO] - Training Epoch: 1/2, step 5696/7134 completed (loss: 0.11846722662448883, acc: 0.9752475023269653)
[2025-02-13 19:40:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:53][root][INFO] - Training Epoch: 1/2, step 5697/7134 completed (loss: 0.18332639336585999, acc: 0.9696969985961914)
[2025-02-13 19:40:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:54][root][INFO] - Training Epoch: 1/2, step 5698/7134 completed (loss: 0.13597916066646576, acc: 0.9715909361839294)
[2025-02-13 19:40:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:54][root][INFO] - Training Epoch: 1/2, step 5699/7134 completed (loss: 0.27185648679733276, acc: 0.9482758641242981)
[2025-02-13 19:40:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:54][root][INFO] - Training Epoch: 1/2, step 5700/7134 completed (loss: 0.1636466532945633, acc: 0.9681528806686401)
[2025-02-13 19:40:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:55][root][INFO] - Training Epoch: 1/2, step 5701/7134 completed (loss: 0.13181667029857635, acc: 0.9675675630569458)
[2025-02-13 19:40:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:55][root][INFO] - Training Epoch: 1/2, step 5702/7134 completed (loss: 0.14737378060817719, acc: 0.9767441749572754)
[2025-02-13 19:40:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:56][root][INFO] - Training Epoch: 1/2, step 5703/7134 completed (loss: 0.06846829503774643, acc: 0.9874213933944702)
[2025-02-13 19:40:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:56][root][INFO] - Training Epoch: 1/2, step 5704/7134 completed (loss: 0.2002575546503067, acc: 0.9529411792755127)
[2025-02-13 19:40:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:56][root][INFO] - Training Epoch: 1/2, step 5705/7134 completed (loss: 0.07489807903766632, acc: 0.9852941036224365)
[2025-02-13 19:40:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:57][root][INFO] - Training Epoch: 1/2, step 5706/7134 completed (loss: 0.1561720073223114, acc: 0.9603960514068604)
[2025-02-13 19:40:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:57][root][INFO] - Training Epoch: 1/2, step 5707/7134 completed (loss: 0.19734139740467072, acc: 0.9599999785423279)
[2025-02-13 19:40:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:58][root][INFO] - Training Epoch: 1/2, step 5708/7134 completed (loss: 0.0417436882853508, acc: 0.9935897588729858)
[2025-02-13 19:40:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:58][root][INFO] - Training Epoch: 1/2, step 5709/7134 completed (loss: 0.24912039935588837, acc: 0.95652174949646)
[2025-02-13 19:40:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:58][root][INFO] - Training Epoch: 1/2, step 5710/7134 completed (loss: 0.178156316280365, acc: 0.954285740852356)
[2025-02-13 19:40:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:59][root][INFO] - Training Epoch: 1/2, step 5711/7134 completed (loss: 0.25340619683265686, acc: 0.9462365508079529)
[2025-02-13 19:40:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:40:59][root][INFO] - Training Epoch: 1/2, step 5712/7134 completed (loss: 0.09032364189624786, acc: 0.975806474685669)
[2025-02-13 19:40:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:00][root][INFO] - Training Epoch: 1/2, step 5713/7134 completed (loss: 0.11276896297931671, acc: 0.983146071434021)
[2025-02-13 19:41:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:00][root][INFO] - Training Epoch: 1/2, step 5714/7134 completed (loss: 0.06236867979168892, acc: 0.989130437374115)
[2025-02-13 19:41:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:00][root][INFO] - Training Epoch: 1/2, step 5715/7134 completed (loss: 0.34107422828674316, acc: 0.9411764740943909)
[2025-02-13 19:41:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:01][root][INFO] - Training Epoch: 1/2, step 5716/7134 completed (loss: 0.44907325506210327, acc: 0.9027777910232544)
[2025-02-13 19:41:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:01][root][INFO] - Training Epoch: 1/2, step 5717/7134 completed (loss: 0.3580833077430725, acc: 0.9181286692619324)
[2025-02-13 19:41:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:02][root][INFO] - Training Epoch: 1/2, step 5718/7134 completed (loss: 0.15472348034381866, acc: 0.9539473652839661)
[2025-02-13 19:41:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:02][root][INFO] - Training Epoch: 1/2, step 5719/7134 completed (loss: 0.22188256680965424, acc: 0.9235293865203857)
[2025-02-13 19:41:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:02][root][INFO] - Training Epoch: 1/2, step 5720/7134 completed (loss: 0.30122271180152893, acc: 0.9230769276618958)
[2025-02-13 19:41:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:03][root][INFO] - Training Epoch: 1/2, step 5721/7134 completed (loss: 0.25023216009140015, acc: 0.9430052042007446)
[2025-02-13 19:41:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:03][root][INFO] - Training Epoch: 1/2, step 5722/7134 completed (loss: 0.10243266075849533, acc: 0.9727272987365723)
[2025-02-13 19:41:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:03][root][INFO] - Training Epoch: 1/2, step 5723/7134 completed (loss: 0.20907656848430634, acc: 0.9312169551849365)
[2025-02-13 19:41:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:04][root][INFO] - Training Epoch: 1/2, step 5724/7134 completed (loss: 0.24208006262779236, acc: 0.9333333373069763)
[2025-02-13 19:41:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:04][root][INFO] - Training Epoch: 1/2, step 5725/7134 completed (loss: 0.1647651642560959, acc: 0.9599999785423279)
[2025-02-13 19:41:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:05][root][INFO] - Training Epoch: 1/2, step 5726/7134 completed (loss: 0.2522110342979431, acc: 0.9341317415237427)
[2025-02-13 19:41:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:05][root][INFO] - Training Epoch: 1/2, step 5727/7134 completed (loss: 0.09140841662883759, acc: 0.9748743772506714)
[2025-02-13 19:41:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:05][root][INFO] - Training Epoch: 1/2, step 5728/7134 completed (loss: 0.05186961963772774, acc: 0.9852941036224365)
[2025-02-13 19:41:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:06][root][INFO] - Training Epoch: 1/2, step 5729/7134 completed (loss: 0.11439014971256256, acc: 0.9740259647369385)
[2025-02-13 19:41:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:06][root][INFO] - Training Epoch: 1/2, step 5730/7134 completed (loss: 0.18167723715305328, acc: 0.9481481313705444)
[2025-02-13 19:41:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:07][root][INFO] - Training Epoch: 1/2, step 5731/7134 completed (loss: 0.18470178544521332, acc: 0.9437500238418579)
[2025-02-13 19:41:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:07][root][INFO] - Training Epoch: 1/2, step 5732/7134 completed (loss: 0.9202825427055359, acc: 0.7831325531005859)
[2025-02-13 19:41:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:07][root][INFO] - Training Epoch: 1/2, step 5733/7134 completed (loss: 0.34180644154548645, acc: 0.9113923907279968)
[2025-02-13 19:41:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:08][root][INFO] - Training Epoch: 1/2, step 5734/7134 completed (loss: 0.29011791944503784, acc: 0.9285714030265808)
[2025-02-13 19:41:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:08][root][INFO] - Training Epoch: 1/2, step 5735/7134 completed (loss: 0.32030045986175537, acc: 0.9139072895050049)
[2025-02-13 19:41:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:09][root][INFO] - Training Epoch: 1/2, step 5736/7134 completed (loss: 0.3776739239692688, acc: 0.9171270728111267)
[2025-02-13 19:41:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:09][root][INFO] - Training Epoch: 1/2, step 5737/7134 completed (loss: 0.18790574371814728, acc: 0.9532163739204407)
[2025-02-13 19:41:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:09][root][INFO] - Training Epoch: 1/2, step 5738/7134 completed (loss: 0.16518697142601013, acc: 0.9545454382896423)
[2025-02-13 19:41:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:10][root][INFO] - Training Epoch: 1/2, step 5739/7134 completed (loss: 0.21421456336975098, acc: 0.9583333134651184)
[2025-02-13 19:41:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:10][root][INFO] - Training Epoch: 1/2, step 5740/7134 completed (loss: 0.13095895946025848, acc: 0.96875)
[2025-02-13 19:41:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:11][root][INFO] - Training Epoch: 1/2, step 5741/7134 completed (loss: 0.13782216608524323, acc: 0.9715909361839294)
[2025-02-13 19:41:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:11][root][INFO] - Training Epoch: 1/2, step 5742/7134 completed (loss: 0.1105261892080307, acc: 0.9775280952453613)
[2025-02-13 19:41:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:11][root][INFO] - Training Epoch: 1/2, step 5743/7134 completed (loss: 0.09212377667427063, acc: 0.9739583134651184)
[2025-02-13 19:41:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:12][root][INFO] - Training Epoch: 1/2, step 5744/7134 completed (loss: 0.07812713831663132, acc: 0.982300877571106)
[2025-02-13 19:41:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:12][root][INFO] - Training Epoch: 1/2, step 5745/7134 completed (loss: 0.29168301820755005, acc: 0.9382022619247437)
[2025-02-13 19:41:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:13][root][INFO] - Training Epoch: 1/2, step 5746/7134 completed (loss: 0.15451140701770782, acc: 0.9629629850387573)
[2025-02-13 19:41:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:13][root][INFO] - Training Epoch: 1/2, step 5747/7134 completed (loss: 0.18934166431427002, acc: 0.9575757384300232)
[2025-02-13 19:41:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:13][root][INFO] - Training Epoch: 1/2, step 5748/7134 completed (loss: 0.12476163357496262, acc: 0.960629940032959)
[2025-02-13 19:41:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:14][root][INFO] - Training Epoch: 1/2, step 5749/7134 completed (loss: 0.22226105630397797, acc: 0.9585798978805542)
[2025-02-13 19:41:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:14][root][INFO] - Training Epoch: 1/2, step 5750/7134 completed (loss: 0.34227487444877625, acc: 0.9385474920272827)
[2025-02-13 19:41:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:15][root][INFO] - Training Epoch: 1/2, step 5751/7134 completed (loss: 0.22846540808677673, acc: 0.940397322177887)
[2025-02-13 19:41:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:15][root][INFO] - Training Epoch: 1/2, step 5752/7134 completed (loss: 1.3497732877731323, acc: 0.7251908183097839)
[2025-02-13 19:41:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:15][root][INFO] - Training Epoch: 1/2, step 5753/7134 completed (loss: 0.28994959592819214, acc: 0.9395973086357117)
[2025-02-13 19:41:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:16][root][INFO] - Training Epoch: 1/2, step 5754/7134 completed (loss: 0.3146182894706726, acc: 0.9482758641242981)
[2025-02-13 19:41:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:16][root][INFO] - Training Epoch: 1/2, step 5755/7134 completed (loss: 0.28920724987983704, acc: 0.948051929473877)
[2025-02-13 19:41:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:17][root][INFO] - Training Epoch: 1/2, step 5756/7134 completed (loss: 0.31900331377983093, acc: 0.9011628031730652)
[2025-02-13 19:41:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:17][root][INFO] - Training Epoch: 1/2, step 5757/7134 completed (loss: 0.34093284606933594, acc: 0.9044944047927856)
[2025-02-13 19:41:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:17][root][INFO] - Training Epoch: 1/2, step 5758/7134 completed (loss: 0.4616434872150421, acc: 0.9197530746459961)
[2025-02-13 19:41:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:18][root][INFO] - Training Epoch: 1/2, step 5759/7134 completed (loss: 0.6280930042266846, acc: 0.8640776872634888)
[2025-02-13 19:41:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:18][root][INFO] - Training Epoch: 1/2, step 5760/7134 completed (loss: 0.20347514748573303, acc: 0.9350649118423462)
[2025-02-13 19:41:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:19][root][INFO] - Training Epoch: 1/2, step 5761/7134 completed (loss: 0.22972621023654938, acc: 0.9279999732971191)
[2025-02-13 19:41:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:19][root][INFO] - Training Epoch: 1/2, step 5762/7134 completed (loss: 0.22347204387187958, acc: 0.9689922332763672)
[2025-02-13 19:41:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:19][root][INFO] - Training Epoch: 1/2, step 5763/7134 completed (loss: 0.13153871893882751, acc: 0.9672130942344666)
[2025-02-13 19:41:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:20][root][INFO] - Training Epoch: 1/2, step 5764/7134 completed (loss: 0.07886935770511627, acc: 0.9770992398262024)
[2025-02-13 19:41:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:20][root][INFO] - Training Epoch: 1/2, step 5765/7134 completed (loss: 0.20839136838912964, acc: 0.9496855139732361)
[2025-02-13 19:41:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:20][root][INFO] - Training Epoch: 1/2, step 5766/7134 completed (loss: 0.10728777199983597, acc: 0.9806451797485352)
[2025-02-13 19:41:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:21][root][INFO] - Training Epoch: 1/2, step 5767/7134 completed (loss: 0.14656904339790344, acc: 0.9751552939414978)
[2025-02-13 19:41:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:21][root][INFO] - Training Epoch: 1/2, step 5768/7134 completed (loss: 0.08434052020311356, acc: 0.981249988079071)
[2025-02-13 19:41:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:22][root][INFO] - Training Epoch: 1/2, step 5769/7134 completed (loss: 0.050946541130542755, acc: 0.9857142567634583)
[2025-02-13 19:41:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:22][root][INFO] - Training Epoch: 1/2, step 5770/7134 completed (loss: 0.06971875578165054, acc: 0.9837837815284729)
[2025-02-13 19:41:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:22][root][INFO] - Training Epoch: 1/2, step 5771/7134 completed (loss: 0.21867482364177704, acc: 0.9570552110671997)
[2025-02-13 19:41:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:23][root][INFO] - Training Epoch: 1/2, step 5772/7134 completed (loss: 0.08968470990657806, acc: 0.9750000238418579)
[2025-02-13 19:41:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:23][root][INFO] - Training Epoch: 1/2, step 5773/7134 completed (loss: 0.1441449671983719, acc: 0.9624060392379761)
[2025-02-13 19:41:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:24][root][INFO] - Training Epoch: 1/2, step 5774/7134 completed (loss: 0.13822181522846222, acc: 0.9629629850387573)
[2025-02-13 19:41:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:24][root][INFO] - Training Epoch: 1/2, step 5775/7134 completed (loss: 0.12256975471973419, acc: 0.9466666579246521)
[2025-02-13 19:41:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:24][root][INFO] - Training Epoch: 1/2, step 5776/7134 completed (loss: 0.26622697710990906, acc: 0.9213483333587646)
[2025-02-13 19:41:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:25][root][INFO] - Training Epoch: 1/2, step 5777/7134 completed (loss: 0.20443055033683777, acc: 0.9719626307487488)
[2025-02-13 19:41:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:25][root][INFO] - Training Epoch: 1/2, step 5778/7134 completed (loss: 0.39015665650367737, acc: 0.9146341681480408)
[2025-02-13 19:41:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:26][root][INFO] - Training Epoch: 1/2, step 5779/7134 completed (loss: 0.15962952375411987, acc: 0.9588235020637512)
[2025-02-13 19:41:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:26][root][INFO] - Training Epoch: 1/2, step 5780/7134 completed (loss: 0.13392126560211182, acc: 0.9601989984512329)
[2025-02-13 19:41:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:26][root][INFO] - Training Epoch: 1/2, step 5781/7134 completed (loss: 0.361855685710907, acc: 0.9017341136932373)
[2025-02-13 19:41:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:27][root][INFO] - Training Epoch: 1/2, step 5782/7134 completed (loss: 0.2292121946811676, acc: 0.9558011293411255)
[2025-02-13 19:41:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:27][root][INFO] - Training Epoch: 1/2, step 5783/7134 completed (loss: 0.19197197258472443, acc: 0.9653179049491882)
[2025-02-13 19:41:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:28][root][INFO] - Training Epoch: 1/2, step 5784/7134 completed (loss: 0.19002340734004974, acc: 0.9435028433799744)
[2025-02-13 19:41:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:28][root][INFO] - Training Epoch: 1/2, step 5785/7134 completed (loss: 0.23804441094398499, acc: 0.9567567706108093)
[2025-02-13 19:41:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:28][root][INFO] - Training Epoch: 1/2, step 5786/7134 completed (loss: 0.14265593886375427, acc: 0.9444444179534912)
[2025-02-13 19:41:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:29][root][INFO] - Training Epoch: 1/2, step 5787/7134 completed (loss: 0.14880046248435974, acc: 0.9581151604652405)
[2025-02-13 19:41:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:29][root][INFO] - Training Epoch: 1/2, step 5788/7134 completed (loss: 0.08278634399175644, acc: 0.9802631735801697)
[2025-02-13 19:41:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:30][root][INFO] - Training Epoch: 1/2, step 5789/7134 completed (loss: 0.12618358433246613, acc: 0.9747474789619446)
[2025-02-13 19:41:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:30][root][INFO] - Training Epoch: 1/2, step 5790/7134 completed (loss: 0.2009379118680954, acc: 0.9583333134651184)
[2025-02-13 19:41:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:31][root][INFO] - Training Epoch: 1/2, step 5791/7134 completed (loss: 0.09459646791219711, acc: 0.9776119589805603)
[2025-02-13 19:41:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:31][root][INFO] - Training Epoch: 1/2, step 5792/7134 completed (loss: 0.5528941750526428, acc: 0.8724831938743591)
[2025-02-13 19:41:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:31][root][INFO] - Training Epoch: 1/2, step 5793/7134 completed (loss: 0.379006564617157, acc: 0.9141104221343994)
[2025-02-13 19:41:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:32][root][INFO] - Training Epoch: 1/2, step 5794/7134 completed (loss: 0.508788526058197, acc: 0.8662420511245728)
[2025-02-13 19:41:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:32][root][INFO] - Training Epoch: 1/2, step 5795/7134 completed (loss: 0.6231468915939331, acc: 0.8796992301940918)
[2025-02-13 19:41:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:32][root][INFO] - Training Epoch: 1/2, step 5796/7134 completed (loss: 0.4143778681755066, acc: 0.9007633328437805)
[2025-02-13 19:41:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:33][root][INFO] - Training Epoch: 1/2, step 5797/7134 completed (loss: 0.26426681876182556, acc: 0.926174521446228)
[2025-02-13 19:41:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:33][root][INFO] - Training Epoch: 1/2, step 5798/7134 completed (loss: 1.1092994213104248, acc: 0.8372092843055725)
[2025-02-13 19:41:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:34][root][INFO] - Training Epoch: 1/2, step 5799/7134 completed (loss: 0.3879780173301697, acc: 0.9291338324546814)
[2025-02-13 19:41:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:34][root][INFO] - Training Epoch: 1/2, step 5800/7134 completed (loss: 0.4505667984485626, acc: 0.9208633303642273)
[2025-02-13 19:41:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:34][root][INFO] - Training Epoch: 1/2, step 5801/7134 completed (loss: 0.4918949007987976, acc: 0.895061731338501)
[2025-02-13 19:41:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:35][root][INFO] - Training Epoch: 1/2, step 5802/7134 completed (loss: 0.20025792717933655, acc: 0.9558823704719543)
[2025-02-13 19:41:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:35][root][INFO] - Training Epoch: 1/2, step 5803/7134 completed (loss: 0.18596011400222778, acc: 0.959770143032074)
[2025-02-13 19:41:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:36][root][INFO] - Training Epoch: 1/2, step 5804/7134 completed (loss: 0.2970968782901764, acc: 0.8980891704559326)
[2025-02-13 19:41:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:36][root][INFO] - Training Epoch: 1/2, step 5805/7134 completed (loss: 0.27551788091659546, acc: 0.940397322177887)
[2025-02-13 19:41:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:37][root][INFO] - Training Epoch: 1/2, step 5806/7134 completed (loss: 0.22070099413394928, acc: 0.9454545378684998)
[2025-02-13 19:41:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:37][root][INFO] - Training Epoch: 1/2, step 5807/7134 completed (loss: 0.280836820602417, acc: 0.9295774698257446)
[2025-02-13 19:41:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:38][root][INFO] - Training Epoch: 1/2, step 5808/7134 completed (loss: 0.15970462560653687, acc: 0.9620253443717957)
[2025-02-13 19:41:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:38][root][INFO] - Training Epoch: 1/2, step 5809/7134 completed (loss: 0.13944366574287415, acc: 0.970588207244873)
[2025-02-13 19:41:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:38][root][INFO] - Training Epoch: 1/2, step 5810/7134 completed (loss: 0.11153607070446014, acc: 0.9586777091026306)
[2025-02-13 19:41:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:39][root][INFO] - Training Epoch: 1/2, step 5811/7134 completed (loss: 0.16119174659252167, acc: 0.9450549483299255)
[2025-02-13 19:41:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:39][root][INFO] - Training Epoch: 1/2, step 5812/7134 completed (loss: 0.103462815284729, acc: 0.9852941036224365)
[2025-02-13 19:41:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:39][root][INFO] - Training Epoch: 1/2, step 5813/7134 completed (loss: 0.1816561073064804, acc: 0.9754098653793335)
[2025-02-13 19:41:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:40][root][INFO] - Training Epoch: 1/2, step 5814/7134 completed (loss: 0.11887917667627335, acc: 0.9599999785423279)
[2025-02-13 19:41:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:40][root][INFO] - Training Epoch: 1/2, step 5815/7134 completed (loss: 0.1595226228237152, acc: 0.9482758641242981)
[2025-02-13 19:41:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:41][root][INFO] - Training Epoch: 1/2, step 5816/7134 completed (loss: 0.04541755095124245, acc: 0.9893048405647278)
[2025-02-13 19:41:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:41][root][INFO] - Training Epoch: 1/2, step 5817/7134 completed (loss: 0.21903261542320251, acc: 0.961904764175415)
[2025-02-13 19:41:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:41][root][INFO] - Training Epoch: 1/2, step 5818/7134 completed (loss: 0.14661410450935364, acc: 0.9477124214172363)
[2025-02-13 19:41:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:42][root][INFO] - Training Epoch: 1/2, step 5819/7134 completed (loss: 0.10390366613864899, acc: 0.98591548204422)
[2025-02-13 19:41:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:42][root][INFO] - Training Epoch: 1/2, step 5820/7134 completed (loss: 0.11849012225866318, acc: 0.9752066135406494)
[2025-02-13 19:41:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:43][root][INFO] - Training Epoch: 1/2, step 5821/7134 completed (loss: 0.06122499704360962, acc: 0.9862068891525269)
[2025-02-13 19:41:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:43][root][INFO] - Training Epoch: 1/2, step 5822/7134 completed (loss: 0.09451374411582947, acc: 0.9731543660163879)
[2025-02-13 19:41:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:43][root][INFO] - Training Epoch: 1/2, step 5823/7134 completed (loss: 0.08575452119112015, acc: 0.9863945841789246)
[2025-02-13 19:41:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:44][root][INFO] - Training Epoch: 1/2, step 5824/7134 completed (loss: 0.07922736555337906, acc: 0.9818181991577148)
[2025-02-13 19:41:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:44][root][INFO] - Training Epoch: 1/2, step 5825/7134 completed (loss: 0.21130387485027313, acc: 0.9465649127960205)
[2025-02-13 19:41:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:44][root][INFO] - Training Epoch: 1/2, step 5826/7134 completed (loss: 0.055609650909900665, acc: 0.9922480583190918)
[2025-02-13 19:41:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:45][root][INFO] - Training Epoch: 1/2, step 5827/7134 completed (loss: 0.16827817261219025, acc: 0.9677419066429138)
[2025-02-13 19:41:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:45][root][INFO] - Training Epoch: 1/2, step 5828/7134 completed (loss: 0.1446082592010498, acc: 0.9542483687400818)
[2025-02-13 19:41:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:46][root][INFO] - Training Epoch: 1/2, step 5829/7134 completed (loss: 0.07076404243707657, acc: 0.9878048896789551)
[2025-02-13 19:41:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:46][root][INFO] - Training Epoch: 1/2, step 5830/7134 completed (loss: 0.1103738471865654, acc: 0.9760000109672546)
[2025-02-13 19:41:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:46][root][INFO] - Training Epoch: 1/2, step 5831/7134 completed (loss: 0.08229215443134308, acc: 0.9795918464660645)
[2025-02-13 19:41:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:47][root][INFO] - Training Epoch: 1/2, step 5832/7134 completed (loss: 0.23020829260349274, acc: 0.9541984796524048)
[2025-02-13 19:41:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:47][root][INFO] - Training Epoch: 1/2, step 5833/7134 completed (loss: 0.09589734673500061, acc: 0.969924807548523)
[2025-02-13 19:41:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:48][root][INFO] - Training Epoch: 1/2, step 5834/7134 completed (loss: 0.1713697761297226, acc: 0.9532163739204407)
[2025-02-13 19:41:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:48][root][INFO] - Training Epoch: 1/2, step 5835/7134 completed (loss: 0.16221994161605835, acc: 0.9599999785423279)
[2025-02-13 19:41:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:48][root][INFO] - Training Epoch: 1/2, step 5836/7134 completed (loss: 0.34696874022483826, acc: 0.9108280539512634)
[2025-02-13 19:41:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:49][root][INFO] - Training Epoch: 1/2, step 5837/7134 completed (loss: 0.5859056711196899, acc: 0.9127516746520996)
[2025-02-13 19:41:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:49][root][INFO] - Training Epoch: 1/2, step 5838/7134 completed (loss: 0.20060253143310547, acc: 0.971222996711731)
[2025-02-13 19:41:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:50][root][INFO] - Training Epoch: 1/2, step 5839/7134 completed (loss: 0.4302879571914673, acc: 0.9202898740768433)
[2025-02-13 19:41:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:50][root][INFO] - Training Epoch: 1/2, step 5840/7134 completed (loss: 0.04821831360459328, acc: 0.9860140085220337)
[2025-02-13 19:41:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:50][root][INFO] - Training Epoch: 1/2, step 5841/7134 completed (loss: 0.41616159677505493, acc: 0.8812500238418579)
[2025-02-13 19:41:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:51][root][INFO] - Training Epoch: 1/2, step 5842/7134 completed (loss: 0.1657792031764984, acc: 0.942148745059967)
[2025-02-13 19:41:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:51][root][INFO] - Training Epoch: 1/2, step 5843/7134 completed (loss: 0.23941271007061005, acc: 0.9583333134651184)
[2025-02-13 19:41:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:52][root][INFO] - Training Epoch: 1/2, step 5844/7134 completed (loss: 0.18320254981517792, acc: 0.9397590160369873)
[2025-02-13 19:41:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:52][root][INFO] - Training Epoch: 1/2, step 5845/7134 completed (loss: 0.23756861686706543, acc: 0.9415584206581116)
[2025-02-13 19:41:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:52][root][INFO] - Training Epoch: 1/2, step 5846/7134 completed (loss: 0.15797042846679688, acc: 0.9404761791229248)
[2025-02-13 19:41:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:53][root][INFO] - Training Epoch: 1/2, step 5847/7134 completed (loss: 0.10859944671392441, acc: 0.9759036302566528)
[2025-02-13 19:41:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:53][root][INFO] - Training Epoch: 1/2, step 5848/7134 completed (loss: 0.20744197070598602, acc: 0.9391891956329346)
[2025-02-13 19:41:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:54][root][INFO] - Training Epoch: 1/2, step 5849/7134 completed (loss: 0.28780385851860046, acc: 0.9285714030265808)
[2025-02-13 19:41:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:54][root][INFO] - Training Epoch: 1/2, step 5850/7134 completed (loss: 0.18853390216827393, acc: 0.9507042169570923)
[2025-02-13 19:41:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:54][root][INFO] - Training Epoch: 1/2, step 5851/7134 completed (loss: 0.25063538551330566, acc: 0.9246575236320496)
[2025-02-13 19:41:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:55][root][INFO] - Training Epoch: 1/2, step 5852/7134 completed (loss: 0.1909557580947876, acc: 0.9510489702224731)
[2025-02-13 19:41:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:55][root][INFO] - Training Epoch: 1/2, step 5853/7134 completed (loss: 0.2564813792705536, acc: 0.9382022619247437)
[2025-02-13 19:41:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:55][root][INFO] - Training Epoch: 1/2, step 5854/7134 completed (loss: 0.30333706736564636, acc: 0.9307692050933838)
[2025-02-13 19:41:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:56][root][INFO] - Training Epoch: 1/2, step 5855/7134 completed (loss: 0.21786418557167053, acc: 0.9491525292396545)
[2025-02-13 19:41:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:56][root][INFO] - Training Epoch: 1/2, step 5856/7134 completed (loss: 0.34426531195640564, acc: 0.9154929518699646)
[2025-02-13 19:41:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:57][root][INFO] - Training Epoch: 1/2, step 5857/7134 completed (loss: 0.2446814477443695, acc: 0.9398496150970459)
[2025-02-13 19:41:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:57][root][INFO] - Training Epoch: 1/2, step 5858/7134 completed (loss: 0.16633136570453644, acc: 0.9449541568756104)
[2025-02-13 19:41:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:57][root][INFO] - Training Epoch: 1/2, step 5859/7134 completed (loss: 0.13977785408496857, acc: 0.9741379022598267)
[2025-02-13 19:41:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:58][root][INFO] - Training Epoch: 1/2, step 5860/7134 completed (loss: 0.2570059597492218, acc: 0.9318181872367859)
[2025-02-13 19:41:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:58][root][INFO] - Training Epoch: 1/2, step 5861/7134 completed (loss: 0.12208288908004761, acc: 0.9714285731315613)
[2025-02-13 19:41:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:59][root][INFO] - Training Epoch: 1/2, step 5862/7134 completed (loss: 0.2142341434955597, acc: 0.9406779408454895)
[2025-02-13 19:41:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:59][root][INFO] - Training Epoch: 1/2, step 5863/7134 completed (loss: 0.28341665863990784, acc: 0.9354838728904724)
[2025-02-13 19:41:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:41:59][root][INFO] - Training Epoch: 1/2, step 5864/7134 completed (loss: 0.3959417939186096, acc: 0.9181286692619324)
[2025-02-13 19:42:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:00][root][INFO] - Training Epoch: 1/2, step 5865/7134 completed (loss: 0.20327532291412354, acc: 0.9513888955116272)
[2025-02-13 19:42:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:00][root][INFO] - Training Epoch: 1/2, step 5866/7134 completed (loss: 0.08232451230287552, acc: 0.9780219793319702)
[2025-02-13 19:42:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:01][root][INFO] - Training Epoch: 1/2, step 5867/7134 completed (loss: 0.16437871754169464, acc: 0.9527027010917664)
[2025-02-13 19:42:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:01][root][INFO] - Training Epoch: 1/2, step 5868/7134 completed (loss: 0.19588886201381683, acc: 0.9542483687400818)
[2025-02-13 19:42:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:01][root][INFO] - Training Epoch: 1/2, step 5869/7134 completed (loss: 0.14804492890834808, acc: 0.9640718698501587)
[2025-02-13 19:42:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:02][root][INFO] - Training Epoch: 1/2, step 5870/7134 completed (loss: 0.14053227007389069, acc: 0.9629629850387573)
[2025-02-13 19:42:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:02][root][INFO] - Training Epoch: 1/2, step 5871/7134 completed (loss: 0.07478772848844528, acc: 0.963302731513977)
[2025-02-13 19:42:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:03][root][INFO] - Training Epoch: 1/2, step 5872/7134 completed (loss: 0.23718084394931793, acc: 0.9440993666648865)
[2025-02-13 19:42:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:03][root][INFO] - Training Epoch: 1/2, step 5873/7134 completed (loss: 0.2644505798816681, acc: 0.9465240836143494)
[2025-02-13 19:42:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:03][root][INFO] - Training Epoch: 1/2, step 5874/7134 completed (loss: 0.11805824935436249, acc: 0.976190447807312)
[2025-02-13 19:42:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:04][root][INFO] - Training Epoch: 1/2, step 5875/7134 completed (loss: 0.19179949164390564, acc: 0.939393937587738)
[2025-02-13 19:42:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:04][root][INFO] - Training Epoch: 1/2, step 5876/7134 completed (loss: 0.09055102616548538, acc: 0.9550561904907227)
[2025-02-13 19:42:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:05][root][INFO] - Training Epoch: 1/2, step 5877/7134 completed (loss: 0.12272795289754868, acc: 0.9720670580863953)
[2025-02-13 19:42:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:05][root][INFO] - Training Epoch: 1/2, step 5878/7134 completed (loss: 0.14442172646522522, acc: 0.9727272987365723)
[2025-02-13 19:42:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:05][root][INFO] - Training Epoch: 1/2, step 5879/7134 completed (loss: 0.0585404597222805, acc: 0.9888888597488403)
[2025-02-13 19:42:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:06][root][INFO] - Training Epoch: 1/2, step 5880/7134 completed (loss: 0.23242805898189545, acc: 0.9642857313156128)
[2025-02-13 19:42:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:06][root][INFO] - Training Epoch: 1/2, step 5881/7134 completed (loss: 0.12945209443569183, acc: 0.9487179517745972)
[2025-02-13 19:42:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:07][root][INFO] - Training Epoch: 1/2, step 5882/7134 completed (loss: 0.053679946810007095, acc: 0.9803921580314636)
[2025-02-13 19:42:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:07][root][INFO] - Training Epoch: 1/2, step 5883/7134 completed (loss: 0.07595781981945038, acc: 0.9769230484962463)
[2025-02-13 19:42:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:08][root][INFO] - Training Epoch: 1/2, step 5884/7134 completed (loss: 0.174137145280838, acc: 0.9523809552192688)
[2025-02-13 19:42:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:08][root][INFO] - Training Epoch: 1/2, step 5885/7134 completed (loss: 0.4004729390144348, acc: 0.9215686321258545)
[2025-02-13 19:42:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:08][root][INFO] - Training Epoch: 1/2, step 5886/7134 completed (loss: 0.11011207848787308, acc: 0.976190447807312)
[2025-02-13 19:42:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:09][root][INFO] - Training Epoch: 1/2, step 5887/7134 completed (loss: 0.09072530269622803, acc: 0.9750000238418579)
[2025-02-13 19:42:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:09][root][INFO] - Training Epoch: 1/2, step 5888/7134 completed (loss: 0.3875807821750641, acc: 0.9219858050346375)
[2025-02-13 19:42:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:10][root][INFO] - Training Epoch: 1/2, step 5889/7134 completed (loss: 0.11887006461620331, acc: 0.9807692170143127)
[2025-02-13 19:42:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:10][root][INFO] - Training Epoch: 1/2, step 5890/7134 completed (loss: 0.07215499132871628, acc: 0.9864864945411682)
[2025-02-13 19:42:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:10][root][INFO] - Training Epoch: 1/2, step 5891/7134 completed (loss: 0.20405888557434082, acc: 0.9642857313156128)
[2025-02-13 19:42:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:11][root][INFO] - Training Epoch: 1/2, step 5892/7134 completed (loss: 0.17537422478199005, acc: 0.9651162624359131)
[2025-02-13 19:42:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:11][root][INFO] - Training Epoch: 1/2, step 5893/7134 completed (loss: 0.09491530060768127, acc: 0.9815950989723206)
[2025-02-13 19:42:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:12][root][INFO] - Training Epoch: 1/2, step 5894/7134 completed (loss: 0.08040130883455276, acc: 0.9864864945411682)
[2025-02-13 19:42:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:12][root][INFO] - Training Epoch: 1/2, step 5895/7134 completed (loss: 0.10657874494791031, acc: 0.96875)
[2025-02-13 19:42:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:12][root][INFO] - Training Epoch: 1/2, step 5896/7134 completed (loss: 0.2571351230144501, acc: 0.9496855139732361)
[2025-02-13 19:42:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:13][root][INFO] - Training Epoch: 1/2, step 5897/7134 completed (loss: 0.07524576783180237, acc: 0.9772727489471436)
[2025-02-13 19:42:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:13][root][INFO] - Training Epoch: 1/2, step 5898/7134 completed (loss: 0.15596355497837067, acc: 0.9732142686843872)
[2025-02-13 19:42:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:14][root][INFO] - Training Epoch: 1/2, step 5899/7134 completed (loss: 0.14802929759025574, acc: 0.9743589758872986)
[2025-02-13 19:42:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:14][root][INFO] - Training Epoch: 1/2, step 5900/7134 completed (loss: 0.14146572351455688, acc: 0.9506173133850098)
[2025-02-13 19:42:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:14][root][INFO] - Training Epoch: 1/2, step 5901/7134 completed (loss: 0.32594048976898193, acc: 0.9175257682800293)
[2025-02-13 19:42:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:15][root][INFO] - Training Epoch: 1/2, step 5902/7134 completed (loss: 0.306026428937912, acc: 0.9489796161651611)
[2025-02-13 19:42:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:15][root][INFO] - Training Epoch: 1/2, step 5903/7134 completed (loss: 0.15756505727767944, acc: 0.9557521939277649)
[2025-02-13 19:42:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:16][root][INFO] - Training Epoch: 1/2, step 5904/7134 completed (loss: 0.19769150018692017, acc: 0.9436619877815247)
[2025-02-13 19:42:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:16][root][INFO] - Training Epoch: 1/2, step 5905/7134 completed (loss: 0.21078433096408844, acc: 0.9459459185600281)
[2025-02-13 19:42:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:16][root][INFO] - Training Epoch: 1/2, step 5906/7134 completed (loss: 0.22585523128509521, acc: 0.9346405267715454)
[2025-02-13 19:42:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:17][root][INFO] - Training Epoch: 1/2, step 5907/7134 completed (loss: 0.15995050966739655, acc: 0.9652777910232544)
[2025-02-13 19:42:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:17][root][INFO] - Training Epoch: 1/2, step 5908/7134 completed (loss: 0.17858344316482544, acc: 0.9545454382896423)
[2025-02-13 19:42:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:18][root][INFO] - Training Epoch: 1/2, step 5909/7134 completed (loss: 0.23228169977664948, acc: 0.9489051103591919)
[2025-02-13 19:42:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:18][root][INFO] - Training Epoch: 1/2, step 5910/7134 completed (loss: 0.32718825340270996, acc: 0.9271523356437683)
[2025-02-13 19:42:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:19][root][INFO] - Training Epoch: 1/2, step 5911/7134 completed (loss: 0.18007096648216248, acc: 0.9652174115180969)
[2025-02-13 19:42:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:19][root][INFO] - Training Epoch: 1/2, step 5912/7134 completed (loss: 0.10761343687772751, acc: 0.984375)
[2025-02-13 19:42:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:19][root][INFO] - Training Epoch: 1/2, step 5913/7134 completed (loss: 0.22620703279972076, acc: 0.9526627063751221)
[2025-02-13 19:42:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:20][root][INFO] - Training Epoch: 1/2, step 5914/7134 completed (loss: 0.27673161029815674, acc: 0.949999988079071)
[2025-02-13 19:42:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:20][root][INFO] - Training Epoch: 1/2, step 5915/7134 completed (loss: 0.14439135789871216, acc: 0.9770992398262024)
[2025-02-13 19:42:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:21][root][INFO] - Training Epoch: 1/2, step 5916/7134 completed (loss: 0.1722612828016281, acc: 0.9602649211883545)
[2025-02-13 19:42:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:21][root][INFO] - Training Epoch: 1/2, step 5917/7134 completed (loss: 0.07618539780378342, acc: 0.984000027179718)
[2025-02-13 19:42:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:21][root][INFO] - Training Epoch: 1/2, step 5918/7134 completed (loss: 0.2574882507324219, acc: 0.9370629191398621)
[2025-02-13 19:42:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:22][root][INFO] - Training Epoch: 1/2, step 5919/7134 completed (loss: 0.16754759848117828, acc: 0.9776119589805603)
[2025-02-13 19:42:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:22][root][INFO] - Training Epoch: 1/2, step 5920/7134 completed (loss: 0.12297137826681137, acc: 0.9626865386962891)
[2025-02-13 19:42:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:23][root][INFO] - Training Epoch: 1/2, step 5921/7134 completed (loss: 0.24992889165878296, acc: 0.9396551847457886)
[2025-02-13 19:42:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:23][root][INFO] - Training Epoch: 1/2, step 5922/7134 completed (loss: 0.8470004200935364, acc: 0.807692289352417)
[2025-02-13 19:42:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:23][root][INFO] - Training Epoch: 1/2, step 5923/7134 completed (loss: 0.2848544716835022, acc: 0.9430379867553711)
[2025-02-13 19:42:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:24][root][INFO] - Training Epoch: 1/2, step 5924/7134 completed (loss: 0.23601895570755005, acc: 0.932692289352417)
[2025-02-13 19:42:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:24][root][INFO] - Training Epoch: 1/2, step 5925/7134 completed (loss: 0.9639351963996887, acc: 0.8222222328186035)
[2025-02-13 19:42:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:25][root][INFO] - Training Epoch: 1/2, step 5926/7134 completed (loss: 0.39462780952453613, acc: 0.9144737124443054)
[2025-02-13 19:42:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:25][root][INFO] - Training Epoch: 1/2, step 5927/7134 completed (loss: 0.29910895228385925, acc: 0.9464285969734192)
[2025-02-13 19:42:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:25][root][INFO] - Training Epoch: 1/2, step 5928/7134 completed (loss: 1.0251848697662354, acc: 0.7972972989082336)
[2025-02-13 19:42:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:26][root][INFO] - Training Epoch: 1/2, step 5929/7134 completed (loss: 0.36200031638145447, acc: 0.9032257795333862)
[2025-02-13 19:42:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:26][root][INFO] - Training Epoch: 1/2, step 5930/7134 completed (loss: 0.7158090472221375, acc: 0.8429751992225647)
[2025-02-13 19:42:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:27][root][INFO] - Training Epoch: 1/2, step 5931/7134 completed (loss: 0.26633861660957336, acc: 0.9658119678497314)
[2025-02-13 19:42:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:27][root][INFO] - Training Epoch: 1/2, step 5932/7134 completed (loss: 0.6302492022514343, acc: 0.8705036044120789)
[2025-02-13 19:42:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:27][root][INFO] - Training Epoch: 1/2, step 5933/7134 completed (loss: 0.5861411690711975, acc: 0.8823529481887817)
[2025-02-13 19:42:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:28][root][INFO] - Training Epoch: 1/2, step 5934/7134 completed (loss: 0.3284469246864319, acc: 0.9285714030265808)
[2025-02-13 19:42:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:28][root][INFO] - Training Epoch: 1/2, step 5935/7134 completed (loss: 0.1481924206018448, acc: 0.9672130942344666)
[2025-02-13 19:42:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:29][root][INFO] - Training Epoch: 1/2, step 5936/7134 completed (loss: 0.4904683530330658, acc: 0.8895705342292786)
[2025-02-13 19:42:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:29][root][INFO] - Training Epoch: 1/2, step 5937/7134 completed (loss: 0.23508624732494354, acc: 0.9507042169570923)
[2025-02-13 19:42:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:29][root][INFO] - Training Epoch: 1/2, step 5938/7134 completed (loss: 0.14662054181098938, acc: 0.9774436354637146)
[2025-02-13 19:42:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:30][root][INFO] - Training Epoch: 1/2, step 5939/7134 completed (loss: 0.9023134708404541, acc: 0.8275862336158752)
[2025-02-13 19:42:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:30][root][INFO] - Training Epoch: 1/2, step 5940/7134 completed (loss: 0.3013172447681427, acc: 0.9192546606063843)
[2025-02-13 19:42:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:31][root][INFO] - Training Epoch: 1/2, step 5941/7134 completed (loss: 0.2839667797088623, acc: 0.9285714030265808)
[2025-02-13 19:42:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:31][root][INFO] - Training Epoch: 1/2, step 5942/7134 completed (loss: 0.11608105897903442, acc: 0.984000027179718)
[2025-02-13 19:42:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:31][root][INFO] - Training Epoch: 1/2, step 5943/7134 completed (loss: 0.09731899946928024, acc: 0.9754601120948792)
[2025-02-13 19:42:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:32][root][INFO] - Training Epoch: 1/2, step 5944/7134 completed (loss: 0.22395026683807373, acc: 0.9438775777816772)
[2025-02-13 19:42:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:32][root][INFO] - Training Epoch: 1/2, step 5945/7134 completed (loss: 0.09706411510705948, acc: 0.9822485446929932)
[2025-02-13 19:42:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:33][root][INFO] - Training Epoch: 1/2, step 5946/7134 completed (loss: 0.23902927339076996, acc: 0.9583333134651184)
[2025-02-13 19:42:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:33][root][INFO] - Training Epoch: 1/2, step 5947/7134 completed (loss: 0.26085346937179565, acc: 0.932692289352417)
[2025-02-13 19:42:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:33][root][INFO] - Training Epoch: 1/2, step 5948/7134 completed (loss: 0.3748311400413513, acc: 0.9186992049217224)
[2025-02-13 19:42:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:34][root][INFO] - Training Epoch: 1/2, step 5949/7134 completed (loss: 0.09282349795103073, acc: 0.9833333492279053)
[2025-02-13 19:42:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:34][root][INFO] - Training Epoch: 1/2, step 5950/7134 completed (loss: 0.2822006642818451, acc: 0.9207921028137207)
[2025-02-13 19:42:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:35][root][INFO] - Training Epoch: 1/2, step 5951/7134 completed (loss: 0.34338101744651794, acc: 0.9345238208770752)
[2025-02-13 19:42:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:35][root][INFO] - Training Epoch: 1/2, step 5952/7134 completed (loss: 0.7102090120315552, acc: 0.8881579041481018)
[2025-02-13 19:42:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:35][root][INFO] - Training Epoch: 1/2, step 5953/7134 completed (loss: 0.34397435188293457, acc: 0.8962264060974121)
[2025-02-13 19:42:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:36][root][INFO] - Training Epoch: 1/2, step 5954/7134 completed (loss: 0.15892931818962097, acc: 0.9702970385551453)
[2025-02-13 19:42:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:36][root][INFO] - Training Epoch: 1/2, step 5955/7134 completed (loss: 0.2067991942167282, acc: 0.9814814925193787)
[2025-02-13 19:42:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:37][root][INFO] - Training Epoch: 1/2, step 5956/7134 completed (loss: 0.34435078501701355, acc: 0.9333333373069763)
[2025-02-13 19:42:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:37][root][INFO] - Training Epoch: 1/2, step 5957/7134 completed (loss: 0.171246737241745, acc: 0.9634146094322205)
[2025-02-13 19:42:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:37][root][INFO] - Training Epoch: 1/2, step 5958/7134 completed (loss: 0.2211136370897293, acc: 0.9371428489685059)
[2025-02-13 19:42:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:38][root][INFO] - Training Epoch: 1/2, step 5959/7134 completed (loss: 0.24465113878250122, acc: 0.936170220375061)
[2025-02-13 19:42:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:38][root][INFO] - Training Epoch: 1/2, step 5960/7134 completed (loss: 0.055659692734479904, acc: 0.988095223903656)
[2025-02-13 19:42:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:39][root][INFO] - Training Epoch: 1/2, step 5961/7134 completed (loss: 0.2569407522678375, acc: 0.9259259104728699)
[2025-02-13 19:42:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:39][root][INFO] - Training Epoch: 1/2, step 5962/7134 completed (loss: 0.13902433216571808, acc: 0.9724137783050537)
[2025-02-13 19:42:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:40][root][INFO] - Training Epoch: 1/2, step 5963/7134 completed (loss: 0.10545177757740021, acc: 0.977142870426178)
[2025-02-13 19:42:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:40][root][INFO] - Training Epoch: 1/2, step 5964/7134 completed (loss: 0.02580123022198677, acc: 1.0)
[2025-02-13 19:42:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:40][root][INFO] - Training Epoch: 1/2, step 5965/7134 completed (loss: 0.1253720223903656, acc: 0.9743589758872986)
[2025-02-13 19:42:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:41][root][INFO] - Training Epoch: 1/2, step 5966/7134 completed (loss: 0.14622549712657928, acc: 0.967391312122345)
[2025-02-13 19:42:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:41][root][INFO] - Training Epoch: 1/2, step 5967/7134 completed (loss: 0.14348195493221283, acc: 0.9583333134651184)
[2025-02-13 19:42:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:42][root][INFO] - Training Epoch: 1/2, step 5968/7134 completed (loss: 0.28246864676475525, acc: 0.9599999785423279)
[2025-02-13 19:42:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:42][root][INFO] - Training Epoch: 1/2, step 5969/7134 completed (loss: 0.19670340418815613, acc: 0.948387086391449)
[2025-02-13 19:42:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:42][root][INFO] - Training Epoch: 1/2, step 5970/7134 completed (loss: 0.13559384644031525, acc: 0.9765625)
[2025-02-13 19:42:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:43][root][INFO] - Training Epoch: 1/2, step 5971/7134 completed (loss: 0.07912563532590866, acc: 0.9789473414421082)
[2025-02-13 19:42:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:43][root][INFO] - Training Epoch: 1/2, step 5972/7134 completed (loss: 0.278138667345047, acc: 0.9263803958892822)
[2025-02-13 19:42:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:44][root][INFO] - Training Epoch: 1/2, step 5973/7134 completed (loss: 0.20500828325748444, acc: 0.9657142758369446)
[2025-02-13 19:42:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:44][root][INFO] - Training Epoch: 1/2, step 5974/7134 completed (loss: 0.09414126724004745, acc: 0.9852941036224365)
[2025-02-13 19:42:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:44][root][INFO] - Training Epoch: 1/2, step 5975/7134 completed (loss: 0.0882301777601242, acc: 0.9756097793579102)
[2025-02-13 19:42:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:45][root][INFO] - Training Epoch: 1/2, step 5976/7134 completed (loss: 0.1669362187385559, acc: 0.9738562107086182)
[2025-02-13 19:42:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:45][root][INFO] - Training Epoch: 1/2, step 5977/7134 completed (loss: 0.13190895318984985, acc: 0.9700000286102295)
[2025-02-13 19:42:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:46][root][INFO] - Training Epoch: 1/2, step 5978/7134 completed (loss: 0.19217367470264435, acc: 0.9627329111099243)
[2025-02-13 19:42:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:46][root][INFO] - Training Epoch: 1/2, step 5979/7134 completed (loss: 0.19705675542354584, acc: 0.9684210419654846)
[2025-02-13 19:42:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:46][root][INFO] - Training Epoch: 1/2, step 5980/7134 completed (loss: 0.09466993063688278, acc: 0.97826087474823)
[2025-02-13 19:42:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:47][root][INFO] - Training Epoch: 1/2, step 5981/7134 completed (loss: 0.13756556808948517, acc: 0.9796954393386841)
[2025-02-13 19:42:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:47][root][INFO] - Training Epoch: 1/2, step 5982/7134 completed (loss: 0.09637293219566345, acc: 0.9838709831237793)
[2025-02-13 19:42:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:48][root][INFO] - Training Epoch: 1/2, step 5983/7134 completed (loss: 0.18038082122802734, acc: 0.959770143032074)
[2025-02-13 19:42:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:48][root][INFO] - Training Epoch: 1/2, step 5984/7134 completed (loss: 0.11597993969917297, acc: 0.9757575988769531)
[2025-02-13 19:42:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:48][root][INFO] - Training Epoch: 1/2, step 5985/7134 completed (loss: 0.1587391346693039, acc: 0.9550561904907227)
[2025-02-13 19:42:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:49][root][INFO] - Training Epoch: 1/2, step 5986/7134 completed (loss: 0.20337842404842377, acc: 0.9629629850387573)
[2025-02-13 19:42:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:49][root][INFO] - Training Epoch: 1/2, step 5987/7134 completed (loss: 0.17774036526679993, acc: 0.9560975432395935)
[2025-02-13 19:42:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:50][root][INFO] - Training Epoch: 1/2, step 5988/7134 completed (loss: 0.06488918513059616, acc: 0.9888888597488403)
[2025-02-13 19:42:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:50][root][INFO] - Training Epoch: 1/2, step 5989/7134 completed (loss: 0.1783963143825531, acc: 0.9615384340286255)
[2025-02-13 19:42:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:50][root][INFO] - Training Epoch: 1/2, step 5990/7134 completed (loss: 0.22228577733039856, acc: 0.9668508172035217)
[2025-02-13 19:42:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:51][root][INFO] - Training Epoch: 1/2, step 5991/7134 completed (loss: 0.2547619044780731, acc: 0.9545454382896423)
[2025-02-13 19:42:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:51][root][INFO] - Training Epoch: 1/2, step 5992/7134 completed (loss: 0.06671151518821716, acc: 0.9897959232330322)
[2025-02-13 19:42:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:52][root][INFO] - Training Epoch: 1/2, step 5993/7134 completed (loss: 0.11513540893793106, acc: 0.966292142868042)
[2025-02-13 19:42:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:52][root][INFO] - Training Epoch: 1/2, step 5994/7134 completed (loss: 0.062357861548662186, acc: 0.9917355179786682)
[2025-02-13 19:42:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:52][root][INFO] - Training Epoch: 1/2, step 5995/7134 completed (loss: 0.10619246959686279, acc: 0.9826589822769165)
[2025-02-13 19:42:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:53][root][INFO] - Training Epoch: 1/2, step 5996/7134 completed (loss: 0.13051526248455048, acc: 0.9866666793823242)
[2025-02-13 19:42:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:53][root][INFO] - Training Epoch: 1/2, step 5997/7134 completed (loss: 0.18920888006687164, acc: 0.9602272510528564)
[2025-02-13 19:42:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:54][root][INFO] - Training Epoch: 1/2, step 5998/7134 completed (loss: 0.14651772379875183, acc: 0.9470899701118469)
[2025-02-13 19:42:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:54][root][INFO] - Training Epoch: 1/2, step 5999/7134 completed (loss: 0.08184587955474854, acc: 0.990338146686554)
[2025-02-13 19:42:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:54][root][INFO] - Training Epoch: 1/2, step 6000/7134 completed (loss: 0.0998338907957077, acc: 0.98591548204422)
[2025-02-13 19:42:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:55][root][INFO] - Training Epoch: 1/2, step 6001/7134 completed (loss: 0.08235000818967819, acc: 0.987730085849762)
[2025-02-13 19:42:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:55][root][INFO] - Training Epoch: 1/2, step 6002/7134 completed (loss: 0.1838347464799881, acc: 0.9669421315193176)
[2025-02-13 19:42:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:56][root][INFO] - Training Epoch: 1/2, step 6003/7134 completed (loss: 0.21528464555740356, acc: 0.948051929473877)
[2025-02-13 19:42:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:56][root][INFO] - Training Epoch: 1/2, step 6004/7134 completed (loss: 0.47832441329956055, acc: 0.9190751314163208)
[2025-02-13 19:42:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:56][root][INFO] - Training Epoch: 1/2, step 6005/7134 completed (loss: 0.23173275589942932, acc: 0.9470198750495911)
[2025-02-13 19:42:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:57][root][INFO] - Training Epoch: 1/2, step 6006/7134 completed (loss: 0.2918131649494171, acc: 0.9279999732971191)
[2025-02-13 19:42:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:57][root][INFO] - Training Epoch: 1/2, step 6007/7134 completed (loss: 0.1698121428489685, acc: 0.9558823704719543)
[2025-02-13 19:42:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:58][root][INFO] - Training Epoch: 1/2, step 6008/7134 completed (loss: 0.23423638939857483, acc: 0.9300699234008789)
[2025-02-13 19:42:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:58][root][INFO] - Training Epoch: 1/2, step 6009/7134 completed (loss: 0.22827348113059998, acc: 0.9398496150970459)
[2025-02-13 19:42:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:58][root][INFO] - Training Epoch: 1/2, step 6010/7134 completed (loss: 0.2792351543903351, acc: 0.9303797483444214)
[2025-02-13 19:42:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:59][root][INFO] - Training Epoch: 1/2, step 6011/7134 completed (loss: 0.3527337610721588, acc: 0.9285714030265808)
[2025-02-13 19:42:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:42:59][root][INFO] - Training Epoch: 1/2, step 6012/7134 completed (loss: 0.19067619740962982, acc: 0.9515151381492615)
[2025-02-13 19:42:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:00][root][INFO] - Training Epoch: 1/2, step 6013/7134 completed (loss: 0.4070715606212616, acc: 0.915730357170105)
[2025-02-13 19:43:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:00][root][INFO] - Training Epoch: 1/2, step 6014/7134 completed (loss: 0.32383012771606445, acc: 0.913385808467865)
[2025-02-13 19:43:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:00][root][INFO] - Training Epoch: 1/2, step 6015/7134 completed (loss: 0.3477495312690735, acc: 0.9189189076423645)
[2025-02-13 19:43:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:01][root][INFO] - Training Epoch: 1/2, step 6016/7134 completed (loss: 0.4919237196445465, acc: 0.8918918967247009)
[2025-02-13 19:43:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:01][root][INFO] - Training Epoch: 1/2, step 6017/7134 completed (loss: 0.4106366038322449, acc: 0.9038461446762085)
[2025-02-13 19:43:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:02][root][INFO] - Training Epoch: 1/2, step 6018/7134 completed (loss: 0.3103887140750885, acc: 0.9230769276618958)
[2025-02-13 19:43:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:02][root][INFO] - Training Epoch: 1/2, step 6019/7134 completed (loss: 0.33217525482177734, acc: 0.9166666865348816)
[2025-02-13 19:43:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:02][root][INFO] - Training Epoch: 1/2, step 6020/7134 completed (loss: 0.3598077893257141, acc: 0.9365079402923584)
[2025-02-13 19:43:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:03][root][INFO] - Training Epoch: 1/2, step 6021/7134 completed (loss: 0.14115352928638458, acc: 0.9512194991111755)
[2025-02-13 19:43:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:03][root][INFO] - Training Epoch: 1/2, step 6022/7134 completed (loss: 0.19663332402706146, acc: 0.9473684430122375)
[2025-02-13 19:43:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:03][root][INFO] - Training Epoch: 1/2, step 6023/7134 completed (loss: 0.29170069098472595, acc: 0.9545454382896423)
[2025-02-13 19:43:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:04][root][INFO] - Training Epoch: 1/2, step 6024/7134 completed (loss: 0.03454551473259926, acc: 1.0)
[2025-02-13 19:43:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:04][root][INFO] - Training Epoch: 1/2, step 6025/7134 completed (loss: 0.2718631625175476, acc: 0.9344262480735779)
[2025-02-13 19:43:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:05][root][INFO] - Training Epoch: 1/2, step 6026/7134 completed (loss: 1.003635048866272, acc: 0.8135592937469482)
[2025-02-13 19:43:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:05][root][INFO] - Training Epoch: 1/2, step 6027/7134 completed (loss: 0.08912306278944016, acc: 0.966292142868042)
[2025-02-13 19:43:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:05][root][INFO] - Training Epoch: 1/2, step 6028/7134 completed (loss: 0.2253427654504776, acc: 0.949999988079071)
[2025-02-13 19:43:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:06][root][INFO] - Training Epoch: 1/2, step 6029/7134 completed (loss: 0.12374810874462128, acc: 0.9718309640884399)
[2025-02-13 19:43:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:06][root][INFO] - Training Epoch: 1/2, step 6030/7134 completed (loss: 0.1059470921754837, acc: 0.9696969985961914)
[2025-02-13 19:43:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:07][root][INFO] - Training Epoch: 1/2, step 6031/7134 completed (loss: 0.1539178043603897, acc: 0.9534883499145508)
[2025-02-13 19:43:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:07][root][INFO] - Training Epoch: 1/2, step 6032/7134 completed (loss: 0.09427089244127274, acc: 0.9702970385551453)
[2025-02-13 19:43:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:07][root][INFO] - Training Epoch: 1/2, step 6033/7134 completed (loss: 0.1787468045949936, acc: 0.9545454382896423)
[2025-02-13 19:43:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:08][root][INFO] - Training Epoch: 1/2, step 6034/7134 completed (loss: 0.4204859733581543, acc: 0.9253731369972229)
[2025-02-13 19:43:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:08][root][INFO] - Training Epoch: 1/2, step 6035/7134 completed (loss: 0.6658303141593933, acc: 0.8544601202011108)
[2025-02-13 19:43:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:09][root][INFO] - Training Epoch: 1/2, step 6036/7134 completed (loss: 0.48750171065330505, acc: 0.8876404762268066)
[2025-02-13 19:43:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:09][root][INFO] - Training Epoch: 1/2, step 6037/7134 completed (loss: 0.27715978026390076, acc: 0.914893627166748)
[2025-02-13 19:43:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:09][root][INFO] - Training Epoch: 1/2, step 6038/7134 completed (loss: 0.31900423765182495, acc: 0.9215686321258545)
[2025-02-13 19:43:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:10][root][INFO] - Training Epoch: 1/2, step 6039/7134 completed (loss: 0.3219611942768097, acc: 0.9219512343406677)
[2025-02-13 19:43:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:10][root][INFO] - Training Epoch: 1/2, step 6040/7134 completed (loss: 0.20485329627990723, acc: 0.933920681476593)
[2025-02-13 19:43:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:11][root][INFO] - Training Epoch: 1/2, step 6041/7134 completed (loss: 0.3973644971847534, acc: 0.8817204236984253)
[2025-02-13 19:43:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:11][root][INFO] - Training Epoch: 1/2, step 6042/7134 completed (loss: 0.13767634332180023, acc: 0.963302731513977)
[2025-02-13 19:43:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:12][root][INFO] - Training Epoch: 1/2, step 6043/7134 completed (loss: 0.249016672372818, acc: 0.9462365508079529)
[2025-02-13 19:43:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:12][root][INFO] - Training Epoch: 1/2, step 6044/7134 completed (loss: 0.16355091333389282, acc: 0.9476190209388733)
[2025-02-13 19:43:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:12][root][INFO] - Training Epoch: 1/2, step 6045/7134 completed (loss: 0.23501773178577423, acc: 0.9330143332481384)
[2025-02-13 19:43:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:13][root][INFO] - Training Epoch: 1/2, step 6046/7134 completed (loss: 0.4890328049659729, acc: 0.8972602486610413)
[2025-02-13 19:43:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:13][root][INFO] - Training Epoch: 1/2, step 6047/7134 completed (loss: 0.4393494725227356, acc: 0.918367326259613)
[2025-02-13 19:43:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:14][root][INFO] - Training Epoch: 1/2, step 6048/7134 completed (loss: 0.2994106113910675, acc: 0.8943089246749878)
[2025-02-13 19:43:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:14][root][INFO] - Training Epoch: 1/2, step 6049/7134 completed (loss: 0.24845653772354126, acc: 0.9230769276618958)
[2025-02-13 19:43:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:14][root][INFO] - Training Epoch: 1/2, step 6050/7134 completed (loss: 0.37274226546287537, acc: 0.8786126971244812)
[2025-02-13 19:43:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:15][root][INFO] - Training Epoch: 1/2, step 6051/7134 completed (loss: 0.4875582456588745, acc: 0.8806818127632141)
[2025-02-13 19:43:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:15][root][INFO] - Training Epoch: 1/2, step 6052/7134 completed (loss: 0.3796916604042053, acc: 0.8733333349227905)
[2025-02-13 19:43:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:16][root][INFO] - Training Epoch: 1/2, step 6053/7134 completed (loss: 0.2827492356300354, acc: 0.9159291982650757)
[2025-02-13 19:43:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:16][root][INFO] - Training Epoch: 1/2, step 6054/7134 completed (loss: 0.22291645407676697, acc: 0.9378238320350647)
[2025-02-13 19:43:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:16][root][INFO] - Training Epoch: 1/2, step 6055/7134 completed (loss: 0.3402647376060486, acc: 0.9415584206581116)
[2025-02-13 19:43:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:17][root][INFO] - Training Epoch: 1/2, step 6056/7134 completed (loss: 0.6497551202774048, acc: 0.8558558821678162)
[2025-02-13 19:43:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:17][root][INFO] - Training Epoch: 1/2, step 6057/7134 completed (loss: 0.3101220428943634, acc: 0.9220778942108154)
[2025-02-13 19:43:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:18][root][INFO] - Training Epoch: 1/2, step 6058/7134 completed (loss: 0.24969901144504547, acc: 0.9322034120559692)
[2025-02-13 19:43:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:18][root][INFO] - Training Epoch: 1/2, step 6059/7134 completed (loss: 0.18847692012786865, acc: 0.931506872177124)
[2025-02-13 19:43:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:18][root][INFO] - Training Epoch: 1/2, step 6060/7134 completed (loss: 0.2082575559616089, acc: 0.9629629850387573)
[2025-02-13 19:43:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:19][root][INFO] - Training Epoch: 1/2, step 6061/7134 completed (loss: 0.28854379057884216, acc: 0.9154929518699646)
[2025-02-13 19:43:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:19][root][INFO] - Training Epoch: 1/2, step 6062/7134 completed (loss: 0.35042762756347656, acc: 0.9047619104385376)
[2025-02-13 19:43:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:20][root][INFO] - Training Epoch: 1/2, step 6063/7134 completed (loss: 0.2371366024017334, acc: 0.9406779408454895)
[2025-02-13 19:43:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:20][root][INFO] - Training Epoch: 1/2, step 6064/7134 completed (loss: 0.48024147748947144, acc: 0.888198733329773)
[2025-02-13 19:43:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:20][root][INFO] - Training Epoch: 1/2, step 6065/7134 completed (loss: 0.19755730032920837, acc: 0.9593908786773682)
[2025-02-13 19:43:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:21][root][INFO] - Training Epoch: 1/2, step 6066/7134 completed (loss: 0.5120463371276855, acc: 0.8296296000480652)
[2025-02-13 19:43:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:21][root][INFO] - Training Epoch: 1/2, step 6067/7134 completed (loss: 0.46599093079566956, acc: 0.8815789222717285)
[2025-02-13 19:43:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:22][root][INFO] - Training Epoch: 1/2, step 6068/7134 completed (loss: 0.6273396611213684, acc: 0.8486486673355103)
[2025-02-13 19:43:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:22][root][INFO] - Training Epoch: 1/2, step 6069/7134 completed (loss: 0.3745880722999573, acc: 0.8764705657958984)
[2025-02-13 19:43:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:22][root][INFO] - Training Epoch: 1/2, step 6070/7134 completed (loss: 0.34652504324913025, acc: 0.9142857193946838)
[2025-02-13 19:43:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:23][root][INFO] - Training Epoch: 1/2, step 6071/7134 completed (loss: 0.35218361020088196, acc: 0.8875739574432373)
[2025-02-13 19:43:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:23][root][INFO] - Training Epoch: 1/2, step 6072/7134 completed (loss: 0.6009275317192078, acc: 0.8583333492279053)
[2025-02-13 19:43:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:24][root][INFO] - Training Epoch: 1/2, step 6073/7134 completed (loss: 0.28378644585609436, acc: 0.9171974658966064)
[2025-02-13 19:43:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:24][root][INFO] - Training Epoch: 1/2, step 6074/7134 completed (loss: 0.500507652759552, acc: 0.9096774458885193)
[2025-02-13 19:43:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:25][root][INFO] - Training Epoch: 1/2, step 6075/7134 completed (loss: 0.2804562449455261, acc: 0.9315789341926575)
[2025-02-13 19:43:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:25][root][INFO] - Training Epoch: 1/2, step 6076/7134 completed (loss: 0.4948526918888092, acc: 0.866310179233551)
[2025-02-13 19:43:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:25][root][INFO] - Training Epoch: 1/2, step 6077/7134 completed (loss: 0.18586687743663788, acc: 0.9526627063751221)
[2025-02-13 19:43:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:26][root][INFO] - Training Epoch: 1/2, step 6078/7134 completed (loss: 0.34979090094566345, acc: 0.909604549407959)
[2025-02-13 19:43:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:26][root][INFO] - Training Epoch: 1/2, step 6079/7134 completed (loss: 0.21603406965732574, acc: 0.9469696879386902)
[2025-02-13 19:43:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:27][root][INFO] - Training Epoch: 1/2, step 6080/7134 completed (loss: 0.1718219667673111, acc: 0.959770143032074)
[2025-02-13 19:43:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:27][root][INFO] - Training Epoch: 1/2, step 6081/7134 completed (loss: 0.3798263370990753, acc: 0.9324324131011963)
[2025-02-13 19:43:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:27][root][INFO] - Training Epoch: 1/2, step 6082/7134 completed (loss: 0.12354112416505814, acc: 0.9664429426193237)
[2025-02-13 19:43:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:28][root][INFO] - Training Epoch: 1/2, step 6083/7134 completed (loss: 0.12209407240152359, acc: 0.9740932583808899)
[2025-02-13 19:43:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:28][root][INFO] - Training Epoch: 1/2, step 6084/7134 completed (loss: 0.22094810009002686, acc: 0.95333331823349)
[2025-02-13 19:43:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:29][root][INFO] - Training Epoch: 1/2, step 6085/7134 completed (loss: 0.1764257550239563, acc: 0.9583333134651184)
[2025-02-13 19:43:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:29][root][INFO] - Training Epoch: 1/2, step 6086/7134 completed (loss: 0.10043758153915405, acc: 0.9562841653823853)
[2025-02-13 19:43:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:29][root][INFO] - Training Epoch: 1/2, step 6087/7134 completed (loss: 0.26889052987098694, acc: 0.9411764740943909)
[2025-02-13 19:43:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:30][root][INFO] - Training Epoch: 1/2, step 6088/7134 completed (loss: 0.3771509826183319, acc: 0.9135135412216187)
[2025-02-13 19:43:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:30][root][INFO] - Training Epoch: 1/2, step 6089/7134 completed (loss: 0.16360431909561157, acc: 0.954285740852356)
[2025-02-13 19:43:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:31][root][INFO] - Training Epoch: 1/2, step 6090/7134 completed (loss: 0.1314929574728012, acc: 0.9679144620895386)
[2025-02-13 19:43:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:31][root][INFO] - Training Epoch: 1/2, step 6091/7134 completed (loss: 0.17966774106025696, acc: 0.9617834687232971)
[2025-02-13 19:43:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:31][root][INFO] - Training Epoch: 1/2, step 6092/7134 completed (loss: 0.1495191603899002, acc: 0.939393937587738)
[2025-02-13 19:43:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:32][root][INFO] - Training Epoch: 1/2, step 6093/7134 completed (loss: 0.2291574478149414, acc: 0.9477611780166626)
[2025-02-13 19:43:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:32][root][INFO] - Training Epoch: 1/2, step 6094/7134 completed (loss: 0.4049363136291504, acc: 0.8977272510528564)
[2025-02-13 19:43:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:33][root][INFO] - Training Epoch: 1/2, step 6095/7134 completed (loss: 0.19590188562870026, acc: 0.9509202241897583)
[2025-02-13 19:43:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:33][root][INFO] - Training Epoch: 1/2, step 6096/7134 completed (loss: 0.33112505078315735, acc: 0.8984771370887756)
[2025-02-13 19:43:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:34][root][INFO] - Training Epoch: 1/2, step 6097/7134 completed (loss: 0.3049406111240387, acc: 0.9130434989929199)
[2025-02-13 19:43:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:34][root][INFO] - Training Epoch: 1/2, step 6098/7134 completed (loss: 0.2728795111179352, acc: 0.9298245906829834)
[2025-02-13 19:43:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:35][root][INFO] - Training Epoch: 1/2, step 6099/7134 completed (loss: 0.23792757093906403, acc: 0.9455782175064087)
[2025-02-13 19:43:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:35][root][INFO] - Training Epoch: 1/2, step 6100/7134 completed (loss: 0.45934194326400757, acc: 0.9358974099159241)
[2025-02-13 19:43:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:35][root][INFO] - Training Epoch: 1/2, step 6101/7134 completed (loss: 0.07374531030654907, acc: 0.9722222089767456)
[2025-02-13 19:43:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:36][root][INFO] - Training Epoch: 1/2, step 6102/7134 completed (loss: 0.2880845069885254, acc: 0.9296875)
[2025-02-13 19:43:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:36][root][INFO] - Training Epoch: 1/2, step 6103/7134 completed (loss: 0.20251473784446716, acc: 0.9545454382896423)
[2025-02-13 19:43:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:37][root][INFO] - Training Epoch: 1/2, step 6104/7134 completed (loss: 0.41855382919311523, acc: 0.8851351141929626)
[2025-02-13 19:43:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:37][root][INFO] - Training Epoch: 1/2, step 6105/7134 completed (loss: 0.6574767827987671, acc: 0.8484848737716675)
[2025-02-13 19:43:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:37][root][INFO] - Training Epoch: 1/2, step 6106/7134 completed (loss: 0.2527627646923065, acc: 0.9527559280395508)
[2025-02-13 19:43:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:38][root][INFO] - Training Epoch: 1/2, step 6107/7134 completed (loss: 0.2015843540430069, acc: 0.9487179517745972)
[2025-02-13 19:43:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:38][root][INFO] - Training Epoch: 1/2, step 6108/7134 completed (loss: 0.21387240290641785, acc: 0.9504132270812988)
[2025-02-13 19:43:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:39][root][INFO] - Training Epoch: 1/2, step 6109/7134 completed (loss: 0.16098114848136902, acc: 0.947826087474823)
[2025-02-13 19:43:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:39][root][INFO] - Training Epoch: 1/2, step 6110/7134 completed (loss: 0.506244421005249, acc: 0.8828125)
[2025-02-13 19:43:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:40][root][INFO] - Training Epoch: 1/2, step 6111/7134 completed (loss: 0.31478551030158997, acc: 0.9347826242446899)
[2025-02-13 19:43:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:40][root][INFO] - Training Epoch: 1/2, step 6112/7134 completed (loss: 0.34229591488838196, acc: 0.9346405267715454)
[2025-02-13 19:43:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:40][root][INFO] - Training Epoch: 1/2, step 6113/7134 completed (loss: 0.2925110459327698, acc: 0.9390243887901306)
[2025-02-13 19:43:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:41][root][INFO] - Training Epoch: 1/2, step 6114/7134 completed (loss: 0.14264991879463196, acc: 0.9720279574394226)
[2025-02-13 19:43:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:41][root][INFO] - Training Epoch: 1/2, step 6115/7134 completed (loss: 0.15384015440940857, acc: 0.9760000109672546)
[2025-02-13 19:43:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:41][root][INFO] - Training Epoch: 1/2, step 6116/7134 completed (loss: 0.3981207013130188, acc: 0.9193548560142517)
[2025-02-13 19:43:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:42][root][INFO] - Training Epoch: 1/2, step 6117/7134 completed (loss: 0.338397353887558, acc: 0.940119743347168)
[2025-02-13 19:43:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:42][root][INFO] - Training Epoch: 1/2, step 6118/7134 completed (loss: 0.14204053580760956, acc: 0.9523809552192688)
[2025-02-13 19:43:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:43][root][INFO] - Training Epoch: 1/2, step 6119/7134 completed (loss: 0.16306494176387787, acc: 0.9513513445854187)
[2025-02-13 19:43:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:43][root][INFO] - Training Epoch: 1/2, step 6120/7134 completed (loss: 0.14785760641098022, acc: 0.9772727489471436)
[2025-02-13 19:43:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:44][root][INFO] - Training Epoch: 1/2, step 6121/7134 completed (loss: 0.26694050431251526, acc: 0.9370078444480896)
[2025-02-13 19:43:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:44][root][INFO] - Training Epoch: 1/2, step 6122/7134 completed (loss: 0.12022272497415543, acc: 0.975806474685669)
[2025-02-13 19:43:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:44][root][INFO] - Training Epoch: 1/2, step 6123/7134 completed (loss: 0.34215882420539856, acc: 0.9487179517745972)
[2025-02-13 19:43:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:45][root][INFO] - Training Epoch: 1/2, step 6124/7134 completed (loss: 0.13812324404716492, acc: 0.9663865566253662)
[2025-02-13 19:43:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:45][root][INFO] - Training Epoch: 1/2, step 6125/7134 completed (loss: 0.5210539698600769, acc: 0.9439252614974976)
[2025-02-13 19:43:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:46][root][INFO] - Training Epoch: 1/2, step 6126/7134 completed (loss: 0.19262927770614624, acc: 0.9668874144554138)
[2025-02-13 19:43:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:46][root][INFO] - Training Epoch: 1/2, step 6127/7134 completed (loss: 0.06840956956148148, acc: 0.9803921580314636)
[2025-02-13 19:43:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:46][root][INFO] - Training Epoch: 1/2, step 6128/7134 completed (loss: 0.2173091024160385, acc: 0.9495798349380493)
[2025-02-13 19:43:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:47][root][INFO] - Training Epoch: 1/2, step 6129/7134 completed (loss: 0.470443457365036, acc: 0.88165682554245)
[2025-02-13 19:43:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:47][root][INFO] - Training Epoch: 1/2, step 6130/7134 completed (loss: 0.3201678991317749, acc: 0.9029850959777832)
[2025-02-13 19:43:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:48][root][INFO] - Training Epoch: 1/2, step 6131/7134 completed (loss: 0.40635159611701965, acc: 0.9029126167297363)
[2025-02-13 19:43:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:48][root][INFO] - Training Epoch: 1/2, step 6132/7134 completed (loss: 0.23004058003425598, acc: 0.9551281929016113)
[2025-02-13 19:43:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:48][root][INFO] - Training Epoch: 1/2, step 6133/7134 completed (loss: 0.3679237365722656, acc: 0.9230769276618958)
[2025-02-13 19:43:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:49][root][INFO] - Training Epoch: 1/2, step 6134/7134 completed (loss: 0.31924375891685486, acc: 0.9075144529342651)
[2025-02-13 19:43:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:49][root][INFO] - Training Epoch: 1/2, step 6135/7134 completed (loss: 0.17423370480537415, acc: 0.961240291595459)
[2025-02-13 19:43:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:50][root][INFO] - Training Epoch: 1/2, step 6136/7134 completed (loss: 0.28044939041137695, acc: 0.9580838084220886)
[2025-02-13 19:43:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:50][root][INFO] - Training Epoch: 1/2, step 6137/7134 completed (loss: 0.21639595925807953, acc: 0.9610389471054077)
[2025-02-13 19:43:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:50][root][INFO] - Training Epoch: 1/2, step 6138/7134 completed (loss: 0.2525342106819153, acc: 0.9570552110671997)
[2025-02-13 19:43:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:51][root][INFO] - Training Epoch: 1/2, step 6139/7134 completed (loss: 0.33052027225494385, acc: 0.8979591727256775)
[2025-02-13 19:43:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:51][root][INFO] - Training Epoch: 1/2, step 6140/7134 completed (loss: 0.20802506804466248, acc: 0.9692307710647583)
[2025-02-13 19:43:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:52][root][INFO] - Training Epoch: 1/2, step 6141/7134 completed (loss: 0.48424893617630005, acc: 0.8840579986572266)
[2025-02-13 19:43:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:52][root][INFO] - Training Epoch: 1/2, step 6142/7134 completed (loss: 0.23085078597068787, acc: 0.9428571462631226)
[2025-02-13 19:43:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:53][root][INFO] - Training Epoch: 1/2, step 6143/7134 completed (loss: 0.20021694898605347, acc: 0.9696969985961914)
[2025-02-13 19:43:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:53][root][INFO] - Training Epoch: 1/2, step 6144/7134 completed (loss: 0.22679762542247772, acc: 0.9452054500579834)
[2025-02-13 19:43:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:53][root][INFO] - Training Epoch: 1/2, step 6145/7134 completed (loss: 0.35853371024131775, acc: 0.9289617538452148)
[2025-02-13 19:43:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:54][root][INFO] - Training Epoch: 1/2, step 6146/7134 completed (loss: 0.3194057047367096, acc: 0.9216867685317993)
[2025-02-13 19:43:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:54][root][INFO] - Training Epoch: 1/2, step 6147/7134 completed (loss: 0.25934427976608276, acc: 0.9370078444480896)
[2025-02-13 19:43:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:54][root][INFO] - Training Epoch: 1/2, step 6148/7134 completed (loss: 0.22119122743606567, acc: 0.951724112033844)
[2025-02-13 19:43:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:55][root][INFO] - Training Epoch: 1/2, step 6149/7134 completed (loss: 0.2638777196407318, acc: 0.9470198750495911)
[2025-02-13 19:43:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:55][root][INFO] - Training Epoch: 1/2, step 6150/7134 completed (loss: 0.17522381246089935, acc: 0.9580419659614563)
[2025-02-13 19:43:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:56][root][INFO] - Training Epoch: 1/2, step 6151/7134 completed (loss: 0.12270393967628479, acc: 0.9681528806686401)
[2025-02-13 19:43:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:56][root][INFO] - Training Epoch: 1/2, step 6152/7134 completed (loss: 0.25233903527259827, acc: 0.9657142758369446)
[2025-02-13 19:43:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:56][root][INFO] - Training Epoch: 1/2, step 6153/7134 completed (loss: 0.14122186601161957, acc: 0.9753086566925049)
[2025-02-13 19:43:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:57][root][INFO] - Training Epoch: 1/2, step 6154/7134 completed (loss: 0.21265095472335815, acc: 0.9613259434700012)
[2025-02-13 19:43:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:57][root][INFO] - Training Epoch: 1/2, step 6155/7134 completed (loss: 0.15031826496124268, acc: 0.9585798978805542)
[2025-02-13 19:43:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:58][root][INFO] - Training Epoch: 1/2, step 6156/7134 completed (loss: 0.15601788461208344, acc: 0.9556962251663208)
[2025-02-13 19:43:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:58][root][INFO] - Training Epoch: 1/2, step 6157/7134 completed (loss: 0.17292717099189758, acc: 0.9556962251663208)
[2025-02-13 19:43:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:58][root][INFO] - Training Epoch: 1/2, step 6158/7134 completed (loss: 0.2997002601623535, acc: 0.9298245906829834)
[2025-02-13 19:43:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:59][root][INFO] - Training Epoch: 1/2, step 6159/7134 completed (loss: 0.28118059039115906, acc: 0.9345238208770752)
[2025-02-13 19:43:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:43:59][root][INFO] - Training Epoch: 1/2, step 6160/7134 completed (loss: 0.4379046559333801, acc: 0.8980891704559326)
[2025-02-13 19:43:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:00][root][INFO] - Training Epoch: 1/2, step 6161/7134 completed (loss: 0.42182186245918274, acc: 0.913385808467865)
[2025-02-13 19:44:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:00][root][INFO] - Training Epoch: 1/2, step 6162/7134 completed (loss: 0.5609838366508484, acc: 0.8782051205635071)
[2025-02-13 19:44:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:01][root][INFO] - Training Epoch: 1/2, step 6163/7134 completed (loss: 0.37375399470329285, acc: 0.930232584476471)
[2025-02-13 19:44:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:01][root][INFO] - Training Epoch: 1/2, step 6164/7134 completed (loss: 0.33027705550193787, acc: 0.9207317233085632)
[2025-02-13 19:44:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:01][root][INFO] - Training Epoch: 1/2, step 6165/7134 completed (loss: 0.2953893840312958, acc: 0.9312977194786072)
[2025-02-13 19:44:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:02][root][INFO] - Training Epoch: 1/2, step 6166/7134 completed (loss: 0.17006710171699524, acc: 0.956204354763031)
[2025-02-13 19:44:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:02][root][INFO] - Training Epoch: 1/2, step 6167/7134 completed (loss: 0.10992018133401871, acc: 0.9767441749572754)
[2025-02-13 19:44:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:03][root][INFO] - Training Epoch: 1/2, step 6168/7134 completed (loss: 0.12216930091381073, acc: 0.9784946441650391)
[2025-02-13 19:44:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:03][root][INFO] - Training Epoch: 1/2, step 6169/7134 completed (loss: 0.18541012704372406, acc: 0.9759036302566528)
[2025-02-13 19:44:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:03][root][INFO] - Training Epoch: 1/2, step 6170/7134 completed (loss: 0.14208319783210754, acc: 0.9568345546722412)
[2025-02-13 19:44:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:04][root][INFO] - Training Epoch: 1/2, step 6171/7134 completed (loss: 0.0853780210018158, acc: 0.9890109896659851)
[2025-02-13 19:44:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:04][root][INFO] - Training Epoch: 1/2, step 6172/7134 completed (loss: 0.11723364144563675, acc: 0.9609375)
[2025-02-13 19:44:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:04][root][INFO] - Training Epoch: 1/2, step 6173/7134 completed (loss: 0.1478758603334427, acc: 0.9551281929016113)
[2025-02-13 19:44:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:05][root][INFO] - Training Epoch: 1/2, step 6174/7134 completed (loss: 0.06442739814519882, acc: 0.9757575988769531)
[2025-02-13 19:44:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:05][root][INFO] - Training Epoch: 1/2, step 6175/7134 completed (loss: 0.12642639875411987, acc: 0.9691358208656311)
[2025-02-13 19:44:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:06][root][INFO] - Training Epoch: 1/2, step 6176/7134 completed (loss: 0.10870921611785889, acc: 0.9817073345184326)
[2025-02-13 19:44:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:06][root][INFO] - Training Epoch: 1/2, step 6177/7134 completed (loss: 0.12605439126491547, acc: 0.9670329689979553)
[2025-02-13 19:44:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:07][root][INFO] - Training Epoch: 1/2, step 6178/7134 completed (loss: 0.1499742567539215, acc: 0.9673202633857727)
[2025-02-13 19:44:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:07][root][INFO] - Training Epoch: 1/2, step 6179/7134 completed (loss: 0.30723994970321655, acc: 0.9333333373069763)
[2025-02-13 19:44:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:07][root][INFO] - Training Epoch: 1/2, step 6180/7134 completed (loss: 0.8363087177276611, acc: 0.8297872543334961)
[2025-02-13 19:44:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:08][root][INFO] - Training Epoch: 1/2, step 6181/7134 completed (loss: 1.1204458475112915, acc: 0.75)
[2025-02-13 19:44:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:08][root][INFO] - Training Epoch: 1/2, step 6182/7134 completed (loss: 0.5260940194129944, acc: 0.9069767594337463)
[2025-02-13 19:44:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:09][root][INFO] - Training Epoch: 1/2, step 6183/7134 completed (loss: 0.7338055968284607, acc: 0.8209876418113708)
[2025-02-13 19:44:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:09][root][INFO] - Training Epoch: 1/2, step 6184/7134 completed (loss: 0.6317936778068542, acc: 0.8608695864677429)
[2025-02-13 19:44:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:09][root][INFO] - Training Epoch: 1/2, step 6185/7134 completed (loss: 0.23708707094192505, acc: 0.9596773982048035)
[2025-02-13 19:44:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:10][root][INFO] - Training Epoch: 1/2, step 6186/7134 completed (loss: 0.17668932676315308, acc: 0.9694656729698181)
[2025-02-13 19:44:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:10][root][INFO] - Training Epoch: 1/2, step 6187/7134 completed (loss: 0.24637693166732788, acc: 0.9415204524993896)
[2025-02-13 19:44:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:11][root][INFO] - Training Epoch: 1/2, step 6188/7134 completed (loss: 0.22819149494171143, acc: 0.9411764740943909)
[2025-02-13 19:44:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:11][root][INFO] - Training Epoch: 1/2, step 6189/7134 completed (loss: 0.35208114981651306, acc: 0.9090909361839294)
[2025-02-13 19:44:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:11][root][INFO] - Training Epoch: 1/2, step 6190/7134 completed (loss: 0.6314757466316223, acc: 0.8857142925262451)
[2025-02-13 19:44:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:12][root][INFO] - Training Epoch: 1/2, step 6191/7134 completed (loss: 0.2428428679704666, acc: 0.934959352016449)
[2025-02-13 19:44:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:12][root][INFO] - Training Epoch: 1/2, step 6192/7134 completed (loss: 0.19529005885124207, acc: 0.9463414549827576)
[2025-02-13 19:44:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:13][root][INFO] - Training Epoch: 1/2, step 6193/7134 completed (loss: 0.3044045865535736, acc: 0.9090909361839294)
[2025-02-13 19:44:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:13][root][INFO] - Training Epoch: 1/2, step 6194/7134 completed (loss: 0.24863462150096893, acc: 0.9180327653884888)
[2025-02-13 19:44:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:13][root][INFO] - Training Epoch: 1/2, step 6195/7134 completed (loss: 0.17439313232898712, acc: 0.9622641801834106)
[2025-02-13 19:44:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:14][root][INFO] - Training Epoch: 1/2, step 6196/7134 completed (loss: 0.19480127096176147, acc: 0.9320987462997437)
[2025-02-13 19:44:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:14][root][INFO] - Training Epoch: 1/2, step 6197/7134 completed (loss: 0.3180480897426605, acc: 0.9488636255264282)
[2025-02-13 19:44:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:15][root][INFO] - Training Epoch: 1/2, step 6198/7134 completed (loss: 0.13773216307163239, acc: 0.9646017551422119)
[2025-02-13 19:44:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:15][root][INFO] - Training Epoch: 1/2, step 6199/7134 completed (loss: 0.16003213822841644, acc: 0.9576719403266907)
[2025-02-13 19:44:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:15][root][INFO] - Training Epoch: 1/2, step 6200/7134 completed (loss: 0.09752058237791061, acc: 0.9692307710647583)
[2025-02-13 19:44:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:16][root][INFO] - Training Epoch: 1/2, step 6201/7134 completed (loss: 0.14102275669574738, acc: 0.9569377899169922)
[2025-02-13 19:44:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:16][root][INFO] - Training Epoch: 1/2, step 6202/7134 completed (loss: 0.30985790491104126, acc: 0.9238095283508301)
[2025-02-13 19:44:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:17][root][INFO] - Training Epoch: 1/2, step 6203/7134 completed (loss: 0.13838817179203033, acc: 0.9593023061752319)
[2025-02-13 19:44:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:17][root][INFO] - Training Epoch: 1/2, step 6204/7134 completed (loss: 0.051089685410261154, acc: 0.9916666746139526)
[2025-02-13 19:44:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:17][root][INFO] - Training Epoch: 1/2, step 6205/7134 completed (loss: 0.14010660350322723, acc: 0.9640287756919861)
[2025-02-13 19:44:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:18][root][INFO] - Training Epoch: 1/2, step 6206/7134 completed (loss: 0.07492737472057343, acc: 0.9813664555549622)
[2025-02-13 19:44:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:18][root][INFO] - Training Epoch: 1/2, step 6207/7134 completed (loss: 0.5185205340385437, acc: 0.8909090757369995)
[2025-02-13 19:44:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:18][root][INFO] - Training Epoch: 1/2, step 6208/7134 completed (loss: 0.194415882229805, acc: 0.9322916865348816)
[2025-02-13 19:44:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:19][root][INFO] - Training Epoch: 1/2, step 6209/7134 completed (loss: 0.1355050802230835, acc: 0.976190447807312)
[2025-02-13 19:44:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:19][root][INFO] - Training Epoch: 1/2, step 6210/7134 completed (loss: 0.21201345324516296, acc: 0.9411764740943909)
[2025-02-13 19:44:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:20][root][INFO] - Training Epoch: 1/2, step 6211/7134 completed (loss: 0.20411035418510437, acc: 0.9454545378684998)
[2025-02-13 19:44:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:20][root][INFO] - Training Epoch: 1/2, step 6212/7134 completed (loss: 0.19522297382354736, acc: 0.9572649598121643)
[2025-02-13 19:44:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:20][root][INFO] - Training Epoch: 1/2, step 6213/7134 completed (loss: 0.14987437427043915, acc: 0.9636363387107849)
[2025-02-13 19:44:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:21][root][INFO] - Training Epoch: 1/2, step 6214/7134 completed (loss: 0.09217890352010727, acc: 0.9830508232116699)
[2025-02-13 19:44:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:21][root][INFO] - Training Epoch: 1/2, step 6215/7134 completed (loss: 0.2634636461734772, acc: 0.9371980428695679)
[2025-02-13 19:44:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:22][root][INFO] - Training Epoch: 1/2, step 6216/7134 completed (loss: 0.08009187132120132, acc: 0.9795918464660645)
[2025-02-13 19:44:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:22][root][INFO] - Training Epoch: 1/2, step 6217/7134 completed (loss: 0.1180403009057045, acc: 0.9556962251663208)
[2025-02-13 19:44:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:22][root][INFO] - Training Epoch: 1/2, step 6218/7134 completed (loss: 0.08782054483890533, acc: 0.9806451797485352)
[2025-02-13 19:44:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:23][root][INFO] - Training Epoch: 1/2, step 6219/7134 completed (loss: 0.14935222268104553, acc: 0.9673202633857727)
[2025-02-13 19:44:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:23][root][INFO] - Training Epoch: 1/2, step 6220/7134 completed (loss: 0.08897789567708969, acc: 0.9885057210922241)
[2025-02-13 19:44:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:24][root][INFO] - Training Epoch: 1/2, step 6221/7134 completed (loss: 0.04171174019575119, acc: 0.9925373196601868)
[2025-02-13 19:44:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:24][root][INFO] - Training Epoch: 1/2, step 6222/7134 completed (loss: 0.33340102434158325, acc: 0.9277108311653137)
[2025-02-13 19:44:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:24][root][INFO] - Training Epoch: 1/2, step 6223/7134 completed (loss: 0.35055720806121826, acc: 0.921875)
[2025-02-13 19:44:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:25][root][INFO] - Training Epoch: 1/2, step 6224/7134 completed (loss: 0.11516666412353516, acc: 0.9662162065505981)
[2025-02-13 19:44:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:25][root][INFO] - Training Epoch: 1/2, step 6225/7134 completed (loss: 0.18828339874744415, acc: 0.9608938694000244)
[2025-02-13 19:44:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:25][root][INFO] - Training Epoch: 1/2, step 6226/7134 completed (loss: 0.3028438091278076, acc: 0.9239130616188049)
[2025-02-13 19:44:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:26][root][INFO] - Training Epoch: 1/2, step 6227/7134 completed (loss: 0.10035821050405502, acc: 0.9624999761581421)
[2025-02-13 19:44:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:26][root][INFO] - Training Epoch: 1/2, step 6228/7134 completed (loss: 0.17068900167942047, acc: 0.957446813583374)
[2025-02-13 19:44:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:27][root][INFO] - Training Epoch: 1/2, step 6229/7134 completed (loss: 0.39489689469337463, acc: 0.8975609540939331)
[2025-02-13 19:44:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:27][root][INFO] - Training Epoch: 1/2, step 6230/7134 completed (loss: 0.33521705865859985, acc: 0.9095744490623474)
[2025-02-13 19:44:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:27][root][INFO] - Training Epoch: 1/2, step 6231/7134 completed (loss: 0.1412300318479538, acc: 0.9649122953414917)
[2025-02-13 19:44:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:28][root][INFO] - Training Epoch: 1/2, step 6232/7134 completed (loss: 0.23546351492404938, acc: 0.9340659379959106)
[2025-02-13 19:44:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:28][root][INFO] - Training Epoch: 1/2, step 6233/7134 completed (loss: 0.1358473002910614, acc: 0.959770143032074)
[2025-02-13 19:44:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:29][root][INFO] - Training Epoch: 1/2, step 6234/7134 completed (loss: 0.3579421639442444, acc: 0.9333333373069763)
[2025-02-13 19:44:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:29][root][INFO] - Training Epoch: 1/2, step 6235/7134 completed (loss: 0.13236916065216064, acc: 0.9583333134651184)
[2025-02-13 19:44:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:29][root][INFO] - Training Epoch: 1/2, step 6236/7134 completed (loss: 0.1881822943687439, acc: 0.9553072452545166)
[2025-02-13 19:44:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:30][root][INFO] - Training Epoch: 1/2, step 6237/7134 completed (loss: 0.291185587644577, acc: 0.9265536665916443)
[2025-02-13 19:44:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:30][root][INFO] - Training Epoch: 1/2, step 6238/7134 completed (loss: 0.15773622691631317, acc: 0.9735099077224731)
[2025-02-13 19:44:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:31][root][INFO] - Training Epoch: 1/2, step 6239/7134 completed (loss: 0.14365996420383453, acc: 0.9689119458198547)
[2025-02-13 19:44:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:31][root][INFO] - Training Epoch: 1/2, step 6240/7134 completed (loss: 0.04942047595977783, acc: 0.9879518151283264)
[2025-02-13 19:44:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:32][root][INFO] - Training Epoch: 1/2, step 6241/7134 completed (loss: 0.0936591625213623, acc: 0.9733333587646484)
[2025-02-13 19:44:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:32][root][INFO] - Training Epoch: 1/2, step 6242/7134 completed (loss: 0.06948557496070862, acc: 0.9863945841789246)
[2025-02-13 19:44:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:32][root][INFO] - Training Epoch: 1/2, step 6243/7134 completed (loss: 0.21607157588005066, acc: 0.9463087320327759)
[2025-02-13 19:44:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:33][root][INFO] - Training Epoch: 1/2, step 6244/7134 completed (loss: 0.18451711535453796, acc: 0.9580838084220886)
[2025-02-13 19:44:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:33][root][INFO] - Training Epoch: 1/2, step 6245/7134 completed (loss: 0.15782268345355988, acc: 0.9585798978805542)
[2025-02-13 19:44:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:34][root][INFO] - Training Epoch: 1/2, step 6246/7134 completed (loss: 0.09508222341537476, acc: 0.9732620120048523)
[2025-02-13 19:44:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:34][root][INFO] - Training Epoch: 1/2, step 6247/7134 completed (loss: 0.18669484555721283, acc: 0.945652186870575)
[2025-02-13 19:44:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:35][root][INFO] - Training Epoch: 1/2, step 6248/7134 completed (loss: 0.4314827024936676, acc: 0.9041916131973267)
[2025-02-13 19:44:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:35][root][INFO] - Training Epoch: 1/2, step 6249/7134 completed (loss: 0.07635392248630524, acc: 0.9735099077224731)
[2025-02-13 19:44:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:35][root][INFO] - Training Epoch: 1/2, step 6250/7134 completed (loss: 0.023029804229736328, acc: 1.0)
[2025-02-13 19:44:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:36][root][INFO] - Training Epoch: 1/2, step 6251/7134 completed (loss: 0.12634976208209991, acc: 0.9801324605941772)
[2025-02-13 19:44:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:36][root][INFO] - Training Epoch: 1/2, step 6252/7134 completed (loss: 0.03445926308631897, acc: 0.9928571581840515)
[2025-02-13 19:44:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:37][root][INFO] - Training Epoch: 1/2, step 6253/7134 completed (loss: 0.12392067909240723, acc: 0.9720279574394226)
[2025-02-13 19:44:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:37][root][INFO] - Training Epoch: 1/2, step 6254/7134 completed (loss: 0.19664393365383148, acc: 0.9626865386962891)
[2025-02-13 19:44:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:37][root][INFO] - Training Epoch: 1/2, step 6255/7134 completed (loss: 0.1286424696445465, acc: 0.9715909361839294)
[2025-02-13 19:44:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:38][root][INFO] - Training Epoch: 1/2, step 6256/7134 completed (loss: 0.11508118361234665, acc: 0.9661017060279846)
[2025-02-13 19:44:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:38][root][INFO] - Training Epoch: 1/2, step 6257/7134 completed (loss: 0.10188527405261993, acc: 0.9822485446929932)
[2025-02-13 19:44:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:38][root][INFO] - Training Epoch: 1/2, step 6258/7134 completed (loss: 0.09491728246212006, acc: 0.9733333587646484)
[2025-02-13 19:44:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:39][root][INFO] - Training Epoch: 1/2, step 6259/7134 completed (loss: 0.07183198630809784, acc: 0.9813664555549622)
[2025-02-13 19:44:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:39][root][INFO] - Training Epoch: 1/2, step 6260/7134 completed (loss: 0.08005285263061523, acc: 0.987261176109314)
[2025-02-13 19:44:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:40][root][INFO] - Training Epoch: 1/2, step 6261/7134 completed (loss: 0.13876333832740784, acc: 0.9627329111099243)
[2025-02-13 19:44:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:40][root][INFO] - Training Epoch: 1/2, step 6262/7134 completed (loss: 0.052980318665504456, acc: 0.9876543283462524)
[2025-02-13 19:44:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:41][root][INFO] - Training Epoch: 1/2, step 6263/7134 completed (loss: 0.03603459149599075, acc: 0.9871794581413269)
[2025-02-13 19:44:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:41][root][INFO] - Training Epoch: 1/2, step 6264/7134 completed (loss: 0.05149426311254501, acc: 0.9726027250289917)
[2025-02-13 19:44:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:41][root][INFO] - Training Epoch: 1/2, step 6265/7134 completed (loss: 0.09847865998744965, acc: 0.987730085849762)
[2025-02-13 19:44:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:42][root][INFO] - Training Epoch: 1/2, step 6266/7134 completed (loss: 0.03967313840985298, acc: 0.9923076629638672)
[2025-02-13 19:44:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:42][root][INFO] - Training Epoch: 1/2, step 6267/7134 completed (loss: 0.05339506268501282, acc: 0.9868420958518982)
[2025-02-13 19:44:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:43][root][INFO] - Training Epoch: 1/2, step 6268/7134 completed (loss: 0.08474106341600418, acc: 0.9875776171684265)
[2025-02-13 19:44:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:43][root][INFO] - Training Epoch: 1/2, step 6269/7134 completed (loss: 0.03079044818878174, acc: 0.9910714030265808)
[2025-02-13 19:44:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:43][root][INFO] - Training Epoch: 1/2, step 6270/7134 completed (loss: 0.021887563169002533, acc: 1.0)
[2025-02-13 19:44:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:44][root][INFO] - Training Epoch: 1/2, step 6271/7134 completed (loss: 0.08946637809276581, acc: 0.9886363744735718)
[2025-02-13 19:44:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:44][root][INFO] - Training Epoch: 1/2, step 6272/7134 completed (loss: 0.02623387798666954, acc: 0.9938650131225586)
[2025-02-13 19:44:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:45][root][INFO] - Training Epoch: 1/2, step 6273/7134 completed (loss: 0.01865667663514614, acc: 1.0)
[2025-02-13 19:44:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:45][root][INFO] - Training Epoch: 1/2, step 6274/7134 completed (loss: 0.020491255447268486, acc: 1.0)
[2025-02-13 19:44:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:45][root][INFO] - Training Epoch: 1/2, step 6275/7134 completed (loss: 0.10625691711902618, acc: 0.9731543660163879)
[2025-02-13 19:44:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:46][root][INFO] - Training Epoch: 1/2, step 6276/7134 completed (loss: 0.13689613342285156, acc: 0.9674796462059021)
[2025-02-13 19:44:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:46][root][INFO] - Training Epoch: 1/2, step 6277/7134 completed (loss: 0.3466985523700714, acc: 0.9056603908538818)
[2025-02-13 19:44:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:47][root][INFO] - Training Epoch: 1/2, step 6278/7134 completed (loss: 0.11291604489088058, acc: 0.9779411554336548)
[2025-02-13 19:44:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:47][root][INFO] - Training Epoch: 1/2, step 6279/7134 completed (loss: 0.07411129027605057, acc: 0.9846153855323792)
[2025-02-13 19:44:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:47][root][INFO] - Training Epoch: 1/2, step 6280/7134 completed (loss: 0.1204032301902771, acc: 0.9701492786407471)
[2025-02-13 19:44:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:48][root][INFO] - Training Epoch: 1/2, step 6281/7134 completed (loss: 0.1252618134021759, acc: 0.9632353186607361)
[2025-02-13 19:44:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:48][root][INFO] - Training Epoch: 1/2, step 6282/7134 completed (loss: 0.11110422760248184, acc: 0.9583333134651184)
[2025-02-13 19:44:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:49][root][INFO] - Training Epoch: 1/2, step 6283/7134 completed (loss: 0.11652468144893646, acc: 0.9826086759567261)
[2025-02-13 19:44:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:49][root][INFO] - Training Epoch: 1/2, step 6284/7134 completed (loss: 0.30095845460891724, acc: 0.9363636374473572)
[2025-02-13 19:44:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:49][root][INFO] - Training Epoch: 1/2, step 6285/7134 completed (loss: 0.2756922245025635, acc: 0.8947368264198303)
[2025-02-13 19:44:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:50][root][INFO] - Training Epoch: 1/2, step 6286/7134 completed (loss: 0.06639062613248825, acc: 0.9861111044883728)
[2025-02-13 19:44:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:50][root][INFO] - Training Epoch: 1/2, step 6287/7134 completed (loss: 0.16495120525360107, acc: 0.9545454382896423)
[2025-02-13 19:44:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:51][root][INFO] - Training Epoch: 1/2, step 6288/7134 completed (loss: 0.054755229502916336, acc: 0.9858155846595764)
[2025-02-13 19:44:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:51][root][INFO] - Training Epoch: 1/2, step 6289/7134 completed (loss: 0.10026893764734268, acc: 0.9696969985961914)
[2025-02-13 19:44:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:51][root][INFO] - Training Epoch: 1/2, step 6290/7134 completed (loss: 0.13052533566951752, acc: 0.9552238583564758)
[2025-02-13 19:44:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:52][root][INFO] - Training Epoch: 1/2, step 6291/7134 completed (loss: 0.05016421154141426, acc: 0.9848484992980957)
[2025-02-13 19:44:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:52][root][INFO] - Training Epoch: 1/2, step 6292/7134 completed (loss: 0.26479050517082214, acc: 0.9642857313156128)
[2025-02-13 19:44:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:53][root][INFO] - Training Epoch: 1/2, step 6293/7134 completed (loss: 0.2517394423484802, acc: 0.9454545378684998)
[2025-02-13 19:44:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:53][root][INFO] - Training Epoch: 1/2, step 6294/7134 completed (loss: 0.21547769010066986, acc: 0.9473684430122375)
[2025-02-13 19:44:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:53][root][INFO] - Training Epoch: 1/2, step 6295/7134 completed (loss: 0.1583409309387207, acc: 0.9696969985961914)
[2025-02-13 19:44:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:54][root][INFO] - Training Epoch: 1/2, step 6296/7134 completed (loss: 0.33595436811447144, acc: 0.9280575513839722)
[2025-02-13 19:44:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:54][root][INFO] - Training Epoch: 1/2, step 6297/7134 completed (loss: 0.1880936175584793, acc: 0.9248120188713074)
[2025-02-13 19:44:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:55][root][INFO] - Training Epoch: 1/2, step 6298/7134 completed (loss: 0.22443005442619324, acc: 0.9513888955116272)
[2025-02-13 19:44:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:55][root][INFO] - Training Epoch: 1/2, step 6299/7134 completed (loss: 0.1699979305267334, acc: 0.9539473652839661)
[2025-02-13 19:44:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:55][root][INFO] - Training Epoch: 1/2, step 6300/7134 completed (loss: 0.20943616330623627, acc: 0.9539473652839661)
[2025-02-13 19:44:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:56][root][INFO] - Training Epoch: 1/2, step 6301/7134 completed (loss: 0.06121113523840904, acc: 0.9675925970077515)
[2025-02-13 19:44:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:56][root][INFO] - Training Epoch: 1/2, step 6302/7134 completed (loss: 0.048435717821121216, acc: 0.9888268113136292)
[2025-02-13 19:44:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:57][root][INFO] - Training Epoch: 1/2, step 6303/7134 completed (loss: 0.06104569509625435, acc: 0.9910314083099365)
[2025-02-13 19:44:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:57][root][INFO] - Training Epoch: 1/2, step 6304/7134 completed (loss: 0.09304609149694443, acc: 0.9715909361839294)
[2025-02-13 19:44:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:57][root][INFO] - Training Epoch: 1/2, step 6305/7134 completed (loss: 0.15589550137519836, acc: 0.9726027250289917)
[2025-02-13 19:44:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:58][root][INFO] - Training Epoch: 1/2, step 6306/7134 completed (loss: 0.11719589680433273, acc: 0.971563994884491)
[2025-02-13 19:44:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:58][root][INFO] - Training Epoch: 1/2, step 6307/7134 completed (loss: 0.2621065378189087, acc: 0.9465649127960205)
[2025-02-13 19:44:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:59][root][INFO] - Training Epoch: 1/2, step 6308/7134 completed (loss: 0.1178029328584671, acc: 0.9792746305465698)
[2025-02-13 19:44:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:59][root][INFO] - Training Epoch: 1/2, step 6309/7134 completed (loss: 0.09202968329191208, acc: 0.9822485446929932)
[2025-02-13 19:44:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:44:59][root][INFO] - Training Epoch: 1/2, step 6310/7134 completed (loss: 0.2928391695022583, acc: 0.9298245906829834)
[2025-02-13 19:45:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:00][root][INFO] - Training Epoch: 1/2, step 6311/7134 completed (loss: 0.11690253019332886, acc: 0.9583333134651184)
[2025-02-13 19:45:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:00][root][INFO] - Training Epoch: 1/2, step 6312/7134 completed (loss: 0.08806444704532623, acc: 0.9808917045593262)
[2025-02-13 19:45:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:01][root][INFO] - Training Epoch: 1/2, step 6313/7134 completed (loss: 0.05701812356710434, acc: 0.9891892075538635)
[2025-02-13 19:45:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:01][root][INFO] - Training Epoch: 1/2, step 6314/7134 completed (loss: 0.13624712824821472, acc: 0.9569892287254333)
[2025-02-13 19:45:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:02][root][INFO] - Training Epoch: 1/2, step 6315/7134 completed (loss: 0.19358748197555542, acc: 0.9664429426193237)
[2025-02-13 19:45:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:02][root][INFO] - Training Epoch: 1/2, step 6316/7134 completed (loss: 0.07975011318922043, acc: 0.9862068891525269)
[2025-02-13 19:45:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:02][root][INFO] - Training Epoch: 1/2, step 6317/7134 completed (loss: 0.07077101618051529, acc: 0.9864864945411682)
[2025-02-13 19:45:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:03][root][INFO] - Training Epoch: 1/2, step 6318/7134 completed (loss: 0.06980608403682709, acc: 0.9802631735801697)
[2025-02-13 19:45:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:03][root][INFO] - Training Epoch: 1/2, step 6319/7134 completed (loss: 0.1107201874256134, acc: 0.9688888788223267)
[2025-02-13 19:45:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:03][root][INFO] - Training Epoch: 1/2, step 6320/7134 completed (loss: 0.07341939210891724, acc: 0.9864864945411682)
[2025-02-13 19:45:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:04][root][INFO] - Training Epoch: 1/2, step 6321/7134 completed (loss: 0.044781848788261414, acc: 0.9904761910438538)
[2025-02-13 19:45:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:04][root][INFO] - Training Epoch: 1/2, step 6322/7134 completed (loss: 0.1562110334634781, acc: 0.9490445852279663)
[2025-02-13 19:45:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:05][root][INFO] - Training Epoch: 1/2, step 6323/7134 completed (loss: 0.13067027926445007, acc: 0.9624999761581421)
[2025-02-13 19:45:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:05][root][INFO] - Training Epoch: 1/2, step 6324/7134 completed (loss: 0.18182162940502167, acc: 0.9617486596107483)
[2025-02-13 19:45:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:05][root][INFO] - Training Epoch: 1/2, step 6325/7134 completed (loss: 0.1491369754076004, acc: 0.9596773982048035)
[2025-02-13 19:45:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:06][root][INFO] - Training Epoch: 1/2, step 6326/7134 completed (loss: 0.24995218217372894, acc: 0.9308176040649414)
[2025-02-13 19:45:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:06][root][INFO] - Training Epoch: 1/2, step 6327/7134 completed (loss: 0.07998455315828323, acc: 0.9890109896659851)
[2025-02-13 19:45:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:07][root][INFO] - Training Epoch: 1/2, step 6328/7134 completed (loss: 0.2373504936695099, acc: 0.9503105878829956)
[2025-02-13 19:45:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:07][root][INFO] - Training Epoch: 1/2, step 6329/7134 completed (loss: 0.11347188800573349, acc: 0.9679487347602844)
[2025-02-13 19:45:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:07][root][INFO] - Training Epoch: 1/2, step 6330/7134 completed (loss: 0.16390426456928253, acc: 0.9716981053352356)
[2025-02-13 19:45:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:08][root][INFO] - Training Epoch: 1/2, step 6331/7134 completed (loss: 0.05124210566282272, acc: 0.9850746393203735)
[2025-02-13 19:45:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:08][root][INFO] - Training Epoch: 1/2, step 6332/7134 completed (loss: 0.30711662769317627, acc: 0.936170220375061)
[2025-02-13 19:45:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:09][root][INFO] - Training Epoch: 1/2, step 6333/7134 completed (loss: 0.17951421439647675, acc: 0.931034505367279)
[2025-02-13 19:45:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:09][root][INFO] - Training Epoch: 1/2, step 6334/7134 completed (loss: 0.07111353427171707, acc: 0.970588207244873)
[2025-02-13 19:45:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:09][root][INFO] - Training Epoch: 1/2, step 6335/7134 completed (loss: 0.06290696561336517, acc: 0.9808917045593262)
[2025-02-13 19:45:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:10][root][INFO] - Training Epoch: 1/2, step 6336/7134 completed (loss: 0.13978037238121033, acc: 0.9561403393745422)
[2025-02-13 19:45:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:10][root][INFO] - Training Epoch: 1/2, step 6337/7134 completed (loss: 0.05872446298599243, acc: 0.9720279574394226)
[2025-02-13 19:45:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:11][root][INFO] - Training Epoch: 1/2, step 6338/7134 completed (loss: 0.08928897976875305, acc: 0.9937106966972351)
[2025-02-13 19:45:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:11][root][INFO] - Training Epoch: 1/2, step 6339/7134 completed (loss: 0.06352028250694275, acc: 0.9869281053543091)
[2025-02-13 19:45:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:12][root][INFO] - Training Epoch: 1/2, step 6340/7134 completed (loss: 0.06526194512844086, acc: 0.9754601120948792)
[2025-02-13 19:45:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:12][root][INFO] - Training Epoch: 1/2, step 6341/7134 completed (loss: 0.12159708142280579, acc: 0.9685039520263672)
[2025-02-13 19:45:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:12][root][INFO] - Training Epoch: 1/2, step 6342/7134 completed (loss: 0.09017068147659302, acc: 0.9826589822769165)
[2025-02-13 19:45:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:13][root][INFO] - Training Epoch: 1/2, step 6343/7134 completed (loss: 0.08941053599119186, acc: 0.9726027250289917)
[2025-02-13 19:45:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:13][root][INFO] - Training Epoch: 1/2, step 6344/7134 completed (loss: 0.18449662625789642, acc: 0.9668508172035217)
[2025-02-13 19:45:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:14][root][INFO] - Training Epoch: 1/2, step 6345/7134 completed (loss: 0.16278915107250214, acc: 0.967391312122345)
[2025-02-13 19:45:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:14][root][INFO] - Training Epoch: 1/2, step 6346/7134 completed (loss: 0.35548752546310425, acc: 0.9360465407371521)
[2025-02-13 19:45:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:15][root][INFO] - Training Epoch: 1/2, step 6347/7134 completed (loss: 0.36422887444496155, acc: 0.9411764740943909)
[2025-02-13 19:45:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:15][root][INFO] - Training Epoch: 1/2, step 6348/7134 completed (loss: 0.16184310615062714, acc: 0.9305555820465088)
[2025-02-13 19:45:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:15][root][INFO] - Training Epoch: 1/2, step 6349/7134 completed (loss: 0.1314704716205597, acc: 0.9621211886405945)
[2025-02-13 19:45:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:16][root][INFO] - Training Epoch: 1/2, step 6350/7134 completed (loss: 0.26827144622802734, acc: 0.9259259104728699)
[2025-02-13 19:45:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:16][root][INFO] - Training Epoch: 1/2, step 6351/7134 completed (loss: 0.22100788354873657, acc: 0.946107804775238)
[2025-02-13 19:45:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:17][root][INFO] - Training Epoch: 1/2, step 6352/7134 completed (loss: 0.3320162296295166, acc: 0.913294792175293)
[2025-02-13 19:45:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:17][root][INFO] - Training Epoch: 1/2, step 6353/7134 completed (loss: 0.23870934545993805, acc: 0.9447852969169617)
[2025-02-13 19:45:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:17][root][INFO] - Training Epoch: 1/2, step 6354/7134 completed (loss: 0.1102714091539383, acc: 0.977011501789093)
[2025-02-13 19:45:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:18][root][INFO] - Training Epoch: 1/2, step 6355/7134 completed (loss: 0.05282318592071533, acc: 0.9933333396911621)
[2025-02-13 19:45:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:18][root][INFO] - Training Epoch: 1/2, step 6356/7134 completed (loss: 0.09761299192905426, acc: 0.9681528806686401)
[2025-02-13 19:45:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:19][root][INFO] - Training Epoch: 1/2, step 6357/7134 completed (loss: 0.12411379814147949, acc: 0.9696969985961914)
[2025-02-13 19:45:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:19][root][INFO] - Training Epoch: 1/2, step 6358/7134 completed (loss: 0.159793421626091, acc: 0.9649122953414917)
[2025-02-13 19:45:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:19][root][INFO] - Training Epoch: 1/2, step 6359/7134 completed (loss: 0.19767852127552032, acc: 0.9608938694000244)
[2025-02-13 19:45:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:20][root][INFO] - Training Epoch: 1/2, step 6360/7134 completed (loss: 0.1690712422132492, acc: 0.9523809552192688)
[2025-02-13 19:45:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:20][root][INFO] - Training Epoch: 1/2, step 6361/7134 completed (loss: 0.18179233372211456, acc: 0.9568345546722412)
[2025-02-13 19:45:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:21][root][INFO] - Training Epoch: 1/2, step 6362/7134 completed (loss: 0.12254124134778976, acc: 0.9677419066429138)
[2025-02-13 19:45:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:21][root][INFO] - Training Epoch: 1/2, step 6363/7134 completed (loss: 0.16001388430595398, acc: 0.9585798978805542)
[2025-02-13 19:45:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:21][root][INFO] - Training Epoch: 1/2, step 6364/7134 completed (loss: 0.1331712156534195, acc: 0.9363057613372803)
[2025-02-13 19:45:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:22][root][INFO] - Training Epoch: 1/2, step 6365/7134 completed (loss: 0.12087009847164154, acc: 0.9661017060279846)
[2025-02-13 19:45:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:22][root][INFO] - Training Epoch: 1/2, step 6366/7134 completed (loss: 0.2018532007932663, acc: 0.9526627063751221)
[2025-02-13 19:45:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:23][root][INFO] - Training Epoch: 1/2, step 6367/7134 completed (loss: 0.149992898106575, acc: 0.9534883499145508)
[2025-02-13 19:45:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:23][root][INFO] - Training Epoch: 1/2, step 6368/7134 completed (loss: 0.18591096997261047, acc: 0.9518072009086609)
[2025-02-13 19:45:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:23][root][INFO] - Training Epoch: 1/2, step 6369/7134 completed (loss: 0.2855188846588135, acc: 0.9464285969734192)
[2025-02-13 19:45:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:24][root][INFO] - Training Epoch: 1/2, step 6370/7134 completed (loss: 0.5819825530052185, acc: 0.8720930218696594)
[2025-02-13 19:45:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:24][root][INFO] - Training Epoch: 1/2, step 6371/7134 completed (loss: 0.30802008509635925, acc: 0.9215686321258545)
[2025-02-13 19:45:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:25][root][INFO] - Training Epoch: 1/2, step 6372/7134 completed (loss: 0.3595740497112274, acc: 0.8706896305084229)
[2025-02-13 19:45:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:25][root][INFO] - Training Epoch: 1/2, step 6373/7134 completed (loss: 0.33499786257743835, acc: 0.9032257795333862)
[2025-02-13 19:45:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:25][root][INFO] - Training Epoch: 1/2, step 6374/7134 completed (loss: 0.14551375806331635, acc: 0.9747899174690247)
[2025-02-13 19:45:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:26][root][INFO] - Training Epoch: 1/2, step 6375/7134 completed (loss: 0.3847390413284302, acc: 0.9171270728111267)
[2025-02-13 19:45:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:26][root][INFO] - Training Epoch: 1/2, step 6376/7134 completed (loss: 0.1829635053873062, acc: 0.934959352016449)
[2025-02-13 19:45:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:27][root][INFO] - Training Epoch: 1/2, step 6377/7134 completed (loss: 0.19675394892692566, acc: 0.9279999732971191)
[2025-02-13 19:45:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:27][root][INFO] - Training Epoch: 1/2, step 6378/7134 completed (loss: 0.4035833179950714, acc: 0.869767427444458)
[2025-02-13 19:45:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:27][root][INFO] - Training Epoch: 1/2, step 6379/7134 completed (loss: 0.2507176399230957, acc: 0.9368420839309692)
[2025-02-13 19:45:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:28][root][INFO] - Training Epoch: 1/2, step 6380/7134 completed (loss: 0.23211833834648132, acc: 0.956250011920929)
[2025-02-13 19:45:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:28][root][INFO] - Training Epoch: 1/2, step 6381/7134 completed (loss: 0.07911594212055206, acc: 0.9864864945411682)
[2025-02-13 19:45:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:29][root][INFO] - Training Epoch: 1/2, step 6382/7134 completed (loss: 0.09665048122406006, acc: 0.9670329689979553)
[2025-02-13 19:45:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:29][root][INFO] - Training Epoch: 1/2, step 6383/7134 completed (loss: 0.2485845983028412, acc: 0.9653179049491882)
[2025-02-13 19:45:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:29][root][INFO] - Training Epoch: 1/2, step 6384/7134 completed (loss: 0.16380180418491364, acc: 0.9746835231781006)
[2025-02-13 19:45:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:30][root][INFO] - Training Epoch: 1/2, step 6385/7134 completed (loss: 0.2616099715232849, acc: 0.9365853667259216)
[2025-02-13 19:45:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:30][root][INFO] - Training Epoch: 1/2, step 6386/7134 completed (loss: 0.12427149713039398, acc: 0.9731183052062988)
[2025-02-13 19:45:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:30][root][INFO] - Training Epoch: 1/2, step 6387/7134 completed (loss: 0.17020002007484436, acc: 0.9696969985961914)
[2025-02-13 19:45:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:31][root][INFO] - Training Epoch: 1/2, step 6388/7134 completed (loss: 0.20086392760276794, acc: 0.9556962251663208)
[2025-02-13 19:45:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:31][root][INFO] - Training Epoch: 1/2, step 6389/7134 completed (loss: 0.17625641822814941, acc: 0.9471153616905212)
[2025-02-13 19:45:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:32][root][INFO] - Training Epoch: 1/2, step 6390/7134 completed (loss: 0.17213019728660583, acc: 0.95652174949646)
[2025-02-13 19:45:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:32][root][INFO] - Training Epoch: 1/2, step 6391/7134 completed (loss: 0.3078308403491974, acc: 0.9244186282157898)
[2025-02-13 19:45:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:33][root][INFO] - Training Epoch: 1/2, step 6392/7134 completed (loss: 0.2785131335258484, acc: 0.926086962223053)
[2025-02-13 19:45:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:33][root][INFO] - Training Epoch: 1/2, step 6393/7134 completed (loss: 0.12226978689432144, acc: 0.9675925970077515)
[2025-02-13 19:45:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:33][root][INFO] - Training Epoch: 1/2, step 6394/7134 completed (loss: 0.28342774510383606, acc: 0.949999988079071)
[2025-02-13 19:45:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:34][root][INFO] - Training Epoch: 1/2, step 6395/7134 completed (loss: 0.1580747365951538, acc: 0.9653465151786804)
[2025-02-13 19:45:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:34][root][INFO] - Training Epoch: 1/2, step 6396/7134 completed (loss: 0.11962241679430008, acc: 0.9712918400764465)
[2025-02-13 19:45:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:35][root][INFO] - Training Epoch: 1/2, step 6397/7134 completed (loss: 0.10459515452384949, acc: 0.9751243591308594)
[2025-02-13 19:45:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:35][root][INFO] - Training Epoch: 1/2, step 6398/7134 completed (loss: 0.14576692879199982, acc: 0.9586206674575806)
[2025-02-13 19:45:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:36][root][INFO] - Training Epoch: 1/2, step 6399/7134 completed (loss: 0.21943840384483337, acc: 0.9550561904907227)
[2025-02-13 19:45:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:36][root][INFO] - Training Epoch: 1/2, step 6400/7134 completed (loss: 0.10557089745998383, acc: 0.9759036302566528)
[2025-02-13 19:45:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:36][root][INFO] - Training Epoch: 1/2, step 6401/7134 completed (loss: 0.11050198972225189, acc: 0.976190447807312)
[2025-02-13 19:45:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:37][root][INFO] - Training Epoch: 1/2, step 6402/7134 completed (loss: 0.22832795977592468, acc: 0.9595959782600403)
[2025-02-13 19:45:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:37][root][INFO] - Training Epoch: 1/2, step 6403/7134 completed (loss: 0.06950969249010086, acc: 0.9896373152732849)
[2025-02-13 19:45:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:38][root][INFO] - Training Epoch: 1/2, step 6404/7134 completed (loss: 0.07967730611562729, acc: 0.9704142212867737)
[2025-02-13 19:45:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:38][root][INFO] - Training Epoch: 1/2, step 6405/7134 completed (loss: 0.13393117487430573, acc: 0.9759615659713745)
[2025-02-13 19:45:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:39][root][INFO] - Training Epoch: 1/2, step 6406/7134 completed (loss: 0.25816676020622253, acc: 0.9350000023841858)
[2025-02-13 19:45:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:39][root][INFO] - Training Epoch: 1/2, step 6407/7134 completed (loss: 0.12437117844820023, acc: 0.9666666388511658)
[2025-02-13 19:45:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:39][root][INFO] - Training Epoch: 1/2, step 6408/7134 completed (loss: 0.2868891954421997, acc: 0.9038461446762085)
[2025-02-13 19:45:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:40][root][INFO] - Training Epoch: 1/2, step 6409/7134 completed (loss: 0.16804596781730652, acc: 0.9817073345184326)
[2025-02-13 19:45:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:40][root][INFO] - Training Epoch: 1/2, step 6410/7134 completed (loss: 0.4678212106227875, acc: 0.8999999761581421)
[2025-02-13 19:45:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:41][root][INFO] - Training Epoch: 1/2, step 6411/7134 completed (loss: 0.3075357973575592, acc: 0.9379844665527344)
[2025-02-13 19:45:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:41][root][INFO] - Training Epoch: 1/2, step 6412/7134 completed (loss: 0.07919532060623169, acc: 0.9741935729980469)
[2025-02-13 19:45:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:41][root][INFO] - Training Epoch: 1/2, step 6413/7134 completed (loss: 0.27439889311790466, acc: 0.9395973086357117)
[2025-02-13 19:45:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:42][root][INFO] - Training Epoch: 1/2, step 6414/7134 completed (loss: 0.13531510531902313, acc: 0.9605262875556946)
[2025-02-13 19:45:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:42][root][INFO] - Training Epoch: 1/2, step 6415/7134 completed (loss: 0.10690409690141678, acc: 0.9772727489471436)
[2025-02-13 19:45:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:43][root][INFO] - Training Epoch: 1/2, step 6416/7134 completed (loss: 0.18558228015899658, acc: 0.95652174949646)
[2025-02-13 19:45:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:43][root][INFO] - Training Epoch: 1/2, step 6417/7134 completed (loss: 0.3941306471824646, acc: 0.930232584476471)
[2025-02-13 19:45:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:43][root][INFO] - Training Epoch: 1/2, step 6418/7134 completed (loss: 0.6003955602645874, acc: 0.8724831938743591)
[2025-02-13 19:45:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:44][root][INFO] - Training Epoch: 1/2, step 6419/7134 completed (loss: 0.42873555421829224, acc: 0.8896104097366333)
[2025-02-13 19:45:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:44][root][INFO] - Training Epoch: 1/2, step 6420/7134 completed (loss: 0.3467799425125122, acc: 0.9177215099334717)
[2025-02-13 19:45:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:45][root][INFO] - Training Epoch: 1/2, step 6421/7134 completed (loss: 0.2582905888557434, acc: 0.9457831382751465)
[2025-02-13 19:45:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:45][root][INFO] - Training Epoch: 1/2, step 6422/7134 completed (loss: 0.30254700779914856, acc: 0.916201114654541)
[2025-02-13 19:45:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:46][root][INFO] - Training Epoch: 1/2, step 6423/7134 completed (loss: 0.28154903650283813, acc: 0.9358974099159241)
[2025-02-13 19:45:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:46][root][INFO] - Training Epoch: 1/2, step 6424/7134 completed (loss: 0.19677110016345978, acc: 0.957446813583374)
[2025-02-13 19:45:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:46][root][INFO] - Training Epoch: 1/2, step 6425/7134 completed (loss: 0.160940021276474, acc: 0.9664804339408875)
[2025-02-13 19:45:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:47][root][INFO] - Training Epoch: 1/2, step 6426/7134 completed (loss: 0.22671186923980713, acc: 0.9441340565681458)
[2025-02-13 19:45:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:47][root][INFO] - Training Epoch: 1/2, step 6427/7134 completed (loss: 0.18820726871490479, acc: 0.9554139971733093)
[2025-02-13 19:45:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:48][root][INFO] - Training Epoch: 1/2, step 6428/7134 completed (loss: 0.14014525711536407, acc: 0.9554139971733093)
[2025-02-13 19:45:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:48][root][INFO] - Training Epoch: 1/2, step 6429/7134 completed (loss: 0.12967602908611298, acc: 0.9612902998924255)
[2025-02-13 19:45:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:48][root][INFO] - Training Epoch: 1/2, step 6430/7134 completed (loss: 0.28316548466682434, acc: 0.949999988079071)
[2025-02-13 19:45:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:49][root][INFO] - Training Epoch: 1/2, step 6431/7134 completed (loss: 0.35441136360168457, acc: 0.9135135412216187)
[2025-02-13 19:45:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:49][root][INFO] - Training Epoch: 1/2, step 6432/7134 completed (loss: 0.9583936333656311, acc: 0.8193548321723938)
[2025-02-13 19:45:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:50][root][INFO] - Training Epoch: 1/2, step 6433/7134 completed (loss: 0.3305128812789917, acc: 0.9261363744735718)
[2025-02-13 19:45:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:50][root][INFO] - Training Epoch: 1/2, step 6434/7134 completed (loss: 0.06763995438814163, acc: 0.9884393215179443)
[2025-02-13 19:45:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:50][root][INFO] - Training Epoch: 1/2, step 6435/7134 completed (loss: 0.15030239522457123, acc: 0.97826087474823)
[2025-02-13 19:45:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:51][root][INFO] - Training Epoch: 1/2, step 6436/7134 completed (loss: 0.23765026032924652, acc: 0.9395973086357117)
[2025-02-13 19:45:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:51][root][INFO] - Training Epoch: 1/2, step 6437/7134 completed (loss: 0.5039466619491577, acc: 0.9276315569877625)
[2025-02-13 19:45:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:52][root][INFO] - Training Epoch: 1/2, step 6438/7134 completed (loss: 0.3752696216106415, acc: 0.9271844625473022)
[2025-02-13 19:45:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:52][root][INFO] - Training Epoch: 1/2, step 6439/7134 completed (loss: 0.2649460434913635, acc: 0.9378882050514221)
[2025-02-13 19:45:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:52][root][INFO] - Training Epoch: 1/2, step 6440/7134 completed (loss: 0.303058922290802, acc: 0.9471153616905212)
[2025-02-13 19:45:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:53][root][INFO] - Training Epoch: 1/2, step 6441/7134 completed (loss: 0.17926499247550964, acc: 0.9622641801834106)
[2025-02-13 19:45:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:53][root][INFO] - Training Epoch: 1/2, step 6442/7134 completed (loss: 0.21285539865493774, acc: 0.9563106894493103)
[2025-02-13 19:45:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:54][root][INFO] - Training Epoch: 1/2, step 6443/7134 completed (loss: 0.09205316007137299, acc: 0.9901960492134094)
[2025-02-13 19:45:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:54][root][INFO] - Training Epoch: 1/2, step 6444/7134 completed (loss: 0.21093249320983887, acc: 0.949999988079071)
[2025-02-13 19:45:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:55][root][INFO] - Training Epoch: 1/2, step 6445/7134 completed (loss: 0.38586747646331787, acc: 0.9458128213882446)
[2025-02-13 19:45:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:55][root][INFO] - Training Epoch: 1/2, step 6446/7134 completed (loss: 0.11529785394668579, acc: 0.9754902124404907)
[2025-02-13 19:45:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:55][root][INFO] - Training Epoch: 1/2, step 6447/7134 completed (loss: 0.29190734028816223, acc: 0.9583333134651184)
[2025-02-13 19:45:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:56][root][INFO] - Training Epoch: 1/2, step 6448/7134 completed (loss: 0.5553306341171265, acc: 0.896774172782898)
[2025-02-13 19:45:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:56][root][INFO] - Training Epoch: 1/2, step 6449/7134 completed (loss: 0.22974258661270142, acc: 0.9430052042007446)
[2025-02-13 19:45:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:57][root][INFO] - Training Epoch: 1/2, step 6450/7134 completed (loss: 0.2116561233997345, acc: 0.9364162087440491)
[2025-02-13 19:45:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:57][root][INFO] - Training Epoch: 1/2, step 6451/7134 completed (loss: 0.2253846377134323, acc: 0.9453781247138977)
[2025-02-13 19:45:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:57][root][INFO] - Training Epoch: 1/2, step 6452/7134 completed (loss: 0.08085326105356216, acc: 0.9846938848495483)
[2025-02-13 19:45:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:58][root][INFO] - Training Epoch: 1/2, step 6453/7134 completed (loss: 0.12539328634738922, acc: 0.9570552110671997)
[2025-02-13 19:45:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:58][root][INFO] - Training Epoch: 1/2, step 6454/7134 completed (loss: 0.2436816543340683, acc: 0.9398906826972961)
[2025-02-13 19:45:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:58][root][INFO] - Training Epoch: 1/2, step 6455/7134 completed (loss: 0.3439510464668274, acc: 0.9333333373069763)
[2025-02-13 19:45:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:59][root][INFO] - Training Epoch: 1/2, step 6456/7134 completed (loss: 0.5993325114250183, acc: 0.8682634830474854)
[2025-02-13 19:45:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:45:59][root][INFO] - Training Epoch: 1/2, step 6457/7134 completed (loss: 0.19128578901290894, acc: 0.9449541568756104)
[2025-02-13 19:45:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:00][root][INFO] - Training Epoch: 1/2, step 6458/7134 completed (loss: 0.19852030277252197, acc: 0.9459459185600281)
[2025-02-13 19:46:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:00][root][INFO] - Training Epoch: 1/2, step 6459/7134 completed (loss: 0.027895549312233925, acc: 0.9938650131225586)
[2025-02-13 19:46:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:00][root][INFO] - Training Epoch: 1/2, step 6460/7134 completed (loss: 0.11039271950721741, acc: 0.9744898080825806)
[2025-02-13 19:46:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:01][root][INFO] - Training Epoch: 1/2, step 6461/7134 completed (loss: 0.06964121758937836, acc: 0.9884393215179443)
[2025-02-13 19:46:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:01][root][INFO] - Training Epoch: 1/2, step 6462/7134 completed (loss: 0.08729971945285797, acc: 0.9893617033958435)
[2025-02-13 19:46:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:02][root][INFO] - Training Epoch: 1/2, step 6463/7134 completed (loss: 0.16669495403766632, acc: 0.9514563083648682)
[2025-02-13 19:46:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:02][root][INFO] - Training Epoch: 1/2, step 6464/7134 completed (loss: 0.13107948005199432, acc: 0.9608938694000244)
[2025-02-13 19:46:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:02][root][INFO] - Training Epoch: 1/2, step 6465/7134 completed (loss: 0.058595210313797, acc: 0.9836065769195557)
[2025-02-13 19:46:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:03][root][INFO] - Training Epoch: 1/2, step 6466/7134 completed (loss: 0.23366239666938782, acc: 0.9312169551849365)
[2025-02-13 19:46:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:03][root][INFO] - Training Epoch: 1/2, step 6467/7134 completed (loss: 0.24220789968967438, acc: 0.9457831382751465)
[2025-02-13 19:46:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:04][root][INFO] - Training Epoch: 1/2, step 6468/7134 completed (loss: 0.11501277983188629, acc: 0.9852941036224365)
[2025-02-13 19:46:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:04][root][INFO] - Training Epoch: 1/2, step 6469/7134 completed (loss: 0.24813951551914215, acc: 0.9392523169517517)
[2025-02-13 19:46:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:05][root][INFO] - Training Epoch: 1/2, step 6470/7134 completed (loss: 0.2231510728597641, acc: 0.9117646813392639)
[2025-02-13 19:46:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:05][root][INFO] - Training Epoch: 1/2, step 6471/7134 completed (loss: 0.06828299909830093, acc: 0.9856114983558655)
[2025-02-13 19:46:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:05][root][INFO] - Training Epoch: 1/2, step 6472/7134 completed (loss: 0.2533019483089447, acc: 0.9399999976158142)
[2025-02-13 19:46:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:06][root][INFO] - Training Epoch: 1/2, step 6473/7134 completed (loss: 0.08335884660482407, acc: 0.9722222089767456)
[2025-02-13 19:46:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:06][root][INFO] - Training Epoch: 1/2, step 6474/7134 completed (loss: 0.1414901614189148, acc: 0.9569892287254333)
[2025-02-13 19:46:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:07][root][INFO] - Training Epoch: 1/2, step 6475/7134 completed (loss: 0.13778385519981384, acc: 0.9642857313156128)
[2025-02-13 19:46:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:07][root][INFO] - Training Epoch: 1/2, step 6476/7134 completed (loss: 0.151719331741333, acc: 0.9599999785423279)
[2025-02-13 19:46:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:07][root][INFO] - Training Epoch: 1/2, step 6477/7134 completed (loss: 0.14472848176956177, acc: 0.95652174949646)
[2025-02-13 19:46:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:08][root][INFO] - Training Epoch: 1/2, step 6478/7134 completed (loss: 0.24543452262878418, acc: 0.95652174949646)
[2025-02-13 19:46:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:08][root][INFO] - Training Epoch: 1/2, step 6479/7134 completed (loss: 0.11990746855735779, acc: 0.9597315192222595)
[2025-02-13 19:46:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:08][root][INFO] - Training Epoch: 1/2, step 6480/7134 completed (loss: 0.11939400434494019, acc: 0.9714285731315613)
[2025-02-13 19:46:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:09][root][INFO] - Training Epoch: 1/2, step 6481/7134 completed (loss: 0.054143358021974564, acc: 0.9928571581840515)
[2025-02-13 19:46:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:09][root][INFO] - Training Epoch: 1/2, step 6482/7134 completed (loss: 0.01671133004128933, acc: 1.0)
[2025-02-13 19:46:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:10][root][INFO] - Training Epoch: 1/2, step 6483/7134 completed (loss: 0.2096557766199112, acc: 0.9513888955116272)
[2025-02-13 19:46:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:10][root][INFO] - Training Epoch: 1/2, step 6484/7134 completed (loss: 0.114157535135746, acc: 0.9766082167625427)
[2025-02-13 19:46:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:10][root][INFO] - Training Epoch: 1/2, step 6485/7134 completed (loss: 0.10470233112573624, acc: 0.9663865566253662)
[2025-02-13 19:46:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:11][root][INFO] - Training Epoch: 1/2, step 6486/7134 completed (loss: 0.47664138674736023, acc: 0.8985507488250732)
[2025-02-13 19:46:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:11][root][INFO] - Training Epoch: 1/2, step 6487/7134 completed (loss: 0.5721572041511536, acc: 0.8602941036224365)
[2025-02-13 19:46:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:12][root][INFO] - Training Epoch: 1/2, step 6488/7134 completed (loss: 0.3526015281677246, acc: 0.8939393758773804)
[2025-02-13 19:46:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:12][root][INFO] - Training Epoch: 1/2, step 6489/7134 completed (loss: 0.47002363204956055, acc: 0.9059829115867615)
[2025-02-13 19:46:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:12][root][INFO] - Training Epoch: 1/2, step 6490/7134 completed (loss: 0.5044348239898682, acc: 0.888059675693512)
[2025-02-13 19:46:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:13][root][INFO] - Training Epoch: 1/2, step 6491/7134 completed (loss: 0.2772452235221863, acc: 0.9350649118423462)
[2025-02-13 19:46:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:13][root][INFO] - Training Epoch: 1/2, step 6492/7134 completed (loss: 0.22003820538520813, acc: 0.9290322661399841)
[2025-02-13 19:46:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:14][root][INFO] - Training Epoch: 1/2, step 6493/7134 completed (loss: 0.3471747636795044, acc: 0.9285714030265808)
[2025-02-13 19:46:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:14][root][INFO] - Training Epoch: 1/2, step 6494/7134 completed (loss: 0.3679489195346832, acc: 0.9292035102844238)
[2025-02-13 19:46:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:14][root][INFO] - Training Epoch: 1/2, step 6495/7134 completed (loss: 0.32188522815704346, acc: 0.9382715821266174)
[2025-02-13 19:46:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:15][root][INFO] - Training Epoch: 1/2, step 6496/7134 completed (loss: 0.4195839762687683, acc: 0.9075144529342651)
[2025-02-13 19:46:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:15][root][INFO] - Training Epoch: 1/2, step 6497/7134 completed (loss: 0.2820383310317993, acc: 0.9444444179534912)
[2025-02-13 19:46:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:16][root][INFO] - Training Epoch: 1/2, step 6498/7134 completed (loss: 0.25822713971138, acc: 0.9352940917015076)
[2025-02-13 19:46:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:16][root][INFO] - Training Epoch: 1/2, step 6499/7134 completed (loss: 0.35995885729789734, acc: 0.9255319237709045)
[2025-02-13 19:46:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:17][root][INFO] - Training Epoch: 1/2, step 6500/7134 completed (loss: 0.4651179611682892, acc: 0.8908045887947083)
[2025-02-13 19:46:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:17][root][INFO] - Training Epoch: 1/2, step 6501/7134 completed (loss: 0.6690288186073303, acc: 0.8350515365600586)
[2025-02-13 19:46:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:17][root][INFO] - Training Epoch: 1/2, step 6502/7134 completed (loss: 0.1966230273246765, acc: 0.9444444179534912)
[2025-02-13 19:46:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:18][root][INFO] - Training Epoch: 1/2, step 6503/7134 completed (loss: 0.31361186504364014, acc: 0.9192546606063843)
[2025-02-13 19:46:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:18][root][INFO] - Training Epoch: 1/2, step 6504/7134 completed (loss: 0.2704068422317505, acc: 0.9109588861465454)
[2025-02-13 19:46:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:19][root][INFO] - Training Epoch: 1/2, step 6505/7134 completed (loss: 0.3611491918563843, acc: 0.9133333563804626)
[2025-02-13 19:46:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:19][root][INFO] - Training Epoch: 1/2, step 6506/7134 completed (loss: 0.2747223377227783, acc: 0.9484536051750183)
[2025-02-13 19:46:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:20][root][INFO] - Training Epoch: 1/2, step 6507/7134 completed (loss: 0.49843549728393555, acc: 0.8650306463241577)
[2025-02-13 19:46:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:20][root][INFO] - Training Epoch: 1/2, step 6508/7134 completed (loss: 0.1668124943971634, acc: 0.949999988079071)
[2025-02-13 19:46:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:20][root][INFO] - Training Epoch: 1/2, step 6509/7134 completed (loss: 0.14415138959884644, acc: 0.9587628841400146)
[2025-02-13 19:46:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:21][root][INFO] - Training Epoch: 1/2, step 6510/7134 completed (loss: 0.2894509732723236, acc: 0.9611650705337524)
[2025-02-13 19:46:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:21][root][INFO] - Training Epoch: 1/2, step 6511/7134 completed (loss: 0.169800266623497, acc: 0.9626865386962891)
[2025-02-13 19:46:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:22][root][INFO] - Training Epoch: 1/2, step 6512/7134 completed (loss: 0.2507683336734772, acc: 0.9272727370262146)
[2025-02-13 19:46:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:22][root][INFO] - Training Epoch: 1/2, step 6513/7134 completed (loss: 0.1201670914888382, acc: 0.970370352268219)
[2025-02-13 19:46:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:22][root][INFO] - Training Epoch: 1/2, step 6514/7134 completed (loss: 0.3840724229812622, acc: 0.9320987462997437)
[2025-02-13 19:46:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:23][root][INFO] - Training Epoch: 1/2, step 6515/7134 completed (loss: 0.17731530964374542, acc: 0.9670329689979553)
[2025-02-13 19:46:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:23][root][INFO] - Training Epoch: 1/2, step 6516/7134 completed (loss: 0.24645152688026428, acc: 0.9411764740943909)
[2025-02-13 19:46:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:23][root][INFO] - Training Epoch: 1/2, step 6517/7134 completed (loss: 0.11838550120592117, acc: 0.9751552939414978)
[2025-02-13 19:46:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:24][root][INFO] - Training Epoch: 1/2, step 6518/7134 completed (loss: 0.08307944238185883, acc: 0.970059871673584)
[2025-02-13 19:46:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:24][root][INFO] - Training Epoch: 1/2, step 6519/7134 completed (loss: 0.17018331587314606, acc: 0.9685039520263672)
[2025-02-13 19:46:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:25][root][INFO] - Training Epoch: 1/2, step 6520/7134 completed (loss: 0.1432960331439972, acc: 0.9805194735527039)
[2025-02-13 19:46:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:25][root][INFO] - Training Epoch: 1/2, step 6521/7134 completed (loss: 0.1412455141544342, acc: 0.9459459185600281)
[2025-02-13 19:46:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:25][root][INFO] - Training Epoch: 1/2, step 6522/7134 completed (loss: 0.21572332084178925, acc: 0.9542483687400818)
[2025-02-13 19:46:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:26][root][INFO] - Training Epoch: 1/2, step 6523/7134 completed (loss: 0.4188960790634155, acc: 0.9166666865348816)
[2025-02-13 19:46:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:26][root][INFO] - Training Epoch: 1/2, step 6524/7134 completed (loss: 0.2152186632156372, acc: 0.9668508172035217)
[2025-02-13 19:46:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:26][root][INFO] - Training Epoch: 1/2, step 6525/7134 completed (loss: 0.07586511969566345, acc: 0.9917355179786682)
[2025-02-13 19:46:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:27][root][INFO] - Training Epoch: 1/2, step 6526/7134 completed (loss: 0.21150434017181396, acc: 0.9644669890403748)
[2025-02-13 19:46:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:27][root][INFO] - Training Epoch: 1/2, step 6527/7134 completed (loss: 0.21442094445228577, acc: 0.9556962251663208)
[2025-02-13 19:46:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:28][root][INFO] - Training Epoch: 1/2, step 6528/7134 completed (loss: 0.24667441844940186, acc: 0.9492753744125366)
[2025-02-13 19:46:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:28][root][INFO] - Training Epoch: 1/2, step 6529/7134 completed (loss: 0.4399164021015167, acc: 0.9236640930175781)
[2025-02-13 19:46:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:28][root][INFO] - Training Epoch: 1/2, step 6530/7134 completed (loss: 0.22454127669334412, acc: 0.939226508140564)
[2025-02-13 19:46:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:29][root][INFO] - Training Epoch: 1/2, step 6531/7134 completed (loss: 0.14010407030582428, acc: 0.957317054271698)
[2025-02-13 19:46:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:29][root][INFO] - Training Epoch: 1/2, step 6532/7134 completed (loss: 0.30341142416000366, acc: 0.9505494236946106)
[2025-02-13 19:46:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:30][root][INFO] - Training Epoch: 1/2, step 6533/7134 completed (loss: 0.07202868163585663, acc: 0.9740259647369385)
[2025-02-13 19:46:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:30][root][INFO] - Training Epoch: 1/2, step 6534/7134 completed (loss: 0.3040813207626343, acc: 0.9421965479850769)
[2025-02-13 19:46:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:30][root][INFO] - Training Epoch: 1/2, step 6535/7134 completed (loss: 0.15572862327098846, acc: 0.9433962106704712)
[2025-02-13 19:46:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:31][root][INFO] - Training Epoch: 1/2, step 6536/7134 completed (loss: 0.13193844258785248, acc: 0.9652174115180969)
[2025-02-13 19:46:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:31][root][INFO] - Training Epoch: 1/2, step 6537/7134 completed (loss: 0.33766576647758484, acc: 0.9107142686843872)
[2025-02-13 19:46:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:31][root][INFO] - Training Epoch: 1/2, step 6538/7134 completed (loss: 0.2381087988615036, acc: 0.9520547986030579)
[2025-02-13 19:46:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:32][root][INFO] - Training Epoch: 1/2, step 6539/7134 completed (loss: 0.10980929434299469, acc: 0.9870129823684692)
[2025-02-13 19:46:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:32][root][INFO] - Training Epoch: 1/2, step 6540/7134 completed (loss: 0.9761770367622375, acc: 0.875)
[2025-02-13 19:46:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:33][root][INFO] - Training Epoch: 1/2, step 6541/7134 completed (loss: 0.17380453646183014, acc: 0.9613259434700012)
[2025-02-13 19:46:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:33][root][INFO] - Training Epoch: 1/2, step 6542/7134 completed (loss: 0.19516734778881073, acc: 0.9571428298950195)
[2025-02-13 19:46:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:33][root][INFO] - Training Epoch: 1/2, step 6543/7134 completed (loss: 0.11380288749933243, acc: 0.9736841917037964)
[2025-02-13 19:46:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:34][root][INFO] - Training Epoch: 1/2, step 6544/7134 completed (loss: 0.2911340892314911, acc: 0.9459459185600281)
[2025-02-13 19:46:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:34][root][INFO] - Training Epoch: 1/2, step 6545/7134 completed (loss: 0.0581546425819397, acc: 0.9917355179786682)
[2025-02-13 19:46:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:35][root][INFO] - Training Epoch: 1/2, step 6546/7134 completed (loss: 0.19274863600730896, acc: 0.97826087474823)
[2025-02-13 19:46:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:35][root][INFO] - Training Epoch: 1/2, step 6547/7134 completed (loss: 0.30480045080184937, acc: 0.9275362491607666)
[2025-02-13 19:46:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:35][root][INFO] - Training Epoch: 1/2, step 6548/7134 completed (loss: 0.10196075588464737, acc: 0.9809523820877075)
[2025-02-13 19:46:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:36][root][INFO] - Training Epoch: 1/2, step 6549/7134 completed (loss: 0.40652668476104736, acc: 0.9007633328437805)
[2025-02-13 19:46:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:36][root][INFO] - Training Epoch: 1/2, step 6550/7134 completed (loss: 0.2856297194957733, acc: 0.9108280539512634)
[2025-02-13 19:46:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:36][root][INFO] - Training Epoch: 1/2, step 6551/7134 completed (loss: 0.1616823673248291, acc: 0.9593908786773682)
[2025-02-13 19:46:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:37][root][INFO] - Training Epoch: 1/2, step 6552/7134 completed (loss: 0.31582000851631165, acc: 0.9251101613044739)
[2025-02-13 19:46:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:37][root][INFO] - Training Epoch: 1/2, step 6553/7134 completed (loss: 0.21654845774173737, acc: 0.9494949579238892)
[2025-02-13 19:46:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:38][root][INFO] - Training Epoch: 1/2, step 6554/7134 completed (loss: 0.18115364015102386, acc: 0.9672897458076477)
[2025-02-13 19:46:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:38][root][INFO] - Training Epoch: 1/2, step 6555/7134 completed (loss: 0.26396507024765015, acc: 0.9353448152542114)
[2025-02-13 19:46:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:38][root][INFO] - Training Epoch: 1/2, step 6556/7134 completed (loss: 0.2660391330718994, acc: 0.9502262473106384)
[2025-02-13 19:46:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:39][root][INFO] - Training Epoch: 1/2, step 6557/7134 completed (loss: 0.32825741171836853, acc: 0.9369369149208069)
[2025-02-13 19:46:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:39][root][INFO] - Training Epoch: 1/2, step 6558/7134 completed (loss: 0.22385744750499725, acc: 0.9579439163208008)
[2025-02-13 19:46:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:40][root][INFO] - Training Epoch: 1/2, step 6559/7134 completed (loss: 0.2657139301300049, acc: 0.9397590160369873)
[2025-02-13 19:46:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:40][root][INFO] - Training Epoch: 1/2, step 6560/7134 completed (loss: 0.10401768982410431, acc: 0.9738219976425171)
[2025-02-13 19:46:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:40][root][INFO] - Training Epoch: 1/2, step 6561/7134 completed (loss: 0.1416092813014984, acc: 0.9581151604652405)
[2025-02-13 19:46:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:41][root][INFO] - Training Epoch: 1/2, step 6562/7134 completed (loss: 0.2621438503265381, acc: 0.9308176040649414)
[2025-02-13 19:46:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:41][root][INFO] - Training Epoch: 1/2, step 6563/7134 completed (loss: 0.17917650938034058, acc: 0.9545454382896423)
[2025-02-13 19:46:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:42][root][INFO] - Training Epoch: 1/2, step 6564/7134 completed (loss: 0.14332425594329834, acc: 0.9819276928901672)
[2025-02-13 19:46:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:42][root][INFO] - Training Epoch: 1/2, step 6565/7134 completed (loss: 0.3753087818622589, acc: 0.8963963985443115)
[2025-02-13 19:46:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:42][root][INFO] - Training Epoch: 1/2, step 6566/7134 completed (loss: 0.21574029326438904, acc: 0.9516128897666931)
[2025-02-13 19:46:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:43][root][INFO] - Training Epoch: 1/2, step 6567/7134 completed (loss: 0.24367378652095795, acc: 0.9288889169692993)
[2025-02-13 19:46:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:43][root][INFO] - Training Epoch: 1/2, step 6568/7134 completed (loss: 0.16213704645633698, acc: 0.9560439586639404)
[2025-02-13 19:46:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:44][root][INFO] - Training Epoch: 1/2, step 6569/7134 completed (loss: 0.14465981721878052, acc: 0.9593908786773682)
[2025-02-13 19:46:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:44][root][INFO] - Training Epoch: 1/2, step 6570/7134 completed (loss: 0.15320123732089996, acc: 0.9655172228813171)
[2025-02-13 19:46:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:44][root][INFO] - Training Epoch: 1/2, step 6571/7134 completed (loss: 0.0882921814918518, acc: 0.9696969985961914)
[2025-02-13 19:46:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:45][root][INFO] - Training Epoch: 1/2, step 6572/7134 completed (loss: 0.14699630439281464, acc: 0.9609755873680115)
[2025-02-13 19:46:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:45][root][INFO] - Training Epoch: 1/2, step 6573/7134 completed (loss: 0.08220728486776352, acc: 0.9805825352668762)
[2025-02-13 19:46:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:45][root][INFO] - Training Epoch: 1/2, step 6574/7134 completed (loss: 0.08755617588758469, acc: 0.9784482717514038)
[2025-02-13 19:46:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:46][root][INFO] - Training Epoch: 1/2, step 6575/7134 completed (loss: 0.19051286578178406, acc: 0.9530516266822815)
[2025-02-13 19:46:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:46][root][INFO] - Training Epoch: 1/2, step 6576/7134 completed (loss: 0.12024758011102676, acc: 0.9724770784378052)
[2025-02-13 19:46:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:47][root][INFO] - Training Epoch: 1/2, step 6577/7134 completed (loss: 0.21962599456310272, acc: 0.9454545378684998)
[2025-02-13 19:46:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:47][root][INFO] - Training Epoch: 1/2, step 6578/7134 completed (loss: 0.182824969291687, acc: 0.9631147384643555)
[2025-02-13 19:46:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:47][root][INFO] - Training Epoch: 1/2, step 6579/7134 completed (loss: 0.4019717574119568, acc: 0.9172932505607605)
[2025-02-13 19:46:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:48][root][INFO] - Training Epoch: 1/2, step 6580/7134 completed (loss: 0.17228427529335022, acc: 0.9631578922271729)
[2025-02-13 19:46:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:48][root][INFO] - Training Epoch: 1/2, step 6581/7134 completed (loss: 0.19577232003211975, acc: 0.9675675630569458)
[2025-02-13 19:46:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:49][root][INFO] - Training Epoch: 1/2, step 6582/7134 completed (loss: 0.3726974129676819, acc: 0.9078947305679321)
[2025-02-13 19:46:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:49][root][INFO] - Training Epoch: 1/2, step 6583/7134 completed (loss: 0.22065360844135284, acc: 0.9547738432884216)
[2025-02-13 19:46:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:49][root][INFO] - Training Epoch: 1/2, step 6584/7134 completed (loss: 0.2130793035030365, acc: 0.9677419066429138)
[2025-02-13 19:46:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:50][root][INFO] - Training Epoch: 1/2, step 6585/7134 completed (loss: 0.17877483367919922, acc: 0.9391891956329346)
[2025-02-13 19:46:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:50][root][INFO] - Training Epoch: 1/2, step 6586/7134 completed (loss: 0.24948637187480927, acc: 0.9570552110671997)
[2025-02-13 19:46:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:50][root][INFO] - Training Epoch: 1/2, step 6587/7134 completed (loss: 0.25726187229156494, acc: 0.9536082744598389)
[2025-02-13 19:46:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:51][root][INFO] - Training Epoch: 1/2, step 6588/7134 completed (loss: 0.21655578911304474, acc: 0.9476743936538696)
[2025-02-13 19:46:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:51][root][INFO] - Training Epoch: 1/2, step 6589/7134 completed (loss: 0.19905370473861694, acc: 0.9440993666648865)
[2025-02-13 19:46:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:52][root][INFO] - Training Epoch: 1/2, step 6590/7134 completed (loss: 0.2504463195800781, acc: 0.9308510422706604)
[2025-02-13 19:46:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:52][root][INFO] - Training Epoch: 1/2, step 6591/7134 completed (loss: 0.20430980622768402, acc: 0.9532163739204407)
[2025-02-13 19:46:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:52][root][INFO] - Training Epoch: 1/2, step 6592/7134 completed (loss: 0.21181617677211761, acc: 0.935960590839386)
[2025-02-13 19:46:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:53][root][INFO] - Training Epoch: 1/2, step 6593/7134 completed (loss: 0.2626309096813202, acc: 0.9399999976158142)
[2025-02-13 19:46:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:53][root][INFO] - Training Epoch: 1/2, step 6594/7134 completed (loss: 0.1736004650592804, acc: 0.9655172228813171)
[2025-02-13 19:46:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:54][root][INFO] - Training Epoch: 1/2, step 6595/7134 completed (loss: 0.1815754771232605, acc: 0.9506173133850098)
[2025-02-13 19:46:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:54][root][INFO] - Training Epoch: 1/2, step 6596/7134 completed (loss: 0.27877581119537354, acc: 0.9216867685317993)
[2025-02-13 19:46:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:54][root][INFO] - Training Epoch: 1/2, step 6597/7134 completed (loss: 0.15875692665576935, acc: 0.9685863852500916)
[2025-02-13 19:46:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:55][root][INFO] - Training Epoch: 1/2, step 6598/7134 completed (loss: 0.2844972014427185, acc: 0.9180327653884888)
[2025-02-13 19:46:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:55][root][INFO] - Training Epoch: 1/2, step 6599/7134 completed (loss: 0.25047993659973145, acc: 0.9365853667259216)
[2025-02-13 19:46:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:55][root][INFO] - Training Epoch: 1/2, step 6600/7134 completed (loss: 0.3537473976612091, acc: 0.9044944047927856)
[2025-02-13 19:46:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:56][root][INFO] - Training Epoch: 1/2, step 6601/7134 completed (loss: 0.38949713110923767, acc: 0.9215686321258545)
[2025-02-13 19:46:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:56][root][INFO] - Training Epoch: 1/2, step 6602/7134 completed (loss: 0.19665154814720154, acc: 0.9455445408821106)
[2025-02-13 19:46:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:57][root][INFO] - Training Epoch: 1/2, step 6603/7134 completed (loss: 0.18365952372550964, acc: 0.9748427867889404)
[2025-02-13 19:46:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:57][root][INFO] - Training Epoch: 1/2, step 6604/7134 completed (loss: 0.4102626442909241, acc: 0.9139785170555115)
[2025-02-13 19:46:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:57][root][INFO] - Training Epoch: 1/2, step 6605/7134 completed (loss: 0.4617089033126831, acc: 0.9032257795333862)
[2025-02-13 19:46:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:58][root][INFO] - Training Epoch: 1/2, step 6606/7134 completed (loss: 0.23153294622898102, acc: 0.9385474920272827)
[2025-02-13 19:46:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:58][root][INFO] - Training Epoch: 1/2, step 6607/7134 completed (loss: 0.3733847439289093, acc: 0.9354838728904724)
[2025-02-13 19:46:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:59][root][INFO] - Training Epoch: 1/2, step 6608/7134 completed (loss: 0.14846745133399963, acc: 0.965753436088562)
[2025-02-13 19:46:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:59][root][INFO] - Training Epoch: 1/2, step 6609/7134 completed (loss: 0.23364248871803284, acc: 0.9371069073677063)
[2025-02-13 19:46:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:46:59][root][INFO] - Training Epoch: 1/2, step 6610/7134 completed (loss: 0.3077627122402191, acc: 0.9151515364646912)
[2025-02-13 19:46:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:00][root][INFO] - Training Epoch: 1/2, step 6611/7134 completed (loss: 0.33166831731796265, acc: 0.9178082346916199)
[2025-02-13 19:47:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:00][root][INFO] - Training Epoch: 1/2, step 6612/7134 completed (loss: 0.41540655493736267, acc: 0.8974359035491943)
[2025-02-13 19:47:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:01][root][INFO] - Training Epoch: 1/2, step 6613/7134 completed (loss: 0.18286344408988953, acc: 0.9395973086357117)
[2025-02-13 19:47:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:01][root][INFO] - Training Epoch: 1/2, step 6614/7134 completed (loss: 0.19025327265262604, acc: 0.9615384340286255)
[2025-02-13 19:47:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:01][root][INFO] - Training Epoch: 1/2, step 6615/7134 completed (loss: 0.26500147581100464, acc: 0.9391891956329346)
[2025-02-13 19:47:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:02][root][INFO] - Training Epoch: 1/2, step 6616/7134 completed (loss: 0.15232950448989868, acc: 0.9696969985961914)
[2025-02-13 19:47:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:02][root][INFO] - Training Epoch: 1/2, step 6617/7134 completed (loss: 0.18983475863933563, acc: 0.9777777791023254)
[2025-02-13 19:47:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:02][root][INFO] - Training Epoch: 1/2, step 6618/7134 completed (loss: 0.07799261808395386, acc: 0.9855072498321533)
[2025-02-13 19:47:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:03][root][INFO] - Training Epoch: 1/2, step 6619/7134 completed (loss: 0.09329597651958466, acc: 0.9717513918876648)
[2025-02-13 19:47:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:03][root][INFO] - Training Epoch: 1/2, step 6620/7134 completed (loss: 0.21293166279792786, acc: 0.9305555820465088)
[2025-02-13 19:47:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:03][root][INFO] - Training Epoch: 1/2, step 6621/7134 completed (loss: 0.1742166429758072, acc: 0.9314285516738892)
[2025-02-13 19:47:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:04][root][INFO] - Training Epoch: 1/2, step 6622/7134 completed (loss: 0.18359558284282684, acc: 0.9358974099159241)
[2025-02-13 19:47:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:04][root][INFO] - Training Epoch: 1/2, step 6623/7134 completed (loss: 0.19239580631256104, acc: 0.946107804775238)
[2025-02-13 19:47:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:04][root][INFO] - Training Epoch: 1/2, step 6624/7134 completed (loss: 0.0875454768538475, acc: 0.987261176109314)
[2025-02-13 19:47:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:05][root][INFO] - Training Epoch: 1/2, step 6625/7134 completed (loss: 0.15816709399223328, acc: 0.9536423683166504)
[2025-02-13 19:47:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:05][root][INFO] - Training Epoch: 1/2, step 6626/7134 completed (loss: 0.1617259830236435, acc: 0.9657142758369446)
[2025-02-13 19:47:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:06][root][INFO] - Training Epoch: 1/2, step 6627/7134 completed (loss: 0.15177975594997406, acc: 0.9677419066429138)
[2025-02-13 19:47:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:06][root][INFO] - Training Epoch: 1/2, step 6628/7134 completed (loss: 0.15767672657966614, acc: 0.9523809552192688)
[2025-02-13 19:47:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:06][root][INFO] - Training Epoch: 1/2, step 6629/7134 completed (loss: 0.26138588786125183, acc: 0.956250011920929)
[2025-02-13 19:47:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:07][root][INFO] - Training Epoch: 1/2, step 6630/7134 completed (loss: 0.21522939205169678, acc: 0.953125)
[2025-02-13 19:47:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:07][root][INFO] - Training Epoch: 1/2, step 6631/7134 completed (loss: 0.11926285177469254, acc: 0.9813664555549622)
[2025-02-13 19:47:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:08][root][INFO] - Training Epoch: 1/2, step 6632/7134 completed (loss: 0.12531043589115143, acc: 0.9695431590080261)
[2025-02-13 19:47:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:08][root][INFO] - Training Epoch: 1/2, step 6633/7134 completed (loss: 0.18040749430656433, acc: 0.9670329689979553)
[2025-02-13 19:47:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:08][root][INFO] - Training Epoch: 1/2, step 6634/7134 completed (loss: 0.11863187700510025, acc: 0.9774436354637146)
[2025-02-13 19:47:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:09][root][INFO] - Training Epoch: 1/2, step 6635/7134 completed (loss: 0.4182451665401459, acc: 0.9150000214576721)
[2025-02-13 19:47:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:09][root][INFO] - Training Epoch: 1/2, step 6636/7134 completed (loss: 0.23067866265773773, acc: 0.9365853667259216)
[2025-02-13 19:47:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:09][root][INFO] - Training Epoch: 1/2, step 6637/7134 completed (loss: 0.17949607968330383, acc: 0.956250011920929)
[2025-02-13 19:47:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:10][root][INFO] - Training Epoch: 1/2, step 6638/7134 completed (loss: 0.15102452039718628, acc: 0.9560439586639404)
[2025-02-13 19:47:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:10][root][INFO] - Training Epoch: 1/2, step 6639/7134 completed (loss: 0.27117958664894104, acc: 0.9281437397003174)
[2025-02-13 19:47:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:11][root][INFO] - Training Epoch: 1/2, step 6640/7134 completed (loss: 0.0919361487030983, acc: 0.9808917045593262)
[2025-02-13 19:47:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:11][root][INFO] - Training Epoch: 1/2, step 6641/7134 completed (loss: 0.37992143630981445, acc: 0.9203540086746216)
[2025-02-13 19:47:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:11][root][INFO] - Training Epoch: 1/2, step 6642/7134 completed (loss: 0.21733281016349792, acc: 0.9277108311653137)
[2025-02-13 19:47:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:12][root][INFO] - Training Epoch: 1/2, step 6643/7134 completed (loss: 0.18071430921554565, acc: 0.9516128897666931)
[2025-02-13 19:47:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:12][root][INFO] - Training Epoch: 1/2, step 6644/7134 completed (loss: 0.29672491550445557, acc: 0.9419087171554565)
[2025-02-13 19:47:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:12][root][INFO] - Training Epoch: 1/2, step 6645/7134 completed (loss: 0.16382168233394623, acc: 0.9629629850387573)
[2025-02-13 19:47:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:13][root][INFO] - Training Epoch: 1/2, step 6646/7134 completed (loss: 0.20842061936855316, acc: 0.9563318490982056)
[2025-02-13 19:47:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:13][root][INFO] - Training Epoch: 1/2, step 6647/7134 completed (loss: 0.10122891515493393, acc: 0.9823529124259949)
[2025-02-13 19:47:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:14][root][INFO] - Training Epoch: 1/2, step 6648/7134 completed (loss: 0.19331148266792297, acc: 0.948387086391449)
[2025-02-13 19:47:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:14][root][INFO] - Training Epoch: 1/2, step 6649/7134 completed (loss: 0.20308686792850494, acc: 0.9516907930374146)
[2025-02-13 19:47:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:14][root][INFO] - Training Epoch: 1/2, step 6650/7134 completed (loss: 0.19806741178035736, acc: 0.9409090876579285)
[2025-02-13 19:47:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:15][root][INFO] - Training Epoch: 1/2, step 6651/7134 completed (loss: 0.24977898597717285, acc: 0.9375)
[2025-02-13 19:47:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:15][root][INFO] - Training Epoch: 1/2, step 6652/7134 completed (loss: 0.3292001485824585, acc: 0.9313725233078003)
[2025-02-13 19:47:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:16][root][INFO] - Training Epoch: 1/2, step 6653/7134 completed (loss: 0.41288772225379944, acc: 0.8727272748947144)
[2025-02-13 19:47:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:16][root][INFO] - Training Epoch: 1/2, step 6654/7134 completed (loss: 0.5187354683876038, acc: 0.8860759735107422)
[2025-02-13 19:47:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:16][root][INFO] - Training Epoch: 1/2, step 6655/7134 completed (loss: 0.1803659349679947, acc: 0.9523809552192688)
[2025-02-13 19:47:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:17][root][INFO] - Training Epoch: 1/2, step 6656/7134 completed (loss: 0.2938917279243469, acc: 0.9105263352394104)
[2025-02-13 19:47:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:17][root][INFO] - Training Epoch: 1/2, step 6657/7134 completed (loss: 0.3458107113838196, acc: 0.9095744490623474)
[2025-02-13 19:47:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:18][root][INFO] - Training Epoch: 1/2, step 6658/7134 completed (loss: 0.169134259223938, acc: 0.9695431590080261)
[2025-02-13 19:47:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:18][root][INFO] - Training Epoch: 1/2, step 6659/7134 completed (loss: 0.1854247897863388, acc: 0.9617486596107483)
[2025-02-13 19:47:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:18][root][INFO] - Training Epoch: 1/2, step 6660/7134 completed (loss: 0.10745050758123398, acc: 0.9713114500045776)
[2025-02-13 19:47:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:19][root][INFO] - Training Epoch: 1/2, step 6661/7134 completed (loss: 0.2212994545698166, acc: 0.93034827709198)
[2025-02-13 19:47:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:19][root][INFO] - Training Epoch: 1/2, step 6662/7134 completed (loss: 0.5133988857269287, acc: 0.8705036044120789)
[2025-02-13 19:47:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:19][root][INFO] - Training Epoch: 1/2, step 6663/7134 completed (loss: 0.1693006008863449, acc: 0.9653679728507996)
[2025-02-13 19:47:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:20][root][INFO] - Training Epoch: 1/2, step 6664/7134 completed (loss: 0.21666847169399261, acc: 0.9428571462631226)
[2025-02-13 19:47:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:20][root][INFO] - Training Epoch: 1/2, step 6665/7134 completed (loss: 0.2704993784427643, acc: 0.9433962106704712)
[2025-02-13 19:47:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:21][root][INFO] - Training Epoch: 1/2, step 6666/7134 completed (loss: 0.14578476548194885, acc: 0.9515151381492615)
[2025-02-13 19:47:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:21][root][INFO] - Training Epoch: 1/2, step 6667/7134 completed (loss: 0.39822763204574585, acc: 0.8911564350128174)
[2025-02-13 19:47:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:21][root][INFO] - Training Epoch: 1/2, step 6668/7134 completed (loss: 0.07309188693761826, acc: 0.9781022071838379)
[2025-02-13 19:47:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:22][root][INFO] - Training Epoch: 1/2, step 6669/7134 completed (loss: 0.11170017719268799, acc: 0.9791666865348816)
[2025-02-13 19:47:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:22][root][INFO] - Training Epoch: 1/2, step 6670/7134 completed (loss: 0.1355542093515396, acc: 0.9568345546722412)
[2025-02-13 19:47:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:22][root][INFO] - Training Epoch: 1/2, step 6671/7134 completed (loss: 0.0811871588230133, acc: 0.9756097793579102)
[2025-02-13 19:47:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:23][root][INFO] - Training Epoch: 1/2, step 6672/7134 completed (loss: 0.0768294408917427, acc: 0.9919999837875366)
[2025-02-13 19:47:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:23][root][INFO] - Training Epoch: 1/2, step 6673/7134 completed (loss: 0.11119132488965988, acc: 0.9622641801834106)
[2025-02-13 19:47:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:24][root][INFO] - Training Epoch: 1/2, step 6674/7134 completed (loss: 0.04839995503425598, acc: 0.991525411605835)
[2025-02-13 19:47:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:24][root][INFO] - Training Epoch: 1/2, step 6675/7134 completed (loss: 0.34443235397338867, acc: 0.9313725233078003)
[2025-02-13 19:47:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:25][root][INFO] - Training Epoch: 1/2, step 6676/7134 completed (loss: 0.0909334346652031, acc: 0.9800000190734863)
[2025-02-13 19:47:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:25][root][INFO] - Training Epoch: 1/2, step 6677/7134 completed (loss: 0.14286492764949799, acc: 0.9694656729698181)
[2025-02-13 19:47:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:25][root][INFO] - Training Epoch: 1/2, step 6678/7134 completed (loss: 0.1143481656908989, acc: 0.9629629850387573)
[2025-02-13 19:47:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:26][root][INFO] - Training Epoch: 1/2, step 6679/7134 completed (loss: 0.15296564996242523, acc: 0.9646017551422119)
[2025-02-13 19:47:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:26][root][INFO] - Training Epoch: 1/2, step 6680/7134 completed (loss: 0.03565632551908493, acc: 1.0)
[2025-02-13 19:47:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:26][root][INFO] - Training Epoch: 1/2, step 6681/7134 completed (loss: 0.01760290563106537, acc: 1.0)
[2025-02-13 19:47:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:27][root][INFO] - Training Epoch: 1/2, step 6682/7134 completed (loss: 0.24242767691612244, acc: 0.9514563083648682)
[2025-02-13 19:47:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:27][root][INFO] - Training Epoch: 1/2, step 6683/7134 completed (loss: 0.29858386516571045, acc: 0.931034505367279)
[2025-02-13 19:47:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:28][root][INFO] - Training Epoch: 1/2, step 6684/7134 completed (loss: 0.25252237915992737, acc: 0.9527559280395508)
[2025-02-13 19:47:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:28][root][INFO] - Training Epoch: 1/2, step 6685/7134 completed (loss: 0.16101162135601044, acc: 0.9440000057220459)
[2025-02-13 19:47:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:28][root][INFO] - Training Epoch: 1/2, step 6686/7134 completed (loss: 0.2578151226043701, acc: 0.9396551847457886)
[2025-02-13 19:47:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:29][root][INFO] - Training Epoch: 1/2, step 6687/7134 completed (loss: 0.16382522881031036, acc: 0.9651162624359131)
[2025-02-13 19:47:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:29][root][INFO] - Training Epoch: 1/2, step 6688/7134 completed (loss: 0.4344539940357208, acc: 0.8920863270759583)
[2025-02-13 19:47:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:30][root][INFO] - Training Epoch: 1/2, step 6689/7134 completed (loss: 0.28190380334854126, acc: 0.9166666865348816)
[2025-02-13 19:47:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:30][root][INFO] - Training Epoch: 1/2, step 6690/7134 completed (loss: 0.38149580359458923, acc: 0.9117646813392639)
[2025-02-13 19:47:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:30][root][INFO] - Training Epoch: 1/2, step 6691/7134 completed (loss: 0.12953364849090576, acc: 0.9652777910232544)
[2025-02-13 19:47:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:31][root][INFO] - Training Epoch: 1/2, step 6692/7134 completed (loss: 0.5687183141708374, acc: 0.8785046935081482)
[2025-02-13 19:47:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:31][root][INFO] - Training Epoch: 1/2, step 6693/7134 completed (loss: 0.25220605731010437, acc: 0.9333333373069763)
[2025-02-13 19:47:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:32][root][INFO] - Training Epoch: 1/2, step 6694/7134 completed (loss: 0.16808538138866425, acc: 0.9536423683166504)
[2025-02-13 19:47:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:32][root][INFO] - Training Epoch: 1/2, step 6695/7134 completed (loss: 0.14371329545974731, acc: 0.9715909361839294)
[2025-02-13 19:47:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:32][root][INFO] - Training Epoch: 1/2, step 6696/7134 completed (loss: 0.23981796205043793, acc: 0.9375)
[2025-02-13 19:47:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:33][root][INFO] - Training Epoch: 1/2, step 6697/7134 completed (loss: 0.4331636130809784, acc: 0.8928571343421936)
[2025-02-13 19:47:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:33][root][INFO] - Training Epoch: 1/2, step 6698/7134 completed (loss: 0.1638835221529007, acc: 0.9530201554298401)
[2025-02-13 19:47:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:33][root][INFO] - Training Epoch: 1/2, step 6699/7134 completed (loss: 0.2254132181406021, acc: 0.961904764175415)
[2025-02-13 19:47:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:34][root][INFO] - Training Epoch: 1/2, step 6700/7134 completed (loss: 0.3976258635520935, acc: 0.8814814686775208)
[2025-02-13 19:47:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:34][root][INFO] - Training Epoch: 1/2, step 6701/7134 completed (loss: 0.7504629492759705, acc: 0.8059701323509216)
[2025-02-13 19:47:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:35][root][INFO] - Training Epoch: 1/2, step 6702/7134 completed (loss: 0.4467505216598511, acc: 0.9130434989929199)
[2025-02-13 19:47:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:35][root][INFO] - Training Epoch: 1/2, step 6703/7134 completed (loss: 0.30850499868392944, acc: 0.9420289993286133)
[2025-02-13 19:47:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:35][root][INFO] - Training Epoch: 1/2, step 6704/7134 completed (loss: 0.30356431007385254, acc: 0.9080459475517273)
[2025-02-13 19:47:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:36][root][INFO] - Training Epoch: 1/2, step 6705/7134 completed (loss: 0.14971520006656647, acc: 0.9768785834312439)
[2025-02-13 19:47:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:36][root][INFO] - Training Epoch: 1/2, step 6706/7134 completed (loss: 0.16176879405975342, acc: 0.9545454382896423)
[2025-02-13 19:47:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:37][root][INFO] - Training Epoch: 1/2, step 6707/7134 completed (loss: 0.2076244056224823, acc: 0.94017094373703)
[2025-02-13 19:47:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:37][root][INFO] - Training Epoch: 1/2, step 6708/7134 completed (loss: 0.23402076959609985, acc: 0.9312977194786072)
[2025-02-13 19:47:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:37][root][INFO] - Training Epoch: 1/2, step 6709/7134 completed (loss: 0.12376601248979568, acc: 0.9451219439506531)
[2025-02-13 19:47:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:38][root][INFO] - Training Epoch: 1/2, step 6710/7134 completed (loss: 0.3179817199707031, acc: 0.932330846786499)
[2025-02-13 19:47:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:38][root][INFO] - Training Epoch: 1/2, step 6711/7134 completed (loss: 0.27583014965057373, acc: 0.949367105960846)
[2025-02-13 19:47:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:38][root][INFO] - Training Epoch: 1/2, step 6712/7134 completed (loss: 0.3752749264240265, acc: 0.9172413945198059)
[2025-02-13 19:47:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:39][root][INFO] - Training Epoch: 1/2, step 6713/7134 completed (loss: 0.5534729361534119, acc: 0.8834356069564819)
[2025-02-13 19:47:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:39][root][INFO] - Training Epoch: 1/2, step 6714/7134 completed (loss: 0.3060556650161743, acc: 0.9190751314163208)
[2025-02-13 19:47:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:40][root][INFO] - Training Epoch: 1/2, step 6715/7134 completed (loss: 0.5107210874557495, acc: 0.8986486196517944)
[2025-02-13 19:47:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:40][root][INFO] - Training Epoch: 1/2, step 6716/7134 completed (loss: 0.2297113537788391, acc: 0.9452054500579834)
[2025-02-13 19:47:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:40][root][INFO] - Training Epoch: 1/2, step 6717/7134 completed (loss: 0.354569673538208, acc: 0.9391891956329346)
[2025-02-13 19:47:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:41][root][INFO] - Training Epoch: 1/2, step 6718/7134 completed (loss: 0.2533126473426819, acc: 0.9448275566101074)
[2025-02-13 19:47:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:41][root][INFO] - Training Epoch: 1/2, step 6719/7134 completed (loss: 0.18023595213890076, acc: 0.9530201554298401)
[2025-02-13 19:47:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:42][root][INFO] - Training Epoch: 1/2, step 6720/7134 completed (loss: 0.35302475094795227, acc: 0.9252873659133911)
[2025-02-13 19:47:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:42][root][INFO] - Training Epoch: 1/2, step 6721/7134 completed (loss: 0.32199549674987793, acc: 0.9235668778419495)
[2025-02-13 19:47:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:42][root][INFO] - Training Epoch: 1/2, step 6722/7134 completed (loss: 0.46106570959091187, acc: 0.8881118893623352)
[2025-02-13 19:47:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:43][root][INFO] - Training Epoch: 1/2, step 6723/7134 completed (loss: 0.2197791188955307, acc: 0.9453551769256592)
[2025-02-13 19:47:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:43][root][INFO] - Training Epoch: 1/2, step 6724/7134 completed (loss: 0.3587949872016907, acc: 0.9433962106704712)
[2025-02-13 19:47:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:43][root][INFO] - Training Epoch: 1/2, step 6725/7134 completed (loss: 0.21524052321910858, acc: 0.9605262875556946)
[2025-02-13 19:47:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:44][root][INFO] - Training Epoch: 1/2, step 6726/7134 completed (loss: 0.45404237508773804, acc: 0.9504132270812988)
[2025-02-13 19:47:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:44][root][INFO] - Training Epoch: 1/2, step 6727/7134 completed (loss: 0.5114014148712158, acc: 0.9064748287200928)
[2025-02-13 19:47:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:45][root][INFO] - Training Epoch: 1/2, step 6728/7134 completed (loss: 0.3058304488658905, acc: 0.9161290526390076)
[2025-02-13 19:47:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:45][root][INFO] - Training Epoch: 1/2, step 6729/7134 completed (loss: 0.11757934838533401, acc: 0.9683544039726257)
[2025-02-13 19:47:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:45][root][INFO] - Training Epoch: 1/2, step 6730/7134 completed (loss: 0.3112817108631134, acc: 0.9195402264595032)
[2025-02-13 19:47:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:46][root][INFO] - Training Epoch: 1/2, step 6731/7134 completed (loss: 0.26131361722946167, acc: 0.9397590160369873)
[2025-02-13 19:47:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:46][root][INFO] - Training Epoch: 1/2, step 6732/7134 completed (loss: 0.18238003551959991, acc: 0.9487179517745972)
[2025-02-13 19:47:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:47][root][INFO] - Training Epoch: 1/2, step 6733/7134 completed (loss: 0.18816353380680084, acc: 0.9568345546722412)
[2025-02-13 19:47:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:47][root][INFO] - Training Epoch: 1/2, step 6734/7134 completed (loss: 0.28241321444511414, acc: 0.9300699234008789)
[2025-02-13 19:47:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:47][root][INFO] - Training Epoch: 1/2, step 6735/7134 completed (loss: 0.2963171601295471, acc: 0.9230769276618958)
[2025-02-13 19:47:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:48][root][INFO] - Training Epoch: 1/2, step 6736/7134 completed (loss: 0.22780735790729523, acc: 0.9333333373069763)
[2025-02-13 19:47:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:48][root][INFO] - Training Epoch: 1/2, step 6737/7134 completed (loss: 0.40347009897232056, acc: 0.9205297827720642)
[2025-02-13 19:47:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:48][root][INFO] - Training Epoch: 1/2, step 6738/7134 completed (loss: 0.14890523254871368, acc: 0.9558823704719543)
[2025-02-13 19:47:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:49][root][INFO] - Training Epoch: 1/2, step 6739/7134 completed (loss: 0.2486458122730255, acc: 0.9642857313156128)
[2025-02-13 19:47:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:49][root][INFO] - Training Epoch: 1/2, step 6740/7134 completed (loss: 0.22342610359191895, acc: 0.9189189076423645)
[2025-02-13 19:47:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:50][root][INFO] - Training Epoch: 1/2, step 6741/7134 completed (loss: 0.10805891454219818, acc: 0.9759036302566528)
[2025-02-13 19:47:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:50][root][INFO] - Training Epoch: 1/2, step 6742/7134 completed (loss: 0.06380143761634827, acc: 0.9823529124259949)
[2025-02-13 19:47:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:50][root][INFO] - Training Epoch: 1/2, step 6743/7134 completed (loss: 0.0866960734128952, acc: 0.9774011373519897)
[2025-02-13 19:47:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:51][root][INFO] - Training Epoch: 1/2, step 6744/7134 completed (loss: 0.09262969344854355, acc: 0.9662162065505981)
[2025-02-13 19:47:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:51][root][INFO] - Training Epoch: 1/2, step 6745/7134 completed (loss: 0.05915353447198868, acc: 0.9825581312179565)
[2025-02-13 19:47:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:51][root][INFO] - Training Epoch: 1/2, step 6746/7134 completed (loss: 0.07255770266056061, acc: 0.9776119589805603)
[2025-02-13 19:47:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:52][root][INFO] - Training Epoch: 1/2, step 6747/7134 completed (loss: 0.09690120816230774, acc: 0.9738219976425171)
[2025-02-13 19:47:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:52][root][INFO] - Training Epoch: 1/2, step 6748/7134 completed (loss: 0.14181110262870789, acc: 0.9803921580314636)
[2025-02-13 19:47:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:53][root][INFO] - Training Epoch: 1/2, step 6749/7134 completed (loss: 0.15669235587120056, acc: 0.9599999785423279)
[2025-02-13 19:47:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:53][root][INFO] - Training Epoch: 1/2, step 6750/7134 completed (loss: 0.13008086383342743, acc: 0.9752475023269653)
[2025-02-13 19:47:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:53][root][INFO] - Training Epoch: 1/2, step 6751/7134 completed (loss: 0.09425864368677139, acc: 0.9850746393203735)
[2025-02-13 19:47:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:54][root][INFO] - Training Epoch: 1/2, step 6752/7134 completed (loss: 0.18383285403251648, acc: 0.9509202241897583)
[2025-02-13 19:47:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:54][root][INFO] - Training Epoch: 1/2, step 6753/7134 completed (loss: 0.06533251702785492, acc: 0.9747474789619446)
[2025-02-13 19:47:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:55][root][INFO] - Training Epoch: 1/2, step 6754/7134 completed (loss: 0.07150866091251373, acc: 0.9895287752151489)
[2025-02-13 19:47:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:55][root][INFO] - Training Epoch: 1/2, step 6755/7134 completed (loss: 0.13705606758594513, acc: 0.9738219976425171)
[2025-02-13 19:47:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:55][root][INFO] - Training Epoch: 1/2, step 6756/7134 completed (loss: 0.04104999080300331, acc: 0.9887640476226807)
[2025-02-13 19:47:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:56][root][INFO] - Training Epoch: 1/2, step 6757/7134 completed (loss: 0.058187901973724365, acc: 0.9806451797485352)
[2025-02-13 19:47:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:56][root][INFO] - Training Epoch: 1/2, step 6758/7134 completed (loss: 0.07371937483549118, acc: 0.9792746305465698)
[2025-02-13 19:47:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:57][root][INFO] - Training Epoch: 1/2, step 6759/7134 completed (loss: 0.10333151370286942, acc: 0.9732142686843872)
[2025-02-13 19:47:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:57][root][INFO] - Training Epoch: 1/2, step 6760/7134 completed (loss: 0.4171925485134125, acc: 0.9468085169792175)
[2025-02-13 19:47:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:57][root][INFO] - Training Epoch: 1/2, step 6761/7134 completed (loss: 0.23417708277702332, acc: 0.9666666388511658)
[2025-02-13 19:47:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:58][root][INFO] - Training Epoch: 1/2, step 6762/7134 completed (loss: 0.07008122652769089, acc: 0.9850000143051147)
[2025-02-13 19:47:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:58][root][INFO] - Training Epoch: 1/2, step 6763/7134 completed (loss: 0.04220995306968689, acc: 0.9893048405647278)
[2025-02-13 19:47:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:58][root][INFO] - Training Epoch: 1/2, step 6764/7134 completed (loss: 0.10908973962068558, acc: 0.9823529124259949)
[2025-02-13 19:47:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:59][root][INFO] - Training Epoch: 1/2, step 6765/7134 completed (loss: 0.0501646064221859, acc: 0.9890109896659851)
[2025-02-13 19:47:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:59][root][INFO] - Training Epoch: 1/2, step 6766/7134 completed (loss: 0.12687276303768158, acc: 0.9714285731315613)
[2025-02-13 19:47:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:47:59][root][INFO] - Training Epoch: 1/2, step 6767/7134 completed (loss: 0.23710419237613678, acc: 0.9375)
[2025-02-13 19:48:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:00][root][INFO] - Training Epoch: 1/2, step 6768/7134 completed (loss: 0.12362765520811081, acc: 0.9841269850730896)
[2025-02-13 19:48:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:00][root][INFO] - Training Epoch: 1/2, step 6769/7134 completed (loss: 0.5514857172966003, acc: 0.8974359035491943)
[2025-02-13 19:48:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:01][root][INFO] - Training Epoch: 1/2, step 6770/7134 completed (loss: 0.1503489911556244, acc: 0.9684210419654846)
[2025-02-13 19:48:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:01][root][INFO] - Training Epoch: 1/2, step 6771/7134 completed (loss: 0.07648225128650665, acc: 0.9824561476707458)
[2025-02-13 19:48:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:01][root][INFO] - Training Epoch: 1/2, step 6772/7134 completed (loss: 0.37405091524124146, acc: 0.9370629191398621)
[2025-02-13 19:48:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:02][root][INFO] - Training Epoch: 1/2, step 6773/7134 completed (loss: 0.1701602041721344, acc: 0.9753086566925049)
[2025-02-13 19:48:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:02][root][INFO] - Training Epoch: 1/2, step 6774/7134 completed (loss: 0.2655318081378937, acc: 0.9289940595626831)
[2025-02-13 19:48:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:03][root][INFO] - Training Epoch: 1/2, step 6775/7134 completed (loss: 0.12322025001049042, acc: 0.9586206674575806)
[2025-02-13 19:48:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:03][root][INFO] - Training Epoch: 1/2, step 6776/7134 completed (loss: 0.12853728234767914, acc: 0.9679999947547913)
[2025-02-13 19:48:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:03][root][INFO] - Training Epoch: 1/2, step 6777/7134 completed (loss: 0.16861853003501892, acc: 0.9828571677207947)
[2025-02-13 19:48:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:04][root][INFO] - Training Epoch: 1/2, step 6778/7134 completed (loss: 0.06438714265823364, acc: 0.9715909361839294)
[2025-02-13 19:48:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:04][root][INFO] - Training Epoch: 1/2, step 6779/7134 completed (loss: 0.11239586770534515, acc: 0.9702380895614624)
[2025-02-13 19:48:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:04][root][INFO] - Training Epoch: 1/2, step 6780/7134 completed (loss: 0.06309272348880768, acc: 0.9880239367485046)
[2025-02-13 19:48:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:05][root][INFO] - Training Epoch: 1/2, step 6781/7134 completed (loss: 0.08335728943347931, acc: 0.9766082167625427)
[2025-02-13 19:48:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:05][root][INFO] - Training Epoch: 1/2, step 6782/7134 completed (loss: 0.025671806186437607, acc: 1.0)
[2025-02-13 19:48:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:06][root][INFO] - Training Epoch: 1/2, step 6783/7134 completed (loss: 0.18415164947509766, acc: 0.9586206674575806)
[2025-02-13 19:48:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:06][root][INFO] - Training Epoch: 1/2, step 6784/7134 completed (loss: 0.09533050656318665, acc: 0.9746835231781006)
[2025-02-13 19:48:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:07][root][INFO] - Training Epoch: 1/2, step 6785/7134 completed (loss: 0.07565981894731522, acc: 0.9673202633857727)
[2025-02-13 19:48:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:07][root][INFO] - Training Epoch: 1/2, step 6786/7134 completed (loss: 0.04051413759589195, acc: 0.9887005686759949)
[2025-02-13 19:48:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:07][root][INFO] - Training Epoch: 1/2, step 6787/7134 completed (loss: 0.07646309584379196, acc: 0.9836065769195557)
[2025-02-13 19:48:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:08][root][INFO] - Training Epoch: 1/2, step 6788/7134 completed (loss: 0.11988189816474915, acc: 0.971222996711731)
[2025-02-13 19:48:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:08][root][INFO] - Training Epoch: 1/2, step 6789/7134 completed (loss: 0.07869578152894974, acc: 0.978723406791687)
[2025-02-13 19:48:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:08][root][INFO] - Training Epoch: 1/2, step 6790/7134 completed (loss: 0.19747163355350494, acc: 0.9659090638160706)
[2025-02-13 19:48:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:09][root][INFO] - Training Epoch: 1/2, step 6791/7134 completed (loss: 0.13102582097053528, acc: 0.9615384340286255)
[2025-02-13 19:48:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:09][root][INFO] - Training Epoch: 1/2, step 6792/7134 completed (loss: 0.04341251403093338, acc: 1.0)
[2025-02-13 19:48:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:10][root][INFO] - Training Epoch: 1/2, step 6793/7134 completed (loss: 0.06831246614456177, acc: 0.9824561476707458)
[2025-02-13 19:48:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:10][root][INFO] - Training Epoch: 1/2, step 6794/7134 completed (loss: 0.08784002810716629, acc: 0.9931972622871399)
[2025-02-13 19:48:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:10][root][INFO] - Training Epoch: 1/2, step 6795/7134 completed (loss: 0.055729806423187256, acc: 0.9942857027053833)
[2025-02-13 19:48:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:11][root][INFO] - Training Epoch: 1/2, step 6796/7134 completed (loss: 0.019206933677196503, acc: 1.0)
[2025-02-13 19:48:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:11][root][INFO] - Training Epoch: 1/2, step 6797/7134 completed (loss: 0.19477427005767822, acc: 0.9512194991111755)
[2025-02-13 19:48:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:12][root][INFO] - Training Epoch: 1/2, step 6798/7134 completed (loss: 0.14195369184017181, acc: 0.9578313231468201)
[2025-02-13 19:48:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:12][root][INFO] - Training Epoch: 1/2, step 6799/7134 completed (loss: 0.08992953598499298, acc: 0.9820359349250793)
[2025-02-13 19:48:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:12][root][INFO] - Training Epoch: 1/2, step 6800/7134 completed (loss: 0.10714589804410934, acc: 0.9683544039726257)
[2025-02-13 19:48:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:13][root][INFO] - Training Epoch: 1/2, step 6801/7134 completed (loss: 0.13365302979946136, acc: 0.9640287756919861)
[2025-02-13 19:48:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:13][root][INFO] - Training Epoch: 1/2, step 6802/7134 completed (loss: 0.08190411329269409, acc: 0.9860140085220337)
[2025-02-13 19:48:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:13][root][INFO] - Training Epoch: 1/2, step 6803/7134 completed (loss: 0.10607034713029861, acc: 0.9722222089767456)
[2025-02-13 19:48:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:14][root][INFO] - Training Epoch: 1/2, step 6804/7134 completed (loss: 0.1899852305650711, acc: 0.9530201554298401)
[2025-02-13 19:48:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:14][root][INFO] - Training Epoch: 1/2, step 6805/7134 completed (loss: 0.25968414545059204, acc: 0.9555555582046509)
[2025-02-13 19:48:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:15][root][INFO] - Training Epoch: 1/2, step 6806/7134 completed (loss: 0.21867042779922485, acc: 0.966292142868042)
[2025-02-13 19:48:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:15][root][INFO] - Training Epoch: 1/2, step 6807/7134 completed (loss: 0.09637057781219482, acc: 0.981249988079071)
[2025-02-13 19:48:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:15][root][INFO] - Training Epoch: 1/2, step 6808/7134 completed (loss: 0.12616640329360962, acc: 0.9597315192222595)
[2025-02-13 19:48:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:16][root][INFO] - Training Epoch: 1/2, step 6809/7134 completed (loss: 0.16269034147262573, acc: 0.9679487347602844)
[2025-02-13 19:48:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:16][root][INFO] - Training Epoch: 1/2, step 6810/7134 completed (loss: 0.13500267267227173, acc: 0.9318181872367859)
[2025-02-13 19:48:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:16][root][INFO] - Training Epoch: 1/2, step 6811/7134 completed (loss: 0.24006520211696625, acc: 0.9709302186965942)
[2025-02-13 19:48:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:17][root][INFO] - Training Epoch: 1/2, step 6812/7134 completed (loss: 0.12681126594543457, acc: 0.9777777791023254)
[2025-02-13 19:48:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:17][root][INFO] - Training Epoch: 1/2, step 6813/7134 completed (loss: 0.09716831147670746, acc: 0.981249988079071)
[2025-02-13 19:48:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:18][root][INFO] - Training Epoch: 1/2, step 6814/7134 completed (loss: 0.09348064661026001, acc: 0.9828571677207947)
[2025-02-13 19:48:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:18][root][INFO] - Training Epoch: 1/2, step 6815/7134 completed (loss: 0.06216346472501755, acc: 0.9941176176071167)
[2025-02-13 19:48:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:18][root][INFO] - Training Epoch: 1/2, step 6816/7134 completed (loss: 0.16297291219234467, acc: 0.9652777910232544)
[2025-02-13 19:48:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:19][root][INFO] - Training Epoch: 1/2, step 6817/7134 completed (loss: 0.2752591073513031, acc: 0.9436619877815247)
[2025-02-13 19:48:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:19][root][INFO] - Training Epoch: 1/2, step 6818/7134 completed (loss: 0.0782892256975174, acc: 0.9931972622871399)
[2025-02-13 19:48:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:20][root][INFO] - Training Epoch: 1/2, step 6819/7134 completed (loss: 0.15285497903823853, acc: 0.956204354763031)
[2025-02-13 19:48:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:20][root][INFO] - Training Epoch: 1/2, step 6820/7134 completed (loss: 0.11111310869455338, acc: 0.970802903175354)
[2025-02-13 19:48:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:20][root][INFO] - Training Epoch: 1/2, step 6821/7134 completed (loss: 0.026887042447924614, acc: 1.0)
[2025-02-13 19:48:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:21][root][INFO] - Training Epoch: 1/2, step 6822/7134 completed (loss: 0.28802651166915894, acc: 0.9379844665527344)
[2025-02-13 19:48:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:21][root][INFO] - Training Epoch: 1/2, step 6823/7134 completed (loss: 0.19279517233371735, acc: 0.9638554453849792)
[2025-02-13 19:48:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:21][root][INFO] - Training Epoch: 1/2, step 6824/7134 completed (loss: 0.3210863471031189, acc: 0.9186992049217224)
[2025-02-13 19:48:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:22][root][INFO] - Training Epoch: 1/2, step 6825/7134 completed (loss: 0.274445116519928, acc: 0.9139785170555115)
[2025-02-13 19:48:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:22][root][INFO] - Training Epoch: 1/2, step 6826/7134 completed (loss: 0.2818778157234192, acc: 0.9275362491607666)
[2025-02-13 19:48:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:23][root][INFO] - Training Epoch: 1/2, step 6827/7134 completed (loss: 0.5823891758918762, acc: 0.890625)
[2025-02-13 19:48:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:23][root][INFO] - Training Epoch: 1/2, step 6828/7134 completed (loss: 0.43270838260650635, acc: 0.8943662047386169)
[2025-02-13 19:48:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:23][root][INFO] - Training Epoch: 1/2, step 6829/7134 completed (loss: 0.3467368483543396, acc: 0.9349112510681152)
[2025-02-13 19:48:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:24][root][INFO] - Training Epoch: 1/2, step 6830/7134 completed (loss: 0.19347718358039856, acc: 0.9459459185600281)
[2025-02-13 19:48:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:24][root][INFO] - Training Epoch: 1/2, step 6831/7134 completed (loss: 0.19095991551876068, acc: 0.9497206807136536)
[2025-02-13 19:48:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:25][root][INFO] - Training Epoch: 1/2, step 6832/7134 completed (loss: 0.28931450843811035, acc: 0.9375)
[2025-02-13 19:48:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:25][root][INFO] - Training Epoch: 1/2, step 6833/7134 completed (loss: 0.2981703579425812, acc: 0.9171974658966064)
[2025-02-13 19:48:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:25][root][INFO] - Training Epoch: 1/2, step 6834/7134 completed (loss: 0.2658364176750183, acc: 0.9463087320327759)
[2025-02-13 19:48:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:26][root][INFO] - Training Epoch: 1/2, step 6835/7134 completed (loss: 0.2757890820503235, acc: 0.9477124214172363)
[2025-02-13 19:48:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:26][root][INFO] - Training Epoch: 1/2, step 6836/7134 completed (loss: 0.18475739657878876, acc: 0.9620853066444397)
[2025-02-13 19:48:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:26][root][INFO] - Training Epoch: 1/2, step 6837/7134 completed (loss: 0.18433374166488647, acc: 0.9646464586257935)
[2025-02-13 19:48:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:27][root][INFO] - Training Epoch: 1/2, step 6838/7134 completed (loss: 0.29177722334861755, acc: 0.9337349534034729)
[2025-02-13 19:48:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:27][root][INFO] - Training Epoch: 1/2, step 6839/7134 completed (loss: 0.428989976644516, acc: 0.9200000166893005)
[2025-02-13 19:48:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:27][root][INFO] - Training Epoch: 1/2, step 6840/7134 completed (loss: 0.3140449523925781, acc: 0.9299362897872925)
[2025-02-13 19:48:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:28][root][INFO] - Training Epoch: 1/2, step 6841/7134 completed (loss: 0.1622769832611084, acc: 0.9451219439506531)
[2025-02-13 19:48:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:28][root][INFO] - Training Epoch: 1/2, step 6842/7134 completed (loss: 0.250519335269928, acc: 0.9450549483299255)
[2025-02-13 19:48:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:29][root][INFO] - Training Epoch: 1/2, step 6843/7134 completed (loss: 0.18623417615890503, acc: 0.9585492014884949)
[2025-02-13 19:48:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:29][root][INFO] - Training Epoch: 1/2, step 6844/7134 completed (loss: 0.24469813704490662, acc: 0.9328858852386475)
[2025-02-13 19:48:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:29][root][INFO] - Training Epoch: 1/2, step 6845/7134 completed (loss: 0.21711473166942596, acc: 0.9615384340286255)
[2025-02-13 19:48:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:30][root][INFO] - Training Epoch: 1/2, step 6846/7134 completed (loss: 0.1927463859319687, acc: 0.9567901492118835)
[2025-02-13 19:48:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:30][root][INFO] - Training Epoch: 1/2, step 6847/7134 completed (loss: 0.16815152764320374, acc: 0.9624999761581421)
[2025-02-13 19:48:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:31][root][INFO] - Training Epoch: 1/2, step 6848/7134 completed (loss: 0.2620967924594879, acc: 0.932584285736084)
[2025-02-13 19:48:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:31][root][INFO] - Training Epoch: 1/2, step 6849/7134 completed (loss: 0.1757550984621048, acc: 0.9571428298950195)
[2025-02-13 19:48:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:31][root][INFO] - Training Epoch: 1/2, step 6850/7134 completed (loss: 0.1852908879518509, acc: 0.9428571462631226)
[2025-02-13 19:48:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:32][root][INFO] - Training Epoch: 1/2, step 6851/7134 completed (loss: 0.24114805459976196, acc: 0.950276255607605)
[2025-02-13 19:48:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:32][root][INFO] - Training Epoch: 1/2, step 6852/7134 completed (loss: 0.16858232021331787, acc: 0.9642857313156128)
[2025-02-13 19:48:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:32][root][INFO] - Training Epoch: 1/2, step 6853/7134 completed (loss: 0.18263311684131622, acc: 0.9479768872261047)
[2025-02-13 19:48:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:33][root][INFO] - Training Epoch: 1/2, step 6854/7134 completed (loss: 0.13828206062316895, acc: 0.9745762944221497)
[2025-02-13 19:48:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:33][root][INFO] - Training Epoch: 1/2, step 6855/7134 completed (loss: 0.08350882679224014, acc: 0.9813084006309509)
[2025-02-13 19:48:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:34][root][INFO] - Training Epoch: 1/2, step 6856/7134 completed (loss: 0.04307880997657776, acc: 0.9879518151283264)
[2025-02-13 19:48:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:34][root][INFO] - Training Epoch: 1/2, step 6857/7134 completed (loss: 0.05619033798575401, acc: 0.988095223903656)
[2025-02-13 19:48:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:34][root][INFO] - Training Epoch: 1/2, step 6858/7134 completed (loss: 0.0189548060297966, acc: 1.0)
[2025-02-13 19:48:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:35][root][INFO] - Training Epoch: 1/2, step 6859/7134 completed (loss: 0.04964263737201691, acc: 0.9887640476226807)
[2025-02-13 19:48:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:35][root][INFO] - Training Epoch: 1/2, step 6860/7134 completed (loss: 0.09558911621570587, acc: 0.9862068891525269)
[2025-02-13 19:48:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:35][root][INFO] - Training Epoch: 1/2, step 6861/7134 completed (loss: 0.0698070377111435, acc: 0.9790209531784058)
[2025-02-13 19:48:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:36][root][INFO] - Training Epoch: 1/2, step 6862/7134 completed (loss: 0.13693223893642426, acc: 0.9621211886405945)
[2025-02-13 19:48:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:36][root][INFO] - Training Epoch: 1/2, step 6863/7134 completed (loss: 0.08945496380329132, acc: 0.9764705896377563)
[2025-02-13 19:48:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:37][root][INFO] - Training Epoch: 1/2, step 6864/7134 completed (loss: 0.07419009506702423, acc: 0.9858155846595764)
[2025-02-13 19:48:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:37][root][INFO] - Training Epoch: 1/2, step 6865/7134 completed (loss: 0.14273808896541595, acc: 0.9593495726585388)
[2025-02-13 19:48:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:37][root][INFO] - Training Epoch: 1/2, step 6866/7134 completed (loss: 0.03038383647799492, acc: 0.9934640526771545)
[2025-02-13 19:48:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:38][root][INFO] - Training Epoch: 1/2, step 6867/7134 completed (loss: 0.024297792464494705, acc: 0.9924812316894531)
[2025-02-13 19:48:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:38][root][INFO] - Training Epoch: 1/2, step 6868/7134 completed (loss: 0.01900339685380459, acc: 1.0)
[2025-02-13 19:48:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:39][root][INFO] - Training Epoch: 1/2, step 6869/7134 completed (loss: 0.02657226100564003, acc: 1.0)
[2025-02-13 19:48:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:39][root][INFO] - Training Epoch: 1/2, step 6870/7134 completed (loss: 0.10817039012908936, acc: 0.9683544039726257)
[2025-02-13 19:48:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:39][root][INFO] - Training Epoch: 1/2, step 6871/7134 completed (loss: 0.03799214959144592, acc: 0.9920634627342224)
[2025-02-13 19:48:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:40][root][INFO] - Training Epoch: 1/2, step 6872/7134 completed (loss: 0.05925370380282402, acc: 0.9943181872367859)
[2025-02-13 19:48:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:40][root][INFO] - Training Epoch: 1/2, step 6873/7134 completed (loss: 0.11069809645414352, acc: 0.9823529124259949)
[2025-02-13 19:48:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:40][root][INFO] - Training Epoch: 1/2, step 6874/7134 completed (loss: 0.03423989191651344, acc: 0.9921875)
[2025-02-13 19:48:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:41][root][INFO] - Training Epoch: 1/2, step 6875/7134 completed (loss: 0.14206218719482422, acc: 0.9664429426193237)
[2025-02-13 19:48:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:41][root][INFO] - Training Epoch: 1/2, step 6876/7134 completed (loss: 0.0774138867855072, acc: 0.9847328066825867)
[2025-02-13 19:48:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:42][root][INFO] - Training Epoch: 1/2, step 6877/7134 completed (loss: 0.07292630523443222, acc: 0.9882352948188782)
[2025-02-13 19:48:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:42][root][INFO] - Training Epoch: 1/2, step 6878/7134 completed (loss: 0.13245730102062225, acc: 0.9607843160629272)
[2025-02-13 19:48:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:42][root][INFO] - Training Epoch: 1/2, step 6879/7134 completed (loss: 0.1310705840587616, acc: 0.9741379022598267)
[2025-02-13 19:48:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:43][root][INFO] - Training Epoch: 1/2, step 6880/7134 completed (loss: 0.06644003838300705, acc: 0.9847715497016907)
[2025-02-13 19:48:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:43][root][INFO] - Training Epoch: 1/2, step 6881/7134 completed (loss: 0.15400128066539764, acc: 0.9627659320831299)
[2025-02-13 19:48:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:43][root][INFO] - Training Epoch: 1/2, step 6882/7134 completed (loss: 0.2971319258213043, acc: 0.9436619877815247)
[2025-02-13 19:48:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:44][root][INFO] - Training Epoch: 1/2, step 6883/7134 completed (loss: 0.16226185858249664, acc: 0.9632353186607361)
[2025-02-13 19:48:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:44][root][INFO] - Training Epoch: 1/2, step 6884/7134 completed (loss: 0.18791450560092926, acc: 0.9545454382896423)
[2025-02-13 19:48:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:45][root][INFO] - Training Epoch: 1/2, step 6885/7134 completed (loss: 0.05360584333539009, acc: 1.0)
[2025-02-13 19:48:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:45][root][INFO] - Training Epoch: 1/2, step 6886/7134 completed (loss: 0.14461290836334229, acc: 0.9594594836235046)
[2025-02-13 19:48:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:45][root][INFO] - Training Epoch: 1/2, step 6887/7134 completed (loss: 0.22909227013587952, acc: 0.939130425453186)
[2025-02-13 19:48:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:46][root][INFO] - Training Epoch: 1/2, step 6888/7134 completed (loss: 0.1345367431640625, acc: 0.9733333587646484)
[2025-02-13 19:48:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:46][root][INFO] - Training Epoch: 1/2, step 6889/7134 completed (loss: 0.11604032665491104, acc: 0.9673202633857727)
[2025-02-13 19:48:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:46][root][INFO] - Training Epoch: 1/2, step 6890/7134 completed (loss: 0.24618998169898987, acc: 0.9462365508079529)
[2025-02-13 19:48:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:47][root][INFO] - Training Epoch: 1/2, step 6891/7134 completed (loss: 0.174202099442482, acc: 0.9567901492118835)
[2025-02-13 19:48:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:47][root][INFO] - Training Epoch: 1/2, step 6892/7134 completed (loss: 0.18156537413597107, acc: 0.9677419066429138)
[2025-02-13 19:48:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:48][root][INFO] - Training Epoch: 1/2, step 6893/7134 completed (loss: 0.23873212933540344, acc: 0.9375)
[2025-02-13 19:48:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:48][root][INFO] - Training Epoch: 1/2, step 6894/7134 completed (loss: 0.16547586023807526, acc: 0.9635416865348816)
[2025-02-13 19:48:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:48][root][INFO] - Training Epoch: 1/2, step 6895/7134 completed (loss: 0.13214465975761414, acc: 0.9753694534301758)
[2025-02-13 19:48:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:49][root][INFO] - Training Epoch: 1/2, step 6896/7134 completed (loss: 0.2467917501926422, acc: 0.9306358098983765)
[2025-02-13 19:48:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:49][root][INFO] - Training Epoch: 1/2, step 6897/7134 completed (loss: 0.1481751799583435, acc: 0.9554455280303955)
[2025-02-13 19:48:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:49][root][INFO] - Training Epoch: 1/2, step 6898/7134 completed (loss: 0.16795644164085388, acc: 0.9595375657081604)
[2025-02-13 19:48:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:50][root][INFO] - Training Epoch: 1/2, step 6899/7134 completed (loss: 0.1336853802204132, acc: 0.9680851101875305)
[2025-02-13 19:48:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:50][root][INFO] - Training Epoch: 1/2, step 6900/7134 completed (loss: 0.3446231782436371, acc: 0.9248554706573486)
[2025-02-13 19:48:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:51][root][INFO] - Training Epoch: 1/2, step 6901/7134 completed (loss: 0.1557362675666809, acc: 0.9621621370315552)
[2025-02-13 19:48:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:51][root][INFO] - Training Epoch: 1/2, step 6902/7134 completed (loss: 0.057673100382089615, acc: 0.9887640476226807)
[2025-02-13 19:48:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:51][root][INFO] - Training Epoch: 1/2, step 6903/7134 completed (loss: 0.08000575006008148, acc: 0.9902439117431641)
[2025-02-13 19:48:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:52][root][INFO] - Training Epoch: 1/2, step 6904/7134 completed (loss: 0.11940320581197739, acc: 0.9740932583808899)
[2025-02-13 19:48:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:52][root][INFO] - Training Epoch: 1/2, step 6905/7134 completed (loss: 0.16459910571575165, acc: 0.9601989984512329)
[2025-02-13 19:48:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:52][root][INFO] - Training Epoch: 1/2, step 6906/7134 completed (loss: 0.18011188507080078, acc: 0.9754902124404907)
[2025-02-13 19:48:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:53][root][INFO] - Training Epoch: 1/2, step 6907/7134 completed (loss: 0.15590894222259521, acc: 0.9689119458198547)
[2025-02-13 19:48:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:53][root][INFO] - Training Epoch: 1/2, step 6908/7134 completed (loss: 0.1767420768737793, acc: 0.9666666388511658)
[2025-02-13 19:48:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:54][root][INFO] - Training Epoch: 1/2, step 6909/7134 completed (loss: 0.13356076180934906, acc: 0.9672130942344666)
[2025-02-13 19:48:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:54][root][INFO] - Training Epoch: 1/2, step 6910/7134 completed (loss: 0.0916610136628151, acc: 0.9791666865348816)
[2025-02-13 19:48:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:54][root][INFO] - Training Epoch: 1/2, step 6911/7134 completed (loss: 0.36438342928886414, acc: 0.9281768202781677)
[2025-02-13 19:48:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:55][root][INFO] - Training Epoch: 1/2, step 6912/7134 completed (loss: 0.1850324273109436, acc: 0.9609755873680115)
[2025-02-13 19:48:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:55][root][INFO] - Training Epoch: 1/2, step 6913/7134 completed (loss: 0.16130895912647247, acc: 0.966183602809906)
[2025-02-13 19:48:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:55][root][INFO] - Training Epoch: 1/2, step 6914/7134 completed (loss: 0.37704718112945557, acc: 0.9215686321258545)
[2025-02-13 19:48:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:56][root][INFO] - Training Epoch: 1/2, step 6915/7134 completed (loss: 0.18364426493644714, acc: 0.9567567706108093)
[2025-02-13 19:48:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:56][root][INFO] - Training Epoch: 1/2, step 6916/7134 completed (loss: 0.201409712433815, acc: 0.9414634108543396)
[2025-02-13 19:48:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:57][root][INFO] - Training Epoch: 1/2, step 6917/7134 completed (loss: 0.17795422673225403, acc: 0.9484536051750183)
[2025-02-13 19:48:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:57][root][INFO] - Training Epoch: 1/2, step 6918/7134 completed (loss: 0.12697356939315796, acc: 0.969072163105011)
[2025-02-13 19:48:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:57][root][INFO] - Training Epoch: 1/2, step 6919/7134 completed (loss: 0.2126665860414505, acc: 0.9593908786773682)
[2025-02-13 19:48:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:58][root][INFO] - Training Epoch: 1/2, step 6920/7134 completed (loss: 0.08320952951908112, acc: 0.970059871673584)
[2025-02-13 19:48:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:58][root][INFO] - Training Epoch: 1/2, step 6921/7134 completed (loss: 0.16508235037326813, acc: 0.9656862616539001)
[2025-02-13 19:48:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:58][root][INFO] - Training Epoch: 1/2, step 6922/7134 completed (loss: 0.18538522720336914, acc: 0.9487179517745972)
[2025-02-13 19:48:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:59][root][INFO] - Training Epoch: 1/2, step 6923/7134 completed (loss: 0.17444594204425812, acc: 0.9561403393745422)
[2025-02-13 19:48:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:48:59][root][INFO] - Training Epoch: 1/2, step 6924/7134 completed (loss: 0.1762673556804657, acc: 0.9515151381492615)
[2025-02-13 19:48:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:00][root][INFO] - Training Epoch: 1/2, step 6925/7134 completed (loss: 0.1768263876438141, acc: 0.9411764740943909)
[2025-02-13 19:49:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:00][root][INFO] - Training Epoch: 1/2, step 6926/7134 completed (loss: 0.209882915019989, acc: 0.9444444179534912)
[2025-02-13 19:49:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:00][root][INFO] - Training Epoch: 1/2, step 6927/7134 completed (loss: 0.21683497726917267, acc: 0.9554455280303955)
[2025-02-13 19:49:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:01][root][INFO] - Training Epoch: 1/2, step 6928/7134 completed (loss: 0.31650346517562866, acc: 0.953125)
[2025-02-13 19:49:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:01][root][INFO] - Training Epoch: 1/2, step 6929/7134 completed (loss: 0.13055956363677979, acc: 0.9659090638160706)
[2025-02-13 19:49:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:02][root][INFO] - Training Epoch: 1/2, step 6930/7134 completed (loss: 0.15587997436523438, acc: 0.9583333134651184)
[2025-02-13 19:49:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:02][root][INFO] - Training Epoch: 1/2, step 6931/7134 completed (loss: 0.2002231478691101, acc: 0.9576719403266907)
[2025-02-13 19:49:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:02][root][INFO] - Training Epoch: 1/2, step 6932/7134 completed (loss: 0.23483924567699432, acc: 0.940119743347168)
[2025-02-13 19:49:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:03][root][INFO] - Training Epoch: 1/2, step 6933/7134 completed (loss: 0.2980649769306183, acc: 0.940119743347168)
[2025-02-13 19:49:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:03][root][INFO] - Training Epoch: 1/2, step 6934/7134 completed (loss: 0.20295710861682892, acc: 0.9551281929016113)
[2025-02-13 19:49:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:03][root][INFO] - Training Epoch: 1/2, step 6935/7134 completed (loss: 0.14872217178344727, acc: 0.9734042286872864)
[2025-02-13 19:49:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:04][root][INFO] - Training Epoch: 1/2, step 6936/7134 completed (loss: 0.11352658271789551, acc: 0.9623655676841736)
[2025-02-13 19:49:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:04][root][INFO] - Training Epoch: 1/2, step 6937/7134 completed (loss: 0.2548842132091522, acc: 0.9589040875434875)
[2025-02-13 19:49:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:05][root][INFO] - Training Epoch: 1/2, step 6938/7134 completed (loss: 0.5229774117469788, acc: 0.8678160905838013)
[2025-02-13 19:49:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:05][root][INFO] - Training Epoch: 1/2, step 6939/7134 completed (loss: 0.27461639046669006, acc: 0.9226190447807312)
[2025-02-13 19:49:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:05][root][INFO] - Training Epoch: 1/2, step 6940/7134 completed (loss: 0.4996768832206726, acc: 0.9034482836723328)
[2025-02-13 19:49:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:06][root][INFO] - Training Epoch: 1/2, step 6941/7134 completed (loss: 0.09536530822515488, acc: 0.9779411554336548)
[2025-02-13 19:49:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:06][root][INFO] - Training Epoch: 1/2, step 6942/7134 completed (loss: 0.22802384197711945, acc: 0.9554139971733093)
[2025-02-13 19:49:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:06][root][INFO] - Training Epoch: 1/2, step 6943/7134 completed (loss: 0.21676412224769592, acc: 0.9469026327133179)
[2025-02-13 19:49:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:07][root][INFO] - Training Epoch: 1/2, step 6944/7134 completed (loss: 0.20654873549938202, acc: 0.951724112033844)
[2025-02-13 19:49:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:07][root][INFO] - Training Epoch: 1/2, step 6945/7134 completed (loss: 0.33570048213005066, acc: 0.953125)
[2025-02-13 19:49:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:08][root][INFO] - Training Epoch: 1/2, step 6946/7134 completed (loss: 0.3830730617046356, acc: 0.9139785170555115)
[2025-02-13 19:49:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:08][root][INFO] - Training Epoch: 1/2, step 6947/7134 completed (loss: 0.23491869866847992, acc: 0.9557521939277649)
[2025-02-13 19:49:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:08][root][INFO] - Training Epoch: 1/2, step 6948/7134 completed (loss: 0.3672120273113251, acc: 0.9142857193946838)
[2025-02-13 19:49:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:09][root][INFO] - Training Epoch: 1/2, step 6949/7134 completed (loss: 0.22694136202335358, acc: 0.939130425453186)
[2025-02-13 19:49:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:09][root][INFO] - Training Epoch: 1/2, step 6950/7134 completed (loss: 0.14548024535179138, acc: 0.9620253443717957)
[2025-02-13 19:49:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:09][root][INFO] - Training Epoch: 1/2, step 6951/7134 completed (loss: 0.27854034304618835, acc: 0.9166666865348816)
[2025-02-13 19:49:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:10][root][INFO] - Training Epoch: 1/2, step 6952/7134 completed (loss: 0.231816366314888, acc: 0.9428571462631226)
[2025-02-13 19:49:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:10][root][INFO] - Training Epoch: 1/2, step 6953/7134 completed (loss: 0.2664143145084381, acc: 0.9398496150970459)
[2025-02-13 19:49:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:11][root][INFO] - Training Epoch: 1/2, step 6954/7134 completed (loss: 0.4194607436656952, acc: 0.9396551847457886)
[2025-02-13 19:49:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:11][root][INFO] - Training Epoch: 1/2, step 6955/7134 completed (loss: 0.17204241454601288, acc: 0.925000011920929)
[2025-02-13 19:49:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:11][root][INFO] - Training Epoch: 1/2, step 6956/7134 completed (loss: 0.16178198158740997, acc: 0.956204354763031)
[2025-02-13 19:49:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:12][root][INFO] - Training Epoch: 1/2, step 6957/7134 completed (loss: 0.16789565980434418, acc: 0.9679999947547913)
[2025-02-13 19:49:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:12][root][INFO] - Training Epoch: 1/2, step 6958/7134 completed (loss: 0.4226873219013214, acc: 0.8803418874740601)
[2025-02-13 19:49:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:12][root][INFO] - Training Epoch: 1/2, step 6959/7134 completed (loss: 0.43070200085639954, acc: 0.9174311757087708)
[2025-02-13 19:49:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:13][root][INFO] - Training Epoch: 1/2, step 6960/7134 completed (loss: 0.24929577112197876, acc: 0.9236111044883728)
[2025-02-13 19:49:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:13][root][INFO] - Training Epoch: 1/2, step 6961/7134 completed (loss: 0.1862790584564209, acc: 0.9487179517745972)
[2025-02-13 19:49:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:14][root][INFO] - Training Epoch: 1/2, step 6962/7134 completed (loss: 0.19255052506923676, acc: 0.95652174949646)
[2025-02-13 19:49:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:14][root][INFO] - Training Epoch: 1/2, step 6963/7134 completed (loss: 0.157741978764534, acc: 0.9470198750495911)
[2025-02-13 19:49:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:14][root][INFO] - Training Epoch: 1/2, step 6964/7134 completed (loss: 0.06811688095331192, acc: 0.9848484992980957)
[2025-02-13 19:49:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:15][root][INFO] - Training Epoch: 1/2, step 6965/7134 completed (loss: 0.04260962828993797, acc: 1.0)
[2025-02-13 19:49:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:15][root][INFO] - Training Epoch: 1/2, step 6966/7134 completed (loss: 0.12099780887365341, acc: 0.9523809552192688)
[2025-02-13 19:49:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:16][root][INFO] - Training Epoch: 1/2, step 6967/7134 completed (loss: 0.07965903729200363, acc: 0.9795918464660645)
[2025-02-13 19:49:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:16][root][INFO] - Training Epoch: 1/2, step 6968/7134 completed (loss: 0.11971604079008102, acc: 0.9798657894134521)
[2025-02-13 19:49:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:16][root][INFO] - Training Epoch: 1/2, step 6969/7134 completed (loss: 0.2913607656955719, acc: 0.9274193644523621)
[2025-02-13 19:49:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:17][root][INFO] - Training Epoch: 1/2, step 6970/7134 completed (loss: 0.29366880655288696, acc: 0.9396551847457886)
[2025-02-13 19:49:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:17][root][INFO] - Training Epoch: 1/2, step 6971/7134 completed (loss: 0.29545941948890686, acc: 0.9236111044883728)
[2025-02-13 19:49:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:17][root][INFO] - Training Epoch: 1/2, step 6972/7134 completed (loss: 0.18358214199543, acc: 0.9534883499145508)
[2025-02-13 19:49:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:18][root][INFO] - Training Epoch: 1/2, step 6973/7134 completed (loss: 0.21628202497959137, acc: 0.982758641242981)
[2025-02-13 19:49:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:18][root][INFO] - Training Epoch: 1/2, step 6974/7134 completed (loss: 0.1872764676809311, acc: 0.9542483687400818)
[2025-02-13 19:49:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:19][root][INFO] - Training Epoch: 1/2, step 6975/7134 completed (loss: 0.20107820630073547, acc: 0.9487179517745972)
[2025-02-13 19:49:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:19][root][INFO] - Training Epoch: 1/2, step 6976/7134 completed (loss: 0.18216954171657562, acc: 0.9642857313156128)
[2025-02-13 19:49:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:19][root][INFO] - Training Epoch: 1/2, step 6977/7134 completed (loss: 0.24568213522434235, acc: 0.9236640930175781)
[2025-02-13 19:49:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:20][root][INFO] - Training Epoch: 1/2, step 6978/7134 completed (loss: 0.13582906126976013, acc: 0.9556962251663208)
[2025-02-13 19:49:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:20][root][INFO] - Training Epoch: 1/2, step 6979/7134 completed (loss: 0.21143318712711334, acc: 0.9470198750495911)
[2025-02-13 19:49:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:20][root][INFO] - Training Epoch: 1/2, step 6980/7134 completed (loss: 0.544774055480957, acc: 0.887005627155304)
[2025-02-13 19:49:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:21][root][INFO] - Training Epoch: 1/2, step 6981/7134 completed (loss: 0.3384867012500763, acc: 0.907216489315033)
[2025-02-13 19:49:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:21][root][INFO] - Training Epoch: 1/2, step 6982/7134 completed (loss: 0.3557206690311432, acc: 0.9319371581077576)
[2025-02-13 19:49:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:22][root][INFO] - Training Epoch: 1/2, step 6983/7134 completed (loss: 0.48613348603248596, acc: 0.8653846383094788)
[2025-02-13 19:49:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:22][root][INFO] - Training Epoch: 1/2, step 6984/7134 completed (loss: 0.23697492480278015, acc: 0.9069767594337463)
[2025-02-13 19:49:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:22][root][INFO] - Training Epoch: 1/2, step 6985/7134 completed (loss: 0.4039919972419739, acc: 0.9012875556945801)
[2025-02-13 19:49:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:23][root][INFO] - Training Epoch: 1/2, step 6986/7134 completed (loss: 0.2098115086555481, acc: 0.9320987462997437)
[2025-02-13 19:49:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:23][root][INFO] - Training Epoch: 1/2, step 6987/7134 completed (loss: 0.23996523022651672, acc: 0.93388432264328)
[2025-02-13 19:49:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:23][root][INFO] - Training Epoch: 1/2, step 6988/7134 completed (loss: 0.12402019649744034, acc: 0.9842519760131836)
[2025-02-13 19:49:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:24][root][INFO] - Training Epoch: 1/2, step 6989/7134 completed (loss: 0.4223124086856842, acc: 0.8918918967247009)
[2025-02-13 19:49:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:24][root][INFO] - Training Epoch: 1/2, step 6990/7134 completed (loss: 0.04425954446196556, acc: 0.9814814925193787)
[2025-02-13 19:49:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:25][root][INFO] - Training Epoch: 1/2, step 6991/7134 completed (loss: 0.4203238785266876, acc: 0.9108911156654358)
[2025-02-13 19:49:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:25][root][INFO] - Training Epoch: 1/2, step 6992/7134 completed (loss: 0.21514493227005005, acc: 0.9279279112815857)
[2025-02-13 19:49:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:25][root][INFO] - Training Epoch: 1/2, step 6993/7134 completed (loss: 0.3567463457584381, acc: 0.8917197585105896)
[2025-02-13 19:49:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:26][root][INFO] - Training Epoch: 1/2, step 6994/7134 completed (loss: 0.5180500149726868, acc: 0.8999999761581421)
[2025-02-13 19:49:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:26][root][INFO] - Training Epoch: 1/2, step 6995/7134 completed (loss: 0.150242879986763, acc: 0.9709302186965942)
[2025-02-13 19:49:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:26][root][INFO] - Training Epoch: 1/2, step 6996/7134 completed (loss: 0.19101710617542267, acc: 0.931506872177124)
[2025-02-13 19:49:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:27][root][INFO] - Training Epoch: 1/2, step 6997/7134 completed (loss: 0.1389404684305191, acc: 0.948387086391449)
[2025-02-13 19:49:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:27][root][INFO] - Training Epoch: 1/2, step 6998/7134 completed (loss: 0.348554402589798, acc: 0.9052631855010986)
[2025-02-13 19:49:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:28][root][INFO] - Training Epoch: 1/2, step 6999/7134 completed (loss: 0.07718753069639206, acc: 0.9774011373519897)
[2025-02-13 19:49:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:28][root][INFO] - Training Epoch: 1/2, step 7000/7134 completed (loss: 0.08787401020526886, acc: 0.9909090995788574)
[2025-02-13 19:49:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:28][root][INFO] - Training Epoch: 1/2, step 7001/7134 completed (loss: 0.3392013907432556, acc: 0.9610389471054077)
[2025-02-13 19:49:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:29][root][INFO] - Training Epoch: 1/2, step 7002/7134 completed (loss: 0.10897666215896606, acc: 0.9747899174690247)
[2025-02-13 19:49:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:29][root][INFO] - Training Epoch: 1/2, step 7003/7134 completed (loss: 0.06063346192240715, acc: 0.9948186278343201)
[2025-02-13 19:49:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:29][root][INFO] - Training Epoch: 1/2, step 7004/7134 completed (loss: 0.266256183385849, acc: 0.9329268336296082)
[2025-02-13 19:49:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:30][root][INFO] - Training Epoch: 1/2, step 7005/7134 completed (loss: 0.16966086626052856, acc: 0.9578313231468201)
[2025-02-13 19:49:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:30][root][INFO] - Training Epoch: 1/2, step 7006/7134 completed (loss: 0.10419365763664246, acc: 0.9689440727233887)
[2025-02-13 19:49:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:30][root][INFO] - Training Epoch: 1/2, step 7007/7134 completed (loss: 0.19095009565353394, acc: 0.9548386931419373)
[2025-02-13 19:49:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:31][root][INFO] - Training Epoch: 1/2, step 7008/7134 completed (loss: 0.1943221539258957, acc: 0.9779005646705627)
[2025-02-13 19:49:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:31][root][INFO] - Training Epoch: 1/2, step 7009/7134 completed (loss: 0.23050910234451294, acc: 0.953125)
[2025-02-13 19:49:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:32][root][INFO] - Training Epoch: 1/2, step 7010/7134 completed (loss: 0.07688603550195694, acc: 0.9806451797485352)
[2025-02-13 19:49:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:32][root][INFO] - Training Epoch: 1/2, step 7011/7134 completed (loss: 0.20116864144802094, acc: 0.9476439952850342)
[2025-02-13 19:49:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:32][root][INFO] - Training Epoch: 1/2, step 7012/7134 completed (loss: 0.08851350098848343, acc: 1.0)
[2025-02-13 19:49:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:33][root][INFO] - Training Epoch: 1/2, step 7013/7134 completed (loss: 0.06474421173334122, acc: 0.9808917045593262)
[2025-02-13 19:49:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:33][root][INFO] - Training Epoch: 1/2, step 7014/7134 completed (loss: 0.1684059351682663, acc: 0.9527027010917664)
[2025-02-13 19:49:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:33][root][INFO] - Training Epoch: 1/2, step 7015/7134 completed (loss: 0.10161659121513367, acc: 0.9776119589805603)
[2025-02-13 19:49:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:34][root][INFO] - Training Epoch: 1/2, step 7016/7134 completed (loss: 0.2762329876422882, acc: 0.9551281929016113)
[2025-02-13 19:49:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:34][root][INFO] - Training Epoch: 1/2, step 7017/7134 completed (loss: 0.22941787540912628, acc: 0.9380530714988708)
[2025-02-13 19:49:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:35][root][INFO] - Training Epoch: 1/2, step 7018/7134 completed (loss: 0.23952297866344452, acc: 0.9567901492118835)
[2025-02-13 19:49:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:35][root][INFO] - Training Epoch: 1/2, step 7019/7134 completed (loss: 0.42254093289375305, acc: 0.905063271522522)
[2025-02-13 19:49:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:35][root][INFO] - Training Epoch: 1/2, step 7020/7134 completed (loss: 0.1150866225361824, acc: 0.9819276928901672)
[2025-02-13 19:49:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:36][root][INFO] - Training Epoch: 1/2, step 7021/7134 completed (loss: 0.13124634325504303, acc: 0.9629629850387573)
[2025-02-13 19:49:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:36][root][INFO] - Training Epoch: 1/2, step 7022/7134 completed (loss: 0.3736710548400879, acc: 0.8611111044883728)
[2025-02-13 19:49:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:37][root][INFO] - Training Epoch: 1/2, step 7023/7134 completed (loss: 0.4421903192996979, acc: 0.9397590160369873)
[2025-02-13 19:49:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:37][root][INFO] - Training Epoch: 1/2, step 7024/7134 completed (loss: 0.3104090392589569, acc: 0.9420289993286133)
[2025-02-13 19:49:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:37][root][INFO] - Training Epoch: 1/2, step 7025/7134 completed (loss: 0.391683429479599, acc: 0.9142857193946838)
[2025-02-13 19:49:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:38][root][INFO] - Training Epoch: 1/2, step 7026/7134 completed (loss: 0.5455751419067383, acc: 0.859375)
[2025-02-13 19:49:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:38][root][INFO] - Training Epoch: 1/2, step 7027/7134 completed (loss: 0.3872511088848114, acc: 0.9154929518699646)
[2025-02-13 19:49:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:38][root][INFO] - Training Epoch: 1/2, step 7028/7134 completed (loss: 0.1025775671005249, acc: 0.9780219793319702)
[2025-02-13 19:49:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:39][root][INFO] - Training Epoch: 1/2, step 7029/7134 completed (loss: 0.16688378155231476, acc: 0.9550561904907227)
[2025-02-13 19:49:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:39][root][INFO] - Training Epoch: 1/2, step 7030/7134 completed (loss: 0.360650897026062, acc: 0.9130434989929199)
[2025-02-13 19:49:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:40][root][INFO] - Training Epoch: 1/2, step 7031/7134 completed (loss: 0.3750044107437134, acc: 0.9259259104728699)
[2025-02-13 19:49:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:40][root][INFO] - Training Epoch: 1/2, step 7032/7134 completed (loss: 0.44295889139175415, acc: 0.8970588445663452)
[2025-02-13 19:49:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:40][root][INFO] - Training Epoch: 1/2, step 7033/7134 completed (loss: 0.3802095651626587, acc: 0.9382715821266174)
[2025-02-13 19:49:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:41][root][INFO] - Training Epoch: 1/2, step 7034/7134 completed (loss: 0.36814871430397034, acc: 0.9375)
[2025-02-13 19:49:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:41][root][INFO] - Training Epoch: 1/2, step 7035/7134 completed (loss: 0.3657558560371399, acc: 0.9365079402923584)
[2025-02-13 19:49:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:41][root][INFO] - Training Epoch: 1/2, step 7036/7134 completed (loss: 0.3921761214733124, acc: 0.8730158805847168)
[2025-02-13 19:49:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:42][root][INFO] - Training Epoch: 1/2, step 7037/7134 completed (loss: 0.2926666736602783, acc: 0.9104477763175964)
[2025-02-13 19:49:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:42][root][INFO] - Training Epoch: 1/2, step 7038/7134 completed (loss: 0.37898239493370056, acc: 0.9367088675498962)
[2025-02-13 19:49:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:43][root][INFO] - Training Epoch: 1/2, step 7039/7134 completed (loss: 0.3397286832332611, acc: 0.9111111164093018)
[2025-02-13 19:49:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:43][root][INFO] - Training Epoch: 1/2, step 7040/7134 completed (loss: 0.460899293422699, acc: 0.8799999952316284)
[2025-02-13 19:49:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:43][root][INFO] - Training Epoch: 1/2, step 7041/7134 completed (loss: 0.38196685910224915, acc: 0.8999999761581421)
[2025-02-13 19:49:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:44][root][INFO] - Training Epoch: 1/2, step 7042/7134 completed (loss: 0.40887266397476196, acc: 0.9154929518699646)
[2025-02-13 19:49:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:44][root][INFO] - Training Epoch: 1/2, step 7043/7134 completed (loss: 0.4384081959724426, acc: 0.9333333373069763)
[2025-02-13 19:49:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:45][root][INFO] - Training Epoch: 1/2, step 7044/7134 completed (loss: 0.37346357107162476, acc: 0.8873239159584045)
[2025-02-13 19:49:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:45][root][INFO] - Training Epoch: 1/2, step 7045/7134 completed (loss: 0.2710838317871094, acc: 0.9241379499435425)
[2025-02-13 19:49:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:45][root][INFO] - Training Epoch: 1/2, step 7046/7134 completed (loss: 0.38819077610969543, acc: 0.910179615020752)
[2025-02-13 19:49:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:46][root][INFO] - Training Epoch: 1/2, step 7047/7134 completed (loss: 0.31657177209854126, acc: 0.9248826503753662)
[2025-02-13 19:49:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:46][root][INFO] - Training Epoch: 1/2, step 7048/7134 completed (loss: 0.04192887246608734, acc: 0.9896907210350037)
[2025-02-13 19:49:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:46][root][INFO] - Training Epoch: 1/2, step 7049/7134 completed (loss: 0.16201837360858917, acc: 0.9537572264671326)
[2025-02-13 19:49:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:47][root][INFO] - Training Epoch: 1/2, step 7050/7134 completed (loss: 0.08279497176408768, acc: 0.9885714054107666)
[2025-02-13 19:49:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:47][root][INFO] - Training Epoch: 1/2, step 7051/7134 completed (loss: 0.09384920448064804, acc: 0.9619565010070801)
[2025-02-13 19:49:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:48][root][INFO] - Training Epoch: 1/2, step 7052/7134 completed (loss: 0.1220521554350853, acc: 0.9768785834312439)
[2025-02-13 19:49:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:48][root][INFO] - Training Epoch: 1/2, step 7053/7134 completed (loss: 0.10611694306135178, acc: 0.9781420826911926)
[2025-02-13 19:49:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:48][root][INFO] - Training Epoch: 1/2, step 7054/7134 completed (loss: 0.26106712222099304, acc: 0.948387086391449)
[2025-02-13 19:49:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:49][root][INFO] - Training Epoch: 1/2, step 7055/7134 completed (loss: 0.26587915420532227, acc: 0.9395604133605957)
[2025-02-13 19:49:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:49][root][INFO] - Training Epoch: 1/2, step 7056/7134 completed (loss: 0.2128443866968155, acc: 0.9388889074325562)
[2025-02-13 19:49:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:50][root][INFO] - Training Epoch: 1/2, step 7057/7134 completed (loss: 0.5849291086196899, acc: 0.9060773253440857)
[2025-02-13 19:49:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:50][root][INFO] - Training Epoch: 1/2, step 7058/7134 completed (loss: 0.2544845640659332, acc: 0.9534883499145508)
[2025-02-13 19:49:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:50][root][INFO] - Training Epoch: 1/2, step 7059/7134 completed (loss: 0.13325081765651703, acc: 0.9551281929016113)
[2025-02-13 19:49:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:51][root][INFO] - Training Epoch: 1/2, step 7060/7134 completed (loss: 0.1891852170228958, acc: 0.9718309640884399)
[2025-02-13 19:49:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:51][root][INFO] - Training Epoch: 1/2, step 7061/7134 completed (loss: 0.2556397318840027, acc: 0.9247311949729919)
[2025-02-13 19:49:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:52][root][INFO] - Training Epoch: 1/2, step 7062/7134 completed (loss: 0.2095697820186615, acc: 0.9491525292396545)
[2025-02-13 19:49:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:52][root][INFO] - Training Epoch: 1/2, step 7063/7134 completed (loss: 0.4205218255519867, acc: 0.9351351261138916)
[2025-02-13 19:49:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:52][root][INFO] - Training Epoch: 1/2, step 7064/7134 completed (loss: 0.24120284616947174, acc: 0.9638554453849792)
[2025-02-13 19:49:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:53][root][INFO] - Training Epoch: 1/2, step 7065/7134 completed (loss: 0.08496041595935822, acc: 0.9898989796638489)
[2025-02-13 19:49:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:53][root][INFO] - Training Epoch: 1/2, step 7066/7134 completed (loss: 0.15773780643939972, acc: 0.963350772857666)
[2025-02-13 19:49:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:53][root][INFO] - Training Epoch: 1/2, step 7067/7134 completed (loss: 0.11317317932844162, acc: 0.9629629850387573)
[2025-02-13 19:49:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:54][root][INFO] - Training Epoch: 1/2, step 7068/7134 completed (loss: 0.04871527478098869, acc: 0.9902439117431641)
[2025-02-13 19:49:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:54][root][INFO] - Training Epoch: 1/2, step 7069/7134 completed (loss: 0.09051340818405151, acc: 0.9826839566230774)
[2025-02-13 19:49:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:55][root][INFO] - Training Epoch: 1/2, step 7070/7134 completed (loss: 0.10521294176578522, acc: 0.9768785834312439)
[2025-02-13 19:49:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:55][root][INFO] - Training Epoch: 1/2, step 7071/7134 completed (loss: 0.18082889914512634, acc: 0.9747899174690247)
[2025-02-13 19:49:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:55][root][INFO] - Training Epoch: 1/2, step 7072/7134 completed (loss: 0.11110421270132065, acc: 0.9646464586257935)
[2025-02-13 19:49:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:56][root][INFO] - Training Epoch: 1/2, step 7073/7134 completed (loss: 0.12975259125232697, acc: 0.9648241400718689)
[2025-02-13 19:49:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:56][root][INFO] - Training Epoch: 1/2, step 7074/7134 completed (loss: 0.525359034538269, acc: 0.8780487775802612)
[2025-02-13 19:49:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:57][root][INFO] - Training Epoch: 1/2, step 7075/7134 completed (loss: 0.2702906131744385, acc: 0.9299362897872925)
[2025-02-13 19:49:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:57][root][INFO] - Training Epoch: 1/2, step 7076/7134 completed (loss: 0.388490229845047, acc: 0.932584285736084)
[2025-02-13 19:49:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:57][root][INFO] - Training Epoch: 1/2, step 7077/7134 completed (loss: 0.3961867094039917, acc: 0.8904109597206116)
[2025-02-13 19:49:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:58][root][INFO] - Training Epoch: 1/2, step 7078/7134 completed (loss: 0.21290989220142365, acc: 0.9448275566101074)
[2025-02-13 19:49:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:58][root][INFO] - Training Epoch: 1/2, step 7079/7134 completed (loss: 0.13628613948822021, acc: 0.9605262875556946)
[2025-02-13 19:49:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:58][root][INFO] - Training Epoch: 1/2, step 7080/7134 completed (loss: 0.21697084605693817, acc: 0.9455782175064087)
[2025-02-13 19:49:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:59][root][INFO] - Training Epoch: 1/2, step 7081/7134 completed (loss: 0.20260679721832275, acc: 0.9653179049491882)
[2025-02-13 19:49:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:49:59][root][INFO] - Training Epoch: 1/2, step 7082/7134 completed (loss: 0.2737761437892914, acc: 0.9407894611358643)
[2025-02-13 19:49:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:00][root][INFO] - Training Epoch: 1/2, step 7083/7134 completed (loss: 0.11501985788345337, acc: 0.9716312289237976)
[2025-02-13 19:50:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:00][root][INFO] - Training Epoch: 1/2, step 7084/7134 completed (loss: 0.1816360056400299, acc: 0.954023003578186)
[2025-02-13 19:50:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:00][root][INFO] - Training Epoch: 1/2, step 7085/7134 completed (loss: 0.2121652364730835, acc: 0.95333331823349)
[2025-02-13 19:50:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:01][root][INFO] - Training Epoch: 1/2, step 7086/7134 completed (loss: 0.15084971487522125, acc: 0.9548386931419373)
[2025-02-13 19:50:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:01][root][INFO] - Training Epoch: 1/2, step 7087/7134 completed (loss: 0.18076816201210022, acc: 0.9844961166381836)
[2025-02-13 19:50:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:02][root][INFO] - Training Epoch: 1/2, step 7088/7134 completed (loss: 0.17456288635730743, acc: 0.9432623982429504)
[2025-02-13 19:50:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:02][root][INFO] - Training Epoch: 1/2, step 7089/7134 completed (loss: 0.27987074851989746, acc: 0.9290322661399841)
[2025-02-13 19:50:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:02][root][INFO] - Training Epoch: 1/2, step 7090/7134 completed (loss: 0.18407809734344482, acc: 0.9702380895614624)
[2025-02-13 19:50:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:03][root][INFO] - Training Epoch: 1/2, step 7091/7134 completed (loss: 0.11214295774698257, acc: 0.9696969985961914)
[2025-02-13 19:50:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:03][root][INFO] - Training Epoch: 1/2, step 7092/7134 completed (loss: 0.17646969854831696, acc: 0.969924807548523)
[2025-02-13 19:50:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:03][root][INFO] - Training Epoch: 1/2, step 7093/7134 completed (loss: 0.2230236977338791, acc: 0.94017094373703)
[2025-02-13 19:50:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:04][root][INFO] - Training Epoch: 1/2, step 7094/7134 completed (loss: 0.2091054916381836, acc: 0.9583333134651184)
[2025-02-13 19:50:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:04][root][INFO] - Training Epoch: 1/2, step 7095/7134 completed (loss: 0.2842733860015869, acc: 0.9322034120559692)
[2025-02-13 19:50:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:05][root][INFO] - Training Epoch: 1/2, step 7096/7134 completed (loss: 0.20585715770721436, acc: 0.9436619877815247)
[2025-02-13 19:50:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:05][root][INFO] - Training Epoch: 1/2, step 7097/7134 completed (loss: 0.3200252950191498, acc: 0.8974359035491943)
[2025-02-13 19:50:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:05][root][INFO] - Training Epoch: 1/2, step 7098/7134 completed (loss: 0.12645375728607178, acc: 0.9790209531784058)
[2025-02-13 19:50:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:06][root][INFO] - Training Epoch: 1/2, step 7099/7134 completed (loss: 0.30032268166542053, acc: 0.9130434989929199)
[2025-02-13 19:50:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:06][root][INFO] - Training Epoch: 1/2, step 7100/7134 completed (loss: 0.22535623610019684, acc: 0.946107804775238)
[2025-02-13 19:50:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:06][root][INFO] - Training Epoch: 1/2, step 7101/7134 completed (loss: 0.2370714694261551, acc: 0.9424460530281067)
[2025-02-13 19:50:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:07][root][INFO] - Training Epoch: 1/2, step 7102/7134 completed (loss: 0.17536480724811554, acc: 0.9576271176338196)
[2025-02-13 19:50:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:07][root][INFO] - Training Epoch: 1/2, step 7103/7134 completed (loss: 0.22957128286361694, acc: 0.921875)
[2025-02-13 19:50:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:08][root][INFO] - Training Epoch: 1/2, step 7104/7134 completed (loss: 0.15067730844020844, acc: 0.9561403393745422)
[2025-02-13 19:50:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:08][root][INFO] - Training Epoch: 1/2, step 7105/7134 completed (loss: 0.11692118644714355, acc: 0.9652777910232544)
[2025-02-13 19:50:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:08][root][INFO] - Training Epoch: 1/2, step 7106/7134 completed (loss: 0.08941614627838135, acc: 0.9851852059364319)
[2025-02-13 19:50:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:09][root][INFO] - Training Epoch: 1/2, step 7107/7134 completed (loss: 0.13175180554389954, acc: 0.95652174949646)
[2025-02-13 19:50:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:09][root][INFO] - Training Epoch: 1/2, step 7108/7134 completed (loss: 0.15223659574985504, acc: 0.9635036587715149)
[2025-02-13 19:50:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:09][root][INFO] - Training Epoch: 1/2, step 7109/7134 completed (loss: 0.24849502742290497, acc: 0.9185185432434082)
[2025-02-13 19:50:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:10][root][INFO] - Training Epoch: 1/2, step 7110/7134 completed (loss: 0.06629382818937302, acc: 0.9849624037742615)
[2025-02-13 19:50:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:10][root][INFO] - Training Epoch: 1/2, step 7111/7134 completed (loss: 0.2208349108695984, acc: 0.9537037014961243)
[2025-02-13 19:50:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:11][root][INFO] - Training Epoch: 1/2, step 7112/7134 completed (loss: 0.23046739399433136, acc: 0.9279279112815857)
[2025-02-13 19:50:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:11][root][INFO] - Training Epoch: 1/2, step 7113/7134 completed (loss: 0.41129326820373535, acc: 0.9044585824012756)
[2025-02-13 19:50:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:11][root][INFO] - Training Epoch: 1/2, step 7114/7134 completed (loss: 0.286990761756897, acc: 0.9266055226325989)
[2025-02-13 19:50:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:12][root][INFO] - Training Epoch: 1/2, step 7115/7134 completed (loss: 0.27760377526283264, acc: 0.9465649127960205)
[2025-02-13 19:50:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:12][root][INFO] - Training Epoch: 1/2, step 7116/7134 completed (loss: 0.2854044437408447, acc: 0.9411764740943909)
[2025-02-13 19:50:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:12][root][INFO] - Training Epoch: 1/2, step 7117/7134 completed (loss: 0.283679723739624, acc: 0.913385808467865)
[2025-02-13 19:50:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:13][root][INFO] - Training Epoch: 1/2, step 7118/7134 completed (loss: 0.22796566784381866, acc: 0.9357143044471741)
[2025-02-13 19:50:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:13][root][INFO] - Training Epoch: 1/2, step 7119/7134 completed (loss: 0.09680032730102539, acc: 0.9793103337287903)
[2025-02-13 19:50:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:14][root][INFO] - Training Epoch: 1/2, step 7120/7134 completed (loss: 0.28536197543144226, acc: 0.9514563083648682)
[2025-02-13 19:50:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:14][root][INFO] - Training Epoch: 1/2, step 7121/7134 completed (loss: 0.030843881890177727, acc: 1.0)
[2025-02-13 19:50:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:14][root][INFO] - Training Epoch: 1/2, step 7122/7134 completed (loss: 0.23033148050308228, acc: 0.9569892287254333)
[2025-02-13 19:50:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:15][root][INFO] - Training Epoch: 1/2, step 7123/7134 completed (loss: 0.22748826444149017, acc: 0.9494949579238892)
[2025-02-13 19:50:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:15][root][INFO] - Training Epoch: 1/2, step 7124/7134 completed (loss: 0.1985911875963211, acc: 0.9590163826942444)
[2025-02-13 19:50:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:15][root][INFO] - Training Epoch: 1/2, step 7125/7134 completed (loss: 0.23869316279888153, acc: 0.9541984796524048)
[2025-02-13 19:50:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:16][root][INFO] - Training Epoch: 1/2, step 7126/7134 completed (loss: 0.3525595963001251, acc: 0.9124087691307068)
[2025-02-13 19:50:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:16][root][INFO] - Training Epoch: 1/2, step 7127/7134 completed (loss: 0.14538060128688812, acc: 0.9506173133850098)
[2025-02-13 19:50:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:17][root][INFO] - Training Epoch: 1/2, step 7128/7134 completed (loss: 0.1658875197172165, acc: 0.965753436088562)
[2025-02-13 19:50:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:17][root][INFO] - Training Epoch: 1/2, step 7129/7134 completed (loss: 0.67940753698349, acc: 0.8372092843055725)
[2025-02-13 19:50:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:17][root][INFO] - Training Epoch: 1/2, step 7130/7134 completed (loss: 0.7539271116256714, acc: 0.8266666531562805)
[2025-02-13 19:50:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:18][root][INFO] - Training Epoch: 1/2, step 7131/7134 completed (loss: 0.25535619258880615, acc: 0.9339622855186462)
[2025-02-13 19:50:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:50:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:51:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:52:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:53:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:21][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.3100, device='cuda:0') eval_epoch_loss=tensor(0.2700, device='cuda:0') eval_epoch_acc=tensor(0.9410, device='cuda:0')
[2025-02-13 19:54:21][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-13 19:54:21][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-13 19:54:21][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_wavlm_llama32_1b_linear_peft_transfer_librispeech-100/asr_epoch_1_step_7132_loss_0.27001067996025085/model.pt
[2025-02-13 19:54:21][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_wavlm_llama32_1b_linear_peft_transfer_librispeech-100 directory
[2025-02-13 19:54:21][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 1 is 0.27001067996025085
[2025-02-13 19:54:21][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 1 is 0.9410226941108704
[2025-02-13 19:54:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:22][root][INFO] - Training Epoch: 1/2, step 7132/7134 completed (loss: 0.11102339625358582, acc: 0.9558823704719543)
[2025-02-13 19:54:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:22][root][INFO] - Training Epoch: 1/2, step 7133/7134 completed (loss: 0.2014123797416687, acc: 0.9624060392379761)
[2025-02-13 19:54:22][slam_llm.utils.train_utils][INFO] - Epoch 1: train_perplexity=1.3516, train_epoch_loss=0.3013, epoch time 3798.5576279982924s
[2025-02-13 19:54:22][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 12 GB
[2025-02-13 19:54:22][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 21 GB
[2025-02-13 19:54:22][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 12 GB
[2025-02-13 19:54:22][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 0
[2025-02-13 19:54:22][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 7 GB
[2025-02-13 19:54:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:24][root][INFO] - Training Epoch: 2/2, step 0/7134 completed (loss: 0.42045846581459045, acc: 0.9230769276618958)
[2025-02-13 19:54:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:24][root][INFO] - Training Epoch: 2/2, step 1/7134 completed (loss: 0.17341351509094238, acc: 0.9681528806686401)
[2025-02-13 19:54:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:24][root][INFO] - Training Epoch: 2/2, step 2/7134 completed (loss: 0.11855478584766388, acc: 0.9772727489471436)
[2025-02-13 19:54:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:25][root][INFO] - Training Epoch: 2/2, step 3/7134 completed (loss: 0.10787203907966614, acc: 0.9593023061752319)
[2025-02-13 19:54:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:25][root][INFO] - Training Epoch: 2/2, step 4/7134 completed (loss: 0.15587562322616577, acc: 0.955974817276001)
[2025-02-13 19:54:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:26][root][INFO] - Training Epoch: 2/2, step 5/7134 completed (loss: 0.07715434581041336, acc: 0.9832402467727661)
[2025-02-13 19:54:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:26][root][INFO] - Training Epoch: 2/2, step 6/7134 completed (loss: 0.021950161084532738, acc: 1.0)
[2025-02-13 19:54:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:26][root][INFO] - Training Epoch: 2/2, step 7/7134 completed (loss: 0.08020659536123276, acc: 0.9784946441650391)
[2025-02-13 19:54:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:27][root][INFO] - Training Epoch: 2/2, step 8/7134 completed (loss: 0.1885693520307541, acc: 0.957317054271698)
[2025-02-13 19:54:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:27][root][INFO] - Training Epoch: 2/2, step 9/7134 completed (loss: 0.06169845163822174, acc: 0.9801324605941772)
[2025-02-13 19:54:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:28][root][INFO] - Training Epoch: 2/2, step 10/7134 completed (loss: 0.24802859127521515, acc: 0.9171597361564636)
[2025-02-13 19:54:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:28][root][INFO] - Training Epoch: 2/2, step 11/7134 completed (loss: 0.10032636672258377, acc: 0.9750000238418579)
[2025-02-13 19:54:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:29][root][INFO] - Training Epoch: 2/2, step 12/7134 completed (loss: 0.0663817822933197, acc: 0.9942196607589722)
[2025-02-13 19:54:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:29][root][INFO] - Training Epoch: 2/2, step 13/7134 completed (loss: 0.1386018991470337, acc: 0.949438214302063)
[2025-02-13 19:54:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:29][root][INFO] - Training Epoch: 2/2, step 14/7134 completed (loss: 0.02631833776831627, acc: 1.0)
[2025-02-13 19:54:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:30][root][INFO] - Training Epoch: 2/2, step 15/7134 completed (loss: 0.041635435074567795, acc: 0.9927536249160767)
[2025-02-13 19:54:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:30][root][INFO] - Training Epoch: 2/2, step 16/7134 completed (loss: 0.04778539016842842, acc: 0.9912280440330505)
[2025-02-13 19:54:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:30][root][INFO] - Training Epoch: 2/2, step 17/7134 completed (loss: 0.2406567484140396, acc: 0.9468085169792175)
[2025-02-13 19:54:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:31][root][INFO] - Training Epoch: 2/2, step 18/7134 completed (loss: 0.016555676236748695, acc: 1.0)
[2025-02-13 19:54:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:31][root][INFO] - Training Epoch: 2/2, step 19/7134 completed (loss: 0.08366432785987854, acc: 0.9653179049491882)
[2025-02-13 19:54:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:32][root][INFO] - Training Epoch: 2/2, step 20/7134 completed (loss: 0.03924392908811569, acc: 0.9943181872367859)
[2025-02-13 19:54:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:32][root][INFO] - Training Epoch: 2/2, step 21/7134 completed (loss: 0.08962980657815933, acc: 0.9772727489471436)
[2025-02-13 19:54:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:32][root][INFO] - Training Epoch: 2/2, step 22/7134 completed (loss: 0.005895962007343769, acc: 1.0)
[2025-02-13 19:54:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:33][root][INFO] - Training Epoch: 2/2, step 23/7134 completed (loss: 0.022851327434182167, acc: 1.0)
[2025-02-13 19:54:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:33][root][INFO] - Training Epoch: 2/2, step 24/7134 completed (loss: 0.03090597875416279, acc: 0.9944444298744202)
[2025-02-13 19:54:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:34][root][INFO] - Training Epoch: 2/2, step 25/7134 completed (loss: 0.10244466364383698, acc: 0.976331353187561)
[2025-02-13 19:54:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:34][root][INFO] - Training Epoch: 2/2, step 26/7134 completed (loss: 0.16997800767421722, acc: 0.9570552110671997)
[2025-02-13 19:54:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:34][root][INFO] - Training Epoch: 2/2, step 27/7134 completed (loss: 0.02725272625684738, acc: 0.9935897588729858)
[2025-02-13 19:54:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:35][root][INFO] - Training Epoch: 2/2, step 28/7134 completed (loss: 0.2962417006492615, acc: 0.9340101480484009)
[2025-02-13 19:54:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:35][root][INFO] - Training Epoch: 2/2, step 29/7134 completed (loss: 0.1530340015888214, acc: 0.9822485446929932)
[2025-02-13 19:54:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:36][root][INFO] - Training Epoch: 2/2, step 30/7134 completed (loss: 0.1517627090215683, acc: 0.9710144996643066)
[2025-02-13 19:54:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:36][root][INFO] - Training Epoch: 2/2, step 31/7134 completed (loss: 0.18311373889446259, acc: 0.9627906680107117)
[2025-02-13 19:54:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:36][root][INFO] - Training Epoch: 2/2, step 32/7134 completed (loss: 0.2589147686958313, acc: 0.945652186870575)
[2025-02-13 19:54:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:37][root][INFO] - Training Epoch: 2/2, step 33/7134 completed (loss: 0.2641151547431946, acc: 0.9461538195610046)
[2025-02-13 19:54:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:37][root][INFO] - Training Epoch: 2/2, step 34/7134 completed (loss: 0.1926598846912384, acc: 0.961904764175415)
[2025-02-13 19:54:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:38][root][INFO] - Training Epoch: 2/2, step 35/7134 completed (loss: 0.1597607582807541, acc: 0.95686274766922)
[2025-02-13 19:54:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:38][root][INFO] - Training Epoch: 2/2, step 36/7134 completed (loss: 0.20369189977645874, acc: 0.9677419066429138)
[2025-02-13 19:54:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:38][root][INFO] - Training Epoch: 2/2, step 37/7134 completed (loss: 0.17985717952251434, acc: 0.9576719403266907)
[2025-02-13 19:54:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:39][root][INFO] - Training Epoch: 2/2, step 38/7134 completed (loss: 0.16128285229206085, acc: 0.9622641801834106)
[2025-02-13 19:54:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:39][root][INFO] - Training Epoch: 2/2, step 39/7134 completed (loss: 0.07400552928447723, acc: 0.9766082167625427)
[2025-02-13 19:54:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:40][root][INFO] - Training Epoch: 2/2, step 40/7134 completed (loss: 0.15795503556728363, acc: 0.9821428656578064)
[2025-02-13 19:54:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:40][root][INFO] - Training Epoch: 2/2, step 41/7134 completed (loss: 0.18996456265449524, acc: 0.9734042286872864)
[2025-02-13 19:54:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:40][root][INFO] - Training Epoch: 2/2, step 42/7134 completed (loss: 0.10333026945590973, acc: 0.9753086566925049)
[2025-02-13 19:54:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:41][root][INFO] - Training Epoch: 2/2, step 43/7134 completed (loss: 0.05890282988548279, acc: 0.9888268113136292)
[2025-02-13 19:54:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:41][root][INFO] - Training Epoch: 2/2, step 44/7134 completed (loss: 0.11739103496074677, acc: 0.9786096215248108)
[2025-02-13 19:54:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:41][root][INFO] - Training Epoch: 2/2, step 45/7134 completed (loss: 0.157060906291008, acc: 0.9669811129570007)
[2025-02-13 19:54:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:42][root][INFO] - Training Epoch: 2/2, step 46/7134 completed (loss: 0.16641072928905487, acc: 0.9534883499145508)
[2025-02-13 19:54:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:42][root][INFO] - Training Epoch: 2/2, step 47/7134 completed (loss: 0.1324767917394638, acc: 0.9720670580863953)
[2025-02-13 19:54:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:43][root][INFO] - Training Epoch: 2/2, step 48/7134 completed (loss: 0.0920271947979927, acc: 0.9855072498321533)
[2025-02-13 19:54:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:43][root][INFO] - Training Epoch: 2/2, step 49/7134 completed (loss: 0.10465452820062637, acc: 0.970588207244873)
[2025-02-13 19:54:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:43][root][INFO] - Training Epoch: 2/2, step 50/7134 completed (loss: 0.12023552507162094, acc: 0.9708737730979919)
[2025-02-13 19:54:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:44][root][INFO] - Training Epoch: 2/2, step 51/7134 completed (loss: 0.09069894999265671, acc: 0.9744898080825806)
[2025-02-13 19:54:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:44][root][INFO] - Training Epoch: 2/2, step 52/7134 completed (loss: 0.07697410136461258, acc: 0.9735099077224731)
[2025-02-13 19:54:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:45][root][INFO] - Training Epoch: 2/2, step 53/7134 completed (loss: 0.08714167773723602, acc: 0.9891892075538635)
[2025-02-13 19:54:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:45][root][INFO] - Training Epoch: 2/2, step 54/7134 completed (loss: 0.042686980217695236, acc: 0.9952380657196045)
[2025-02-13 19:54:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:45][root][INFO] - Training Epoch: 2/2, step 55/7134 completed (loss: 0.17597457766532898, acc: 0.945652186870575)
[2025-02-13 19:54:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:46][root][INFO] - Training Epoch: 2/2, step 56/7134 completed (loss: 0.05869856849312782, acc: 0.9846153855323792)
[2025-02-13 19:54:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:46][root][INFO] - Training Epoch: 2/2, step 57/7134 completed (loss: 0.2928500473499298, acc: 0.9230769276618958)
[2025-02-13 19:54:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:46][root][INFO] - Training Epoch: 2/2, step 58/7134 completed (loss: 0.23311690986156464, acc: 0.9441340565681458)
[2025-02-13 19:54:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:47][root][INFO] - Training Epoch: 2/2, step 59/7134 completed (loss: 0.07661867141723633, acc: 0.9839572310447693)
[2025-02-13 19:54:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:47][root][INFO] - Training Epoch: 2/2, step 60/7134 completed (loss: 0.15125644207000732, acc: 0.949367105960846)
[2025-02-13 19:54:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:48][root][INFO] - Training Epoch: 2/2, step 61/7134 completed (loss: 0.12216787785291672, acc: 0.9720670580863953)
[2025-02-13 19:54:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:48][root][INFO] - Training Epoch: 2/2, step 62/7134 completed (loss: 0.18768194317817688, acc: 0.9606741666793823)
[2025-02-13 19:54:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:48][root][INFO] - Training Epoch: 2/2, step 63/7134 completed (loss: 0.2593581974506378, acc: 0.9395604133605957)
[2025-02-13 19:54:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:49][root][INFO] - Training Epoch: 2/2, step 64/7134 completed (loss: 0.22771583497524261, acc: 0.9411764740943909)
[2025-02-13 19:54:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:49][root][INFO] - Training Epoch: 2/2, step 65/7134 completed (loss: 0.2671048939228058, acc: 0.936170220375061)
[2025-02-13 19:54:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:50][root][INFO] - Training Epoch: 2/2, step 66/7134 completed (loss: 0.1499919295310974, acc: 0.9621211886405945)
[2025-02-13 19:54:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:50][root][INFO] - Training Epoch: 2/2, step 67/7134 completed (loss: 0.26551002264022827, acc: 0.931034505367279)
[2025-02-13 19:54:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:50][root][INFO] - Training Epoch: 2/2, step 68/7134 completed (loss: 0.25431811809539795, acc: 0.9269663095474243)
[2025-02-13 19:54:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:51][root][INFO] - Training Epoch: 2/2, step 69/7134 completed (loss: 0.19398309290409088, acc: 0.9470899701118469)
[2025-02-13 19:54:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:51][root][INFO] - Training Epoch: 2/2, step 70/7134 completed (loss: 0.18240000307559967, acc: 0.953125)
[2025-02-13 19:54:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:51][root][INFO] - Training Epoch: 2/2, step 71/7134 completed (loss: 0.15969012677669525, acc: 0.9454545378684998)
[2025-02-13 19:54:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:52][root][INFO] - Training Epoch: 2/2, step 72/7134 completed (loss: 0.08366702497005463, acc: 0.976331353187561)
[2025-02-13 19:54:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:52][root][INFO] - Training Epoch: 2/2, step 73/7134 completed (loss: 0.13358202576637268, acc: 0.9710982441902161)
[2025-02-13 19:54:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:53][root][INFO] - Training Epoch: 2/2, step 74/7134 completed (loss: 0.11634030193090439, acc: 0.9709302186965942)
[2025-02-13 19:54:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:53][root][INFO] - Training Epoch: 2/2, step 75/7134 completed (loss: 0.27027183771133423, acc: 0.9583333134651184)
[2025-02-13 19:54:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:53][root][INFO] - Training Epoch: 2/2, step 76/7134 completed (loss: 0.13316987454891205, acc: 0.97826087474823)
[2025-02-13 19:54:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:54][root][INFO] - Training Epoch: 2/2, step 77/7134 completed (loss: 0.17651031911373138, acc: 0.9668508172035217)
[2025-02-13 19:54:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:54][root][INFO] - Training Epoch: 2/2, step 78/7134 completed (loss: 0.2622409462928772, acc: 0.9337748289108276)
[2025-02-13 19:54:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:55][root][INFO] - Training Epoch: 2/2, step 79/7134 completed (loss: 0.29811206459999084, acc: 0.9333333373069763)
[2025-02-13 19:54:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:55][root][INFO] - Training Epoch: 2/2, step 80/7134 completed (loss: 0.1256784200668335, acc: 0.9775280952453613)
[2025-02-13 19:54:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:55][root][INFO] - Training Epoch: 2/2, step 81/7134 completed (loss: 0.15901020169258118, acc: 0.9682539701461792)
[2025-02-13 19:54:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:56][root][INFO] - Training Epoch: 2/2, step 82/7134 completed (loss: 0.21190324425697327, acc: 0.9518072009086609)
[2025-02-13 19:54:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:56][root][INFO] - Training Epoch: 2/2, step 83/7134 completed (loss: 0.12628190219402313, acc: 0.978723406791687)
[2025-02-13 19:54:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:57][root][INFO] - Training Epoch: 2/2, step 84/7134 completed (loss: 0.18832392990589142, acc: 0.9424460530281067)
[2025-02-13 19:54:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:57][root][INFO] - Training Epoch: 2/2, step 85/7134 completed (loss: 0.25689589977264404, acc: 0.9399999976158142)
[2025-02-13 19:54:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:57][root][INFO] - Training Epoch: 2/2, step 86/7134 completed (loss: 0.24157556891441345, acc: 0.9285714030265808)
[2025-02-13 19:54:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:58][root][INFO] - Training Epoch: 2/2, step 87/7134 completed (loss: 0.3093867599964142, acc: 0.9220778942108154)
[2025-02-13 19:54:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:58][root][INFO] - Training Epoch: 2/2, step 88/7134 completed (loss: 0.24066157639026642, acc: 0.9457831382751465)
[2025-02-13 19:54:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:59][root][INFO] - Training Epoch: 2/2, step 89/7134 completed (loss: 0.17779329419136047, acc: 0.9657142758369446)
[2025-02-13 19:54:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:59][root][INFO] - Training Epoch: 2/2, step 90/7134 completed (loss: 0.12926003336906433, acc: 0.9663865566253662)
[2025-02-13 19:54:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:54:59][root][INFO] - Training Epoch: 2/2, step 91/7134 completed (loss: 0.38683614134788513, acc: 0.9173553586006165)
[2025-02-13 19:54:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:00][root][INFO] - Training Epoch: 2/2, step 92/7134 completed (loss: 0.14082913100719452, acc: 0.9675324559211731)
[2025-02-13 19:55:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:00][root][INFO] - Training Epoch: 2/2, step 93/7134 completed (loss: 0.24749656021595, acc: 0.9485714435577393)
[2025-02-13 19:55:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:01][root][INFO] - Training Epoch: 2/2, step 94/7134 completed (loss: 0.3767452836036682, acc: 0.9245283007621765)
[2025-02-13 19:55:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:01][root][INFO] - Training Epoch: 2/2, step 95/7134 completed (loss: 0.1361001580953598, acc: 0.9695122241973877)
[2025-02-13 19:55:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:01][root][INFO] - Training Epoch: 2/2, step 96/7134 completed (loss: 0.07320433855056763, acc: 0.9906542301177979)
[2025-02-13 19:55:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:02][root][INFO] - Training Epoch: 2/2, step 97/7134 completed (loss: 0.8676871061325073, acc: 0.8095238208770752)
[2025-02-13 19:55:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:02][root][INFO] - Training Epoch: 2/2, step 98/7134 completed (loss: 0.2778501808643341, acc: 0.922535240650177)
[2025-02-13 19:55:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:02][root][INFO] - Training Epoch: 2/2, step 99/7134 completed (loss: 0.2143559753894806, acc: 0.9384615421295166)
[2025-02-13 19:55:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:03][root][INFO] - Training Epoch: 2/2, step 100/7134 completed (loss: 0.32690325379371643, acc: 0.9337016344070435)
[2025-02-13 19:55:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:03][root][INFO] - Training Epoch: 2/2, step 101/7134 completed (loss: 0.19416776299476624, acc: 0.953125)
[2025-02-13 19:55:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:04][root][INFO] - Training Epoch: 2/2, step 102/7134 completed (loss: 0.10078302025794983, acc: 0.9726775884628296)
[2025-02-13 19:55:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:04][root][INFO] - Training Epoch: 2/2, step 103/7134 completed (loss: 0.21939638257026672, acc: 0.9520547986030579)
[2025-02-13 19:55:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:05][root][INFO] - Training Epoch: 2/2, step 104/7134 completed (loss: 0.09886030852794647, acc: 0.9733333587646484)
[2025-02-13 19:55:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:05][root][INFO] - Training Epoch: 2/2, step 105/7134 completed (loss: 0.1190405860543251, acc: 0.9776119589805603)
[2025-02-13 19:55:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:05][root][INFO] - Training Epoch: 2/2, step 106/7134 completed (loss: 0.1031893864274025, acc: 0.9751552939414978)
[2025-02-13 19:55:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:06][root][INFO] - Training Epoch: 2/2, step 107/7134 completed (loss: 0.561623752117157, acc: 0.8662790656089783)
[2025-02-13 19:55:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:06][root][INFO] - Training Epoch: 2/2, step 108/7134 completed (loss: 0.2108810991048813, acc: 0.9621621370315552)
[2025-02-13 19:55:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:06][root][INFO] - Training Epoch: 2/2, step 109/7134 completed (loss: 0.4106570780277252, acc: 0.9126983880996704)
[2025-02-13 19:55:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:07][root][INFO] - Training Epoch: 2/2, step 110/7134 completed (loss: 0.13562405109405518, acc: 0.9608938694000244)
[2025-02-13 19:55:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:07][root][INFO] - Training Epoch: 2/2, step 111/7134 completed (loss: 0.18211476504802704, acc: 0.9545454382896423)
[2025-02-13 19:55:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:08][root][INFO] - Training Epoch: 2/2, step 112/7134 completed (loss: 0.22599560022354126, acc: 0.9444444179534912)
[2025-02-13 19:55:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:08][root][INFO] - Training Epoch: 2/2, step 113/7134 completed (loss: 0.08594910055398941, acc: 0.9832402467727661)
[2025-02-13 19:55:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:08][root][INFO] - Training Epoch: 2/2, step 114/7134 completed (loss: 0.2049562782049179, acc: 0.9485714435577393)
[2025-02-13 19:55:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:09][root][INFO] - Training Epoch: 2/2, step 115/7134 completed (loss: 0.3890860080718994, acc: 0.9009901285171509)
[2025-02-13 19:55:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:09][root][INFO] - Training Epoch: 2/2, step 116/7134 completed (loss: 0.19671641290187836, acc: 0.959770143032074)
[2025-02-13 19:55:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:09][root][INFO] - Training Epoch: 2/2, step 117/7134 completed (loss: 0.1556146889925003, acc: 0.9515151381492615)
[2025-02-13 19:55:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:10][root][INFO] - Training Epoch: 2/2, step 118/7134 completed (loss: 0.2137453258037567, acc: 0.9611111283302307)
[2025-02-13 19:55:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:10][root][INFO] - Training Epoch: 2/2, step 119/7134 completed (loss: 0.13612058758735657, acc: 0.978723406791687)
[2025-02-13 19:55:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:11][root][INFO] - Training Epoch: 2/2, step 120/7134 completed (loss: 0.11427982896566391, acc: 0.9702380895614624)
[2025-02-13 19:55:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:11][root][INFO] - Training Epoch: 2/2, step 121/7134 completed (loss: 0.1882402002811432, acc: 0.9553072452545166)
[2025-02-13 19:55:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:11][root][INFO] - Training Epoch: 2/2, step 122/7134 completed (loss: 0.14503443241119385, acc: 0.9725274443626404)
[2025-02-13 19:55:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:12][root][INFO] - Training Epoch: 2/2, step 123/7134 completed (loss: 0.2002629190683365, acc: 0.9622641801834106)
[2025-02-13 19:55:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:12][root][INFO] - Training Epoch: 2/2, step 124/7134 completed (loss: 0.20590545237064362, acc: 0.9722222089767456)
[2025-02-13 19:55:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:12][root][INFO] - Training Epoch: 2/2, step 125/7134 completed (loss: 0.25250229239463806, acc: 0.9313725233078003)
[2025-02-13 19:55:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:13][root][INFO] - Training Epoch: 2/2, step 126/7134 completed (loss: 0.2387378215789795, acc: 0.936170220375061)
[2025-02-13 19:55:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:13][root][INFO] - Training Epoch: 2/2, step 127/7134 completed (loss: 0.27200961112976074, acc: 0.9270833134651184)
[2025-02-13 19:55:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:14][root][INFO] - Training Epoch: 2/2, step 128/7134 completed (loss: 0.1891797035932541, acc: 0.9569377899169922)
[2025-02-13 19:55:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:14][root][INFO] - Training Epoch: 2/2, step 129/7134 completed (loss: 0.2623783349990845, acc: 0.9273743033409119)
[2025-02-13 19:55:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:14][root][INFO] - Training Epoch: 2/2, step 130/7134 completed (loss: 0.1326364427804947, acc: 0.9588235020637512)
[2025-02-13 19:55:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:15][root][INFO] - Training Epoch: 2/2, step 131/7134 completed (loss: 0.06381832808256149, acc: 0.988304078578949)
[2025-02-13 19:55:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:15][root][INFO] - Training Epoch: 2/2, step 132/7134 completed (loss: 0.24982526898384094, acc: 0.9371069073677063)
[2025-02-13 19:55:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:16][root][INFO] - Training Epoch: 2/2, step 133/7134 completed (loss: 0.14046888053417206, acc: 0.970588207244873)
[2025-02-13 19:55:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:16][root][INFO] - Training Epoch: 2/2, step 134/7134 completed (loss: 0.12406472861766815, acc: 0.9624060392379761)
[2025-02-13 19:55:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:16][root][INFO] - Training Epoch: 2/2, step 135/7134 completed (loss: 0.15891651809215546, acc: 0.9523809552192688)
[2025-02-13 19:55:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:17][root][INFO] - Training Epoch: 2/2, step 136/7134 completed (loss: 0.1716354340314865, acc: 0.9510869383811951)
[2025-02-13 19:55:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:17][root][INFO] - Training Epoch: 2/2, step 137/7134 completed (loss: 0.17118880152702332, acc: 0.9547738432884216)
[2025-02-13 19:55:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:18][root][INFO] - Training Epoch: 2/2, step 138/7134 completed (loss: 0.1751244068145752, acc: 0.9408602118492126)
[2025-02-13 19:55:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:18][root][INFO] - Training Epoch: 2/2, step 139/7134 completed (loss: 0.1090615838766098, acc: 0.9627659320831299)
[2025-02-13 19:55:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:18][root][INFO] - Training Epoch: 2/2, step 140/7134 completed (loss: 0.11664249002933502, acc: 0.9631901979446411)
[2025-02-13 19:55:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:19][root][INFO] - Training Epoch: 2/2, step 141/7134 completed (loss: 0.06753045320510864, acc: 0.9850000143051147)
[2025-02-13 19:55:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:19][root][INFO] - Training Epoch: 2/2, step 142/7134 completed (loss: 0.17267557978630066, acc: 0.9661017060279846)
[2025-02-13 19:55:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:19][root][INFO] - Training Epoch: 2/2, step 143/7134 completed (loss: 0.32854175567626953, acc: 0.9226519465446472)
[2025-02-13 19:55:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:20][root][INFO] - Training Epoch: 2/2, step 144/7134 completed (loss: 0.2692181169986725, acc: 0.9189189076423645)
[2025-02-13 19:55:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:20][root][INFO] - Training Epoch: 2/2, step 145/7134 completed (loss: 0.701123833656311, acc: 0.8393782377243042)
[2025-02-13 19:55:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:21][root][INFO] - Training Epoch: 2/2, step 146/7134 completed (loss: 0.3631313443183899, acc: 0.8700565099716187)
[2025-02-13 19:55:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:21][root][INFO] - Training Epoch: 2/2, step 147/7134 completed (loss: 0.5801424384117126, acc: 0.9034482836723328)
[2025-02-13 19:55:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:21][root][INFO] - Training Epoch: 2/2, step 148/7134 completed (loss: 0.7400906085968018, acc: 0.7983871102333069)
[2025-02-13 19:55:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:22][root][INFO] - Training Epoch: 2/2, step 149/7134 completed (loss: 0.5562562942504883, acc: 0.8883248567581177)
[2025-02-13 19:55:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:22][root][INFO] - Training Epoch: 2/2, step 150/7134 completed (loss: 0.4676031172275543, acc: 0.8848167657852173)
[2025-02-13 19:55:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:23][root][INFO] - Training Epoch: 2/2, step 151/7134 completed (loss: 0.1982656866312027, acc: 0.9462365508079529)
[2025-02-13 19:55:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:23][root][INFO] - Training Epoch: 2/2, step 152/7134 completed (loss: 0.19480504095554352, acc: 0.9473684430122375)
[2025-02-13 19:55:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:23][root][INFO] - Training Epoch: 2/2, step 153/7134 completed (loss: 0.5216879844665527, acc: 0.9060773253440857)
[2025-02-13 19:55:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:24][root][INFO] - Training Epoch: 2/2, step 154/7134 completed (loss: 0.4767993092536926, acc: 0.915032684803009)
[2025-02-13 19:55:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:24][root][INFO] - Training Epoch: 2/2, step 155/7134 completed (loss: 0.43992123007774353, acc: 0.9071038365364075)
[2025-02-13 19:55:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:24][root][INFO] - Training Epoch: 2/2, step 156/7134 completed (loss: 0.2028464674949646, acc: 0.9408866763114929)
[2025-02-13 19:55:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:25][root][INFO] - Training Epoch: 2/2, step 157/7134 completed (loss: 0.40820005536079407, acc: 0.9259259104728699)
[2025-02-13 19:55:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:25][root][INFO] - Training Epoch: 2/2, step 158/7134 completed (loss: 0.12592144310474396, acc: 0.9824561476707458)
[2025-02-13 19:55:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:26][root][INFO] - Training Epoch: 2/2, step 159/7134 completed (loss: 0.24967233836650848, acc: 0.9479768872261047)
[2025-02-13 19:55:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:26][root][INFO] - Training Epoch: 2/2, step 160/7134 completed (loss: 0.1261185109615326, acc: 0.9830508232116699)
[2025-02-13 19:55:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:26][root][INFO] - Training Epoch: 2/2, step 161/7134 completed (loss: 0.04024340584874153, acc: 1.0)
[2025-02-13 19:55:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:27][root][INFO] - Training Epoch: 2/2, step 162/7134 completed (loss: 0.15027160942554474, acc: 0.9506173133850098)
[2025-02-13 19:55:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:27][root][INFO] - Training Epoch: 2/2, step 163/7134 completed (loss: 0.07921842485666275, acc: 0.9784172773361206)
[2025-02-13 19:55:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:28][root][INFO] - Training Epoch: 2/2, step 164/7134 completed (loss: 0.19552916288375854, acc: 0.939393937587738)
[2025-02-13 19:55:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:28][root][INFO] - Training Epoch: 2/2, step 165/7134 completed (loss: 0.24265219271183014, acc: 0.9399999976158142)
[2025-02-13 19:55:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:28][root][INFO] - Training Epoch: 2/2, step 166/7134 completed (loss: 0.16587451100349426, acc: 0.9736841917037964)
[2025-02-13 19:55:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:29][root][INFO] - Training Epoch: 2/2, step 167/7134 completed (loss: 1.4387377500534058, acc: 0.6927710771560669)
[2025-02-13 19:55:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:29][root][INFO] - Training Epoch: 2/2, step 168/7134 completed (loss: 3.2405788898468018, acc: 0.4363636374473572)
[2025-02-13 19:55:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:30][root][INFO] - Training Epoch: 2/2, step 169/7134 completed (loss: 2.9392588138580322, acc: 0.4609375)
[2025-02-13 19:55:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:30][root][INFO] - Training Epoch: 2/2, step 170/7134 completed (loss: 1.7223222255706787, acc: 0.6904761791229248)
[2025-02-13 19:55:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:31][root][INFO] - Training Epoch: 2/2, step 171/7134 completed (loss: 1.8096612691879272, acc: 0.6534653306007385)
[2025-02-13 19:55:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:31][root][INFO] - Training Epoch: 2/2, step 172/7134 completed (loss: 0.7979004383087158, acc: 0.8294573426246643)
[2025-02-13 19:55:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:31][root][INFO] - Training Epoch: 2/2, step 173/7134 completed (loss: 0.6758773326873779, acc: 0.8240000009536743)
[2025-02-13 19:55:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:32][root][INFO] - Training Epoch: 2/2, step 174/7134 completed (loss: 0.9201528429985046, acc: 0.8289473652839661)
[2025-02-13 19:55:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:32][root][INFO] - Training Epoch: 2/2, step 175/7134 completed (loss: 0.5639265775680542, acc: 0.8476821184158325)
[2025-02-13 19:55:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:33][root][INFO] - Training Epoch: 2/2, step 176/7134 completed (loss: 0.6606487035751343, acc: 0.8633880019187927)
[2025-02-13 19:55:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:33][root][INFO] - Training Epoch: 2/2, step 177/7134 completed (loss: 1.0135222673416138, acc: 0.826347291469574)
[2025-02-13 19:55:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:33][root][INFO] - Training Epoch: 2/2, step 178/7134 completed (loss: 0.6023558378219604, acc: 0.8831169009208679)
[2025-02-13 19:55:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:34][root][INFO] - Training Epoch: 2/2, step 179/7134 completed (loss: 0.20422790944576263, acc: 0.9416058659553528)
[2025-02-13 19:55:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:34][root][INFO] - Training Epoch: 2/2, step 180/7134 completed (loss: 0.19444499909877777, acc: 0.9664429426193237)
[2025-02-13 19:55:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:34][root][INFO] - Training Epoch: 2/2, step 181/7134 completed (loss: 0.09239280968904495, acc: 0.9862068891525269)
[2025-02-13 19:55:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:35][root][INFO] - Training Epoch: 2/2, step 182/7134 completed (loss: 0.12818345427513123, acc: 0.9689440727233887)
[2025-02-13 19:55:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:35][root][INFO] - Training Epoch: 2/2, step 183/7134 completed (loss: 0.2614232301712036, acc: 0.931506872177124)
[2025-02-13 19:55:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:36][root][INFO] - Training Epoch: 2/2, step 184/7134 completed (loss: 0.2472737729549408, acc: 0.953125)
[2025-02-13 19:55:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:36][root][INFO] - Training Epoch: 2/2, step 185/7134 completed (loss: 0.1264120489358902, acc: 0.9795918464660645)
[2025-02-13 19:55:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:36][root][INFO] - Training Epoch: 2/2, step 186/7134 completed (loss: 0.18452318012714386, acc: 0.9776119589805603)
[2025-02-13 19:55:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:37][root][INFO] - Training Epoch: 2/2, step 187/7134 completed (loss: 0.29728299379348755, acc: 0.915032684803009)
[2025-02-13 19:55:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:37][root][INFO] - Training Epoch: 2/2, step 188/7134 completed (loss: 0.41642776131629944, acc: 0.8963414430618286)
[2025-02-13 19:55:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:37][root][INFO] - Training Epoch: 2/2, step 189/7134 completed (loss: 0.3855958580970764, acc: 0.9098360538482666)
[2025-02-13 19:55:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:38][root][INFO] - Training Epoch: 2/2, step 190/7134 completed (loss: 0.5598011016845703, acc: 0.9029850959777832)
[2025-02-13 19:55:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:38][root][INFO] - Training Epoch: 2/2, step 191/7134 completed (loss: 0.38861921429634094, acc: 0.9038461446762085)
[2025-02-13 19:55:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:39][root][INFO] - Training Epoch: 2/2, step 192/7134 completed (loss: 0.14023254811763763, acc: 0.9709302186965942)
[2025-02-13 19:55:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:39][root][INFO] - Training Epoch: 2/2, step 193/7134 completed (loss: 0.30561363697052, acc: 0.9281437397003174)
[2025-02-13 19:55:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:39][root][INFO] - Training Epoch: 2/2, step 194/7134 completed (loss: 0.45410943031311035, acc: 0.8742138147354126)
[2025-02-13 19:55:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:40][root][INFO] - Training Epoch: 2/2, step 195/7134 completed (loss: 0.327147901058197, acc: 0.9140625)
[2025-02-13 19:55:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:40][root][INFO] - Training Epoch: 2/2, step 196/7134 completed (loss: 0.5106184482574463, acc: 0.8826815485954285)
[2025-02-13 19:55:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:41][root][INFO] - Training Epoch: 2/2, step 197/7134 completed (loss: 0.15168482065200806, acc: 0.9594594836235046)
[2025-02-13 19:55:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:41][root][INFO] - Training Epoch: 2/2, step 198/7134 completed (loss: 0.15106356143951416, acc: 0.9554139971733093)
[2025-02-13 19:55:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:41][root][INFO] - Training Epoch: 2/2, step 199/7134 completed (loss: 0.410754919052124, acc: 0.9191176295280457)
[2025-02-13 19:55:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:42][root][INFO] - Training Epoch: 2/2, step 200/7134 completed (loss: 0.10167139023542404, acc: 0.9863945841789246)
[2025-02-13 19:55:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:42][root][INFO] - Training Epoch: 2/2, step 201/7134 completed (loss: 0.06662560254335403, acc: 0.9858155846595764)
[2025-02-13 19:55:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:43][root][INFO] - Training Epoch: 2/2, step 202/7134 completed (loss: 0.30225318670272827, acc: 0.9419354796409607)
[2025-02-13 19:55:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:43][root][INFO] - Training Epoch: 2/2, step 203/7134 completed (loss: 0.4054539203643799, acc: 0.9112426042556763)
[2025-02-13 19:55:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:43][root][INFO] - Training Epoch: 2/2, step 204/7134 completed (loss: 0.452707439661026, acc: 0.893081784248352)
[2025-02-13 19:55:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:44][root][INFO] - Training Epoch: 2/2, step 205/7134 completed (loss: 0.5345426797866821, acc: 0.8650000095367432)
[2025-02-13 19:55:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:44][root][INFO] - Training Epoch: 2/2, step 206/7134 completed (loss: 0.22404029965400696, acc: 0.9513888955116272)
[2025-02-13 19:55:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:45][root][INFO] - Training Epoch: 2/2, step 207/7134 completed (loss: 0.3559480309486389, acc: 0.918367326259613)
[2025-02-13 19:55:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:45][root][INFO] - Training Epoch: 2/2, step 208/7134 completed (loss: 0.5836999416351318, acc: 0.8357142806053162)
[2025-02-13 19:55:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:45][root][INFO] - Training Epoch: 2/2, step 209/7134 completed (loss: 0.4715809226036072, acc: 0.910614550113678)
[2025-02-13 19:55:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:46][root][INFO] - Training Epoch: 2/2, step 210/7134 completed (loss: 0.4508063495159149, acc: 0.9030836820602417)
[2025-02-13 19:55:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:46][root][INFO] - Training Epoch: 2/2, step 211/7134 completed (loss: 0.3510177731513977, acc: 0.9054054021835327)
[2025-02-13 19:55:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:46][root][INFO] - Training Epoch: 2/2, step 212/7134 completed (loss: 0.5291436314582825, acc: 0.9008264541625977)
[2025-02-13 19:55:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:47][root][INFO] - Training Epoch: 2/2, step 213/7134 completed (loss: 0.5435678958892822, acc: 0.9200000166893005)
[2025-02-13 19:55:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:47][root][INFO] - Training Epoch: 2/2, step 214/7134 completed (loss: 0.1512426733970642, acc: 0.9603960514068604)
[2025-02-13 19:55:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:47][root][INFO] - Training Epoch: 2/2, step 215/7134 completed (loss: 0.20734179019927979, acc: 0.949999988079071)
[2025-02-13 19:55:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:48][root][INFO] - Training Epoch: 2/2, step 216/7134 completed (loss: 0.09468375146389008, acc: 0.983146071434021)
[2025-02-13 19:55:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:48][root][INFO] - Training Epoch: 2/2, step 217/7134 completed (loss: 0.1507038027048111, acc: 0.9791666865348816)
[2025-02-13 19:55:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:49][root][INFO] - Training Epoch: 2/2, step 218/7134 completed (loss: 0.07413159310817719, acc: 0.9839572310447693)
[2025-02-13 19:55:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:49][root][INFO] - Training Epoch: 2/2, step 219/7134 completed (loss: 0.1291847825050354, acc: 0.9561403393745422)
[2025-02-13 19:55:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:49][root][INFO] - Training Epoch: 2/2, step 220/7134 completed (loss: 0.10031729191541672, acc: 0.9826086759567261)
[2025-02-13 19:55:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:50][root][INFO] - Training Epoch: 2/2, step 221/7134 completed (loss: 0.12026961147785187, acc: 0.96875)
[2025-02-13 19:55:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:50][root][INFO] - Training Epoch: 2/2, step 222/7134 completed (loss: 0.1559007316827774, acc: 0.9537572264671326)
[2025-02-13 19:55:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:51][root][INFO] - Training Epoch: 2/2, step 223/7134 completed (loss: 0.3015153110027313, acc: 0.929729700088501)
[2025-02-13 19:55:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:51][root][INFO] - Training Epoch: 2/2, step 224/7134 completed (loss: 0.6360348463058472, acc: 0.8715083599090576)
[2025-02-13 19:55:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:51][root][INFO] - Training Epoch: 2/2, step 225/7134 completed (loss: 0.41041380167007446, acc: 0.9112426042556763)
[2025-02-13 19:55:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:52][root][INFO] - Training Epoch: 2/2, step 226/7134 completed (loss: 0.25726690888404846, acc: 0.9428571462631226)
[2025-02-13 19:55:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:52][root][INFO] - Training Epoch: 2/2, step 227/7134 completed (loss: 0.29693251848220825, acc: 0.9202127456665039)
[2025-02-13 19:55:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:52][root][INFO] - Training Epoch: 2/2, step 228/7134 completed (loss: 0.39654740691185, acc: 0.916167676448822)
[2025-02-13 19:55:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:53][root][INFO] - Training Epoch: 2/2, step 229/7134 completed (loss: 0.33110910654067993, acc: 0.9200000166893005)
[2025-02-13 19:55:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:53][root][INFO] - Training Epoch: 2/2, step 230/7134 completed (loss: 0.22088679671287537, acc: 0.939226508140564)
[2025-02-13 19:55:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:54][root][INFO] - Training Epoch: 2/2, step 231/7134 completed (loss: 0.16924792528152466, acc: 0.9385964870452881)
[2025-02-13 19:55:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:54][root][INFO] - Training Epoch: 2/2, step 232/7134 completed (loss: 0.2758091986179352, acc: 0.9370629191398621)
[2025-02-13 19:55:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:54][root][INFO] - Training Epoch: 2/2, step 233/7134 completed (loss: 0.29599863290786743, acc: 0.9051094651222229)
[2025-02-13 19:55:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:55][root][INFO] - Training Epoch: 2/2, step 234/7134 completed (loss: 0.2826661765575409, acc: 0.9507042169570923)
[2025-02-13 19:55:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:55][root][INFO] - Training Epoch: 2/2, step 235/7134 completed (loss: 0.286582887172699, acc: 0.9390243887901306)
[2025-02-13 19:55:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:55][root][INFO] - Training Epoch: 2/2, step 236/7134 completed (loss: 0.16425400972366333, acc: 0.9470198750495911)
[2025-02-13 19:55:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:56][root][INFO] - Training Epoch: 2/2, step 237/7134 completed (loss: 0.3025658428668976, acc: 0.9426751732826233)
[2025-02-13 19:55:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:56][root][INFO] - Training Epoch: 2/2, step 238/7134 completed (loss: 0.39000165462493896, acc: 0.9160305261611938)
[2025-02-13 19:55:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:57][root][INFO] - Training Epoch: 2/2, step 239/7134 completed (loss: 0.20946043729782104, acc: 0.9622641801834106)
[2025-02-13 19:55:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:57][root][INFO] - Training Epoch: 2/2, step 240/7134 completed (loss: 0.29989108443260193, acc: 0.9514563083648682)
[2025-02-13 19:55:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:57][root][INFO] - Training Epoch: 2/2, step 241/7134 completed (loss: 0.3627215027809143, acc: 0.9179104566574097)
[2025-02-13 19:55:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:58][root][INFO] - Training Epoch: 2/2, step 242/7134 completed (loss: 0.4702892005443573, acc: 0.8859060406684875)
[2025-02-13 19:55:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:58][root][INFO] - Training Epoch: 2/2, step 243/7134 completed (loss: 0.5775965452194214, acc: 0.8689655065536499)
[2025-02-13 19:55:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:59][root][INFO] - Training Epoch: 2/2, step 244/7134 completed (loss: 0.21958453953266144, acc: 0.9640287756919861)
[2025-02-13 19:55:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:59][root][INFO] - Training Epoch: 2/2, step 245/7134 completed (loss: 0.21317000687122345, acc: 0.9344262480735779)
[2025-02-13 19:55:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:55:59][root][INFO] - Training Epoch: 2/2, step 246/7134 completed (loss: 0.15889260172843933, acc: 0.95652174949646)
[2025-02-13 19:55:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:00][root][INFO] - Training Epoch: 2/2, step 247/7134 completed (loss: 0.2791942358016968, acc: 0.9444444179534912)
[2025-02-13 19:56:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:00][root][INFO] - Training Epoch: 2/2, step 248/7134 completed (loss: 0.22365057468414307, acc: 0.934959352016449)
[2025-02-13 19:56:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:00][root][INFO] - Training Epoch: 2/2, step 249/7134 completed (loss: 0.10660181939601898, acc: 0.989130437374115)
[2025-02-13 19:56:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:01][root][INFO] - Training Epoch: 2/2, step 250/7134 completed (loss: 0.3037010729312897, acc: 0.9271523356437683)
[2025-02-13 19:56:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:01][root][INFO] - Training Epoch: 2/2, step 251/7134 completed (loss: 0.25063008069992065, acc: 0.935251772403717)
[2025-02-13 19:56:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:02][root][INFO] - Training Epoch: 2/2, step 252/7134 completed (loss: 0.11207898706197739, acc: 0.9640287756919861)
[2025-02-13 19:56:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:02][root][INFO] - Training Epoch: 2/2, step 253/7134 completed (loss: 0.10763068497180939, acc: 0.984000027179718)
[2025-02-13 19:56:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:02][root][INFO] - Training Epoch: 2/2, step 254/7134 completed (loss: 0.1779070794582367, acc: 0.9473684430122375)
[2025-02-13 19:56:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:03][root][INFO] - Training Epoch: 2/2, step 255/7134 completed (loss: 0.1423485428094864, acc: 0.9615384340286255)
[2025-02-13 19:56:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:03][root][INFO] - Training Epoch: 2/2, step 256/7134 completed (loss: 0.5662214159965515, acc: 0.8648648858070374)
[2025-02-13 19:56:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:03][root][INFO] - Training Epoch: 2/2, step 257/7134 completed (loss: 0.3316700756549835, acc: 0.9390243887901306)
[2025-02-13 19:56:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:04][root][INFO] - Training Epoch: 2/2, step 258/7134 completed (loss: 0.11194618046283722, acc: 0.9838709831237793)
[2025-02-13 19:56:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:04][root][INFO] - Training Epoch: 2/2, step 259/7134 completed (loss: 0.14896874129772186, acc: 0.9696969985961914)
[2025-02-13 19:56:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:05][root][INFO] - Training Epoch: 2/2, step 260/7134 completed (loss: 0.17640380561351776, acc: 0.9548872113227844)
[2025-02-13 19:56:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:05][root][INFO] - Training Epoch: 2/2, step 261/7134 completed (loss: 0.2796134948730469, acc: 0.9351851940155029)
[2025-02-13 19:56:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:05][root][INFO] - Training Epoch: 2/2, step 262/7134 completed (loss: 0.06629206240177155, acc: 0.9918699264526367)
[2025-02-13 19:56:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:06][root][INFO] - Training Epoch: 2/2, step 263/7134 completed (loss: 0.16008725762367249, acc: 0.9629629850387573)
[2025-02-13 19:56:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:06][root][INFO] - Training Epoch: 2/2, step 264/7134 completed (loss: 0.07023726403713226, acc: 0.9882352948188782)
[2025-02-13 19:56:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:07][root][INFO] - Training Epoch: 2/2, step 265/7134 completed (loss: 0.2015615552663803, acc: 0.9550561904907227)
[2025-02-13 19:56:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:07][root][INFO] - Training Epoch: 2/2, step 266/7134 completed (loss: 0.2484954595565796, acc: 0.921875)
[2025-02-13 19:56:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:07][root][INFO] - Training Epoch: 2/2, step 267/7134 completed (loss: 0.21634207665920258, acc: 0.9308510422706604)
[2025-02-13 19:56:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:08][root][INFO] - Training Epoch: 2/2, step 268/7134 completed (loss: 0.19226151704788208, acc: 0.9454545378684998)
[2025-02-13 19:56:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:08][root][INFO] - Training Epoch: 2/2, step 269/7134 completed (loss: 0.27663654088974, acc: 0.9404761791229248)
[2025-02-13 19:56:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:08][root][INFO] - Training Epoch: 2/2, step 270/7134 completed (loss: 0.2083919495344162, acc: 0.956250011920929)
[2025-02-13 19:56:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:09][root][INFO] - Training Epoch: 2/2, step 271/7134 completed (loss: 0.12225359678268433, acc: 0.9777777791023254)
[2025-02-13 19:56:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:09][root][INFO] - Training Epoch: 2/2, step 272/7134 completed (loss: 0.28024864196777344, acc: 0.9385474920272827)
[2025-02-13 19:56:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:10][root][INFO] - Training Epoch: 2/2, step 273/7134 completed (loss: 0.16697758436203003, acc: 0.9521276354789734)
[2025-02-13 19:56:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:10][root][INFO] - Training Epoch: 2/2, step 274/7134 completed (loss: 0.2626414895057678, acc: 0.9415204524993896)
[2025-02-13 19:56:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:10][root][INFO] - Training Epoch: 2/2, step 275/7134 completed (loss: 0.16260391473770142, acc: 0.9695431590080261)
[2025-02-13 19:56:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:11][root][INFO] - Training Epoch: 2/2, step 276/7134 completed (loss: 0.11834269762039185, acc: 0.9639175534248352)
[2025-02-13 19:56:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:11][root][INFO] - Training Epoch: 2/2, step 277/7134 completed (loss: 0.23449061810970306, acc: 0.9513513445854187)
[2025-02-13 19:56:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:11][root][INFO] - Training Epoch: 2/2, step 278/7134 completed (loss: 0.23347651958465576, acc: 0.953125)
[2025-02-13 19:56:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:12][root][INFO] - Training Epoch: 2/2, step 279/7134 completed (loss: 0.21781669557094574, acc: 0.9459459185600281)
[2025-02-13 19:56:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:12][root][INFO] - Training Epoch: 2/2, step 280/7134 completed (loss: 0.2847946286201477, acc: 0.9440000057220459)
[2025-02-13 19:56:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:13][root][INFO] - Training Epoch: 2/2, step 281/7134 completed (loss: 0.14125196635723114, acc: 0.9745762944221497)
[2025-02-13 19:56:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:13][root][INFO] - Training Epoch: 2/2, step 282/7134 completed (loss: 0.3044905960559845, acc: 0.9303797483444214)
[2025-02-13 19:56:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:13][root][INFO] - Training Epoch: 2/2, step 283/7134 completed (loss: 0.12937963008880615, acc: 0.9694656729698181)
[2025-02-13 19:56:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:14][root][INFO] - Training Epoch: 2/2, step 284/7134 completed (loss: 0.07605605572462082, acc: 0.9720279574394226)
[2025-02-13 19:56:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:14][root][INFO] - Training Epoch: 2/2, step 285/7134 completed (loss: 0.25578171014785767, acc: 0.9333333373069763)
[2025-02-13 19:56:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:14][root][INFO] - Training Epoch: 2/2, step 286/7134 completed (loss: 0.13183999061584473, acc: 0.9583333134651184)
[2025-02-13 19:56:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:15][root][INFO] - Training Epoch: 2/2, step 287/7134 completed (loss: 0.21134568750858307, acc: 0.9597315192222595)
[2025-02-13 19:56:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:15][root][INFO] - Training Epoch: 2/2, step 288/7134 completed (loss: 0.12948229908943176, acc: 0.9685039520263672)
[2025-02-13 19:56:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:16][root][INFO] - Training Epoch: 2/2, step 289/7134 completed (loss: 0.12131630629301071, acc: 0.9767441749572754)
[2025-02-13 19:56:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:16][root][INFO] - Training Epoch: 2/2, step 290/7134 completed (loss: 0.32528239488601685, acc: 0.9155844449996948)
[2025-02-13 19:56:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:16][root][INFO] - Training Epoch: 2/2, step 291/7134 completed (loss: 0.32490333914756775, acc: 0.9319728016853333)
[2025-02-13 19:56:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:17][root][INFO] - Training Epoch: 2/2, step 292/7134 completed (loss: 0.604883074760437, acc: 0.8888888955116272)
[2025-02-13 19:56:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:17][root][INFO] - Training Epoch: 2/2, step 293/7134 completed (loss: 0.3561680316925049, acc: 0.8938053250312805)
[2025-02-13 19:56:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:18][root][INFO] - Training Epoch: 2/2, step 294/7134 completed (loss: 0.3076351284980774, acc: 0.9322034120559692)
[2025-02-13 19:56:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:18][root][INFO] - Training Epoch: 2/2, step 295/7134 completed (loss: 0.5097469091415405, acc: 0.8910256624221802)
[2025-02-13 19:56:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:18][root][INFO] - Training Epoch: 2/2, step 296/7134 completed (loss: 0.12396000325679779, acc: 0.9696969985961914)
[2025-02-13 19:56:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:19][root][INFO] - Training Epoch: 2/2, step 297/7134 completed (loss: 0.2919920086860657, acc: 0.9504132270812988)
[2025-02-13 19:56:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:19][root][INFO] - Training Epoch: 2/2, step 298/7134 completed (loss: 0.1145174503326416, acc: 0.9615384340286255)
[2025-02-13 19:56:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:20][root][INFO] - Training Epoch: 2/2, step 299/7134 completed (loss: 0.06933443993330002, acc: 0.9927007555961609)
[2025-02-13 19:56:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:20][root][INFO] - Training Epoch: 2/2, step 300/7134 completed (loss: 0.047402217984199524, acc: 0.9927536249160767)
[2025-02-13 19:56:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:20][root][INFO] - Training Epoch: 2/2, step 301/7134 completed (loss: 0.12153919041156769, acc: 0.9720279574394226)
[2025-02-13 19:56:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:21][root][INFO] - Training Epoch: 2/2, step 302/7134 completed (loss: 0.11273656785488129, acc: 0.9801980257034302)
[2025-02-13 19:56:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:21][root][INFO] - Training Epoch: 2/2, step 303/7134 completed (loss: 0.1527072936296463, acc: 0.9639639854431152)
[2025-02-13 19:56:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:21][root][INFO] - Training Epoch: 2/2, step 304/7134 completed (loss: 0.09935085475444794, acc: 0.9801980257034302)
[2025-02-13 19:56:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:22][root][INFO] - Training Epoch: 2/2, step 305/7134 completed (loss: 0.23749202489852905, acc: 0.96875)
[2025-02-13 19:56:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:22][root][INFO] - Training Epoch: 2/2, step 306/7134 completed (loss: 0.07272830605506897, acc: 0.991304337978363)
[2025-02-13 19:56:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:23][root][INFO] - Training Epoch: 2/2, step 307/7134 completed (loss: 0.14135907590389252, acc: 0.9626168012619019)
[2025-02-13 19:56:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:23][root][INFO] - Training Epoch: 2/2, step 308/7134 completed (loss: 0.11952874809503555, acc: 0.960629940032959)
[2025-02-13 19:56:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:23][root][INFO] - Training Epoch: 2/2, step 309/7134 completed (loss: 0.1037268117070198, acc: 0.9734513163566589)
[2025-02-13 19:56:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:24][root][INFO] - Training Epoch: 2/2, step 310/7134 completed (loss: 0.08779974281787872, acc: 0.9844961166381836)
[2025-02-13 19:56:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:24][root][INFO] - Training Epoch: 2/2, step 311/7134 completed (loss: 0.13501985371112823, acc: 0.969924807548523)
[2025-02-13 19:56:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:25][root][INFO] - Training Epoch: 2/2, step 312/7134 completed (loss: 0.033652935177087784, acc: 1.0)
[2025-02-13 19:56:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:25][root][INFO] - Training Epoch: 2/2, step 313/7134 completed (loss: 0.03850875049829483, acc: 1.0)
[2025-02-13 19:56:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:25][root][INFO] - Training Epoch: 2/2, step 314/7134 completed (loss: 0.09399721026420593, acc: 0.970370352268219)
[2025-02-13 19:56:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:26][root][INFO] - Training Epoch: 2/2, step 315/7134 completed (loss: 0.23002247512340546, acc: 0.949999988079071)
[2025-02-13 19:56:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:26][root][INFO] - Training Epoch: 2/2, step 316/7134 completed (loss: 0.16648650169372559, acc: 0.9729729890823364)
[2025-02-13 19:56:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:27][root][INFO] - Training Epoch: 2/2, step 317/7134 completed (loss: 0.07972291111946106, acc: 0.9873417615890503)
[2025-02-13 19:56:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:27][root][INFO] - Training Epoch: 2/2, step 318/7134 completed (loss: 0.15484315156936646, acc: 0.9408602118492126)
[2025-02-13 19:56:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:27][root][INFO] - Training Epoch: 2/2, step 319/7134 completed (loss: 0.18785704672336578, acc: 0.9503105878829956)
[2025-02-13 19:56:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:28][root][INFO] - Training Epoch: 2/2, step 320/7134 completed (loss: 0.1615157425403595, acc: 0.9642857313156128)
[2025-02-13 19:56:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:28][root][INFO] - Training Epoch: 2/2, step 321/7134 completed (loss: 0.14347264170646667, acc: 0.9712643623352051)
[2025-02-13 19:56:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:29][root][INFO] - Training Epoch: 2/2, step 322/7134 completed (loss: 0.08896633237600327, acc: 0.9934640526771545)
[2025-02-13 19:56:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:29][root][INFO] - Training Epoch: 2/2, step 323/7134 completed (loss: 0.31665483117103577, acc: 0.918367326259613)
[2025-02-13 19:56:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:29][root][INFO] - Training Epoch: 2/2, step 324/7134 completed (loss: 0.24435338377952576, acc: 0.9398148059844971)
[2025-02-13 19:56:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:30][root][INFO] - Training Epoch: 2/2, step 325/7134 completed (loss: 0.0951685830950737, acc: 0.9772727489471436)
[2025-02-13 19:56:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:30][root][INFO] - Training Epoch: 2/2, step 326/7134 completed (loss: 0.10903942584991455, acc: 0.9550561904907227)
[2025-02-13 19:56:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:31][root][INFO] - Training Epoch: 2/2, step 327/7134 completed (loss: 0.13570570945739746, acc: 0.9696969985961914)
[2025-02-13 19:56:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:31][root][INFO] - Training Epoch: 2/2, step 328/7134 completed (loss: 0.09519658237695694, acc: 0.981249988079071)
[2025-02-13 19:56:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:31][root][INFO] - Training Epoch: 2/2, step 329/7134 completed (loss: 0.07289339601993561, acc: 0.9850746393203735)
[2025-02-13 19:56:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:32][root][INFO] - Training Epoch: 2/2, step 330/7134 completed (loss: 0.1080380231142044, acc: 0.9780701994895935)
[2025-02-13 19:56:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:32][root][INFO] - Training Epoch: 2/2, step 331/7134 completed (loss: 0.0690430998802185, acc: 0.9895833134651184)
[2025-02-13 19:56:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:33][root][INFO] - Training Epoch: 2/2, step 332/7134 completed (loss: 0.022777676582336426, acc: 0.9900497794151306)
[2025-02-13 19:56:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:33][root][INFO] - Training Epoch: 2/2, step 333/7134 completed (loss: 0.19337229430675507, acc: 0.9462365508079529)
[2025-02-13 19:56:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:33][root][INFO] - Training Epoch: 2/2, step 334/7134 completed (loss: 0.15320564806461334, acc: 0.9725274443626404)
[2025-02-13 19:56:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:34][root][INFO] - Training Epoch: 2/2, step 335/7134 completed (loss: 0.06951861083507538, acc: 0.9797297120094299)
[2025-02-13 19:56:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:34][root][INFO] - Training Epoch: 2/2, step 336/7134 completed (loss: 0.35275793075561523, acc: 0.9481865167617798)
[2025-02-13 19:56:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:34][root][INFO] - Training Epoch: 2/2, step 337/7134 completed (loss: 0.16264446079730988, acc: 0.9627329111099243)
[2025-02-13 19:56:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:35][root][INFO] - Training Epoch: 2/2, step 338/7134 completed (loss: 0.19555801153182983, acc: 0.9528796076774597)
[2025-02-13 19:56:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:35][root][INFO] - Training Epoch: 2/2, step 339/7134 completed (loss: 0.14400318264961243, acc: 0.9679144620895386)
[2025-02-13 19:56:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:36][root][INFO] - Training Epoch: 2/2, step 340/7134 completed (loss: 0.21487247943878174, acc: 0.939393937587738)
[2025-02-13 19:56:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:36][root][INFO] - Training Epoch: 2/2, step 341/7134 completed (loss: 0.13874714076519012, acc: 0.9591836929321289)
[2025-02-13 19:56:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:36][root][INFO] - Training Epoch: 2/2, step 342/7134 completed (loss: 0.1384771764278412, acc: 0.9685534834861755)
[2025-02-13 19:56:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:37][root][INFO] - Training Epoch: 2/2, step 343/7134 completed (loss: 0.23725414276123047, acc: 0.9461538195610046)
[2025-02-13 19:56:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:37][root][INFO] - Training Epoch: 2/2, step 344/7134 completed (loss: 0.11199440807104111, acc: 0.961240291595459)
[2025-02-13 19:56:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:37][root][INFO] - Training Epoch: 2/2, step 345/7134 completed (loss: 0.17048661410808563, acc: 0.9645389914512634)
[2025-02-13 19:56:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:38][root][INFO] - Training Epoch: 2/2, step 346/7134 completed (loss: 0.06098613142967224, acc: 0.9868420958518982)
[2025-02-13 19:56:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:38][root][INFO] - Training Epoch: 2/2, step 347/7134 completed (loss: 0.2173726111650467, acc: 0.949999988079071)
[2025-02-13 19:56:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:39][root][INFO] - Training Epoch: 2/2, step 348/7134 completed (loss: 0.15102313458919525, acc: 0.9526627063751221)
[2025-02-13 19:56:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:39][root][INFO] - Training Epoch: 2/2, step 349/7134 completed (loss: 0.21121124923229218, acc: 0.9644970297813416)
[2025-02-13 19:56:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:39][root][INFO] - Training Epoch: 2/2, step 350/7134 completed (loss: 0.1708272248506546, acc: 0.942105233669281)
[2025-02-13 19:56:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:40][root][INFO] - Training Epoch: 2/2, step 351/7134 completed (loss: 0.13124492764472961, acc: 0.9836956262588501)
[2025-02-13 19:56:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:40][root][INFO] - Training Epoch: 2/2, step 352/7134 completed (loss: 0.1090303286910057, acc: 0.9707602262496948)
[2025-02-13 19:56:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:41][root][INFO] - Training Epoch: 2/2, step 353/7134 completed (loss: 0.1226736530661583, acc: 0.9631901979446411)
[2025-02-13 19:56:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:41][root][INFO] - Training Epoch: 2/2, step 354/7134 completed (loss: 0.17487242817878723, acc: 0.9547738432884216)
[2025-02-13 19:56:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:42][root][INFO] - Training Epoch: 2/2, step 355/7134 completed (loss: 0.2344828099012375, acc: 0.9467455744743347)
[2025-02-13 19:56:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:42][root][INFO] - Training Epoch: 2/2, step 356/7134 completed (loss: 0.08768501877784729, acc: 0.970059871673584)
[2025-02-13 19:56:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:42][root][INFO] - Training Epoch: 2/2, step 357/7134 completed (loss: 0.07448852807283401, acc: 0.9825581312179565)
[2025-02-13 19:56:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:43][root][INFO] - Training Epoch: 2/2, step 358/7134 completed (loss: 0.12450587004423141, acc: 0.9602272510528564)
[2025-02-13 19:56:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:43][root][INFO] - Training Epoch: 2/2, step 359/7134 completed (loss: 0.0769016295671463, acc: 0.9882352948188782)
[2025-02-13 19:56:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:43][root][INFO] - Training Epoch: 2/2, step 360/7134 completed (loss: 0.10787730664014816, acc: 0.9542483687400818)
[2025-02-13 19:56:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:44][root][INFO] - Training Epoch: 2/2, step 361/7134 completed (loss: 0.07737982273101807, acc: 0.9815950989723206)
[2025-02-13 19:56:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:44][root][INFO] - Training Epoch: 2/2, step 362/7134 completed (loss: 0.09693503379821777, acc: 0.9556962251663208)
[2025-02-13 19:56:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:45][root][INFO] - Training Epoch: 2/2, step 363/7134 completed (loss: 0.13358691334724426, acc: 0.9726775884628296)
[2025-02-13 19:56:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:45][root][INFO] - Training Epoch: 2/2, step 364/7134 completed (loss: 0.11011207103729248, acc: 0.9611650705337524)
[2025-02-13 19:56:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:45][root][INFO] - Training Epoch: 2/2, step 365/7134 completed (loss: 0.05831426382064819, acc: 0.9873417615890503)
[2025-02-13 19:56:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:46][root][INFO] - Training Epoch: 2/2, step 366/7134 completed (loss: 0.056342657655477524, acc: 0.9808917045593262)
[2025-02-13 19:56:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:46][root][INFO] - Training Epoch: 2/2, step 367/7134 completed (loss: 0.035446520894765854, acc: 0.9948453903198242)
[2025-02-13 19:56:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:47][root][INFO] - Training Epoch: 2/2, step 368/7134 completed (loss: 0.04040517285466194, acc: 0.9881656765937805)
[2025-02-13 19:56:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:47][root][INFO] - Training Epoch: 2/2, step 369/7134 completed (loss: 0.1507990062236786, acc: 0.9509202241897583)
[2025-02-13 19:56:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:47][root][INFO] - Training Epoch: 2/2, step 370/7134 completed (loss: 0.1351340264081955, acc: 0.9620253443717957)
[2025-02-13 19:56:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:48][root][INFO] - Training Epoch: 2/2, step 371/7134 completed (loss: 0.08645717799663544, acc: 0.9751552939414978)
[2025-02-13 19:56:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:48][root][INFO] - Training Epoch: 2/2, step 372/7134 completed (loss: 0.07695480436086655, acc: 0.9811320900917053)
[2025-02-13 19:56:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:49][root][INFO] - Training Epoch: 2/2, step 373/7134 completed (loss: 0.05409405753016472, acc: 0.9869281053543091)
[2025-02-13 19:56:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:49][root][INFO] - Training Epoch: 2/2, step 374/7134 completed (loss: 0.10722466558218002, acc: 0.9797297120094299)
[2025-02-13 19:56:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:49][root][INFO] - Training Epoch: 2/2, step 375/7134 completed (loss: 0.181194007396698, acc: 0.957446813583374)
[2025-02-13 19:56:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:50][root][INFO] - Training Epoch: 2/2, step 376/7134 completed (loss: 0.2737722098827362, acc: 0.9277777671813965)
[2025-02-13 19:56:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:50][root][INFO] - Training Epoch: 2/2, step 377/7134 completed (loss: 0.1364196091890335, acc: 0.9720670580863953)
[2025-02-13 19:56:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:51][root][INFO] - Training Epoch: 2/2, step 378/7134 completed (loss: 0.1998608410358429, acc: 0.9378530979156494)
[2025-02-13 19:56:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:51][root][INFO] - Training Epoch: 2/2, step 379/7134 completed (loss: 0.05339138209819794, acc: 0.9852941036224365)
[2025-02-13 19:56:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:51][root][INFO] - Training Epoch: 2/2, step 380/7134 completed (loss: 0.3685586750507355, acc: 0.9226804375648499)
[2025-02-13 19:56:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:52][root][INFO] - Training Epoch: 2/2, step 381/7134 completed (loss: 0.2044437974691391, acc: 0.9736841917037964)
[2025-02-13 19:56:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:52][root][INFO] - Training Epoch: 2/2, step 382/7134 completed (loss: 0.1004282608628273, acc: 0.9692307710647583)
[2025-02-13 19:56:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:53][root][INFO] - Training Epoch: 2/2, step 383/7134 completed (loss: 0.1198527067899704, acc: 0.9689922332763672)
[2025-02-13 19:56:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:53][root][INFO] - Training Epoch: 2/2, step 384/7134 completed (loss: 0.22577928006649017, acc: 0.9668508172035217)
[2025-02-13 19:56:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:53][root][INFO] - Training Epoch: 2/2, step 385/7134 completed (loss: 0.09607324004173279, acc: 0.9738562107086182)
[2025-02-13 19:56:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:54][root][INFO] - Training Epoch: 2/2, step 386/7134 completed (loss: 0.045743800699710846, acc: 0.9909909963607788)
[2025-02-13 19:56:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:54][root][INFO] - Training Epoch: 2/2, step 387/7134 completed (loss: 0.09865157306194305, acc: 0.9585798978805542)
[2025-02-13 19:56:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:54][root][INFO] - Training Epoch: 2/2, step 388/7134 completed (loss: 0.10931748151779175, acc: 0.9760765433311462)
[2025-02-13 19:56:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:55][root][INFO] - Training Epoch: 2/2, step 389/7134 completed (loss: 0.15787307918071747, acc: 0.9714285731315613)
[2025-02-13 19:56:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:55][root][INFO] - Training Epoch: 2/2, step 390/7134 completed (loss: 0.07924193143844604, acc: 0.9932432174682617)
[2025-02-13 19:56:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:56][root][INFO] - Training Epoch: 2/2, step 391/7134 completed (loss: 0.09383416175842285, acc: 0.9800000190734863)
[2025-02-13 19:56:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:56][root][INFO] - Training Epoch: 2/2, step 392/7134 completed (loss: 0.13377709686756134, acc: 0.9651162624359131)
[2025-02-13 19:56:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:56][root][INFO] - Training Epoch: 2/2, step 393/7134 completed (loss: 0.10106799751520157, acc: 0.9679144620895386)
[2025-02-13 19:56:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:57][root][INFO] - Training Epoch: 2/2, step 394/7134 completed (loss: 0.08459439128637314, acc: 0.9923664331436157)
[2025-02-13 19:56:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:57][root][INFO] - Training Epoch: 2/2, step 395/7134 completed (loss: 0.2293299436569214, acc: 0.9627659320831299)
[2025-02-13 19:56:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:57][root][INFO] - Training Epoch: 2/2, step 396/7134 completed (loss: 0.15118619799613953, acc: 0.9741379022598267)
[2025-02-13 19:56:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:58][root][INFO] - Training Epoch: 2/2, step 397/7134 completed (loss: 0.23920473456382751, acc: 0.9510869383811951)
[2025-02-13 19:56:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:58][root][INFO] - Training Epoch: 2/2, step 398/7134 completed (loss: 0.06326141953468323, acc: 0.9898989796638489)
[2025-02-13 19:56:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:59][root][INFO] - Training Epoch: 2/2, step 399/7134 completed (loss: 0.08199197798967361, acc: 0.9683544039726257)
[2025-02-13 19:56:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:59][root][INFO] - Training Epoch: 2/2, step 400/7134 completed (loss: 0.29979416728019714, acc: 0.905063271522522)
[2025-02-13 19:56:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:56:59][root][INFO] - Training Epoch: 2/2, step 401/7134 completed (loss: 0.16272851824760437, acc: 0.9509803652763367)
[2025-02-13 19:57:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:00][root][INFO] - Training Epoch: 2/2, step 402/7134 completed (loss: 0.20419251918792725, acc: 0.9597989916801453)
[2025-02-13 19:57:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:00][root][INFO] - Training Epoch: 2/2, step 403/7134 completed (loss: 0.09169048815965652, acc: 0.9694322943687439)
[2025-02-13 19:57:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:01][root][INFO] - Training Epoch: 2/2, step 404/7134 completed (loss: 0.10105820000171661, acc: 0.9731183052062988)
[2025-02-13 19:57:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:01][root][INFO] - Training Epoch: 2/2, step 405/7134 completed (loss: 0.34317857027053833, acc: 0.9011628031730652)
[2025-02-13 19:57:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:01][root][INFO] - Training Epoch: 2/2, step 406/7134 completed (loss: 0.35290977358818054, acc: 0.9405405521392822)
[2025-02-13 19:57:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:02][root][INFO] - Training Epoch: 2/2, step 407/7134 completed (loss: 0.3163156807422638, acc: 0.9299362897872925)
[2025-02-13 19:57:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:02][root][INFO] - Training Epoch: 2/2, step 408/7134 completed (loss: 0.2365676462650299, acc: 0.9329608678817749)
[2025-02-13 19:57:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:02][root][INFO] - Training Epoch: 2/2, step 409/7134 completed (loss: 0.1650850772857666, acc: 0.9631578922271729)
[2025-02-13 19:57:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:03][root][INFO] - Training Epoch: 2/2, step 410/7134 completed (loss: 0.26318126916885376, acc: 0.939393937587738)
[2025-02-13 19:57:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:03][root][INFO] - Training Epoch: 2/2, step 411/7134 completed (loss: 0.11917640268802643, acc: 0.9639639854431152)
[2025-02-13 19:57:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:04][root][INFO] - Training Epoch: 2/2, step 412/7134 completed (loss: 0.11278550326824188, acc: 0.9730941653251648)
[2025-02-13 19:57:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:04][root][INFO] - Training Epoch: 2/2, step 413/7134 completed (loss: 0.15229982137680054, acc: 0.9476439952850342)
[2025-02-13 19:57:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:04][root][INFO] - Training Epoch: 2/2, step 414/7134 completed (loss: 0.07654907554388046, acc: 0.9826589822769165)
[2025-02-13 19:57:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:05][root][INFO] - Training Epoch: 2/2, step 415/7134 completed (loss: 0.08710679411888123, acc: 0.976190447807312)
[2025-02-13 19:57:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:05][root][INFO] - Training Epoch: 2/2, step 416/7134 completed (loss: 0.0744917243719101, acc: 0.9711538553237915)
[2025-02-13 19:57:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:06][root][INFO] - Training Epoch: 2/2, step 417/7134 completed (loss: 0.18339140713214874, acc: 0.9450549483299255)
[2025-02-13 19:57:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:06][root][INFO] - Training Epoch: 2/2, step 418/7134 completed (loss: 0.11753633618354797, acc: 0.9613526463508606)
[2025-02-13 19:57:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:06][root][INFO] - Training Epoch: 2/2, step 419/7134 completed (loss: 0.07864080369472504, acc: 0.9801980257034302)
[2025-02-13 19:57:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:07][root][INFO] - Training Epoch: 2/2, step 420/7134 completed (loss: 0.14312699437141418, acc: 0.9571428298950195)
[2025-02-13 19:57:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:07][root][INFO] - Training Epoch: 2/2, step 421/7134 completed (loss: 0.10371120274066925, acc: 0.9748427867889404)
[2025-02-13 19:57:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:07][root][INFO] - Training Epoch: 2/2, step 422/7134 completed (loss: 0.09617596864700317, acc: 0.9585798978805542)
[2025-02-13 19:57:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:08][root][INFO] - Training Epoch: 2/2, step 423/7134 completed (loss: 0.03902630880475044, acc: 0.9943820238113403)
[2025-02-13 19:57:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:08][root][INFO] - Training Epoch: 2/2, step 424/7134 completed (loss: 0.06358572840690613, acc: 0.9876543283462524)
[2025-02-13 19:57:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:09][root][INFO] - Training Epoch: 2/2, step 425/7134 completed (loss: 0.016595903784036636, acc: 0.9943181872367859)
[2025-02-13 19:57:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:09][root][INFO] - Training Epoch: 2/2, step 426/7134 completed (loss: 0.030930398032069206, acc: 0.9941176176071167)
[2025-02-13 19:57:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:09][root][INFO] - Training Epoch: 2/2, step 427/7134 completed (loss: 0.03922730311751366, acc: 0.9874213933944702)
[2025-02-13 19:57:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:10][root][INFO] - Training Epoch: 2/2, step 428/7134 completed (loss: 0.029264966025948524, acc: 1.0)
[2025-02-13 19:57:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:10][root][INFO] - Training Epoch: 2/2, step 429/7134 completed (loss: 0.29459676146507263, acc: 0.9210526347160339)
[2025-02-13 19:57:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:10][root][INFO] - Training Epoch: 2/2, step 430/7134 completed (loss: 0.06329971551895142, acc: 0.9882352948188782)
[2025-02-13 19:57:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:11][root][INFO] - Training Epoch: 2/2, step 431/7134 completed (loss: 0.07725867629051208, acc: 0.9881656765937805)
[2025-02-13 19:57:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:11][root][INFO] - Training Epoch: 2/2, step 432/7134 completed (loss: 0.08866994082927704, acc: 0.9848484992980957)
[2025-02-13 19:57:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:12][root][INFO] - Training Epoch: 2/2, step 433/7134 completed (loss: 0.11516762524843216, acc: 0.9680851101875305)
[2025-02-13 19:57:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:12][root][INFO] - Training Epoch: 2/2, step 434/7134 completed (loss: 0.13022343814373016, acc: 0.9567901492118835)
[2025-02-13 19:57:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:12][root][INFO] - Training Epoch: 2/2, step 435/7134 completed (loss: 0.1337074339389801, acc: 0.9611111283302307)
[2025-02-13 19:57:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:13][root][INFO] - Training Epoch: 2/2, step 436/7134 completed (loss: 0.051534514874219894, acc: 1.0)
[2025-02-13 19:57:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:13][root][INFO] - Training Epoch: 2/2, step 437/7134 completed (loss: 0.16419537365436554, acc: 0.9647887349128723)
[2025-02-13 19:57:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:14][root][INFO] - Training Epoch: 2/2, step 438/7134 completed (loss: 0.042786404490470886, acc: 0.9921259880065918)
[2025-02-13 19:57:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:14][root][INFO] - Training Epoch: 2/2, step 439/7134 completed (loss: 0.1361333727836609, acc: 0.959770143032074)
[2025-02-13 19:57:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:14][root][INFO] - Training Epoch: 2/2, step 440/7134 completed (loss: 0.11151254177093506, acc: 0.9692307710647583)
[2025-02-13 19:57:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:15][root][INFO] - Training Epoch: 2/2, step 441/7134 completed (loss: 0.05423658713698387, acc: 0.9863013625144958)
[2025-02-13 19:57:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:15][root][INFO] - Training Epoch: 2/2, step 442/7134 completed (loss: 0.019881803542375565, acc: 0.9922480583190918)
[2025-02-13 19:57:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:15][root][INFO] - Training Epoch: 2/2, step 443/7134 completed (loss: 0.025035666301846504, acc: 0.9931972622871399)
[2025-02-13 19:57:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:16][root][INFO] - Training Epoch: 2/2, step 444/7134 completed (loss: 0.018202301114797592, acc: 1.0)
[2025-02-13 19:57:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:16][root][INFO] - Training Epoch: 2/2, step 445/7134 completed (loss: 0.13731342554092407, acc: 0.9622641801834106)
[2025-02-13 19:57:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:17][root][INFO] - Training Epoch: 2/2, step 446/7134 completed (loss: 0.052038200199604034, acc: 0.9822485446929932)
[2025-02-13 19:57:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:17][root][INFO] - Training Epoch: 2/2, step 447/7134 completed (loss: 0.0483492910861969, acc: 0.9938650131225586)
[2025-02-13 19:57:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:17][root][INFO] - Training Epoch: 2/2, step 448/7134 completed (loss: 0.23830118775367737, acc: 0.9470899701118469)
[2025-02-13 19:57:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:18][root][INFO] - Training Epoch: 2/2, step 449/7134 completed (loss: 0.17163914442062378, acc: 0.9468085169792175)
[2025-02-13 19:57:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:18][root][INFO] - Training Epoch: 2/2, step 450/7134 completed (loss: 0.13333410024642944, acc: 0.9622641801834106)
[2025-02-13 19:57:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:18][root][INFO] - Training Epoch: 2/2, step 451/7134 completed (loss: 0.2854679822921753, acc: 0.9279661178588867)
[2025-02-13 19:57:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:19][root][INFO] - Training Epoch: 2/2, step 452/7134 completed (loss: 0.2513054311275482, acc: 0.9438202381134033)
[2025-02-13 19:57:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:19][root][INFO] - Training Epoch: 2/2, step 453/7134 completed (loss: 0.14506256580352783, acc: 0.9503546357154846)
[2025-02-13 19:57:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:20][root][INFO] - Training Epoch: 2/2, step 454/7134 completed (loss: 0.24823860824108124, acc: 0.9409282803535461)
[2025-02-13 19:57:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:20][root][INFO] - Training Epoch: 2/2, step 455/7134 completed (loss: 0.19008393585681915, acc: 0.9487179517745972)
[2025-02-13 19:57:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:20][root][INFO] - Training Epoch: 2/2, step 456/7134 completed (loss: 0.07535593956708908, acc: 0.987730085849762)
[2025-02-13 19:57:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:21][root][INFO] - Training Epoch: 2/2, step 457/7134 completed (loss: 0.24376341700553894, acc: 0.9586206674575806)
[2025-02-13 19:57:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:21][root][INFO] - Training Epoch: 2/2, step 458/7134 completed (loss: 0.1435602307319641, acc: 0.9435483813285828)
[2025-02-13 19:57:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:22][root][INFO] - Training Epoch: 2/2, step 459/7134 completed (loss: 0.1685539036989212, acc: 0.9518072009086609)
[2025-02-13 19:57:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:22][root][INFO] - Training Epoch: 2/2, step 460/7134 completed (loss: 0.13658346235752106, acc: 0.9666666388511658)
[2025-02-13 19:57:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:22][root][INFO] - Training Epoch: 2/2, step 461/7134 completed (loss: 0.132165789604187, acc: 0.9529411792755127)
[2025-02-13 19:57:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:23][root][INFO] - Training Epoch: 2/2, step 462/7134 completed (loss: 0.16738109290599823, acc: 0.9675925970077515)
[2025-02-13 19:57:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:23][root][INFO] - Training Epoch: 2/2, step 463/7134 completed (loss: 0.3548928499221802, acc: 0.9353233575820923)
[2025-02-13 19:57:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:24][root][INFO] - Training Epoch: 2/2, step 464/7134 completed (loss: 0.14981523156166077, acc: 0.9659090638160706)
[2025-02-13 19:57:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:24][root][INFO] - Training Epoch: 2/2, step 465/7134 completed (loss: 0.30013608932495117, acc: 0.9553072452545166)
[2025-02-13 19:57:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:24][root][INFO] - Training Epoch: 2/2, step 466/7134 completed (loss: 0.1977057158946991, acc: 0.9473684430122375)
[2025-02-13 19:57:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:25][root][INFO] - Training Epoch: 2/2, step 467/7134 completed (loss: 0.2646007239818573, acc: 0.9523809552192688)
[2025-02-13 19:57:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:25][root][INFO] - Training Epoch: 2/2, step 468/7134 completed (loss: 0.20612646639347076, acc: 0.9399999976158142)
[2025-02-13 19:57:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:25][root][INFO] - Training Epoch: 2/2, step 469/7134 completed (loss: 0.21650844812393188, acc: 0.9513888955116272)
[2025-02-13 19:57:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:26][root][INFO] - Training Epoch: 2/2, step 470/7134 completed (loss: 0.2213934361934662, acc: 0.9459459185600281)
[2025-02-13 19:57:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:26][root][INFO] - Training Epoch: 2/2, step 471/7134 completed (loss: 0.2223498523235321, acc: 0.9560439586639404)
[2025-02-13 19:57:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:27][root][INFO] - Training Epoch: 2/2, step 472/7134 completed (loss: 0.19334273040294647, acc: 0.9666666388511658)
[2025-02-13 19:57:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:27][root][INFO] - Training Epoch: 2/2, step 473/7134 completed (loss: 0.22409658133983612, acc: 0.9469696879386902)
[2025-02-13 19:57:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:27][root][INFO] - Training Epoch: 2/2, step 474/7134 completed (loss: 0.2732762396335602, acc: 0.9575757384300232)
[2025-02-13 19:57:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:28][root][INFO] - Training Epoch: 2/2, step 475/7134 completed (loss: 0.2755224406719208, acc: 0.9580419659614563)
[2025-02-13 19:57:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:28][root][INFO] - Training Epoch: 2/2, step 476/7134 completed (loss: 0.25393879413604736, acc: 0.949999988079071)
[2025-02-13 19:57:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:28][root][INFO] - Training Epoch: 2/2, step 477/7134 completed (loss: 0.2097073346376419, acc: 0.9411764740943909)
[2025-02-13 19:57:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:29][root][INFO] - Training Epoch: 2/2, step 478/7134 completed (loss: 0.3008657991886139, acc: 0.9217391014099121)
[2025-02-13 19:57:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:29][root][INFO] - Training Epoch: 2/2, step 479/7134 completed (loss: 0.21312347054481506, acc: 0.9271523356437683)
[2025-02-13 19:57:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:30][root][INFO] - Training Epoch: 2/2, step 480/7134 completed (loss: 0.15250974893569946, acc: 0.949999988079071)
[2025-02-13 19:57:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:30][root][INFO] - Training Epoch: 2/2, step 481/7134 completed (loss: 0.13677139580249786, acc: 0.9813664555549622)
[2025-02-13 19:57:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:30][root][INFO] - Training Epoch: 2/2, step 482/7134 completed (loss: 0.13030961155891418, acc: 0.955974817276001)
[2025-02-13 19:57:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:31][root][INFO] - Training Epoch: 2/2, step 483/7134 completed (loss: 0.09804431349039078, acc: 0.9725274443626404)
[2025-02-13 19:57:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:31][root][INFO] - Training Epoch: 2/2, step 484/7134 completed (loss: 0.23778952658176422, acc: 0.9285714030265808)
[2025-02-13 19:57:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:31][root][INFO] - Training Epoch: 2/2, step 485/7134 completed (loss: 0.2871086597442627, acc: 0.9192546606063843)
[2025-02-13 19:57:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:32][root][INFO] - Training Epoch: 2/2, step 486/7134 completed (loss: 0.20014159381389618, acc: 0.948387086391449)
[2025-02-13 19:57:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:32][root][INFO] - Training Epoch: 2/2, step 487/7134 completed (loss: 0.14321781694889069, acc: 0.9691358208656311)
[2025-02-13 19:57:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:33][root][INFO] - Training Epoch: 2/2, step 488/7134 completed (loss: 0.300883024930954, acc: 0.9552238583564758)
[2025-02-13 19:57:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:33][root][INFO] - Training Epoch: 2/2, step 489/7134 completed (loss: 0.4180999994277954, acc: 0.9496402740478516)
[2025-02-13 19:57:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:33][root][INFO] - Training Epoch: 2/2, step 490/7134 completed (loss: 0.12743200361728668, acc: 0.9810126423835754)
[2025-02-13 19:57:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:34][root][INFO] - Training Epoch: 2/2, step 491/7134 completed (loss: 0.16567537188529968, acc: 0.9513888955116272)
[2025-02-13 19:57:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:34][root][INFO] - Training Epoch: 2/2, step 492/7134 completed (loss: 0.07886447757482529, acc: 0.9806451797485352)
[2025-02-13 19:57:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:34][root][INFO] - Training Epoch: 2/2, step 493/7134 completed (loss: 0.08892982453107834, acc: 0.9704142212867737)
[2025-02-13 19:57:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:35][root][INFO] - Training Epoch: 2/2, step 494/7134 completed (loss: 0.03840544819831848, acc: 0.9933333396911621)
[2025-02-13 19:57:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:35][root][INFO] - Training Epoch: 2/2, step 495/7134 completed (loss: 0.12506455183029175, acc: 0.9752066135406494)
[2025-02-13 19:57:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:36][root][INFO] - Training Epoch: 2/2, step 496/7134 completed (loss: 0.08984193205833435, acc: 0.9684210419654846)
[2025-02-13 19:57:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:36][root][INFO] - Training Epoch: 2/2, step 497/7134 completed (loss: 0.14031726121902466, acc: 0.9615384340286255)
[2025-02-13 19:57:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:36][root][INFO] - Training Epoch: 2/2, step 498/7134 completed (loss: 0.1289551705121994, acc: 0.9731543660163879)
[2025-02-13 19:57:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:37][root][INFO] - Training Epoch: 2/2, step 499/7134 completed (loss: 0.0679595023393631, acc: 0.9823529124259949)
[2025-02-13 19:57:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:37][root][INFO] - Training Epoch: 2/2, step 500/7134 completed (loss: 0.08733685314655304, acc: 0.9767441749572754)
[2025-02-13 19:57:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:38][root][INFO] - Training Epoch: 2/2, step 501/7134 completed (loss: 0.06423213332891464, acc: 0.9884393215179443)
[2025-02-13 19:57:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:38][root][INFO] - Training Epoch: 2/2, step 502/7134 completed (loss: 0.06918586790561676, acc: 0.9805194735527039)
[2025-02-13 19:57:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:38][root][INFO] - Training Epoch: 2/2, step 503/7134 completed (loss: 0.15812614560127258, acc: 0.9659090638160706)
[2025-02-13 19:57:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:39][root][INFO] - Training Epoch: 2/2, step 504/7134 completed (loss: 0.11335530132055283, acc: 0.9751552939414978)
[2025-02-13 19:57:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:39][root][INFO] - Training Epoch: 2/2, step 505/7134 completed (loss: 0.09006790071725845, acc: 0.988095223903656)
[2025-02-13 19:57:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:40][root][INFO] - Training Epoch: 2/2, step 506/7134 completed (loss: 0.16143016517162323, acc: 0.9772727489471436)
[2025-02-13 19:57:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:40][root][INFO] - Training Epoch: 2/2, step 507/7134 completed (loss: 0.09251832962036133, acc: 0.9726775884628296)
[2025-02-13 19:57:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:40][root][INFO] - Training Epoch: 2/2, step 508/7134 completed (loss: 0.12137646228075027, acc: 0.9647058844566345)
[2025-02-13 19:57:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:41][root][INFO] - Training Epoch: 2/2, step 509/7134 completed (loss: 0.23308806121349335, acc: 0.9256756901741028)
[2025-02-13 19:57:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:41][root][INFO] - Training Epoch: 2/2, step 510/7134 completed (loss: 0.31244900822639465, acc: 0.9268292784690857)
[2025-02-13 19:57:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:41][root][INFO] - Training Epoch: 2/2, step 511/7134 completed (loss: 0.18181753158569336, acc: 0.9523809552192688)
[2025-02-13 19:57:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:42][root][INFO] - Training Epoch: 2/2, step 512/7134 completed (loss: 0.2840760350227356, acc: 0.9083969593048096)
[2025-02-13 19:57:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:42][root][INFO] - Training Epoch: 2/2, step 513/7134 completed (loss: 0.28720414638519287, acc: 0.9395973086357117)
[2025-02-13 19:57:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:43][root][INFO] - Training Epoch: 2/2, step 514/7134 completed (loss: 0.33577415347099304, acc: 0.9351851940155029)
[2025-02-13 19:57:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:43][root][INFO] - Training Epoch: 2/2, step 515/7134 completed (loss: 0.14805914461612701, acc: 0.9642857313156128)
[2025-02-13 19:57:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:43][root][INFO] - Training Epoch: 2/2, step 516/7134 completed (loss: 0.19106563925743103, acc: 0.9576271176338196)
[2025-02-13 19:57:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:44][root][INFO] - Training Epoch: 2/2, step 517/7134 completed (loss: 0.3226626515388489, acc: 0.9420289993286133)
[2025-02-13 19:57:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:44][root][INFO] - Training Epoch: 2/2, step 518/7134 completed (loss: 0.13841944932937622, acc: 0.9801324605941772)
[2025-02-13 19:57:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:44][root][INFO] - Training Epoch: 2/2, step 519/7134 completed (loss: 0.11610652506351471, acc: 0.9739130139350891)
[2025-02-13 19:57:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:45][root][INFO] - Training Epoch: 2/2, step 520/7134 completed (loss: 0.06689202040433884, acc: 0.9868420958518982)
[2025-02-13 19:57:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:45][root][INFO] - Training Epoch: 2/2, step 521/7134 completed (loss: 0.17293089628219604, acc: 0.9707602262496948)
[2025-02-13 19:57:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:46][root][INFO] - Training Epoch: 2/2, step 522/7134 completed (loss: 0.11769679188728333, acc: 0.9719101190567017)
[2025-02-13 19:57:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:46][root][INFO] - Training Epoch: 2/2, step 523/7134 completed (loss: 0.12211967259645462, acc: 0.9709302186965942)
[2025-02-13 19:57:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:46][root][INFO] - Training Epoch: 2/2, step 524/7134 completed (loss: 0.15945346653461456, acc: 0.9717513918876648)
[2025-02-13 19:57:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:47][root][INFO] - Training Epoch: 2/2, step 525/7134 completed (loss: 0.17582182586193085, acc: 0.9754601120948792)
[2025-02-13 19:57:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:47][root][INFO] - Training Epoch: 2/2, step 526/7134 completed (loss: 0.11909856647253036, acc: 0.9659090638160706)
[2025-02-13 19:57:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:47][root][INFO] - Training Epoch: 2/2, step 527/7134 completed (loss: 0.14461661875247955, acc: 0.96875)
[2025-02-13 19:57:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:48][root][INFO] - Training Epoch: 2/2, step 528/7134 completed (loss: 0.07936938107013702, acc: 0.9741379022598267)
[2025-02-13 19:57:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:48][root][INFO] - Training Epoch: 2/2, step 529/7134 completed (loss: 0.19041751325130463, acc: 0.9426751732826233)
[2025-02-13 19:57:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:49][root][INFO] - Training Epoch: 2/2, step 530/7134 completed (loss: 0.1594693958759308, acc: 0.9461538195610046)
[2025-02-13 19:57:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:49][root][INFO] - Training Epoch: 2/2, step 531/7134 completed (loss: 0.3102378845214844, acc: 0.9383561611175537)
[2025-02-13 19:57:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:49][root][INFO] - Training Epoch: 2/2, step 532/7134 completed (loss: 0.049396637827157974, acc: 0.991150438785553)
[2025-02-13 19:57:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:50][root][INFO] - Training Epoch: 2/2, step 533/7134 completed (loss: 0.30015185475349426, acc: 0.918367326259613)
[2025-02-13 19:57:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:50][root][INFO] - Training Epoch: 2/2, step 534/7134 completed (loss: 0.09282344579696655, acc: 0.976047933101654)
[2025-02-13 19:57:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:51][root][INFO] - Training Epoch: 2/2, step 535/7134 completed (loss: 0.186459481716156, acc: 0.956204354763031)
[2025-02-13 19:57:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:51][root][INFO] - Training Epoch: 2/2, step 536/7134 completed (loss: 0.08055298775434494, acc: 0.9878048896789551)
[2025-02-13 19:57:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:51][root][INFO] - Training Epoch: 2/2, step 537/7134 completed (loss: 0.1448134183883667, acc: 0.9653179049491882)
[2025-02-13 19:57:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:52][root][INFO] - Training Epoch: 2/2, step 538/7134 completed (loss: 0.11304234713315964, acc: 0.9759036302566528)
[2025-02-13 19:57:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:52][root][INFO] - Training Epoch: 2/2, step 539/7134 completed (loss: 0.1421593725681305, acc: 0.9694656729698181)
[2025-02-13 19:57:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:53][root][INFO] - Training Epoch: 2/2, step 540/7134 completed (loss: 0.19247488677501678, acc: 0.9433962106704712)
[2025-02-13 19:57:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:53][root][INFO] - Training Epoch: 2/2, step 541/7134 completed (loss: 0.2136038988828659, acc: 0.9523809552192688)
[2025-02-13 19:57:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:53][root][INFO] - Training Epoch: 2/2, step 542/7134 completed (loss: 0.3249116837978363, acc: 0.9399999976158142)
[2025-02-13 19:57:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:54][root][INFO] - Training Epoch: 2/2, step 543/7134 completed (loss: 0.38054001331329346, acc: 0.8815789222717285)
[2025-02-13 19:57:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:54][root][INFO] - Training Epoch: 2/2, step 544/7134 completed (loss: 0.1969948261976242, acc: 0.949999988079071)
[2025-02-13 19:57:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:55][root][INFO] - Training Epoch: 2/2, step 545/7134 completed (loss: 0.30055972933769226, acc: 0.9398496150970459)
[2025-02-13 19:57:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:55][root][INFO] - Training Epoch: 2/2, step 546/7134 completed (loss: 0.3922966718673706, acc: 0.8796296119689941)
[2025-02-13 19:57:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:55][root][INFO] - Training Epoch: 2/2, step 547/7134 completed (loss: 0.1726980209350586, acc: 0.9735099077224731)
[2025-02-13 19:57:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:56][root][INFO] - Training Epoch: 2/2, step 548/7134 completed (loss: 0.1378994882106781, acc: 0.9605262875556946)
[2025-02-13 19:57:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:56][root][INFO] - Training Epoch: 2/2, step 549/7134 completed (loss: 0.3600519299507141, acc: 0.8999999761581421)
[2025-02-13 19:57:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:56][root][INFO] - Training Epoch: 2/2, step 550/7134 completed (loss: 0.17695969343185425, acc: 0.960629940032959)
[2025-02-13 19:57:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:57][root][INFO] - Training Epoch: 2/2, step 551/7134 completed (loss: 0.22930502891540527, acc: 0.9375)
[2025-02-13 19:57:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:57][root][INFO] - Training Epoch: 2/2, step 552/7134 completed (loss: 0.1637086570262909, acc: 0.9591836929321289)
[2025-02-13 19:57:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:57][root][INFO] - Training Epoch: 2/2, step 553/7134 completed (loss: 0.16449449956417084, acc: 0.9368420839309692)
[2025-02-13 19:57:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:58][root][INFO] - Training Epoch: 2/2, step 554/7134 completed (loss: 0.3195442259311676, acc: 0.9363057613372803)
[2025-02-13 19:57:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:58][root][INFO] - Training Epoch: 2/2, step 555/7134 completed (loss: 0.3652602732181549, acc: 0.9155844449996948)
[2025-02-13 19:57:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:59][root][INFO] - Training Epoch: 2/2, step 556/7134 completed (loss: 0.19508302211761475, acc: 0.949999988079071)
[2025-02-13 19:57:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:59][root][INFO] - Training Epoch: 2/2, step 557/7134 completed (loss: 0.17226846516132355, acc: 0.9503546357154846)
[2025-02-13 19:57:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:57:59][root][INFO] - Training Epoch: 2/2, step 558/7134 completed (loss: 0.22339287400245667, acc: 0.9551281929016113)
[2025-02-13 19:58:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:00][root][INFO] - Training Epoch: 2/2, step 559/7134 completed (loss: 0.2976917028427124, acc: 0.926174521446228)
[2025-02-13 19:58:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:00][root][INFO] - Training Epoch: 2/2, step 560/7134 completed (loss: 0.0996747687458992, acc: 0.9756097793579102)
[2025-02-13 19:58:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:00][root][INFO] - Training Epoch: 2/2, step 561/7134 completed (loss: 0.20637452602386475, acc: 0.949999988079071)
[2025-02-13 19:58:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:01][root][INFO] - Training Epoch: 2/2, step 562/7134 completed (loss: 0.1793275624513626, acc: 0.9285714030265808)
[2025-02-13 19:58:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:01][root][INFO] - Training Epoch: 2/2, step 563/7134 completed (loss: 0.38022637367248535, acc: 0.9230769276618958)
[2025-02-13 19:58:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:02][root][INFO] - Training Epoch: 2/2, step 564/7134 completed (loss: 0.3408384621143341, acc: 0.9532163739204407)
[2025-02-13 19:58:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:02][root][INFO] - Training Epoch: 2/2, step 565/7134 completed (loss: 0.2098069190979004, acc: 0.9527559280395508)
[2025-02-13 19:58:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:02][root][INFO] - Training Epoch: 2/2, step 566/7134 completed (loss: 0.30200067162513733, acc: 0.9189189076423645)
[2025-02-13 19:58:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:03][root][INFO] - Training Epoch: 2/2, step 567/7134 completed (loss: 0.26266559958457947, acc: 0.9281437397003174)
[2025-02-13 19:58:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:03][root][INFO] - Training Epoch: 2/2, step 568/7134 completed (loss: 0.18813934922218323, acc: 0.9736841917037964)
[2025-02-13 19:58:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:04][root][INFO] - Training Epoch: 2/2, step 569/7134 completed (loss: 0.11712430417537689, acc: 0.9776536226272583)
[2025-02-13 19:58:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:04][root][INFO] - Training Epoch: 2/2, step 570/7134 completed (loss: 0.22169487178325653, acc: 0.93034827709198)
[2025-02-13 19:58:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:05][root][INFO] - Training Epoch: 2/2, step 571/7134 completed (loss: 0.0886172279715538, acc: 0.9890109896659851)
[2025-02-13 19:58:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:05][root][INFO] - Training Epoch: 2/2, step 572/7134 completed (loss: 0.0920170471072197, acc: 0.977142870426178)
[2025-02-13 19:58:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:05][root][INFO] - Training Epoch: 2/2, step 573/7134 completed (loss: 0.11261129379272461, acc: 0.9757575988769531)
[2025-02-13 19:58:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:06][root][INFO] - Training Epoch: 2/2, step 574/7134 completed (loss: 0.11409039795398712, acc: 0.9585798978805542)
[2025-02-13 19:58:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:06][root][INFO] - Training Epoch: 2/2, step 575/7134 completed (loss: 0.15012095868587494, acc: 0.9766082167625427)
[2025-02-13 19:58:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:06][root][INFO] - Training Epoch: 2/2, step 576/7134 completed (loss: 0.11475736647844315, acc: 0.9623655676841736)
[2025-02-13 19:58:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:07][root][INFO] - Training Epoch: 2/2, step 577/7134 completed (loss: 0.14013832807540894, acc: 0.9636363387107849)
[2025-02-13 19:58:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:07][root][INFO] - Training Epoch: 2/2, step 578/7134 completed (loss: 0.13097892701625824, acc: 0.9701492786407471)
[2025-02-13 19:58:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:08][root][INFO] - Training Epoch: 2/2, step 579/7134 completed (loss: 0.15677107870578766, acc: 0.9677419066429138)
[2025-02-13 19:58:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:08][root][INFO] - Training Epoch: 2/2, step 580/7134 completed (loss: 0.48764994740486145, acc: 0.9111111164093018)
[2025-02-13 19:58:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:08][root][INFO] - Training Epoch: 2/2, step 581/7134 completed (loss: 0.06256360560655594, acc: 0.9802631735801697)
[2025-02-13 19:58:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:09][root][INFO] - Training Epoch: 2/2, step 582/7134 completed (loss: 0.11941459774971008, acc: 0.97826087474823)
[2025-02-13 19:58:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:09][root][INFO] - Training Epoch: 2/2, step 583/7134 completed (loss: 0.10604896396398544, acc: 0.9820359349250793)
[2025-02-13 19:58:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:10][root][INFO] - Training Epoch: 2/2, step 584/7134 completed (loss: 0.08890029788017273, acc: 0.9836065769195557)
[2025-02-13 19:58:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:10][root][INFO] - Training Epoch: 2/2, step 585/7134 completed (loss: 0.08618618547916412, acc: 0.9891892075538635)
[2025-02-13 19:58:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:10][root][INFO] - Training Epoch: 2/2, step 586/7134 completed (loss: 0.09193292260169983, acc: 0.9680851101875305)
[2025-02-13 19:58:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:11][root][INFO] - Training Epoch: 2/2, step 587/7134 completed (loss: 0.12452109903097153, acc: 0.9668508172035217)
[2025-02-13 19:58:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:11][root][INFO] - Training Epoch: 2/2, step 588/7134 completed (loss: 0.08562660962343216, acc: 0.9729729890823364)
[2025-02-13 19:58:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:12][root][INFO] - Training Epoch: 2/2, step 589/7134 completed (loss: 0.18691548705101013, acc: 0.9693251252174377)
[2025-02-13 19:58:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:12][root][INFO] - Training Epoch: 2/2, step 590/7134 completed (loss: 0.12150873988866806, acc: 0.9735099077224731)
[2025-02-13 19:58:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:12][root][INFO] - Training Epoch: 2/2, step 591/7134 completed (loss: 0.1232265830039978, acc: 0.9887005686759949)
[2025-02-13 19:58:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:13][root][INFO] - Training Epoch: 2/2, step 592/7134 completed (loss: 0.08304169028997421, acc: 0.9829545617103577)
[2025-02-13 19:58:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:13][root][INFO] - Training Epoch: 2/2, step 593/7134 completed (loss: 0.11935672163963318, acc: 0.9824561476707458)
[2025-02-13 19:58:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:13][root][INFO] - Training Epoch: 2/2, step 594/7134 completed (loss: 0.2238781601190567, acc: 0.9580838084220886)
[2025-02-13 19:58:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:14][root][INFO] - Training Epoch: 2/2, step 595/7134 completed (loss: 0.16448469460010529, acc: 0.9677419066429138)
[2025-02-13 19:58:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:14][root][INFO] - Training Epoch: 2/2, step 596/7134 completed (loss: 0.08762300759553909, acc: 0.9777777791023254)
[2025-02-13 19:58:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:15][root][INFO] - Training Epoch: 2/2, step 597/7134 completed (loss: 0.19989867508411407, acc: 0.9608938694000244)
[2025-02-13 19:58:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:15][root][INFO] - Training Epoch: 2/2, step 598/7134 completed (loss: 0.25159451365470886, acc: 0.95652174949646)
[2025-02-13 19:58:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:15][root][INFO] - Training Epoch: 2/2, step 599/7134 completed (loss: 0.14675776660442352, acc: 0.9731183052062988)
[2025-02-13 19:58:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:16][root][INFO] - Training Epoch: 2/2, step 600/7134 completed (loss: 0.14094693958759308, acc: 0.9508196711540222)
[2025-02-13 19:58:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:16][root][INFO] - Training Epoch: 2/2, step 601/7134 completed (loss: 0.12034719437360764, acc: 0.9659090638160706)
[2025-02-13 19:58:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:17][root][INFO] - Training Epoch: 2/2, step 602/7134 completed (loss: 0.13366714119911194, acc: 0.9621621370315552)
[2025-02-13 19:58:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:17][root][INFO] - Training Epoch: 2/2, step 603/7134 completed (loss: 0.17598506808280945, acc: 0.9523809552192688)
[2025-02-13 19:58:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:17][root][INFO] - Training Epoch: 2/2, step 604/7134 completed (loss: 0.16590352356433868, acc: 0.9696969985961914)
[2025-02-13 19:58:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:18][root][INFO] - Training Epoch: 2/2, step 605/7134 completed (loss: 0.10365685075521469, acc: 0.9786096215248108)
[2025-02-13 19:58:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:18][root][INFO] - Training Epoch: 2/2, step 606/7134 completed (loss: 0.10670546442270279, acc: 0.9700000286102295)
[2025-02-13 19:58:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:19][root][INFO] - Training Epoch: 2/2, step 607/7134 completed (loss: 0.10441052168607712, acc: 0.9712643623352051)
[2025-02-13 19:58:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:19][root][INFO] - Training Epoch: 2/2, step 608/7134 completed (loss: 0.056418873369693756, acc: 0.9837837815284729)
[2025-02-13 19:58:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:19][root][INFO] - Training Epoch: 2/2, step 609/7134 completed (loss: 0.11706886440515518, acc: 0.9743589758872986)
[2025-02-13 19:58:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:20][root][INFO] - Training Epoch: 2/2, step 610/7134 completed (loss: 0.09533847123384476, acc: 0.9901477694511414)
[2025-02-13 19:58:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:20][root][INFO] - Training Epoch: 2/2, step 611/7134 completed (loss: 0.15050749480724335, acc: 0.9661017060279846)
[2025-02-13 19:58:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:20][root][INFO] - Training Epoch: 2/2, step 612/7134 completed (loss: 0.162067711353302, acc: 0.95652174949646)
[2025-02-13 19:58:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:21][root][INFO] - Training Epoch: 2/2, step 613/7134 completed (loss: 0.1558045595884323, acc: 0.9595959782600403)
[2025-02-13 19:58:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:21][root][INFO] - Training Epoch: 2/2, step 614/7134 completed (loss: 0.038891926407814026, acc: 0.989130437374115)
[2025-02-13 19:58:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:22][root][INFO] - Training Epoch: 2/2, step 615/7134 completed (loss: 0.06353588402271271, acc: 0.9894737005233765)
[2025-02-13 19:58:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:22][root][INFO] - Training Epoch: 2/2, step 616/7134 completed (loss: 0.09275511652231216, acc: 0.981566846370697)
[2025-02-13 19:58:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:22][root][INFO] - Training Epoch: 2/2, step 617/7134 completed (loss: 0.09638229757547379, acc: 0.9760765433311462)
[2025-02-13 19:58:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:23][root][INFO] - Training Epoch: 2/2, step 618/7134 completed (loss: 0.06848776340484619, acc: 0.9839572310447693)
[2025-02-13 19:58:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:23][root][INFO] - Training Epoch: 2/2, step 619/7134 completed (loss: 0.15046252310276031, acc: 0.9836065769195557)
[2025-02-13 19:58:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:23][root][INFO] - Training Epoch: 2/2, step 620/7134 completed (loss: 0.09206371009349823, acc: 0.965753436088562)
[2025-02-13 19:58:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:24][root][INFO] - Training Epoch: 2/2, step 621/7134 completed (loss: 0.16418495774269104, acc: 0.9733333587646484)
[2025-02-13 19:58:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:24][root][INFO] - Training Epoch: 2/2, step 622/7134 completed (loss: 0.20144939422607422, acc: 0.9539473652839661)
[2025-02-13 19:58:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:25][root][INFO] - Training Epoch: 2/2, step 623/7134 completed (loss: 0.3384408950805664, acc: 0.9496402740478516)
[2025-02-13 19:58:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:25][root][INFO] - Training Epoch: 2/2, step 624/7134 completed (loss: 0.12420183420181274, acc: 0.985401451587677)
[2025-02-13 19:58:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:25][root][INFO] - Training Epoch: 2/2, step 625/7134 completed (loss: 0.1887495517730713, acc: 0.949999988079071)
[2025-02-13 19:58:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:26][root][INFO] - Training Epoch: 2/2, step 626/7134 completed (loss: 0.1338910460472107, acc: 0.9760000109672546)
[2025-02-13 19:58:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:26][root][INFO] - Training Epoch: 2/2, step 627/7134 completed (loss: 0.10986844450235367, acc: 0.9659863710403442)
[2025-02-13 19:58:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:27][root][INFO] - Training Epoch: 2/2, step 628/7134 completed (loss: 0.08661432564258575, acc: 0.9727891087532043)
[2025-02-13 19:58:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:27][root][INFO] - Training Epoch: 2/2, step 629/7134 completed (loss: 0.19196587800979614, acc: 0.9645389914512634)
[2025-02-13 19:58:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:27][root][INFO] - Training Epoch: 2/2, step 630/7134 completed (loss: 0.18518568575382233, acc: 0.9490445852279663)
[2025-02-13 19:58:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:28][root][INFO] - Training Epoch: 2/2, step 631/7134 completed (loss: 0.1392895132303238, acc: 0.9740259647369385)
[2025-02-13 19:58:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:28][root][INFO] - Training Epoch: 2/2, step 632/7134 completed (loss: 0.08331901580095291, acc: 0.9788732528686523)
[2025-02-13 19:58:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:29][root][INFO] - Training Epoch: 2/2, step 633/7134 completed (loss: 0.13014280796051025, acc: 0.96875)
[2025-02-13 19:58:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:29][root][INFO] - Training Epoch: 2/2, step 634/7134 completed (loss: 0.19930382072925568, acc: 0.9740259647369385)
[2025-02-13 19:58:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:29][root][INFO] - Training Epoch: 2/2, step 635/7134 completed (loss: 0.147284135222435, acc: 0.9545454382896423)
[2025-02-13 19:58:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:30][root][INFO] - Training Epoch: 2/2, step 636/7134 completed (loss: 0.14687363803386688, acc: 0.970059871673584)
[2025-02-13 19:58:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:30][root][INFO] - Training Epoch: 2/2, step 637/7134 completed (loss: 0.10613656789064407, acc: 0.971222996711731)
[2025-02-13 19:58:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:30][root][INFO] - Training Epoch: 2/2, step 638/7134 completed (loss: 0.0868522971868515, acc: 0.9731543660163879)
[2025-02-13 19:58:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:31][root][INFO] - Training Epoch: 2/2, step 639/7134 completed (loss: 0.09921669960021973, acc: 0.9714285731315613)
[2025-02-13 19:58:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:31][root][INFO] - Training Epoch: 2/2, step 640/7134 completed (loss: 0.17643587291240692, acc: 0.95652174949646)
[2025-02-13 19:58:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:32][root][INFO] - Training Epoch: 2/2, step 641/7134 completed (loss: 0.09456134587526321, acc: 0.97826087474823)
[2025-02-13 19:58:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:32][root][INFO] - Training Epoch: 2/2, step 642/7134 completed (loss: 0.10426731407642365, acc: 0.9852941036224365)
[2025-02-13 19:58:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:32][root][INFO] - Training Epoch: 2/2, step 643/7134 completed (loss: 0.046780120581388474, acc: 0.9943820238113403)
[2025-02-13 19:58:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:33][root][INFO] - Training Epoch: 2/2, step 644/7134 completed (loss: 0.057123247534036636, acc: 0.984375)
[2025-02-13 19:58:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:33][root][INFO] - Training Epoch: 2/2, step 645/7134 completed (loss: 0.07768067717552185, acc: 0.9724137783050537)
[2025-02-13 19:58:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:34][root][INFO] - Training Epoch: 2/2, step 646/7134 completed (loss: 0.07721414417028427, acc: 0.9810126423835754)
[2025-02-13 19:58:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:34][root][INFO] - Training Epoch: 2/2, step 647/7134 completed (loss: 0.04667248949408531, acc: 0.9925925731658936)
[2025-02-13 19:58:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:34][root][INFO] - Training Epoch: 2/2, step 648/7134 completed (loss: 0.23616372048854828, acc: 0.9615384340286255)
[2025-02-13 19:58:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:35][root][INFO] - Training Epoch: 2/2, step 649/7134 completed (loss: 0.22547315061092377, acc: 0.9459459185600281)
[2025-02-13 19:58:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:35][root][INFO] - Training Epoch: 2/2, step 650/7134 completed (loss: 0.2796965539455414, acc: 0.9109588861465454)
[2025-02-13 19:58:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:35][root][INFO] - Training Epoch: 2/2, step 651/7134 completed (loss: 0.38221487402915955, acc: 0.9350649118423462)
[2025-02-13 19:58:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:36][root][INFO] - Training Epoch: 2/2, step 652/7134 completed (loss: 0.33292776346206665, acc: 0.9436619877815247)
[2025-02-13 19:58:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:36][root][INFO] - Training Epoch: 2/2, step 653/7134 completed (loss: 0.10331776738166809, acc: 0.9696969985961914)
[2025-02-13 19:58:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:36][root][INFO] - Training Epoch: 2/2, step 654/7134 completed (loss: 0.2499610185623169, acc: 0.9398496150970459)
[2025-02-13 19:58:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:37][root][INFO] - Training Epoch: 2/2, step 655/7134 completed (loss: 0.09039870649576187, acc: 0.9741379022598267)
[2025-02-13 19:58:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:37][root][INFO] - Training Epoch: 2/2, step 656/7134 completed (loss: 0.1690404862165451, acc: 0.9595959782600403)
[2025-02-13 19:58:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:38][root][INFO] - Training Epoch: 2/2, step 657/7134 completed (loss: 0.16821856796741486, acc: 0.9577465057373047)
[2025-02-13 19:58:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:38][root][INFO] - Training Epoch: 2/2, step 658/7134 completed (loss: 0.12219437956809998, acc: 0.9661017060279846)
[2025-02-13 19:58:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:38][root][INFO] - Training Epoch: 2/2, step 659/7134 completed (loss: 0.07769203186035156, acc: 0.9820359349250793)
[2025-02-13 19:58:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:39][root][INFO] - Training Epoch: 2/2, step 660/7134 completed (loss: 0.18027259409427643, acc: 0.9529411792755127)
[2025-02-13 19:58:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:39][root][INFO] - Training Epoch: 2/2, step 661/7134 completed (loss: 0.18226927518844604, acc: 0.944915235042572)
[2025-02-13 19:58:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:40][root][INFO] - Training Epoch: 2/2, step 662/7134 completed (loss: 0.13926340639591217, acc: 0.9611111283302307)
[2025-02-13 19:58:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:40][root][INFO] - Training Epoch: 2/2, step 663/7134 completed (loss: 0.15376237034797668, acc: 0.9611650705337524)
[2025-02-13 19:58:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:40][root][INFO] - Training Epoch: 2/2, step 664/7134 completed (loss: 0.2090592086315155, acc: 0.9370629191398621)
[2025-02-13 19:58:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:41][root][INFO] - Training Epoch: 2/2, step 665/7134 completed (loss: 0.3212023079395294, acc: 0.9453551769256592)
[2025-02-13 19:58:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:41][root][INFO] - Training Epoch: 2/2, step 666/7134 completed (loss: 0.1360420137643814, acc: 0.9657142758369446)
[2025-02-13 19:58:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:41][root][INFO] - Training Epoch: 2/2, step 667/7134 completed (loss: 0.29788675904273987, acc: 0.940119743347168)
[2025-02-13 19:58:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:42][root][INFO] - Training Epoch: 2/2, step 668/7134 completed (loss: 0.10487992316484451, acc: 0.9726775884628296)
[2025-02-13 19:58:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:42][root][INFO] - Training Epoch: 2/2, step 669/7134 completed (loss: 0.18224695324897766, acc: 0.9664804339408875)
[2025-02-13 19:58:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:43][root][INFO] - Training Epoch: 2/2, step 670/7134 completed (loss: 0.1862419843673706, acc: 0.9589040875434875)
[2025-02-13 19:58:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:43][root][INFO] - Training Epoch: 2/2, step 671/7134 completed (loss: 0.12140851467847824, acc: 0.9683257937431335)
[2025-02-13 19:58:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:43][root][INFO] - Training Epoch: 2/2, step 672/7134 completed (loss: 0.20335045456886292, acc: 0.96875)
[2025-02-13 19:58:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:44][root][INFO] - Training Epoch: 2/2, step 673/7134 completed (loss: 0.24094024300575256, acc: 0.9353233575820923)
[2025-02-13 19:58:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:44][root][INFO] - Training Epoch: 2/2, step 674/7134 completed (loss: 0.09314150363206863, acc: 0.9695122241973877)
[2025-02-13 19:58:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:44][root][INFO] - Training Epoch: 2/2, step 675/7134 completed (loss: 0.14212870597839355, acc: 0.9668508172035217)
[2025-02-13 19:58:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:45][root][INFO] - Training Epoch: 2/2, step 676/7134 completed (loss: 0.09023762494325638, acc: 0.989130437374115)
[2025-02-13 19:58:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:45][root][INFO] - Training Epoch: 2/2, step 677/7134 completed (loss: 0.11840689927339554, acc: 0.9735449552536011)
[2025-02-13 19:58:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:46][root][INFO] - Training Epoch: 2/2, step 678/7134 completed (loss: 0.12947651743888855, acc: 0.9634146094322205)
[2025-02-13 19:58:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:46][root][INFO] - Training Epoch: 2/2, step 679/7134 completed (loss: 0.05346636474132538, acc: 0.9924812316894531)
[2025-02-13 19:58:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:46][root][INFO] - Training Epoch: 2/2, step 680/7134 completed (loss: 0.14675675332546234, acc: 0.9648241400718689)
[2025-02-13 19:58:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:47][root][INFO] - Training Epoch: 2/2, step 681/7134 completed (loss: 0.15381915867328644, acc: 0.9684210419654846)
[2025-02-13 19:58:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:47][root][INFO] - Training Epoch: 2/2, step 682/7134 completed (loss: 0.05635266751050949, acc: 0.9887005686759949)
[2025-02-13 19:58:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:48][root][INFO] - Training Epoch: 2/2, step 683/7134 completed (loss: 0.08168650418519974, acc: 0.9729729890823364)
[2025-02-13 19:58:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:48][root][INFO] - Training Epoch: 2/2, step 684/7134 completed (loss: 0.12035881727933884, acc: 0.9722222089767456)
[2025-02-13 19:58:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:48][root][INFO] - Training Epoch: 2/2, step 685/7134 completed (loss: 0.10183773189783096, acc: 0.9698492288589478)
[2025-02-13 19:58:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:49][root][INFO] - Training Epoch: 2/2, step 686/7134 completed (loss: 0.022790607064962387, acc: 1.0)
[2025-02-13 19:58:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:49][root][INFO] - Training Epoch: 2/2, step 687/7134 completed (loss: 0.04421519488096237, acc: 0.9921259880065918)
[2025-02-13 19:58:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:49][root][INFO] - Training Epoch: 2/2, step 688/7134 completed (loss: 0.04136745259165764, acc: 0.9936708807945251)
[2025-02-13 19:58:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:50][root][INFO] - Training Epoch: 2/2, step 689/7134 completed (loss: 0.07344945520162582, acc: 0.9851852059364319)
[2025-02-13 19:58:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:50][root][INFO] - Training Epoch: 2/2, step 690/7134 completed (loss: 0.1894429326057434, acc: 0.9677419066429138)
[2025-02-13 19:58:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:51][root][INFO] - Training Epoch: 2/2, step 691/7134 completed (loss: 0.16034820675849915, acc: 0.9610389471054077)
[2025-02-13 19:58:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:51][root][INFO] - Training Epoch: 2/2, step 692/7134 completed (loss: 0.23703432083129883, acc: 0.9340101480484009)
[2025-02-13 19:58:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:51][root][INFO] - Training Epoch: 2/2, step 693/7134 completed (loss: 0.17926187813282013, acc: 0.9503546357154846)
[2025-02-13 19:58:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:52][root][INFO] - Training Epoch: 2/2, step 694/7134 completed (loss: 0.3383837044239044, acc: 0.9515151381492615)
[2025-02-13 19:58:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:52][root][INFO] - Training Epoch: 2/2, step 695/7134 completed (loss: 0.21252146363258362, acc: 0.9328358173370361)
[2025-02-13 19:58:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:53][root][INFO] - Training Epoch: 2/2, step 696/7134 completed (loss: 0.18729735910892487, acc: 0.9496855139732361)
[2025-02-13 19:58:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:53][root][INFO] - Training Epoch: 2/2, step 697/7134 completed (loss: 0.14091458916664124, acc: 0.95652174949646)
[2025-02-13 19:58:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:53][root][INFO] - Training Epoch: 2/2, step 698/7134 completed (loss: 0.05427682027220726, acc: 0.9924242496490479)
[2025-02-13 19:58:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:54][root][INFO] - Training Epoch: 2/2, step 699/7134 completed (loss: 0.1075739935040474, acc: 0.965753436088562)
[2025-02-13 19:58:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:54][root][INFO] - Training Epoch: 2/2, step 700/7134 completed (loss: 0.1607360690832138, acc: 0.9520000219345093)
[2025-02-13 19:58:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:54][root][INFO] - Training Epoch: 2/2, step 701/7134 completed (loss: 0.28440624475479126, acc: 0.9130434989929199)
[2025-02-13 19:58:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:55][root][INFO] - Training Epoch: 2/2, step 702/7134 completed (loss: 0.18167521059513092, acc: 0.940397322177887)
[2025-02-13 19:58:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:55][root][INFO] - Training Epoch: 2/2, step 703/7134 completed (loss: 0.17272593080997467, acc: 0.9527027010917664)
[2025-02-13 19:58:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:56][root][INFO] - Training Epoch: 2/2, step 704/7134 completed (loss: 0.20180034637451172, acc: 0.9545454382896423)
[2025-02-13 19:58:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:56][root][INFO] - Training Epoch: 2/2, step 705/7134 completed (loss: 0.24336549639701843, acc: 0.9311926364898682)
[2025-02-13 19:58:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:56][root][INFO] - Training Epoch: 2/2, step 706/7134 completed (loss: 0.11073622852563858, acc: 0.9716981053352356)
[2025-02-13 19:58:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:57][root][INFO] - Training Epoch: 2/2, step 707/7134 completed (loss: 0.1243223026394844, acc: 0.9851484894752502)
[2025-02-13 19:58:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:57][root][INFO] - Training Epoch: 2/2, step 708/7134 completed (loss: 0.10111743211746216, acc: 0.9719101190567017)
[2025-02-13 19:58:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:58][root][INFO] - Training Epoch: 2/2, step 709/7134 completed (loss: 0.1350117325782776, acc: 0.9599999785423279)
[2025-02-13 19:58:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:58][root][INFO] - Training Epoch: 2/2, step 710/7134 completed (loss: 0.10125991702079773, acc: 0.984375)
[2025-02-13 19:58:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:58][root][INFO] - Training Epoch: 2/2, step 711/7134 completed (loss: 0.11425802856683731, acc: 0.9558823704719543)
[2025-02-13 19:58:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:59][root][INFO] - Training Epoch: 2/2, step 712/7134 completed (loss: 0.04971885681152344, acc: 0.9895833134651184)
[2025-02-13 19:58:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:59][root][INFO] - Training Epoch: 2/2, step 713/7134 completed (loss: 0.06451111286878586, acc: 0.9849246144294739)
[2025-02-13 19:58:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:58:59][root][INFO] - Training Epoch: 2/2, step 714/7134 completed (loss: 0.11566528677940369, acc: 0.9800000190734863)
[2025-02-13 19:59:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:00][root][INFO] - Training Epoch: 2/2, step 715/7134 completed (loss: 0.046632181853055954, acc: 0.9836956262588501)
[2025-02-13 19:59:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:00][root][INFO] - Training Epoch: 2/2, step 716/7134 completed (loss: 0.05659094080328941, acc: 0.9852941036224365)
[2025-02-13 19:59:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:01][root][INFO] - Training Epoch: 2/2, step 717/7134 completed (loss: 0.11126247048377991, acc: 0.9732620120048523)
[2025-02-13 19:59:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:01][root][INFO] - Training Epoch: 2/2, step 718/7134 completed (loss: 0.05467674136161804, acc: 0.9788359999656677)
[2025-02-13 19:59:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:01][root][INFO] - Training Epoch: 2/2, step 719/7134 completed (loss: 0.06101316213607788, acc: 0.9929078221321106)
[2025-02-13 19:59:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:02][root][INFO] - Training Epoch: 2/2, step 720/7134 completed (loss: 0.06208045035600662, acc: 0.9837837815284729)
[2025-02-13 19:59:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:02][root][INFO] - Training Epoch: 2/2, step 721/7134 completed (loss: 0.05842076241970062, acc: 0.9894737005233765)
[2025-02-13 19:59:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:02][root][INFO] - Training Epoch: 2/2, step 722/7134 completed (loss: 0.10010629892349243, acc: 0.9696969985961914)
[2025-02-13 19:59:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:03][root][INFO] - Training Epoch: 2/2, step 723/7134 completed (loss: 0.049645014107227325, acc: 0.989130437374115)
[2025-02-13 19:59:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:03][root][INFO] - Training Epoch: 2/2, step 724/7134 completed (loss: 0.09711579233407974, acc: 0.9874213933944702)
[2025-02-13 19:59:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:04][root][INFO] - Training Epoch: 2/2, step 725/7134 completed (loss: 0.02849234640598297, acc: 1.0)
[2025-02-13 19:59:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:04][root][INFO] - Training Epoch: 2/2, step 726/7134 completed (loss: 0.029916470870375633, acc: 0.9888268113136292)
[2025-02-13 19:59:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:05][root][INFO] - Training Epoch: 2/2, step 727/7134 completed (loss: 0.10720707476139069, acc: 0.9826589822769165)
[2025-02-13 19:59:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:05][root][INFO] - Training Epoch: 2/2, step 728/7134 completed (loss: 0.028119929134845734, acc: 0.9942857027053833)
[2025-02-13 19:59:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:05][root][INFO] - Training Epoch: 2/2, step 729/7134 completed (loss: 0.04958128184080124, acc: 0.9873417615890503)
[2025-02-13 19:59:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:06][root][INFO] - Training Epoch: 2/2, step 730/7134 completed (loss: 0.06532245129346848, acc: 0.9893617033958435)
[2025-02-13 19:59:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:06][root][INFO] - Training Epoch: 2/2, step 731/7134 completed (loss: 0.012519801035523415, acc: 1.0)
[2025-02-13 19:59:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:07][root][INFO] - Training Epoch: 2/2, step 732/7134 completed (loss: 0.20285442471504211, acc: 0.9590163826942444)
[2025-02-13 19:59:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:07][root][INFO] - Training Epoch: 2/2, step 733/7134 completed (loss: 0.1520368754863739, acc: 0.9538461565971375)
[2025-02-13 19:59:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:07][root][INFO] - Training Epoch: 2/2, step 734/7134 completed (loss: 0.08366931974887848, acc: 0.9677419066429138)
[2025-02-13 19:59:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:08][root][INFO] - Training Epoch: 2/2, step 735/7134 completed (loss: 0.0629865750670433, acc: 0.98591548204422)
[2025-02-13 19:59:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:08][root][INFO] - Training Epoch: 2/2, step 736/7134 completed (loss: 0.2855895161628723, acc: 0.9504950642585754)
[2025-02-13 19:59:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:09][root][INFO] - Training Epoch: 2/2, step 737/7134 completed (loss: 0.30540820956230164, acc: 0.9237288236618042)
[2025-02-13 19:59:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:09][root][INFO] - Training Epoch: 2/2, step 738/7134 completed (loss: 0.05694537237286568, acc: 0.9905660152435303)
[2025-02-13 19:59:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:09][root][INFO] - Training Epoch: 2/2, step 739/7134 completed (loss: 0.07666243612766266, acc: 0.9826086759567261)
[2025-02-13 19:59:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:10][root][INFO] - Training Epoch: 2/2, step 740/7134 completed (loss: 0.03376743942499161, acc: 1.0)
[2025-02-13 19:59:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:10][root][INFO] - Training Epoch: 2/2, step 741/7134 completed (loss: 0.07050834596157074, acc: 0.9851852059364319)
[2025-02-13 19:59:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:10][root][INFO] - Training Epoch: 2/2, step 742/7134 completed (loss: 0.11299419403076172, acc: 0.9789473414421082)
[2025-02-13 19:59:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:11][root][INFO] - Training Epoch: 2/2, step 743/7134 completed (loss: 0.26461654901504517, acc: 0.9455782175064087)
[2025-02-13 19:59:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:11][root][INFO] - Training Epoch: 2/2, step 744/7134 completed (loss: 0.06374730914831161, acc: 0.9900000095367432)
[2025-02-13 19:59:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:12][root][INFO] - Training Epoch: 2/2, step 745/7134 completed (loss: 0.11547042429447174, acc: 0.970588207244873)
[2025-02-13 19:59:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:12][root][INFO] - Training Epoch: 2/2, step 746/7134 completed (loss: 0.1259172409772873, acc: 0.9770992398262024)
[2025-02-13 19:59:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:12][root][INFO] - Training Epoch: 2/2, step 747/7134 completed (loss: 0.23857633769512177, acc: 0.9370078444480896)
[2025-02-13 19:59:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:13][root][INFO] - Training Epoch: 2/2, step 748/7134 completed (loss: 0.14714708924293518, acc: 0.9626865386962891)
[2025-02-13 19:59:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:13][root][INFO] - Training Epoch: 2/2, step 749/7134 completed (loss: 0.09009329974651337, acc: 0.9714285731315613)
[2025-02-13 19:59:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:14][root][INFO] - Training Epoch: 2/2, step 750/7134 completed (loss: 0.08377783000469208, acc: 0.9834710955619812)
[2025-02-13 19:59:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:14][root][INFO] - Training Epoch: 2/2, step 751/7134 completed (loss: 0.1722298413515091, acc: 0.9599999785423279)
[2025-02-13 19:59:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:15][root][INFO] - Training Epoch: 2/2, step 752/7134 completed (loss: 0.23192250728607178, acc: 0.9729729890823364)
[2025-02-13 19:59:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:15][root][INFO] - Training Epoch: 2/2, step 753/7134 completed (loss: 0.07086959481239319, acc: 0.984375)
[2025-02-13 19:59:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:15][root][INFO] - Training Epoch: 2/2, step 754/7134 completed (loss: 0.09802613407373428, acc: 0.9710144996643066)
[2025-02-13 19:59:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:16][root][INFO] - Training Epoch: 2/2, step 755/7134 completed (loss: 0.1647222340106964, acc: 0.9679999947547913)
[2025-02-13 19:59:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:16][root][INFO] - Training Epoch: 2/2, step 756/7134 completed (loss: 0.05012141540646553, acc: 0.9910714030265808)
[2025-02-13 19:59:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:16][root][INFO] - Training Epoch: 2/2, step 757/7134 completed (loss: 0.01683632843196392, acc: 1.0)
[2025-02-13 19:59:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:17][root][INFO] - Training Epoch: 2/2, step 758/7134 completed (loss: 0.0465535931289196, acc: 0.9909909963607788)
[2025-02-13 19:59:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:17][root][INFO] - Training Epoch: 2/2, step 759/7134 completed (loss: 0.07795840501785278, acc: 0.9909909963607788)
[2025-02-13 19:59:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:18][root][INFO] - Training Epoch: 2/2, step 760/7134 completed (loss: 0.08165495097637177, acc: 0.9772727489471436)
[2025-02-13 19:59:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:18][root][INFO] - Training Epoch: 2/2, step 761/7134 completed (loss: 0.12720881402492523, acc: 0.9580838084220886)
[2025-02-13 19:59:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:18][root][INFO] - Training Epoch: 2/2, step 762/7134 completed (loss: 0.11993928998708725, acc: 0.9637681245803833)
[2025-02-13 19:59:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:19][root][INFO] - Training Epoch: 2/2, step 763/7134 completed (loss: 0.21258413791656494, acc: 0.9605262875556946)
[2025-02-13 19:59:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:19][root][INFO] - Training Epoch: 2/2, step 764/7134 completed (loss: 0.08500918000936508, acc: 0.976331353187561)
[2025-02-13 19:59:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:20][root][INFO] - Training Epoch: 2/2, step 765/7134 completed (loss: 0.06059842184185982, acc: 0.9748427867889404)
[2025-02-13 19:59:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:20][root][INFO] - Training Epoch: 2/2, step 766/7134 completed (loss: 0.10704243183135986, acc: 0.956250011920929)
[2025-02-13 19:59:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:20][root][INFO] - Training Epoch: 2/2, step 767/7134 completed (loss: 0.081606425344944, acc: 0.9702380895614624)
[2025-02-13 19:59:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:21][root][INFO] - Training Epoch: 2/2, step 768/7134 completed (loss: 0.18021425604820251, acc: 0.9726775884628296)
[2025-02-13 19:59:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:21][root][INFO] - Training Epoch: 2/2, step 769/7134 completed (loss: 0.11534778773784637, acc: 0.9813664555549622)
[2025-02-13 19:59:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:21][root][INFO] - Training Epoch: 2/2, step 770/7134 completed (loss: 0.07832926511764526, acc: 0.9876543283462524)
[2025-02-13 19:59:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:22][root][INFO] - Training Epoch: 2/2, step 771/7134 completed (loss: 0.0510198213160038, acc: 0.9927007555961609)
[2025-02-13 19:59:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:22][root][INFO] - Training Epoch: 2/2, step 772/7134 completed (loss: 0.09432599693536758, acc: 0.9659863710403442)
[2025-02-13 19:59:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:23][root][INFO] - Training Epoch: 2/2, step 773/7134 completed (loss: 0.1537976711988449, acc: 0.9526315927505493)
[2025-02-13 19:59:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:23][root][INFO] - Training Epoch: 2/2, step 774/7134 completed (loss: 0.12801600992679596, acc: 0.9507042169570923)
[2025-02-13 19:59:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:23][root][INFO] - Training Epoch: 2/2, step 775/7134 completed (loss: 0.02502267248928547, acc: 1.0)
[2025-02-13 19:59:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:24][root][INFO] - Training Epoch: 2/2, step 776/7134 completed (loss: 0.02425687201321125, acc: 0.993630588054657)
[2025-02-13 19:59:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:24][root][INFO] - Training Epoch: 2/2, step 777/7134 completed (loss: 0.060398414731025696, acc: 0.9866666793823242)
[2025-02-13 19:59:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:24][root][INFO] - Training Epoch: 2/2, step 778/7134 completed (loss: 0.10818956792354584, acc: 0.9874213933944702)
[2025-02-13 19:59:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:25][root][INFO] - Training Epoch: 2/2, step 779/7134 completed (loss: 0.10614565759897232, acc: 0.9766082167625427)
[2025-02-13 19:59:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:25][root][INFO] - Training Epoch: 2/2, step 780/7134 completed (loss: 0.1636631339788437, acc: 0.9675675630569458)
[2025-02-13 19:59:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:26][root][INFO] - Training Epoch: 2/2, step 781/7134 completed (loss: 0.20432011783123016, acc: 0.9460784196853638)
[2025-02-13 19:59:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:26][root][INFO] - Training Epoch: 2/2, step 782/7134 completed (loss: 0.1765405237674713, acc: 0.949367105960846)
[2025-02-13 19:59:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:26][root][INFO] - Training Epoch: 2/2, step 783/7134 completed (loss: 0.12615461647510529, acc: 0.9626168012619019)
[2025-02-13 19:59:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:27][root][INFO] - Training Epoch: 2/2, step 784/7134 completed (loss: 0.0532616525888443, acc: 0.9950248599052429)
[2025-02-13 19:59:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:27][root][INFO] - Training Epoch: 2/2, step 785/7134 completed (loss: 0.16683736443519592, acc: 0.9599999785423279)
[2025-02-13 19:59:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:28][root][INFO] - Training Epoch: 2/2, step 786/7134 completed (loss: 0.16511237621307373, acc: 0.9776119589805603)
[2025-02-13 19:59:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:28][root][INFO] - Training Epoch: 2/2, step 787/7134 completed (loss: 0.042718105018138885, acc: 0.9849624037742615)
[2025-02-13 19:59:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:28][root][INFO] - Training Epoch: 2/2, step 788/7134 completed (loss: 0.20745839178562164, acc: 0.9441340565681458)
[2025-02-13 19:59:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:29][root][INFO] - Training Epoch: 2/2, step 789/7134 completed (loss: 0.08870693296194077, acc: 0.9807692170143127)
[2025-02-13 19:59:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:29][root][INFO] - Training Epoch: 2/2, step 790/7134 completed (loss: 0.1879853457212448, acc: 0.9594594836235046)
[2025-02-13 19:59:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:30][root][INFO] - Training Epoch: 2/2, step 791/7134 completed (loss: 0.24688118696212769, acc: 0.9512194991111755)
[2025-02-13 19:59:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:30][root][INFO] - Training Epoch: 2/2, step 792/7134 completed (loss: 0.12741407752037048, acc: 0.9556962251663208)
[2025-02-13 19:59:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:31][root][INFO] - Training Epoch: 2/2, step 793/7134 completed (loss: 0.05897306650876999, acc: 0.987500011920929)
[2025-02-13 19:59:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:31][root][INFO] - Training Epoch: 2/2, step 794/7134 completed (loss: 0.12349694222211838, acc: 0.9655172228813171)
[2025-02-13 19:59:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:31][root][INFO] - Training Epoch: 2/2, step 795/7134 completed (loss: 0.05117994174361229, acc: 1.0)
[2025-02-13 19:59:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:32][root][INFO] - Training Epoch: 2/2, step 796/7134 completed (loss: 0.1640111654996872, acc: 0.9523809552192688)
[2025-02-13 19:59:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:32][root][INFO] - Training Epoch: 2/2, step 797/7134 completed (loss: 0.07163143903017044, acc: 0.9763779640197754)
[2025-02-13 19:59:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:32][root][INFO] - Training Epoch: 2/2, step 798/7134 completed (loss: 0.05163750424981117, acc: 0.9898989796638489)
[2025-02-13 19:59:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:33][root][INFO] - Training Epoch: 2/2, step 799/7134 completed (loss: 0.12569661438465118, acc: 0.9669421315193176)
[2025-02-13 19:59:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:33][root][INFO] - Training Epoch: 2/2, step 800/7134 completed (loss: 0.12650887668132782, acc: 0.9555555582046509)
[2025-02-13 19:59:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:34][root][INFO] - Training Epoch: 2/2, step 801/7134 completed (loss: 0.22994862496852875, acc: 0.9548386931419373)
[2025-02-13 19:59:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:34][root][INFO] - Training Epoch: 2/2, step 802/7134 completed (loss: 0.10529177635908127, acc: 0.9716312289237976)
[2025-02-13 19:59:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:34][root][INFO] - Training Epoch: 2/2, step 803/7134 completed (loss: 0.059914421290159225, acc: 0.9927536249160767)
[2025-02-13 19:59:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:35][root][INFO] - Training Epoch: 2/2, step 804/7134 completed (loss: 0.18393707275390625, acc: 0.961240291595459)
[2025-02-13 19:59:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:35][root][INFO] - Training Epoch: 2/2, step 805/7134 completed (loss: 0.29636380076408386, acc: 0.9090909361839294)
[2025-02-13 19:59:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:35][root][INFO] - Training Epoch: 2/2, step 806/7134 completed (loss: 0.10568393766880035, acc: 0.9691358208656311)
[2025-02-13 19:59:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:36][root][INFO] - Training Epoch: 2/2, step 807/7134 completed (loss: 0.13071928918361664, acc: 0.949999988079071)
[2025-02-13 19:59:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:36][root][INFO] - Training Epoch: 2/2, step 808/7134 completed (loss: 0.07011725008487701, acc: 0.9746192693710327)
[2025-02-13 19:59:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:37][root][INFO] - Training Epoch: 2/2, step 809/7134 completed (loss: 0.0505569726228714, acc: 0.9866666793823242)
[2025-02-13 19:59:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:37][root][INFO] - Training Epoch: 2/2, step 810/7134 completed (loss: 0.16638675332069397, acc: 0.9735449552536011)
[2025-02-13 19:59:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:37][root][INFO] - Training Epoch: 2/2, step 811/7134 completed (loss: 0.16978003084659576, acc: 0.9586206674575806)
[2025-02-13 19:59:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:38][root][INFO] - Training Epoch: 2/2, step 812/7134 completed (loss: 0.08441268652677536, acc: 0.9736841917037964)
[2025-02-13 19:59:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:38][root][INFO] - Training Epoch: 2/2, step 813/7134 completed (loss: 0.1925623118877411, acc: 0.9647058844566345)
[2025-02-13 19:59:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:38][root][INFO] - Training Epoch: 2/2, step 814/7134 completed (loss: 0.06826366484165192, acc: 0.9887005686759949)
[2025-02-13 19:59:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:39][root][INFO] - Training Epoch: 2/2, step 815/7134 completed (loss: 0.08927903324365616, acc: 0.9871794581413269)
[2025-02-13 19:59:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:39][root][INFO] - Training Epoch: 2/2, step 816/7134 completed (loss: 0.18345126509666443, acc: 0.9503546357154846)
[2025-02-13 19:59:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:40][root][INFO] - Training Epoch: 2/2, step 817/7134 completed (loss: 0.20358452200889587, acc: 0.9424460530281067)
[2025-02-13 19:59:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:40][root][INFO] - Training Epoch: 2/2, step 818/7134 completed (loss: 0.3080121576786041, acc: 0.9542483687400818)
[2025-02-13 19:59:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:41][root][INFO] - Training Epoch: 2/2, step 819/7134 completed (loss: 0.31064364314079285, acc: 0.9246575236320496)
[2025-02-13 19:59:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:41][root][INFO] - Training Epoch: 2/2, step 820/7134 completed (loss: 0.2063566893339157, acc: 0.948051929473877)
[2025-02-13 19:59:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:41][root][INFO] - Training Epoch: 2/2, step 821/7134 completed (loss: 0.10105087608098984, acc: 0.9826589822769165)
[2025-02-13 19:59:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:42][root][INFO] - Training Epoch: 2/2, step 822/7134 completed (loss: 0.1188458800315857, acc: 0.9683544039726257)
[2025-02-13 19:59:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:42][root][INFO] - Training Epoch: 2/2, step 823/7134 completed (loss: 0.0943121463060379, acc: 0.9756097793579102)
[2025-02-13 19:59:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:43][root][INFO] - Training Epoch: 2/2, step 824/7134 completed (loss: 0.1649366021156311, acc: 0.9545454382896423)
[2025-02-13 19:59:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:43][root][INFO] - Training Epoch: 2/2, step 825/7134 completed (loss: 0.0918179303407669, acc: 0.9886363744735718)
[2025-02-13 19:59:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:43][root][INFO] - Training Epoch: 2/2, step 826/7134 completed (loss: 0.08029294013977051, acc: 0.9707602262496948)
[2025-02-13 19:59:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:44][root][INFO] - Training Epoch: 2/2, step 827/7134 completed (loss: 0.10489118844270706, acc: 0.9590643048286438)
[2025-02-13 19:59:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:44][root][INFO] - Training Epoch: 2/2, step 828/7134 completed (loss: 0.12062577158212662, acc: 0.9675324559211731)
[2025-02-13 19:59:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:45][root][INFO] - Training Epoch: 2/2, step 829/7134 completed (loss: 0.14659586548805237, acc: 0.9615384340286255)
[2025-02-13 19:59:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:45][root][INFO] - Training Epoch: 2/2, step 830/7134 completed (loss: 0.09378454089164734, acc: 0.971222996711731)
[2025-02-13 19:59:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:46][root][INFO] - Training Epoch: 2/2, step 831/7134 completed (loss: 0.059723082929849625, acc: 0.9879518151283264)
[2025-02-13 19:59:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:46][root][INFO] - Training Epoch: 2/2, step 832/7134 completed (loss: 0.22813457250595093, acc: 0.9523809552192688)
[2025-02-13 19:59:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:46][root][INFO] - Training Epoch: 2/2, step 833/7134 completed (loss: 0.028760656714439392, acc: 1.0)
[2025-02-13 19:59:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:47][root][INFO] - Training Epoch: 2/2, step 834/7134 completed (loss: 0.12461009621620178, acc: 0.970588207244873)
[2025-02-13 19:59:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:47][root][INFO] - Training Epoch: 2/2, step 835/7134 completed (loss: 0.11536470055580139, acc: 0.9707602262496948)
[2025-02-13 19:59:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:48][root][INFO] - Training Epoch: 2/2, step 836/7134 completed (loss: 0.11046497523784637, acc: 0.9543147087097168)
[2025-02-13 19:59:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:48][root][INFO] - Training Epoch: 2/2, step 837/7134 completed (loss: 0.1440420299768448, acc: 0.9717513918876648)
[2025-02-13 19:59:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:48][root][INFO] - Training Epoch: 2/2, step 838/7134 completed (loss: 0.09753935039043427, acc: 0.9803921580314636)
[2025-02-13 19:59:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:49][root][INFO] - Training Epoch: 2/2, step 839/7134 completed (loss: 0.051820307970047, acc: 0.9801980257034302)
[2025-02-13 19:59:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:49][root][INFO] - Training Epoch: 2/2, step 840/7134 completed (loss: 0.07930932939052582, acc: 0.970059871673584)
[2025-02-13 19:59:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:50][root][INFO] - Training Epoch: 2/2, step 841/7134 completed (loss: 0.03467830643057823, acc: 0.9941520690917969)
[2025-02-13 19:59:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:50][root][INFO] - Training Epoch: 2/2, step 842/7134 completed (loss: 0.0697697252035141, acc: 0.9823529124259949)
[2025-02-13 19:59:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:50][root][INFO] - Training Epoch: 2/2, step 843/7134 completed (loss: 0.0487336739897728, acc: 0.9824561476707458)
[2025-02-13 19:59:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:51][root][INFO] - Training Epoch: 2/2, step 844/7134 completed (loss: 0.07463152706623077, acc: 0.9726027250289917)
[2025-02-13 19:59:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:51][root][INFO] - Training Epoch: 2/2, step 845/7134 completed (loss: 0.07338505983352661, acc: 0.9777777791023254)
[2025-02-13 19:59:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:52][root][INFO] - Training Epoch: 2/2, step 846/7134 completed (loss: 0.03384228050708771, acc: 0.9863013625144958)
[2025-02-13 19:59:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:52][root][INFO] - Training Epoch: 2/2, step 847/7134 completed (loss: 0.15908610820770264, acc: 0.9640718698501587)
[2025-02-13 19:59:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:52][root][INFO] - Training Epoch: 2/2, step 848/7134 completed (loss: 0.10302041471004486, acc: 0.9707602262496948)
[2025-02-13 19:59:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:53][root][INFO] - Training Epoch: 2/2, step 849/7134 completed (loss: 0.05315374583005905, acc: 0.9878048896789551)
[2025-02-13 19:59:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:53][root][INFO] - Training Epoch: 2/2, step 850/7134 completed (loss: 0.07041580229997635, acc: 0.9865771532058716)
[2025-02-13 19:59:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:54][root][INFO] - Training Epoch: 2/2, step 851/7134 completed (loss: 0.166956827044487, acc: 0.9476743936538696)
[2025-02-13 19:59:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:54][root][INFO] - Training Epoch: 2/2, step 852/7134 completed (loss: 0.07640638202428818, acc: 0.9842932224273682)
[2025-02-13 19:59:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:54][root][INFO] - Training Epoch: 2/2, step 853/7134 completed (loss: 0.15645158290863037, acc: 0.9790576100349426)
[2025-02-13 19:59:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:55][root][INFO] - Training Epoch: 2/2, step 854/7134 completed (loss: 0.05154748633503914, acc: 0.9878048896789551)
[2025-02-13 19:59:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:55][root][INFO] - Training Epoch: 2/2, step 855/7134 completed (loss: 0.05875829979777336, acc: 0.9942196607589722)
[2025-02-13 19:59:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:56][root][INFO] - Training Epoch: 2/2, step 856/7134 completed (loss: 0.04602210223674774, acc: 0.9890710115432739)
[2025-02-13 19:59:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:56][root][INFO] - Training Epoch: 2/2, step 857/7134 completed (loss: 0.09250064939260483, acc: 0.9642857313156128)
[2025-02-13 19:59:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:56][root][INFO] - Training Epoch: 2/2, step 858/7134 completed (loss: 0.20645354688167572, acc: 0.9583333134651184)
[2025-02-13 19:59:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:57][root][INFO] - Training Epoch: 2/2, step 859/7134 completed (loss: 0.20097622275352478, acc: 0.9378530979156494)
[2025-02-13 19:59:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:57][root][INFO] - Training Epoch: 2/2, step 860/7134 completed (loss: 0.1755618005990982, acc: 0.9536423683166504)
[2025-02-13 19:59:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:57][root][INFO] - Training Epoch: 2/2, step 861/7134 completed (loss: 0.13050296902656555, acc: 0.9462365508079529)
[2025-02-13 19:59:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:58][root][INFO] - Training Epoch: 2/2, step 862/7134 completed (loss: 0.13597063720226288, acc: 0.977142870426178)
[2025-02-13 19:59:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:58][root][INFO] - Training Epoch: 2/2, step 863/7134 completed (loss: 0.17667384445667267, acc: 0.9562841653823853)
[2025-02-13 19:59:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:59][root][INFO] - Training Epoch: 2/2, step 864/7134 completed (loss: 0.1841857135295868, acc: 0.9682539701461792)
[2025-02-13 19:59:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:59][root][INFO] - Training Epoch: 2/2, step 865/7134 completed (loss: 0.24456177651882172, acc: 0.9360465407371521)
[2025-02-13 19:59:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 19:59:59][root][INFO] - Training Epoch: 2/2, step 866/7134 completed (loss: 0.21557432413101196, acc: 0.9571428298950195)
[2025-02-13 20:00:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:00][root][INFO] - Training Epoch: 2/2, step 867/7134 completed (loss: 0.21119341254234314, acc: 0.956250011920929)
[2025-02-13 20:00:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:00][root][INFO] - Training Epoch: 2/2, step 868/7134 completed (loss: 0.3549955487251282, acc: 0.930232584476471)
[2025-02-13 20:00:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:01][root][INFO] - Training Epoch: 2/2, step 869/7134 completed (loss: 0.19919021427631378, acc: 0.9518072009086609)
[2025-02-13 20:00:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:01][root][INFO] - Training Epoch: 2/2, step 870/7134 completed (loss: 0.1389458328485489, acc: 0.9649122953414917)
[2025-02-13 20:00:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:02][root][INFO] - Training Epoch: 2/2, step 871/7134 completed (loss: 0.15439629554748535, acc: 0.9520547986030579)
[2025-02-13 20:00:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:02][root][INFO] - Training Epoch: 2/2, step 872/7134 completed (loss: 0.07843563705682755, acc: 0.9722222089767456)
[2025-02-13 20:00:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:02][root][INFO] - Training Epoch: 2/2, step 873/7134 completed (loss: 0.18684959411621094, acc: 0.9515151381492615)
[2025-02-13 20:00:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:03][root][INFO] - Training Epoch: 2/2, step 874/7134 completed (loss: 0.08737307786941528, acc: 0.9861111044883728)
[2025-02-13 20:00:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:03][root][INFO] - Training Epoch: 2/2, step 875/7134 completed (loss: 0.056423790752887726, acc: 0.9856114983558655)
[2025-02-13 20:00:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:03][root][INFO] - Training Epoch: 2/2, step 876/7134 completed (loss: 0.06922084838151932, acc: 0.9868420958518982)
[2025-02-13 20:00:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:04][root][INFO] - Training Epoch: 2/2, step 877/7134 completed (loss: 0.23838268220424652, acc: 0.9386503100395203)
[2025-02-13 20:00:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:04][root][INFO] - Training Epoch: 2/2, step 878/7134 completed (loss: 0.12467046082019806, acc: 0.96875)
[2025-02-13 20:00:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:05][root][INFO] - Training Epoch: 2/2, step 879/7134 completed (loss: 0.16478337347507477, acc: 0.9554139971733093)
[2025-02-13 20:00:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:05][root][INFO] - Training Epoch: 2/2, step 880/7134 completed (loss: 0.08178317546844482, acc: 0.9693251252174377)
[2025-02-13 20:00:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:05][root][INFO] - Training Epoch: 2/2, step 881/7134 completed (loss: 0.08204497396945953, acc: 0.9756097793579102)
[2025-02-13 20:00:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:06][root][INFO] - Training Epoch: 2/2, step 882/7134 completed (loss: 0.07826019823551178, acc: 0.9838709831237793)
[2025-02-13 20:00:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:06][root][INFO] - Training Epoch: 2/2, step 883/7134 completed (loss: 0.12227673083543777, acc: 0.9605262875556946)
[2025-02-13 20:00:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:06][root][INFO] - Training Epoch: 2/2, step 884/7134 completed (loss: 0.09927346557378769, acc: 0.9820359349250793)
[2025-02-13 20:00:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:07][root][INFO] - Training Epoch: 2/2, step 885/7134 completed (loss: 0.034121546894311905, acc: 1.0)
[2025-02-13 20:00:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:07][root][INFO] - Training Epoch: 2/2, step 886/7134 completed (loss: 0.12019719928503036, acc: 0.9831932783126831)
[2025-02-13 20:00:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:07][root][INFO] - Training Epoch: 2/2, step 887/7134 completed (loss: 0.10511129349470139, acc: 0.9850746393203735)
[2025-02-13 20:00:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:08][root][INFO] - Training Epoch: 2/2, step 888/7134 completed (loss: 0.1709907054901123, acc: 0.9530201554298401)
[2025-02-13 20:00:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:08][root][INFO] - Training Epoch: 2/2, step 889/7134 completed (loss: 0.04642431437969208, acc: 0.9900990128517151)
[2025-02-13 20:00:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:09][root][INFO] - Training Epoch: 2/2, step 890/7134 completed (loss: 0.17539237439632416, acc: 0.9463087320327759)
[2025-02-13 20:00:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:09][root][INFO] - Training Epoch: 2/2, step 891/7134 completed (loss: 0.19042982161045074, acc: 0.9608938694000244)
[2025-02-13 20:00:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:09][root][INFO] - Training Epoch: 2/2, step 892/7134 completed (loss: 0.07725954800844193, acc: 0.9821428656578064)
[2025-02-13 20:00:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:10][root][INFO] - Training Epoch: 2/2, step 893/7134 completed (loss: 0.19734172523021698, acc: 0.9607843160629272)
[2025-02-13 20:00:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:10][root][INFO] - Training Epoch: 2/2, step 894/7134 completed (loss: 0.029248762875795364, acc: 1.0)
[2025-02-13 20:00:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:10][root][INFO] - Training Epoch: 2/2, step 895/7134 completed (loss: 0.09759128838777542, acc: 0.9863013625144958)
[2025-02-13 20:00:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:11][root][INFO] - Training Epoch: 2/2, step 896/7134 completed (loss: 0.05854886397719383, acc: 0.9763779640197754)
[2025-02-13 20:00:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:11][root][INFO] - Training Epoch: 2/2, step 897/7134 completed (loss: 0.11495912075042725, acc: 0.9832402467727661)
[2025-02-13 20:00:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:12][root][INFO] - Training Epoch: 2/2, step 898/7134 completed (loss: 0.1917399913072586, acc: 0.9431818127632141)
[2025-02-13 20:00:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:12][root][INFO] - Training Epoch: 2/2, step 899/7134 completed (loss: 0.11891096830368042, acc: 0.9679144620895386)
[2025-02-13 20:00:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:12][root][INFO] - Training Epoch: 2/2, step 900/7134 completed (loss: 0.10368765890598297, acc: 0.9756097793579102)
[2025-02-13 20:00:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:13][root][INFO] - Training Epoch: 2/2, step 901/7134 completed (loss: 0.16499483585357666, acc: 0.9642857313156128)
[2025-02-13 20:00:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:13][root][INFO] - Training Epoch: 2/2, step 902/7134 completed (loss: 0.07693804800510406, acc: 0.9870129823684692)
[2025-02-13 20:00:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:13][root][INFO] - Training Epoch: 2/2, step 903/7134 completed (loss: 0.0351727120578289, acc: 0.9942196607589722)
[2025-02-13 20:00:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:14][root][INFO] - Training Epoch: 2/2, step 904/7134 completed (loss: 0.07885875552892685, acc: 0.9774011373519897)
[2025-02-13 20:00:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:14][root][INFO] - Training Epoch: 2/2, step 905/7134 completed (loss: 0.03561902791261673, acc: 1.0)
[2025-02-13 20:00:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:15][root][INFO] - Training Epoch: 2/2, step 906/7134 completed (loss: 0.08259890973567963, acc: 0.9779005646705627)
[2025-02-13 20:00:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:15][root][INFO] - Training Epoch: 2/2, step 907/7134 completed (loss: 0.06411612033843994, acc: 0.9892473220825195)
[2025-02-13 20:00:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:15][root][INFO] - Training Epoch: 2/2, step 908/7134 completed (loss: 0.08647160977125168, acc: 0.9833333492279053)
[2025-02-13 20:00:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:16][root][INFO] - Training Epoch: 2/2, step 909/7134 completed (loss: 0.07841219007968903, acc: 0.9736841917037964)
[2025-02-13 20:00:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:16][root][INFO] - Training Epoch: 2/2, step 910/7134 completed (loss: 0.10884024202823639, acc: 0.981249988079071)
[2025-02-13 20:00:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:16][root][INFO] - Training Epoch: 2/2, step 911/7134 completed (loss: 0.07843516021966934, acc: 0.9806451797485352)
[2025-02-13 20:00:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:17][root][INFO] - Training Epoch: 2/2, step 912/7134 completed (loss: 0.1490647792816162, acc: 0.9578947424888611)
[2025-02-13 20:00:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:17][root][INFO] - Training Epoch: 2/2, step 913/7134 completed (loss: 0.08630192279815674, acc: 0.9780219793319702)
[2025-02-13 20:00:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:18][root][INFO] - Training Epoch: 2/2, step 914/7134 completed (loss: 0.11494959890842438, acc: 0.9696969985961914)
[2025-02-13 20:00:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:18][root][INFO] - Training Epoch: 2/2, step 915/7134 completed (loss: 0.07998230308294296, acc: 0.9857142567634583)
[2025-02-13 20:00:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:18][root][INFO] - Training Epoch: 2/2, step 916/7134 completed (loss: 0.09087861329317093, acc: 0.9767441749572754)
[2025-02-13 20:00:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:19][root][INFO] - Training Epoch: 2/2, step 917/7134 completed (loss: 0.1571643054485321, acc: 0.9610389471054077)
[2025-02-13 20:00:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:19][root][INFO] - Training Epoch: 2/2, step 918/7134 completed (loss: 0.09526696056127548, acc: 0.9801324605941772)
[2025-02-13 20:00:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:20][root][INFO] - Training Epoch: 2/2, step 919/7134 completed (loss: 0.04070524871349335, acc: 0.9870129823684692)
[2025-02-13 20:00:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:20][root][INFO] - Training Epoch: 2/2, step 920/7134 completed (loss: 0.0947936624288559, acc: 0.9709302186965942)
[2025-02-13 20:00:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:20][root][INFO] - Training Epoch: 2/2, step 921/7134 completed (loss: 0.04284149408340454, acc: 0.9941520690917969)
[2025-02-13 20:00:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:21][root][INFO] - Training Epoch: 2/2, step 922/7134 completed (loss: 0.030207926407456398, acc: 0.9925373196601868)
[2025-02-13 20:00:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:21][root][INFO] - Training Epoch: 2/2, step 923/7134 completed (loss: 0.06841865926980972, acc: 0.9941520690917969)
[2025-02-13 20:00:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:21][root][INFO] - Training Epoch: 2/2, step 924/7134 completed (loss: 0.05317937955260277, acc: 0.9774011373519897)
[2025-02-13 20:00:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:22][root][INFO] - Training Epoch: 2/2, step 925/7134 completed (loss: 0.29080939292907715, acc: 0.9419354796409607)
[2025-02-13 20:00:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:22][root][INFO] - Training Epoch: 2/2, step 926/7134 completed (loss: 0.14300423860549927, acc: 0.9611111283302307)
[2025-02-13 20:00:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:23][root][INFO] - Training Epoch: 2/2, step 927/7134 completed (loss: 0.19754959642887115, acc: 0.9640287756919861)
[2025-02-13 20:00:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:23][root][INFO] - Training Epoch: 2/2, step 928/7134 completed (loss: 0.008616318926215172, acc: 1.0)
[2025-02-13 20:00:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:24][root][INFO] - Training Epoch: 2/2, step 929/7134 completed (loss: 0.11950210481882095, acc: 0.9802631735801697)
[2025-02-13 20:00:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:24][root][INFO] - Training Epoch: 2/2, step 930/7134 completed (loss: 0.18262454867362976, acc: 0.9354838728904724)
[2025-02-13 20:00:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:24][root][INFO] - Training Epoch: 2/2, step 931/7134 completed (loss: 0.11634403467178345, acc: 0.9779411554336548)
[2025-02-13 20:00:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:25][root][INFO] - Training Epoch: 2/2, step 932/7134 completed (loss: 0.0942976176738739, acc: 0.9702380895614624)
[2025-02-13 20:00:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:25][root][INFO] - Training Epoch: 2/2, step 933/7134 completed (loss: 0.18408401310443878, acc: 0.9580838084220886)
[2025-02-13 20:00:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:26][root][INFO] - Training Epoch: 2/2, step 934/7134 completed (loss: 0.12626294791698456, acc: 0.9825581312179565)
[2025-02-13 20:00:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:26][root][INFO] - Training Epoch: 2/2, step 935/7134 completed (loss: 0.034665267914533615, acc: 0.9922480583190918)
[2025-02-13 20:00:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:26][root][INFO] - Training Epoch: 2/2, step 936/7134 completed (loss: 0.1166125014424324, acc: 0.960629940032959)
[2025-02-13 20:00:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:27][root][INFO] - Training Epoch: 2/2, step 937/7134 completed (loss: 0.06274419277906418, acc: 0.9918699264526367)
[2025-02-13 20:00:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:27][root][INFO] - Training Epoch: 2/2, step 938/7134 completed (loss: 0.09087243676185608, acc: 0.9743589758872986)
[2025-02-13 20:00:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:28][root][INFO] - Training Epoch: 2/2, step 939/7134 completed (loss: 0.1386789232492447, acc: 0.9695122241973877)
[2025-02-13 20:00:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:28][root][INFO] - Training Epoch: 2/2, step 940/7134 completed (loss: 0.08326040208339691, acc: 0.9710982441902161)
[2025-02-13 20:00:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:29][root][INFO] - Training Epoch: 2/2, step 941/7134 completed (loss: 0.0780249759554863, acc: 0.9775280952453613)
[2025-02-13 20:00:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:29][root][INFO] - Training Epoch: 2/2, step 942/7134 completed (loss: 0.18137405812740326, acc: 0.9593023061752319)
[2025-02-13 20:00:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:29][root][INFO] - Training Epoch: 2/2, step 943/7134 completed (loss: 0.09200902283191681, acc: 0.9754601120948792)
[2025-02-13 20:00:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:30][root][INFO] - Training Epoch: 2/2, step 944/7134 completed (loss: 0.10282555222511292, acc: 0.9745222926139832)
[2025-02-13 20:00:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:30][root][INFO] - Training Epoch: 2/2, step 945/7134 completed (loss: 0.11811923235654831, acc: 0.9801324605941772)
[2025-02-13 20:00:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:31][root][INFO] - Training Epoch: 2/2, step 946/7134 completed (loss: 0.0293340552598238, acc: 1.0)
[2025-02-13 20:00:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:31][root][INFO] - Training Epoch: 2/2, step 947/7134 completed (loss: 0.1367557793855667, acc: 0.970588207244873)
[2025-02-13 20:00:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:31][root][INFO] - Training Epoch: 2/2, step 948/7134 completed (loss: 0.0577118881046772, acc: 0.9759036302566528)
[2025-02-13 20:00:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:32][root][INFO] - Training Epoch: 2/2, step 949/7134 completed (loss: 0.04657962545752525, acc: 1.0)
[2025-02-13 20:00:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:32][root][INFO] - Training Epoch: 2/2, step 950/7134 completed (loss: 0.07796822488307953, acc: 0.9805194735527039)
[2025-02-13 20:00:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:32][root][INFO] - Training Epoch: 2/2, step 951/7134 completed (loss: 0.09562854468822479, acc: 0.9694656729698181)
[2025-02-13 20:00:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:33][root][INFO] - Training Epoch: 2/2, step 952/7134 completed (loss: 0.12918628752231598, acc: 0.9558823704719543)
[2025-02-13 20:00:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:33][root][INFO] - Training Epoch: 2/2, step 953/7134 completed (loss: 0.12438786774873734, acc: 0.9695122241973877)
[2025-02-13 20:00:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:34][root][INFO] - Training Epoch: 2/2, step 954/7134 completed (loss: 0.12778645753860474, acc: 0.9655172228813171)
[2025-02-13 20:00:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:34][root][INFO] - Training Epoch: 2/2, step 955/7134 completed (loss: 0.1584784835577011, acc: 0.9604519605636597)
[2025-02-13 20:00:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:34][root][INFO] - Training Epoch: 2/2, step 956/7134 completed (loss: 0.12900394201278687, acc: 0.97826087474823)
[2025-02-13 20:00:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:35][root][INFO] - Training Epoch: 2/2, step 957/7134 completed (loss: 0.15665888786315918, acc: 0.9754902124404907)
[2025-02-13 20:00:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:35][root][INFO] - Training Epoch: 2/2, step 958/7134 completed (loss: 0.10916373878717422, acc: 0.9776119589805603)
[2025-02-13 20:00:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:35][root][INFO] - Training Epoch: 2/2, step 959/7134 completed (loss: 0.1732879877090454, acc: 0.938144326210022)
[2025-02-13 20:00:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:36][root][INFO] - Training Epoch: 2/2, step 960/7134 completed (loss: 0.2198476642370224, acc: 0.9552238583564758)
[2025-02-13 20:00:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:36][root][INFO] - Training Epoch: 2/2, step 961/7134 completed (loss: 0.14696578681468964, acc: 0.9560439586639404)
[2025-02-13 20:00:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:37][root][INFO] - Training Epoch: 2/2, step 962/7134 completed (loss: 0.18651871383190155, acc: 0.9415204524993896)
[2025-02-13 20:00:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:37][root][INFO] - Training Epoch: 2/2, step 963/7134 completed (loss: 0.160088911652565, acc: 0.9534883499145508)
[2025-02-13 20:00:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:37][root][INFO] - Training Epoch: 2/2, step 964/7134 completed (loss: 0.36550918221473694, acc: 0.9085366129875183)
[2025-02-13 20:00:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:38][root][INFO] - Training Epoch: 2/2, step 965/7134 completed (loss: 0.27663499116897583, acc: 0.9304812550544739)
[2025-02-13 20:00:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:38][root][INFO] - Training Epoch: 2/2, step 966/7134 completed (loss: 0.11346092820167542, acc: 0.9631901979446411)
[2025-02-13 20:00:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:38][root][INFO] - Training Epoch: 2/2, step 967/7134 completed (loss: 0.06787242740392685, acc: 0.9840425252914429)
[2025-02-13 20:00:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:39][root][INFO] - Training Epoch: 2/2, step 968/7134 completed (loss: 0.18112412095069885, acc: 0.969924807548523)
[2025-02-13 20:00:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:39][root][INFO] - Training Epoch: 2/2, step 969/7134 completed (loss: 0.1270754039287567, acc: 0.954285740852356)
[2025-02-13 20:00:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:40][root][INFO] - Training Epoch: 2/2, step 970/7134 completed (loss: 0.25626635551452637, acc: 0.9485294222831726)
[2025-02-13 20:00:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:40][root][INFO] - Training Epoch: 2/2, step 971/7134 completed (loss: 0.19384030997753143, acc: 0.9573459625244141)
[2025-02-13 20:00:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:40][root][INFO] - Training Epoch: 2/2, step 972/7134 completed (loss: 0.2654358744621277, acc: 0.9179487228393555)
[2025-02-13 20:00:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:41][root][INFO] - Training Epoch: 2/2, step 973/7134 completed (loss: 0.18755005300045013, acc: 0.9450549483299255)
[2025-02-13 20:00:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:41][root][INFO] - Training Epoch: 2/2, step 974/7134 completed (loss: 0.17574156820774078, acc: 0.9603960514068604)
[2025-02-13 20:00:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:42][root][INFO] - Training Epoch: 2/2, step 975/7134 completed (loss: 0.23081167042255402, acc: 0.9627659320831299)
[2025-02-13 20:00:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:42][root][INFO] - Training Epoch: 2/2, step 976/7134 completed (loss: 0.09545871615409851, acc: 0.9735099077224731)
[2025-02-13 20:00:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:42][root][INFO] - Training Epoch: 2/2, step 977/7134 completed (loss: 0.2835291922092438, acc: 0.9382022619247437)
[2025-02-13 20:00:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:43][root][INFO] - Training Epoch: 2/2, step 978/7134 completed (loss: 0.08464789390563965, acc: 0.9793814420700073)
[2025-02-13 20:00:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:43][root][INFO] - Training Epoch: 2/2, step 979/7134 completed (loss: 0.10881087183952332, acc: 0.9783783555030823)
[2025-02-13 20:00:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:43][root][INFO] - Training Epoch: 2/2, step 980/7134 completed (loss: 0.13481327891349792, acc: 0.9671361446380615)
[2025-02-13 20:00:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:44][root][INFO] - Training Epoch: 2/2, step 981/7134 completed (loss: 0.25539588928222656, acc: 0.9685534834861755)
[2025-02-13 20:00:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:44][root][INFO] - Training Epoch: 2/2, step 982/7134 completed (loss: 0.28554919362068176, acc: 0.9485714435577393)
[2025-02-13 20:00:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:45][root][INFO] - Training Epoch: 2/2, step 983/7134 completed (loss: 0.13087086379528046, acc: 0.969924807548523)
[2025-02-13 20:00:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:45][root][INFO] - Training Epoch: 2/2, step 984/7134 completed (loss: 0.0462275967001915, acc: 1.0)
[2025-02-13 20:00:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:45][root][INFO] - Training Epoch: 2/2, step 985/7134 completed (loss: 0.0977591797709465, acc: 0.9722222089767456)
[2025-02-13 20:00:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:46][root][INFO] - Training Epoch: 2/2, step 986/7134 completed (loss: 0.05028602480888367, acc: 0.9888268113136292)
[2025-02-13 20:00:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:46][root][INFO] - Training Epoch: 2/2, step 987/7134 completed (loss: 0.05161590874195099, acc: 0.9932885766029358)
[2025-02-13 20:00:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:47][root][INFO] - Training Epoch: 2/2, step 988/7134 completed (loss: 0.3164246082305908, acc: 0.9265536665916443)
[2025-02-13 20:00:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:47][root][INFO] - Training Epoch: 2/2, step 989/7134 completed (loss: 0.21717068552970886, acc: 0.9386503100395203)
[2025-02-13 20:00:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:47][root][INFO] - Training Epoch: 2/2, step 990/7134 completed (loss: 0.20964349806308746, acc: 0.9395604133605957)
[2025-02-13 20:00:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:48][root][INFO] - Training Epoch: 2/2, step 991/7134 completed (loss: 0.37607240676879883, acc: 0.9182389974594116)
[2025-02-13 20:00:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:48][root][INFO] - Training Epoch: 2/2, step 992/7134 completed (loss: 0.11051010340452194, acc: 0.9826589822769165)
[2025-02-13 20:00:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:48][root][INFO] - Training Epoch: 2/2, step 993/7134 completed (loss: 0.127028688788414, acc: 0.971222996711731)
[2025-02-13 20:00:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:49][root][INFO] - Training Epoch: 2/2, step 994/7134 completed (loss: 0.15802983939647675, acc: 0.9660193920135498)
[2025-02-13 20:00:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:49][root][INFO] - Training Epoch: 2/2, step 995/7134 completed (loss: 0.12809260189533234, acc: 0.9591836929321289)
[2025-02-13 20:00:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:50][root][INFO] - Training Epoch: 2/2, step 996/7134 completed (loss: 0.16391073167324066, acc: 0.9476743936538696)
[2025-02-13 20:00:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:50][root][INFO] - Training Epoch: 2/2, step 997/7134 completed (loss: 0.08616580814123154, acc: 0.9647058844566345)
[2025-02-13 20:00:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:50][root][INFO] - Training Epoch: 2/2, step 998/7134 completed (loss: 0.15014034509658813, acc: 0.9640718698501587)
[2025-02-13 20:00:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:51][root][INFO] - Training Epoch: 2/2, step 999/7134 completed (loss: 0.11102636903524399, acc: 0.9846153855323792)
[2025-02-13 20:00:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:51][root][INFO] - Training Epoch: 2/2, step 1000/7134 completed (loss: 0.08744170516729355, acc: 0.9852941036224365)
[2025-02-13 20:00:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:52][root][INFO] - Training Epoch: 2/2, step 1001/7134 completed (loss: 0.0619642473757267, acc: 0.9818181991577148)
[2025-02-13 20:00:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:52][root][INFO] - Training Epoch: 2/2, step 1002/7134 completed (loss: 0.06939934194087982, acc: 0.9874213933944702)
[2025-02-13 20:00:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:52][root][INFO] - Training Epoch: 2/2, step 1003/7134 completed (loss: 0.05305900424718857, acc: 0.9930555820465088)
[2025-02-13 20:00:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:53][root][INFO] - Training Epoch: 2/2, step 1004/7134 completed (loss: 0.193182572722435, acc: 0.9292035102844238)
[2025-02-13 20:00:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:53][root][INFO] - Training Epoch: 2/2, step 1005/7134 completed (loss: 0.14092904329299927, acc: 0.9645389914512634)
[2025-02-13 20:00:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:53][root][INFO] - Training Epoch: 2/2, step 1006/7134 completed (loss: 0.11795918643474579, acc: 0.9681528806686401)
[2025-02-13 20:00:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:54][root][INFO] - Training Epoch: 2/2, step 1007/7134 completed (loss: 0.3929567337036133, acc: 0.9154929518699646)
[2025-02-13 20:00:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:54][root][INFO] - Training Epoch: 2/2, step 1008/7134 completed (loss: 0.11282409727573395, acc: 0.9691358208656311)
[2025-02-13 20:00:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:55][root][INFO] - Training Epoch: 2/2, step 1009/7134 completed (loss: 0.18458466231822968, acc: 0.961240291595459)
[2025-02-13 20:00:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:55][root][INFO] - Training Epoch: 2/2, step 1010/7134 completed (loss: 0.11911024153232574, acc: 0.9743589758872986)
[2025-02-13 20:00:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:55][root][INFO] - Training Epoch: 2/2, step 1011/7134 completed (loss: 0.080172598361969, acc: 0.9811320900917053)
[2025-02-13 20:00:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:56][root][INFO] - Training Epoch: 2/2, step 1012/7134 completed (loss: 0.17790727317333221, acc: 0.9624999761581421)
[2025-02-13 20:00:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:56][root][INFO] - Training Epoch: 2/2, step 1013/7134 completed (loss: 0.23937523365020752, acc: 0.9397590160369873)
[2025-02-13 20:00:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:56][root][INFO] - Training Epoch: 2/2, step 1014/7134 completed (loss: 0.1470567286014557, acc: 0.9503546357154846)
[2025-02-13 20:00:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:57][root][INFO] - Training Epoch: 2/2, step 1015/7134 completed (loss: 0.09754451364278793, acc: 0.9735099077224731)
[2025-02-13 20:00:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:57][root][INFO] - Training Epoch: 2/2, step 1016/7134 completed (loss: 0.06556900590658188, acc: 0.9831932783126831)
[2025-02-13 20:00:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:58][root][INFO] - Training Epoch: 2/2, step 1017/7134 completed (loss: 0.05953620374202728, acc: 0.9885714054107666)
[2025-02-13 20:00:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:58][root][INFO] - Training Epoch: 2/2, step 1018/7134 completed (loss: 0.16816338896751404, acc: 0.9503105878829956)
[2025-02-13 20:00:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:59][root][INFO] - Training Epoch: 2/2, step 1019/7134 completed (loss: 0.05085262283682823, acc: 0.9935064911842346)
[2025-02-13 20:00:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:59][root][INFO] - Training Epoch: 2/2, step 1020/7134 completed (loss: 0.02883678488433361, acc: 1.0)
[2025-02-13 20:00:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:00:59][root][INFO] - Training Epoch: 2/2, step 1021/7134 completed (loss: 0.09139279276132584, acc: 0.9738562107086182)
[2025-02-13 20:01:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:00][root][INFO] - Training Epoch: 2/2, step 1022/7134 completed (loss: 0.03382661193609238, acc: 0.9884393215179443)
[2025-02-13 20:01:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:00][root][INFO] - Training Epoch: 2/2, step 1023/7134 completed (loss: 0.07603728771209717, acc: 0.9888268113136292)
[2025-02-13 20:01:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:01][root][INFO] - Training Epoch: 2/2, step 1024/7134 completed (loss: 0.14076454937458038, acc: 0.9701492786407471)
[2025-02-13 20:01:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:01][root][INFO] - Training Epoch: 2/2, step 1025/7134 completed (loss: 0.09901716560125351, acc: 0.9716312289237976)
[2025-02-13 20:01:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:02][root][INFO] - Training Epoch: 2/2, step 1026/7134 completed (loss: 0.2887929081916809, acc: 0.9469026327133179)
[2025-02-13 20:01:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:02][root][INFO] - Training Epoch: 2/2, step 1027/7134 completed (loss: 0.07059326767921448, acc: 0.970588207244873)
[2025-02-13 20:01:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:02][root][INFO] - Training Epoch: 2/2, step 1028/7134 completed (loss: 0.025879772379994392, acc: 1.0)
[2025-02-13 20:01:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:03][root][INFO] - Training Epoch: 2/2, step 1029/7134 completed (loss: 0.04155958443880081, acc: 0.9876543283462524)
[2025-02-13 20:01:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:03][root][INFO] - Training Epoch: 2/2, step 1030/7134 completed (loss: 0.026121389120817184, acc: 0.9885057210922241)
[2025-02-13 20:01:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:04][root][INFO] - Training Epoch: 2/2, step 1031/7134 completed (loss: 0.12347745895385742, acc: 0.9627329111099243)
[2025-02-13 20:01:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:04][root][INFO] - Training Epoch: 2/2, step 1032/7134 completed (loss: 0.119589664041996, acc: 0.9627659320831299)
[2025-02-13 20:01:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:04][root][INFO] - Training Epoch: 2/2, step 1033/7134 completed (loss: 0.036146316677331924, acc: 1.0)
[2025-02-13 20:01:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:05][root][INFO] - Training Epoch: 2/2, step 1034/7134 completed (loss: 0.2266089767217636, acc: 0.9350649118423462)
[2025-02-13 20:01:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:05][root][INFO] - Training Epoch: 2/2, step 1035/7134 completed (loss: 0.335927277803421, acc: 0.8918918967247009)
[2025-02-13 20:01:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:05][root][INFO] - Training Epoch: 2/2, step 1036/7134 completed (loss: 0.503669023513794, acc: 0.9067796468734741)
[2025-02-13 20:01:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:06][root][INFO] - Training Epoch: 2/2, step 1037/7134 completed (loss: 0.09652832895517349, acc: 0.9734042286872864)
[2025-02-13 20:01:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:06][root][INFO] - Training Epoch: 2/2, step 1038/7134 completed (loss: 0.11798117309808731, acc: 0.984000027179718)
[2025-02-13 20:01:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:07][root][INFO] - Training Epoch: 2/2, step 1039/7134 completed (loss: 0.1251702755689621, acc: 0.9724770784378052)
[2025-02-13 20:01:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:07][root][INFO] - Training Epoch: 2/2, step 1040/7134 completed (loss: 0.04905858635902405, acc: 0.9855072498321533)
[2025-02-13 20:01:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:08][root][INFO] - Training Epoch: 2/2, step 1041/7134 completed (loss: 0.03778393566608429, acc: 1.0)
[2025-02-13 20:01:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:08][root][INFO] - Training Epoch: 2/2, step 1042/7134 completed (loss: 0.11999277025461197, acc: 0.9664804339408875)
[2025-02-13 20:01:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:08][root][INFO] - Training Epoch: 2/2, step 1043/7134 completed (loss: 0.10574852675199509, acc: 0.9900990128517151)
[2025-02-13 20:01:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:09][root][INFO] - Training Epoch: 2/2, step 1044/7134 completed (loss: 0.05844084173440933, acc: 0.9813664555549622)
[2025-02-13 20:01:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:09][root][INFO] - Training Epoch: 2/2, step 1045/7134 completed (loss: 0.2785063683986664, acc: 0.9378882050514221)
[2025-02-13 20:01:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:10][root][INFO] - Training Epoch: 2/2, step 1046/7134 completed (loss: 0.11441703140735626, acc: 0.969924807548523)
[2025-02-13 20:01:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:10][root][INFO] - Training Epoch: 2/2, step 1047/7134 completed (loss: 0.3302786648273468, acc: 0.9085714221000671)
[2025-02-13 20:01:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:11][root][INFO] - Training Epoch: 2/2, step 1048/7134 completed (loss: 0.17647238075733185, acc: 0.9681528806686401)
[2025-02-13 20:01:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:11][root][INFO] - Training Epoch: 2/2, step 1049/7134 completed (loss: 0.09668492525815964, acc: 0.9736841917037964)
[2025-02-13 20:01:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:12][root][INFO] - Training Epoch: 2/2, step 1050/7134 completed (loss: 0.2495863437652588, acc: 0.9166666865348816)
[2025-02-13 20:01:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:12][root][INFO] - Training Epoch: 2/2, step 1051/7134 completed (loss: 0.0880928784608841, acc: 0.977142870426178)
[2025-02-13 20:01:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:12][root][INFO] - Training Epoch: 2/2, step 1052/7134 completed (loss: 0.09016198664903641, acc: 0.9759036302566528)
[2025-02-13 20:01:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:13][root][INFO] - Training Epoch: 2/2, step 1053/7134 completed (loss: 0.09078522771596909, acc: 0.976047933101654)
[2025-02-13 20:01:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:13][root][INFO] - Training Epoch: 2/2, step 1054/7134 completed (loss: 0.09275292605161667, acc: 0.9791666865348816)
[2025-02-13 20:01:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:14][root][INFO] - Training Epoch: 2/2, step 1055/7134 completed (loss: 0.07390806823968887, acc: 0.9923664331436157)
[2025-02-13 20:01:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:14][root][INFO] - Training Epoch: 2/2, step 1056/7134 completed (loss: 0.11218084394931793, acc: 0.9767441749572754)
[2025-02-13 20:01:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:14][root][INFO] - Training Epoch: 2/2, step 1057/7134 completed (loss: 0.08981160074472427, acc: 0.9740932583808899)
[2025-02-13 20:01:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:15][root][INFO] - Training Epoch: 2/2, step 1058/7134 completed (loss: 0.10100088268518448, acc: 0.9789473414421082)
[2025-02-13 20:01:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:15][root][INFO] - Training Epoch: 2/2, step 1059/7134 completed (loss: 0.09501403570175171, acc: 0.9763033390045166)
[2025-02-13 20:01:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:15][root][INFO] - Training Epoch: 2/2, step 1060/7134 completed (loss: 0.12488172203302383, acc: 0.9615384340286255)
[2025-02-13 20:01:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:16][root][INFO] - Training Epoch: 2/2, step 1061/7134 completed (loss: 0.10972139239311218, acc: 0.9742268323898315)
[2025-02-13 20:01:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:16][root][INFO] - Training Epoch: 2/2, step 1062/7134 completed (loss: 0.12160000205039978, acc: 0.9638554453849792)
[2025-02-13 20:01:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:17][root][INFO] - Training Epoch: 2/2, step 1063/7134 completed (loss: 0.04402930662035942, acc: 0.9941176176071167)
[2025-02-13 20:01:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:17][root][INFO] - Training Epoch: 2/2, step 1064/7134 completed (loss: 0.16672298312187195, acc: 0.9513513445854187)
[2025-02-13 20:01:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:17][root][INFO] - Training Epoch: 2/2, step 1065/7134 completed (loss: 0.1576748639345169, acc: 0.950276255607605)
[2025-02-13 20:01:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:18][root][INFO] - Training Epoch: 2/2, step 1066/7134 completed (loss: 0.19323743879795074, acc: 0.9541984796524048)
[2025-02-13 20:01:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:18][root][INFO] - Training Epoch: 2/2, step 1067/7134 completed (loss: 0.0721914991736412, acc: 0.9934640526771545)
[2025-02-13 20:01:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:19][root][INFO] - Training Epoch: 2/2, step 1068/7134 completed (loss: 0.09891712665557861, acc: 0.9765625)
[2025-02-13 20:01:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:19][root][INFO] - Training Epoch: 2/2, step 1069/7134 completed (loss: 0.11599474400281906, acc: 0.9722222089767456)
[2025-02-13 20:01:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:20][root][INFO] - Training Epoch: 2/2, step 1070/7134 completed (loss: 0.2556718587875366, acc: 0.9383561611175537)
[2025-02-13 20:01:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:20][root][INFO] - Training Epoch: 2/2, step 1071/7134 completed (loss: 0.0573049858212471, acc: 0.981249988079071)
[2025-02-13 20:01:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:20][root][INFO] - Training Epoch: 2/2, step 1072/7134 completed (loss: 0.03865085169672966, acc: 0.9878048896789551)
[2025-02-13 20:01:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:21][root][INFO] - Training Epoch: 2/2, step 1073/7134 completed (loss: 0.05736871063709259, acc: 0.9813664555549622)
[2025-02-13 20:01:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:21][root][INFO] - Training Epoch: 2/2, step 1074/7134 completed (loss: 0.11043114215135574, acc: 0.9716312289237976)
[2025-02-13 20:01:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:22][root][INFO] - Training Epoch: 2/2, step 1075/7134 completed (loss: 0.31055524945259094, acc: 0.9270073175430298)
[2025-02-13 20:01:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:22][root][INFO] - Training Epoch: 2/2, step 1076/7134 completed (loss: 0.2518095374107361, acc: 0.936170220375061)
[2025-02-13 20:01:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:22][root][INFO] - Training Epoch: 2/2, step 1077/7134 completed (loss: 0.11965712904930115, acc: 0.9557521939277649)
[2025-02-13 20:01:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:23][root][INFO] - Training Epoch: 2/2, step 1078/7134 completed (loss: 0.3901403844356537, acc: 0.9182389974594116)
[2025-02-13 20:01:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:23][root][INFO] - Training Epoch: 2/2, step 1079/7134 completed (loss: 0.177742138504982, acc: 0.9493087530136108)
[2025-02-13 20:01:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:24][root][INFO] - Training Epoch: 2/2, step 1080/7134 completed (loss: 0.18495865166187286, acc: 0.9689922332763672)
[2025-02-13 20:01:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:24][root][INFO] - Training Epoch: 2/2, step 1081/7134 completed (loss: 0.15868090093135834, acc: 0.9638554453849792)
[2025-02-13 20:01:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:24][root][INFO] - Training Epoch: 2/2, step 1082/7134 completed (loss: 0.04981055110692978, acc: 0.9775784611701965)
[2025-02-13 20:01:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:25][root][INFO] - Training Epoch: 2/2, step 1083/7134 completed (loss: 0.11981664597988129, acc: 0.9746192693710327)
[2025-02-13 20:01:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:25][root][INFO] - Training Epoch: 2/2, step 1084/7134 completed (loss: 0.05822398141026497, acc: 0.9840425252914429)
[2025-02-13 20:01:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:26][root][INFO] - Training Epoch: 2/2, step 1085/7134 completed (loss: 0.06266874819993973, acc: 0.9811320900917053)
[2025-02-13 20:01:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:26][root][INFO] - Training Epoch: 2/2, step 1086/7134 completed (loss: 0.16198861598968506, acc: 0.9354838728904724)
[2025-02-13 20:01:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:26][root][INFO] - Training Epoch: 2/2, step 1087/7134 completed (loss: 0.09094854444265366, acc: 0.9784946441650391)
[2025-02-13 20:01:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:27][root][INFO] - Training Epoch: 2/2, step 1088/7134 completed (loss: 0.09571672230958939, acc: 0.9829545617103577)
[2025-02-13 20:01:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:27][root][INFO] - Training Epoch: 2/2, step 1089/7134 completed (loss: 0.17470017075538635, acc: 0.9631901979446411)
[2025-02-13 20:01:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:28][root][INFO] - Training Epoch: 2/2, step 1090/7134 completed (loss: 0.17323021590709686, acc: 0.9523809552192688)
[2025-02-13 20:01:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:28][root][INFO] - Training Epoch: 2/2, step 1091/7134 completed (loss: 0.12232107669115067, acc: 0.9627329111099243)
[2025-02-13 20:01:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:29][root][INFO] - Training Epoch: 2/2, step 1092/7134 completed (loss: 0.0864076241850853, acc: 0.9880239367485046)
[2025-02-13 20:01:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:29][root][INFO] - Training Epoch: 2/2, step 1093/7134 completed (loss: 0.09006261825561523, acc: 0.9795918464660645)
[2025-02-13 20:01:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:29][root][INFO] - Training Epoch: 2/2, step 1094/7134 completed (loss: 0.32786861062049866, acc: 0.9398496150970459)
[2025-02-13 20:01:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:30][root][INFO] - Training Epoch: 2/2, step 1095/7134 completed (loss: 0.12399955093860626, acc: 0.9788359999656677)
[2025-02-13 20:01:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:30][root][INFO] - Training Epoch: 2/2, step 1096/7134 completed (loss: 0.2237999141216278, acc: 0.9301075339317322)
[2025-02-13 20:01:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:31][root][INFO] - Training Epoch: 2/2, step 1097/7134 completed (loss: 0.2053062617778778, acc: 0.9562841653823853)
[2025-02-13 20:01:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:31][root][INFO] - Training Epoch: 2/2, step 1098/7134 completed (loss: 0.05529990792274475, acc: 0.9833333492279053)
[2025-02-13 20:01:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:31][root][INFO] - Training Epoch: 2/2, step 1099/7134 completed (loss: 0.15022951364517212, acc: 0.9583333134651184)
[2025-02-13 20:01:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:32][root][INFO] - Training Epoch: 2/2, step 1100/7134 completed (loss: 0.10787638276815414, acc: 0.9649122953414917)
[2025-02-13 20:01:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:32][root][INFO] - Training Epoch: 2/2, step 1101/7134 completed (loss: 0.2274176925420761, acc: 0.9487179517745972)
[2025-02-13 20:01:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:33][root][INFO] - Training Epoch: 2/2, step 1102/7134 completed (loss: 0.3401614725589752, acc: 0.9097222089767456)
[2025-02-13 20:01:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:33][root][INFO] - Training Epoch: 2/2, step 1103/7134 completed (loss: 0.40277358889579773, acc: 0.9047619104385376)
[2025-02-13 20:01:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:33][root][INFO] - Training Epoch: 2/2, step 1104/7134 completed (loss: 0.09361770004034042, acc: 0.9733333587646484)
[2025-02-13 20:01:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:34][root][INFO] - Training Epoch: 2/2, step 1105/7134 completed (loss: 0.12209697812795639, acc: 0.9506173133850098)
[2025-02-13 20:01:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:34][root][INFO] - Training Epoch: 2/2, step 1106/7134 completed (loss: 0.05992277339100838, acc: 0.9735099077224731)
[2025-02-13 20:01:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:34][root][INFO] - Training Epoch: 2/2, step 1107/7134 completed (loss: 0.038414422422647476, acc: 0.9944444298744202)
[2025-02-13 20:01:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:35][root][INFO] - Training Epoch: 2/2, step 1108/7134 completed (loss: 0.06549457460641861, acc: 0.9822485446929932)
[2025-02-13 20:01:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:35][root][INFO] - Training Epoch: 2/2, step 1109/7134 completed (loss: 0.11975476145744324, acc: 0.9653179049491882)
[2025-02-13 20:01:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:36][root][INFO] - Training Epoch: 2/2, step 1110/7134 completed (loss: 0.13609427213668823, acc: 0.9731183052062988)
[2025-02-13 20:01:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:36][root][INFO] - Training Epoch: 2/2, step 1111/7134 completed (loss: 0.08410190790891647, acc: 0.987500011920929)
[2025-02-13 20:01:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:36][root][INFO] - Training Epoch: 2/2, step 1112/7134 completed (loss: 0.058165524154901505, acc: 0.9860140085220337)
[2025-02-13 20:01:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:37][root][INFO] - Training Epoch: 2/2, step 1113/7134 completed (loss: 0.09768199920654297, acc: 0.9748427867889404)
[2025-02-13 20:01:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:37][root][INFO] - Training Epoch: 2/2, step 1114/7134 completed (loss: 0.18046389520168304, acc: 0.9576719403266907)
[2025-02-13 20:01:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:38][root][INFO] - Training Epoch: 2/2, step 1115/7134 completed (loss: 0.06099674478173256, acc: 0.9806451797485352)
[2025-02-13 20:01:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:38][root][INFO] - Training Epoch: 2/2, step 1116/7134 completed (loss: 0.09429727494716644, acc: 0.9512194991111755)
[2025-02-13 20:01:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:38][root][INFO] - Training Epoch: 2/2, step 1117/7134 completed (loss: 0.055207062512636185, acc: 0.9868420958518982)
[2025-02-13 20:01:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:39][root][INFO] - Training Epoch: 2/2, step 1118/7134 completed (loss: 0.08109214901924133, acc: 0.9694656729698181)
[2025-02-13 20:01:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:39][root][INFO] - Training Epoch: 2/2, step 1119/7134 completed (loss: 0.1073240116238594, acc: 0.9793103337287903)
[2025-02-13 20:01:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:40][root][INFO] - Training Epoch: 2/2, step 1120/7134 completed (loss: 0.1458243727684021, acc: 0.9770992398262024)
[2025-02-13 20:01:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:40][root][INFO] - Training Epoch: 2/2, step 1121/7134 completed (loss: 0.10777958482503891, acc: 0.9741935729980469)
[2025-02-13 20:01:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:40][root][INFO] - Training Epoch: 2/2, step 1122/7134 completed (loss: 0.02322627790272236, acc: 1.0)
[2025-02-13 20:01:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:41][root][INFO] - Training Epoch: 2/2, step 1123/7134 completed (loss: 0.047834958881139755, acc: 0.9862068891525269)
[2025-02-13 20:01:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:41][root][INFO] - Training Epoch: 2/2, step 1124/7134 completed (loss: 0.09027368575334549, acc: 0.9662162065505981)
[2025-02-13 20:01:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:42][root][INFO] - Training Epoch: 2/2, step 1125/7134 completed (loss: 0.04770027473568916, acc: 0.988095223903656)
[2025-02-13 20:01:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:42][root][INFO] - Training Epoch: 2/2, step 1126/7134 completed (loss: 0.09598591923713684, acc: 0.9775280952453613)
[2025-02-13 20:01:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:42][root][INFO] - Training Epoch: 2/2, step 1127/7134 completed (loss: 0.047027770429849625, acc: 0.9852941036224365)
[2025-02-13 20:01:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:43][root][INFO] - Training Epoch: 2/2, step 1128/7134 completed (loss: 0.04246047884225845, acc: 1.0)
[2025-02-13 20:01:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:43][root][INFO] - Training Epoch: 2/2, step 1129/7134 completed (loss: 0.13300476968288422, acc: 0.9750000238418579)
[2025-02-13 20:01:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:44][root][INFO] - Training Epoch: 2/2, step 1130/7134 completed (loss: 0.025987984612584114, acc: 0.9928057789802551)
[2025-02-13 20:01:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:44][root][INFO] - Training Epoch: 2/2, step 1131/7134 completed (loss: 0.10356053709983826, acc: 0.9898989796638489)
[2025-02-13 20:01:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:44][root][INFO] - Training Epoch: 2/2, step 1132/7134 completed (loss: 0.3066653609275818, acc: 0.9328858852386475)
[2025-02-13 20:01:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:45][root][INFO] - Training Epoch: 2/2, step 1133/7134 completed (loss: 0.15987543761730194, acc: 0.9710982441902161)
[2025-02-13 20:01:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:45][root][INFO] - Training Epoch: 2/2, step 1134/7134 completed (loss: 0.14097872376441956, acc: 0.9725274443626404)
[2025-02-13 20:01:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:46][root][INFO] - Training Epoch: 2/2, step 1135/7134 completed (loss: 0.13687558472156525, acc: 0.9664429426193237)
[2025-02-13 20:01:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:46][root][INFO] - Training Epoch: 2/2, step 1136/7134 completed (loss: 0.21938343346118927, acc: 0.9387755393981934)
[2025-02-13 20:01:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:46][root][INFO] - Training Epoch: 2/2, step 1137/7134 completed (loss: 0.13714858889579773, acc: 0.9716981053352356)
[2025-02-13 20:01:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:47][root][INFO] - Training Epoch: 2/2, step 1138/7134 completed (loss: 0.11544906347990036, acc: 0.9649999737739563)
[2025-02-13 20:01:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:47][root][INFO] - Training Epoch: 2/2, step 1139/7134 completed (loss: 0.18034400045871735, acc: 0.9560439586639404)
[2025-02-13 20:01:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:48][root][INFO] - Training Epoch: 2/2, step 1140/7134 completed (loss: 0.05635282024741173, acc: 0.9894737005233765)
[2025-02-13 20:01:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:48][root][INFO] - Training Epoch: 2/2, step 1141/7134 completed (loss: 0.04077036306262016, acc: 0.9909090995788574)
[2025-02-13 20:01:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:48][root][INFO] - Training Epoch: 2/2, step 1142/7134 completed (loss: 0.13818268477916718, acc: 0.9666666388511658)
[2025-02-13 20:01:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:49][root][INFO] - Training Epoch: 2/2, step 1143/7134 completed (loss: 0.21160122752189636, acc: 0.9397590160369873)
[2025-02-13 20:01:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:49][root][INFO] - Training Epoch: 2/2, step 1144/7134 completed (loss: 0.1984187513589859, acc: 0.976190447807312)
[2025-02-13 20:01:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:50][root][INFO] - Training Epoch: 2/2, step 1145/7134 completed (loss: 0.094504714012146, acc: 0.9888268113136292)
[2025-02-13 20:01:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:50][root][INFO] - Training Epoch: 2/2, step 1146/7134 completed (loss: 0.026494961231946945, acc: 1.0)
[2025-02-13 20:01:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:50][root][INFO] - Training Epoch: 2/2, step 1147/7134 completed (loss: 0.03346700593829155, acc: 0.9945945739746094)
[2025-02-13 20:01:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:51][root][INFO] - Training Epoch: 2/2, step 1148/7134 completed (loss: 0.06589124351739883, acc: 0.9784172773361206)
[2025-02-13 20:01:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:51][root][INFO] - Training Epoch: 2/2, step 1149/7134 completed (loss: 0.12977364659309387, acc: 0.9652777910232544)
[2025-02-13 20:01:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:52][root][INFO] - Training Epoch: 2/2, step 1150/7134 completed (loss: 0.07243682444095612, acc: 0.9803921580314636)
[2025-02-13 20:01:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:52][root][INFO] - Training Epoch: 2/2, step 1151/7134 completed (loss: 0.0642767995595932, acc: 0.9915966391563416)
[2025-02-13 20:01:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:52][root][INFO] - Training Epoch: 2/2, step 1152/7134 completed (loss: 0.035956237465143204, acc: 1.0)
[2025-02-13 20:01:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:53][root][INFO] - Training Epoch: 2/2, step 1153/7134 completed (loss: 0.1574058085680008, acc: 0.9803921580314636)
[2025-02-13 20:01:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:53][root][INFO] - Training Epoch: 2/2, step 1154/7134 completed (loss: 0.09734155237674713, acc: 0.984375)
[2025-02-13 20:01:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:54][root][INFO] - Training Epoch: 2/2, step 1155/7134 completed (loss: 0.20026849210262299, acc: 0.9316239356994629)
[2025-02-13 20:01:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:54][root][INFO] - Training Epoch: 2/2, step 1156/7134 completed (loss: 0.044487226754426956, acc: 0.9924242496490479)
[2025-02-13 20:01:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:54][root][INFO] - Training Epoch: 2/2, step 1157/7134 completed (loss: 0.27514559030532837, acc: 0.9379310607910156)
[2025-02-13 20:01:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:55][root][INFO] - Training Epoch: 2/2, step 1158/7134 completed (loss: 0.22736327350139618, acc: 0.9652174115180969)
[2025-02-13 20:01:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:55][root][INFO] - Training Epoch: 2/2, step 1159/7134 completed (loss: 0.12304306030273438, acc: 0.970059871673584)
[2025-02-13 20:01:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:56][root][INFO] - Training Epoch: 2/2, step 1160/7134 completed (loss: 0.057729728519916534, acc: 0.9876543283462524)
[2025-02-13 20:01:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:56][root][INFO] - Training Epoch: 2/2, step 1161/7134 completed (loss: 0.0916425883769989, acc: 0.9710144996643066)
[2025-02-13 20:01:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:56][root][INFO] - Training Epoch: 2/2, step 1162/7134 completed (loss: 0.1795533001422882, acc: 0.949367105960846)
[2025-02-13 20:01:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:57][root][INFO] - Training Epoch: 2/2, step 1163/7134 completed (loss: 0.06378590315580368, acc: 0.9862068891525269)
[2025-02-13 20:01:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:57][root][INFO] - Training Epoch: 2/2, step 1164/7134 completed (loss: 0.12064225971698761, acc: 0.9668508172035217)
[2025-02-13 20:01:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:58][root][INFO] - Training Epoch: 2/2, step 1165/7134 completed (loss: 0.026403693482279778, acc: 0.9948717951774597)
[2025-02-13 20:01:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:58][root][INFO] - Training Epoch: 2/2, step 1166/7134 completed (loss: 0.04291777312755585, acc: 0.9946808218955994)
[2025-02-13 20:01:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:58][root][INFO] - Training Epoch: 2/2, step 1167/7134 completed (loss: 0.021395180374383926, acc: 1.0)
[2025-02-13 20:01:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:59][root][INFO] - Training Epoch: 2/2, step 1168/7134 completed (loss: 0.07384482026100159, acc: 0.9841269850730896)
[2025-02-13 20:01:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:01:59][root][INFO] - Training Epoch: 2/2, step 1169/7134 completed (loss: 0.1355975866317749, acc: 0.9680851101875305)
[2025-02-13 20:01:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:00][root][INFO] - Training Epoch: 2/2, step 1170/7134 completed (loss: 0.21625182032585144, acc: 0.9491525292396545)
[2025-02-13 20:02:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:00][root][INFO] - Training Epoch: 2/2, step 1171/7134 completed (loss: 0.041321758180856705, acc: 0.9933333396911621)
[2025-02-13 20:02:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:00][root][INFO] - Training Epoch: 2/2, step 1172/7134 completed (loss: 0.12495101243257523, acc: 0.976190447807312)
[2025-02-13 20:02:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:01][root][INFO] - Training Epoch: 2/2, step 1173/7134 completed (loss: 0.08890824019908905, acc: 0.9788359999656677)
[2025-02-13 20:02:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:01][root][INFO] - Training Epoch: 2/2, step 1174/7134 completed (loss: 0.055004265159368515, acc: 0.9754601120948792)
[2025-02-13 20:02:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:02][root][INFO] - Training Epoch: 2/2, step 1175/7134 completed (loss: 0.125360369682312, acc: 0.9764705896377563)
[2025-02-13 20:02:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:02][root][INFO] - Training Epoch: 2/2, step 1176/7134 completed (loss: 0.040610477328300476, acc: 0.9861111044883728)
[2025-02-13 20:02:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:02][root][INFO] - Training Epoch: 2/2, step 1177/7134 completed (loss: 0.021799685433506966, acc: 1.0)
[2025-02-13 20:02:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:03][root][INFO] - Training Epoch: 2/2, step 1178/7134 completed (loss: 0.06388489156961441, acc: 0.9807692170143127)
[2025-02-13 20:02:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:03][root][INFO] - Training Epoch: 2/2, step 1179/7134 completed (loss: 0.035335056483745575, acc: 0.9919354915618896)
[2025-02-13 20:02:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:03][root][INFO] - Training Epoch: 2/2, step 1180/7134 completed (loss: 0.058146875351667404, acc: 0.9864864945411682)
[2025-02-13 20:02:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:04][root][INFO] - Training Epoch: 2/2, step 1181/7134 completed (loss: 0.04399602860212326, acc: 0.9890109896659851)
[2025-02-13 20:02:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:04][root][INFO] - Training Epoch: 2/2, step 1182/7134 completed (loss: 0.033476006239652634, acc: 0.9931972622871399)
[2025-02-13 20:02:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:05][root][INFO] - Training Epoch: 2/2, step 1183/7134 completed (loss: 0.024042312055826187, acc: 0.9942196607589722)
[2025-02-13 20:02:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:05][root][INFO] - Training Epoch: 2/2, step 1184/7134 completed (loss: 0.0958874300122261, acc: 0.9710982441902161)
[2025-02-13 20:02:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:05][root][INFO] - Training Epoch: 2/2, step 1185/7134 completed (loss: 0.02879934012889862, acc: 0.9939393997192383)
[2025-02-13 20:02:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:06][root][INFO] - Training Epoch: 2/2, step 1186/7134 completed (loss: 0.04973525181412697, acc: 0.9945651888847351)
[2025-02-13 20:02:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:06][root][INFO] - Training Epoch: 2/2, step 1187/7134 completed (loss: 0.07451702654361725, acc: 0.9821428656578064)
[2025-02-13 20:02:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:07][root][INFO] - Training Epoch: 2/2, step 1188/7134 completed (loss: 0.0418996587395668, acc: 0.9892473220825195)
[2025-02-13 20:02:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:07][root][INFO] - Training Epoch: 2/2, step 1189/7134 completed (loss: 0.10129036009311676, acc: 0.976331353187561)
[2025-02-13 20:02:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:07][root][INFO] - Training Epoch: 2/2, step 1190/7134 completed (loss: 0.03299010172486305, acc: 0.9777777791023254)
[2025-02-13 20:02:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:08][root][INFO] - Training Epoch: 2/2, step 1191/7134 completed (loss: 0.310045063495636, acc: 0.9455782175064087)
[2025-02-13 20:02:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:08][root][INFO] - Training Epoch: 2/2, step 1192/7134 completed (loss: 0.023008650168776512, acc: 1.0)
[2025-02-13 20:02:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:09][root][INFO] - Training Epoch: 2/2, step 1193/7134 completed (loss: 0.08907783776521683, acc: 0.9921259880065918)
[2025-02-13 20:02:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:09][root][INFO] - Training Epoch: 2/2, step 1194/7134 completed (loss: 0.08438634872436523, acc: 0.9731543660163879)
[2025-02-13 20:02:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:09][root][INFO] - Training Epoch: 2/2, step 1195/7134 completed (loss: 0.05897348374128342, acc: 0.9923076629638672)
[2025-02-13 20:02:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:10][root][INFO] - Training Epoch: 2/2, step 1196/7134 completed (loss: 0.1891280561685562, acc: 0.9645389914512634)
[2025-02-13 20:02:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:10][root][INFO] - Training Epoch: 2/2, step 1197/7134 completed (loss: 0.09789223968982697, acc: 0.9863013625144958)
[2025-02-13 20:02:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:11][root][INFO] - Training Epoch: 2/2, step 1198/7134 completed (loss: 0.13058337569236755, acc: 0.9750000238418579)
[2025-02-13 20:02:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:11][root][INFO] - Training Epoch: 2/2, step 1199/7134 completed (loss: 0.09971562027931213, acc: 0.9805194735527039)
[2025-02-13 20:02:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:12][root][INFO] - Training Epoch: 2/2, step 1200/7134 completed (loss: 0.038603775203228, acc: 1.0)
[2025-02-13 20:02:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:12][root][INFO] - Training Epoch: 2/2, step 1201/7134 completed (loss: 0.12410148233175278, acc: 0.9685534834861755)
[2025-02-13 20:02:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:12][root][INFO] - Training Epoch: 2/2, step 1202/7134 completed (loss: 0.11675439029932022, acc: 0.97826087474823)
[2025-02-13 20:02:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:13][root][INFO] - Training Epoch: 2/2, step 1203/7134 completed (loss: 0.06426340341567993, acc: 0.9800000190734863)
[2025-02-13 20:02:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:13][root][INFO] - Training Epoch: 2/2, step 1204/7134 completed (loss: 0.1715151071548462, acc: 0.970588207244873)
[2025-02-13 20:02:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:14][root][INFO] - Training Epoch: 2/2, step 1205/7134 completed (loss: 0.23183679580688477, acc: 0.9482758641242981)
[2025-02-13 20:02:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:14][root][INFO] - Training Epoch: 2/2, step 1206/7134 completed (loss: 0.1346929967403412, acc: 0.979899525642395)
[2025-02-13 20:02:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:15][root][INFO] - Training Epoch: 2/2, step 1207/7134 completed (loss: 0.09361698478460312, acc: 0.9838709831237793)
[2025-02-13 20:02:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:15][root][INFO] - Training Epoch: 2/2, step 1208/7134 completed (loss: 0.1990601271390915, acc: 0.9541984796524048)
[2025-02-13 20:02:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:15][root][INFO] - Training Epoch: 2/2, step 1209/7134 completed (loss: 0.22028125822544098, acc: 0.9262295365333557)
[2025-02-13 20:02:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:16][root][INFO] - Training Epoch: 2/2, step 1210/7134 completed (loss: 0.20054911077022552, acc: 0.9399999976158142)
[2025-02-13 20:02:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:16][root][INFO] - Training Epoch: 2/2, step 1211/7134 completed (loss: 0.5427007079124451, acc: 0.8920863270759583)
[2025-02-13 20:02:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:17][root][INFO] - Training Epoch: 2/2, step 1212/7134 completed (loss: 0.2657715380191803, acc: 0.9416058659553528)
[2025-02-13 20:02:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:17][root][INFO] - Training Epoch: 2/2, step 1213/7134 completed (loss: 0.2564171254634857, acc: 0.9349112510681152)
[2025-02-13 20:02:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:17][root][INFO] - Training Epoch: 2/2, step 1214/7134 completed (loss: 0.26725003123283386, acc: 0.9133333563804626)
[2025-02-13 20:02:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:18][root][INFO] - Training Epoch: 2/2, step 1215/7134 completed (loss: 0.24941852688789368, acc: 0.929411768913269)
[2025-02-13 20:02:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:18][root][INFO] - Training Epoch: 2/2, step 1216/7134 completed (loss: 0.1293451488018036, acc: 0.9668874144554138)
[2025-02-13 20:02:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:19][root][INFO] - Training Epoch: 2/2, step 1217/7134 completed (loss: 0.1639784574508667, acc: 0.9671052694320679)
[2025-02-13 20:02:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:19][root][INFO] - Training Epoch: 2/2, step 1218/7134 completed (loss: 0.14202377200126648, acc: 0.9624999761581421)
[2025-02-13 20:02:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:19][root][INFO] - Training Epoch: 2/2, step 1219/7134 completed (loss: 0.2496340125799179, acc: 0.9466666579246521)
[2025-02-13 20:02:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:20][root][INFO] - Training Epoch: 2/2, step 1220/7134 completed (loss: 0.2170623540878296, acc: 0.9536082744598389)
[2025-02-13 20:02:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:20][root][INFO] - Training Epoch: 2/2, step 1221/7134 completed (loss: 0.1688769906759262, acc: 0.9547511339187622)
[2025-02-13 20:02:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:21][root][INFO] - Training Epoch: 2/2, step 1222/7134 completed (loss: 0.3318374752998352, acc: 0.9320388436317444)
[2025-02-13 20:02:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:21][root][INFO] - Training Epoch: 2/2, step 1223/7134 completed (loss: 0.30202099680900574, acc: 0.9375)
[2025-02-13 20:02:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:21][root][INFO] - Training Epoch: 2/2, step 1224/7134 completed (loss: 0.17673762142658234, acc: 0.9528301954269409)
[2025-02-13 20:02:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:22][root][INFO] - Training Epoch: 2/2, step 1225/7134 completed (loss: 0.2582578957080841, acc: 0.9406392574310303)
[2025-02-13 20:02:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:22][root][INFO] - Training Epoch: 2/2, step 1226/7134 completed (loss: 0.379841685295105, acc: 0.9126983880996704)
[2025-02-13 20:02:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:23][root][INFO] - Training Epoch: 2/2, step 1227/7134 completed (loss: 0.3706696927547455, acc: 0.9278350472450256)
[2025-02-13 20:02:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:23][root][INFO] - Training Epoch: 2/2, step 1228/7134 completed (loss: 0.40797650814056396, acc: 0.9360465407371521)
[2025-02-13 20:02:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:23][root][INFO] - Training Epoch: 2/2, step 1229/7134 completed (loss: 0.12284716963768005, acc: 0.9801980257034302)
[2025-02-13 20:02:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:24][root][INFO] - Training Epoch: 2/2, step 1230/7134 completed (loss: 0.29085665941238403, acc: 0.9245283007621765)
[2025-02-13 20:02:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:24][root][INFO] - Training Epoch: 2/2, step 1231/7134 completed (loss: 0.24064713716506958, acc: 0.9353448152542114)
[2025-02-13 20:02:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:25][root][INFO] - Training Epoch: 2/2, step 1232/7134 completed (loss: 0.25645968317985535, acc: 0.9372197389602661)
[2025-02-13 20:02:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:25][root][INFO] - Training Epoch: 2/2, step 1233/7134 completed (loss: 0.30463141202926636, acc: 0.9095022678375244)
[2025-02-13 20:02:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:26][root][INFO] - Training Epoch: 2/2, step 1234/7134 completed (loss: 0.1076861247420311, acc: 0.9615384340286255)
[2025-02-13 20:02:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:26][root][INFO] - Training Epoch: 2/2, step 1235/7134 completed (loss: 0.17584317922592163, acc: 0.9693877696990967)
[2025-02-13 20:02:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:26][root][INFO] - Training Epoch: 2/2, step 1236/7134 completed (loss: 0.26480570435523987, acc: 0.929411768913269)
[2025-02-13 20:02:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:27][root][INFO] - Training Epoch: 2/2, step 1237/7134 completed (loss: 0.19651617109775543, acc: 0.9602649211883545)
[2025-02-13 20:02:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:27][root][INFO] - Training Epoch: 2/2, step 1238/7134 completed (loss: 0.7214102149009705, acc: 0.8186046481132507)
[2025-02-13 20:02:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:28][root][INFO] - Training Epoch: 2/2, step 1239/7134 completed (loss: 0.2825517952442169, acc: 0.9329268336296082)
[2025-02-13 20:02:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:28][root][INFO] - Training Epoch: 2/2, step 1240/7134 completed (loss: 0.30386918783187866, acc: 0.9127907156944275)
[2025-02-13 20:02:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:28][root][INFO] - Training Epoch: 2/2, step 1241/7134 completed (loss: 0.060243818908929825, acc: 0.9836065769195557)
[2025-02-13 20:02:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:29][root][INFO] - Training Epoch: 2/2, step 1242/7134 completed (loss: 0.2728447914123535, acc: 0.9319371581077576)
[2025-02-13 20:02:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:29][root][INFO] - Training Epoch: 2/2, step 1243/7134 completed (loss: 0.2616702914237976, acc: 0.9342105388641357)
[2025-02-13 20:02:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:29][root][INFO] - Training Epoch: 2/2, step 1244/7134 completed (loss: 0.20197629928588867, acc: 0.9577465057373047)
[2025-02-13 20:02:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:30][root][INFO] - Training Epoch: 2/2, step 1245/7134 completed (loss: 0.2917768061161041, acc: 0.939393937587738)
[2025-02-13 20:02:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:30][root][INFO] - Training Epoch: 2/2, step 1246/7134 completed (loss: 0.33895784616470337, acc: 0.9105263352394104)
[2025-02-13 20:02:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:31][root][INFO] - Training Epoch: 2/2, step 1247/7134 completed (loss: 0.148133784532547, acc: 0.9759036302566528)
[2025-02-13 20:02:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:31][root][INFO] - Training Epoch: 2/2, step 1248/7134 completed (loss: 0.1337069571018219, acc: 0.9599999785423279)
[2025-02-13 20:02:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:32][root][INFO] - Training Epoch: 2/2, step 1249/7134 completed (loss: 0.08633190393447876, acc: 0.9913793206214905)
[2025-02-13 20:02:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:32][root][INFO] - Training Epoch: 2/2, step 1250/7134 completed (loss: 0.06736177206039429, acc: 0.9866666793823242)
[2025-02-13 20:02:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:32][root][INFO] - Training Epoch: 2/2, step 1251/7134 completed (loss: 0.09028665721416473, acc: 0.9939393997192383)
[2025-02-13 20:02:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:33][root][INFO] - Training Epoch: 2/2, step 1252/7134 completed (loss: 0.08960617333650589, acc: 0.9763779640197754)
[2025-02-13 20:02:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:33][root][INFO] - Training Epoch: 2/2, step 1253/7134 completed (loss: 0.20104563236236572, acc: 0.9658119678497314)
[2025-02-13 20:02:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:34][root][INFO] - Training Epoch: 2/2, step 1254/7134 completed (loss: 0.19764374196529388, acc: 0.95333331823349)
[2025-02-13 20:02:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:34][root][INFO] - Training Epoch: 2/2, step 1255/7134 completed (loss: 0.12262442708015442, acc: 0.9862068891525269)
[2025-02-13 20:02:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:34][root][INFO] - Training Epoch: 2/2, step 1256/7134 completed (loss: 0.08713649213314056, acc: 0.9847328066825867)
[2025-02-13 20:02:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:35][root][INFO] - Training Epoch: 2/2, step 1257/7134 completed (loss: 0.07944772392511368, acc: 0.9694656729698181)
[2025-02-13 20:02:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:35][root][INFO] - Training Epoch: 2/2, step 1258/7134 completed (loss: 0.1628565788269043, acc: 0.9702380895614624)
[2025-02-13 20:02:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:36][root][INFO] - Training Epoch: 2/2, step 1259/7134 completed (loss: 0.24697190523147583, acc: 0.9519230723381042)
[2025-02-13 20:02:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:36][root][INFO] - Training Epoch: 2/2, step 1260/7134 completed (loss: 0.09118856489658356, acc: 0.9865771532058716)
[2025-02-13 20:02:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:36][root][INFO] - Training Epoch: 2/2, step 1261/7134 completed (loss: 0.11776657402515411, acc: 0.9864864945411682)
[2025-02-13 20:02:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:37][root][INFO] - Training Epoch: 2/2, step 1262/7134 completed (loss: 0.016395503655076027, acc: 1.0)
[2025-02-13 20:02:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:37][root][INFO] - Training Epoch: 2/2, step 1263/7134 completed (loss: 0.06079654023051262, acc: 0.9870967864990234)
[2025-02-13 20:02:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:38][root][INFO] - Training Epoch: 2/2, step 1264/7134 completed (loss: 0.05416186898946762, acc: 0.9851852059364319)
[2025-02-13 20:02:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:38][root][INFO] - Training Epoch: 2/2, step 1265/7134 completed (loss: 0.087801992893219, acc: 0.9828571677207947)
[2025-02-13 20:02:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:38][root][INFO] - Training Epoch: 2/2, step 1266/7134 completed (loss: 0.11645122617483139, acc: 0.9541984796524048)
[2025-02-13 20:02:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:39][root][INFO] - Training Epoch: 2/2, step 1267/7134 completed (loss: 0.06727667897939682, acc: 0.9870129823684692)
[2025-02-13 20:02:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:39][root][INFO] - Training Epoch: 2/2, step 1268/7134 completed (loss: 0.17019778490066528, acc: 0.9407407641410828)
[2025-02-13 20:02:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:40][root][INFO] - Training Epoch: 2/2, step 1269/7134 completed (loss: 0.46979716420173645, acc: 0.939393937587738)
[2025-02-13 20:02:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:40][root][INFO] - Training Epoch: 2/2, step 1270/7134 completed (loss: 0.09783453494310379, acc: 0.9794520735740662)
[2025-02-13 20:02:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:40][root][INFO] - Training Epoch: 2/2, step 1271/7134 completed (loss: 0.19910751283168793, acc: 0.9583333134651184)
[2025-02-13 20:02:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:41][root][INFO] - Training Epoch: 2/2, step 1272/7134 completed (loss: 0.033217187970876694, acc: 0.9937106966972351)
[2025-02-13 20:02:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:41][root][INFO] - Training Epoch: 2/2, step 1273/7134 completed (loss: 0.1269141286611557, acc: 0.970370352268219)
[2025-02-13 20:02:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:42][root][INFO] - Training Epoch: 2/2, step 1274/7134 completed (loss: 0.07562173157930374, acc: 0.9834710955619812)
[2025-02-13 20:02:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:42][root][INFO] - Training Epoch: 2/2, step 1275/7134 completed (loss: 0.06569524854421616, acc: 0.991304337978363)
[2025-02-13 20:02:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:42][root][INFO] - Training Epoch: 2/2, step 1276/7134 completed (loss: 0.07590791583061218, acc: 0.9772727489471436)
[2025-02-13 20:02:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:43][root][INFO] - Training Epoch: 2/2, step 1277/7134 completed (loss: 0.035515930503606796, acc: 1.0)
[2025-02-13 20:02:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:43][root][INFO] - Training Epoch: 2/2, step 1278/7134 completed (loss: 0.20317447185516357, acc: 0.949999988079071)
[2025-02-13 20:02:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:43][root][INFO] - Training Epoch: 2/2, step 1279/7134 completed (loss: 0.13283422589302063, acc: 0.9510869383811951)
[2025-02-13 20:02:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:44][root][INFO] - Training Epoch: 2/2, step 1280/7134 completed (loss: 0.09179815649986267, acc: 0.9732620120048523)
[2025-02-13 20:02:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:44][root][INFO] - Training Epoch: 2/2, step 1281/7134 completed (loss: 0.1638837456703186, acc: 0.9621211886405945)
[2025-02-13 20:02:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:45][root][INFO] - Training Epoch: 2/2, step 1282/7134 completed (loss: 0.42008692026138306, acc: 0.9125683307647705)
[2025-02-13 20:02:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:45][root][INFO] - Training Epoch: 2/2, step 1283/7134 completed (loss: 0.08116179704666138, acc: 0.9767441749572754)
[2025-02-13 20:02:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:45][root][INFO] - Training Epoch: 2/2, step 1284/7134 completed (loss: 0.213616281747818, acc: 0.9520958065986633)
[2025-02-13 20:02:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:46][root][INFO] - Training Epoch: 2/2, step 1285/7134 completed (loss: 0.18232960999011993, acc: 0.9750000238418579)
[2025-02-13 20:02:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:46][root][INFO] - Training Epoch: 2/2, step 1286/7134 completed (loss: 0.14652180671691895, acc: 0.9817073345184326)
[2025-02-13 20:02:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:47][root][INFO] - Training Epoch: 2/2, step 1287/7134 completed (loss: 0.1696176677942276, acc: 0.9481481313705444)
[2025-02-13 20:02:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:47][root][INFO] - Training Epoch: 2/2, step 1288/7134 completed (loss: 0.08122314512729645, acc: 0.984455943107605)
[2025-02-13 20:02:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:47][root][INFO] - Training Epoch: 2/2, step 1289/7134 completed (loss: 0.22403690218925476, acc: 0.951724112033844)
[2025-02-13 20:02:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:48][root][INFO] - Training Epoch: 2/2, step 1290/7134 completed (loss: 0.06598358601331711, acc: 0.9880239367485046)
[2025-02-13 20:02:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:48][root][INFO] - Training Epoch: 2/2, step 1291/7134 completed (loss: 0.1639498919248581, acc: 0.9647058844566345)
[2025-02-13 20:02:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:49][root][INFO] - Training Epoch: 2/2, step 1292/7134 completed (loss: 0.10105423629283905, acc: 0.9670329689979553)
[2025-02-13 20:02:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:49][root][INFO] - Training Epoch: 2/2, step 1293/7134 completed (loss: 0.11056205630302429, acc: 0.9743589758872986)
[2025-02-13 20:02:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:49][root][INFO] - Training Epoch: 2/2, step 1294/7134 completed (loss: 0.16299690306186676, acc: 0.9508196711540222)
[2025-02-13 20:02:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:50][root][INFO] - Training Epoch: 2/2, step 1295/7134 completed (loss: 0.07330544292926788, acc: 0.9850746393203735)
[2025-02-13 20:02:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:50][root][INFO] - Training Epoch: 2/2, step 1296/7134 completed (loss: 0.06711962074041367, acc: 0.9807692170143127)
[2025-02-13 20:02:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:51][root][INFO] - Training Epoch: 2/2, step 1297/7134 completed (loss: 0.18065492808818817, acc: 0.9553571343421936)
[2025-02-13 20:02:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:51][root][INFO] - Training Epoch: 2/2, step 1298/7134 completed (loss: 0.0796871930360794, acc: 0.9834710955619812)
[2025-02-13 20:02:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:51][root][INFO] - Training Epoch: 2/2, step 1299/7134 completed (loss: 0.08339818567037582, acc: 0.9759036302566528)
[2025-02-13 20:02:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:52][root][INFO] - Training Epoch: 2/2, step 1300/7134 completed (loss: 0.09478242695331573, acc: 0.9851484894752502)
[2025-02-13 20:02:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:52][root][INFO] - Training Epoch: 2/2, step 1301/7134 completed (loss: 0.06916505843400955, acc: 0.9868420958518982)
[2025-02-13 20:02:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:53][root][INFO] - Training Epoch: 2/2, step 1302/7134 completed (loss: 0.20217767357826233, acc: 0.9405940771102905)
[2025-02-13 20:02:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:53][root][INFO] - Training Epoch: 2/2, step 1303/7134 completed (loss: 0.07352510094642639, acc: 0.9900000095367432)
[2025-02-13 20:02:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:54][root][INFO] - Training Epoch: 2/2, step 1304/7134 completed (loss: 0.168214350938797, acc: 0.9604519605636597)
[2025-02-13 20:02:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:54][root][INFO] - Training Epoch: 2/2, step 1305/7134 completed (loss: 0.15872634947299957, acc: 0.969072163105011)
[2025-02-13 20:02:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:54][root][INFO] - Training Epoch: 2/2, step 1306/7134 completed (loss: 0.16263918578624725, acc: 0.9473684430122375)
[2025-02-13 20:02:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:55][root][INFO] - Training Epoch: 2/2, step 1307/7134 completed (loss: 0.045534420758485794, acc: 0.9856114983558655)
[2025-02-13 20:02:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:55][root][INFO] - Training Epoch: 2/2, step 1308/7134 completed (loss: 0.10079783201217651, acc: 0.9741935729980469)
[2025-02-13 20:02:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:56][root][INFO] - Training Epoch: 2/2, step 1309/7134 completed (loss: 0.12949034571647644, acc: 0.9516907930374146)
[2025-02-13 20:02:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:56][root][INFO] - Training Epoch: 2/2, step 1310/7134 completed (loss: 0.12490297108888626, acc: 0.9558011293411255)
[2025-02-13 20:02:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:56][root][INFO] - Training Epoch: 2/2, step 1311/7134 completed (loss: 0.08620517700910568, acc: 0.9711538553237915)
[2025-02-13 20:02:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:57][root][INFO] - Training Epoch: 2/2, step 1312/7134 completed (loss: 0.17319060862064362, acc: 0.9689440727233887)
[2025-02-13 20:02:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:57][root][INFO] - Training Epoch: 2/2, step 1313/7134 completed (loss: 0.2658425569534302, acc: 0.9444444179534912)
[2025-02-13 20:02:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:57][root][INFO] - Training Epoch: 2/2, step 1314/7134 completed (loss: 0.06921009719371796, acc: 0.9855072498321533)
[2025-02-13 20:02:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:58][root][INFO] - Training Epoch: 2/2, step 1315/7134 completed (loss: 0.2016247659921646, acc: 0.9513513445854187)
[2025-02-13 20:02:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:58][root][INFO] - Training Epoch: 2/2, step 1316/7134 completed (loss: 0.2387840300798416, acc: 0.9593495726585388)
[2025-02-13 20:02:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:59][root][INFO] - Training Epoch: 2/2, step 1317/7134 completed (loss: 0.1224028468132019, acc: 0.9552238583564758)
[2025-02-13 20:02:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:59][root][INFO] - Training Epoch: 2/2, step 1318/7134 completed (loss: 0.10267594456672668, acc: 0.9670329689979553)
[2025-02-13 20:02:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:02:59][root][INFO] - Training Epoch: 2/2, step 1319/7134 completed (loss: 0.1432199329137802, acc: 0.9818181991577148)
[2025-02-13 20:02:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:00][root][INFO] - Training Epoch: 2/2, step 1320/7134 completed (loss: 0.22384706139564514, acc: 0.9470198750495911)
[2025-02-13 20:03:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:00][root][INFO] - Training Epoch: 2/2, step 1321/7134 completed (loss: 0.08969028294086456, acc: 0.9792746305465698)
[2025-02-13 20:03:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:00][root][INFO] - Training Epoch: 2/2, step 1322/7134 completed (loss: 0.13635937869548798, acc: 0.9659863710403442)
[2025-02-13 20:03:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:01][root][INFO] - Training Epoch: 2/2, step 1323/7134 completed (loss: 0.2203587144613266, acc: 0.9718309640884399)
[2025-02-13 20:03:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:01][root][INFO] - Training Epoch: 2/2, step 1324/7134 completed (loss: 0.15076711773872375, acc: 0.9607843160629272)
[2025-02-13 20:03:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:02][root][INFO] - Training Epoch: 2/2, step 1325/7134 completed (loss: 0.11614301055669785, acc: 0.9611650705337524)
[2025-02-13 20:03:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:02][root][INFO] - Training Epoch: 2/2, step 1326/7134 completed (loss: 0.27210280299186707, acc: 0.9497717022895813)
[2025-02-13 20:03:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:03][root][INFO] - Training Epoch: 2/2, step 1327/7134 completed (loss: 0.13363279402256012, acc: 0.9644970297813416)
[2025-02-13 20:03:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:03][root][INFO] - Training Epoch: 2/2, step 1328/7134 completed (loss: 0.17266635596752167, acc: 0.9603960514068604)
[2025-02-13 20:03:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:03][root][INFO] - Training Epoch: 2/2, step 1329/7134 completed (loss: 0.1808241456747055, acc: 0.9599999785423279)
[2025-02-13 20:03:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:04][root][INFO] - Training Epoch: 2/2, step 1330/7134 completed (loss: 0.06863141059875488, acc: 0.9856114983558655)
[2025-02-13 20:03:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:04][root][INFO] - Training Epoch: 2/2, step 1331/7134 completed (loss: 0.05154154822230339, acc: 0.98591548204422)
[2025-02-13 20:03:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:04][root][INFO] - Training Epoch: 2/2, step 1332/7134 completed (loss: 0.08301935344934464, acc: 0.9649122953414917)
[2025-02-13 20:03:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:05][root][INFO] - Training Epoch: 2/2, step 1333/7134 completed (loss: 0.15641498565673828, acc: 0.9605262875556946)
[2025-02-13 20:03:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:05][root][INFO] - Training Epoch: 2/2, step 1334/7134 completed (loss: 0.4274061322212219, acc: 0.895061731338501)
[2025-02-13 20:03:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:06][root][INFO] - Training Epoch: 2/2, step 1335/7134 completed (loss: 0.29049891233444214, acc: 0.9424083828926086)
[2025-02-13 20:03:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:06][root][INFO] - Training Epoch: 2/2, step 1336/7134 completed (loss: 0.15394355356693268, acc: 0.9679144620895386)
[2025-02-13 20:03:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:06][root][INFO] - Training Epoch: 2/2, step 1337/7134 completed (loss: 0.17268630862236023, acc: 0.9741935729980469)
[2025-02-13 20:03:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:07][root][INFO] - Training Epoch: 2/2, step 1338/7134 completed (loss: 0.17067022621631622, acc: 0.96875)
[2025-02-13 20:03:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:07][root][INFO] - Training Epoch: 2/2, step 1339/7134 completed (loss: 0.1814829409122467, acc: 0.9591836929321289)
[2025-02-13 20:03:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:08][root][INFO] - Training Epoch: 2/2, step 1340/7134 completed (loss: 0.2758699953556061, acc: 0.9675324559211731)
[2025-02-13 20:03:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:08][root][INFO] - Training Epoch: 2/2, step 1341/7134 completed (loss: 0.05168124660849571, acc: 0.9780219793319702)
[2025-02-13 20:03:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:08][root][INFO] - Training Epoch: 2/2, step 1342/7134 completed (loss: 0.09287883341312408, acc: 0.9735099077224731)
[2025-02-13 20:03:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:09][root][INFO] - Training Epoch: 2/2, step 1343/7134 completed (loss: 0.18378333747386932, acc: 0.9485294222831726)
[2025-02-13 20:03:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:09][root][INFO] - Training Epoch: 2/2, step 1344/7134 completed (loss: 0.0966666042804718, acc: 0.9816513657569885)
[2025-02-13 20:03:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:09][root][INFO] - Training Epoch: 2/2, step 1345/7134 completed (loss: 0.1206250935792923, acc: 0.9784946441650391)
[2025-02-13 20:03:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:10][root][INFO] - Training Epoch: 2/2, step 1346/7134 completed (loss: 0.11216948181390762, acc: 0.9836065769195557)
[2025-02-13 20:03:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:10][root][INFO] - Training Epoch: 2/2, step 1347/7134 completed (loss: 0.11329041421413422, acc: 0.9722222089767456)
[2025-02-13 20:03:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:11][root][INFO] - Training Epoch: 2/2, step 1348/7134 completed (loss: 0.048556987196207047, acc: 0.9944751262664795)
[2025-02-13 20:03:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:11][root][INFO] - Training Epoch: 2/2, step 1349/7134 completed (loss: 0.025357889011502266, acc: 1.0)
[2025-02-13 20:03:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:12][root][INFO] - Training Epoch: 2/2, step 1350/7134 completed (loss: 0.040315233170986176, acc: 0.9880239367485046)
[2025-02-13 20:03:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:12][root][INFO] - Training Epoch: 2/2, step 1351/7134 completed (loss: 0.024556338787078857, acc: 1.0)
[2025-02-13 20:03:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:12][root][INFO] - Training Epoch: 2/2, step 1352/7134 completed (loss: 0.06411249190568924, acc: 0.9878048896789551)
[2025-02-13 20:03:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:13][root][INFO] - Training Epoch: 2/2, step 1353/7134 completed (loss: 0.06444321572780609, acc: 0.9916666746139526)
[2025-02-13 20:03:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:13][root][INFO] - Training Epoch: 2/2, step 1354/7134 completed (loss: 0.017441319301724434, acc: 1.0)
[2025-02-13 20:03:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:14][root][INFO] - Training Epoch: 2/2, step 1355/7134 completed (loss: 0.020138273015618324, acc: 0.9935483932495117)
[2025-02-13 20:03:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:14][root][INFO] - Training Epoch: 2/2, step 1356/7134 completed (loss: 0.06049351394176483, acc: 0.9825581312179565)
[2025-02-13 20:03:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:14][root][INFO] - Training Epoch: 2/2, step 1357/7134 completed (loss: 0.029332417994737625, acc: 0.9947090148925781)
[2025-02-13 20:03:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:15][root][INFO] - Training Epoch: 2/2, step 1358/7134 completed (loss: 0.06255842000246048, acc: 0.9886363744735718)
[2025-02-13 20:03:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:15][root][INFO] - Training Epoch: 2/2, step 1359/7134 completed (loss: 0.021268509328365326, acc: 1.0)
[2025-02-13 20:03:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:16][root][INFO] - Training Epoch: 2/2, step 1360/7134 completed (loss: 0.0144578218460083, acc: 1.0)
[2025-02-13 20:03:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:16][root][INFO] - Training Epoch: 2/2, step 1361/7134 completed (loss: 0.05656451731920242, acc: 0.9881656765937805)
[2025-02-13 20:03:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:17][root][INFO] - Training Epoch: 2/2, step 1362/7134 completed (loss: 0.05723116174340248, acc: 0.9836065769195557)
[2025-02-13 20:03:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:17][root][INFO] - Training Epoch: 2/2, step 1363/7134 completed (loss: 0.05438537523150444, acc: 0.982758641242981)
[2025-02-13 20:03:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:18][root][INFO] - Training Epoch: 2/2, step 1364/7134 completed (loss: 0.06616811454296112, acc: 0.9823529124259949)
[2025-02-13 20:03:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:18][root][INFO] - Training Epoch: 2/2, step 1365/7134 completed (loss: 0.013573974370956421, acc: 1.0)
[2025-02-13 20:03:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:18][root][INFO] - Training Epoch: 2/2, step 1366/7134 completed (loss: 0.02197348326444626, acc: 0.9918699264526367)
[2025-02-13 20:03:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:19][root][INFO] - Training Epoch: 2/2, step 1367/7134 completed (loss: 0.01845088228583336, acc: 1.0)
[2025-02-13 20:03:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:19][root][INFO] - Training Epoch: 2/2, step 1368/7134 completed (loss: 0.04818752035498619, acc: 0.982758641242981)
[2025-02-13 20:03:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:20][root][INFO] - Training Epoch: 2/2, step 1369/7134 completed (loss: 0.050959181040525436, acc: 0.9927007555961609)
[2025-02-13 20:03:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:20][root][INFO] - Training Epoch: 2/2, step 1370/7134 completed (loss: 0.08371560275554657, acc: 0.9795918464660645)
[2025-02-13 20:03:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:21][root][INFO] - Training Epoch: 2/2, step 1371/7134 completed (loss: 0.08584907650947571, acc: 0.9814814925193787)
[2025-02-13 20:03:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:21][root][INFO] - Training Epoch: 2/2, step 1372/7134 completed (loss: 0.12466839700937271, acc: 0.9545454382896423)
[2025-02-13 20:03:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:22][root][INFO] - Training Epoch: 2/2, step 1373/7134 completed (loss: 0.0837840735912323, acc: 0.9844961166381836)
[2025-02-13 20:03:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:22][root][INFO] - Training Epoch: 2/2, step 1374/7134 completed (loss: 0.1561911702156067, acc: 0.9583333134651184)
[2025-02-13 20:03:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:22][root][INFO] - Training Epoch: 2/2, step 1375/7134 completed (loss: 0.16499042510986328, acc: 0.9509803652763367)
[2025-02-13 20:03:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:23][root][INFO] - Training Epoch: 2/2, step 1376/7134 completed (loss: 0.18989421427249908, acc: 0.9375)
[2025-02-13 20:03:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:23][root][INFO] - Training Epoch: 2/2, step 1377/7134 completed (loss: 0.27140218019485474, acc: 0.9313725233078003)
[2025-02-13 20:03:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:24][root][INFO] - Training Epoch: 2/2, step 1378/7134 completed (loss: 0.19503217935562134, acc: 0.9615384340286255)
[2025-02-13 20:03:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:24][root][INFO] - Training Epoch: 2/2, step 1379/7134 completed (loss: 0.1529511958360672, acc: 0.9603960514068604)
[2025-02-13 20:03:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:24][root][INFO] - Training Epoch: 2/2, step 1380/7134 completed (loss: 0.14539887011051178, acc: 0.9814814925193787)
[2025-02-13 20:03:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:25][root][INFO] - Training Epoch: 2/2, step 1381/7134 completed (loss: 0.07404765486717224, acc: 0.9637681245803833)
[2025-02-13 20:03:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:25][root][INFO] - Training Epoch: 2/2, step 1382/7134 completed (loss: 0.1861162781715393, acc: 0.9538461565971375)
[2025-02-13 20:03:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:26][root][INFO] - Training Epoch: 2/2, step 1383/7134 completed (loss: 0.18142655491828918, acc: 0.970588207244873)
[2025-02-13 20:03:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:26][root][INFO] - Training Epoch: 2/2, step 1384/7134 completed (loss: 0.24683518707752228, acc: 0.936170220375061)
[2025-02-13 20:03:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:26][root][INFO] - Training Epoch: 2/2, step 1385/7134 completed (loss: 0.19579890370368958, acc: 0.9440000057220459)
[2025-02-13 20:03:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:27][root][INFO] - Training Epoch: 2/2, step 1386/7134 completed (loss: 0.08724502474069595, acc: 0.9788732528686523)
[2025-02-13 20:03:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:27][root][INFO] - Training Epoch: 2/2, step 1387/7134 completed (loss: 0.08918905258178711, acc: 0.9790209531784058)
[2025-02-13 20:03:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:27][root][INFO] - Training Epoch: 2/2, step 1388/7134 completed (loss: 0.15822118520736694, acc: 0.9520000219345093)
[2025-02-13 20:03:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:28][root][INFO] - Training Epoch: 2/2, step 1389/7134 completed (loss: 0.23187583684921265, acc: 0.9477611780166626)
[2025-02-13 20:03:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:28][root][INFO] - Training Epoch: 2/2, step 1390/7134 completed (loss: 0.1366027146577835, acc: 0.970802903175354)
[2025-02-13 20:03:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:29][root][INFO] - Training Epoch: 2/2, step 1391/7134 completed (loss: 0.22031846642494202, acc: 0.9338235259056091)
[2025-02-13 20:03:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:29][root][INFO] - Training Epoch: 2/2, step 1392/7134 completed (loss: 0.09276050329208374, acc: 0.9837398529052734)
[2025-02-13 20:03:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:29][root][INFO] - Training Epoch: 2/2, step 1393/7134 completed (loss: 0.2103736251592636, acc: 0.9583333134651184)
[2025-02-13 20:03:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:30][root][INFO] - Training Epoch: 2/2, step 1394/7134 completed (loss: 0.10500586777925491, acc: 0.9905660152435303)
[2025-02-13 20:03:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:30][root][INFO] - Training Epoch: 2/2, step 1395/7134 completed (loss: 0.18761591613292694, acc: 0.96875)
[2025-02-13 20:03:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:31][root][INFO] - Training Epoch: 2/2, step 1396/7134 completed (loss: 0.09016162902116776, acc: 0.9760000109672546)
[2025-02-13 20:03:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:31][root][INFO] - Training Epoch: 2/2, step 1397/7134 completed (loss: 0.36463308334350586, acc: 0.9054054021835327)
[2025-02-13 20:03:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:31][root][INFO] - Training Epoch: 2/2, step 1398/7134 completed (loss: 0.059042029082775116, acc: 0.9900990128517151)
[2025-02-13 20:03:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:32][root][INFO] - Training Epoch: 2/2, step 1399/7134 completed (loss: 0.16395454108715057, acc: 0.9568345546722412)
[2025-02-13 20:03:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:32][root][INFO] - Training Epoch: 2/2, step 1400/7134 completed (loss: 0.08275704830884933, acc: 0.9927536249160767)
[2025-02-13 20:03:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:33][root][INFO] - Training Epoch: 2/2, step 1401/7134 completed (loss: 0.07803121209144592, acc: 0.9851852059364319)
[2025-02-13 20:03:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:33][root][INFO] - Training Epoch: 2/2, step 1402/7134 completed (loss: 0.08933427184820175, acc: 0.9850746393203735)
[2025-02-13 20:03:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:33][root][INFO] - Training Epoch: 2/2, step 1403/7134 completed (loss: 0.08640174567699432, acc: 0.9822485446929932)
[2025-02-13 20:03:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:34][root][INFO] - Training Epoch: 2/2, step 1404/7134 completed (loss: 0.0667228102684021, acc: 0.9821428656578064)
[2025-02-13 20:03:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:34][root][INFO] - Training Epoch: 2/2, step 1405/7134 completed (loss: 0.073737733066082, acc: 0.988095223903656)
[2025-02-13 20:03:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:34][root][INFO] - Training Epoch: 2/2, step 1406/7134 completed (loss: 0.028185056522488594, acc: 0.9937888383865356)
[2025-02-13 20:03:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:35][root][INFO] - Training Epoch: 2/2, step 1407/7134 completed (loss: 0.047421395778656006, acc: 0.9941860437393188)
[2025-02-13 20:03:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:35][root][INFO] - Training Epoch: 2/2, step 1408/7134 completed (loss: 0.026539187878370285, acc: 0.9931034445762634)
[2025-02-13 20:03:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:36][root][INFO] - Training Epoch: 2/2, step 1409/7134 completed (loss: 0.05542406812310219, acc: 0.9874213933944702)
[2025-02-13 20:03:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:36][root][INFO] - Training Epoch: 2/2, step 1410/7134 completed (loss: 0.08477538079023361, acc: 0.9779005646705627)
[2025-02-13 20:03:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:36][root][INFO] - Training Epoch: 2/2, step 1411/7134 completed (loss: 0.03533458709716797, acc: 0.9942857027053833)
[2025-02-13 20:03:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:37][root][INFO] - Training Epoch: 2/2, step 1412/7134 completed (loss: 0.07316526770591736, acc: 0.9842105507850647)
[2025-02-13 20:03:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:37][root][INFO] - Training Epoch: 2/2, step 1413/7134 completed (loss: 0.06908128410577774, acc: 0.9820359349250793)
[2025-02-13 20:03:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:38][root][INFO] - Training Epoch: 2/2, step 1414/7134 completed (loss: 0.02981533855199814, acc: 1.0)
[2025-02-13 20:03:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:38][root][INFO] - Training Epoch: 2/2, step 1415/7134 completed (loss: 0.13197432458400726, acc: 0.9635036587715149)
[2025-02-13 20:03:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:38][root][INFO] - Training Epoch: 2/2, step 1416/7134 completed (loss: 0.050981681793928146, acc: 0.994350254535675)
[2025-02-13 20:03:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:39][root][INFO] - Training Epoch: 2/2, step 1417/7134 completed (loss: 0.025755293667316437, acc: 1.0)
[2025-02-13 20:03:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:39][root][INFO] - Training Epoch: 2/2, step 1418/7134 completed (loss: 0.01655024290084839, acc: 1.0)
[2025-02-13 20:03:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:40][root][INFO] - Training Epoch: 2/2, step 1419/7134 completed (loss: 0.11062738299369812, acc: 0.9772727489471436)
[2025-02-13 20:03:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:40][root][INFO] - Training Epoch: 2/2, step 1420/7134 completed (loss: 0.0530649833381176, acc: 0.9894737005233765)
[2025-02-13 20:03:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:40][root][INFO] - Training Epoch: 2/2, step 1421/7134 completed (loss: 0.027262579649686813, acc: 0.9932885766029358)
[2025-02-13 20:03:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:41][root][INFO] - Training Epoch: 2/2, step 1422/7134 completed (loss: 0.13528354465961456, acc: 0.9638554453849792)
[2025-02-13 20:03:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:41][root][INFO] - Training Epoch: 2/2, step 1423/7134 completed (loss: 0.28009775280952454, acc: 0.9236640930175781)
[2025-02-13 20:03:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:41][root][INFO] - Training Epoch: 2/2, step 1424/7134 completed (loss: 0.1654670089483261, acc: 0.9430894255638123)
[2025-02-13 20:03:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:42][root][INFO] - Training Epoch: 2/2, step 1425/7134 completed (loss: 0.15783430635929108, acc: 0.9537572264671326)
[2025-02-13 20:03:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:42][root][INFO] - Training Epoch: 2/2, step 1426/7134 completed (loss: 0.10827852785587311, acc: 0.9742268323898315)
[2025-02-13 20:03:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:43][root][INFO] - Training Epoch: 2/2, step 1427/7134 completed (loss: 0.17345088720321655, acc: 0.9265536665916443)
[2025-02-13 20:03:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:43][root][INFO] - Training Epoch: 2/2, step 1428/7134 completed (loss: 0.8609142303466797, acc: 0.7983871102333069)
[2025-02-13 20:03:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:43][root][INFO] - Training Epoch: 2/2, step 1429/7134 completed (loss: 0.1371137946844101, acc: 0.9814814925193787)
[2025-02-13 20:03:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:44][root][INFO] - Training Epoch: 2/2, step 1430/7134 completed (loss: 0.10047268122434616, acc: 0.9856114983558655)
[2025-02-13 20:03:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:44][root][INFO] - Training Epoch: 2/2, step 1431/7134 completed (loss: 0.07674197107553482, acc: 0.9784946441650391)
[2025-02-13 20:03:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:45][root][INFO] - Training Epoch: 2/2, step 1432/7134 completed (loss: 0.0655953511595726, acc: 0.979899525642395)
[2025-02-13 20:03:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:45][root][INFO] - Training Epoch: 2/2, step 1433/7134 completed (loss: 0.07523957639932632, acc: 0.9803921580314636)
[2025-02-13 20:03:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:45][root][INFO] - Training Epoch: 2/2, step 1434/7134 completed (loss: 0.0706419125199318, acc: 0.9850746393203735)
[2025-02-13 20:03:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:46][root][INFO] - Training Epoch: 2/2, step 1435/7134 completed (loss: 0.06378518790006638, acc: 0.9946236610412598)
[2025-02-13 20:03:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:46][root][INFO] - Training Epoch: 2/2, step 1436/7134 completed (loss: 0.082882821559906, acc: 0.9768518805503845)
[2025-02-13 20:03:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:47][root][INFO] - Training Epoch: 2/2, step 1437/7134 completed (loss: 0.08930546790361404, acc: 0.9731183052062988)
[2025-02-13 20:03:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:47][root][INFO] - Training Epoch: 2/2, step 1438/7134 completed (loss: 0.0603700689971447, acc: 0.9878787994384766)
[2025-02-13 20:03:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:47][root][INFO] - Training Epoch: 2/2, step 1439/7134 completed (loss: 0.057988978922367096, acc: 0.9851852059364319)
[2025-02-13 20:03:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:48][root][INFO] - Training Epoch: 2/2, step 1440/7134 completed (loss: 0.10010496526956558, acc: 0.9689440727233887)
[2025-02-13 20:03:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:48][root][INFO] - Training Epoch: 2/2, step 1441/7134 completed (loss: 0.04098128154873848, acc: 0.9942528605461121)
[2025-02-13 20:03:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:49][root][INFO] - Training Epoch: 2/2, step 1442/7134 completed (loss: 0.05907250568270683, acc: 0.9821428656578064)
[2025-02-13 20:03:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:49][root][INFO] - Training Epoch: 2/2, step 1443/7134 completed (loss: 0.22773624956607819, acc: 0.9506173133850098)
[2025-02-13 20:03:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:49][root][INFO] - Training Epoch: 2/2, step 1444/7134 completed (loss: 0.08416138589382172, acc: 0.9791666865348816)
[2025-02-13 20:03:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:50][root][INFO] - Training Epoch: 2/2, step 1445/7134 completed (loss: 0.13316549360752106, acc: 0.9745222926139832)
[2025-02-13 20:03:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:50][root][INFO] - Training Epoch: 2/2, step 1446/7134 completed (loss: 0.07788471132516861, acc: 0.9848484992980957)
[2025-02-13 20:03:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:51][root][INFO] - Training Epoch: 2/2, step 1447/7134 completed (loss: 0.05958321690559387, acc: 0.9892473220825195)
[2025-02-13 20:03:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:51][root][INFO] - Training Epoch: 2/2, step 1448/7134 completed (loss: 0.3209020495414734, acc: 0.9477611780166626)
[2025-02-13 20:03:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:51][root][INFO] - Training Epoch: 2/2, step 1449/7134 completed (loss: 0.25825729966163635, acc: 0.9289617538452148)
[2025-02-13 20:03:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:52][root][INFO] - Training Epoch: 2/2, step 1450/7134 completed (loss: 0.1628275364637375, acc: 0.9518716335296631)
[2025-02-13 20:03:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:52][root][INFO] - Training Epoch: 2/2, step 1451/7134 completed (loss: 0.10855323076248169, acc: 0.963350772857666)
[2025-02-13 20:03:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:53][root][INFO] - Training Epoch: 2/2, step 1452/7134 completed (loss: 0.1420145034790039, acc: 0.9722222089767456)
[2025-02-13 20:03:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:53][root][INFO] - Training Epoch: 2/2, step 1453/7134 completed (loss: 0.38400915265083313, acc: 0.9011628031730652)
[2025-02-13 20:03:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:53][root][INFO] - Training Epoch: 2/2, step 1454/7134 completed (loss: 0.14344586431980133, acc: 0.9427083134651184)
[2025-02-13 20:03:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:54][root][INFO] - Training Epoch: 2/2, step 1455/7134 completed (loss: 0.3129537105560303, acc: 0.931034505367279)
[2025-02-13 20:03:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:54][root][INFO] - Training Epoch: 2/2, step 1456/7134 completed (loss: 0.17095908522605896, acc: 0.9504132270812988)
[2025-02-13 20:03:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:54][root][INFO] - Training Epoch: 2/2, step 1457/7134 completed (loss: 0.054647836834192276, acc: 0.9875776171684265)
[2025-02-13 20:03:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:55][root][INFO] - Training Epoch: 2/2, step 1458/7134 completed (loss: 0.04569002240896225, acc: 0.9940828680992126)
[2025-02-13 20:03:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:55][root][INFO] - Training Epoch: 2/2, step 1459/7134 completed (loss: 0.035544320940971375, acc: 1.0)
[2025-02-13 20:03:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:56][root][INFO] - Training Epoch: 2/2, step 1460/7134 completed (loss: 0.18925414979457855, acc: 0.9644970297813416)
[2025-02-13 20:03:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:56][root][INFO] - Training Epoch: 2/2, step 1461/7134 completed (loss: 0.16932059824466705, acc: 0.9536423683166504)
[2025-02-13 20:03:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:56][root][INFO] - Training Epoch: 2/2, step 1462/7134 completed (loss: 0.08550912141799927, acc: 0.9701492786407471)
[2025-02-13 20:03:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:57][root][INFO] - Training Epoch: 2/2, step 1463/7134 completed (loss: 0.20710140466690063, acc: 0.9411764740943909)
[2025-02-13 20:03:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:57][root][INFO] - Training Epoch: 2/2, step 1464/7134 completed (loss: 0.19535009562969208, acc: 0.9638554453849792)
[2025-02-13 20:03:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:58][root][INFO] - Training Epoch: 2/2, step 1465/7134 completed (loss: 0.11893432587385178, acc: 0.9679487347602844)
[2025-02-13 20:03:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:58][root][INFO] - Training Epoch: 2/2, step 1466/7134 completed (loss: 0.018773779273033142, acc: 0.994413435459137)
[2025-02-13 20:03:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:58][root][INFO] - Training Epoch: 2/2, step 1467/7134 completed (loss: 0.05000218003988266, acc: 0.9939393997192383)
[2025-02-13 20:03:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:59][root][INFO] - Training Epoch: 2/2, step 1468/7134 completed (loss: 0.06971142441034317, acc: 0.983146071434021)
[2025-02-13 20:03:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:03:59][root][INFO] - Training Epoch: 2/2, step 1469/7134 completed (loss: 0.039736613631248474, acc: 0.9886363744735718)
[2025-02-13 20:03:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:00][root][INFO] - Training Epoch: 2/2, step 1470/7134 completed (loss: 0.0979209914803505, acc: 0.9808917045593262)
[2025-02-13 20:04:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:00][root][INFO] - Training Epoch: 2/2, step 1471/7134 completed (loss: 0.1699407696723938, acc: 0.9461538195610046)
[2025-02-13 20:04:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:00][root][INFO] - Training Epoch: 2/2, step 1472/7134 completed (loss: 0.05215585231781006, acc: 0.9824561476707458)
[2025-02-13 20:04:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:01][root][INFO] - Training Epoch: 2/2, step 1473/7134 completed (loss: 0.09729291498661041, acc: 0.9696969985961914)
[2025-02-13 20:04:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:01][root][INFO] - Training Epoch: 2/2, step 1474/7134 completed (loss: 0.10750257223844528, acc: 0.9793103337287903)
[2025-02-13 20:04:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:02][root][INFO] - Training Epoch: 2/2, step 1475/7134 completed (loss: 0.22876593470573425, acc: 0.9515151381492615)
[2025-02-13 20:04:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:02][root][INFO] - Training Epoch: 2/2, step 1476/7134 completed (loss: 0.12493326514959335, acc: 0.9733333587646484)
[2025-02-13 20:04:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:02][root][INFO] - Training Epoch: 2/2, step 1477/7134 completed (loss: 0.09145812690258026, acc: 0.9793103337287903)
[2025-02-13 20:04:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:03][root][INFO] - Training Epoch: 2/2, step 1478/7134 completed (loss: 0.16094492375850677, acc: 0.9802631735801697)
[2025-02-13 20:04:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:03][root][INFO] - Training Epoch: 2/2, step 1479/7134 completed (loss: 0.04937925562262535, acc: 0.982758641242981)
[2025-02-13 20:04:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:04][root][INFO] - Training Epoch: 2/2, step 1480/7134 completed (loss: 0.07908786833286285, acc: 0.9834254384040833)
[2025-02-13 20:04:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:04][root][INFO] - Training Epoch: 2/2, step 1481/7134 completed (loss: 0.11083948612213135, acc: 0.9833333492279053)
[2025-02-13 20:04:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:05][root][INFO] - Training Epoch: 2/2, step 1482/7134 completed (loss: 0.1517677754163742, acc: 0.9585798978805542)
[2025-02-13 20:04:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:05][root][INFO] - Training Epoch: 2/2, step 1483/7134 completed (loss: 0.024346686899662018, acc: 1.0)
[2025-02-13 20:04:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:05][root][INFO] - Training Epoch: 2/2, step 1484/7134 completed (loss: 0.23899799585342407, acc: 0.9638554453849792)
[2025-02-13 20:04:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:06][root][INFO] - Training Epoch: 2/2, step 1485/7134 completed (loss: 0.07416652888059616, acc: 0.9824561476707458)
[2025-02-13 20:04:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:06][root][INFO] - Training Epoch: 2/2, step 1486/7134 completed (loss: 0.0392807275056839, acc: 1.0)
[2025-02-13 20:04:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:07][root][INFO] - Training Epoch: 2/2, step 1487/7134 completed (loss: 0.0724877268075943, acc: 0.9837837815284729)
[2025-02-13 20:04:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:07][root][INFO] - Training Epoch: 2/2, step 1488/7134 completed (loss: 0.07203910499811172, acc: 0.9720930457115173)
[2025-02-13 20:04:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:08][root][INFO] - Training Epoch: 2/2, step 1489/7134 completed (loss: 0.12161813676357269, acc: 0.9744898080825806)
[2025-02-13 20:04:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:08][root][INFO] - Training Epoch: 2/2, step 1490/7134 completed (loss: 0.0982251763343811, acc: 0.982300877571106)
[2025-02-13 20:04:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:08][root][INFO] - Training Epoch: 2/2, step 1491/7134 completed (loss: 0.05260641500353813, acc: 0.9952380657196045)
[2025-02-13 20:04:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:09][root][INFO] - Training Epoch: 2/2, step 1492/7134 completed (loss: 0.10699480772018433, acc: 0.9763033390045166)
[2025-02-13 20:04:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:09][root][INFO] - Training Epoch: 2/2, step 1493/7134 completed (loss: 0.07976192235946655, acc: 0.9813084006309509)
[2025-02-13 20:04:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:10][root][INFO] - Training Epoch: 2/2, step 1494/7134 completed (loss: 0.062749944627285, acc: 0.984455943107605)
[2025-02-13 20:04:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:10][root][INFO] - Training Epoch: 2/2, step 1495/7134 completed (loss: 0.046202365309000015, acc: 0.995121955871582)
[2025-02-13 20:04:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:10][root][INFO] - Training Epoch: 2/2, step 1496/7134 completed (loss: 0.06649287790060043, acc: 0.981249988079071)
[2025-02-13 20:04:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:11][root][INFO] - Training Epoch: 2/2, step 1497/7134 completed (loss: 0.04883432388305664, acc: 0.9907407164573669)
[2025-02-13 20:04:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:11][root][INFO] - Training Epoch: 2/2, step 1498/7134 completed (loss: 0.06312020123004913, acc: 0.9837837815284729)
[2025-02-13 20:04:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:12][root][INFO] - Training Epoch: 2/2, step 1499/7134 completed (loss: 0.11860346049070358, acc: 0.9627906680107117)
[2025-02-13 20:04:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:12][root][INFO] - Training Epoch: 2/2, step 1500/7134 completed (loss: 0.14852577447891235, acc: 0.9589040875434875)
[2025-02-13 20:04:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:13][root][INFO] - Training Epoch: 2/2, step 1501/7134 completed (loss: 0.15868693590164185, acc: 0.976331353187561)
[2025-02-13 20:04:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:13][root][INFO] - Training Epoch: 2/2, step 1502/7134 completed (loss: 0.12529174983501434, acc: 0.9681817889213562)
[2025-02-13 20:04:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:14][root][INFO] - Training Epoch: 2/2, step 1503/7134 completed (loss: 0.15424621105194092, acc: 0.9714285731315613)
[2025-02-13 20:04:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:14][root][INFO] - Training Epoch: 2/2, step 1504/7134 completed (loss: 0.08765865862369537, acc: 0.9735449552536011)
[2025-02-13 20:04:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:14][root][INFO] - Training Epoch: 2/2, step 1505/7134 completed (loss: 0.07315164804458618, acc: 0.9810126423835754)
[2025-02-13 20:04:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:15][root][INFO] - Training Epoch: 2/2, step 1506/7134 completed (loss: 0.13783393800258636, acc: 0.9685534834861755)
[2025-02-13 20:04:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:15][root][INFO] - Training Epoch: 2/2, step 1507/7134 completed (loss: 0.11371292918920517, acc: 0.96875)
[2025-02-13 20:04:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:16][root][INFO] - Training Epoch: 2/2, step 1508/7134 completed (loss: 0.06989598274230957, acc: 0.9813084006309509)
[2025-02-13 20:04:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:16][root][INFO] - Training Epoch: 2/2, step 1509/7134 completed (loss: 0.08368300646543503, acc: 0.9777777791023254)
[2025-02-13 20:04:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:16][root][INFO] - Training Epoch: 2/2, step 1510/7134 completed (loss: 0.16572369635105133, acc: 0.9677419066429138)
[2025-02-13 20:04:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:17][root][INFO] - Training Epoch: 2/2, step 1511/7134 completed (loss: 0.14519095420837402, acc: 0.9553072452545166)
[2025-02-13 20:04:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:17][root][INFO] - Training Epoch: 2/2, step 1512/7134 completed (loss: 0.021784231066703796, acc: 1.0)
[2025-02-13 20:04:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:18][root][INFO] - Training Epoch: 2/2, step 1513/7134 completed (loss: 0.06696661561727524, acc: 0.9723756909370422)
[2025-02-13 20:04:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:18][root][INFO] - Training Epoch: 2/2, step 1514/7134 completed (loss: 0.08139189332723618, acc: 0.9740932583808899)
[2025-02-13 20:04:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:18][root][INFO] - Training Epoch: 2/2, step 1515/7134 completed (loss: 0.11479058861732483, acc: 0.9729729890823364)
[2025-02-13 20:04:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:19][root][INFO] - Training Epoch: 2/2, step 1516/7134 completed (loss: 0.18335436284542084, acc: 0.949999988079071)
[2025-02-13 20:04:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:19][root][INFO] - Training Epoch: 2/2, step 1517/7134 completed (loss: 0.1982247680425644, acc: 0.9553571343421936)
[2025-02-13 20:04:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:20][root][INFO] - Training Epoch: 2/2, step 1518/7134 completed (loss: 0.08095389604568481, acc: 0.969072163105011)
[2025-02-13 20:04:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:20][root][INFO] - Training Epoch: 2/2, step 1519/7134 completed (loss: 0.11701711267232895, acc: 0.9708737730979919)
[2025-02-13 20:04:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:20][root][INFO] - Training Epoch: 2/2, step 1520/7134 completed (loss: 0.04386276379227638, acc: 0.9927007555961609)
[2025-02-13 20:04:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:21][root][INFO] - Training Epoch: 2/2, step 1521/7134 completed (loss: 0.5623116493225098, acc: 0.875)
[2025-02-13 20:04:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:21][root][INFO] - Training Epoch: 2/2, step 1522/7134 completed (loss: 0.23408348858356476, acc: 0.9365079402923584)
[2025-02-13 20:04:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:22][root][INFO] - Training Epoch: 2/2, step 1523/7134 completed (loss: 0.3925052881240845, acc: 0.875)
[2025-02-13 20:04:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:22][root][INFO] - Training Epoch: 2/2, step 1524/7134 completed (loss: 0.11859942227602005, acc: 0.9726027250289917)
[2025-02-13 20:04:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:22][root][INFO] - Training Epoch: 2/2, step 1525/7134 completed (loss: 0.21470533311367035, acc: 0.949999988079071)
[2025-02-13 20:04:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:23][root][INFO] - Training Epoch: 2/2, step 1526/7134 completed (loss: 0.262492835521698, acc: 0.9047619104385376)
[2025-02-13 20:04:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:23][root][INFO] - Training Epoch: 2/2, step 1527/7134 completed (loss: 0.4594460129737854, acc: 0.9026548862457275)
[2025-02-13 20:04:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:24][root][INFO] - Training Epoch: 2/2, step 1528/7134 completed (loss: 0.16088129580020905, acc: 0.9710144996643066)
[2025-02-13 20:04:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:24][root][INFO] - Training Epoch: 2/2, step 1529/7134 completed (loss: 0.10041853040456772, acc: 0.978723406791687)
[2025-02-13 20:04:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:25][root][INFO] - Training Epoch: 2/2, step 1530/7134 completed (loss: 0.20391322672367096, acc: 0.9464285969734192)
[2025-02-13 20:04:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:25][root][INFO] - Training Epoch: 2/2, step 1531/7134 completed (loss: 0.5802621841430664, acc: 0.8712121248245239)
[2025-02-13 20:04:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:25][root][INFO] - Training Epoch: 2/2, step 1532/7134 completed (loss: 0.08712355047464371, acc: 0.9794871807098389)
[2025-02-13 20:04:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:26][root][INFO] - Training Epoch: 2/2, step 1533/7134 completed (loss: 0.1997927725315094, acc: 0.9518072009086609)
[2025-02-13 20:04:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:26][root][INFO] - Training Epoch: 2/2, step 1534/7134 completed (loss: 0.1322699338197708, acc: 0.9784172773361206)
[2025-02-13 20:04:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:27][root][INFO] - Training Epoch: 2/2, step 1535/7134 completed (loss: 0.1378834843635559, acc: 0.9485714435577393)
[2025-02-13 20:04:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:27][root][INFO] - Training Epoch: 2/2, step 1536/7134 completed (loss: 0.06695564836263657, acc: 0.9876543283462524)
[2025-02-13 20:04:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:27][root][INFO] - Training Epoch: 2/2, step 1537/7134 completed (loss: 0.20661194622516632, acc: 0.9415204524993896)
[2025-02-13 20:04:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:28][root][INFO] - Training Epoch: 2/2, step 1538/7134 completed (loss: 0.3317951560020447, acc: 0.9464285969734192)
[2025-02-13 20:04:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:28][root][INFO] - Training Epoch: 2/2, step 1539/7134 completed (loss: 0.22351236641407013, acc: 0.9575757384300232)
[2025-02-13 20:04:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:28][root][INFO] - Training Epoch: 2/2, step 1540/7134 completed (loss: 0.1403050720691681, acc: 0.9620253443717957)
[2025-02-13 20:04:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:29][root][INFO] - Training Epoch: 2/2, step 1541/7134 completed (loss: 0.1464826911687851, acc: 0.9631578922271729)
[2025-02-13 20:04:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:29][root][INFO] - Training Epoch: 2/2, step 1542/7134 completed (loss: 0.10141084343194962, acc: 0.966292142868042)
[2025-02-13 20:04:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:30][root][INFO] - Training Epoch: 2/2, step 1543/7134 completed (loss: 0.06579552590847015, acc: 0.9904761910438538)
[2025-02-13 20:04:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:30][root][INFO] - Training Epoch: 2/2, step 1544/7134 completed (loss: 0.090629443526268, acc: 0.9800000190734863)
[2025-02-13 20:04:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:30][root][INFO] - Training Epoch: 2/2, step 1545/7134 completed (loss: 0.11518938094377518, acc: 0.9642857313156128)
[2025-02-13 20:04:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:31][root][INFO] - Training Epoch: 2/2, step 1546/7134 completed (loss: 0.40313589572906494, acc: 0.9176470637321472)
[2025-02-13 20:04:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:31][root][INFO] - Training Epoch: 2/2, step 1547/7134 completed (loss: 0.2170562744140625, acc: 0.9444444179534912)
[2025-02-13 20:04:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:32][root][INFO] - Training Epoch: 2/2, step 1548/7134 completed (loss: 0.09758437424898148, acc: 0.9842105507850647)
[2025-02-13 20:04:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:32][root][INFO] - Training Epoch: 2/2, step 1549/7134 completed (loss: 0.038341596722602844, acc: 0.9942857027053833)
[2025-02-13 20:04:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:32][root][INFO] - Training Epoch: 2/2, step 1550/7134 completed (loss: 0.0460476353764534, acc: 0.9882352948188782)
[2025-02-13 20:04:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:33][root][INFO] - Training Epoch: 2/2, step 1551/7134 completed (loss: 0.04870320484042168, acc: 0.9933775067329407)
[2025-02-13 20:04:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:33][root][INFO] - Training Epoch: 2/2, step 1552/7134 completed (loss: 0.05278736725449562, acc: 0.9846938848495483)
[2025-02-13 20:04:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:34][root][INFO] - Training Epoch: 2/2, step 1553/7134 completed (loss: 0.07172923535108566, acc: 0.9805194735527039)
[2025-02-13 20:04:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:34][root][INFO] - Training Epoch: 2/2, step 1554/7134 completed (loss: 0.09189769625663757, acc: 0.9710982441902161)
[2025-02-13 20:04:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:34][root][INFO] - Training Epoch: 2/2, step 1555/7134 completed (loss: 0.04200511798262596, acc: 0.987500011920929)
[2025-02-13 20:04:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:35][root][INFO] - Training Epoch: 2/2, step 1556/7134 completed (loss: 0.11183347553014755, acc: 0.9717513918876648)
[2025-02-13 20:04:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:35][root][INFO] - Training Epoch: 2/2, step 1557/7134 completed (loss: 0.08349427580833435, acc: 0.9722222089767456)
[2025-02-13 20:04:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:36][root][INFO] - Training Epoch: 2/2, step 1558/7134 completed (loss: 0.1646198183298111, acc: 0.9589040875434875)
[2025-02-13 20:04:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:36][root][INFO] - Training Epoch: 2/2, step 1559/7134 completed (loss: 0.054011836647987366, acc: 1.0)
[2025-02-13 20:04:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:36][root][INFO] - Training Epoch: 2/2, step 1560/7134 completed (loss: 0.07955805957317352, acc: 0.9746192693710327)
[2025-02-13 20:04:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:37][root][INFO] - Training Epoch: 2/2, step 1561/7134 completed (loss: 0.24111276865005493, acc: 0.9568965435028076)
[2025-02-13 20:04:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:37][root][INFO] - Training Epoch: 2/2, step 1562/7134 completed (loss: 0.1051567792892456, acc: 0.9704142212867737)
[2025-02-13 20:04:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:37][root][INFO] - Training Epoch: 2/2, step 1563/7134 completed (loss: 0.14367379248142242, acc: 0.9509803652763367)
[2025-02-13 20:04:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:38][root][INFO] - Training Epoch: 2/2, step 1564/7134 completed (loss: 0.21383103728294373, acc: 0.9306930899620056)
[2025-02-13 20:04:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:38][root][INFO] - Training Epoch: 2/2, step 1565/7134 completed (loss: 0.11319989711046219, acc: 0.9685534834861755)
[2025-02-13 20:04:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:39][root][INFO] - Training Epoch: 2/2, step 1566/7134 completed (loss: 0.13677293062210083, acc: 0.9723756909370422)
[2025-02-13 20:04:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:39][root][INFO] - Training Epoch: 2/2, step 1567/7134 completed (loss: 0.29201459884643555, acc: 0.9317073225975037)
[2025-02-13 20:04:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:39][root][INFO] - Training Epoch: 2/2, step 1568/7134 completed (loss: 0.2500476837158203, acc: 0.9375)
[2025-02-13 20:04:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:40][root][INFO] - Training Epoch: 2/2, step 1569/7134 completed (loss: 0.13798773288726807, acc: 0.9454545378684998)
[2025-02-13 20:04:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:40][root][INFO] - Training Epoch: 2/2, step 1570/7134 completed (loss: 0.4318309426307678, acc: 0.9154929518699646)
[2025-02-13 20:04:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:41][root][INFO] - Training Epoch: 2/2, step 1571/7134 completed (loss: 0.20038053393363953, acc: 0.9631336331367493)
[2025-02-13 20:04:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:41][root][INFO] - Training Epoch: 2/2, step 1572/7134 completed (loss: 0.3044338822364807, acc: 0.9530516266822815)
[2025-02-13 20:04:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:41][root][INFO] - Training Epoch: 2/2, step 1573/7134 completed (loss: 0.0939640924334526, acc: 0.970059871673584)
[2025-02-13 20:04:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:42][root][INFO] - Training Epoch: 2/2, step 1574/7134 completed (loss: 0.204537495970726, acc: 0.9461883306503296)
[2025-02-13 20:04:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:42][root][INFO] - Training Epoch: 2/2, step 1575/7134 completed (loss: 0.13793031871318817, acc: 0.9681817889213562)
[2025-02-13 20:04:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:43][root][INFO] - Training Epoch: 2/2, step 1576/7134 completed (loss: 0.13086169958114624, acc: 0.9718309640884399)
[2025-02-13 20:04:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:43][root][INFO] - Training Epoch: 2/2, step 1577/7134 completed (loss: 0.12092495709657669, acc: 0.966183602809906)
[2025-02-13 20:04:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:43][root][INFO] - Training Epoch: 2/2, step 1578/7134 completed (loss: 0.16966718435287476, acc: 0.9545454382896423)
[2025-02-13 20:04:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:44][root][INFO] - Training Epoch: 2/2, step 1579/7134 completed (loss: 0.08091955631971359, acc: 0.9759615659713745)
[2025-02-13 20:04:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:44][root][INFO] - Training Epoch: 2/2, step 1580/7134 completed (loss: 0.24350661039352417, acc: 0.9275362491607666)
[2025-02-13 20:04:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:45][root][INFO] - Training Epoch: 2/2, step 1581/7134 completed (loss: 0.2692979872226715, acc: 0.9285714030265808)
[2025-02-13 20:04:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:45][root][INFO] - Training Epoch: 2/2, step 1582/7134 completed (loss: 0.19242869317531586, acc: 0.95333331823349)
[2025-02-13 20:04:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:45][root][INFO] - Training Epoch: 2/2, step 1583/7134 completed (loss: 0.12946228682994843, acc: 0.9828571677207947)
[2025-02-13 20:04:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:46][root][INFO] - Training Epoch: 2/2, step 1584/7134 completed (loss: 0.1500043272972107, acc: 0.954954981803894)
[2025-02-13 20:04:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:46][root][INFO] - Training Epoch: 2/2, step 1585/7134 completed (loss: 0.09733859449625015, acc: 0.9757281541824341)
[2025-02-13 20:04:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:47][root][INFO] - Training Epoch: 2/2, step 1586/7134 completed (loss: 0.16692061722278595, acc: 0.9516907930374146)
[2025-02-13 20:04:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:47][root][INFO] - Training Epoch: 2/2, step 1587/7134 completed (loss: 0.13523653149604797, acc: 0.9617834687232971)
[2025-02-13 20:04:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:47][root][INFO] - Training Epoch: 2/2, step 1588/7134 completed (loss: 0.07349775731563568, acc: 0.987261176109314)
[2025-02-13 20:04:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:48][root][INFO] - Training Epoch: 2/2, step 1589/7134 completed (loss: 0.08108377456665039, acc: 0.9939024448394775)
[2025-02-13 20:04:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:48][root][INFO] - Training Epoch: 2/2, step 1590/7134 completed (loss: 0.08455689996480942, acc: 0.9714285731315613)
[2025-02-13 20:04:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:49][root][INFO] - Training Epoch: 2/2, step 1591/7134 completed (loss: 0.07799290865659714, acc: 0.9767441749572754)
[2025-02-13 20:04:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:49][root][INFO] - Training Epoch: 2/2, step 1592/7134 completed (loss: 0.07517192512750626, acc: 0.9805194735527039)
[2025-02-13 20:04:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:50][root][INFO] - Training Epoch: 2/2, step 1593/7134 completed (loss: 0.07229016721248627, acc: 0.9893048405647278)
[2025-02-13 20:04:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:50][root][INFO] - Training Epoch: 2/2, step 1594/7134 completed (loss: 0.04497362673282623, acc: 0.9921875)
[2025-02-13 20:04:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:50][root][INFO] - Training Epoch: 2/2, step 1595/7134 completed (loss: 0.051065851002931595, acc: 0.9915966391563416)
[2025-02-13 20:04:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:51][root][INFO] - Training Epoch: 2/2, step 1596/7134 completed (loss: 0.22660928964614868, acc: 0.9652777910232544)
[2025-02-13 20:04:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:51][root][INFO] - Training Epoch: 2/2, step 1597/7134 completed (loss: 0.033889010548591614, acc: 0.9933775067329407)
[2025-02-13 20:04:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:52][root][INFO] - Training Epoch: 2/2, step 1598/7134 completed (loss: 0.15003718435764313, acc: 0.9666666388511658)
[2025-02-13 20:04:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:52][root][INFO] - Training Epoch: 2/2, step 1599/7134 completed (loss: 0.1967279613018036, acc: 0.9379310607910156)
[2025-02-13 20:04:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:52][root][INFO] - Training Epoch: 2/2, step 1600/7134 completed (loss: 0.18896183371543884, acc: 0.9642857313156128)
[2025-02-13 20:04:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:53][root][INFO] - Training Epoch: 2/2, step 1601/7134 completed (loss: 0.07548575848340988, acc: 0.9818181991577148)
[2025-02-13 20:04:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:53][root][INFO] - Training Epoch: 2/2, step 1602/7134 completed (loss: 0.12347768992185593, acc: 0.9743589758872986)
[2025-02-13 20:04:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:54][root][INFO] - Training Epoch: 2/2, step 1603/7134 completed (loss: 0.08562152832746506, acc: 0.9759036302566528)
[2025-02-13 20:04:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:54][root][INFO] - Training Epoch: 2/2, step 1604/7134 completed (loss: 0.07669702917337418, acc: 0.9818181991577148)
[2025-02-13 20:04:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:54][root][INFO] - Training Epoch: 2/2, step 1605/7134 completed (loss: 0.09761368483304977, acc: 0.9764705896377563)
[2025-02-13 20:04:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:55][root][INFO] - Training Epoch: 2/2, step 1606/7134 completed (loss: 0.06378118693828583, acc: 0.9931972622871399)
[2025-02-13 20:04:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:55][root][INFO] - Training Epoch: 2/2, step 1607/7134 completed (loss: 0.020710278302431107, acc: 1.0)
[2025-02-13 20:04:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:56][root][INFO] - Training Epoch: 2/2, step 1608/7134 completed (loss: 0.030332323163747787, acc: 0.9921259880065918)
[2025-02-13 20:04:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:56][root][INFO] - Training Epoch: 2/2, step 1609/7134 completed (loss: 0.17791566252708435, acc: 0.9513888955116272)
[2025-02-13 20:04:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:56][root][INFO] - Training Epoch: 2/2, step 1610/7134 completed (loss: 0.09101727604866028, acc: 0.9770992398262024)
[2025-02-13 20:04:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:57][root][INFO] - Training Epoch: 2/2, step 1611/7134 completed (loss: 0.13185296952724457, acc: 0.9691358208656311)
[2025-02-13 20:04:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:57][root][INFO] - Training Epoch: 2/2, step 1612/7134 completed (loss: 0.15160158276557922, acc: 0.9578313231468201)
[2025-02-13 20:04:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:58][root][INFO] - Training Epoch: 2/2, step 1613/7134 completed (loss: 0.12267528474330902, acc: 0.9627329111099243)
[2025-02-13 20:04:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:58][root][INFO] - Training Epoch: 2/2, step 1614/7134 completed (loss: 0.12537463009357452, acc: 0.9605262875556946)
[2025-02-13 20:04:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:58][root][INFO] - Training Epoch: 2/2, step 1615/7134 completed (loss: 0.09880981594324112, acc: 0.9638554453849792)
[2025-02-13 20:04:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:59][root][INFO] - Training Epoch: 2/2, step 1616/7134 completed (loss: 0.09345915913581848, acc: 0.9870967864990234)
[2025-02-13 20:04:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:04:59][root][INFO] - Training Epoch: 2/2, step 1617/7134 completed (loss: 0.11605613678693771, acc: 0.9856114983558655)
[2025-02-13 20:04:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:00][root][INFO] - Training Epoch: 2/2, step 1618/7134 completed (loss: 0.26029765605926514, acc: 0.9465649127960205)
[2025-02-13 20:05:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:00][root][INFO] - Training Epoch: 2/2, step 1619/7134 completed (loss: 0.1631961613893509, acc: 0.957317054271698)
[2025-02-13 20:05:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:00][root][INFO] - Training Epoch: 2/2, step 1620/7134 completed (loss: 0.16776950657367706, acc: 0.948051929473877)
[2025-02-13 20:05:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:01][root][INFO] - Training Epoch: 2/2, step 1621/7134 completed (loss: 0.21930105984210968, acc: 0.9477124214172363)
[2025-02-13 20:05:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:01][root][INFO] - Training Epoch: 2/2, step 1622/7134 completed (loss: 0.12445412576198578, acc: 0.9542483687400818)
[2025-02-13 20:05:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:02][root][INFO] - Training Epoch: 2/2, step 1623/7134 completed (loss: 0.08691858500242233, acc: 0.9743589758872986)
[2025-02-13 20:05:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:02][root][INFO] - Training Epoch: 2/2, step 1624/7134 completed (loss: 0.15309752523899078, acc: 0.9545454382896423)
[2025-02-13 20:05:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:03][root][INFO] - Training Epoch: 2/2, step 1625/7134 completed (loss: 0.08383379131555557, acc: 0.9750000238418579)
[2025-02-13 20:05:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:03][root][INFO] - Training Epoch: 2/2, step 1626/7134 completed (loss: 0.14179734885692596, acc: 0.9587628841400146)
[2025-02-13 20:05:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:04][root][INFO] - Training Epoch: 2/2, step 1627/7134 completed (loss: 0.1431887149810791, acc: 0.9512194991111755)
[2025-02-13 20:05:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:04][root][INFO] - Training Epoch: 2/2, step 1628/7134 completed (loss: 0.030876589938998222, acc: 0.9814814925193787)
[2025-02-13 20:05:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:04][root][INFO] - Training Epoch: 2/2, step 1629/7134 completed (loss: 0.1508433222770691, acc: 0.9848484992980957)
[2025-02-13 20:05:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:05][root][INFO] - Training Epoch: 2/2, step 1630/7134 completed (loss: 0.2516157329082489, acc: 0.957446813583374)
[2025-02-13 20:05:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:05][root][INFO] - Training Epoch: 2/2, step 1631/7134 completed (loss: 0.18061524629592896, acc: 0.935251772403717)
[2025-02-13 20:05:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:06][root][INFO] - Training Epoch: 2/2, step 1632/7134 completed (loss: 0.06431913375854492, acc: 0.9850746393203735)
[2025-02-13 20:05:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:06][root][INFO] - Training Epoch: 2/2, step 1633/7134 completed (loss: 0.07915793359279633, acc: 0.9857142567634583)
[2025-02-13 20:05:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:06][root][INFO] - Training Epoch: 2/2, step 1634/7134 completed (loss: 0.13960178196430206, acc: 0.9622641801834106)
[2025-02-13 20:05:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:07][root][INFO] - Training Epoch: 2/2, step 1635/7134 completed (loss: 0.10262563824653625, acc: 0.9629629850387573)
[2025-02-13 20:05:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:07][root][INFO] - Training Epoch: 2/2, step 1636/7134 completed (loss: 0.22789938747882843, acc: 0.9357798099517822)
[2025-02-13 20:05:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:08][root][INFO] - Training Epoch: 2/2, step 1637/7134 completed (loss: 0.05558943375945091, acc: 0.9875776171684265)
[2025-02-13 20:05:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:08][root][INFO] - Training Epoch: 2/2, step 1638/7134 completed (loss: 0.2625845968723297, acc: 0.9291338324546814)
[2025-02-13 20:05:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:08][root][INFO] - Training Epoch: 2/2, step 1639/7134 completed (loss: 0.4103054404258728, acc: 0.8913043737411499)
[2025-02-13 20:05:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:09][root][INFO] - Training Epoch: 2/2, step 1640/7134 completed (loss: 0.18209783732891083, acc: 0.9420289993286133)
[2025-02-13 20:05:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:09][root][INFO] - Training Epoch: 2/2, step 1641/7134 completed (loss: 0.09893844276666641, acc: 0.9736841917037964)
[2025-02-13 20:05:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:10][root][INFO] - Training Epoch: 2/2, step 1642/7134 completed (loss: 0.11293471604585648, acc: 0.9748427867889404)
[2025-02-13 20:05:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:10][root][INFO] - Training Epoch: 2/2, step 1643/7134 completed (loss: 0.34156548976898193, acc: 0.918181836605072)
[2025-02-13 20:05:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:10][root][INFO] - Training Epoch: 2/2, step 1644/7134 completed (loss: 0.14193575084209442, acc: 0.9622641801834106)
[2025-02-13 20:05:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:11][root][INFO] - Training Epoch: 2/2, step 1645/7134 completed (loss: 0.21506629884243011, acc: 0.9655172228813171)
[2025-02-13 20:05:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:11][root][INFO] - Training Epoch: 2/2, step 1646/7134 completed (loss: 0.1541513204574585, acc: 0.9548386931419373)
[2025-02-13 20:05:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:12][root][INFO] - Training Epoch: 2/2, step 1647/7134 completed (loss: 0.12532243132591248, acc: 0.9671361446380615)
[2025-02-13 20:05:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:12][root][INFO] - Training Epoch: 2/2, step 1648/7134 completed (loss: 0.17206451296806335, acc: 0.96517413854599)
[2025-02-13 20:05:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:12][root][INFO] - Training Epoch: 2/2, step 1649/7134 completed (loss: 0.05601786449551582, acc: 0.991150438785553)
[2025-02-13 20:05:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:13][root][INFO] - Training Epoch: 2/2, step 1650/7134 completed (loss: 0.05512983724474907, acc: 0.9949748516082764)
[2025-02-13 20:05:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:13][root][INFO] - Training Epoch: 2/2, step 1651/7134 completed (loss: 0.14272548258304596, acc: 0.9818181991577148)
[2025-02-13 20:05:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:14][root][INFO] - Training Epoch: 2/2, step 1652/7134 completed (loss: 0.2086630016565323, acc: 0.9444444179534912)
[2025-02-13 20:05:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:14][root][INFO] - Training Epoch: 2/2, step 1653/7134 completed (loss: 0.12019368261098862, acc: 0.9617486596107483)
[2025-02-13 20:05:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:15][root][INFO] - Training Epoch: 2/2, step 1654/7134 completed (loss: 0.11293592303991318, acc: 0.9742489457130432)
[2025-02-13 20:05:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:15][root][INFO] - Training Epoch: 2/2, step 1655/7134 completed (loss: 0.06868388503789902, acc: 0.9897959232330322)
[2025-02-13 20:05:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:15][root][INFO] - Training Epoch: 2/2, step 1656/7134 completed (loss: 0.1441676765680313, acc: 0.9686098694801331)
[2025-02-13 20:05:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:16][root][INFO] - Training Epoch: 2/2, step 1657/7134 completed (loss: 0.1007622703909874, acc: 0.961240291595459)
[2025-02-13 20:05:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:16][root][INFO] - Training Epoch: 2/2, step 1658/7134 completed (loss: 0.09425405412912369, acc: 0.9751037359237671)
[2025-02-13 20:05:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:17][root][INFO] - Training Epoch: 2/2, step 1659/7134 completed (loss: 0.1016976609826088, acc: 0.9679999947547913)
[2025-02-13 20:05:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:17][root][INFO] - Training Epoch: 2/2, step 1660/7134 completed (loss: 0.11430113017559052, acc: 0.9759615659713745)
[2025-02-13 20:05:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:18][root][INFO] - Training Epoch: 2/2, step 1661/7134 completed (loss: 0.08678914606571198, acc: 0.9870129823684692)
[2025-02-13 20:05:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:18][root][INFO] - Training Epoch: 2/2, step 1662/7134 completed (loss: 0.0667794942855835, acc: 0.9857819676399231)
[2025-02-13 20:05:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:18][root][INFO] - Training Epoch: 2/2, step 1663/7134 completed (loss: 0.07386131584644318, acc: 0.9829787015914917)
[2025-02-13 20:05:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:19][root][INFO] - Training Epoch: 2/2, step 1664/7134 completed (loss: 0.07165567576885223, acc: 0.9800000190734863)
[2025-02-13 20:05:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:19][root][INFO] - Training Epoch: 2/2, step 1665/7134 completed (loss: 0.08358809351921082, acc: 0.9754601120948792)
[2025-02-13 20:05:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:20][root][INFO] - Training Epoch: 2/2, step 1666/7134 completed (loss: 0.03768322989344597, acc: 0.9957982897758484)
[2025-02-13 20:05:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:20][root][INFO] - Training Epoch: 2/2, step 1667/7134 completed (loss: 0.083397276699543, acc: 0.9747474789619446)
[2025-02-13 20:05:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:21][root][INFO] - Training Epoch: 2/2, step 1668/7134 completed (loss: 0.052162691950798035, acc: 0.9865471124649048)
[2025-02-13 20:05:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:21][root][INFO] - Training Epoch: 2/2, step 1669/7134 completed (loss: 0.10708210617303848, acc: 0.9809160232543945)
[2025-02-13 20:05:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:21][root][INFO] - Training Epoch: 2/2, step 1670/7134 completed (loss: 0.17449140548706055, acc: 0.9657794833183289)
[2025-02-13 20:05:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:22][root][INFO] - Training Epoch: 2/2, step 1671/7134 completed (loss: 0.11469906568527222, acc: 0.9744898080825806)
[2025-02-13 20:05:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:22][root][INFO] - Training Epoch: 2/2, step 1672/7134 completed (loss: 0.16876360774040222, acc: 0.9766082167625427)
[2025-02-13 20:05:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:23][root][INFO] - Training Epoch: 2/2, step 1673/7134 completed (loss: 0.1712893694639206, acc: 0.9627329111099243)
[2025-02-13 20:05:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:23][root][INFO] - Training Epoch: 2/2, step 1674/7134 completed (loss: 0.12091246247291565, acc: 0.9671052694320679)
[2025-02-13 20:05:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:24][root][INFO] - Training Epoch: 2/2, step 1675/7134 completed (loss: 0.18477153778076172, acc: 0.9589040875434875)
[2025-02-13 20:05:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:24][root][INFO] - Training Epoch: 2/2, step 1676/7134 completed (loss: 0.12817256152629852, acc: 0.9743589758872986)
[2025-02-13 20:05:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:24][root][INFO] - Training Epoch: 2/2, step 1677/7134 completed (loss: 0.2708348333835602, acc: 0.9432623982429504)
[2025-02-13 20:05:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:25][root][INFO] - Training Epoch: 2/2, step 1678/7134 completed (loss: 0.1928519308567047, acc: 0.9640718698501587)
[2025-02-13 20:05:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:25][root][INFO] - Training Epoch: 2/2, step 1679/7134 completed (loss: 0.10360708832740784, acc: 0.9817073345184326)
[2025-02-13 20:05:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:26][root][INFO] - Training Epoch: 2/2, step 1680/7134 completed (loss: 0.10064750164747238, acc: 0.9823529124259949)
[2025-02-13 20:05:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:26][root][INFO] - Training Epoch: 2/2, step 1681/7134 completed (loss: 0.11689221113920212, acc: 0.9811320900917053)
[2025-02-13 20:05:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:26][root][INFO] - Training Epoch: 2/2, step 1682/7134 completed (loss: 0.1102219671010971, acc: 0.9726027250289917)
[2025-02-13 20:05:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:27][root][INFO] - Training Epoch: 2/2, step 1683/7134 completed (loss: 0.1179950162768364, acc: 0.9793103337287903)
[2025-02-13 20:05:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:27][root][INFO] - Training Epoch: 2/2, step 1684/7134 completed (loss: 0.06568891555070877, acc: 0.9858155846595764)
[2025-02-13 20:05:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:28][root][INFO] - Training Epoch: 2/2, step 1685/7134 completed (loss: 0.10503623634576797, acc: 0.9878787994384766)
[2025-02-13 20:05:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:28][root][INFO] - Training Epoch: 2/2, step 1686/7134 completed (loss: 0.048217423260211945, acc: 0.9924812316894531)
[2025-02-13 20:05:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:28][root][INFO] - Training Epoch: 2/2, step 1687/7134 completed (loss: 0.08239664137363434, acc: 0.982300877571106)
[2025-02-13 20:05:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:29][root][INFO] - Training Epoch: 2/2, step 1688/7134 completed (loss: 0.05827454477548599, acc: 0.9794520735740662)
[2025-02-13 20:05:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:29][root][INFO] - Training Epoch: 2/2, step 1689/7134 completed (loss: 0.09289298206567764, acc: 0.9806451797485352)
[2025-02-13 20:05:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:30][root][INFO] - Training Epoch: 2/2, step 1690/7134 completed (loss: 0.05021242797374725, acc: 1.0)
[2025-02-13 20:05:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:30][root][INFO] - Training Epoch: 2/2, step 1691/7134 completed (loss: 0.057934705168008804, acc: 0.9858155846595764)
[2025-02-13 20:05:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:30][root][INFO] - Training Epoch: 2/2, step 1692/7134 completed (loss: 0.07449781894683838, acc: 0.9837398529052734)
[2025-02-13 20:05:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:31][root][INFO] - Training Epoch: 2/2, step 1693/7134 completed (loss: 0.07567606121301651, acc: 0.9795918464660645)
[2025-02-13 20:05:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:31][root][INFO] - Training Epoch: 2/2, step 1694/7134 completed (loss: 0.10553862154483795, acc: 0.9793103337287903)
[2025-02-13 20:05:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:32][root][INFO] - Training Epoch: 2/2, step 1695/7134 completed (loss: 0.10001515597105026, acc: 0.9747899174690247)
[2025-02-13 20:05:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:32][root][INFO] - Training Epoch: 2/2, step 1696/7134 completed (loss: 0.16922174394130707, acc: 0.9624060392379761)
[2025-02-13 20:05:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:32][root][INFO] - Training Epoch: 2/2, step 1697/7134 completed (loss: 0.06954453140497208, acc: 0.9860140085220337)
[2025-02-13 20:05:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:33][root][INFO] - Training Epoch: 2/2, step 1698/7134 completed (loss: 0.05064476281404495, acc: 0.9790209531784058)
[2025-02-13 20:05:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:33][root][INFO] - Training Epoch: 2/2, step 1699/7134 completed (loss: 0.05643971264362335, acc: 0.9850746393203735)
[2025-02-13 20:05:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:33][root][INFO] - Training Epoch: 2/2, step 1700/7134 completed (loss: 0.09502808004617691, acc: 0.9626865386962891)
[2025-02-13 20:05:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:34][root][INFO] - Training Epoch: 2/2, step 1701/7134 completed (loss: 0.14003878831863403, acc: 0.9666666388511658)
[2025-02-13 20:05:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:34][root][INFO] - Training Epoch: 2/2, step 1702/7134 completed (loss: 0.10278905928134918, acc: 0.976047933101654)
[2025-02-13 20:05:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:35][root][INFO] - Training Epoch: 2/2, step 1703/7134 completed (loss: 0.3078668713569641, acc: 0.9271523356437683)
[2025-02-13 20:05:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:35][root][INFO] - Training Epoch: 2/2, step 1704/7134 completed (loss: 0.28059884905815125, acc: 0.9329268336296082)
[2025-02-13 20:05:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:36][root][INFO] - Training Epoch: 2/2, step 1705/7134 completed (loss: 0.21238313615322113, acc: 0.9679999947547913)
[2025-02-13 20:05:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:36][root][INFO] - Training Epoch: 2/2, step 1706/7134 completed (loss: 0.04061207175254822, acc: 1.0)
[2025-02-13 20:05:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:36][root][INFO] - Training Epoch: 2/2, step 1707/7134 completed (loss: 0.27247482538223267, acc: 0.9482758641242981)
[2025-02-13 20:05:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:37][root][INFO] - Training Epoch: 2/2, step 1708/7134 completed (loss: 0.08096713572740555, acc: 0.9925925731658936)
[2025-02-13 20:05:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:37][root][INFO] - Training Epoch: 2/2, step 1709/7134 completed (loss: 0.12504678964614868, acc: 0.9599999785423279)
[2025-02-13 20:05:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:37][root][INFO] - Training Epoch: 2/2, step 1710/7134 completed (loss: 0.14557509124279022, acc: 0.9662162065505981)
[2025-02-13 20:05:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:38][root][INFO] - Training Epoch: 2/2, step 1711/7134 completed (loss: 0.08089303970336914, acc: 0.9919354915618896)
[2025-02-13 20:05:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:38][root][INFO] - Training Epoch: 2/2, step 1712/7134 completed (loss: 0.067649707198143, acc: 0.9841269850730896)
[2025-02-13 20:05:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:39][root][INFO] - Training Epoch: 2/2, step 1713/7134 completed (loss: 0.09605356305837631, acc: 0.9809523820877075)
[2025-02-13 20:05:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:39][root][INFO] - Training Epoch: 2/2, step 1714/7134 completed (loss: 0.11960190534591675, acc: 0.9716312289237976)
[2025-02-13 20:05:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:39][root][INFO] - Training Epoch: 2/2, step 1715/7134 completed (loss: 0.04719354957342148, acc: 0.9905660152435303)
[2025-02-13 20:05:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:40][root][INFO] - Training Epoch: 2/2, step 1716/7134 completed (loss: 0.26301082968711853, acc: 0.9605262875556946)
[2025-02-13 20:05:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:40][root][INFO] - Training Epoch: 2/2, step 1717/7134 completed (loss: 0.1138678714632988, acc: 0.9534883499145508)
[2025-02-13 20:05:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:40][root][INFO] - Training Epoch: 2/2, step 1718/7134 completed (loss: 0.1611114740371704, acc: 0.9658119678497314)
[2025-02-13 20:05:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:41][root][INFO] - Training Epoch: 2/2, step 1719/7134 completed (loss: 0.17880259454250336, acc: 0.976331353187561)
[2025-02-13 20:05:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:41][root][INFO] - Training Epoch: 2/2, step 1720/7134 completed (loss: 0.20580261945724487, acc: 0.9307692050933838)
[2025-02-13 20:05:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:42][root][INFO] - Training Epoch: 2/2, step 1721/7134 completed (loss: 0.09194938838481903, acc: 0.984375)
[2025-02-13 20:05:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:42][root][INFO] - Training Epoch: 2/2, step 1722/7134 completed (loss: 0.06782833486795425, acc: 0.9898989796638489)
[2025-02-13 20:05:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:42][root][INFO] - Training Epoch: 2/2, step 1723/7134 completed (loss: 0.21169166266918182, acc: 0.9567901492118835)
[2025-02-13 20:05:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:43][root][INFO] - Training Epoch: 2/2, step 1724/7134 completed (loss: 0.1337512582540512, acc: 0.9765625)
[2025-02-13 20:05:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:43][root][INFO] - Training Epoch: 2/2, step 1725/7134 completed (loss: 0.0610029436647892, acc: 1.0)
[2025-02-13 20:05:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:44][root][INFO] - Training Epoch: 2/2, step 1726/7134 completed (loss: 0.09548444300889969, acc: 0.9677419066429138)
[2025-02-13 20:05:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:44][root][INFO] - Training Epoch: 2/2, step 1727/7134 completed (loss: 0.06085333228111267, acc: 0.982300877571106)
[2025-02-13 20:05:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:44][root][INFO] - Training Epoch: 2/2, step 1728/7134 completed (loss: 0.05466509982943535, acc: 0.9846153855323792)
[2025-02-13 20:05:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:45][root][INFO] - Training Epoch: 2/2, step 1729/7134 completed (loss: 0.1441732496023178, acc: 0.954954981803894)
[2025-02-13 20:05:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:45][root][INFO] - Training Epoch: 2/2, step 1730/7134 completed (loss: 0.08190374821424484, acc: 0.9813084006309509)
[2025-02-13 20:05:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:46][root][INFO] - Training Epoch: 2/2, step 1731/7134 completed (loss: 0.300487756729126, acc: 0.9156626462936401)
[2025-02-13 20:05:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:46][root][INFO] - Training Epoch: 2/2, step 1732/7134 completed (loss: 0.05233201012015343, acc: 0.9907407164573669)
[2025-02-13 20:05:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:46][root][INFO] - Training Epoch: 2/2, step 1733/7134 completed (loss: 0.14506252110004425, acc: 0.9541284441947937)
[2025-02-13 20:05:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:47][root][INFO] - Training Epoch: 2/2, step 1734/7134 completed (loss: 0.1959444284439087, acc: 0.9457831382751465)
[2025-02-13 20:05:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:47][root][INFO] - Training Epoch: 2/2, step 1735/7134 completed (loss: 0.0693618655204773, acc: 0.9867549538612366)
[2025-02-13 20:05:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:48][root][INFO] - Training Epoch: 2/2, step 1736/7134 completed (loss: 0.16506870090961456, acc: 0.9620253443717957)
[2025-02-13 20:05:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:48][root][INFO] - Training Epoch: 2/2, step 1737/7134 completed (loss: 0.1224154531955719, acc: 0.9800000190734863)
[2025-02-13 20:05:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:48][root][INFO] - Training Epoch: 2/2, step 1738/7134 completed (loss: 0.06629559397697449, acc: 0.9664804339408875)
[2025-02-13 20:05:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:49][root][INFO] - Training Epoch: 2/2, step 1739/7134 completed (loss: 0.069781593978405, acc: 0.9849624037742615)
[2025-02-13 20:05:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:49][root][INFO] - Training Epoch: 2/2, step 1740/7134 completed (loss: 0.07975789904594421, acc: 0.9797297120094299)
[2025-02-13 20:05:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:49][root][INFO] - Training Epoch: 2/2, step 1741/7134 completed (loss: 0.0766056701540947, acc: 0.9813664555549622)
[2025-02-13 20:05:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:50][root][INFO] - Training Epoch: 2/2, step 1742/7134 completed (loss: 0.18605932593345642, acc: 0.9602649211883545)
[2025-02-13 20:05:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:50][root][INFO] - Training Epoch: 2/2, step 1743/7134 completed (loss: 0.1614142507314682, acc: 0.9795918464660645)
[2025-02-13 20:05:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:51][root][INFO] - Training Epoch: 2/2, step 1744/7134 completed (loss: 0.09288235008716583, acc: 0.969924807548523)
[2025-02-13 20:05:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:51][root][INFO] - Training Epoch: 2/2, step 1745/7134 completed (loss: 0.06764596700668335, acc: 0.9823529124259949)
[2025-02-13 20:05:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:51][root][INFO] - Training Epoch: 2/2, step 1746/7134 completed (loss: 0.0588008388876915, acc: 0.9929078221321106)
[2025-02-13 20:05:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:52][root][INFO] - Training Epoch: 2/2, step 1747/7134 completed (loss: 0.12644048035144806, acc: 0.957317054271698)
[2025-02-13 20:05:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:52][root][INFO] - Training Epoch: 2/2, step 1748/7134 completed (loss: 0.07256818562746048, acc: 0.9823529124259949)
[2025-02-13 20:05:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:52][root][INFO] - Training Epoch: 2/2, step 1749/7134 completed (loss: 0.1712699830532074, acc: 0.9685863852500916)
[2025-02-13 20:05:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:53][root][INFO] - Training Epoch: 2/2, step 1750/7134 completed (loss: 0.17728763818740845, acc: 0.9822485446929932)
[2025-02-13 20:05:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:53][root][INFO] - Training Epoch: 2/2, step 1751/7134 completed (loss: 0.18011592328548431, acc: 0.9583333134651184)
[2025-02-13 20:05:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:54][root][INFO] - Training Epoch: 2/2, step 1752/7134 completed (loss: 0.229789599776268, acc: 0.9636363387107849)
[2025-02-13 20:05:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:54][root][INFO] - Training Epoch: 2/2, step 1753/7134 completed (loss: 0.1637212634086609, acc: 0.9602649211883545)
[2025-02-13 20:05:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:54][root][INFO] - Training Epoch: 2/2, step 1754/7134 completed (loss: 0.12038224935531616, acc: 0.9673202633857727)
[2025-02-13 20:05:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:55][root][INFO] - Training Epoch: 2/2, step 1755/7134 completed (loss: 0.04843531548976898, acc: 1.0)
[2025-02-13 20:05:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:55][root][INFO] - Training Epoch: 2/2, step 1756/7134 completed (loss: 0.1494203507900238, acc: 0.9800000190734863)
[2025-02-13 20:05:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:56][root][INFO] - Training Epoch: 2/2, step 1757/7134 completed (loss: 0.05025699734687805, acc: 0.9917355179786682)
[2025-02-13 20:05:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:56][root][INFO] - Training Epoch: 2/2, step 1758/7134 completed (loss: 0.09250905364751816, acc: 0.9823529124259949)
[2025-02-13 20:05:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:56][root][INFO] - Training Epoch: 2/2, step 1759/7134 completed (loss: 0.06713367253541946, acc: 0.9868420958518982)
[2025-02-13 20:05:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:57][root][INFO] - Training Epoch: 2/2, step 1760/7134 completed (loss: 0.04635041952133179, acc: 0.9918032884597778)
[2025-02-13 20:05:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:57][root][INFO] - Training Epoch: 2/2, step 1761/7134 completed (loss: 0.05127518251538277, acc: 1.0)
[2025-02-13 20:05:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:57][root][INFO] - Training Epoch: 2/2, step 1762/7134 completed (loss: 0.056960444897413254, acc: 0.9902912378311157)
[2025-02-13 20:05:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:58][root][INFO] - Training Epoch: 2/2, step 1763/7134 completed (loss: 0.0761423110961914, acc: 0.9863013625144958)
[2025-02-13 20:05:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:58][root][INFO] - Training Epoch: 2/2, step 1764/7134 completed (loss: 0.16669337451457977, acc: 0.975806474685669)
[2025-02-13 20:05:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:59][root][INFO] - Training Epoch: 2/2, step 1765/7134 completed (loss: 0.0975336879491806, acc: 0.9542483687400818)
[2025-02-13 20:05:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:59][root][INFO] - Training Epoch: 2/2, step 1766/7134 completed (loss: 0.13476301729679108, acc: 0.9740259647369385)
[2025-02-13 20:05:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:05:59][root][INFO] - Training Epoch: 2/2, step 1767/7134 completed (loss: 0.2027333527803421, acc: 0.9661017060279846)
[2025-02-13 20:05:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:00][root][INFO] - Training Epoch: 2/2, step 1768/7134 completed (loss: 0.06057167425751686, acc: 0.9821428656578064)
[2025-02-13 20:06:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:00][root][INFO] - Training Epoch: 2/2, step 1769/7134 completed (loss: 0.10791971534490585, acc: 0.9599999785423279)
[2025-02-13 20:06:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:01][root][INFO] - Training Epoch: 2/2, step 1770/7134 completed (loss: 0.10700167715549469, acc: 0.9794520735740662)
[2025-02-13 20:06:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:01][root][INFO] - Training Epoch: 2/2, step 1771/7134 completed (loss: 0.12662047147750854, acc: 0.9675324559211731)
[2025-02-13 20:06:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:01][root][INFO] - Training Epoch: 2/2, step 1772/7134 completed (loss: 0.13467150926589966, acc: 0.9571428298950195)
[2025-02-13 20:06:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:02][root][INFO] - Training Epoch: 2/2, step 1773/7134 completed (loss: 0.08833406120538712, acc: 0.9858155846595764)
[2025-02-13 20:06:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:02][root][INFO] - Training Epoch: 2/2, step 1774/7134 completed (loss: 0.041704315692186356, acc: 1.0)
[2025-02-13 20:06:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:02][root][INFO] - Training Epoch: 2/2, step 1775/7134 completed (loss: 0.05516006425023079, acc: 0.9931507110595703)
[2025-02-13 20:06:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:03][root][INFO] - Training Epoch: 2/2, step 1776/7134 completed (loss: 0.07936438918113708, acc: 0.9655172228813171)
[2025-02-13 20:06:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:03][root][INFO] - Training Epoch: 2/2, step 1777/7134 completed (loss: 0.03533457964658737, acc: 0.9808917045593262)
[2025-02-13 20:06:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:04][root][INFO] - Training Epoch: 2/2, step 1778/7134 completed (loss: 0.20108459889888763, acc: 0.9496855139732361)
[2025-02-13 20:06:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:04][root][INFO] - Training Epoch: 2/2, step 1779/7134 completed (loss: 0.1276087909936905, acc: 0.982300877571106)
[2025-02-13 20:06:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:04][root][INFO] - Training Epoch: 2/2, step 1780/7134 completed (loss: 0.04823782294988632, acc: 1.0)
[2025-02-13 20:06:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:06:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:07:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:08:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:09:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:10][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.2625, device='cuda:0') eval_epoch_loss=tensor(0.2331, device='cuda:0') eval_epoch_acc=tensor(0.9465, device='cuda:0')
[2025-02-13 20:10:10][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-13 20:10:10][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-13 20:10:10][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_wavlm_llama32_1b_linear_peft_transfer_librispeech-100/asr_epoch_2_step_1781_loss_0.23309457302093506/model.pt
[2025-02-13 20:10:10][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_wavlm_llama32_1b_linear_peft_transfer_librispeech-100 directory
[2025-02-13 20:10:10][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 2 is 0.23309457302093506
[2025-02-13 20:10:10][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.9465076923370361
[2025-02-13 20:10:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:10][root][INFO] - Training Epoch: 2/2, step 1781/7134 completed (loss: 0.06064863130450249, acc: 0.9798657894134521)
[2025-02-13 20:10:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:11][root][INFO] - Training Epoch: 2/2, step 1782/7134 completed (loss: 0.03277727589011192, acc: 0.9935897588729858)
[2025-02-13 20:10:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:11][root][INFO] - Training Epoch: 2/2, step 1783/7134 completed (loss: 0.0994749665260315, acc: 0.9710982441902161)
[2025-02-13 20:10:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:11][root][INFO] - Training Epoch: 2/2, step 1784/7134 completed (loss: 0.07133372128009796, acc: 0.9935897588729858)
[2025-02-13 20:10:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:12][root][INFO] - Training Epoch: 2/2, step 1785/7134 completed (loss: 0.12764333188533783, acc: 0.9578313231468201)
[2025-02-13 20:10:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:12][root][INFO] - Training Epoch: 2/2, step 1786/7134 completed (loss: 0.048015251755714417, acc: 1.0)
[2025-02-13 20:10:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:13][root][INFO] - Training Epoch: 2/2, step 1787/7134 completed (loss: 0.15134143829345703, acc: 0.9626865386962891)
[2025-02-13 20:10:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:13][root][INFO] - Training Epoch: 2/2, step 1788/7134 completed (loss: 0.12653368711471558, acc: 0.9793103337287903)
[2025-02-13 20:10:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:13][root][INFO] - Training Epoch: 2/2, step 1789/7134 completed (loss: 0.10286734253168106, acc: 0.9802631735801697)
[2025-02-13 20:10:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:14][root][INFO] - Training Epoch: 2/2, step 1790/7134 completed (loss: 0.33681485056877136, acc: 0.9290322661399841)
[2025-02-13 20:10:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:14][root][INFO] - Training Epoch: 2/2, step 1791/7134 completed (loss: 0.3712834417819977, acc: 0.9386503100395203)
[2025-02-13 20:10:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:14][root][INFO] - Training Epoch: 2/2, step 1792/7134 completed (loss: 0.6406745910644531, acc: 0.8938547372817993)
[2025-02-13 20:10:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:15][root][INFO] - Training Epoch: 2/2, step 1793/7134 completed (loss: 0.36636626720428467, acc: 0.9349112510681152)
[2025-02-13 20:10:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:15][root][INFO] - Training Epoch: 2/2, step 1794/7134 completed (loss: 0.3376375138759613, acc: 0.9276315569877625)
[2025-02-13 20:10:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:16][root][INFO] - Training Epoch: 2/2, step 1795/7134 completed (loss: 0.3650044798851013, acc: 0.8999999761581421)
[2025-02-13 20:10:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:16][root][INFO] - Training Epoch: 2/2, step 1796/7134 completed (loss: 0.3532712161540985, acc: 0.9152542352676392)
[2025-02-13 20:10:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:16][root][INFO] - Training Epoch: 2/2, step 1797/7134 completed (loss: 0.278685063123703, acc: 0.9142857193946838)
[2025-02-13 20:10:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:17][root][INFO] - Training Epoch: 2/2, step 1798/7134 completed (loss: 0.3143383264541626, acc: 0.9224806427955627)
[2025-02-13 20:10:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:17][root][INFO] - Training Epoch: 2/2, step 1799/7134 completed (loss: 0.3168017268180847, acc: 0.9191918969154358)
[2025-02-13 20:10:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:18][root][INFO] - Training Epoch: 2/2, step 1800/7134 completed (loss: 0.22500601410865784, acc: 0.938095211982727)
[2025-02-13 20:10:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:18][root][INFO] - Training Epoch: 2/2, step 1801/7134 completed (loss: 0.2771458923816681, acc: 0.9459459185600281)
[2025-02-13 20:10:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:18][root][INFO] - Training Epoch: 2/2, step 1802/7134 completed (loss: 0.201494038105011, acc: 0.9521276354789734)
[2025-02-13 20:10:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:19][root][INFO] - Training Epoch: 2/2, step 1803/7134 completed (loss: 0.3066960275173187, acc: 0.9207317233085632)
[2025-02-13 20:10:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:19][root][INFO] - Training Epoch: 2/2, step 1804/7134 completed (loss: 0.07845373451709747, acc: 0.9735449552536011)
[2025-02-13 20:10:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:20][root][INFO] - Training Epoch: 2/2, step 1805/7134 completed (loss: 0.13575580716133118, acc: 0.9766082167625427)
[2025-02-13 20:10:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:20][root][INFO] - Training Epoch: 2/2, step 1806/7134 completed (loss: 0.13623638451099396, acc: 0.9508196711540222)
[2025-02-13 20:10:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:20][root][INFO] - Training Epoch: 2/2, step 1807/7134 completed (loss: 0.204396054148674, acc: 0.9578313231468201)
[2025-02-13 20:10:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:21][root][INFO] - Training Epoch: 2/2, step 1808/7134 completed (loss: 0.06916973739862442, acc: 0.9830508232116699)
[2025-02-13 20:10:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:21][root][INFO] - Training Epoch: 2/2, step 1809/7134 completed (loss: 0.06748567521572113, acc: 0.9884393215179443)
[2025-02-13 20:10:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:21][root][INFO] - Training Epoch: 2/2, step 1810/7134 completed (loss: 0.08592686057090759, acc: 0.9835164546966553)
[2025-02-13 20:10:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:22][root][INFO] - Training Epoch: 2/2, step 1811/7134 completed (loss: 0.21155765652656555, acc: 0.957446813583374)
[2025-02-13 20:10:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:22][root][INFO] - Training Epoch: 2/2, step 1812/7134 completed (loss: 0.13413777947425842, acc: 0.9717513918876648)
[2025-02-13 20:10:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:23][root][INFO] - Training Epoch: 2/2, step 1813/7134 completed (loss: 0.1648210883140564, acc: 0.9613259434700012)
[2025-02-13 20:10:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:23][root][INFO] - Training Epoch: 2/2, step 1814/7134 completed (loss: 0.09458935260772705, acc: 0.988950252532959)
[2025-02-13 20:10:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:23][root][INFO] - Training Epoch: 2/2, step 1815/7134 completed (loss: 0.12694625556468964, acc: 0.9745222926139832)
[2025-02-13 20:10:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:24][root][INFO] - Training Epoch: 2/2, step 1816/7134 completed (loss: 0.18524973094463348, acc: 0.9513888955116272)
[2025-02-13 20:10:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:24][root][INFO] - Training Epoch: 2/2, step 1817/7134 completed (loss: 0.12207362055778503, acc: 0.9583333134651184)
[2025-02-13 20:10:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:24][root][INFO] - Training Epoch: 2/2, step 1818/7134 completed (loss: 0.13635393977165222, acc: 0.9685863852500916)
[2025-02-13 20:10:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:25][root][INFO] - Training Epoch: 2/2, step 1819/7134 completed (loss: 0.08109889179468155, acc: 0.9767441749572754)
[2025-02-13 20:10:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:25][root][INFO] - Training Epoch: 2/2, step 1820/7134 completed (loss: 0.08306589722633362, acc: 0.9858490824699402)
[2025-02-13 20:10:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:26][root][INFO] - Training Epoch: 2/2, step 1821/7134 completed (loss: 0.07564859837293625, acc: 0.9766355156898499)
[2025-02-13 20:10:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:26][root][INFO] - Training Epoch: 2/2, step 1822/7134 completed (loss: 0.0527469739317894, acc: 0.9934210777282715)
[2025-02-13 20:10:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:26][root][INFO] - Training Epoch: 2/2, step 1823/7134 completed (loss: 0.12404987215995789, acc: 0.9767441749572754)
[2025-02-13 20:10:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:27][root][INFO] - Training Epoch: 2/2, step 1824/7134 completed (loss: 0.048175398260354996, acc: 0.9890710115432739)
[2025-02-13 20:10:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:27][root][INFO] - Training Epoch: 2/2, step 1825/7134 completed (loss: 0.058065664023160934, acc: 0.9906542301177979)
[2025-02-13 20:10:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:27][root][INFO] - Training Epoch: 2/2, step 1826/7134 completed (loss: 0.05066012218594551, acc: 0.9824561476707458)
[2025-02-13 20:10:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:28][root][INFO] - Training Epoch: 2/2, step 1827/7134 completed (loss: 0.10898688435554504, acc: 0.9763033390045166)
[2025-02-13 20:10:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:28][root][INFO] - Training Epoch: 2/2, step 1828/7134 completed (loss: 0.05309337377548218, acc: 0.9777777791023254)
[2025-02-13 20:10:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:29][root][INFO] - Training Epoch: 2/2, step 1829/7134 completed (loss: 0.07556670159101486, acc: 0.9838709831237793)
[2025-02-13 20:10:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:29][root][INFO] - Training Epoch: 2/2, step 1830/7134 completed (loss: 0.06726823002099991, acc: 0.9792746305465698)
[2025-02-13 20:10:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:29][root][INFO] - Training Epoch: 2/2, step 1831/7134 completed (loss: 0.062244921922683716, acc: 0.9772727489471436)
[2025-02-13 20:10:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:30][root][INFO] - Training Epoch: 2/2, step 1832/7134 completed (loss: 0.1651952564716339, acc: 0.9534883499145508)
[2025-02-13 20:10:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:30][root][INFO] - Training Epoch: 2/2, step 1833/7134 completed (loss: 0.515930712223053, acc: 0.9024389982223511)
[2025-02-13 20:10:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:31][root][INFO] - Training Epoch: 2/2, step 1834/7134 completed (loss: 0.054623957723379135, acc: 0.9871794581413269)
[2025-02-13 20:10:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:31][root][INFO] - Training Epoch: 2/2, step 1835/7134 completed (loss: 0.20378246903419495, acc: 0.9415584206581116)
[2025-02-13 20:10:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:31][root][INFO] - Training Epoch: 2/2, step 1836/7134 completed (loss: 0.14855505526065826, acc: 0.9679144620895386)
[2025-02-13 20:10:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:32][root][INFO] - Training Epoch: 2/2, step 1837/7134 completed (loss: 0.10632870346307755, acc: 0.9670329689979553)
[2025-02-13 20:10:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:32][root][INFO] - Training Epoch: 2/2, step 1838/7134 completed (loss: 0.11823153495788574, acc: 0.9816513657569885)
[2025-02-13 20:10:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:32][root][INFO] - Training Epoch: 2/2, step 1839/7134 completed (loss: 0.09276199340820312, acc: 0.9670329689979553)
[2025-02-13 20:10:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:33][root][INFO] - Training Epoch: 2/2, step 1840/7134 completed (loss: 0.1430639624595642, acc: 0.9620253443717957)
[2025-02-13 20:10:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:33][root][INFO] - Training Epoch: 2/2, step 1841/7134 completed (loss: 0.14617878198623657, acc: 0.9636363387107849)
[2025-02-13 20:10:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:34][root][INFO] - Training Epoch: 2/2, step 1842/7134 completed (loss: 0.14500141143798828, acc: 0.9634146094322205)
[2025-02-13 20:10:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:34][root][INFO] - Training Epoch: 2/2, step 1843/7134 completed (loss: 0.15817944705486298, acc: 0.9702380895614624)
[2025-02-13 20:10:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:34][root][INFO] - Training Epoch: 2/2, step 1844/7134 completed (loss: 0.1033441498875618, acc: 0.9716312289237976)
[2025-02-13 20:10:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:35][root][INFO] - Training Epoch: 2/2, step 1845/7134 completed (loss: 0.0747494250535965, acc: 0.989130437374115)
[2025-02-13 20:10:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:35][root][INFO] - Training Epoch: 2/2, step 1846/7134 completed (loss: 0.0999390259385109, acc: 0.9743589758872986)
[2025-02-13 20:10:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:35][root][INFO] - Training Epoch: 2/2, step 1847/7134 completed (loss: 0.12015867233276367, acc: 0.9772727489471436)
[2025-02-13 20:10:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:36][root][INFO] - Training Epoch: 2/2, step 1848/7134 completed (loss: 0.1472042053937912, acc: 0.9680851101875305)
[2025-02-13 20:10:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:36][root][INFO] - Training Epoch: 2/2, step 1849/7134 completed (loss: 0.09016454219818115, acc: 0.9647887349128723)
[2025-02-13 20:10:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:37][root][INFO] - Training Epoch: 2/2, step 1850/7134 completed (loss: 0.2232436090707779, acc: 0.938144326210022)
[2025-02-13 20:10:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:37][root][INFO] - Training Epoch: 2/2, step 1851/7134 completed (loss: 0.13902337849140167, acc: 0.9487179517745972)
[2025-02-13 20:10:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:38][root][INFO] - Training Epoch: 2/2, step 1852/7134 completed (loss: 0.26750656962394714, acc: 0.9437500238418579)
[2025-02-13 20:10:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:38][root][INFO] - Training Epoch: 2/2, step 1853/7134 completed (loss: 0.17417973279953003, acc: 0.9497487545013428)
[2025-02-13 20:10:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:38][root][INFO] - Training Epoch: 2/2, step 1854/7134 completed (loss: 0.1818254441022873, acc: 0.959276020526886)
[2025-02-13 20:10:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:39][root][INFO] - Training Epoch: 2/2, step 1855/7134 completed (loss: 0.2379373162984848, acc: 0.9431279897689819)
[2025-02-13 20:10:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:39][root][INFO] - Training Epoch: 2/2, step 1856/7134 completed (loss: 0.12124449759721756, acc: 0.9591836929321289)
[2025-02-13 20:10:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:39][root][INFO] - Training Epoch: 2/2, step 1857/7134 completed (loss: 0.23836511373519897, acc: 0.9264705777168274)
[2025-02-13 20:10:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:40][root][INFO] - Training Epoch: 2/2, step 1858/7134 completed (loss: 0.106581911444664, acc: 0.9693877696990967)
[2025-02-13 20:10:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:40][root][INFO] - Training Epoch: 2/2, step 1859/7134 completed (loss: 0.0933147743344307, acc: 0.9856459498405457)
[2025-02-13 20:10:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:41][root][INFO] - Training Epoch: 2/2, step 1860/7134 completed (loss: 0.1161259338259697, acc: 0.9672130942344666)
[2025-02-13 20:10:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:41][root][INFO] - Training Epoch: 2/2, step 1861/7134 completed (loss: 0.11750246584415436, acc: 0.9738219976425171)
[2025-02-13 20:10:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:41][root][INFO] - Training Epoch: 2/2, step 1862/7134 completed (loss: 0.0520206093788147, acc: 0.9894179701805115)
[2025-02-13 20:10:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:42][root][INFO] - Training Epoch: 2/2, step 1863/7134 completed (loss: 0.11944743245840073, acc: 0.9802955389022827)
[2025-02-13 20:10:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:42][root][INFO] - Training Epoch: 2/2, step 1864/7134 completed (loss: 0.4438976049423218, acc: 0.9187816977500916)
[2025-02-13 20:10:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:42][root][INFO] - Training Epoch: 2/2, step 1865/7134 completed (loss: 0.1854398399591446, acc: 0.9731543660163879)
[2025-02-13 20:10:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:43][root][INFO] - Training Epoch: 2/2, step 1866/7134 completed (loss: 0.0351732075214386, acc: 0.9941176176071167)
[2025-02-13 20:10:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:43][root][INFO] - Training Epoch: 2/2, step 1867/7134 completed (loss: 0.08353201299905777, acc: 0.9829545617103577)
[2025-02-13 20:10:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:44][root][INFO] - Training Epoch: 2/2, step 1868/7134 completed (loss: 0.23014633357524872, acc: 0.9386503100395203)
[2025-02-13 20:10:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:44][root][INFO] - Training Epoch: 2/2, step 1869/7134 completed (loss: 0.11954602599143982, acc: 0.9794520735740662)
[2025-02-13 20:10:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:44][root][INFO] - Training Epoch: 2/2, step 1870/7134 completed (loss: 0.05574136599898338, acc: 0.9891892075538635)
[2025-02-13 20:10:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:45][root][INFO] - Training Epoch: 2/2, step 1871/7134 completed (loss: 0.26056352257728577, acc: 0.9548386931419373)
[2025-02-13 20:10:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:45][root][INFO] - Training Epoch: 2/2, step 1872/7134 completed (loss: 1.488696813583374, acc: 0.695652186870575)
[2025-02-13 20:10:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:46][root][INFO] - Training Epoch: 2/2, step 1873/7134 completed (loss: 0.5423002243041992, acc: 0.8925619721412659)
[2025-02-13 20:10:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:46][root][INFO] - Training Epoch: 2/2, step 1874/7134 completed (loss: 0.07038075476884842, acc: 0.9942528605461121)
[2025-02-13 20:10:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:46][root][INFO] - Training Epoch: 2/2, step 1875/7134 completed (loss: 0.10513774305582047, acc: 0.9553571343421936)
[2025-02-13 20:10:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:47][root][INFO] - Training Epoch: 2/2, step 1876/7134 completed (loss: 0.4465840458869934, acc: 0.8571428656578064)
[2025-02-13 20:10:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:47][root][INFO] - Training Epoch: 2/2, step 1877/7134 completed (loss: 0.08252520114183426, acc: 0.9774011373519897)
[2025-02-13 20:10:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:48][root][INFO] - Training Epoch: 2/2, step 1878/7134 completed (loss: 0.08113755285739899, acc: 0.9803921580314636)
[2025-02-13 20:10:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:48][root][INFO] - Training Epoch: 2/2, step 1879/7134 completed (loss: 0.6242859959602356, acc: 0.8765432238578796)
[2025-02-13 20:10:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:48][root][INFO] - Training Epoch: 2/2, step 1880/7134 completed (loss: 0.17814260721206665, acc: 0.9593495726585388)
[2025-02-13 20:10:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:49][root][INFO] - Training Epoch: 2/2, step 1881/7134 completed (loss: 0.11860968172550201, acc: 0.9577465057373047)
[2025-02-13 20:10:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:49][root][INFO] - Training Epoch: 2/2, step 1882/7134 completed (loss: 0.2149326652288437, acc: 0.95652174949646)
[2025-02-13 20:10:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:50][root][INFO] - Training Epoch: 2/2, step 1883/7134 completed (loss: 0.22333352267742157, acc: 0.9457364082336426)
[2025-02-13 20:10:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:50][root][INFO] - Training Epoch: 2/2, step 1884/7134 completed (loss: 0.16834552586078644, acc: 0.9594594836235046)
[2025-02-13 20:10:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:50][root][INFO] - Training Epoch: 2/2, step 1885/7134 completed (loss: 0.2997439503669739, acc: 0.9507042169570923)
[2025-02-13 20:10:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:51][root][INFO] - Training Epoch: 2/2, step 1886/7134 completed (loss: 0.27098408341407776, acc: 0.9285714030265808)
[2025-02-13 20:10:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:51][root][INFO] - Training Epoch: 2/2, step 1887/7134 completed (loss: 0.21916650235652924, acc: 0.9536423683166504)
[2025-02-13 20:10:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:51][root][INFO] - Training Epoch: 2/2, step 1888/7134 completed (loss: 0.1290900856256485, acc: 0.9743589758872986)
[2025-02-13 20:10:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:52][root][INFO] - Training Epoch: 2/2, step 1889/7134 completed (loss: 0.12898030877113342, acc: 0.9824561476707458)
[2025-02-13 20:10:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:52][root][INFO] - Training Epoch: 2/2, step 1890/7134 completed (loss: 0.1054326519370079, acc: 0.9602649211883545)
[2025-02-13 20:10:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:53][root][INFO] - Training Epoch: 2/2, step 1891/7134 completed (loss: 0.2396310269832611, acc: 0.9186046719551086)
[2025-02-13 20:10:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:53][root][INFO] - Training Epoch: 2/2, step 1892/7134 completed (loss: 0.15544261038303375, acc: 0.9640718698501587)
[2025-02-13 20:10:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:53][root][INFO] - Training Epoch: 2/2, step 1893/7134 completed (loss: 0.18931160867214203, acc: 0.9382715821266174)
[2025-02-13 20:10:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:54][root][INFO] - Training Epoch: 2/2, step 1894/7134 completed (loss: 0.2064848393201828, acc: 0.9594594836235046)
[2025-02-13 20:10:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:54][root][INFO] - Training Epoch: 2/2, step 1895/7134 completed (loss: 0.5957300662994385, acc: 0.8651685118675232)
[2025-02-13 20:10:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:55][root][INFO] - Training Epoch: 2/2, step 1896/7134 completed (loss: 0.16451208293437958, acc: 0.9454545378684998)
[2025-02-13 20:10:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:55][root][INFO] - Training Epoch: 2/2, step 1897/7134 completed (loss: 0.13247962296009064, acc: 0.9647887349128723)
[2025-02-13 20:10:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:55][root][INFO] - Training Epoch: 2/2, step 1898/7134 completed (loss: 0.12333979457616806, acc: 0.9793103337287903)
[2025-02-13 20:10:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:56][root][INFO] - Training Epoch: 2/2, step 1899/7134 completed (loss: 0.0608457513153553, acc: 1.0)
[2025-02-13 20:10:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:56][root][INFO] - Training Epoch: 2/2, step 1900/7134 completed (loss: 0.0667654499411583, acc: 0.9852941036224365)
[2025-02-13 20:10:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:57][root][INFO] - Training Epoch: 2/2, step 1901/7134 completed (loss: 0.13919682800769806, acc: 0.9856114983558655)
[2025-02-13 20:10:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:57][root][INFO] - Training Epoch: 2/2, step 1902/7134 completed (loss: 0.10876483470201492, acc: 0.971222996711731)
[2025-02-13 20:10:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:57][root][INFO] - Training Epoch: 2/2, step 1903/7134 completed (loss: 0.09065958112478256, acc: 0.9738562107086182)
[2025-02-13 20:10:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:58][root][INFO] - Training Epoch: 2/2, step 1904/7134 completed (loss: 0.20339713990688324, acc: 0.9428571462631226)
[2025-02-13 20:10:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:58][root][INFO] - Training Epoch: 2/2, step 1905/7134 completed (loss: 0.12125683575868607, acc: 0.978723406791687)
[2025-02-13 20:10:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:58][root][INFO] - Training Epoch: 2/2, step 1906/7134 completed (loss: 0.09834194928407669, acc: 0.985401451587677)
[2025-02-13 20:10:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:59][root][INFO] - Training Epoch: 2/2, step 1907/7134 completed (loss: 0.1358187347650528, acc: 0.9795918464660645)
[2025-02-13 20:10:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:10:59][root][INFO] - Training Epoch: 2/2, step 1908/7134 completed (loss: 0.17154328525066376, acc: 0.9766355156898499)
[2025-02-13 20:10:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:00][root][INFO] - Training Epoch: 2/2, step 1909/7134 completed (loss: 0.17701372504234314, acc: 0.9591836929321289)
[2025-02-13 20:11:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:00][root][INFO] - Training Epoch: 2/2, step 1910/7134 completed (loss: 0.10295175760984421, acc: 0.9767441749572754)
[2025-02-13 20:11:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:01][root][INFO] - Training Epoch: 2/2, step 1911/7134 completed (loss: 0.08554226160049438, acc: 0.9796954393386841)
[2025-02-13 20:11:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:01][root][INFO] - Training Epoch: 2/2, step 1912/7134 completed (loss: 0.046884581446647644, acc: 0.9757575988769531)
[2025-02-13 20:11:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:01][root][INFO] - Training Epoch: 2/2, step 1913/7134 completed (loss: 0.16665925085544586, acc: 0.9685039520263672)
[2025-02-13 20:11:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:02][root][INFO] - Training Epoch: 2/2, step 1914/7134 completed (loss: 0.1703333705663681, acc: 0.9617486596107483)
[2025-02-13 20:11:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:02][root][INFO] - Training Epoch: 2/2, step 1915/7134 completed (loss: 0.07906442135572433, acc: 0.9860140085220337)
[2025-02-13 20:11:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:03][root][INFO] - Training Epoch: 2/2, step 1916/7134 completed (loss: 0.09659436345100403, acc: 0.9725274443626404)
[2025-02-13 20:11:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:03][root][INFO] - Training Epoch: 2/2, step 1917/7134 completed (loss: 0.06547514349222183, acc: 0.9878787994384766)
[2025-02-13 20:11:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:03][root][INFO] - Training Epoch: 2/2, step 1918/7134 completed (loss: 0.15512828528881073, acc: 0.9668874144554138)
[2025-02-13 20:11:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:04][root][INFO] - Training Epoch: 2/2, step 1919/7134 completed (loss: 0.0868355929851532, acc: 0.9817073345184326)
[2025-02-13 20:11:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:04][root][INFO] - Training Epoch: 2/2, step 1920/7134 completed (loss: 0.05155090242624283, acc: 0.9931972622871399)
[2025-02-13 20:11:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:04][root][INFO] - Training Epoch: 2/2, step 1921/7134 completed (loss: 0.09064775705337524, acc: 0.9766082167625427)
[2025-02-13 20:11:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:05][root][INFO] - Training Epoch: 2/2, step 1922/7134 completed (loss: 0.050449758768081665, acc: 0.9836065769195557)
[2025-02-13 20:11:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:05][root][INFO] - Training Epoch: 2/2, step 1923/7134 completed (loss: 0.04550368711352348, acc: 0.9930070042610168)
[2025-02-13 20:11:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:06][root][INFO] - Training Epoch: 2/2, step 1924/7134 completed (loss: 0.14659816026687622, acc: 0.9453125)
[2025-02-13 20:11:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:06][root][INFO] - Training Epoch: 2/2, step 1925/7134 completed (loss: 0.1291639357805252, acc: 0.9748954176902771)
[2025-02-13 20:11:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:06][root][INFO] - Training Epoch: 2/2, step 1926/7134 completed (loss: 0.05693592503666878, acc: 0.9910714030265808)
[2025-02-13 20:11:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:07][root][INFO] - Training Epoch: 2/2, step 1927/7134 completed (loss: 0.1401837170124054, acc: 0.9716981053352356)
[2025-02-13 20:11:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:07][root][INFO] - Training Epoch: 2/2, step 1928/7134 completed (loss: 0.08188154548406601, acc: 0.988095223903656)
[2025-02-13 20:11:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:07][root][INFO] - Training Epoch: 2/2, step 1929/7134 completed (loss: 0.05603209510445595, acc: 0.9888268113136292)
[2025-02-13 20:11:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:08][root][INFO] - Training Epoch: 2/2, step 1930/7134 completed (loss: 0.245293989777565, acc: 0.9227052927017212)
[2025-02-13 20:11:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:08][root][INFO] - Training Epoch: 2/2, step 1931/7134 completed (loss: 0.3107753098011017, acc: 0.9351851940155029)
[2025-02-13 20:11:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:09][root][INFO] - Training Epoch: 2/2, step 1932/7134 completed (loss: 0.17041638493537903, acc: 0.9664804339408875)
[2025-02-13 20:11:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:09][root][INFO] - Training Epoch: 2/2, step 1933/7134 completed (loss: 0.28316715359687805, acc: 0.9396985173225403)
[2025-02-13 20:11:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:09][root][INFO] - Training Epoch: 2/2, step 1934/7134 completed (loss: 0.06620492041110992, acc: 0.9828571677207947)
[2025-02-13 20:11:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:10][root][INFO] - Training Epoch: 2/2, step 1935/7134 completed (loss: 0.36868026852607727, acc: 0.9390243887901306)
[2025-02-13 20:11:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:10][root][INFO] - Training Epoch: 2/2, step 1936/7134 completed (loss: 0.06432514637708664, acc: 0.9738562107086182)
[2025-02-13 20:11:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:11][root][INFO] - Training Epoch: 2/2, step 1937/7134 completed (loss: 0.1865222007036209, acc: 0.9512194991111755)
[2025-02-13 20:11:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:11][root][INFO] - Training Epoch: 2/2, step 1938/7134 completed (loss: 0.3414457440376282, acc: 0.9239130616188049)
[2025-02-13 20:11:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:11][root][INFO] - Training Epoch: 2/2, step 1939/7134 completed (loss: 0.11066900938749313, acc: 0.9568965435028076)
[2025-02-13 20:11:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:12][root][INFO] - Training Epoch: 2/2, step 1940/7134 completed (loss: 0.18897542357444763, acc: 0.940119743347168)
[2025-02-13 20:11:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:12][root][INFO] - Training Epoch: 2/2, step 1941/7134 completed (loss: 0.09374789148569107, acc: 0.9704142212867737)
[2025-02-13 20:11:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:13][root][INFO] - Training Epoch: 2/2, step 1942/7134 completed (loss: 0.08244498819112778, acc: 0.9722222089767456)
[2025-02-13 20:11:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:13][root][INFO] - Training Epoch: 2/2, step 1943/7134 completed (loss: 0.08046791702508926, acc: 0.9789473414421082)
[2025-02-13 20:11:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:13][root][INFO] - Training Epoch: 2/2, step 1944/7134 completed (loss: 0.061187442392110825, acc: 0.9836065769195557)
[2025-02-13 20:11:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:14][root][INFO] - Training Epoch: 2/2, step 1945/7134 completed (loss: 0.04576112702488899, acc: 1.0)
[2025-02-13 20:11:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:14][root][INFO] - Training Epoch: 2/2, step 1946/7134 completed (loss: 0.11643014848232269, acc: 0.9714285731315613)
[2025-02-13 20:11:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:15][root][INFO] - Training Epoch: 2/2, step 1947/7134 completed (loss: 0.11367665231227875, acc: 0.9768785834312439)
[2025-02-13 20:11:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:15][root][INFO] - Training Epoch: 2/2, step 1948/7134 completed (loss: 0.10394084453582764, acc: 0.9615384340286255)
[2025-02-13 20:11:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:15][root][INFO] - Training Epoch: 2/2, step 1949/7134 completed (loss: 0.1830926239490509, acc: 0.9870129823684692)
[2025-02-13 20:11:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:16][root][INFO] - Training Epoch: 2/2, step 1950/7134 completed (loss: 0.07058852165937424, acc: 0.9855072498321533)
[2025-02-13 20:11:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:16][root][INFO] - Training Epoch: 2/2, step 1951/7134 completed (loss: 0.08326876908540726, acc: 0.9849624037742615)
[2025-02-13 20:11:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:17][root][INFO] - Training Epoch: 2/2, step 1952/7134 completed (loss: 0.06574684381484985, acc: 0.9819276928901672)
[2025-02-13 20:11:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:17][root][INFO] - Training Epoch: 2/2, step 1953/7134 completed (loss: 0.057476453483104706, acc: 0.9873417615890503)
[2025-02-13 20:11:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:17][root][INFO] - Training Epoch: 2/2, step 1954/7134 completed (loss: 0.03512471541762352, acc: 1.0)
[2025-02-13 20:11:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:18][root][INFO] - Training Epoch: 2/2, step 1955/7134 completed (loss: 0.05428381636738777, acc: 1.0)
[2025-02-13 20:11:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:18][root][INFO] - Training Epoch: 2/2, step 1956/7134 completed (loss: 0.02768438309431076, acc: 1.0)
[2025-02-13 20:11:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:19][root][INFO] - Training Epoch: 2/2, step 1957/7134 completed (loss: 0.10850245505571365, acc: 0.9624060392379761)
[2025-02-13 20:11:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:19][root][INFO] - Training Epoch: 2/2, step 1958/7134 completed (loss: 0.13129045069217682, acc: 0.9555555582046509)
[2025-02-13 20:11:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:19][root][INFO] - Training Epoch: 2/2, step 1959/7134 completed (loss: 0.10729098320007324, acc: 0.9732142686843872)
[2025-02-13 20:11:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:20][root][INFO] - Training Epoch: 2/2, step 1960/7134 completed (loss: 0.051344260573387146, acc: 0.9833333492279053)
[2025-02-13 20:11:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:20][root][INFO] - Training Epoch: 2/2, step 1961/7134 completed (loss: 0.045993365347385406, acc: 0.9873417615890503)
[2025-02-13 20:11:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:21][root][INFO] - Training Epoch: 2/2, step 1962/7134 completed (loss: 0.03680003434419632, acc: 1.0)
[2025-02-13 20:11:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:21][root][INFO] - Training Epoch: 2/2, step 1963/7134 completed (loss: 0.15549318492412567, acc: 0.9602649211883545)
[2025-02-13 20:11:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:21][root][INFO] - Training Epoch: 2/2, step 1964/7134 completed (loss: 0.1251119077205658, acc: 0.9797979593276978)
[2025-02-13 20:11:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:22][root][INFO] - Training Epoch: 2/2, step 1965/7134 completed (loss: 0.12046699225902557, acc: 0.966292142868042)
[2025-02-13 20:11:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:22][root][INFO] - Training Epoch: 2/2, step 1966/7134 completed (loss: 0.1996966004371643, acc: 0.9617834687232971)
[2025-02-13 20:11:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:23][root][INFO] - Training Epoch: 2/2, step 1967/7134 completed (loss: 0.13310639560222626, acc: 0.9684210419654846)
[2025-02-13 20:11:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:23][root][INFO] - Training Epoch: 2/2, step 1968/7134 completed (loss: 0.08321298658847809, acc: 0.9680851101875305)
[2025-02-13 20:11:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:24][root][INFO] - Training Epoch: 2/2, step 1969/7134 completed (loss: 0.12010335177183151, acc: 0.9661017060279846)
[2025-02-13 20:11:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:24][root][INFO] - Training Epoch: 2/2, step 1970/7134 completed (loss: 0.10705890506505966, acc: 0.9822485446929932)
[2025-02-13 20:11:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:24][root][INFO] - Training Epoch: 2/2, step 1971/7134 completed (loss: 0.3678518235683441, acc: 0.9371727705001831)
[2025-02-13 20:11:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:25][root][INFO] - Training Epoch: 2/2, step 1972/7134 completed (loss: 0.38009920716285706, acc: 0.9210526347160339)
[2025-02-13 20:11:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:25][root][INFO] - Training Epoch: 2/2, step 1973/7134 completed (loss: 0.634811520576477, acc: 0.8680555820465088)
[2025-02-13 20:11:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:26][root][INFO] - Training Epoch: 2/2, step 1974/7134 completed (loss: 0.10539129376411438, acc: 0.9742268323898315)
[2025-02-13 20:11:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:26][root][INFO] - Training Epoch: 2/2, step 1975/7134 completed (loss: 0.03471320495009422, acc: 0.9937499761581421)
[2025-02-13 20:11:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:26][root][INFO] - Training Epoch: 2/2, step 1976/7134 completed (loss: 0.23460902273654938, acc: 0.9471153616905212)
[2025-02-13 20:11:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:27][root][INFO] - Training Epoch: 2/2, step 1977/7134 completed (loss: 0.08522709459066391, acc: 0.9880239367485046)
[2025-02-13 20:11:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:27][root][INFO] - Training Epoch: 2/2, step 1978/7134 completed (loss: 0.054117489606142044, acc: 0.9941860437393188)
[2025-02-13 20:11:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:28][root][INFO] - Training Epoch: 2/2, step 1979/7134 completed (loss: 0.0635540634393692, acc: 0.9890109896659851)
[2025-02-13 20:11:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:28][root][INFO] - Training Epoch: 2/2, step 1980/7134 completed (loss: 0.13552115857601166, acc: 0.9790576100349426)
[2025-02-13 20:11:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:28][root][INFO] - Training Epoch: 2/2, step 1981/7134 completed (loss: 0.12049245089292526, acc: 0.9780219793319702)
[2025-02-13 20:11:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:29][root][INFO] - Training Epoch: 2/2, step 1982/7134 completed (loss: 0.07739244401454926, acc: 0.9743589758872986)
[2025-02-13 20:11:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:29][root][INFO] - Training Epoch: 2/2, step 1983/7134 completed (loss: 0.08000915497541428, acc: 0.9892473220825195)
[2025-02-13 20:11:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:30][root][INFO] - Training Epoch: 2/2, step 1984/7134 completed (loss: 0.06817393749952316, acc: 0.9825581312179565)
[2025-02-13 20:11:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:30][root][INFO] - Training Epoch: 2/2, step 1985/7134 completed (loss: 0.1033758819103241, acc: 0.9689440727233887)
[2025-02-13 20:11:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:30][root][INFO] - Training Epoch: 2/2, step 1986/7134 completed (loss: 0.029089083895087242, acc: 0.9888268113136292)
[2025-02-13 20:11:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:31][root][INFO] - Training Epoch: 2/2, step 1987/7134 completed (loss: 0.12854400277137756, acc: 0.966292142868042)
[2025-02-13 20:11:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:31][root][INFO] - Training Epoch: 2/2, step 1988/7134 completed (loss: 0.11913959681987762, acc: 0.9757575988769531)
[2025-02-13 20:11:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:32][root][INFO] - Training Epoch: 2/2, step 1989/7134 completed (loss: 0.14408017694950104, acc: 0.9726027250289917)
[2025-02-13 20:11:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:32][root][INFO] - Training Epoch: 2/2, step 1990/7134 completed (loss: 0.19661279022693634, acc: 0.9723756909370422)
[2025-02-13 20:11:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:32][root][INFO] - Training Epoch: 2/2, step 1991/7134 completed (loss: 0.06330209225416183, acc: 0.9838709831237793)
[2025-02-13 20:11:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:33][root][INFO] - Training Epoch: 2/2, step 1992/7134 completed (loss: 0.22580119967460632, acc: 0.9347826242446899)
[2025-02-13 20:11:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:33][root][INFO] - Training Epoch: 2/2, step 1993/7134 completed (loss: 0.08885079622268677, acc: 0.9826086759567261)
[2025-02-13 20:11:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:34][root][INFO] - Training Epoch: 2/2, step 1994/7134 completed (loss: 0.05254039913415909, acc: 0.9922480583190918)
[2025-02-13 20:11:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:34][root][INFO] - Training Epoch: 2/2, step 1995/7134 completed (loss: 0.060857221484184265, acc: 0.9862068891525269)
[2025-02-13 20:11:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:34][root][INFO] - Training Epoch: 2/2, step 1996/7134 completed (loss: 0.09739359468221664, acc: 0.9635036587715149)
[2025-02-13 20:11:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:35][root][INFO] - Training Epoch: 2/2, step 1997/7134 completed (loss: 0.10041515529155731, acc: 0.9714285731315613)
[2025-02-13 20:11:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:35][root][INFO] - Training Epoch: 2/2, step 1998/7134 completed (loss: 0.12316384166479111, acc: 0.9647887349128723)
[2025-02-13 20:11:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:36][root][INFO] - Training Epoch: 2/2, step 1999/7134 completed (loss: 0.1202273890376091, acc: 0.9580419659614563)
[2025-02-13 20:11:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:36][root][INFO] - Training Epoch: 2/2, step 2000/7134 completed (loss: 0.08659341931343079, acc: 0.9861111044883728)
[2025-02-13 20:11:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:36][root][INFO] - Training Epoch: 2/2, step 2001/7134 completed (loss: 0.05784681439399719, acc: 0.9862068891525269)
[2025-02-13 20:11:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:37][root][INFO] - Training Epoch: 2/2, step 2002/7134 completed (loss: 0.10419730842113495, acc: 0.9849624037742615)
[2025-02-13 20:11:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:37][root][INFO] - Training Epoch: 2/2, step 2003/7134 completed (loss: 0.030966557562351227, acc: 0.9912280440330505)
[2025-02-13 20:11:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:38][root][INFO] - Training Epoch: 2/2, step 2004/7134 completed (loss: 0.48904484510421753, acc: 0.8934426307678223)
[2025-02-13 20:11:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:38][root][INFO] - Training Epoch: 2/2, step 2005/7134 completed (loss: 0.21458427608013153, acc: 0.918367326259613)
[2025-02-13 20:11:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:38][root][INFO] - Training Epoch: 2/2, step 2006/7134 completed (loss: 0.026513492688536644, acc: 0.9883720874786377)
[2025-02-13 20:11:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:39][root][INFO] - Training Epoch: 2/2, step 2007/7134 completed (loss: 0.06523669511079788, acc: 0.9747899174690247)
[2025-02-13 20:11:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:39][root][INFO] - Training Epoch: 2/2, step 2008/7134 completed (loss: 0.10139212012290955, acc: 0.9726027250289917)
[2025-02-13 20:11:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:40][root][INFO] - Training Epoch: 2/2, step 2009/7134 completed (loss: 0.15224161744117737, acc: 0.9629629850387573)
[2025-02-13 20:11:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:40][root][INFO] - Training Epoch: 2/2, step 2010/7134 completed (loss: 0.06902599334716797, acc: 0.9781022071838379)
[2025-02-13 20:11:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:40][root][INFO] - Training Epoch: 2/2, step 2011/7134 completed (loss: 0.05636036768555641, acc: 0.9841269850730896)
[2025-02-13 20:11:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:41][root][INFO] - Training Epoch: 2/2, step 2012/7134 completed (loss: 0.0826496034860611, acc: 0.9745762944221497)
[2025-02-13 20:11:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:41][root][INFO] - Training Epoch: 2/2, step 2013/7134 completed (loss: 0.0424795001745224, acc: 0.9918032884597778)
[2025-02-13 20:11:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:42][root][INFO] - Training Epoch: 2/2, step 2014/7134 completed (loss: 0.05336439982056618, acc: 0.9876543283462524)
[2025-02-13 20:11:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:42][root][INFO] - Training Epoch: 2/2, step 2015/7134 completed (loss: 0.03266448155045509, acc: 0.9937499761581421)
[2025-02-13 20:11:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:42][root][INFO] - Training Epoch: 2/2, step 2016/7134 completed (loss: 0.16580414772033691, acc: 0.9523809552192688)
[2025-02-13 20:11:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:43][root][INFO] - Training Epoch: 2/2, step 2017/7134 completed (loss: 0.20176851749420166, acc: 0.9536423683166504)
[2025-02-13 20:11:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:43][root][INFO] - Training Epoch: 2/2, step 2018/7134 completed (loss: 0.03445126861333847, acc: 1.0)
[2025-02-13 20:11:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:43][root][INFO] - Training Epoch: 2/2, step 2019/7134 completed (loss: 0.06054719164967537, acc: 0.9867549538612366)
[2025-02-13 20:11:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:44][root][INFO] - Training Epoch: 2/2, step 2020/7134 completed (loss: 0.11760411411523819, acc: 0.9586777091026306)
[2025-02-13 20:11:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:44][root][INFO] - Training Epoch: 2/2, step 2021/7134 completed (loss: 0.1326865702867508, acc: 0.9621621370315552)
[2025-02-13 20:11:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:45][root][INFO] - Training Epoch: 2/2, step 2022/7134 completed (loss: 0.17707288265228271, acc: 0.9552238583564758)
[2025-02-13 20:11:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:45][root][INFO] - Training Epoch: 2/2, step 2023/7134 completed (loss: 0.10721731185913086, acc: 0.9693251252174377)
[2025-02-13 20:11:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:46][root][INFO] - Training Epoch: 2/2, step 2024/7134 completed (loss: 0.08415775001049042, acc: 0.9767441749572754)
[2025-02-13 20:11:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:46][root][INFO] - Training Epoch: 2/2, step 2025/7134 completed (loss: 0.19049592316150665, acc: 0.9766082167625427)
[2025-02-13 20:11:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:46][root][INFO] - Training Epoch: 2/2, step 2026/7134 completed (loss: 0.07473216205835342, acc: 0.9824561476707458)
[2025-02-13 20:11:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:47][root][INFO] - Training Epoch: 2/2, step 2027/7134 completed (loss: 0.07562894374132156, acc: 0.9849624037742615)
[2025-02-13 20:11:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:47][root][INFO] - Training Epoch: 2/2, step 2028/7134 completed (loss: 0.10822068154811859, acc: 0.9693251252174377)
[2025-02-13 20:11:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:48][root][INFO] - Training Epoch: 2/2, step 2029/7134 completed (loss: 0.139298215508461, acc: 0.9588235020637512)
[2025-02-13 20:11:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:48][root][INFO] - Training Epoch: 2/2, step 2030/7134 completed (loss: 0.14264893531799316, acc: 0.9518072009086609)
[2025-02-13 20:11:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:48][root][INFO] - Training Epoch: 2/2, step 2031/7134 completed (loss: 0.07008535414934158, acc: 0.9834254384040833)
[2025-02-13 20:11:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:49][root][INFO] - Training Epoch: 2/2, step 2032/7134 completed (loss: 0.24050383269786835, acc: 0.9415204524993896)
[2025-02-13 20:11:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:49][root][INFO] - Training Epoch: 2/2, step 2033/7134 completed (loss: 0.0352429561316967, acc: 0.9923664331436157)
[2025-02-13 20:11:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:50][root][INFO] - Training Epoch: 2/2, step 2034/7134 completed (loss: 0.24472109973430634, acc: 0.9328858852386475)
[2025-02-13 20:11:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:50][root][INFO] - Training Epoch: 2/2, step 2035/7134 completed (loss: 0.05591903626918793, acc: 0.9946236610412598)
[2025-02-13 20:11:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:51][root][INFO] - Training Epoch: 2/2, step 2036/7134 completed (loss: 0.05364974960684776, acc: 0.9923664331436157)
[2025-02-13 20:11:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:51][root][INFO] - Training Epoch: 2/2, step 2037/7134 completed (loss: 0.07704632729291916, acc: 0.9815950989723206)
[2025-02-13 20:11:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:51][root][INFO] - Training Epoch: 2/2, step 2038/7134 completed (loss: 0.050714101642370224, acc: 0.9780219793319702)
[2025-02-13 20:11:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:52][root][INFO] - Training Epoch: 2/2, step 2039/7134 completed (loss: 0.04513685032725334, acc: 0.9927007555961609)
[2025-02-13 20:11:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:52][root][INFO] - Training Epoch: 2/2, step 2040/7134 completed (loss: 0.18915243446826935, acc: 0.9741379022598267)
[2025-02-13 20:11:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:53][root][INFO] - Training Epoch: 2/2, step 2041/7134 completed (loss: 0.07083368301391602, acc: 0.9878787994384766)
[2025-02-13 20:11:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:53][root][INFO] - Training Epoch: 2/2, step 2042/7134 completed (loss: 0.06194177642464638, acc: 0.9830508232116699)
[2025-02-13 20:11:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:54][root][INFO] - Training Epoch: 2/2, step 2043/7134 completed (loss: 0.050392262637615204, acc: 0.9932885766029358)
[2025-02-13 20:11:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:54][root][INFO] - Training Epoch: 2/2, step 2044/7134 completed (loss: 0.06380156427621841, acc: 0.9919999837875366)
[2025-02-13 20:11:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:54][root][INFO] - Training Epoch: 2/2, step 2045/7134 completed (loss: 0.10154637694358826, acc: 0.9750000238418579)
[2025-02-13 20:11:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:55][root][INFO] - Training Epoch: 2/2, step 2046/7134 completed (loss: 0.20473521947860718, acc: 0.9579831957817078)
[2025-02-13 20:11:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:55][root][INFO] - Training Epoch: 2/2, step 2047/7134 completed (loss: 0.2171102911233902, acc: 0.931034505367279)
[2025-02-13 20:11:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:56][root][INFO] - Training Epoch: 2/2, step 2048/7134 completed (loss: 0.2659551203250885, acc: 0.9464285969734192)
[2025-02-13 20:11:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:56][root][INFO] - Training Epoch: 2/2, step 2049/7134 completed (loss: 0.3224160075187683, acc: 0.9363636374473572)
[2025-02-13 20:11:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:56][root][INFO] - Training Epoch: 2/2, step 2050/7134 completed (loss: 0.1883547008037567, acc: 0.9520547986030579)
[2025-02-13 20:11:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:57][root][INFO] - Training Epoch: 2/2, step 2051/7134 completed (loss: 0.13029712438583374, acc: 0.9557521939277649)
[2025-02-13 20:11:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:57][root][INFO] - Training Epoch: 2/2, step 2052/7134 completed (loss: 0.18754927814006805, acc: 0.9513888955116272)
[2025-02-13 20:11:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:58][root][INFO] - Training Epoch: 2/2, step 2053/7134 completed (loss: 0.3219933807849884, acc: 0.9465649127960205)
[2025-02-13 20:11:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:58][root][INFO] - Training Epoch: 2/2, step 2054/7134 completed (loss: 0.29217737913131714, acc: 0.9294871687889099)
[2025-02-13 20:11:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:58][root][INFO] - Training Epoch: 2/2, step 2055/7134 completed (loss: 0.1607159823179245, acc: 0.965753436088562)
[2025-02-13 20:11:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:59][root][INFO] - Training Epoch: 2/2, step 2056/7134 completed (loss: 0.2558368146419525, acc: 0.9322034120559692)
[2025-02-13 20:11:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:11:59][root][INFO] - Training Epoch: 2/2, step 2057/7134 completed (loss: 0.15755009651184082, acc: 0.9606741666793823)
[2025-02-13 20:11:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:00][root][INFO] - Training Epoch: 2/2, step 2058/7134 completed (loss: 0.3047740161418915, acc: 0.9421965479850769)
[2025-02-13 20:12:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:00][root][INFO] - Training Epoch: 2/2, step 2059/7134 completed (loss: 0.31652045249938965, acc: 0.9219858050346375)
[2025-02-13 20:12:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:00][root][INFO] - Training Epoch: 2/2, step 2060/7134 completed (loss: 0.15598490834236145, acc: 0.9490445852279663)
[2025-02-13 20:12:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:01][root][INFO] - Training Epoch: 2/2, step 2061/7134 completed (loss: 0.24102015793323517, acc: 0.9523809552192688)
[2025-02-13 20:12:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:01][root][INFO] - Training Epoch: 2/2, step 2062/7134 completed (loss: 0.2470598667860031, acc: 0.9453125)
[2025-02-13 20:12:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:02][root][INFO] - Training Epoch: 2/2, step 2063/7134 completed (loss: 0.107747882604599, acc: 0.9645389914512634)
[2025-02-13 20:12:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:02][root][INFO] - Training Epoch: 2/2, step 2064/7134 completed (loss: 0.127996563911438, acc: 0.9779411554336548)
[2025-02-13 20:12:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:02][root][INFO] - Training Epoch: 2/2, step 2065/7134 completed (loss: 0.1530732810497284, acc: 0.9785714149475098)
[2025-02-13 20:12:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:03][root][INFO] - Training Epoch: 2/2, step 2066/7134 completed (loss: 0.09899099171161652, acc: 0.9837398529052734)
[2025-02-13 20:12:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:03][root][INFO] - Training Epoch: 2/2, step 2067/7134 completed (loss: 0.1572355329990387, acc: 0.9583333134651184)
[2025-02-13 20:12:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:04][root][INFO] - Training Epoch: 2/2, step 2068/7134 completed (loss: 0.10016115009784698, acc: 0.9509803652763367)
[2025-02-13 20:12:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:04][root][INFO] - Training Epoch: 2/2, step 2069/7134 completed (loss: 0.08838336169719696, acc: 0.9865771532058716)
[2025-02-13 20:12:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:04][root][INFO] - Training Epoch: 2/2, step 2070/7134 completed (loss: 0.036110054701566696, acc: 1.0)
[2025-02-13 20:12:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:05][root][INFO] - Training Epoch: 2/2, step 2071/7134 completed (loss: 0.020853359252214432, acc: 1.0)
[2025-02-13 20:12:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:05][root][INFO] - Training Epoch: 2/2, step 2072/7134 completed (loss: 0.06460992991924286, acc: 0.970588207244873)
[2025-02-13 20:12:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:06][root][INFO] - Training Epoch: 2/2, step 2073/7134 completed (loss: 0.08287880569696426, acc: 0.9752066135406494)
[2025-02-13 20:12:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:06][root][INFO] - Training Epoch: 2/2, step 2074/7134 completed (loss: 0.14103730022907257, acc: 0.9738219976425171)
[2025-02-13 20:12:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:07][root][INFO] - Training Epoch: 2/2, step 2075/7134 completed (loss: 0.0926176905632019, acc: 0.9685534834861755)
[2025-02-13 20:12:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:07][root][INFO] - Training Epoch: 2/2, step 2076/7134 completed (loss: 0.11130159348249435, acc: 0.9795918464660645)
[2025-02-13 20:12:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:07][root][INFO] - Training Epoch: 2/2, step 2077/7134 completed (loss: 0.12863750755786896, acc: 0.9661017060279846)
[2025-02-13 20:12:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:08][root][INFO] - Training Epoch: 2/2, step 2078/7134 completed (loss: 0.09492088109254837, acc: 0.9796954393386841)
[2025-02-13 20:12:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:08][root][INFO] - Training Epoch: 2/2, step 2079/7134 completed (loss: 0.11750941723585129, acc: 0.9578313231468201)
[2025-02-13 20:12:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:09][root][INFO] - Training Epoch: 2/2, step 2080/7134 completed (loss: 0.09908131510019302, acc: 0.9738562107086182)
[2025-02-13 20:12:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:09][root][INFO] - Training Epoch: 2/2, step 2081/7134 completed (loss: 0.08432050794363022, acc: 0.9745222926139832)
[2025-02-13 20:12:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:10][root][INFO] - Training Epoch: 2/2, step 2082/7134 completed (loss: 0.11248227208852768, acc: 0.9661017060279846)
[2025-02-13 20:12:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:10][root][INFO] - Training Epoch: 2/2, step 2083/7134 completed (loss: 0.10590793192386627, acc: 0.9767441749572754)
[2025-02-13 20:12:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:10][root][INFO] - Training Epoch: 2/2, step 2084/7134 completed (loss: 0.10339450091123581, acc: 0.9666666388511658)
[2025-02-13 20:12:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:11][root][INFO] - Training Epoch: 2/2, step 2085/7134 completed (loss: 0.01652584783732891, acc: 1.0)
[2025-02-13 20:12:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:11][root][INFO] - Training Epoch: 2/2, step 2086/7134 completed (loss: 0.03029564395546913, acc: 1.0)
[2025-02-13 20:12:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:11][root][INFO] - Training Epoch: 2/2, step 2087/7134 completed (loss: 0.019759073853492737, acc: 1.0)
[2025-02-13 20:12:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:12][root][INFO] - Training Epoch: 2/2, step 2088/7134 completed (loss: 0.18662875890731812, acc: 0.9644970297813416)
[2025-02-13 20:12:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:12][root][INFO] - Training Epoch: 2/2, step 2089/7134 completed (loss: 0.11475978791713715, acc: 0.9702380895614624)
[2025-02-13 20:12:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:13][root][INFO] - Training Epoch: 2/2, step 2090/7134 completed (loss: 0.035867173224687576, acc: 1.0)
[2025-02-13 20:12:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:13][root][INFO] - Training Epoch: 2/2, step 2091/7134 completed (loss: 0.02914181724190712, acc: 0.9931034445762634)
[2025-02-13 20:12:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:14][root][INFO] - Training Epoch: 2/2, step 2092/7134 completed (loss: 0.050272829830646515, acc: 0.9867549538612366)
[2025-02-13 20:12:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:14][root][INFO] - Training Epoch: 2/2, step 2093/7134 completed (loss: 0.056241411715745926, acc: 0.9894737005233765)
[2025-02-13 20:12:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:14][root][INFO] - Training Epoch: 2/2, step 2094/7134 completed (loss: 0.12408187985420227, acc: 0.9552238583564758)
[2025-02-13 20:12:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:15][root][INFO] - Training Epoch: 2/2, step 2095/7134 completed (loss: 0.08770398050546646, acc: 0.9800994992256165)
[2025-02-13 20:12:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:15][root][INFO] - Training Epoch: 2/2, step 2096/7134 completed (loss: 0.06421486288309097, acc: 0.9860140085220337)
[2025-02-13 20:12:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:16][root][INFO] - Training Epoch: 2/2, step 2097/7134 completed (loss: 0.04368515685200691, acc: 0.9939758777618408)
[2025-02-13 20:12:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:16][root][INFO] - Training Epoch: 2/2, step 2098/7134 completed (loss: 0.07023786008358002, acc: 0.9671052694320679)
[2025-02-13 20:12:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:16][root][INFO] - Training Epoch: 2/2, step 2099/7134 completed (loss: 0.050663478672504425, acc: 0.984375)
[2025-02-13 20:12:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:17][root][INFO] - Training Epoch: 2/2, step 2100/7134 completed (loss: 0.1092468798160553, acc: 0.9657142758369446)
[2025-02-13 20:12:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:17][root][INFO] - Training Epoch: 2/2, step 2101/7134 completed (loss: 0.06355901807546616, acc: 0.9821428656578064)
[2025-02-13 20:12:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:18][root][INFO] - Training Epoch: 2/2, step 2102/7134 completed (loss: 0.05306970328092575, acc: 0.9837837815284729)
[2025-02-13 20:12:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:18][root][INFO] - Training Epoch: 2/2, step 2103/7134 completed (loss: 0.0793914794921875, acc: 0.981249988079071)
[2025-02-13 20:12:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:18][root][INFO] - Training Epoch: 2/2, step 2104/7134 completed (loss: 0.029780300334095955, acc: 1.0)
[2025-02-13 20:12:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:19][root][INFO] - Training Epoch: 2/2, step 2105/7134 completed (loss: 0.06205637753009796, acc: 0.9929078221321106)
[2025-02-13 20:12:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:19][root][INFO] - Training Epoch: 2/2, step 2106/7134 completed (loss: 0.08825832605361938, acc: 0.9671052694320679)
[2025-02-13 20:12:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:20][root][INFO] - Training Epoch: 2/2, step 2107/7134 completed (loss: 0.06679193675518036, acc: 0.9779005646705627)
[2025-02-13 20:12:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:20][root][INFO] - Training Epoch: 2/2, step 2108/7134 completed (loss: 0.05505915358662605, acc: 0.9937888383865356)
[2025-02-13 20:12:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:20][root][INFO] - Training Epoch: 2/2, step 2109/7134 completed (loss: 0.1296825408935547, acc: 0.9710144996643066)
[2025-02-13 20:12:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:21][root][INFO] - Training Epoch: 2/2, step 2110/7134 completed (loss: 0.12334941327571869, acc: 0.9580838084220886)
[2025-02-13 20:12:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:21][root][INFO] - Training Epoch: 2/2, step 2111/7134 completed (loss: 0.23784588277339935, acc: 0.9481865167617798)
[2025-02-13 20:12:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:22][root][INFO] - Training Epoch: 2/2, step 2112/7134 completed (loss: 0.09775074571371078, acc: 0.9666666388511658)
[2025-02-13 20:12:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:22][root][INFO] - Training Epoch: 2/2, step 2113/7134 completed (loss: 0.060523536056280136, acc: 0.9866666793823242)
[2025-02-13 20:12:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:22][root][INFO] - Training Epoch: 2/2, step 2114/7134 completed (loss: 0.16213169693946838, acc: 0.9704142212867737)
[2025-02-13 20:12:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:23][root][INFO] - Training Epoch: 2/2, step 2115/7134 completed (loss: 0.12347118556499481, acc: 0.9655172228813171)
[2025-02-13 20:12:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:23][root][INFO] - Training Epoch: 2/2, step 2116/7134 completed (loss: 0.07239094376564026, acc: 0.9863945841789246)
[2025-02-13 20:12:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:24][root][INFO] - Training Epoch: 2/2, step 2117/7134 completed (loss: 0.09977686405181885, acc: 0.9717513918876648)
[2025-02-13 20:12:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:24][root][INFO] - Training Epoch: 2/2, step 2118/7134 completed (loss: 0.08036106824874878, acc: 0.976331353187561)
[2025-02-13 20:12:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:25][root][INFO] - Training Epoch: 2/2, step 2119/7134 completed (loss: 0.07970607280731201, acc: 0.9768785834312439)
[2025-02-13 20:12:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:25][root][INFO] - Training Epoch: 2/2, step 2120/7134 completed (loss: 0.10716951638460159, acc: 0.9814814925193787)
[2025-02-13 20:12:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:25][root][INFO] - Training Epoch: 2/2, step 2121/7134 completed (loss: 0.19069331884384155, acc: 0.9530201554298401)
[2025-02-13 20:12:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:26][root][INFO] - Training Epoch: 2/2, step 2122/7134 completed (loss: 0.11412261426448822, acc: 0.9661017060279846)
[2025-02-13 20:12:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:26][root][INFO] - Training Epoch: 2/2, step 2123/7134 completed (loss: 0.3109257221221924, acc: 0.9342105388641357)
[2025-02-13 20:12:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:27][root][INFO] - Training Epoch: 2/2, step 2124/7134 completed (loss: 0.09472446143627167, acc: 0.9662162065505981)
[2025-02-13 20:12:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:27][root][INFO] - Training Epoch: 2/2, step 2125/7134 completed (loss: 0.04557347297668457, acc: 0.9880239367485046)
[2025-02-13 20:12:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:28][root][INFO] - Training Epoch: 2/2, step 2126/7134 completed (loss: 0.10652598738670349, acc: 0.9645389914512634)
[2025-02-13 20:12:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:28][root][INFO] - Training Epoch: 2/2, step 2127/7134 completed (loss: 0.04376935958862305, acc: 1.0)
[2025-02-13 20:12:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:28][root][INFO] - Training Epoch: 2/2, step 2128/7134 completed (loss: 0.08554999530315399, acc: 0.9777777791023254)
[2025-02-13 20:12:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:29][root][INFO] - Training Epoch: 2/2, step 2129/7134 completed (loss: 0.15671104192733765, acc: 0.9644970297813416)
[2025-02-13 20:12:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:29][root][INFO] - Training Epoch: 2/2, step 2130/7134 completed (loss: 0.11316652595996857, acc: 0.9662162065505981)
[2025-02-13 20:12:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:30][root][INFO] - Training Epoch: 2/2, step 2131/7134 completed (loss: 0.05311409384012222, acc: 0.987730085849762)
[2025-02-13 20:12:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:30][root][INFO] - Training Epoch: 2/2, step 2132/7134 completed (loss: 0.036751966923475266, acc: 0.9878048896789551)
[2025-02-13 20:12:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:30][root][INFO] - Training Epoch: 2/2, step 2133/7134 completed (loss: 0.1065671369433403, acc: 0.9707602262496948)
[2025-02-13 20:12:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:31][root][INFO] - Training Epoch: 2/2, step 2134/7134 completed (loss: 0.06248675286769867, acc: 0.982758641242981)
[2025-02-13 20:12:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:31][root][INFO] - Training Epoch: 2/2, step 2135/7134 completed (loss: 0.04388388618826866, acc: 0.9933775067329407)
[2025-02-13 20:12:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:32][root][INFO] - Training Epoch: 2/2, step 2136/7134 completed (loss: 0.07890147715806961, acc: 0.9835164546966553)
[2025-02-13 20:12:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:32][root][INFO] - Training Epoch: 2/2, step 2137/7134 completed (loss: 0.09192457795143127, acc: 0.978723406791687)
[2025-02-13 20:12:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:32][root][INFO] - Training Epoch: 2/2, step 2138/7134 completed (loss: 0.03660622984170914, acc: 0.9940119981765747)
[2025-02-13 20:12:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:33][root][INFO] - Training Epoch: 2/2, step 2139/7134 completed (loss: 0.03612156957387924, acc: 0.9825581312179565)
[2025-02-13 20:12:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:33][root][INFO] - Training Epoch: 2/2, step 2140/7134 completed (loss: 0.1971389204263687, acc: 0.9491525292396545)
[2025-02-13 20:12:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:34][root][INFO] - Training Epoch: 2/2, step 2141/7134 completed (loss: 0.0841711163520813, acc: 0.9893617033958435)
[2025-02-13 20:12:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:34][root][INFO] - Training Epoch: 2/2, step 2142/7134 completed (loss: 0.12248925119638443, acc: 0.9774011373519897)
[2025-02-13 20:12:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:34][root][INFO] - Training Epoch: 2/2, step 2143/7134 completed (loss: 0.037384994328022, acc: 0.9931034445762634)
[2025-02-13 20:12:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:35][root][INFO] - Training Epoch: 2/2, step 2144/7134 completed (loss: 0.0620197057723999, acc: 0.9886363744735718)
[2025-02-13 20:12:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:35][root][INFO] - Training Epoch: 2/2, step 2145/7134 completed (loss: 0.08170469850301743, acc: 0.9883720874786377)
[2025-02-13 20:12:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:36][root][INFO] - Training Epoch: 2/2, step 2146/7134 completed (loss: 0.052281513810157776, acc: 0.9864864945411682)
[2025-02-13 20:12:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:36][root][INFO] - Training Epoch: 2/2, step 2147/7134 completed (loss: 0.13507325947284698, acc: 0.9595959782600403)
[2025-02-13 20:12:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:36][root][INFO] - Training Epoch: 2/2, step 2148/7134 completed (loss: 0.06482920795679092, acc: 0.9829545617103577)
[2025-02-13 20:12:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:37][root][INFO] - Training Epoch: 2/2, step 2149/7134 completed (loss: 0.03559769690036774, acc: 1.0)
[2025-02-13 20:12:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:37][root][INFO] - Training Epoch: 2/2, step 2150/7134 completed (loss: 0.04744532331824303, acc: 0.9887005686759949)
[2025-02-13 20:12:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:38][root][INFO] - Training Epoch: 2/2, step 2151/7134 completed (loss: 0.12954619526863098, acc: 0.9638554453849792)
[2025-02-13 20:12:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:38][root][INFO] - Training Epoch: 2/2, step 2152/7134 completed (loss: 0.12617424130439758, acc: 0.9741935729980469)
[2025-02-13 20:12:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:39][root][INFO] - Training Epoch: 2/2, step 2153/7134 completed (loss: 0.11070963740348816, acc: 0.9803921580314636)
[2025-02-13 20:12:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:39][root][INFO] - Training Epoch: 2/2, step 2154/7134 completed (loss: 0.13609647750854492, acc: 0.9885057210922241)
[2025-02-13 20:12:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:39][root][INFO] - Training Epoch: 2/2, step 2155/7134 completed (loss: 0.13678431510925293, acc: 0.9743589758872986)
[2025-02-13 20:12:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:40][root][INFO] - Training Epoch: 2/2, step 2156/7134 completed (loss: 0.06019586697220802, acc: 0.9779411554336548)
[2025-02-13 20:12:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:40][root][INFO] - Training Epoch: 2/2, step 2157/7134 completed (loss: 0.09086710214614868, acc: 0.9747474789619446)
[2025-02-13 20:12:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:41][root][INFO] - Training Epoch: 2/2, step 2158/7134 completed (loss: 0.11917325854301453, acc: 0.969072163105011)
[2025-02-13 20:12:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:41][root][INFO] - Training Epoch: 2/2, step 2159/7134 completed (loss: 0.06600241363048553, acc: 0.976190447807312)
[2025-02-13 20:12:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:41][root][INFO] - Training Epoch: 2/2, step 2160/7134 completed (loss: 0.06516345590353012, acc: 0.9756097793579102)
[2025-02-13 20:12:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:42][root][INFO] - Training Epoch: 2/2, step 2161/7134 completed (loss: 0.08925364911556244, acc: 0.9888888597488403)
[2025-02-13 20:12:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:42][root][INFO] - Training Epoch: 2/2, step 2162/7134 completed (loss: 0.09122762829065323, acc: 0.9741935729980469)
[2025-02-13 20:12:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:43][root][INFO] - Training Epoch: 2/2, step 2163/7134 completed (loss: 0.057507991790771484, acc: 0.9800000190734863)
[2025-02-13 20:12:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:43][root][INFO] - Training Epoch: 2/2, step 2164/7134 completed (loss: 0.03712475672364235, acc: 1.0)
[2025-02-13 20:12:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:44][root][INFO] - Training Epoch: 2/2, step 2165/7134 completed (loss: 0.0835997685790062, acc: 0.9817073345184326)
[2025-02-13 20:12:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:44][root][INFO] - Training Epoch: 2/2, step 2166/7134 completed (loss: 0.0343683660030365, acc: 0.994350254535675)
[2025-02-13 20:12:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:44][root][INFO] - Training Epoch: 2/2, step 2167/7134 completed (loss: 0.14495499432086945, acc: 0.9708737730979919)
[2025-02-13 20:12:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:45][root][INFO] - Training Epoch: 2/2, step 2168/7134 completed (loss: 0.029517119750380516, acc: 0.9928057789802551)
[2025-02-13 20:12:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:45][root][INFO] - Training Epoch: 2/2, step 2169/7134 completed (loss: 0.12082452327013016, acc: 0.979899525642395)
[2025-02-13 20:12:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:45][root][INFO] - Training Epoch: 2/2, step 2170/7134 completed (loss: 0.04969429969787598, acc: 0.9869281053543091)
[2025-02-13 20:12:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:46][root][INFO] - Training Epoch: 2/2, step 2171/7134 completed (loss: 0.21801836788654327, acc: 0.9567567706108093)
[2025-02-13 20:12:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:46][root][INFO] - Training Epoch: 2/2, step 2172/7134 completed (loss: 0.062175873667001724, acc: 0.9952606558799744)
[2025-02-13 20:12:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:47][root][INFO] - Training Epoch: 2/2, step 2173/7134 completed (loss: 0.10737629234790802, acc: 0.9863013625144958)
[2025-02-13 20:12:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:47][root][INFO] - Training Epoch: 2/2, step 2174/7134 completed (loss: 0.05880327150225639, acc: 0.9940119981765747)
[2025-02-13 20:12:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:47][root][INFO] - Training Epoch: 2/2, step 2175/7134 completed (loss: 0.06558072566986084, acc: 0.9757575988769531)
[2025-02-13 20:12:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:48][root][INFO] - Training Epoch: 2/2, step 2176/7134 completed (loss: 0.08161959052085876, acc: 0.97826087474823)
[2025-02-13 20:12:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:48][root][INFO] - Training Epoch: 2/2, step 2177/7134 completed (loss: 0.09894423186779022, acc: 0.9781420826911926)
[2025-02-13 20:12:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:49][root][INFO] - Training Epoch: 2/2, step 2178/7134 completed (loss: 0.029575960710644722, acc: 0.9947916865348816)
[2025-02-13 20:12:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:49][root][INFO] - Training Epoch: 2/2, step 2179/7134 completed (loss: 0.07682574540376663, acc: 0.982758641242981)
[2025-02-13 20:12:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:50][root][INFO] - Training Epoch: 2/2, step 2180/7134 completed (loss: 0.12719479203224182, acc: 0.9780219793319702)
[2025-02-13 20:12:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:50][root][INFO] - Training Epoch: 2/2, step 2181/7134 completed (loss: 0.06780318915843964, acc: 0.9813664555549622)
[2025-02-13 20:12:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:50][root][INFO] - Training Epoch: 2/2, step 2182/7134 completed (loss: 0.17485520243644714, acc: 0.9538461565971375)
[2025-02-13 20:12:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:51][root][INFO] - Training Epoch: 2/2, step 2183/7134 completed (loss: 0.09649685770273209, acc: 0.9944751262664795)
[2025-02-13 20:12:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:51][root][INFO] - Training Epoch: 2/2, step 2184/7134 completed (loss: 0.06807807832956314, acc: 0.9718309640884399)
[2025-02-13 20:12:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:52][root][INFO] - Training Epoch: 2/2, step 2185/7134 completed (loss: 0.1114584356546402, acc: 0.9748427867889404)
[2025-02-13 20:12:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:52][root][INFO] - Training Epoch: 2/2, step 2186/7134 completed (loss: 0.2155359387397766, acc: 0.9466666579246521)
[2025-02-13 20:12:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:52][root][INFO] - Training Epoch: 2/2, step 2187/7134 completed (loss: 0.3012460470199585, acc: 0.935251772403717)
[2025-02-13 20:12:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:53][root][INFO] - Training Epoch: 2/2, step 2188/7134 completed (loss: 0.04942281171679497, acc: 0.9939393997192383)
[2025-02-13 20:12:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:53][root][INFO] - Training Epoch: 2/2, step 2189/7134 completed (loss: 0.10516036301851273, acc: 0.9740259647369385)
[2025-02-13 20:12:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:54][root][INFO] - Training Epoch: 2/2, step 2190/7134 completed (loss: 0.12831760942935944, acc: 0.9720670580863953)
[2025-02-13 20:12:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:54][root][INFO] - Training Epoch: 2/2, step 2191/7134 completed (loss: 0.06921490281820297, acc: 0.9789473414421082)
[2025-02-13 20:12:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:54][root][INFO] - Training Epoch: 2/2, step 2192/7134 completed (loss: 0.029820315539836884, acc: 0.9925925731658936)
[2025-02-13 20:12:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:55][root][INFO] - Training Epoch: 2/2, step 2193/7134 completed (loss: 0.10630636662244797, acc: 0.9672130942344666)
[2025-02-13 20:12:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:55][root][INFO] - Training Epoch: 2/2, step 2194/7134 completed (loss: 0.12370938807725906, acc: 0.9671052694320679)
[2025-02-13 20:12:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:56][root][INFO] - Training Epoch: 2/2, step 2195/7134 completed (loss: 0.04037480801343918, acc: 0.9876543283462524)
[2025-02-13 20:12:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:56][root][INFO] - Training Epoch: 2/2, step 2196/7134 completed (loss: 0.041019417345523834, acc: 0.9835164546966553)
[2025-02-13 20:12:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:56][root][INFO] - Training Epoch: 2/2, step 2197/7134 completed (loss: 0.11439996212720871, acc: 0.9763779640197754)
[2025-02-13 20:12:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:57][root][INFO] - Training Epoch: 2/2, step 2198/7134 completed (loss: 0.060419946908950806, acc: 0.9885057210922241)
[2025-02-13 20:12:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:57][root][INFO] - Training Epoch: 2/2, step 2199/7134 completed (loss: 0.020895862951874733, acc: 0.9936708807945251)
[2025-02-13 20:12:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:57][root][INFO] - Training Epoch: 2/2, step 2200/7134 completed (loss: 0.05304879695177078, acc: 0.9842519760131836)
[2025-02-13 20:12:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:58][root][INFO] - Training Epoch: 2/2, step 2201/7134 completed (loss: 0.06789515167474747, acc: 0.9932432174682617)
[2025-02-13 20:12:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:58][root][INFO] - Training Epoch: 2/2, step 2202/7134 completed (loss: 0.05882519483566284, acc: 0.985401451587677)
[2025-02-13 20:12:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:59][root][INFO] - Training Epoch: 2/2, step 2203/7134 completed (loss: 0.04015817865729332, acc: 0.9940119981765747)
[2025-02-13 20:12:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:59][root][INFO] - Training Epoch: 2/2, step 2204/7134 completed (loss: 0.03413950279355049, acc: 0.9885057210922241)
[2025-02-13 20:12:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:12:59][root][INFO] - Training Epoch: 2/2, step 2205/7134 completed (loss: 0.017545277252793312, acc: 1.0)
[2025-02-13 20:13:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:00][root][INFO] - Training Epoch: 2/2, step 2206/7134 completed (loss: 0.06811564415693283, acc: 0.9932885766029358)
[2025-02-13 20:13:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:00][root][INFO] - Training Epoch: 2/2, step 2207/7134 completed (loss: 0.03694264963269234, acc: 0.9863945841789246)
[2025-02-13 20:13:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:01][root][INFO] - Training Epoch: 2/2, step 2208/7134 completed (loss: 0.07317406684160233, acc: 0.9870967864990234)
[2025-02-13 20:13:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:01][root][INFO] - Training Epoch: 2/2, step 2209/7134 completed (loss: 0.043227262794971466, acc: 0.9878048896789551)
[2025-02-13 20:13:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:01][root][INFO] - Training Epoch: 2/2, step 2210/7134 completed (loss: 0.07288173586130142, acc: 0.9817073345184326)
[2025-02-13 20:13:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:02][root][INFO] - Training Epoch: 2/2, step 2211/7134 completed (loss: 0.025684500113129616, acc: 1.0)
[2025-02-13 20:13:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:02][root][INFO] - Training Epoch: 2/2, step 2212/7134 completed (loss: 0.03387720137834549, acc: 0.977142870426178)
[2025-02-13 20:13:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:03][root][INFO] - Training Epoch: 2/2, step 2213/7134 completed (loss: 0.05280512571334839, acc: 0.9823529124259949)
[2025-02-13 20:13:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:03][root][INFO] - Training Epoch: 2/2, step 2214/7134 completed (loss: 0.09177651256322861, acc: 0.9797297120094299)
[2025-02-13 20:13:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:03][root][INFO] - Training Epoch: 2/2, step 2215/7134 completed (loss: 0.032269399613142014, acc: 1.0)
[2025-02-13 20:13:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:04][root][INFO] - Training Epoch: 2/2, step 2216/7134 completed (loss: 0.0393030121922493, acc: 0.988304078578949)
[2025-02-13 20:13:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:04][root][INFO] - Training Epoch: 2/2, step 2217/7134 completed (loss: 0.03161736577749252, acc: 0.9880239367485046)
[2025-02-13 20:13:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:04][root][INFO] - Training Epoch: 2/2, step 2218/7134 completed (loss: 0.08397123217582703, acc: 0.9691358208656311)
[2025-02-13 20:13:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:05][root][INFO] - Training Epoch: 2/2, step 2219/7134 completed (loss: 0.05395446717739105, acc: 0.9857142567634583)
[2025-02-13 20:13:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:05][root][INFO] - Training Epoch: 2/2, step 2220/7134 completed (loss: 0.05063696205615997, acc: 0.9925925731658936)
[2025-02-13 20:13:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:06][root][INFO] - Training Epoch: 2/2, step 2221/7134 completed (loss: 0.07768826186656952, acc: 0.9875776171684265)
[2025-02-13 20:13:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:06][root][INFO] - Training Epoch: 2/2, step 2222/7134 completed (loss: 0.12270428240299225, acc: 0.965753436088562)
[2025-02-13 20:13:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:06][root][INFO] - Training Epoch: 2/2, step 2223/7134 completed (loss: 0.07924071699380875, acc: 0.9798657894134521)
[2025-02-13 20:13:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:07][root][INFO] - Training Epoch: 2/2, step 2224/7134 completed (loss: 0.14956338703632355, acc: 0.9556962251663208)
[2025-02-13 20:13:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:07][root][INFO] - Training Epoch: 2/2, step 2225/7134 completed (loss: 0.052696239203214645, acc: 0.9863945841789246)
[2025-02-13 20:13:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:07][root][INFO] - Training Epoch: 2/2, step 2226/7134 completed (loss: 0.04943050816655159, acc: 0.9865771532058716)
[2025-02-13 20:13:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:08][root][INFO] - Training Epoch: 2/2, step 2227/7134 completed (loss: 0.0493825301527977, acc: 0.9805194735527039)
[2025-02-13 20:13:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:08][root][INFO] - Training Epoch: 2/2, step 2228/7134 completed (loss: 0.07333339005708694, acc: 0.9767441749572754)
[2025-02-13 20:13:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:09][root][INFO] - Training Epoch: 2/2, step 2229/7134 completed (loss: 0.07741004973649979, acc: 0.9902912378311157)
[2025-02-13 20:13:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:09][root][INFO] - Training Epoch: 2/2, step 2230/7134 completed (loss: 0.018423832952976227, acc: 1.0)
[2025-02-13 20:13:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:09][root][INFO] - Training Epoch: 2/2, step 2231/7134 completed (loss: 0.10763203352689743, acc: 0.9663865566253662)
[2025-02-13 20:13:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:10][root][INFO] - Training Epoch: 2/2, step 2232/7134 completed (loss: 0.07708672434091568, acc: 0.9655172228813171)
[2025-02-13 20:13:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:10][root][INFO] - Training Epoch: 2/2, step 2233/7134 completed (loss: 0.09024263918399811, acc: 0.9807692170143127)
[2025-02-13 20:13:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:11][root][INFO] - Training Epoch: 2/2, step 2234/7134 completed (loss: 0.03065735101699829, acc: 0.9935064911842346)
[2025-02-13 20:13:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:11][root][INFO] - Training Epoch: 2/2, step 2235/7134 completed (loss: 0.1065768450498581, acc: 0.9790209531784058)
[2025-02-13 20:13:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:11][root][INFO] - Training Epoch: 2/2, step 2236/7134 completed (loss: 0.0669461116194725, acc: 0.9933333396911621)
[2025-02-13 20:13:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:12][root][INFO] - Training Epoch: 2/2, step 2237/7134 completed (loss: 0.07311663776636124, acc: 0.9733333587646484)
[2025-02-13 20:13:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:12][root][INFO] - Training Epoch: 2/2, step 2238/7134 completed (loss: 0.1181558221578598, acc: 0.9691358208656311)
[2025-02-13 20:13:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:13][root][INFO] - Training Epoch: 2/2, step 2239/7134 completed (loss: 0.3448159992694855, acc: 0.9197860956192017)
[2025-02-13 20:13:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:13][root][INFO] - Training Epoch: 2/2, step 2240/7134 completed (loss: 0.13238562643527985, acc: 0.9659090638160706)
[2025-02-13 20:13:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:13][root][INFO] - Training Epoch: 2/2, step 2241/7134 completed (loss: 0.16481590270996094, acc: 0.96875)
[2025-02-13 20:13:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:14][root][INFO] - Training Epoch: 2/2, step 2242/7134 completed (loss: 0.16485480964183807, acc: 0.9629629850387573)
[2025-02-13 20:13:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:14][root][INFO] - Training Epoch: 2/2, step 2243/7134 completed (loss: 0.3814271092414856, acc: 0.8903225660324097)
[2025-02-13 20:13:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:15][root][INFO] - Training Epoch: 2/2, step 2244/7134 completed (loss: 0.14912369847297668, acc: 0.9589040875434875)
[2025-02-13 20:13:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:15][root][INFO] - Training Epoch: 2/2, step 2245/7134 completed (loss: 0.2569446265697479, acc: 0.9346405267715454)
[2025-02-13 20:13:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:16][root][INFO] - Training Epoch: 2/2, step 2246/7134 completed (loss: 0.49997401237487793, acc: 0.874316930770874)
[2025-02-13 20:13:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:16][root][INFO] - Training Epoch: 2/2, step 2247/7134 completed (loss: 0.2818682789802551, acc: 0.9306358098983765)
[2025-02-13 20:13:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:16][root][INFO] - Training Epoch: 2/2, step 2248/7134 completed (loss: 0.33586570620536804, acc: 0.913294792175293)
[2025-02-13 20:13:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:17][root][INFO] - Training Epoch: 2/2, step 2249/7134 completed (loss: 0.16460929811000824, acc: 0.9613259434700012)
[2025-02-13 20:13:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:17][root][INFO] - Training Epoch: 2/2, step 2250/7134 completed (loss: 0.17689715325832367, acc: 0.978723406791687)
[2025-02-13 20:13:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:18][root][INFO] - Training Epoch: 2/2, step 2251/7134 completed (loss: 0.055064596235752106, acc: 0.9836065769195557)
[2025-02-13 20:13:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:18][root][INFO] - Training Epoch: 2/2, step 2252/7134 completed (loss: 0.052341144531965256, acc: 0.989130437374115)
[2025-02-13 20:13:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:18][root][INFO] - Training Epoch: 2/2, step 2253/7134 completed (loss: 0.2028951346874237, acc: 0.9681528806686401)
[2025-02-13 20:13:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:19][root][INFO] - Training Epoch: 2/2, step 2254/7134 completed (loss: 0.23013335466384888, acc: 0.9596773982048035)
[2025-02-13 20:13:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:19][root][INFO] - Training Epoch: 2/2, step 2255/7134 completed (loss: 0.07906392961740494, acc: 0.9874213933944702)
[2025-02-13 20:13:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:20][root][INFO] - Training Epoch: 2/2, step 2256/7134 completed (loss: 0.1478535681962967, acc: 0.9512194991111755)
[2025-02-13 20:13:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:20][root][INFO] - Training Epoch: 2/2, step 2257/7134 completed (loss: 0.08666638284921646, acc: 0.9814814925193787)
[2025-02-13 20:13:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:20][root][INFO] - Training Epoch: 2/2, step 2258/7134 completed (loss: 0.10940656810998917, acc: 0.9844961166381836)
[2025-02-13 20:13:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:21][root][INFO] - Training Epoch: 2/2, step 2259/7134 completed (loss: 0.13629741966724396, acc: 0.9743589758872986)
[2025-02-13 20:13:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:21][root][INFO] - Training Epoch: 2/2, step 2260/7134 completed (loss: 0.2881148159503937, acc: 0.9350649118423462)
[2025-02-13 20:13:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:22][root][INFO] - Training Epoch: 2/2, step 2261/7134 completed (loss: 0.19392423331737518, acc: 0.9320987462997437)
[2025-02-13 20:13:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:22][root][INFO] - Training Epoch: 2/2, step 2262/7134 completed (loss: 0.08710336685180664, acc: 0.969924807548523)
[2025-02-13 20:13:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:22][root][INFO] - Training Epoch: 2/2, step 2263/7134 completed (loss: 0.28084060549736023, acc: 0.932330846786499)
[2025-02-13 20:13:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:23][root][INFO] - Training Epoch: 2/2, step 2264/7134 completed (loss: 0.32328686118125916, acc: 0.929729700088501)
[2025-02-13 20:13:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:23][root][INFO] - Training Epoch: 2/2, step 2265/7134 completed (loss: 0.25954577326774597, acc: 0.948051929473877)
[2025-02-13 20:13:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:23][root][INFO] - Training Epoch: 2/2, step 2266/7134 completed (loss: 0.16939084231853485, acc: 0.950276255607605)
[2025-02-13 20:13:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:24][root][INFO] - Training Epoch: 2/2, step 2267/7134 completed (loss: 0.10281068831682205, acc: 0.9837398529052734)
[2025-02-13 20:13:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:24][root][INFO] - Training Epoch: 2/2, step 2268/7134 completed (loss: 0.10092684626579285, acc: 0.9589040875434875)
[2025-02-13 20:13:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:25][root][INFO] - Training Epoch: 2/2, step 2269/7134 completed (loss: 0.06674408912658691, acc: 0.9832402467727661)
[2025-02-13 20:13:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:25][root][INFO] - Training Epoch: 2/2, step 2270/7134 completed (loss: 0.20866207778453827, acc: 0.9453125)
[2025-02-13 20:13:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:25][root][INFO] - Training Epoch: 2/2, step 2271/7134 completed (loss: 0.18298523128032684, acc: 0.9459459185600281)
[2025-02-13 20:13:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:26][root][INFO] - Training Epoch: 2/2, step 2272/7134 completed (loss: 0.1307055950164795, acc: 0.9516128897666931)
[2025-02-13 20:13:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:26][root][INFO] - Training Epoch: 2/2, step 2273/7134 completed (loss: 0.07422158867120743, acc: 0.9833333492279053)
[2025-02-13 20:13:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:27][root][INFO] - Training Epoch: 2/2, step 2274/7134 completed (loss: 0.452680379152298, acc: 0.9333333373069763)
[2025-02-13 20:13:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:27][root][INFO] - Training Epoch: 2/2, step 2275/7134 completed (loss: 0.10907972604036331, acc: 0.9852941036224365)
[2025-02-13 20:13:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:27][root][INFO] - Training Epoch: 2/2, step 2276/7134 completed (loss: 0.15448053181171417, acc: 0.9894179701805115)
[2025-02-13 20:13:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:28][root][INFO] - Training Epoch: 2/2, step 2277/7134 completed (loss: 0.12740716338157654, acc: 0.9743589758872986)
[2025-02-13 20:13:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:28][root][INFO] - Training Epoch: 2/2, step 2278/7134 completed (loss: 0.2838052213191986, acc: 0.9225806593894958)
[2025-02-13 20:13:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:29][root][INFO] - Training Epoch: 2/2, step 2279/7134 completed (loss: 0.08885521441698074, acc: 0.9847715497016907)
[2025-02-13 20:13:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:29][root][INFO] - Training Epoch: 2/2, step 2280/7134 completed (loss: 0.173065647482872, acc: 0.9495412707328796)
[2025-02-13 20:13:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:29][root][INFO] - Training Epoch: 2/2, step 2281/7134 completed (loss: 0.37520378828048706, acc: 0.9202127456665039)
[2025-02-13 20:13:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:30][root][INFO] - Training Epoch: 2/2, step 2282/7134 completed (loss: 0.03146081045269966, acc: 0.9947368502616882)
[2025-02-13 20:13:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:30][root][INFO] - Training Epoch: 2/2, step 2283/7134 completed (loss: 0.17314177751541138, acc: 0.9711538553237915)
[2025-02-13 20:13:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:31][root][INFO] - Training Epoch: 2/2, step 2284/7134 completed (loss: 0.06567885726690292, acc: 0.9935483932495117)
[2025-02-13 20:13:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:31][root][INFO] - Training Epoch: 2/2, step 2285/7134 completed (loss: 0.07684281468391418, acc: 0.9850000143051147)
[2025-02-13 20:13:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:32][root][INFO] - Training Epoch: 2/2, step 2286/7134 completed (loss: 0.1771758794784546, acc: 0.9482758641242981)
[2025-02-13 20:13:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:32][root][INFO] - Training Epoch: 2/2, step 2287/7134 completed (loss: 0.08488277345895767, acc: 0.9815950989723206)
[2025-02-13 20:13:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:32][root][INFO] - Training Epoch: 2/2, step 2288/7134 completed (loss: 0.05241293087601662, acc: 0.9945054650306702)
[2025-02-13 20:13:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:33][root][INFO] - Training Epoch: 2/2, step 2289/7134 completed (loss: 0.11024939268827438, acc: 0.9679144620895386)
[2025-02-13 20:13:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:33][root][INFO] - Training Epoch: 2/2, step 2290/7134 completed (loss: 0.08021940290927887, acc: 0.9838709831237793)
[2025-02-13 20:13:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:34][root][INFO] - Training Epoch: 2/2, step 2291/7134 completed (loss: 0.07463763654232025, acc: 0.9834254384040833)
[2025-02-13 20:13:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:34][root][INFO] - Training Epoch: 2/2, step 2292/7134 completed (loss: 0.18402163684368134, acc: 0.9595959782600403)
[2025-02-13 20:13:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:35][root][INFO] - Training Epoch: 2/2, step 2293/7134 completed (loss: 0.10471883416175842, acc: 0.9771689772605896)
[2025-02-13 20:13:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:35][root][INFO] - Training Epoch: 2/2, step 2294/7134 completed (loss: 0.09188855439424515, acc: 0.9856459498405457)
[2025-02-13 20:13:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:35][root][INFO] - Training Epoch: 2/2, step 2295/7134 completed (loss: 0.15213502943515778, acc: 0.95703125)
[2025-02-13 20:13:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:36][root][INFO] - Training Epoch: 2/2, step 2296/7134 completed (loss: 0.04954824596643448, acc: 0.9903846383094788)
[2025-02-13 20:13:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:36][root][INFO] - Training Epoch: 2/2, step 2297/7134 completed (loss: 0.05081874132156372, acc: 0.9858490824699402)
[2025-02-13 20:13:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:37][root][INFO] - Training Epoch: 2/2, step 2298/7134 completed (loss: 0.08725905418395996, acc: 0.976190447807312)
[2025-02-13 20:13:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:37][root][INFO] - Training Epoch: 2/2, step 2299/7134 completed (loss: 0.1539769172668457, acc: 0.9784946441650391)
[2025-02-13 20:13:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:37][root][INFO] - Training Epoch: 2/2, step 2300/7134 completed (loss: 0.08688843995332718, acc: 0.9747474789619446)
[2025-02-13 20:13:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:38][root][INFO] - Training Epoch: 2/2, step 2301/7134 completed (loss: 0.12879987061023712, acc: 0.9728260636329651)
[2025-02-13 20:13:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:38][root][INFO] - Training Epoch: 2/2, step 2302/7134 completed (loss: 0.13632342219352722, acc: 0.9692307710647583)
[2025-02-13 20:13:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:39][root][INFO] - Training Epoch: 2/2, step 2303/7134 completed (loss: 0.11790184676647186, acc: 0.9585253596305847)
[2025-02-13 20:13:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:39][root][INFO] - Training Epoch: 2/2, step 2304/7134 completed (loss: 0.21959109604358673, acc: 0.9515418410301208)
[2025-02-13 20:13:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:39][root][INFO] - Training Epoch: 2/2, step 2305/7134 completed (loss: 0.07705849409103394, acc: 0.9858490824699402)
[2025-02-13 20:13:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:40][root][INFO] - Training Epoch: 2/2, step 2306/7134 completed (loss: 0.042036399245262146, acc: 0.9953488111495972)
[2025-02-13 20:13:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:40][root][INFO] - Training Epoch: 2/2, step 2307/7134 completed (loss: 0.06771773099899292, acc: 0.9872881174087524)
[2025-02-13 20:13:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:41][root][INFO] - Training Epoch: 2/2, step 2308/7134 completed (loss: 0.08532004058361053, acc: 0.9823788404464722)
[2025-02-13 20:13:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:41][root][INFO] - Training Epoch: 2/2, step 2309/7134 completed (loss: 0.058070555329322815, acc: 0.9894179701805115)
[2025-02-13 20:13:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:42][root][INFO] - Training Epoch: 2/2, step 2310/7134 completed (loss: 0.08209938555955887, acc: 0.9904761910438538)
[2025-02-13 20:13:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:42][root][INFO] - Training Epoch: 2/2, step 2311/7134 completed (loss: 0.04451759159564972, acc: 0.9901477694511414)
[2025-02-13 20:13:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:42][root][INFO] - Training Epoch: 2/2, step 2312/7134 completed (loss: 0.050651513040065765, acc: 0.9953051805496216)
[2025-02-13 20:13:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:43][root][INFO] - Training Epoch: 2/2, step 2313/7134 completed (loss: 0.1280457228422165, acc: 0.9663461446762085)
[2025-02-13 20:13:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:43][root][INFO] - Training Epoch: 2/2, step 2314/7134 completed (loss: 0.08650673180818558, acc: 0.9724770784378052)
[2025-02-13 20:13:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:43][root][INFO] - Training Epoch: 2/2, step 2315/7134 completed (loss: 0.13431207835674286, acc: 0.9627906680107117)
[2025-02-13 20:13:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:44][root][INFO] - Training Epoch: 2/2, step 2316/7134 completed (loss: 0.06277740001678467, acc: 0.9841269850730896)
[2025-02-13 20:13:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:44][root][INFO] - Training Epoch: 2/2, step 2317/7134 completed (loss: 0.12818025052547455, acc: 0.9663865566253662)
[2025-02-13 20:13:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:45][root][INFO] - Training Epoch: 2/2, step 2318/7134 completed (loss: 0.017658574506640434, acc: 1.0)
[2025-02-13 20:13:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:45][root][INFO] - Training Epoch: 2/2, step 2319/7134 completed (loss: 0.04062572866678238, acc: 0.9864864945411682)
[2025-02-13 20:13:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:45][root][INFO] - Training Epoch: 2/2, step 2320/7134 completed (loss: 0.04521556571125984, acc: 0.9897435903549194)
[2025-02-13 20:13:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:46][root][INFO] - Training Epoch: 2/2, step 2321/7134 completed (loss: 0.047928713262081146, acc: 0.9882352948188782)
[2025-02-13 20:13:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:46][root][INFO] - Training Epoch: 2/2, step 2322/7134 completed (loss: 0.0765499547123909, acc: 0.9754601120948792)
[2025-02-13 20:13:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:47][root][INFO] - Training Epoch: 2/2, step 2323/7134 completed (loss: 0.1347518414258957, acc: 0.9490445852279663)
[2025-02-13 20:13:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:47][root][INFO] - Training Epoch: 2/2, step 2324/7134 completed (loss: 0.23622262477874756, acc: 0.9698795080184937)
[2025-02-13 20:13:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:47][root][INFO] - Training Epoch: 2/2, step 2325/7134 completed (loss: 0.09863672405481339, acc: 0.9857142567634583)
[2025-02-13 20:13:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:48][root][INFO] - Training Epoch: 2/2, step 2326/7134 completed (loss: 0.04730364680290222, acc: 0.9846153855323792)
[2025-02-13 20:13:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:48][root][INFO] - Training Epoch: 2/2, step 2327/7134 completed (loss: 0.052795689553022385, acc: 0.9865771532058716)
[2025-02-13 20:13:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:49][root][INFO] - Training Epoch: 2/2, step 2328/7134 completed (loss: 0.1390896886587143, acc: 0.9597315192222595)
[2025-02-13 20:13:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:49][root][INFO] - Training Epoch: 2/2, step 2329/7134 completed (loss: 0.10568318516016006, acc: 0.9673202633857727)
[2025-02-13 20:13:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:49][root][INFO] - Training Epoch: 2/2, step 2330/7134 completed (loss: 0.014608926139771938, acc: 1.0)
[2025-02-13 20:13:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:50][root][INFO] - Training Epoch: 2/2, step 2331/7134 completed (loss: 0.028921378776431084, acc: 0.9935483932495117)
[2025-02-13 20:13:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:50][root][INFO] - Training Epoch: 2/2, step 2332/7134 completed (loss: 0.03275567665696144, acc: 0.9939393997192383)
[2025-02-13 20:13:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:50][root][INFO] - Training Epoch: 2/2, step 2333/7134 completed (loss: 0.08524680882692337, acc: 0.9878048896789551)
[2025-02-13 20:13:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:51][root][INFO] - Training Epoch: 2/2, step 2334/7134 completed (loss: 0.17130513489246368, acc: 0.9681528806686401)
[2025-02-13 20:13:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:51][root][INFO] - Training Epoch: 2/2, step 2335/7134 completed (loss: 0.032694440335035324, acc: 0.9873417615890503)
[2025-02-13 20:13:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:51][root][INFO] - Training Epoch: 2/2, step 2336/7134 completed (loss: 0.038256578147411346, acc: 0.9865771532058716)
[2025-02-13 20:13:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:52][root][INFO] - Training Epoch: 2/2, step 2337/7134 completed (loss: 0.03682338073849678, acc: 0.9930555820465088)
[2025-02-13 20:13:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:52][root][INFO] - Training Epoch: 2/2, step 2338/7134 completed (loss: 0.03136666491627693, acc: 1.0)
[2025-02-13 20:13:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:53][root][INFO] - Training Epoch: 2/2, step 2339/7134 completed (loss: 0.10136377811431885, acc: 0.9743589758872986)
[2025-02-13 20:13:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:53][root][INFO] - Training Epoch: 2/2, step 2340/7134 completed (loss: 0.0261156614869833, acc: 1.0)
[2025-02-13 20:13:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:53][root][INFO] - Training Epoch: 2/2, step 2341/7134 completed (loss: 0.042141616344451904, acc: 0.9917355179786682)
[2025-02-13 20:13:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:54][root][INFO] - Training Epoch: 2/2, step 2342/7134 completed (loss: 0.035189930349588394, acc: 0.9940476417541504)
[2025-02-13 20:13:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:54][root][INFO] - Training Epoch: 2/2, step 2343/7134 completed (loss: 0.027928059920668602, acc: 0.9937499761581421)
[2025-02-13 20:13:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:54][root][INFO] - Training Epoch: 2/2, step 2344/7134 completed (loss: 0.07330217957496643, acc: 0.9691358208656311)
[2025-02-13 20:13:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:55][root][INFO] - Training Epoch: 2/2, step 2345/7134 completed (loss: 0.07123131304979324, acc: 0.982758641242981)
[2025-02-13 20:13:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:55][root][INFO] - Training Epoch: 2/2, step 2346/7134 completed (loss: 0.03788413107395172, acc: 0.988950252532959)
[2025-02-13 20:13:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:56][root][INFO] - Training Epoch: 2/2, step 2347/7134 completed (loss: 0.06399572640657425, acc: 0.98591548204422)
[2025-02-13 20:13:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:56][root][INFO] - Training Epoch: 2/2, step 2348/7134 completed (loss: 0.2817363142967224, acc: 0.9226804375648499)
[2025-02-13 20:13:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:56][root][INFO] - Training Epoch: 2/2, step 2349/7134 completed (loss: 0.19992920756340027, acc: 0.9312499761581421)
[2025-02-13 20:13:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:57][root][INFO] - Training Epoch: 2/2, step 2350/7134 completed (loss: 0.19756820797920227, acc: 0.9620253443717957)
[2025-02-13 20:13:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:57][root][INFO] - Training Epoch: 2/2, step 2351/7134 completed (loss: 0.07750797271728516, acc: 0.9781420826911926)
[2025-02-13 20:13:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:58][root][INFO] - Training Epoch: 2/2, step 2352/7134 completed (loss: 0.16954104602336884, acc: 0.9548386931419373)
[2025-02-13 20:13:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:58][root][INFO] - Training Epoch: 2/2, step 2353/7134 completed (loss: 0.1753920465707779, acc: 0.9612902998924255)
[2025-02-13 20:13:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:58][root][INFO] - Training Epoch: 2/2, step 2354/7134 completed (loss: 0.07782404869794846, acc: 0.9836065769195557)
[2025-02-13 20:13:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:59][root][INFO] - Training Epoch: 2/2, step 2355/7134 completed (loss: 0.1449361890554428, acc: 0.9723756909370422)
[2025-02-13 20:13:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:59][root][INFO] - Training Epoch: 2/2, step 2356/7134 completed (loss: 0.17426106333732605, acc: 0.9637681245803833)
[2025-02-13 20:13:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:13:59][root][INFO] - Training Epoch: 2/2, step 2357/7134 completed (loss: 0.1184118390083313, acc: 0.9682539701461792)
[2025-02-13 20:14:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:00][root][INFO] - Training Epoch: 2/2, step 2358/7134 completed (loss: 0.2579159140586853, acc: 0.9336734414100647)
[2025-02-13 20:14:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:00][root][INFO] - Training Epoch: 2/2, step 2359/7134 completed (loss: 0.11161305010318756, acc: 0.96875)
[2025-02-13 20:14:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:01][root][INFO] - Training Epoch: 2/2, step 2360/7134 completed (loss: 0.09536966681480408, acc: 0.9746835231781006)
[2025-02-13 20:14:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:01][root][INFO] - Training Epoch: 2/2, step 2361/7134 completed (loss: 0.10115688294172287, acc: 0.9642857313156128)
[2025-02-13 20:14:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:01][root][INFO] - Training Epoch: 2/2, step 2362/7134 completed (loss: 0.13756759464740753, acc: 0.9639639854431152)
[2025-02-13 20:14:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:02][root][INFO] - Training Epoch: 2/2, step 2363/7134 completed (loss: 0.10535762459039688, acc: 0.9790576100349426)
[2025-02-13 20:14:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:02][root][INFO] - Training Epoch: 2/2, step 2364/7134 completed (loss: 0.11306726187467575, acc: 0.9803921580314636)
[2025-02-13 20:14:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:02][root][INFO] - Training Epoch: 2/2, step 2365/7134 completed (loss: 0.08680307120084763, acc: 0.9709302186965942)
[2025-02-13 20:14:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:03][root][INFO] - Training Epoch: 2/2, step 2366/7134 completed (loss: 0.08209406584501266, acc: 0.9794871807098389)
[2025-02-13 20:14:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:03][root][INFO] - Training Epoch: 2/2, step 2367/7134 completed (loss: 0.07846375554800034, acc: 0.9797979593276978)
[2025-02-13 20:14:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:04][root][INFO] - Training Epoch: 2/2, step 2368/7134 completed (loss: 0.16152289509773254, acc: 0.9597989916801453)
[2025-02-13 20:14:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:04][root][INFO] - Training Epoch: 2/2, step 2369/7134 completed (loss: 0.12277910113334656, acc: 0.9764150977134705)
[2025-02-13 20:14:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:05][root][INFO] - Training Epoch: 2/2, step 2370/7134 completed (loss: 0.03510219603776932, acc: 1.0)
[2025-02-13 20:14:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:05][root][INFO] - Training Epoch: 2/2, step 2371/7134 completed (loss: 0.08524172753095627, acc: 0.9751552939414978)
[2025-02-13 20:14:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:05][root][INFO] - Training Epoch: 2/2, step 2372/7134 completed (loss: 0.17769131064414978, acc: 0.9750000238418579)
[2025-02-13 20:14:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:06][root][INFO] - Training Epoch: 2/2, step 2373/7134 completed (loss: 0.07539694011211395, acc: 0.9792746305465698)
[2025-02-13 20:14:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:06][root][INFO] - Training Epoch: 2/2, step 2374/7134 completed (loss: 0.0818314403295517, acc: 0.9779005646705627)
[2025-02-13 20:14:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:07][root][INFO] - Training Epoch: 2/2, step 2375/7134 completed (loss: 0.11243986338376999, acc: 0.9515151381492615)
[2025-02-13 20:14:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:07][root][INFO] - Training Epoch: 2/2, step 2376/7134 completed (loss: 0.23478703200817108, acc: 0.9411764740943909)
[2025-02-13 20:14:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:07][root][INFO] - Training Epoch: 2/2, step 2377/7134 completed (loss: 0.1462816298007965, acc: 0.9672897458076477)
[2025-02-13 20:14:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:08][root][INFO] - Training Epoch: 2/2, step 2378/7134 completed (loss: 0.16928890347480774, acc: 0.9653465151786804)
[2025-02-13 20:14:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:08][root][INFO] - Training Epoch: 2/2, step 2379/7134 completed (loss: 0.27490657567977905, acc: 0.918367326259613)
[2025-02-13 20:14:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:09][root][INFO] - Training Epoch: 2/2, step 2380/7134 completed (loss: 0.1532433182001114, acc: 0.95652174949646)
[2025-02-13 20:14:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:09][root][INFO] - Training Epoch: 2/2, step 2381/7134 completed (loss: 0.41477563977241516, acc: 0.8819875717163086)
[2025-02-13 20:14:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:09][root][INFO] - Training Epoch: 2/2, step 2382/7134 completed (loss: 0.12466749548912048, acc: 0.9691358208656311)
[2025-02-13 20:14:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:10][root][INFO] - Training Epoch: 2/2, step 2383/7134 completed (loss: 0.12866367399692535, acc: 0.9672130942344666)
[2025-02-13 20:14:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:10][root][INFO] - Training Epoch: 2/2, step 2384/7134 completed (loss: 0.2169039249420166, acc: 0.954023003578186)
[2025-02-13 20:14:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:10][root][INFO] - Training Epoch: 2/2, step 2385/7134 completed (loss: 0.0713249146938324, acc: 0.9780219793319702)
[2025-02-13 20:14:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:11][root][INFO] - Training Epoch: 2/2, step 2386/7134 completed (loss: 0.15882527828216553, acc: 0.9470899701118469)
[2025-02-13 20:14:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:11][root][INFO] - Training Epoch: 2/2, step 2387/7134 completed (loss: 0.15691491961479187, acc: 0.976190447807312)
[2025-02-13 20:14:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:12][root][INFO] - Training Epoch: 2/2, step 2388/7134 completed (loss: 0.2721119821071625, acc: 0.9230769276618958)
[2025-02-13 20:14:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:12][root][INFO] - Training Epoch: 2/2, step 2389/7134 completed (loss: 0.21964210271835327, acc: 0.9526066184043884)
[2025-02-13 20:14:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:12][root][INFO] - Training Epoch: 2/2, step 2390/7134 completed (loss: 0.3028382658958435, acc: 0.928909957408905)
[2025-02-13 20:14:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:13][root][INFO] - Training Epoch: 2/2, step 2391/7134 completed (loss: 0.18315647542476654, acc: 0.9567567706108093)
[2025-02-13 20:14:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:13][root][INFO] - Training Epoch: 2/2, step 2392/7134 completed (loss: 0.21688513457775116, acc: 0.9617486596107483)
[2025-02-13 20:14:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:14][root][INFO] - Training Epoch: 2/2, step 2393/7134 completed (loss: 0.16271451115608215, acc: 0.9533678889274597)
[2025-02-13 20:14:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:14][root][INFO] - Training Epoch: 2/2, step 2394/7134 completed (loss: 0.11582104116678238, acc: 0.9642857313156128)
[2025-02-13 20:14:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:15][root][INFO] - Training Epoch: 2/2, step 2395/7134 completed (loss: 0.38666588068008423, acc: 0.8914285898208618)
[2025-02-13 20:14:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:15][root][INFO] - Training Epoch: 2/2, step 2396/7134 completed (loss: 0.3940875828266144, acc: 0.8787878751754761)
[2025-02-13 20:14:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:15][root][INFO] - Training Epoch: 2/2, step 2397/7134 completed (loss: 0.28289246559143066, acc: 0.9329268336296082)
[2025-02-13 20:14:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:16][root][INFO] - Training Epoch: 2/2, step 2398/7134 completed (loss: 0.07776416838169098, acc: 0.9891892075538635)
[2025-02-13 20:14:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:16][root][INFO] - Training Epoch: 2/2, step 2399/7134 completed (loss: 0.33678776025772095, acc: 0.9015544056892395)
[2025-02-13 20:14:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:17][root][INFO] - Training Epoch: 2/2, step 2400/7134 completed (loss: 0.3097974956035614, acc: 0.909547746181488)
[2025-02-13 20:14:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:17][root][INFO] - Training Epoch: 2/2, step 2401/7134 completed (loss: 0.7502071857452393, acc: 0.8269230723381042)
[2025-02-13 20:14:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:17][root][INFO] - Training Epoch: 2/2, step 2402/7134 completed (loss: 0.28892627358436584, acc: 0.9130434989929199)
[2025-02-13 20:14:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:18][root][INFO] - Training Epoch: 2/2, step 2403/7134 completed (loss: 0.12188474088907242, acc: 0.970059871673584)
[2025-02-13 20:14:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:18][root][INFO] - Training Epoch: 2/2, step 2404/7134 completed (loss: 0.09502147883176804, acc: 0.9802631735801697)
[2025-02-13 20:14:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:18][root][INFO] - Training Epoch: 2/2, step 2405/7134 completed (loss: 0.2665277123451233, acc: 0.9554139971733093)
[2025-02-13 20:14:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:19][root][INFO] - Training Epoch: 2/2, step 2406/7134 completed (loss: 0.05693340301513672, acc: 0.9940828680992126)
[2025-02-13 20:14:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:19][root][INFO] - Training Epoch: 2/2, step 2407/7134 completed (loss: 0.08704762160778046, acc: 0.9673202633857727)
[2025-02-13 20:14:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:20][root][INFO] - Training Epoch: 2/2, step 2408/7134 completed (loss: 0.214776873588562, acc: 0.950276255607605)
[2025-02-13 20:14:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:20][root][INFO] - Training Epoch: 2/2, step 2409/7134 completed (loss: 0.14830251038074493, acc: 0.956250011920929)
[2025-02-13 20:14:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:20][root][INFO] - Training Epoch: 2/2, step 2410/7134 completed (loss: 0.05337059870362282, acc: 0.9929577708244324)
[2025-02-13 20:14:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:21][root][INFO] - Training Epoch: 2/2, step 2411/7134 completed (loss: 0.07637438923120499, acc: 0.9723756909370422)
[2025-02-13 20:14:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:21][root][INFO] - Training Epoch: 2/2, step 2412/7134 completed (loss: 0.048673126846551895, acc: 0.9772727489471436)
[2025-02-13 20:14:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:21][root][INFO] - Training Epoch: 2/2, step 2413/7134 completed (loss: 0.045348525047302246, acc: 0.9935064911842346)
[2025-02-13 20:14:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:22][root][INFO] - Training Epoch: 2/2, step 2414/7134 completed (loss: 0.0645766332745552, acc: 0.9836065769195557)
[2025-02-13 20:14:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:22][root][INFO] - Training Epoch: 2/2, step 2415/7134 completed (loss: 0.29158535599708557, acc: 0.9634146094322205)
[2025-02-13 20:14:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:23][root][INFO] - Training Epoch: 2/2, step 2416/7134 completed (loss: 0.05640285834670067, acc: 0.9937106966972351)
[2025-02-13 20:14:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:23][root][INFO] - Training Epoch: 2/2, step 2417/7134 completed (loss: 0.15807956457138062, acc: 0.9756097793579102)
[2025-02-13 20:14:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:23][root][INFO] - Training Epoch: 2/2, step 2418/7134 completed (loss: 0.1081811785697937, acc: 0.9720279574394226)
[2025-02-13 20:14:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:24][root][INFO] - Training Epoch: 2/2, step 2419/7134 completed (loss: 0.0771503821015358, acc: 0.987730085849762)
[2025-02-13 20:14:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:24][root][INFO] - Training Epoch: 2/2, step 2420/7134 completed (loss: 0.09499894827604294, acc: 0.9722222089767456)
[2025-02-13 20:14:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:25][root][INFO] - Training Epoch: 2/2, step 2421/7134 completed (loss: 0.06295133382081985, acc: 0.9837837815284729)
[2025-02-13 20:14:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:25][root][INFO] - Training Epoch: 2/2, step 2422/7134 completed (loss: 0.11781412363052368, acc: 0.977142870426178)
[2025-02-13 20:14:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:25][root][INFO] - Training Epoch: 2/2, step 2423/7134 completed (loss: 0.07244640588760376, acc: 0.9940476417541504)
[2025-02-13 20:14:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:26][root][INFO] - Training Epoch: 2/2, step 2424/7134 completed (loss: 0.10932376235723495, acc: 0.9715909361839294)
[2025-02-13 20:14:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:26][root][INFO] - Training Epoch: 2/2, step 2425/7134 completed (loss: 0.12451422959566116, acc: 0.9719101190567017)
[2025-02-13 20:14:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:27][root][INFO] - Training Epoch: 2/2, step 2426/7134 completed (loss: 0.08287424594163895, acc: 0.982758641242981)
[2025-02-13 20:14:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:27][root][INFO] - Training Epoch: 2/2, step 2427/7134 completed (loss: 0.07273686677217484, acc: 0.9739583134651184)
[2025-02-13 20:14:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:27][root][INFO] - Training Epoch: 2/2, step 2428/7134 completed (loss: 0.08323456346988678, acc: 0.9736841917037964)
[2025-02-13 20:14:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:28][root][INFO] - Training Epoch: 2/2, step 2429/7134 completed (loss: 0.13400733470916748, acc: 0.9556962251663208)
[2025-02-13 20:14:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:28][root][INFO] - Training Epoch: 2/2, step 2430/7134 completed (loss: 0.06159891188144684, acc: 0.9800000190734863)
[2025-02-13 20:14:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:29][root][INFO] - Training Epoch: 2/2, step 2431/7134 completed (loss: 0.0436219684779644, acc: 0.9930070042610168)
[2025-02-13 20:14:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:29][root][INFO] - Training Epoch: 2/2, step 2432/7134 completed (loss: 0.18812154233455658, acc: 0.9504132270812988)
[2025-02-13 20:14:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:29][root][INFO] - Training Epoch: 2/2, step 2433/7134 completed (loss: 0.11742934584617615, acc: 0.9784172773361206)
[2025-02-13 20:14:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:30][root][INFO] - Training Epoch: 2/2, step 2434/7134 completed (loss: 0.21767300367355347, acc: 0.9530201554298401)
[2025-02-13 20:14:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:30][root][INFO] - Training Epoch: 2/2, step 2435/7134 completed (loss: 0.18299423158168793, acc: 0.9470198750495911)
[2025-02-13 20:14:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:30][root][INFO] - Training Epoch: 2/2, step 2436/7134 completed (loss: 0.07518476247787476, acc: 0.9746192693710327)
[2025-02-13 20:14:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:31][root][INFO] - Training Epoch: 2/2, step 2437/7134 completed (loss: 0.1167706623673439, acc: 0.9788732528686523)
[2025-02-13 20:14:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:31][root][INFO] - Training Epoch: 2/2, step 2438/7134 completed (loss: 0.15867017209529877, acc: 0.9691358208656311)
[2025-02-13 20:14:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:32][root][INFO] - Training Epoch: 2/2, step 2439/7134 completed (loss: 0.14367136359214783, acc: 0.9679144620895386)
[2025-02-13 20:14:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:32][root][INFO] - Training Epoch: 2/2, step 2440/7134 completed (loss: 0.05275193229317665, acc: 1.0)
[2025-02-13 20:14:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:32][root][INFO] - Training Epoch: 2/2, step 2441/7134 completed (loss: 0.33017727732658386, acc: 0.9359999895095825)
[2025-02-13 20:14:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:33][root][INFO] - Training Epoch: 2/2, step 2442/7134 completed (loss: 0.20614184439182281, acc: 0.9704142212867737)
[2025-02-13 20:14:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:33][root][INFO] - Training Epoch: 2/2, step 2443/7134 completed (loss: 0.1383231282234192, acc: 0.9754601120948792)
[2025-02-13 20:14:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:33][root][INFO] - Training Epoch: 2/2, step 2444/7134 completed (loss: 0.18664589524269104, acc: 0.961240291595459)
[2025-02-13 20:14:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:34][root][INFO] - Training Epoch: 2/2, step 2445/7134 completed (loss: 0.14546708762645721, acc: 0.9745222926139832)
[2025-02-13 20:14:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:34][root][INFO] - Training Epoch: 2/2, step 2446/7134 completed (loss: 0.17203772068023682, acc: 0.9415584206581116)
[2025-02-13 20:14:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:34][root][INFO] - Training Epoch: 2/2, step 2447/7134 completed (loss: 0.074367955327034, acc: 0.983146071434021)
[2025-02-13 20:14:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:35][root][INFO] - Training Epoch: 2/2, step 2448/7134 completed (loss: 0.11203599721193314, acc: 0.9731543660163879)
[2025-02-13 20:14:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:35][root][INFO] - Training Epoch: 2/2, step 2449/7134 completed (loss: 0.2525691092014313, acc: 0.9418604373931885)
[2025-02-13 20:14:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:36][root][INFO] - Training Epoch: 2/2, step 2450/7134 completed (loss: 0.0884602889418602, acc: 0.9823529124259949)
[2025-02-13 20:14:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:36][root][INFO] - Training Epoch: 2/2, step 2451/7134 completed (loss: 0.03974887356162071, acc: 0.9893048405647278)
[2025-02-13 20:14:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:36][root][INFO] - Training Epoch: 2/2, step 2452/7134 completed (loss: 0.06352166086435318, acc: 0.9838709831237793)
[2025-02-13 20:14:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:37][root][INFO] - Training Epoch: 2/2, step 2453/7134 completed (loss: 0.12387088686227798, acc: 0.9740932583808899)
[2025-02-13 20:14:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:37][root][INFO] - Training Epoch: 2/2, step 2454/7134 completed (loss: 0.08191407471895218, acc: 0.978723406791687)
[2025-02-13 20:14:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:38][root][INFO] - Training Epoch: 2/2, step 2455/7134 completed (loss: 0.049840278923511505, acc: 0.9938650131225586)
[2025-02-13 20:14:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:38][root][INFO] - Training Epoch: 2/2, step 2456/7134 completed (loss: 0.13559888303279877, acc: 0.9585492014884949)
[2025-02-13 20:14:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:38][root][INFO] - Training Epoch: 2/2, step 2457/7134 completed (loss: 0.13135656714439392, acc: 0.9738562107086182)
[2025-02-13 20:14:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:39][root][INFO] - Training Epoch: 2/2, step 2458/7134 completed (loss: 0.2110375612974167, acc: 0.9624999761581421)
[2025-02-13 20:14:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:39][root][INFO] - Training Epoch: 2/2, step 2459/7134 completed (loss: 0.053176265209913254, acc: 0.988304078578949)
[2025-02-13 20:14:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:39][root][INFO] - Training Epoch: 2/2, step 2460/7134 completed (loss: 0.12417241185903549, acc: 0.9670329689979553)
[2025-02-13 20:14:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:40][root][INFO] - Training Epoch: 2/2, step 2461/7134 completed (loss: 0.03990840166807175, acc: 0.9874213933944702)
[2025-02-13 20:14:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:40][root][INFO] - Training Epoch: 2/2, step 2462/7134 completed (loss: 0.18025268614292145, acc: 0.9575757384300232)
[2025-02-13 20:14:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:41][root][INFO] - Training Epoch: 2/2, step 2463/7134 completed (loss: 0.020769957453012466, acc: 1.0)
[2025-02-13 20:14:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:41][root][INFO] - Training Epoch: 2/2, step 2464/7134 completed (loss: 0.11665905267000198, acc: 0.9666666388511658)
[2025-02-13 20:14:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:41][root][INFO] - Training Epoch: 2/2, step 2465/7134 completed (loss: 0.08733987808227539, acc: 0.9832402467727661)
[2025-02-13 20:14:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:42][root][INFO] - Training Epoch: 2/2, step 2466/7134 completed (loss: 0.035769231617450714, acc: 0.9880239367485046)
[2025-02-13 20:14:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:42][root][INFO] - Training Epoch: 2/2, step 2467/7134 completed (loss: 0.13928373157978058, acc: 0.9606741666793823)
[2025-02-13 20:14:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:43][root][INFO] - Training Epoch: 2/2, step 2468/7134 completed (loss: 0.07595235109329224, acc: 0.9767441749572754)
[2025-02-13 20:14:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:43][root][INFO] - Training Epoch: 2/2, step 2469/7134 completed (loss: 0.02640908770263195, acc: 0.9941860437393188)
[2025-02-13 20:14:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:43][root][INFO] - Training Epoch: 2/2, step 2470/7134 completed (loss: 0.13786838948726654, acc: 0.957446813583374)
[2025-02-13 20:14:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:44][root][INFO] - Training Epoch: 2/2, step 2471/7134 completed (loss: 0.05632161721587181, acc: 0.978723406791687)
[2025-02-13 20:14:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:44][root][INFO] - Training Epoch: 2/2, step 2472/7134 completed (loss: 0.017483755946159363, acc: 1.0)
[2025-02-13 20:14:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:44][root][INFO] - Training Epoch: 2/2, step 2473/7134 completed (loss: 0.08412537723779678, acc: 0.9906542301177979)
[2025-02-13 20:14:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:45][root][INFO] - Training Epoch: 2/2, step 2474/7134 completed (loss: 0.021836766973137856, acc: 1.0)
[2025-02-13 20:14:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:45][root][INFO] - Training Epoch: 2/2, step 2475/7134 completed (loss: 0.062452591955661774, acc: 0.9918032884597778)
[2025-02-13 20:14:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:46][root][INFO] - Training Epoch: 2/2, step 2476/7134 completed (loss: 0.042385682463645935, acc: 0.988304078578949)
[2025-02-13 20:14:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:46][root][INFO] - Training Epoch: 2/2, step 2477/7134 completed (loss: 0.02774648927152157, acc: 1.0)
[2025-02-13 20:14:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:46][root][INFO] - Training Epoch: 2/2, step 2478/7134 completed (loss: 0.0427401103079319, acc: 0.9865771532058716)
[2025-02-13 20:14:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:47][root][INFO] - Training Epoch: 2/2, step 2479/7134 completed (loss: 0.19687630236148834, acc: 0.9387755393981934)
[2025-02-13 20:14:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:47][root][INFO] - Training Epoch: 2/2, step 2480/7134 completed (loss: 0.05949680507183075, acc: 0.9807692170143127)
[2025-02-13 20:14:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:48][root][INFO] - Training Epoch: 2/2, step 2481/7134 completed (loss: 0.03698809817433357, acc: 1.0)
[2025-02-13 20:14:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:48][root][INFO] - Training Epoch: 2/2, step 2482/7134 completed (loss: 0.06559474021196365, acc: 0.9870129823684692)
[2025-02-13 20:14:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:49][root][INFO] - Training Epoch: 2/2, step 2483/7134 completed (loss: 0.08214258402585983, acc: 0.9820359349250793)
[2025-02-13 20:14:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:49][root][INFO] - Training Epoch: 2/2, step 2484/7134 completed (loss: 0.0981515422463417, acc: 0.9638554453849792)
[2025-02-13 20:14:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:49][root][INFO] - Training Epoch: 2/2, step 2485/7134 completed (loss: 0.05921434611082077, acc: 0.9851852059364319)
[2025-02-13 20:14:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:50][root][INFO] - Training Epoch: 2/2, step 2486/7134 completed (loss: 0.058940134942531586, acc: 0.9931507110595703)
[2025-02-13 20:14:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:50][root][INFO] - Training Epoch: 2/2, step 2487/7134 completed (loss: 0.15287767350673676, acc: 0.9652777910232544)
[2025-02-13 20:14:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:51][root][INFO] - Training Epoch: 2/2, step 2488/7134 completed (loss: 0.08316687494516373, acc: 0.9878787994384766)
[2025-02-13 20:14:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:51][root][INFO] - Training Epoch: 2/2, step 2489/7134 completed (loss: 0.07039064168930054, acc: 0.9822485446929932)
[2025-02-13 20:14:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:52][root][INFO] - Training Epoch: 2/2, step 2490/7134 completed (loss: 0.08150263875722885, acc: 0.9935064911842346)
[2025-02-13 20:14:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:52][root][INFO] - Training Epoch: 2/2, step 2491/7134 completed (loss: 0.19216769933700562, acc: 0.9701492786407471)
[2025-02-13 20:14:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:52][root][INFO] - Training Epoch: 2/2, step 2492/7134 completed (loss: 0.07660380750894547, acc: 0.9811320900917053)
[2025-02-13 20:14:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:53][root][INFO] - Training Epoch: 2/2, step 2493/7134 completed (loss: 0.07909232378005981, acc: 0.976190447807312)
[2025-02-13 20:14:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:53][root][INFO] - Training Epoch: 2/2, step 2494/7134 completed (loss: 0.08295129239559174, acc: 0.9753086566925049)
[2025-02-13 20:14:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:54][root][INFO] - Training Epoch: 2/2, step 2495/7134 completed (loss: 0.116395965218544, acc: 0.9764705896377563)
[2025-02-13 20:14:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:54][root][INFO] - Training Epoch: 2/2, step 2496/7134 completed (loss: 0.09443879127502441, acc: 0.9746835231781006)
[2025-02-13 20:14:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:55][root][INFO] - Training Epoch: 2/2, step 2497/7134 completed (loss: 0.11208447813987732, acc: 0.977011501789093)
[2025-02-13 20:14:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:55][root][INFO] - Training Epoch: 2/2, step 2498/7134 completed (loss: 0.07689296454191208, acc: 0.9828571677207947)
[2025-02-13 20:14:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:55][root][INFO] - Training Epoch: 2/2, step 2499/7134 completed (loss: 0.05600130558013916, acc: 0.9939758777618408)
[2025-02-13 20:14:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:56][root][INFO] - Training Epoch: 2/2, step 2500/7134 completed (loss: 0.10399486869573593, acc: 0.9864864945411682)
[2025-02-13 20:14:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:56][root][INFO] - Training Epoch: 2/2, step 2501/7134 completed (loss: 0.13740457594394684, acc: 0.966292142868042)
[2025-02-13 20:14:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:57][root][INFO] - Training Epoch: 2/2, step 2502/7134 completed (loss: 0.02840820699930191, acc: 1.0)
[2025-02-13 20:14:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:57][root][INFO] - Training Epoch: 2/2, step 2503/7134 completed (loss: 0.06528768688440323, acc: 0.97826087474823)
[2025-02-13 20:14:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:57][root][INFO] - Training Epoch: 2/2, step 2504/7134 completed (loss: 0.1292056143283844, acc: 0.9620253443717957)
[2025-02-13 20:14:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:58][root][INFO] - Training Epoch: 2/2, step 2505/7134 completed (loss: 0.08995798230171204, acc: 0.9767441749572754)
[2025-02-13 20:14:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:58][root][INFO] - Training Epoch: 2/2, step 2506/7134 completed (loss: 0.03981463238596916, acc: 1.0)
[2025-02-13 20:14:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:59][root][INFO] - Training Epoch: 2/2, step 2507/7134 completed (loss: 0.03551141545176506, acc: 0.9888888597488403)
[2025-02-13 20:14:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:59][root][INFO] - Training Epoch: 2/2, step 2508/7134 completed (loss: 0.09765549749135971, acc: 0.9830508232116699)
[2025-02-13 20:14:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:14:59][root][INFO] - Training Epoch: 2/2, step 2509/7134 completed (loss: 0.03691880404949188, acc: 0.9882352948188782)
[2025-02-13 20:15:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:00][root][INFO] - Training Epoch: 2/2, step 2510/7134 completed (loss: 0.03766319528222084, acc: 0.9857142567634583)
[2025-02-13 20:15:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:00][root][INFO] - Training Epoch: 2/2, step 2511/7134 completed (loss: 0.14119669795036316, acc: 0.9726775884628296)
[2025-02-13 20:15:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:01][root][INFO] - Training Epoch: 2/2, step 2512/7134 completed (loss: 0.13557922840118408, acc: 0.9529411792755127)
[2025-02-13 20:15:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:01][root][INFO] - Training Epoch: 2/2, step 2513/7134 completed (loss: 0.05124349892139435, acc: 0.9892473220825195)
[2025-02-13 20:15:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:01][root][INFO] - Training Epoch: 2/2, step 2514/7134 completed (loss: 0.058394238352775574, acc: 0.987261176109314)
[2025-02-13 20:15:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:02][root][INFO] - Training Epoch: 2/2, step 2515/7134 completed (loss: 0.04413362592458725, acc: 0.9870129823684692)
[2025-02-13 20:15:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:02][root][INFO] - Training Epoch: 2/2, step 2516/7134 completed (loss: 0.11631932854652405, acc: 0.9888268113136292)
[2025-02-13 20:15:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:03][root][INFO] - Training Epoch: 2/2, step 2517/7134 completed (loss: 0.0967579111456871, acc: 0.9833333492279053)
[2025-02-13 20:15:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:03][root][INFO] - Training Epoch: 2/2, step 2518/7134 completed (loss: 0.08470212668180466, acc: 0.9607843160629272)
[2025-02-13 20:15:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:04][root][INFO] - Training Epoch: 2/2, step 2519/7134 completed (loss: 0.020345309749245644, acc: 1.0)
[2025-02-13 20:15:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:04][root][INFO] - Training Epoch: 2/2, step 2520/7134 completed (loss: 0.046979039907455444, acc: 0.9934210777282715)
[2025-02-13 20:15:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:04][root][INFO] - Training Epoch: 2/2, step 2521/7134 completed (loss: 0.16301563382148743, acc: 0.9670329689979553)
[2025-02-13 20:15:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:05][root][INFO] - Training Epoch: 2/2, step 2522/7134 completed (loss: 0.04332038760185242, acc: 1.0)
[2025-02-13 20:15:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:05][root][INFO] - Training Epoch: 2/2, step 2523/7134 completed (loss: 0.09045811742544174, acc: 0.9754601120948792)
[2025-02-13 20:15:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:06][root][INFO] - Training Epoch: 2/2, step 2524/7134 completed (loss: 0.03250932693481445, acc: 0.9878048896789551)
[2025-02-13 20:15:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:06][root][INFO] - Training Epoch: 2/2, step 2525/7134 completed (loss: 0.07656031847000122, acc: 0.9794520735740662)
[2025-02-13 20:15:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:06][root][INFO] - Training Epoch: 2/2, step 2526/7134 completed (loss: 0.008873559534549713, acc: 1.0)
[2025-02-13 20:15:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:07][root][INFO] - Training Epoch: 2/2, step 2527/7134 completed (loss: 0.04560308903455734, acc: 0.994535505771637)
[2025-02-13 20:15:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:07][root][INFO] - Training Epoch: 2/2, step 2528/7134 completed (loss: 0.03646063059568405, acc: 0.9939758777618408)
[2025-02-13 20:15:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:08][root][INFO] - Training Epoch: 2/2, step 2529/7134 completed (loss: 0.06156530976295471, acc: 0.988095223903656)
[2025-02-13 20:15:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:08][root][INFO] - Training Epoch: 2/2, step 2530/7134 completed (loss: 0.05922674387693405, acc: 0.9887640476226807)
[2025-02-13 20:15:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:08][root][INFO] - Training Epoch: 2/2, step 2531/7134 completed (loss: 0.03371570631861687, acc: 0.9939758777618408)
[2025-02-13 20:15:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:09][root][INFO] - Training Epoch: 2/2, step 2532/7134 completed (loss: 0.029133211821317673, acc: 1.0)
[2025-02-13 20:15:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:09][root][INFO] - Training Epoch: 2/2, step 2533/7134 completed (loss: 0.06440338492393494, acc: 0.9807692170143127)
[2025-02-13 20:15:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:10][root][INFO] - Training Epoch: 2/2, step 2534/7134 completed (loss: 0.07327108830213547, acc: 0.9832402467727661)
[2025-02-13 20:15:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:10][root][INFO] - Training Epoch: 2/2, step 2535/7134 completed (loss: 0.028815923258662224, acc: 1.0)
[2025-02-13 20:15:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:10][root][INFO] - Training Epoch: 2/2, step 2536/7134 completed (loss: 0.05959346890449524, acc: 0.984455943107605)
[2025-02-13 20:15:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:11][root][INFO] - Training Epoch: 2/2, step 2537/7134 completed (loss: 0.08203936368227005, acc: 0.9794520735740662)
[2025-02-13 20:15:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:11][root][INFO] - Training Epoch: 2/2, step 2538/7134 completed (loss: 0.1488949954509735, acc: 0.977011501789093)
[2025-02-13 20:15:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:12][root][INFO] - Training Epoch: 2/2, step 2539/7134 completed (loss: 0.09995871037244797, acc: 0.9849624037742615)
[2025-02-13 20:15:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:12][root][INFO] - Training Epoch: 2/2, step 2540/7134 completed (loss: 0.07560135424137115, acc: 0.9801324605941772)
[2025-02-13 20:15:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:12][root][INFO] - Training Epoch: 2/2, step 2541/7134 completed (loss: 0.0311652272939682, acc: 1.0)
[2025-02-13 20:15:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:13][root][INFO] - Training Epoch: 2/2, step 2542/7134 completed (loss: 0.09862534701824188, acc: 0.9750000238418579)
[2025-02-13 20:15:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:13][root][INFO] - Training Epoch: 2/2, step 2543/7134 completed (loss: 0.10516524314880371, acc: 0.9748427867889404)
[2025-02-13 20:15:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:14][root][INFO] - Training Epoch: 2/2, step 2544/7134 completed (loss: 0.07351423054933548, acc: 0.9702380895614624)
[2025-02-13 20:15:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:14][root][INFO] - Training Epoch: 2/2, step 2545/7134 completed (loss: 0.03479590266942978, acc: 0.9925373196601868)
[2025-02-13 20:15:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:14][root][INFO] - Training Epoch: 2/2, step 2546/7134 completed (loss: 0.20754612982273102, acc: 0.9336734414100647)
[2025-02-13 20:15:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:15][root][INFO] - Training Epoch: 2/2, step 2547/7134 completed (loss: 0.13138385117053986, acc: 0.9568345546722412)
[2025-02-13 20:15:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:15][root][INFO] - Training Epoch: 2/2, step 2548/7134 completed (loss: 0.02349849045276642, acc: 1.0)
[2025-02-13 20:15:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:15][root][INFO] - Training Epoch: 2/2, step 2549/7134 completed (loss: 0.05138253793120384, acc: 0.9918699264526367)
[2025-02-13 20:15:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:16][root][INFO] - Training Epoch: 2/2, step 2550/7134 completed (loss: 0.02695300243794918, acc: 0.9863013625144958)
[2025-02-13 20:15:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:16][root][INFO] - Training Epoch: 2/2, step 2551/7134 completed (loss: 0.13284863531589508, acc: 0.9710144996643066)
[2025-02-13 20:15:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:17][root][INFO] - Training Epoch: 2/2, step 2552/7134 completed (loss: 0.013369562104344368, acc: 1.0)
[2025-02-13 20:15:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:17][root][INFO] - Training Epoch: 2/2, step 2553/7134 completed (loss: 0.08610368520021439, acc: 0.9868420958518982)
[2025-02-13 20:15:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:17][root][INFO] - Training Epoch: 2/2, step 2554/7134 completed (loss: 0.1492813676595688, acc: 0.9763779640197754)
[2025-02-13 20:15:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:18][root][INFO] - Training Epoch: 2/2, step 2555/7134 completed (loss: 0.04995567724108696, acc: 0.9909909963607788)
[2025-02-13 20:15:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:18][root][INFO] - Training Epoch: 2/2, step 2556/7134 completed (loss: 0.06652677804231644, acc: 0.9900000095367432)
[2025-02-13 20:15:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:19][root][INFO] - Training Epoch: 2/2, step 2557/7134 completed (loss: 0.08055025339126587, acc: 0.9795918464660645)
[2025-02-13 20:15:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:19][root][INFO] - Training Epoch: 2/2, step 2558/7134 completed (loss: 0.027642566710710526, acc: 1.0)
[2025-02-13 20:15:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:19][root][INFO] - Training Epoch: 2/2, step 2559/7134 completed (loss: 0.06491781026124954, acc: 0.981249988079071)
[2025-02-13 20:15:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:20][root][INFO] - Training Epoch: 2/2, step 2560/7134 completed (loss: 0.0622144378721714, acc: 0.9797297120094299)
[2025-02-13 20:15:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:20][root][INFO] - Training Epoch: 2/2, step 2561/7134 completed (loss: 0.03417183831334114, acc: 1.0)
[2025-02-13 20:15:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:21][root][INFO] - Training Epoch: 2/2, step 2562/7134 completed (loss: 0.023642608895897865, acc: 0.9919999837875366)
[2025-02-13 20:15:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:21][root][INFO] - Training Epoch: 2/2, step 2563/7134 completed (loss: 0.0693436861038208, acc: 0.9940828680992126)
[2025-02-13 20:15:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:21][root][INFO] - Training Epoch: 2/2, step 2564/7134 completed (loss: 0.18795345723628998, acc: 0.9568345546722412)
[2025-02-13 20:15:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:22][root][INFO] - Training Epoch: 2/2, step 2565/7134 completed (loss: 0.04998531565070152, acc: 0.9928057789802551)
[2025-02-13 20:15:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:22][root][INFO] - Training Epoch: 2/2, step 2566/7134 completed (loss: 0.0401621051132679, acc: 1.0)
[2025-02-13 20:15:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:22][root][INFO] - Training Epoch: 2/2, step 2567/7134 completed (loss: 0.04758762568235397, acc: 0.9924812316894531)
[2025-02-13 20:15:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:23][root][INFO] - Training Epoch: 2/2, step 2568/7134 completed (loss: 0.11194006353616714, acc: 0.9871794581413269)
[2025-02-13 20:15:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:23][root][INFO] - Training Epoch: 2/2, step 2569/7134 completed (loss: 0.03192134201526642, acc: 0.98591548204422)
[2025-02-13 20:15:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:24][root][INFO] - Training Epoch: 2/2, step 2570/7134 completed (loss: 0.020200615748763084, acc: 1.0)
[2025-02-13 20:15:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:24][root][INFO] - Training Epoch: 2/2, step 2571/7134 completed (loss: 0.08388132601976395, acc: 0.9836065769195557)
[2025-02-13 20:15:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:25][root][INFO] - Training Epoch: 2/2, step 2572/7134 completed (loss: 0.08709434419870377, acc: 0.9795918464660645)
[2025-02-13 20:15:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:25][root][INFO] - Training Epoch: 2/2, step 2573/7134 completed (loss: 0.05868399888277054, acc: 0.9928057789802551)
[2025-02-13 20:15:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:25][root][INFO] - Training Epoch: 2/2, step 2574/7134 completed (loss: 0.02166368067264557, acc: 1.0)
[2025-02-13 20:15:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:26][root][INFO] - Training Epoch: 2/2, step 2575/7134 completed (loss: 0.11802016943693161, acc: 0.9918032884597778)
[2025-02-13 20:15:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:26][root][INFO] - Training Epoch: 2/2, step 2576/7134 completed (loss: 0.04077991843223572, acc: 0.9918699264526367)
[2025-02-13 20:15:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:27][root][INFO] - Training Epoch: 2/2, step 2577/7134 completed (loss: 0.1882970929145813, acc: 0.9763779640197754)
[2025-02-13 20:15:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:27][root][INFO] - Training Epoch: 2/2, step 2578/7134 completed (loss: 0.21902532875537872, acc: 0.9411764740943909)
[2025-02-13 20:15:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:27][root][INFO] - Training Epoch: 2/2, step 2579/7134 completed (loss: 0.1804378181695938, acc: 0.9487179517745972)
[2025-02-13 20:15:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:28][root][INFO] - Training Epoch: 2/2, step 2580/7134 completed (loss: 0.09549623727798462, acc: 0.9768785834312439)
[2025-02-13 20:15:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:28][root][INFO] - Training Epoch: 2/2, step 2581/7134 completed (loss: 0.19236890971660614, acc: 0.9661017060279846)
[2025-02-13 20:15:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:29][root][INFO] - Training Epoch: 2/2, step 2582/7134 completed (loss: 0.22120067477226257, acc: 0.9604519605636597)
[2025-02-13 20:15:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:29][root][INFO] - Training Epoch: 2/2, step 2583/7134 completed (loss: 0.15959767997264862, acc: 0.9482758641242981)
[2025-02-13 20:15:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:29][root][INFO] - Training Epoch: 2/2, step 2584/7134 completed (loss: 0.21310864388942719, acc: 0.957446813583374)
[2025-02-13 20:15:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:30][root][INFO] - Training Epoch: 2/2, step 2585/7134 completed (loss: 0.2001098245382309, acc: 0.9602649211883545)
[2025-02-13 20:15:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:30][root][INFO] - Training Epoch: 2/2, step 2586/7134 completed (loss: 0.1387774497270584, acc: 0.9538461565971375)
[2025-02-13 20:15:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:30][root][INFO] - Training Epoch: 2/2, step 2587/7134 completed (loss: 0.0956353172659874, acc: 0.9750000238418579)
[2025-02-13 20:15:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:31][root][INFO] - Training Epoch: 2/2, step 2588/7134 completed (loss: 0.05326681584119797, acc: 0.9863945841789246)
[2025-02-13 20:15:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:31][root][INFO] - Training Epoch: 2/2, step 2589/7134 completed (loss: 0.20614418387413025, acc: 0.9444444179534912)
[2025-02-13 20:15:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:32][root][INFO] - Training Epoch: 2/2, step 2590/7134 completed (loss: 0.09653220325708389, acc: 0.970370352268219)
[2025-02-13 20:15:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:32][root][INFO] - Training Epoch: 2/2, step 2591/7134 completed (loss: 0.24008043110370636, acc: 0.9356725215911865)
[2025-02-13 20:15:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:32][root][INFO] - Training Epoch: 2/2, step 2592/7134 completed (loss: 0.18871764838695526, acc: 0.9553072452545166)
[2025-02-13 20:15:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:33][root][INFO] - Training Epoch: 2/2, step 2593/7134 completed (loss: 0.11992346495389938, acc: 0.9817073345184326)
[2025-02-13 20:15:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:33][root][INFO] - Training Epoch: 2/2, step 2594/7134 completed (loss: 0.12700733542442322, acc: 0.9673202633857727)
[2025-02-13 20:15:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:34][root][INFO] - Training Epoch: 2/2, step 2595/7134 completed (loss: 0.06576038897037506, acc: 0.9939024448394775)
[2025-02-13 20:15:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:34][root][INFO] - Training Epoch: 2/2, step 2596/7134 completed (loss: 0.10208704322576523, acc: 0.9764705896377563)
[2025-02-13 20:15:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:34][root][INFO] - Training Epoch: 2/2, step 2597/7134 completed (loss: 0.1449572890996933, acc: 0.9717513918876648)
[2025-02-13 20:15:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:35][root][INFO] - Training Epoch: 2/2, step 2598/7134 completed (loss: 0.15562336146831512, acc: 0.9536423683166504)
[2025-02-13 20:15:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:35][root][INFO] - Training Epoch: 2/2, step 2599/7134 completed (loss: 0.055618707090616226, acc: 0.9937106966972351)
[2025-02-13 20:15:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:35][root][INFO] - Training Epoch: 2/2, step 2600/7134 completed (loss: 0.1584990918636322, acc: 0.9719101190567017)
[2025-02-13 20:15:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:36][root][INFO] - Training Epoch: 2/2, step 2601/7134 completed (loss: 0.13894878327846527, acc: 0.9741935729980469)
[2025-02-13 20:15:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:36][root][INFO] - Training Epoch: 2/2, step 2602/7134 completed (loss: 0.08987520635128021, acc: 0.9702380895614624)
[2025-02-13 20:15:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:37][root][INFO] - Training Epoch: 2/2, step 2603/7134 completed (loss: 0.0751853734254837, acc: 0.9934640526771545)
[2025-02-13 20:15:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:37][root][INFO] - Training Epoch: 2/2, step 2604/7134 completed (loss: 0.0891270637512207, acc: 0.9788359999656677)
[2025-02-13 20:15:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:37][root][INFO] - Training Epoch: 2/2, step 2605/7134 completed (loss: 0.0730779841542244, acc: 0.991150438785553)
[2025-02-13 20:15:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:38][root][INFO] - Training Epoch: 2/2, step 2606/7134 completed (loss: 0.10107090324163437, acc: 0.9733333587646484)
[2025-02-13 20:15:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:38][root][INFO] - Training Epoch: 2/2, step 2607/7134 completed (loss: 0.07786086201667786, acc: 0.9887640476226807)
[2025-02-13 20:15:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:39][root][INFO] - Training Epoch: 2/2, step 2608/7134 completed (loss: 0.10555040091276169, acc: 0.9841269850730896)
[2025-02-13 20:15:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:39][root][INFO] - Training Epoch: 2/2, step 2609/7134 completed (loss: 0.16669818758964539, acc: 0.9738562107086182)
[2025-02-13 20:15:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:39][root][INFO] - Training Epoch: 2/2, step 2610/7134 completed (loss: 0.10644785314798355, acc: 0.9784172773361206)
[2025-02-13 20:15:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:40][root][INFO] - Training Epoch: 2/2, step 2611/7134 completed (loss: 0.22401873767375946, acc: 0.9553072452545166)
[2025-02-13 20:15:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:40][root][INFO] - Training Epoch: 2/2, step 2612/7134 completed (loss: 0.12656816840171814, acc: 0.9599999785423279)
[2025-02-13 20:15:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:41][root][INFO] - Training Epoch: 2/2, step 2613/7134 completed (loss: 0.13689777255058289, acc: 0.9642857313156128)
[2025-02-13 20:15:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:41][root][INFO] - Training Epoch: 2/2, step 2614/7134 completed (loss: 0.17489853501319885, acc: 0.9627659320831299)
[2025-02-13 20:15:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:42][root][INFO] - Training Epoch: 2/2, step 2615/7134 completed (loss: 0.14680135250091553, acc: 0.983146071434021)
[2025-02-13 20:15:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:42][root][INFO] - Training Epoch: 2/2, step 2616/7134 completed (loss: 0.16682575643062592, acc: 0.9831932783126831)
[2025-02-13 20:15:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:42][root][INFO] - Training Epoch: 2/2, step 2617/7134 completed (loss: 0.3203754723072052, acc: 0.9289340376853943)
[2025-02-13 20:15:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:43][root][INFO] - Training Epoch: 2/2, step 2618/7134 completed (loss: 0.06856931746006012, acc: 0.9743589758872986)
[2025-02-13 20:15:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:43][root][INFO] - Training Epoch: 2/2, step 2619/7134 completed (loss: 0.09440044313669205, acc: 0.9637681245803833)
[2025-02-13 20:15:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:44][root][INFO] - Training Epoch: 2/2, step 2620/7134 completed (loss: 0.1120191216468811, acc: 0.9735449552536011)
[2025-02-13 20:15:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:44][root][INFO] - Training Epoch: 2/2, step 2621/7134 completed (loss: 0.2254505306482315, acc: 0.9411764740943909)
[2025-02-13 20:15:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:44][root][INFO] - Training Epoch: 2/2, step 2622/7134 completed (loss: 0.1378176510334015, acc: 0.9520547986030579)
[2025-02-13 20:15:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:45][root][INFO] - Training Epoch: 2/2, step 2623/7134 completed (loss: 0.1896725594997406, acc: 0.9449541568756104)
[2025-02-13 20:15:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:45][root][INFO] - Training Epoch: 2/2, step 2624/7134 completed (loss: 0.21759003400802612, acc: 0.9428571462631226)
[2025-02-13 20:15:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:46][root][INFO] - Training Epoch: 2/2, step 2625/7134 completed (loss: 0.19670291244983673, acc: 0.9464285969734192)
[2025-02-13 20:15:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:46][root][INFO] - Training Epoch: 2/2, step 2626/7134 completed (loss: 0.07154495269060135, acc: 0.9885057210922241)
[2025-02-13 20:15:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:47][root][INFO] - Training Epoch: 2/2, step 2627/7134 completed (loss: 0.11454935371875763, acc: 0.9815950989723206)
[2025-02-13 20:15:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:47][root][INFO] - Training Epoch: 2/2, step 2628/7134 completed (loss: 0.21041607856750488, acc: 0.9666666388511658)
[2025-02-13 20:15:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:47][root][INFO] - Training Epoch: 2/2, step 2629/7134 completed (loss: 0.07439795136451721, acc: 0.9880239367485046)
[2025-02-13 20:15:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:48][root][INFO] - Training Epoch: 2/2, step 2630/7134 completed (loss: 0.10471111536026001, acc: 0.9710144996643066)
[2025-02-13 20:15:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:48][root][INFO] - Training Epoch: 2/2, step 2631/7134 completed (loss: 0.13925185799598694, acc: 0.9746192693710327)
[2025-02-13 20:15:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:49][root][INFO] - Training Epoch: 2/2, step 2632/7134 completed (loss: 0.040260326117277145, acc: 0.9826086759567261)
[2025-02-13 20:15:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:49][root][INFO] - Training Epoch: 2/2, step 2633/7134 completed (loss: 0.18483687937259674, acc: 0.9505494236946106)
[2025-02-13 20:15:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:49][root][INFO] - Training Epoch: 2/2, step 2634/7134 completed (loss: 0.04675609990954399, acc: 0.9940828680992126)
[2025-02-13 20:15:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:50][root][INFO] - Training Epoch: 2/2, step 2635/7134 completed (loss: 0.2791078984737396, acc: 0.9388889074325562)
[2025-02-13 20:15:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:50][root][INFO] - Training Epoch: 2/2, step 2636/7134 completed (loss: 0.17540283501148224, acc: 0.9497206807136536)
[2025-02-13 20:15:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:51][root][INFO] - Training Epoch: 2/2, step 2637/7134 completed (loss: 0.30284541845321655, acc: 0.9202454090118408)
[2025-02-13 20:15:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:51][root][INFO] - Training Epoch: 2/2, step 2638/7134 completed (loss: 0.25627049803733826, acc: 0.9402984976768494)
[2025-02-13 20:15:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:51][root][INFO] - Training Epoch: 2/2, step 2639/7134 completed (loss: 0.4164181649684906, acc: 0.9097744226455688)
[2025-02-13 20:15:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:52][root][INFO] - Training Epoch: 2/2, step 2640/7134 completed (loss: 0.3815464675426483, acc: 0.9353233575820923)
[2025-02-13 20:15:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:52][root][INFO] - Training Epoch: 2/2, step 2641/7134 completed (loss: 0.16421599686145782, acc: 0.977142870426178)
[2025-02-13 20:15:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:53][root][INFO] - Training Epoch: 2/2, step 2642/7134 completed (loss: 0.49766331911087036, acc: 0.9107142686843872)
[2025-02-13 20:15:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:53][root][INFO] - Training Epoch: 2/2, step 2643/7134 completed (loss: 0.3254142701625824, acc: 0.9476743936538696)
[2025-02-13 20:15:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:53][root][INFO] - Training Epoch: 2/2, step 2644/7134 completed (loss: 0.18888765573501587, acc: 0.9629629850387573)
[2025-02-13 20:15:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:54][root][INFO] - Training Epoch: 2/2, step 2645/7134 completed (loss: 0.16472128033638, acc: 0.954285740852356)
[2025-02-13 20:15:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:54][root][INFO] - Training Epoch: 2/2, step 2646/7134 completed (loss: 0.32391706109046936, acc: 0.9130434989929199)
[2025-02-13 20:15:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:55][root][INFO] - Training Epoch: 2/2, step 2647/7134 completed (loss: 0.23984093964099884, acc: 0.945652186870575)
[2025-02-13 20:15:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:55][root][INFO] - Training Epoch: 2/2, step 2648/7134 completed (loss: 0.3055664002895355, acc: 0.9306930899620056)
[2025-02-13 20:15:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:56][root][INFO] - Training Epoch: 2/2, step 2649/7134 completed (loss: 0.34315934777259827, acc: 0.8972973227500916)
[2025-02-13 20:15:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:56][root][INFO] - Training Epoch: 2/2, step 2650/7134 completed (loss: 0.35655367374420166, acc: 0.9235293865203857)
[2025-02-13 20:15:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:56][root][INFO] - Training Epoch: 2/2, step 2651/7134 completed (loss: 0.14533886313438416, acc: 0.9658536314964294)
[2025-02-13 20:15:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:57][root][INFO] - Training Epoch: 2/2, step 2652/7134 completed (loss: 0.10985460877418518, acc: 0.9719101190567017)
[2025-02-13 20:15:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:57][root][INFO] - Training Epoch: 2/2, step 2653/7134 completed (loss: 0.0799858570098877, acc: 0.9869281053543091)
[2025-02-13 20:15:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:58][root][INFO] - Training Epoch: 2/2, step 2654/7134 completed (loss: 0.07836978882551193, acc: 0.9714285731315613)
[2025-02-13 20:15:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:58][root][INFO] - Training Epoch: 2/2, step 2655/7134 completed (loss: 0.08905930817127228, acc: 0.9689440727233887)
[2025-02-13 20:15:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:58][root][INFO] - Training Epoch: 2/2, step 2656/7134 completed (loss: 0.23659829795360565, acc: 0.9593023061752319)
[2025-02-13 20:15:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:59][root][INFO] - Training Epoch: 2/2, step 2657/7134 completed (loss: 0.05817210674285889, acc: 0.9873417615890503)
[2025-02-13 20:15:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:59][root][INFO] - Training Epoch: 2/2, step 2658/7134 completed (loss: 0.2536751627922058, acc: 0.9292929172515869)
[2025-02-13 20:15:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:15:59][root][INFO] - Training Epoch: 2/2, step 2659/7134 completed (loss: 0.10954093188047409, acc: 0.9850746393203735)
[2025-02-13 20:16:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:00][root][INFO] - Training Epoch: 2/2, step 2660/7134 completed (loss: 0.22991590201854706, acc: 0.94017094373703)
[2025-02-13 20:16:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:00][root][INFO] - Training Epoch: 2/2, step 2661/7134 completed (loss: 0.05874449387192726, acc: 0.9822485446929932)
[2025-02-13 20:16:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:01][root][INFO] - Training Epoch: 2/2, step 2662/7134 completed (loss: 0.06642808020114899, acc: 0.9876543283462524)
[2025-02-13 20:16:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:01][root][INFO] - Training Epoch: 2/2, step 2663/7134 completed (loss: 0.10582824051380157, acc: 0.9689119458198547)
[2025-02-13 20:16:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:01][root][INFO] - Training Epoch: 2/2, step 2664/7134 completed (loss: 0.08386018872261047, acc: 0.9825581312179565)
[2025-02-13 20:16:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:02][root][INFO] - Training Epoch: 2/2, step 2665/7134 completed (loss: 0.05848704278469086, acc: 0.9819276928901672)
[2025-02-13 20:16:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:02][root][INFO] - Training Epoch: 2/2, step 2666/7134 completed (loss: 0.025936542078852654, acc: 1.0)
[2025-02-13 20:16:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:03][root][INFO] - Training Epoch: 2/2, step 2667/7134 completed (loss: 0.02500961720943451, acc: 1.0)
[2025-02-13 20:16:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:03][root][INFO] - Training Epoch: 2/2, step 2668/7134 completed (loss: 0.0740574300289154, acc: 0.9885714054107666)
[2025-02-13 20:16:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:04][root][INFO] - Training Epoch: 2/2, step 2669/7134 completed (loss: 0.01418122835457325, acc: 1.0)
[2025-02-13 20:16:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:04][root][INFO] - Training Epoch: 2/2, step 2670/7134 completed (loss: 0.04824914038181305, acc: 0.9828571677207947)
[2025-02-13 20:16:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:04][root][INFO] - Training Epoch: 2/2, step 2671/7134 completed (loss: 0.07324138283729553, acc: 0.977142870426178)
[2025-02-13 20:16:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:05][root][INFO] - Training Epoch: 2/2, step 2672/7134 completed (loss: 0.021935252472758293, acc: 0.9939758777618408)
[2025-02-13 20:16:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:05][root][INFO] - Training Epoch: 2/2, step 2673/7134 completed (loss: 0.028692912310361862, acc: 0.9944751262664795)
[2025-02-13 20:16:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:06][root][INFO] - Training Epoch: 2/2, step 2674/7134 completed (loss: 0.07801475375890732, acc: 0.9814814925193787)
[2025-02-13 20:16:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:06][root][INFO] - Training Epoch: 2/2, step 2675/7134 completed (loss: 0.01987147517502308, acc: 1.0)
[2025-02-13 20:16:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:06][root][INFO] - Training Epoch: 2/2, step 2676/7134 completed (loss: 0.04306003078818321, acc: 0.9943181872367859)
[2025-02-13 20:16:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:07][root][INFO] - Training Epoch: 2/2, step 2677/7134 completed (loss: 0.025234360247850418, acc: 1.0)
[2025-02-13 20:16:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:07][root][INFO] - Training Epoch: 2/2, step 2678/7134 completed (loss: 0.02250353991985321, acc: 0.994350254535675)
[2025-02-13 20:16:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:07][root][INFO] - Training Epoch: 2/2, step 2679/7134 completed (loss: 0.0197866540402174, acc: 0.9949495196342468)
[2025-02-13 20:16:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:08][root][INFO] - Training Epoch: 2/2, step 2680/7134 completed (loss: 0.04720922186970711, acc: 0.9835164546966553)
[2025-02-13 20:16:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:08][root][INFO] - Training Epoch: 2/2, step 2681/7134 completed (loss: 0.039646588265895844, acc: 0.9873417615890503)
[2025-02-13 20:16:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:09][root][INFO] - Training Epoch: 2/2, step 2682/7134 completed (loss: 0.06092566251754761, acc: 0.9731543660163879)
[2025-02-13 20:16:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:09][root][INFO] - Training Epoch: 2/2, step 2683/7134 completed (loss: 0.02758411131799221, acc: 0.9933775067329407)
[2025-02-13 20:16:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:10][root][INFO] - Training Epoch: 2/2, step 2684/7134 completed (loss: 0.1255902796983719, acc: 0.96875)
[2025-02-13 20:16:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:10][root][INFO] - Training Epoch: 2/2, step 2685/7134 completed (loss: 0.03049049712717533, acc: 1.0)
[2025-02-13 20:16:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:10][root][INFO] - Training Epoch: 2/2, step 2686/7134 completed (loss: 0.08641644567251205, acc: 0.9750000238418579)
[2025-02-13 20:16:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:11][root][INFO] - Training Epoch: 2/2, step 2687/7134 completed (loss: 0.12973685562610626, acc: 0.9831932783126831)
[2025-02-13 20:16:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:11][root][INFO] - Training Epoch: 2/2, step 2688/7134 completed (loss: 0.09443818777799606, acc: 0.9774436354637146)
[2025-02-13 20:16:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:12][root][INFO] - Training Epoch: 2/2, step 2689/7134 completed (loss: 0.100617915391922, acc: 0.9756097793579102)
[2025-02-13 20:16:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:12][root][INFO] - Training Epoch: 2/2, step 2690/7134 completed (loss: 0.09120691567659378, acc: 1.0)
[2025-02-13 20:16:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:12][root][INFO] - Training Epoch: 2/2, step 2691/7134 completed (loss: 0.19054840505123138, acc: 0.9380530714988708)
[2025-02-13 20:16:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:13][root][INFO] - Training Epoch: 2/2, step 2692/7134 completed (loss: 0.2064630687236786, acc: 0.942148745059967)
[2025-02-13 20:16:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:13][root][INFO] - Training Epoch: 2/2, step 2693/7134 completed (loss: 0.21974067389965057, acc: 0.9646017551422119)
[2025-02-13 20:16:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:13][root][INFO] - Training Epoch: 2/2, step 2694/7134 completed (loss: 0.2568033039569855, acc: 0.9271523356437683)
[2025-02-13 20:16:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:14][root][INFO] - Training Epoch: 2/2, step 2695/7134 completed (loss: 0.08188523352146149, acc: 0.9777777791023254)
[2025-02-13 20:16:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:14][root][INFO] - Training Epoch: 2/2, step 2696/7134 completed (loss: 0.14656586945056915, acc: 0.9571428298950195)
[2025-02-13 20:16:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:15][root][INFO] - Training Epoch: 2/2, step 2697/7134 completed (loss: 0.21323107182979584, acc: 0.9415584206581116)
[2025-02-13 20:16:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:15][root][INFO] - Training Epoch: 2/2, step 2698/7134 completed (loss: 0.23348036408424377, acc: 0.9455782175064087)
[2025-02-13 20:16:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:16][root][INFO] - Training Epoch: 2/2, step 2699/7134 completed (loss: 0.05529481917619705, acc: 0.9913793206214905)
[2025-02-13 20:16:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:16][root][INFO] - Training Epoch: 2/2, step 2700/7134 completed (loss: 0.05758460611104965, acc: 0.9857142567634583)
[2025-02-13 20:16:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:16][root][INFO] - Training Epoch: 2/2, step 2701/7134 completed (loss: 0.11391769349575043, acc: 0.9774436354637146)
[2025-02-13 20:16:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:17][root][INFO] - Training Epoch: 2/2, step 2702/7134 completed (loss: 0.27928996086120605, acc: 0.9444444179534912)
[2025-02-13 20:16:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:17][root][INFO] - Training Epoch: 2/2, step 2703/7134 completed (loss: 0.04157590866088867, acc: 0.9866666793823242)
[2025-02-13 20:16:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:18][root][INFO] - Training Epoch: 2/2, step 2704/7134 completed (loss: 0.1749885380268097, acc: 0.9642857313156128)
[2025-02-13 20:16:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:18][root][INFO] - Training Epoch: 2/2, step 2705/7134 completed (loss: 0.23527516424655914, acc: 0.9436619877815247)
[2025-02-13 20:16:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:18][root][INFO] - Training Epoch: 2/2, step 2706/7134 completed (loss: 0.0900757759809494, acc: 0.9931507110595703)
[2025-02-13 20:16:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:19][root][INFO] - Training Epoch: 2/2, step 2707/7134 completed (loss: 0.14142508804798126, acc: 0.9666666388511658)
[2025-02-13 20:16:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:19][root][INFO] - Training Epoch: 2/2, step 2708/7134 completed (loss: 0.15933528542518616, acc: 0.9599999785423279)
[2025-02-13 20:16:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:20][root][INFO] - Training Epoch: 2/2, step 2709/7134 completed (loss: 0.08393656462430954, acc: 0.9647887349128723)
[2025-02-13 20:16:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:20][root][INFO] - Training Epoch: 2/2, step 2710/7134 completed (loss: 0.09459300339221954, acc: 0.9800000190734863)
[2025-02-13 20:16:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:21][root][INFO] - Training Epoch: 2/2, step 2711/7134 completed (loss: 0.07273881137371063, acc: 0.9793814420700073)
[2025-02-13 20:16:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:21][root][INFO] - Training Epoch: 2/2, step 2712/7134 completed (loss: 0.10039685666561127, acc: 0.978723406791687)
[2025-02-13 20:16:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:21][root][INFO] - Training Epoch: 2/2, step 2713/7134 completed (loss: 0.10438716411590576, acc: 0.9750000238418579)
[2025-02-13 20:16:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:22][root][INFO] - Training Epoch: 2/2, step 2714/7134 completed (loss: 0.04792894050478935, acc: 0.9779411554336548)
[2025-02-13 20:16:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:22][root][INFO] - Training Epoch: 2/2, step 2715/7134 completed (loss: 0.07244788110256195, acc: 0.9793103337287903)
[2025-02-13 20:16:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:23][root][INFO] - Training Epoch: 2/2, step 2716/7134 completed (loss: 0.053207363933324814, acc: 0.9883720874786377)
[2025-02-13 20:16:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:23][root][INFO] - Training Epoch: 2/2, step 2717/7134 completed (loss: 0.04807518795132637, acc: 1.0)
[2025-02-13 20:16:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:23][root][INFO] - Training Epoch: 2/2, step 2718/7134 completed (loss: 0.22154200077056885, acc: 0.9305555820465088)
[2025-02-13 20:16:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:24][root][INFO] - Training Epoch: 2/2, step 2719/7134 completed (loss: 0.14911574125289917, acc: 0.9856114983558655)
[2025-02-13 20:16:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:24][root][INFO] - Training Epoch: 2/2, step 2720/7134 completed (loss: 0.20223267376422882, acc: 0.9489051103591919)
[2025-02-13 20:16:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:25][root][INFO] - Training Epoch: 2/2, step 2721/7134 completed (loss: 0.13173089921474457, acc: 0.9801324605941772)
[2025-02-13 20:16:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:25][root][INFO] - Training Epoch: 2/2, step 2722/7134 completed (loss: 0.1356774866580963, acc: 0.978723406791687)
[2025-02-13 20:16:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:25][root][INFO] - Training Epoch: 2/2, step 2723/7134 completed (loss: 0.30763188004493713, acc: 0.9520958065986633)
[2025-02-13 20:16:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:26][root][INFO] - Training Epoch: 2/2, step 2724/7134 completed (loss: 0.060182876884937286, acc: 0.9887005686759949)
[2025-02-13 20:16:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:26][root][INFO] - Training Epoch: 2/2, step 2725/7134 completed (loss: 0.0726601853966713, acc: 0.9701492786407471)
[2025-02-13 20:16:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:27][root][INFO] - Training Epoch: 2/2, step 2726/7134 completed (loss: 0.11420820653438568, acc: 0.9593495726585388)
[2025-02-13 20:16:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:27][root][INFO] - Training Epoch: 2/2, step 2727/7134 completed (loss: 0.037976909428834915, acc: 0.9912280440330505)
[2025-02-13 20:16:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:27][root][INFO] - Training Epoch: 2/2, step 2728/7134 completed (loss: 0.5983225703239441, acc: 0.8296296000480652)
[2025-02-13 20:16:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:28][root][INFO] - Training Epoch: 2/2, step 2729/7134 completed (loss: 0.44774407148361206, acc: 0.9071428775787354)
[2025-02-13 20:16:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:28][root][INFO] - Training Epoch: 2/2, step 2730/7134 completed (loss: 0.1654035449028015, acc: 0.9646017551422119)
[2025-02-13 20:16:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:28][root][INFO] - Training Epoch: 2/2, step 2731/7134 completed (loss: 0.19826586544513702, acc: 0.9611650705337524)
[2025-02-13 20:16:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:29][root][INFO] - Training Epoch: 2/2, step 2732/7134 completed (loss: 0.29684606194496155, acc: 0.9264705777168274)
[2025-02-13 20:16:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:29][root][INFO] - Training Epoch: 2/2, step 2733/7134 completed (loss: 0.21553966403007507, acc: 0.949999988079071)
[2025-02-13 20:16:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:30][root][INFO] - Training Epoch: 2/2, step 2734/7134 completed (loss: 0.22271887958049774, acc: 0.9669421315193176)
[2025-02-13 20:16:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:30][root][INFO] - Training Epoch: 2/2, step 2735/7134 completed (loss: 0.1757756620645523, acc: 0.9634146094322205)
[2025-02-13 20:16:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:30][root][INFO] - Training Epoch: 2/2, step 2736/7134 completed (loss: 0.2506645917892456, acc: 0.9433962106704712)
[2025-02-13 20:16:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:31][root][INFO] - Training Epoch: 2/2, step 2737/7134 completed (loss: 0.2180173248052597, acc: 0.9571428298950195)
[2025-02-13 20:16:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:31][root][INFO] - Training Epoch: 2/2, step 2738/7134 completed (loss: 0.2197897732257843, acc: 0.940119743347168)
[2025-02-13 20:16:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:32][root][INFO] - Training Epoch: 2/2, step 2739/7134 completed (loss: 0.18699981272220612, acc: 0.95652174949646)
[2025-02-13 20:16:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:32][root][INFO] - Training Epoch: 2/2, step 2740/7134 completed (loss: 0.1677880436182022, acc: 0.9591836929321289)
[2025-02-13 20:16:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:33][root][INFO] - Training Epoch: 2/2, step 2741/7134 completed (loss: 0.08101895451545715, acc: 0.9913793206214905)
[2025-02-13 20:16:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:33][root][INFO] - Training Epoch: 2/2, step 2742/7134 completed (loss: 0.0837320014834404, acc: 0.9663865566253662)
[2025-02-13 20:16:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:33][root][INFO] - Training Epoch: 2/2, step 2743/7134 completed (loss: 0.09338745474815369, acc: 0.978723406791687)
[2025-02-13 20:16:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:34][root][INFO] - Training Epoch: 2/2, step 2744/7134 completed (loss: 0.04154285043478012, acc: 1.0)
[2025-02-13 20:16:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:34][root][INFO] - Training Epoch: 2/2, step 2745/7134 completed (loss: 0.4016368091106415, acc: 0.886904776096344)
[2025-02-13 20:16:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:34][root][INFO] - Training Epoch: 2/2, step 2746/7134 completed (loss: 0.19559413194656372, acc: 0.9545454382896423)
[2025-02-13 20:16:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:35][root][INFO] - Training Epoch: 2/2, step 2747/7134 completed (loss: 0.19980968534946442, acc: 0.9629629850387573)
[2025-02-13 20:16:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:35][root][INFO] - Training Epoch: 2/2, step 2748/7134 completed (loss: 0.04524974524974823, acc: 0.9870129823684692)
[2025-02-13 20:16:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:36][root][INFO] - Training Epoch: 2/2, step 2749/7134 completed (loss: 0.17079918086528778, acc: 0.9587628841400146)
[2025-02-13 20:16:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:36][root][INFO] - Training Epoch: 2/2, step 2750/7134 completed (loss: 0.16275162994861603, acc: 0.9638554453849792)
[2025-02-13 20:16:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:36][root][INFO] - Training Epoch: 2/2, step 2751/7134 completed (loss: 0.1344965547323227, acc: 0.954954981803894)
[2025-02-13 20:16:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:37][root][INFO] - Training Epoch: 2/2, step 2752/7134 completed (loss: 0.21294273436069489, acc: 0.9343065619468689)
[2025-02-13 20:16:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:37][root][INFO] - Training Epoch: 2/2, step 2753/7134 completed (loss: 0.13406464457511902, acc: 0.9663865566253662)
[2025-02-13 20:16:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:37][root][INFO] - Training Epoch: 2/2, step 2754/7134 completed (loss: 0.14624513685703278, acc: 0.9750000238418579)
[2025-02-13 20:16:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:38][root][INFO] - Training Epoch: 2/2, step 2755/7134 completed (loss: 0.194483682513237, acc: 0.9609375)
[2025-02-13 20:16:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:38][root][INFO] - Training Epoch: 2/2, step 2756/7134 completed (loss: 0.1798071265220642, acc: 0.9594594836235046)
[2025-02-13 20:16:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:39][root][INFO] - Training Epoch: 2/2, step 2757/7134 completed (loss: 0.21053408086299896, acc: 0.9347826242446899)
[2025-02-13 20:16:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:39][root][INFO] - Training Epoch: 2/2, step 2758/7134 completed (loss: 0.2359165996313095, acc: 0.9590163826942444)
[2025-02-13 20:16:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:39][root][INFO] - Training Epoch: 2/2, step 2759/7134 completed (loss: 0.07634786516427994, acc: 0.9571428298950195)
[2025-02-13 20:16:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:40][root][INFO] - Training Epoch: 2/2, step 2760/7134 completed (loss: 0.19645626842975616, acc: 0.9666666388511658)
[2025-02-13 20:16:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:40][root][INFO] - Training Epoch: 2/2, step 2761/7134 completed (loss: 0.2070440798997879, acc: 0.9578947424888611)
[2025-02-13 20:16:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:41][root][INFO] - Training Epoch: 2/2, step 2762/7134 completed (loss: 0.20667453110218048, acc: 0.9408866763114929)
[2025-02-13 20:16:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:41][root][INFO] - Training Epoch: 2/2, step 2763/7134 completed (loss: 0.09604649990797043, acc: 0.9781420826911926)
[2025-02-13 20:16:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:41][root][INFO] - Training Epoch: 2/2, step 2764/7134 completed (loss: 0.15605734288692474, acc: 0.9583333134651184)
[2025-02-13 20:16:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:42][root][INFO] - Training Epoch: 2/2, step 2765/7134 completed (loss: 0.17596547305583954, acc: 0.970588207244873)
[2025-02-13 20:16:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:42][root][INFO] - Training Epoch: 2/2, step 2766/7134 completed (loss: 0.07937025278806686, acc: 0.987261176109314)
[2025-02-13 20:16:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:43][root][INFO] - Training Epoch: 2/2, step 2767/7134 completed (loss: 0.1381252557039261, acc: 0.9850000143051147)
[2025-02-13 20:16:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:43][root][INFO] - Training Epoch: 2/2, step 2768/7134 completed (loss: 0.08073348551988602, acc: 0.9759036302566528)
[2025-02-13 20:16:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:44][root][INFO] - Training Epoch: 2/2, step 2769/7134 completed (loss: 0.0948464572429657, acc: 0.9684210419654846)
[2025-02-13 20:16:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:44][root][INFO] - Training Epoch: 2/2, step 2770/7134 completed (loss: 0.1036657840013504, acc: 0.9753086566925049)
[2025-02-13 20:16:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:44][root][INFO] - Training Epoch: 2/2, step 2771/7134 completed (loss: 0.25263670086860657, acc: 0.9383886456489563)
[2025-02-13 20:16:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:45][root][INFO] - Training Epoch: 2/2, step 2772/7134 completed (loss: 0.1095719113945961, acc: 0.9750000238418579)
[2025-02-13 20:16:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:45][root][INFO] - Training Epoch: 2/2, step 2773/7134 completed (loss: 0.13198940455913544, acc: 0.9748743772506714)
[2025-02-13 20:16:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:46][root][INFO] - Training Epoch: 2/2, step 2774/7134 completed (loss: 0.2109414041042328, acc: 0.9512194991111755)
[2025-02-13 20:16:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:46][root][INFO] - Training Epoch: 2/2, step 2775/7134 completed (loss: 0.16897258162498474, acc: 0.9453551769256592)
[2025-02-13 20:16:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:46][root][INFO] - Training Epoch: 2/2, step 2776/7134 completed (loss: 0.11783468723297119, acc: 0.9870129823684692)
[2025-02-13 20:16:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:47][root][INFO] - Training Epoch: 2/2, step 2777/7134 completed (loss: 0.03626800701022148, acc: 0.9942857027053833)
[2025-02-13 20:16:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:47][root][INFO] - Training Epoch: 2/2, step 2778/7134 completed (loss: 0.13791097700595856, acc: 0.9646464586257935)
[2025-02-13 20:16:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:48][root][INFO] - Training Epoch: 2/2, step 2779/7134 completed (loss: 0.1521604210138321, acc: 0.9634703397750854)
[2025-02-13 20:16:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:48][root][INFO] - Training Epoch: 2/2, step 2780/7134 completed (loss: 0.16131557524204254, acc: 0.9469026327133179)
[2025-02-13 20:16:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:48][root][INFO] - Training Epoch: 2/2, step 2781/7134 completed (loss: 0.11625038087368011, acc: 0.967391312122345)
[2025-02-13 20:16:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:49][root][INFO] - Training Epoch: 2/2, step 2782/7134 completed (loss: 0.07445868104696274, acc: 0.9910714030265808)
[2025-02-13 20:16:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:49][root][INFO] - Training Epoch: 2/2, step 2783/7134 completed (loss: 0.04440454766154289, acc: 1.0)
[2025-02-13 20:16:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:50][root][INFO] - Training Epoch: 2/2, step 2784/7134 completed (loss: 0.07943660020828247, acc: 0.9710144996643066)
[2025-02-13 20:16:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:50][root][INFO] - Training Epoch: 2/2, step 2785/7134 completed (loss: 0.0919804498553276, acc: 0.9757281541824341)
[2025-02-13 20:16:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:50][root][INFO] - Training Epoch: 2/2, step 2786/7134 completed (loss: 0.17528598010540009, acc: 0.9511111378669739)
[2025-02-13 20:16:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:51][root][INFO] - Training Epoch: 2/2, step 2787/7134 completed (loss: 0.0453236848115921, acc: 0.9884393215179443)
[2025-02-13 20:16:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:51][root][INFO] - Training Epoch: 2/2, step 2788/7134 completed (loss: 0.16589288413524628, acc: 0.9681528806686401)
[2025-02-13 20:16:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:52][root][INFO] - Training Epoch: 2/2, step 2789/7134 completed (loss: 1.0400902032852173, acc: 0.7714285850524902)
[2025-02-13 20:16:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:52][root][INFO] - Training Epoch: 2/2, step 2790/7134 completed (loss: 0.8081162571907043, acc: 0.8083333373069763)
[2025-02-13 20:16:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:52][root][INFO] - Training Epoch: 2/2, step 2791/7134 completed (loss: 0.2174636870622635, acc: 0.9536423683166504)
[2025-02-13 20:16:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:53][root][INFO] - Training Epoch: 2/2, step 2792/7134 completed (loss: 0.16111387312412262, acc: 0.9651162624359131)
[2025-02-13 20:16:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:53][root][INFO] - Training Epoch: 2/2, step 2793/7134 completed (loss: 0.08035773783922195, acc: 0.9880239367485046)
[2025-02-13 20:16:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:53][root][INFO] - Training Epoch: 2/2, step 2794/7134 completed (loss: 0.03292153775691986, acc: 1.0)
[2025-02-13 20:16:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:54][root][INFO] - Training Epoch: 2/2, step 2795/7134 completed (loss: 0.30395957827568054, acc: 0.9259259104728699)
[2025-02-13 20:16:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:54][root][INFO] - Training Epoch: 2/2, step 2796/7134 completed (loss: 0.16525019705295563, acc: 0.9593495726585388)
[2025-02-13 20:16:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:55][root][INFO] - Training Epoch: 2/2, step 2797/7134 completed (loss: 0.16380177438259125, acc: 0.9268292784690857)
[2025-02-13 20:16:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:55][root][INFO] - Training Epoch: 2/2, step 2798/7134 completed (loss: 0.21607984602451324, acc: 0.9333333373069763)
[2025-02-13 20:16:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:55][root][INFO] - Training Epoch: 2/2, step 2799/7134 completed (loss: 0.12290284037590027, acc: 0.9921875)
[2025-02-13 20:16:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:56][root][INFO] - Training Epoch: 2/2, step 2800/7134 completed (loss: 0.1827269047498703, acc: 0.9635036587715149)
[2025-02-13 20:16:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:56][root][INFO] - Training Epoch: 2/2, step 2801/7134 completed (loss: 0.2293405383825302, acc: 0.9453551769256592)
[2025-02-13 20:16:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:56][root][INFO] - Training Epoch: 2/2, step 2802/7134 completed (loss: 0.17834457755088806, acc: 0.9648241400718689)
[2025-02-13 20:16:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:57][root][INFO] - Training Epoch: 2/2, step 2803/7134 completed (loss: 0.09631650894880295, acc: 0.9814814925193787)
[2025-02-13 20:16:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:57][root][INFO] - Training Epoch: 2/2, step 2804/7134 completed (loss: 0.12228851020336151, acc: 0.9710982441902161)
[2025-02-13 20:16:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:58][root][INFO] - Training Epoch: 2/2, step 2805/7134 completed (loss: 0.06339295208454132, acc: 0.9883720874786377)
[2025-02-13 20:16:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:58][root][INFO] - Training Epoch: 2/2, step 2806/7134 completed (loss: 0.22840182483196259, acc: 0.9558823704719543)
[2025-02-13 20:16:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:58][root][INFO] - Training Epoch: 2/2, step 2807/7134 completed (loss: 0.09909176826477051, acc: 0.9805825352668762)
[2025-02-13 20:16:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:59][root][INFO] - Training Epoch: 2/2, step 2808/7134 completed (loss: 0.17744871973991394, acc: 0.9648241400718689)
[2025-02-13 20:16:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:16:59][root][INFO] - Training Epoch: 2/2, step 2809/7134 completed (loss: 0.11045707017183304, acc: 0.9685039520263672)
[2025-02-13 20:16:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:00][root][INFO] - Training Epoch: 2/2, step 2810/7134 completed (loss: 0.12322616577148438, acc: 0.9726775884628296)
[2025-02-13 20:17:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:00][root][INFO] - Training Epoch: 2/2, step 2811/7134 completed (loss: 0.0379817895591259, acc: 0.9944751262664795)
[2025-02-13 20:17:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:00][root][INFO] - Training Epoch: 2/2, step 2812/7134 completed (loss: 0.048561595380306244, acc: 0.9933775067329407)
[2025-02-13 20:17:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:01][root][INFO] - Training Epoch: 2/2, step 2813/7134 completed (loss: 0.059832047671079636, acc: 0.9885057210922241)
[2025-02-13 20:17:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:01][root][INFO] - Training Epoch: 2/2, step 2814/7134 completed (loss: 0.1557922512292862, acc: 0.9767441749572754)
[2025-02-13 20:17:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:02][root][INFO] - Training Epoch: 2/2, step 2815/7134 completed (loss: 0.08227347582578659, acc: 0.9725274443626404)
[2025-02-13 20:17:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:02][root][INFO] - Training Epoch: 2/2, step 2816/7134 completed (loss: 0.10035143792629242, acc: 0.9738562107086182)
[2025-02-13 20:17:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:02][root][INFO] - Training Epoch: 2/2, step 2817/7134 completed (loss: 0.10612320154905319, acc: 0.9695431590080261)
[2025-02-13 20:17:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:03][root][INFO] - Training Epoch: 2/2, step 2818/7134 completed (loss: 0.07644254714250565, acc: 0.9893617033958435)
[2025-02-13 20:17:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:03][root][INFO] - Training Epoch: 2/2, step 2819/7134 completed (loss: 0.05222838744521141, acc: 0.9863013625144958)
[2025-02-13 20:17:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:03][root][INFO] - Training Epoch: 2/2, step 2820/7134 completed (loss: 0.22445252537727356, acc: 0.9325153231620789)
[2025-02-13 20:17:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:04][root][INFO] - Training Epoch: 2/2, step 2821/7134 completed (loss: 0.14550013840198517, acc: 0.9533678889274597)
[2025-02-13 20:17:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:04][root][INFO] - Training Epoch: 2/2, step 2822/7134 completed (loss: 0.27688416838645935, acc: 0.9405405521392822)
[2025-02-13 20:17:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:05][root][INFO] - Training Epoch: 2/2, step 2823/7134 completed (loss: 0.11666062474250793, acc: 0.9751552939414978)
[2025-02-13 20:17:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:05][root][INFO] - Training Epoch: 2/2, step 2824/7134 completed (loss: 0.3003472685813904, acc: 0.9182389974594116)
[2025-02-13 20:17:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:05][root][INFO] - Training Epoch: 2/2, step 2825/7134 completed (loss: 0.10352667421102524, acc: 0.9833333492279053)
[2025-02-13 20:17:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:06][root][INFO] - Training Epoch: 2/2, step 2826/7134 completed (loss: 0.15317785739898682, acc: 0.977011501789093)
[2025-02-13 20:17:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:06][root][INFO] - Training Epoch: 2/2, step 2827/7134 completed (loss: 0.10968600958585739, acc: 0.9856459498405457)
[2025-02-13 20:17:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:07][root][INFO] - Training Epoch: 2/2, step 2828/7134 completed (loss: 0.09811662882566452, acc: 0.9810126423835754)
[2025-02-13 20:17:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:07][root][INFO] - Training Epoch: 2/2, step 2829/7134 completed (loss: 0.15831582248210907, acc: 0.9642857313156128)
[2025-02-13 20:17:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:07][root][INFO] - Training Epoch: 2/2, step 2830/7134 completed (loss: 0.02532022073864937, acc: 1.0)
[2025-02-13 20:17:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:08][root][INFO] - Training Epoch: 2/2, step 2831/7134 completed (loss: 0.06023465096950531, acc: 0.9864864945411682)
[2025-02-13 20:17:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:08][root][INFO] - Training Epoch: 2/2, step 2832/7134 completed (loss: 0.12347251921892166, acc: 0.9784946441650391)
[2025-02-13 20:17:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:09][root][INFO] - Training Epoch: 2/2, step 2833/7134 completed (loss: 0.17185628414154053, acc: 0.9788359999656677)
[2025-02-13 20:17:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:09][root][INFO] - Training Epoch: 2/2, step 2834/7134 completed (loss: 0.16025178134441376, acc: 0.9555555582046509)
[2025-02-13 20:17:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:09][root][INFO] - Training Epoch: 2/2, step 2835/7134 completed (loss: 0.048605870455503464, acc: 0.9802631735801697)
[2025-02-13 20:17:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:10][root][INFO] - Training Epoch: 2/2, step 2836/7134 completed (loss: 0.1009933203458786, acc: 0.9800000190734863)
[2025-02-13 20:17:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:10][root][INFO] - Training Epoch: 2/2, step 2837/7134 completed (loss: 0.06301217526197433, acc: 0.9824561476707458)
[2025-02-13 20:17:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:11][root][INFO] - Training Epoch: 2/2, step 2838/7134 completed (loss: 0.1679357886314392, acc: 0.9655172228813171)
[2025-02-13 20:17:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:11][root][INFO] - Training Epoch: 2/2, step 2839/7134 completed (loss: 0.17116668820381165, acc: 0.9552238583564758)
[2025-02-13 20:17:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:11][root][INFO] - Training Epoch: 2/2, step 2840/7134 completed (loss: 0.07450597733259201, acc: 0.9823529124259949)
[2025-02-13 20:17:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:12][root][INFO] - Training Epoch: 2/2, step 2841/7134 completed (loss: 0.10377558320760727, acc: 0.9622641801834106)
[2025-02-13 20:17:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:12][root][INFO] - Training Epoch: 2/2, step 2842/7134 completed (loss: 0.20045173168182373, acc: 0.9427083134651184)
[2025-02-13 20:17:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:13][root][INFO] - Training Epoch: 2/2, step 2843/7134 completed (loss: 0.09381342679262161, acc: 0.9748427867889404)
[2025-02-13 20:17:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:13][root][INFO] - Training Epoch: 2/2, step 2844/7134 completed (loss: 0.20472700893878937, acc: 0.9510869383811951)
[2025-02-13 20:17:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:13][root][INFO] - Training Epoch: 2/2, step 2845/7134 completed (loss: 0.11725009232759476, acc: 0.9594594836235046)
[2025-02-13 20:17:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:14][root][INFO] - Training Epoch: 2/2, step 2846/7134 completed (loss: 0.14524975419044495, acc: 0.9542483687400818)
[2025-02-13 20:17:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:14][root][INFO] - Training Epoch: 2/2, step 2847/7134 completed (loss: 0.09613314270973206, acc: 0.987261176109314)
[2025-02-13 20:17:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:15][root][INFO] - Training Epoch: 2/2, step 2848/7134 completed (loss: 0.06243492290377617, acc: 0.9908257126808167)
[2025-02-13 20:17:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:15][root][INFO] - Training Epoch: 2/2, step 2849/7134 completed (loss: 0.23727211356163025, acc: 0.9428571462631226)
[2025-02-13 20:17:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:15][root][INFO] - Training Epoch: 2/2, step 2850/7134 completed (loss: 0.18982715904712677, acc: 0.9572649598121643)
[2025-02-13 20:17:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:16][root][INFO] - Training Epoch: 2/2, step 2851/7134 completed (loss: 0.10072910785675049, acc: 0.970370352268219)
[2025-02-13 20:17:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:16][root][INFO] - Training Epoch: 2/2, step 2852/7134 completed (loss: 0.14897164702415466, acc: 0.9593495726585388)
[2025-02-13 20:17:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:17][root][INFO] - Training Epoch: 2/2, step 2853/7134 completed (loss: 0.08406376093626022, acc: 0.9897959232330322)
[2025-02-13 20:17:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:17][root][INFO] - Training Epoch: 2/2, step 2854/7134 completed (loss: 0.1676478385925293, acc: 0.9523809552192688)
[2025-02-13 20:17:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:17][root][INFO] - Training Epoch: 2/2, step 2855/7134 completed (loss: 0.1525982916355133, acc: 0.9527027010917664)
[2025-02-13 20:17:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:18][root][INFO] - Training Epoch: 2/2, step 2856/7134 completed (loss: 0.07638516277074814, acc: 0.9937888383865356)
[2025-02-13 20:17:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:18][root][INFO] - Training Epoch: 2/2, step 2857/7134 completed (loss: 0.17628981173038483, acc: 0.976047933101654)
[2025-02-13 20:17:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:19][root][INFO] - Training Epoch: 2/2, step 2858/7134 completed (loss: 0.05117933079600334, acc: 0.9848484992980957)
[2025-02-13 20:17:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:19][root][INFO] - Training Epoch: 2/2, step 2859/7134 completed (loss: 0.34912437200546265, acc: 0.9285714030265808)
[2025-02-13 20:17:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:19][root][INFO] - Training Epoch: 2/2, step 2860/7134 completed (loss: 0.12034245580434799, acc: 0.9609375)
[2025-02-13 20:17:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:20][root][INFO] - Training Epoch: 2/2, step 2861/7134 completed (loss: 0.047178126871585846, acc: 0.9910714030265808)
[2025-02-13 20:17:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:20][root][INFO] - Training Epoch: 2/2, step 2862/7134 completed (loss: 0.07273266464471817, acc: 0.9924812316894531)
[2025-02-13 20:17:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:21][root][INFO] - Training Epoch: 2/2, step 2863/7134 completed (loss: 0.04763861000537872, acc: 0.9944751262664795)
[2025-02-13 20:17:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:21][root][INFO] - Training Epoch: 2/2, step 2864/7134 completed (loss: 0.06510314345359802, acc: 0.9894737005233765)
[2025-02-13 20:17:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:21][root][INFO] - Training Epoch: 2/2, step 2865/7134 completed (loss: 0.0877203568816185, acc: 0.9826086759567261)
[2025-02-13 20:17:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:22][root][INFO] - Training Epoch: 2/2, step 2866/7134 completed (loss: 0.09966188669204712, acc: 0.9677419066429138)
[2025-02-13 20:17:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:22][root][INFO] - Training Epoch: 2/2, step 2867/7134 completed (loss: 0.051511023193597794, acc: 0.9876543283462524)
[2025-02-13 20:17:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:23][root][INFO] - Training Epoch: 2/2, step 2868/7134 completed (loss: 0.06971003860235214, acc: 0.9764705896377563)
[2025-02-13 20:17:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:23][root][INFO] - Training Epoch: 2/2, step 2869/7134 completed (loss: 0.12131284177303314, acc: 0.9793103337287903)
[2025-02-13 20:17:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:23][root][INFO] - Training Epoch: 2/2, step 2870/7134 completed (loss: 0.3063660264015198, acc: 0.9545454382896423)
[2025-02-13 20:17:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:24][root][INFO] - Training Epoch: 2/2, step 2871/7134 completed (loss: 0.14815369248390198, acc: 0.9724137783050537)
[2025-02-13 20:17:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:24][root][INFO] - Training Epoch: 2/2, step 2872/7134 completed (loss: 0.03532686457037926, acc: 0.991150438785553)
[2025-02-13 20:17:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:24][root][INFO] - Training Epoch: 2/2, step 2873/7134 completed (loss: 0.08672577142715454, acc: 0.9767441749572754)
[2025-02-13 20:17:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:25][root][INFO] - Training Epoch: 2/2, step 2874/7134 completed (loss: 0.04455395042896271, acc: 0.9861111044883728)
[2025-02-13 20:17:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:25][root][INFO] - Training Epoch: 2/2, step 2875/7134 completed (loss: 0.16695483028888702, acc: 0.9583333134651184)
[2025-02-13 20:17:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:26][root][INFO] - Training Epoch: 2/2, step 2876/7134 completed (loss: 0.286392480134964, acc: 0.9150943160057068)
[2025-02-13 20:17:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:26][root][INFO] - Training Epoch: 2/2, step 2877/7134 completed (loss: 0.044655703008174896, acc: 0.9914529919624329)
[2025-02-13 20:17:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:26][root][INFO] - Training Epoch: 2/2, step 2878/7134 completed (loss: 0.10753417760133743, acc: 0.9912280440330505)
[2025-02-13 20:17:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:27][root][INFO] - Training Epoch: 2/2, step 2879/7134 completed (loss: 0.040358856320381165, acc: 0.9863945841789246)
[2025-02-13 20:17:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:27][root][INFO] - Training Epoch: 2/2, step 2880/7134 completed (loss: 0.03447907418012619, acc: 0.9863013625144958)
[2025-02-13 20:17:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:28][root][INFO] - Training Epoch: 2/2, step 2881/7134 completed (loss: 0.13252106308937073, acc: 0.9682539701461792)
[2025-02-13 20:17:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:28][root][INFO] - Training Epoch: 2/2, step 2882/7134 completed (loss: 0.1615789830684662, acc: 0.9537572264671326)
[2025-02-13 20:17:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:28][root][INFO] - Training Epoch: 2/2, step 2883/7134 completed (loss: 0.22920866310596466, acc: 0.9580838084220886)
[2025-02-13 20:17:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:29][root][INFO] - Training Epoch: 2/2, step 2884/7134 completed (loss: 0.2078114151954651, acc: 0.9515151381492615)
[2025-02-13 20:17:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:29][root][INFO] - Training Epoch: 2/2, step 2885/7134 completed (loss: 0.2739061117172241, acc: 0.9151515364646912)
[2025-02-13 20:17:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:30][root][INFO] - Training Epoch: 2/2, step 2886/7134 completed (loss: 0.2555042803287506, acc: 0.9588235020637512)
[2025-02-13 20:17:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:30][root][INFO] - Training Epoch: 2/2, step 2887/7134 completed (loss: 0.18728500604629517, acc: 0.9475982785224915)
[2025-02-13 20:17:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:30][root][INFO] - Training Epoch: 2/2, step 2888/7134 completed (loss: 0.1746329665184021, acc: 0.9473684430122375)
[2025-02-13 20:17:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:31][root][INFO] - Training Epoch: 2/2, step 2889/7134 completed (loss: 0.11356448382139206, acc: 0.9832402467727661)
[2025-02-13 20:17:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:31][root][INFO] - Training Epoch: 2/2, step 2890/7134 completed (loss: 0.09157387912273407, acc: 0.9743589758872986)
[2025-02-13 20:17:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:32][root][INFO] - Training Epoch: 2/2, step 2891/7134 completed (loss: 0.05787068232893944, acc: 0.9777777791023254)
[2025-02-13 20:17:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:32][root][INFO] - Training Epoch: 2/2, step 2892/7134 completed (loss: 0.10365156829357147, acc: 0.9704142212867737)
[2025-02-13 20:17:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:32][root][INFO] - Training Epoch: 2/2, step 2893/7134 completed (loss: 0.15957802534103394, acc: 0.9508196711540222)
[2025-02-13 20:17:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:33][root][INFO] - Training Epoch: 2/2, step 2894/7134 completed (loss: 0.2531810998916626, acc: 0.9407407641410828)
[2025-02-13 20:17:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:33][root][INFO] - Training Epoch: 2/2, step 2895/7134 completed (loss: 0.054547205567359924, acc: 0.991525411605835)
[2025-02-13 20:17:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:33][root][INFO] - Training Epoch: 2/2, step 2896/7134 completed (loss: 0.15448488295078278, acc: 0.970059871673584)
[2025-02-13 20:17:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:34][root][INFO] - Training Epoch: 2/2, step 2897/7134 completed (loss: 0.2252356857061386, acc: 0.9473684430122375)
[2025-02-13 20:17:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:34][root][INFO] - Training Epoch: 2/2, step 2898/7134 completed (loss: 0.08394785225391388, acc: 0.9728260636329651)
[2025-02-13 20:17:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:35][root][INFO] - Training Epoch: 2/2, step 2899/7134 completed (loss: 0.10981222987174988, acc: 0.9640718698501587)
[2025-02-13 20:17:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:35][root][INFO] - Training Epoch: 2/2, step 2900/7134 completed (loss: 0.12599536776542664, acc: 0.978723406791687)
[2025-02-13 20:17:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:35][root][INFO] - Training Epoch: 2/2, step 2901/7134 completed (loss: 0.07444699108600616, acc: 0.9864864945411682)
[2025-02-13 20:17:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:36][root][INFO] - Training Epoch: 2/2, step 2902/7134 completed (loss: 0.08472493290901184, acc: 0.9743589758872986)
[2025-02-13 20:17:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:36][root][INFO] - Training Epoch: 2/2, step 2903/7134 completed (loss: 0.04393262416124344, acc: 1.0)
[2025-02-13 20:17:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:36][root][INFO] - Training Epoch: 2/2, step 2904/7134 completed (loss: 0.045027412474155426, acc: 0.9891892075538635)
[2025-02-13 20:17:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:37][root][INFO] - Training Epoch: 2/2, step 2905/7134 completed (loss: 0.10578552633523941, acc: 0.9786096215248108)
[2025-02-13 20:17:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:37][root][INFO] - Training Epoch: 2/2, step 2906/7134 completed (loss: 0.09799212217330933, acc: 0.9842932224273682)
[2025-02-13 20:17:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:38][root][INFO] - Training Epoch: 2/2, step 2907/7134 completed (loss: 0.07498694956302643, acc: 0.9880239367485046)
[2025-02-13 20:17:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:38][root][INFO] - Training Epoch: 2/2, step 2908/7134 completed (loss: 0.12413206696510315, acc: 0.9712643623352051)
[2025-02-13 20:17:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:38][root][INFO] - Training Epoch: 2/2, step 2909/7134 completed (loss: 0.05710217356681824, acc: 0.9836065769195557)
[2025-02-13 20:17:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:39][root][INFO] - Training Epoch: 2/2, step 2910/7134 completed (loss: 0.10411583632230759, acc: 0.9839572310447693)
[2025-02-13 20:17:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:39][root][INFO] - Training Epoch: 2/2, step 2911/7134 completed (loss: 0.07585727423429489, acc: 0.9790209531784058)
[2025-02-13 20:17:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:40][root][INFO] - Training Epoch: 2/2, step 2912/7134 completed (loss: 0.06732122600078583, acc: 0.9863013625144958)
[2025-02-13 20:17:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:40][root][INFO] - Training Epoch: 2/2, step 2913/7134 completed (loss: 0.1656171977519989, acc: 0.9640287756919861)
[2025-02-13 20:17:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:40][root][INFO] - Training Epoch: 2/2, step 2914/7134 completed (loss: 0.06947946548461914, acc: 0.9878787994384766)
[2025-02-13 20:17:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:41][root][INFO] - Training Epoch: 2/2, step 2915/7134 completed (loss: 0.2590893507003784, acc: 0.9379310607910156)
[2025-02-13 20:17:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:41][root][INFO] - Training Epoch: 2/2, step 2916/7134 completed (loss: 0.14240668714046478, acc: 0.9649122953414917)
[2025-02-13 20:17:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:42][root][INFO] - Training Epoch: 2/2, step 2917/7134 completed (loss: 0.09696000814437866, acc: 0.9649122953414917)
[2025-02-13 20:17:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:42][root][INFO] - Training Epoch: 2/2, step 2918/7134 completed (loss: 0.08608498424291611, acc: 0.9707602262496948)
[2025-02-13 20:17:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:42][root][INFO] - Training Epoch: 2/2, step 2919/7134 completed (loss: 0.10180236399173737, acc: 0.9698795080184937)
[2025-02-13 20:17:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:43][root][INFO] - Training Epoch: 2/2, step 2920/7134 completed (loss: 0.13883566856384277, acc: 0.9622641801834106)
[2025-02-13 20:17:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:43][root][INFO] - Training Epoch: 2/2, step 2921/7134 completed (loss: 0.07383015006780624, acc: 0.9857142567634583)
[2025-02-13 20:17:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:43][root][INFO] - Training Epoch: 2/2, step 2922/7134 completed (loss: 0.2602720558643341, acc: 0.9386503100395203)
[2025-02-13 20:17:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:44][root][INFO] - Training Epoch: 2/2, step 2923/7134 completed (loss: 0.4781261384487152, acc: 0.922535240650177)
[2025-02-13 20:17:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:44][root][INFO] - Training Epoch: 2/2, step 2924/7134 completed (loss: 0.16212888062000275, acc: 0.9695122241973877)
[2025-02-13 20:17:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:45][root][INFO] - Training Epoch: 2/2, step 2925/7134 completed (loss: 0.167231485247612, acc: 0.9823529124259949)
[2025-02-13 20:17:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:45][root][INFO] - Training Epoch: 2/2, step 2926/7134 completed (loss: 0.4080607295036316, acc: 0.9497487545013428)
[2025-02-13 20:17:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:45][root][INFO] - Training Epoch: 2/2, step 2927/7134 completed (loss: 0.09071847051382065, acc: 0.9826589822769165)
[2025-02-13 20:17:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:46][root][INFO] - Training Epoch: 2/2, step 2928/7134 completed (loss: 0.14526742696762085, acc: 0.9652777910232544)
[2025-02-13 20:17:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:46][root][INFO] - Training Epoch: 2/2, step 2929/7134 completed (loss: 0.059847183525562286, acc: 1.0)
[2025-02-13 20:17:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:47][root][INFO] - Training Epoch: 2/2, step 2930/7134 completed (loss: 0.042614664882421494, acc: 0.9863945841789246)
[2025-02-13 20:17:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:47][root][INFO] - Training Epoch: 2/2, step 2931/7134 completed (loss: 0.37422409653663635, acc: 0.9102563858032227)
[2025-02-13 20:17:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:47][root][INFO] - Training Epoch: 2/2, step 2932/7134 completed (loss: 0.08132342249155045, acc: 0.9839572310447693)
[2025-02-13 20:17:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:48][root][INFO] - Training Epoch: 2/2, step 2933/7134 completed (loss: 0.08837719261646271, acc: 0.9781022071838379)
[2025-02-13 20:17:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:48][root][INFO] - Training Epoch: 2/2, step 2934/7134 completed (loss: 0.03678765892982483, acc: 1.0)
[2025-02-13 20:17:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:48][root][INFO] - Training Epoch: 2/2, step 2935/7134 completed (loss: 0.04903404414653778, acc: 0.9947090148925781)
[2025-02-13 20:17:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:49][root][INFO] - Training Epoch: 2/2, step 2936/7134 completed (loss: 0.1942535638809204, acc: 0.9690265655517578)
[2025-02-13 20:17:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:49][root][INFO] - Training Epoch: 2/2, step 2937/7134 completed (loss: 0.13344600796699524, acc: 0.970588207244873)
[2025-02-13 20:17:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:50][root][INFO] - Training Epoch: 2/2, step 2938/7134 completed (loss: 0.09784158319234848, acc: 0.9781420826911926)
[2025-02-13 20:17:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:50][root][INFO] - Training Epoch: 2/2, step 2939/7134 completed (loss: 0.08803706616163254, acc: 0.9801324605941772)
[2025-02-13 20:17:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:50][root][INFO] - Training Epoch: 2/2, step 2940/7134 completed (loss: 0.10190040618181229, acc: 0.9781420826911926)
[2025-02-13 20:17:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:51][root][INFO] - Training Epoch: 2/2, step 2941/7134 completed (loss: 0.1698412150144577, acc: 0.9550561904907227)
[2025-02-13 20:17:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:51][root][INFO] - Training Epoch: 2/2, step 2942/7134 completed (loss: 0.06496509164571762, acc: 0.9878787994384766)
[2025-02-13 20:17:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:52][root][INFO] - Training Epoch: 2/2, step 2943/7134 completed (loss: 0.08605305850505829, acc: 0.9797297120094299)
[2025-02-13 20:17:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:52][root][INFO] - Training Epoch: 2/2, step 2944/7134 completed (loss: 0.08294151723384857, acc: 0.9698795080184937)
[2025-02-13 20:17:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:53][root][INFO] - Training Epoch: 2/2, step 2945/7134 completed (loss: 0.1217951700091362, acc: 0.9798657894134521)
[2025-02-13 20:17:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:53][root][INFO] - Training Epoch: 2/2, step 2946/7134 completed (loss: 0.15341250598430634, acc: 0.9444444179534912)
[2025-02-13 20:17:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:53][root][INFO] - Training Epoch: 2/2, step 2947/7134 completed (loss: 0.16871699690818787, acc: 0.956250011920929)
[2025-02-13 20:17:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:54][root][INFO] - Training Epoch: 2/2, step 2948/7134 completed (loss: 0.08301354199647903, acc: 0.9930555820465088)
[2025-02-13 20:17:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:54][root][INFO] - Training Epoch: 2/2, step 2949/7134 completed (loss: 0.06125981733202934, acc: 0.9934640526771545)
[2025-02-13 20:17:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:55][root][INFO] - Training Epoch: 2/2, step 2950/7134 completed (loss: 0.113481305539608, acc: 0.9608938694000244)
[2025-02-13 20:17:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:55][root][INFO] - Training Epoch: 2/2, step 2951/7134 completed (loss: 0.1211514100432396, acc: 0.9813664555549622)
[2025-02-13 20:17:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:55][root][INFO] - Training Epoch: 2/2, step 2952/7134 completed (loss: 0.044443279504776, acc: 1.0)
[2025-02-13 20:17:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:56][root][INFO] - Training Epoch: 2/2, step 2953/7134 completed (loss: 0.10491981357336044, acc: 0.9795918464660645)
[2025-02-13 20:17:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:56][root][INFO] - Training Epoch: 2/2, step 2954/7134 completed (loss: 0.14633044600486755, acc: 0.9587628841400146)
[2025-02-13 20:17:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:57][root][INFO] - Training Epoch: 2/2, step 2955/7134 completed (loss: 0.08875219523906708, acc: 0.9696969985961914)
[2025-02-13 20:17:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:57][root][INFO] - Training Epoch: 2/2, step 2956/7134 completed (loss: 0.06924089044332504, acc: 0.9815950989723206)
[2025-02-13 20:17:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:57][root][INFO] - Training Epoch: 2/2, step 2957/7134 completed (loss: 0.04093090444803238, acc: 0.9885057210922241)
[2025-02-13 20:17:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:58][root][INFO] - Training Epoch: 2/2, step 2958/7134 completed (loss: 0.05122818052768707, acc: 0.9861111044883728)
[2025-02-13 20:17:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:58][root][INFO] - Training Epoch: 2/2, step 2959/7134 completed (loss: 0.026725947856903076, acc: 1.0)
[2025-02-13 20:17:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:58][root][INFO] - Training Epoch: 2/2, step 2960/7134 completed (loss: 0.053711529821157455, acc: 0.9945054650306702)
[2025-02-13 20:17:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:59][root][INFO] - Training Epoch: 2/2, step 2961/7134 completed (loss: 0.055026065558195114, acc: 0.9940119981765747)
[2025-02-13 20:17:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:17:59][root][INFO] - Training Epoch: 2/2, step 2962/7134 completed (loss: 0.045768897980451584, acc: 0.9940476417541504)
[2025-02-13 20:17:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:00][root][INFO] - Training Epoch: 2/2, step 2963/7134 completed (loss: 0.044379521161317825, acc: 0.9888268113136292)
[2025-02-13 20:18:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:00][root][INFO] - Training Epoch: 2/2, step 2964/7134 completed (loss: 0.024198591709136963, acc: 1.0)
[2025-02-13 20:18:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:00][root][INFO] - Training Epoch: 2/2, step 2965/7134 completed (loss: 0.1281803697347641, acc: 0.9829545617103577)
[2025-02-13 20:18:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:01][root][INFO] - Training Epoch: 2/2, step 2966/7134 completed (loss: 0.016521278768777847, acc: 1.0)
[2025-02-13 20:18:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:01][root][INFO] - Training Epoch: 2/2, step 2967/7134 completed (loss: 0.21571549773216248, acc: 0.9344262480735779)
[2025-02-13 20:18:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:02][root][INFO] - Training Epoch: 2/2, step 2968/7134 completed (loss: 0.41287097334861755, acc: 0.9125000238418579)
[2025-02-13 20:18:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:02][root][INFO] - Training Epoch: 2/2, step 2969/7134 completed (loss: 0.9045803546905518, acc: 0.8203883767127991)
[2025-02-13 20:18:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:02][root][INFO] - Training Epoch: 2/2, step 2970/7134 completed (loss: 0.6440486907958984, acc: 0.8656716346740723)
[2025-02-13 20:18:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:03][root][INFO] - Training Epoch: 2/2, step 2971/7134 completed (loss: 0.18847860395908356, acc: 0.9613259434700012)
[2025-02-13 20:18:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:03][root][INFO] - Training Epoch: 2/2, step 2972/7134 completed (loss: 0.2323138415813446, acc: 0.9399999976158142)
[2025-02-13 20:18:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:04][root][INFO] - Training Epoch: 2/2, step 2973/7134 completed (loss: 0.12430651485919952, acc: 0.9757575988769531)
[2025-02-13 20:18:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:04][root][INFO] - Training Epoch: 2/2, step 2974/7134 completed (loss: 0.15821507573127747, acc: 0.9707602262496948)
[2025-02-13 20:18:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:04][root][INFO] - Training Epoch: 2/2, step 2975/7134 completed (loss: 0.11856262385845184, acc: 0.9651162624359131)
[2025-02-13 20:18:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:05][root][INFO] - Training Epoch: 2/2, step 2976/7134 completed (loss: 0.13434071838855743, acc: 0.9716981053352356)
[2025-02-13 20:18:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:05][root][INFO] - Training Epoch: 2/2, step 2977/7134 completed (loss: 0.126680389046669, acc: 0.9586206674575806)
[2025-02-13 20:18:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:05][root][INFO] - Training Epoch: 2/2, step 2978/7134 completed (loss: 0.12412696331739426, acc: 0.9759036302566528)
[2025-02-13 20:18:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:06][root][INFO] - Training Epoch: 2/2, step 2979/7134 completed (loss: 0.09254763275384903, acc: 0.9793103337287903)
[2025-02-13 20:18:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:06][root][INFO] - Training Epoch: 2/2, step 2980/7134 completed (loss: 0.2471141517162323, acc: 0.9371428489685059)
[2025-02-13 20:18:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:07][root][INFO] - Training Epoch: 2/2, step 2981/7134 completed (loss: 0.22419971227645874, acc: 0.959770143032074)
[2025-02-13 20:18:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:07][root][INFO] - Training Epoch: 2/2, step 2982/7134 completed (loss: 0.2445133924484253, acc: 0.9395604133605957)
[2025-02-13 20:18:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:07][root][INFO] - Training Epoch: 2/2, step 2983/7134 completed (loss: 0.17215676605701447, acc: 0.9631901979446411)
[2025-02-13 20:18:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:08][root][INFO] - Training Epoch: 2/2, step 2984/7134 completed (loss: 0.18952369689941406, acc: 0.9615384340286255)
[2025-02-13 20:18:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:08][root][INFO] - Training Epoch: 2/2, step 2985/7134 completed (loss: 0.10405515134334564, acc: 0.9804878234863281)
[2025-02-13 20:18:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:09][root][INFO] - Training Epoch: 2/2, step 2986/7134 completed (loss: 0.14970983564853668, acc: 0.9587628841400146)
[2025-02-13 20:18:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:09][root][INFO] - Training Epoch: 2/2, step 2987/7134 completed (loss: 0.1314399093389511, acc: 0.9836956262588501)
[2025-02-13 20:18:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:09][root][INFO] - Training Epoch: 2/2, step 2988/7134 completed (loss: 0.11197992414236069, acc: 0.9783783555030823)
[2025-02-13 20:18:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:10][root][INFO] - Training Epoch: 2/2, step 2989/7134 completed (loss: 0.13132697343826294, acc: 0.970059871673584)
[2025-02-13 20:18:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:10][root][INFO] - Training Epoch: 2/2, step 2990/7134 completed (loss: 0.28801462054252625, acc: 0.9189189076423645)
[2025-02-13 20:18:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:11][root][INFO] - Training Epoch: 2/2, step 2991/7134 completed (loss: 0.49018990993499756, acc: 0.8541666865348816)
[2025-02-13 20:18:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:11][root][INFO] - Training Epoch: 2/2, step 2992/7134 completed (loss: 0.28577709197998047, acc: 0.9433962106704712)
[2025-02-13 20:18:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:11][root][INFO] - Training Epoch: 2/2, step 2993/7134 completed (loss: 0.1309105008840561, acc: 0.9597989916801453)
[2025-02-13 20:18:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:12][root][INFO] - Training Epoch: 2/2, step 2994/7134 completed (loss: 0.027296166867017746, acc: 0.9920634627342224)
[2025-02-13 20:18:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:12][root][INFO] - Training Epoch: 2/2, step 2995/7134 completed (loss: 0.08079730719327927, acc: 0.9902912378311157)
[2025-02-13 20:18:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:13][root][INFO] - Training Epoch: 2/2, step 2996/7134 completed (loss: 0.13308340311050415, acc: 0.9587628841400146)
[2025-02-13 20:18:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:13][root][INFO] - Training Epoch: 2/2, step 2997/7134 completed (loss: 0.05477633699774742, acc: 0.9858490824699402)
[2025-02-13 20:18:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:13][root][INFO] - Training Epoch: 2/2, step 2998/7134 completed (loss: 0.09773657470941544, acc: 0.988095223903656)
[2025-02-13 20:18:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:14][root][INFO] - Training Epoch: 2/2, step 2999/7134 completed (loss: 0.08802953362464905, acc: 0.981249988079071)
[2025-02-13 20:18:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:14][root][INFO] - Training Epoch: 2/2, step 3000/7134 completed (loss: 0.21523459255695343, acc: 0.9444444179534912)
[2025-02-13 20:18:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:15][root][INFO] - Training Epoch: 2/2, step 3001/7134 completed (loss: 0.10823672264814377, acc: 0.9593908786773682)
[2025-02-13 20:18:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:15][root][INFO] - Training Epoch: 2/2, step 3002/7134 completed (loss: 0.14562073349952698, acc: 0.9631336331367493)
[2025-02-13 20:18:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:15][root][INFO] - Training Epoch: 2/2, step 3003/7134 completed (loss: 0.0963645651936531, acc: 0.9949238300323486)
[2025-02-13 20:18:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:16][root][INFO] - Training Epoch: 2/2, step 3004/7134 completed (loss: 0.038135770708322525, acc: 0.9851484894752502)
[2025-02-13 20:18:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:16][root][INFO] - Training Epoch: 2/2, step 3005/7134 completed (loss: 0.027484722435474396, acc: 1.0)
[2025-02-13 20:18:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:16][root][INFO] - Training Epoch: 2/2, step 3006/7134 completed (loss: 0.03613622859120369, acc: 0.9942857027053833)
[2025-02-13 20:18:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:17][root][INFO] - Training Epoch: 2/2, step 3007/7134 completed (loss: 0.07202040404081345, acc: 0.9767441749572754)
[2025-02-13 20:18:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:17][root][INFO] - Training Epoch: 2/2, step 3008/7134 completed (loss: 0.17184330523014069, acc: 0.9727891087532043)
[2025-02-13 20:18:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:18][root][INFO] - Training Epoch: 2/2, step 3009/7134 completed (loss: 0.18845322728157043, acc: 0.976190447807312)
[2025-02-13 20:18:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:18][root][INFO] - Training Epoch: 2/2, step 3010/7134 completed (loss: 0.09924066811800003, acc: 0.9714285731315613)
[2025-02-13 20:18:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:18][root][INFO] - Training Epoch: 2/2, step 3011/7134 completed (loss: 0.31692442297935486, acc: 0.9212121367454529)
[2025-02-13 20:18:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:19][root][INFO] - Training Epoch: 2/2, step 3012/7134 completed (loss: 0.2343684434890747, acc: 0.9640287756919861)
[2025-02-13 20:18:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:19][root][INFO] - Training Epoch: 2/2, step 3013/7134 completed (loss: 0.02883128821849823, acc: 1.0)
[2025-02-13 20:18:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:19][root][INFO] - Training Epoch: 2/2, step 3014/7134 completed (loss: 0.09181657433509827, acc: 0.9873417615890503)
[2025-02-13 20:18:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:20][root][INFO] - Training Epoch: 2/2, step 3015/7134 completed (loss: 0.06840422004461288, acc: 0.9788732528686523)
[2025-02-13 20:18:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:20][root][INFO] - Training Epoch: 2/2, step 3016/7134 completed (loss: 0.08660668134689331, acc: 0.9794520735740662)
[2025-02-13 20:18:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:21][root][INFO] - Training Epoch: 2/2, step 3017/7134 completed (loss: 0.038351383060216904, acc: 0.9834710955619812)
[2025-02-13 20:18:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:21][root][INFO] - Training Epoch: 2/2, step 3018/7134 completed (loss: 0.09140356630086899, acc: 0.9694656729698181)
[2025-02-13 20:18:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:22][root][INFO] - Training Epoch: 2/2, step 3019/7134 completed (loss: 0.20072048902511597, acc: 0.970588207244873)
[2025-02-13 20:18:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:22][root][INFO] - Training Epoch: 2/2, step 3020/7134 completed (loss: 0.05046219751238823, acc: 0.9928057789802551)
[2025-02-13 20:18:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:22][root][INFO] - Training Epoch: 2/2, step 3021/7134 completed (loss: 0.07596035301685333, acc: 0.9848484992980957)
[2025-02-13 20:18:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:23][root][INFO] - Training Epoch: 2/2, step 3022/7134 completed (loss: 0.089767687022686, acc: 0.969924807548523)
[2025-02-13 20:18:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:23][root][INFO] - Training Epoch: 2/2, step 3023/7134 completed (loss: 0.08997385203838348, acc: 0.9692307710647583)
[2025-02-13 20:18:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:24][root][INFO] - Training Epoch: 2/2, step 3024/7134 completed (loss: 0.034176964312791824, acc: 0.9833333492279053)
[2025-02-13 20:18:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:24][root][INFO] - Training Epoch: 2/2, step 3025/7134 completed (loss: 0.1337871104478836, acc: 0.9923076629638672)
[2025-02-13 20:18:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:24][root][INFO] - Training Epoch: 2/2, step 3026/7134 completed (loss: 0.1277085244655609, acc: 0.9754098653793335)
[2025-02-13 20:18:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:25][root][INFO] - Training Epoch: 2/2, step 3027/7134 completed (loss: 0.13212339580059052, acc: 0.9583333134651184)
[2025-02-13 20:18:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:25][root][INFO] - Training Epoch: 2/2, step 3028/7134 completed (loss: 0.07498166710138321, acc: 0.9793814420700073)
[2025-02-13 20:18:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:25][root][INFO] - Training Epoch: 2/2, step 3029/7134 completed (loss: 0.08204780519008636, acc: 0.9931507110595703)
[2025-02-13 20:18:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:26][root][INFO] - Training Epoch: 2/2, step 3030/7134 completed (loss: 0.051826804876327515, acc: 0.9864864945411682)
[2025-02-13 20:18:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:26][root][INFO] - Training Epoch: 2/2, step 3031/7134 completed (loss: 0.039073646068573, acc: 1.0)
[2025-02-13 20:18:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:27][root][INFO] - Training Epoch: 2/2, step 3032/7134 completed (loss: 0.10027776658535004, acc: 0.975806474685669)
[2025-02-13 20:18:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:27][root][INFO] - Training Epoch: 2/2, step 3033/7134 completed (loss: 0.06660100817680359, acc: 0.984375)
[2025-02-13 20:18:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:27][root][INFO] - Training Epoch: 2/2, step 3034/7134 completed (loss: 0.09993001818656921, acc: 0.9819819927215576)
[2025-02-13 20:18:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:28][root][INFO] - Training Epoch: 2/2, step 3035/7134 completed (loss: 0.04535292461514473, acc: 0.9795918464660645)
[2025-02-13 20:18:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:28][root][INFO] - Training Epoch: 2/2, step 3036/7134 completed (loss: 0.15102475881576538, acc: 0.9520000219345093)
[2025-02-13 20:18:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:28][root][INFO] - Training Epoch: 2/2, step 3037/7134 completed (loss: 0.09503272920846939, acc: 0.9597315192222595)
[2025-02-13 20:18:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:29][root][INFO] - Training Epoch: 2/2, step 3038/7134 completed (loss: 0.07761454582214355, acc: 0.9925373196601868)
[2025-02-13 20:18:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:29][root][INFO] - Training Epoch: 2/2, step 3039/7134 completed (loss: 0.12231079488992691, acc: 0.9797297120094299)
[2025-02-13 20:18:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:30][root][INFO] - Training Epoch: 2/2, step 3040/7134 completed (loss: 0.07867033779621124, acc: 0.9844961166381836)
[2025-02-13 20:18:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:30][root][INFO] - Training Epoch: 2/2, step 3041/7134 completed (loss: 0.17340654134750366, acc: 0.9642857313156128)
[2025-02-13 20:18:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:30][root][INFO] - Training Epoch: 2/2, step 3042/7134 completed (loss: 0.08165967464447021, acc: 0.9807692170143127)
[2025-02-13 20:18:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:31][root][INFO] - Training Epoch: 2/2, step 3043/7134 completed (loss: 0.06857704371213913, acc: 0.9821428656578064)
[2025-02-13 20:18:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:31][root][INFO] - Training Epoch: 2/2, step 3044/7134 completed (loss: 0.05781504884362221, acc: 0.9849624037742615)
[2025-02-13 20:18:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:31][root][INFO] - Training Epoch: 2/2, step 3045/7134 completed (loss: 0.03426554799079895, acc: 0.9922480583190918)
[2025-02-13 20:18:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:32][root][INFO] - Training Epoch: 2/2, step 3046/7134 completed (loss: 0.06589862704277039, acc: 0.9822485446929932)
[2025-02-13 20:18:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:32][root][INFO] - Training Epoch: 2/2, step 3047/7134 completed (loss: 0.10311923176050186, acc: 0.9830508232116699)
[2025-02-13 20:18:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:33][root][INFO] - Training Epoch: 2/2, step 3048/7134 completed (loss: 0.1306115835905075, acc: 0.9631578922271729)
[2025-02-13 20:18:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:33][root][INFO] - Training Epoch: 2/2, step 3049/7134 completed (loss: 0.08328063786029816, acc: 0.981249988079071)
[2025-02-13 20:18:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:33][root][INFO] - Training Epoch: 2/2, step 3050/7134 completed (loss: 0.08545536547899246, acc: 0.9796954393386841)
[2025-02-13 20:18:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:34][root][INFO] - Training Epoch: 2/2, step 3051/7134 completed (loss: 0.09132636338472366, acc: 0.9702380895614624)
[2025-02-13 20:18:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:34][root][INFO] - Training Epoch: 2/2, step 3052/7134 completed (loss: 0.07505034655332565, acc: 0.9891892075538635)
[2025-02-13 20:18:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:35][root][INFO] - Training Epoch: 2/2, step 3053/7134 completed (loss: 0.07219281047582626, acc: 0.9701492786407471)
[2025-02-13 20:18:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:35][root][INFO] - Training Epoch: 2/2, step 3054/7134 completed (loss: 0.05605364218354225, acc: 0.978723406791687)
[2025-02-13 20:18:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:35][root][INFO] - Training Epoch: 2/2, step 3055/7134 completed (loss: 0.09802335500717163, acc: 0.9729729890823364)
[2025-02-13 20:18:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:36][root][INFO] - Training Epoch: 2/2, step 3056/7134 completed (loss: 0.06705500930547714, acc: 0.9729729890823364)
[2025-02-13 20:18:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:36][root][INFO] - Training Epoch: 2/2, step 3057/7134 completed (loss: 0.06681980192661285, acc: 0.9815950989723206)
[2025-02-13 20:18:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:37][root][INFO] - Training Epoch: 2/2, step 3058/7134 completed (loss: 0.11115308851003647, acc: 0.9736841917037964)
[2025-02-13 20:18:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:37][root][INFO] - Training Epoch: 2/2, step 3059/7134 completed (loss: 0.028050841763615608, acc: 0.9942857027053833)
[2025-02-13 20:18:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:37][root][INFO] - Training Epoch: 2/2, step 3060/7134 completed (loss: 0.11469832062721252, acc: 0.963350772857666)
[2025-02-13 20:18:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:38][root][INFO] - Training Epoch: 2/2, step 3061/7134 completed (loss: 0.07035321742296219, acc: 0.981249988079071)
[2025-02-13 20:18:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:38][root][INFO] - Training Epoch: 2/2, step 3062/7134 completed (loss: 0.03524799644947052, acc: 0.9937106966972351)
[2025-02-13 20:18:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:39][root][INFO] - Training Epoch: 2/2, step 3063/7134 completed (loss: 0.09042684733867645, acc: 0.9829545617103577)
[2025-02-13 20:18:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:39][root][INFO] - Training Epoch: 2/2, step 3064/7134 completed (loss: 0.041458286345005035, acc: 0.9940828680992126)
[2025-02-13 20:18:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:40][root][INFO] - Training Epoch: 2/2, step 3065/7134 completed (loss: 0.034938983619213104, acc: 0.9887640476226807)
[2025-02-13 20:18:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:40][root][INFO] - Training Epoch: 2/2, step 3066/7134 completed (loss: 0.039410799741744995, acc: 0.9933775067329407)
[2025-02-13 20:18:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:40][root][INFO] - Training Epoch: 2/2, step 3067/7134 completed (loss: 0.026620611548423767, acc: 1.0)
[2025-02-13 20:18:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:41][root][INFO] - Training Epoch: 2/2, step 3068/7134 completed (loss: 0.08300960063934326, acc: 0.9776536226272583)
[2025-02-13 20:18:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:41][root][INFO] - Training Epoch: 2/2, step 3069/7134 completed (loss: 0.01462570857256651, acc: 1.0)
[2025-02-13 20:18:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:42][root][INFO] - Training Epoch: 2/2, step 3070/7134 completed (loss: 0.022516626864671707, acc: 1.0)
[2025-02-13 20:18:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:42][root][INFO] - Training Epoch: 2/2, step 3071/7134 completed (loss: 0.06101769208908081, acc: 0.9944444298744202)
[2025-02-13 20:18:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:43][root][INFO] - Training Epoch: 2/2, step 3072/7134 completed (loss: 0.060780592262744904, acc: 0.9906103014945984)
[2025-02-13 20:18:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:43][root][INFO] - Training Epoch: 2/2, step 3073/7134 completed (loss: 0.041940946131944656, acc: 0.9830508232116699)
[2025-02-13 20:18:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:43][root][INFO] - Training Epoch: 2/2, step 3074/7134 completed (loss: 0.0216426532715559, acc: 0.9933775067329407)
[2025-02-13 20:18:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:44][root][INFO] - Training Epoch: 2/2, step 3075/7134 completed (loss: 0.045858483761548996, acc: 0.9876543283462524)
[2025-02-13 20:18:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:44][root][INFO] - Training Epoch: 2/2, step 3076/7134 completed (loss: 0.034166719764471054, acc: 1.0)
[2025-02-13 20:18:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:45][root][INFO] - Training Epoch: 2/2, step 3077/7134 completed (loss: 0.1240217462182045, acc: 0.9790576100349426)
[2025-02-13 20:18:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:45][root][INFO] - Training Epoch: 2/2, step 3078/7134 completed (loss: 0.07422249019145966, acc: 0.9791666865348816)
[2025-02-13 20:18:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:45][root][INFO] - Training Epoch: 2/2, step 3079/7134 completed (loss: 0.13166135549545288, acc: 0.9735449552536011)
[2025-02-13 20:18:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:46][root][INFO] - Training Epoch: 2/2, step 3080/7134 completed (loss: 0.1855945885181427, acc: 0.9583333134651184)
[2025-02-13 20:18:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:46][root][INFO] - Training Epoch: 2/2, step 3081/7134 completed (loss: 0.13732494413852692, acc: 0.9603960514068604)
[2025-02-13 20:18:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:46][root][INFO] - Training Epoch: 2/2, step 3082/7134 completed (loss: 0.23306308686733246, acc: 0.9552238583564758)
[2025-02-13 20:18:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:47][root][INFO] - Training Epoch: 2/2, step 3083/7134 completed (loss: 0.0669398307800293, acc: 0.9891892075538635)
[2025-02-13 20:18:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:47][root][INFO] - Training Epoch: 2/2, step 3084/7134 completed (loss: 0.12193375080823898, acc: 0.9581151604652405)
[2025-02-13 20:18:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:48][root][INFO] - Training Epoch: 2/2, step 3085/7134 completed (loss: 0.1573760062456131, acc: 0.9677419066429138)
[2025-02-13 20:18:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:48][root][INFO] - Training Epoch: 2/2, step 3086/7134 completed (loss: 0.12934668362140656, acc: 0.9817351698875427)
[2025-02-13 20:18:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:48][root][INFO] - Training Epoch: 2/2, step 3087/7134 completed (loss: 0.0936521515250206, acc: 0.9822485446929932)
[2025-02-13 20:18:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:49][root][INFO] - Training Epoch: 2/2, step 3088/7134 completed (loss: 0.13421638309955597, acc: 0.9672897458076477)
[2025-02-13 20:18:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:49][root][INFO] - Training Epoch: 2/2, step 3089/7134 completed (loss: 0.060721445828676224, acc: 0.9860464930534363)
[2025-02-13 20:18:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:50][root][INFO] - Training Epoch: 2/2, step 3090/7134 completed (loss: 0.29499921202659607, acc: 0.9548022747039795)
[2025-02-13 20:18:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:50][root][INFO] - Training Epoch: 2/2, step 3091/7134 completed (loss: 0.3523156940937042, acc: 0.9184549450874329)
[2025-02-13 20:18:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:50][root][INFO] - Training Epoch: 2/2, step 3092/7134 completed (loss: 0.22427713871002197, acc: 0.9236640930175781)
[2025-02-13 20:18:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:51][root][INFO] - Training Epoch: 2/2, step 3093/7134 completed (loss: 0.13856148719787598, acc: 0.9718309640884399)
[2025-02-13 20:18:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:51][root][INFO] - Training Epoch: 2/2, step 3094/7134 completed (loss: 0.07983892410993576, acc: 0.9884393215179443)
[2025-02-13 20:18:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:52][root][INFO] - Training Epoch: 2/2, step 3095/7134 completed (loss: 0.19447112083435059, acc: 0.9666666388511658)
[2025-02-13 20:18:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:52][root][INFO] - Training Epoch: 2/2, step 3096/7134 completed (loss: 0.12079406529664993, acc: 0.963350772857666)
[2025-02-13 20:18:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:52][root][INFO] - Training Epoch: 2/2, step 3097/7134 completed (loss: 0.10301879793405533, acc: 0.9756097793579102)
[2025-02-13 20:18:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:53][root][INFO] - Training Epoch: 2/2, step 3098/7134 completed (loss: 0.1962575912475586, acc: 0.9673202633857727)
[2025-02-13 20:18:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:53][root][INFO] - Training Epoch: 2/2, step 3099/7134 completed (loss: 0.12454479932785034, acc: 0.956204354763031)
[2025-02-13 20:18:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:54][root][INFO] - Training Epoch: 2/2, step 3100/7134 completed (loss: 0.10896018892526627, acc: 0.9865471124649048)
[2025-02-13 20:18:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:54][root][INFO] - Training Epoch: 2/2, step 3101/7134 completed (loss: 0.09699130803346634, acc: 0.9746192693710327)
[2025-02-13 20:18:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:54][root][INFO] - Training Epoch: 2/2, step 3102/7134 completed (loss: 0.06655842065811157, acc: 0.9938650131225586)
[2025-02-13 20:18:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:55][root][INFO] - Training Epoch: 2/2, step 3103/7134 completed (loss: 0.09634663909673691, acc: 0.9784482717514038)
[2025-02-13 20:18:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:55][root][INFO] - Training Epoch: 2/2, step 3104/7134 completed (loss: 0.05434265732765198, acc: 0.9844961166381836)
[2025-02-13 20:18:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:56][root][INFO] - Training Epoch: 2/2, step 3105/7134 completed (loss: 0.13311444222927094, acc: 0.97826087474823)
[2025-02-13 20:18:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:56][root][INFO] - Training Epoch: 2/2, step 3106/7134 completed (loss: 0.09519555419683456, acc: 0.9597315192222595)
[2025-02-13 20:18:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:57][root][INFO] - Training Epoch: 2/2, step 3107/7134 completed (loss: 0.06082627549767494, acc: 0.9767441749572754)
[2025-02-13 20:18:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:57][root][INFO] - Training Epoch: 2/2, step 3108/7134 completed (loss: 0.09470450133085251, acc: 0.9727891087532043)
[2025-02-13 20:18:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:57][root][INFO] - Training Epoch: 2/2, step 3109/7134 completed (loss: 0.11418084800243378, acc: 0.9769230484962463)
[2025-02-13 20:18:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:58][root][INFO] - Training Epoch: 2/2, step 3110/7134 completed (loss: 0.06408035755157471, acc: 0.9708737730979919)
[2025-02-13 20:18:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:58][root][INFO] - Training Epoch: 2/2, step 3111/7134 completed (loss: 0.2643987238407135, acc: 0.9202898740768433)
[2025-02-13 20:18:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:59][root][INFO] - Training Epoch: 2/2, step 3112/7134 completed (loss: 0.25812071561813354, acc: 0.9489051103591919)
[2025-02-13 20:18:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:59][root][INFO] - Training Epoch: 2/2, step 3113/7134 completed (loss: 0.08124460279941559, acc: 0.9817073345184326)
[2025-02-13 20:18:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:18:59][root][INFO] - Training Epoch: 2/2, step 3114/7134 completed (loss: 0.06245013698935509, acc: 0.9849624037742615)
[2025-02-13 20:18:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:00][root][INFO] - Training Epoch: 2/2, step 3115/7134 completed (loss: 0.10689021646976471, acc: 0.9802631735801697)
[2025-02-13 20:19:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:00][root][INFO] - Training Epoch: 2/2, step 3116/7134 completed (loss: 0.1339440941810608, acc: 0.9774011373519897)
[2025-02-13 20:19:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:01][root][INFO] - Training Epoch: 2/2, step 3117/7134 completed (loss: 0.1784716546535492, acc: 0.966292142868042)
[2025-02-13 20:19:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:01][root][INFO] - Training Epoch: 2/2, step 3118/7134 completed (loss: 0.3018394708633423, acc: 0.9558011293411255)
[2025-02-13 20:19:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:01][root][INFO] - Training Epoch: 2/2, step 3119/7134 completed (loss: 0.2392028570175171, acc: 0.9576719403266907)
[2025-02-13 20:19:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:02][root][INFO] - Training Epoch: 2/2, step 3120/7134 completed (loss: 0.23947666585445404, acc: 0.9567901492118835)
[2025-02-13 20:19:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:02][root][INFO] - Training Epoch: 2/2, step 3121/7134 completed (loss: 0.11367134749889374, acc: 0.9715909361839294)
[2025-02-13 20:19:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:03][root][INFO] - Training Epoch: 2/2, step 3122/7134 completed (loss: 0.15257783234119415, acc: 0.9631578922271729)
[2025-02-13 20:19:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:03][root][INFO] - Training Epoch: 2/2, step 3123/7134 completed (loss: 0.12641845643520355, acc: 0.9567567706108093)
[2025-02-13 20:19:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:03][root][INFO] - Training Epoch: 2/2, step 3124/7134 completed (loss: 0.1883115917444229, acc: 0.9555555582046509)
[2025-02-13 20:19:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:04][root][INFO] - Training Epoch: 2/2, step 3125/7134 completed (loss: 0.05965656414628029, acc: 0.9863945841789246)
[2025-02-13 20:19:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:04][root][INFO] - Training Epoch: 2/2, step 3126/7134 completed (loss: 0.17178933322429657, acc: 0.9664804339408875)
[2025-02-13 20:19:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:05][root][INFO] - Training Epoch: 2/2, step 3127/7134 completed (loss: 0.10582657158374786, acc: 0.9812206625938416)
[2025-02-13 20:19:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:05][root][INFO] - Training Epoch: 2/2, step 3128/7134 completed (loss: 0.21218430995941162, acc: 0.9572192430496216)
[2025-02-13 20:19:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:05][root][INFO] - Training Epoch: 2/2, step 3129/7134 completed (loss: 0.12915605306625366, acc: 0.9772727489471436)
[2025-02-13 20:19:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:06][root][INFO] - Training Epoch: 2/2, step 3130/7134 completed (loss: 0.04891001060605049, acc: 0.9847715497016907)
[2025-02-13 20:19:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:06][root][INFO] - Training Epoch: 2/2, step 3131/7134 completed (loss: 0.13576364517211914, acc: 0.9661017060279846)
[2025-02-13 20:19:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:07][root][INFO] - Training Epoch: 2/2, step 3132/7134 completed (loss: 0.028112463653087616, acc: 1.0)
[2025-02-13 20:19:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:07][root][INFO] - Training Epoch: 2/2, step 3133/7134 completed (loss: 0.2545769214630127, acc: 0.9659090638160706)
[2025-02-13 20:19:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:08][root][INFO] - Training Epoch: 2/2, step 3134/7134 completed (loss: 0.09008399397134781, acc: 0.9765625)
[2025-02-13 20:19:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:08][root][INFO] - Training Epoch: 2/2, step 3135/7134 completed (loss: 0.12155238538980484, acc: 0.9587628841400146)
[2025-02-13 20:19:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:08][root][INFO] - Training Epoch: 2/2, step 3136/7134 completed (loss: 0.07271755486726761, acc: 0.9770992398262024)
[2025-02-13 20:19:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:09][root][INFO] - Training Epoch: 2/2, step 3137/7134 completed (loss: 0.07383554428815842, acc: 0.9858155846595764)
[2025-02-13 20:19:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:09][root][INFO] - Training Epoch: 2/2, step 3138/7134 completed (loss: 0.19630999863147736, acc: 0.9777777791023254)
[2025-02-13 20:19:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:09][root][INFO] - Training Epoch: 2/2, step 3139/7134 completed (loss: 0.16315717995166779, acc: 0.9640287756919861)
[2025-02-13 20:19:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:10][root][INFO] - Training Epoch: 2/2, step 3140/7134 completed (loss: 0.11331571638584137, acc: 0.9794520735740662)
[2025-02-13 20:19:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:10][root][INFO] - Training Epoch: 2/2, step 3141/7134 completed (loss: 0.17025746405124664, acc: 0.9729729890823364)
[2025-02-13 20:19:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:11][root][INFO] - Training Epoch: 2/2, step 3142/7134 completed (loss: 0.08018699288368225, acc: 0.976047933101654)
[2025-02-13 20:19:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:11][root][INFO] - Training Epoch: 2/2, step 3143/7134 completed (loss: 0.15280967950820923, acc: 0.9677419066429138)
[2025-02-13 20:19:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:12][root][INFO] - Training Epoch: 2/2, step 3144/7134 completed (loss: 0.10417509078979492, acc: 0.9807692170143127)
[2025-02-13 20:19:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:12][root][INFO] - Training Epoch: 2/2, step 3145/7134 completed (loss: 0.13568100333213806, acc: 0.9729729890823364)
[2025-02-13 20:19:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:12][root][INFO] - Training Epoch: 2/2, step 3146/7134 completed (loss: 0.1590574085712433, acc: 0.9545454382896423)
[2025-02-13 20:19:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:13][root][INFO] - Training Epoch: 2/2, step 3147/7134 completed (loss: 0.10375300794839859, acc: 0.9863013625144958)
[2025-02-13 20:19:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:13][root][INFO] - Training Epoch: 2/2, step 3148/7134 completed (loss: 0.05788882449269295, acc: 0.9888268113136292)
[2025-02-13 20:19:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:14][root][INFO] - Training Epoch: 2/2, step 3149/7134 completed (loss: 0.05147701874375343, acc: 0.9935483932495117)
[2025-02-13 20:19:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:14][root][INFO] - Training Epoch: 2/2, step 3150/7134 completed (loss: 0.7463499903678894, acc: 0.8500000238418579)
[2025-02-13 20:19:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:14][root][INFO] - Training Epoch: 2/2, step 3151/7134 completed (loss: 0.18755249679088593, acc: 0.9618320465087891)
[2025-02-13 20:19:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:15][root][INFO] - Training Epoch: 2/2, step 3152/7134 completed (loss: 0.09304818511009216, acc: 0.9788732528686523)
[2025-02-13 20:19:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:15][root][INFO] - Training Epoch: 2/2, step 3153/7134 completed (loss: 0.13063615560531616, acc: 0.9719101190567017)
[2025-02-13 20:19:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:16][root][INFO] - Training Epoch: 2/2, step 3154/7134 completed (loss: 0.18985295295715332, acc: 0.9624060392379761)
[2025-02-13 20:19:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:16][root][INFO] - Training Epoch: 2/2, step 3155/7134 completed (loss: 0.3136417269706726, acc: 0.9398496150970459)
[2025-02-13 20:19:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:16][root][INFO] - Training Epoch: 2/2, step 3156/7134 completed (loss: 0.34773871302604675, acc: 0.9244186282157898)
[2025-02-13 20:19:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:17][root][INFO] - Training Epoch: 2/2, step 3157/7134 completed (loss: 0.14570534229278564, acc: 0.947826087474823)
[2025-02-13 20:19:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:17][root][INFO] - Training Epoch: 2/2, step 3158/7134 completed (loss: 0.0986727848649025, acc: 0.978723406791687)
[2025-02-13 20:19:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:18][root][INFO] - Training Epoch: 2/2, step 3159/7134 completed (loss: 0.19139645993709564, acc: 0.9587628841400146)
[2025-02-13 20:19:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:18][root][INFO] - Training Epoch: 2/2, step 3160/7134 completed (loss: 0.07835087925195694, acc: 0.9807692170143127)
[2025-02-13 20:19:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:18][root][INFO] - Training Epoch: 2/2, step 3161/7134 completed (loss: 0.15197929739952087, acc: 0.9617834687232971)
[2025-02-13 20:19:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:19][root][INFO] - Training Epoch: 2/2, step 3162/7134 completed (loss: 0.056208278983831406, acc: 0.9863013625144958)
[2025-02-13 20:19:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:19][root][INFO] - Training Epoch: 2/2, step 3163/7134 completed (loss: 0.0333615206182003, acc: 1.0)
[2025-02-13 20:19:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:20][root][INFO] - Training Epoch: 2/2, step 3164/7134 completed (loss: 0.0898727998137474, acc: 0.9855072498321533)
[2025-02-13 20:19:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:20][root][INFO] - Training Epoch: 2/2, step 3165/7134 completed (loss: 0.09422291815280914, acc: 0.9860140085220337)
[2025-02-13 20:19:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:20][root][INFO] - Training Epoch: 2/2, step 3166/7134 completed (loss: 0.2097092568874359, acc: 0.9556962251663208)
[2025-02-13 20:19:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:21][root][INFO] - Training Epoch: 2/2, step 3167/7134 completed (loss: 0.1310475766658783, acc: 0.9814814925193787)
[2025-02-13 20:19:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:21][root][INFO] - Training Epoch: 2/2, step 3168/7134 completed (loss: 0.12750446796417236, acc: 0.9673202633857727)
[2025-02-13 20:19:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:21][root][INFO] - Training Epoch: 2/2, step 3169/7134 completed (loss: 0.1692122370004654, acc: 0.9508196711540222)
[2025-02-13 20:19:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:22][root][INFO] - Training Epoch: 2/2, step 3170/7134 completed (loss: 0.07720430940389633, acc: 0.9779411554336548)
[2025-02-13 20:19:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:22][root][INFO] - Training Epoch: 2/2, step 3171/7134 completed (loss: 0.09530513733625412, acc: 0.9725274443626404)
[2025-02-13 20:19:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:23][root][INFO] - Training Epoch: 2/2, step 3172/7134 completed (loss: 0.09446821361780167, acc: 0.9603174328804016)
[2025-02-13 20:19:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:23][root][INFO] - Training Epoch: 2/2, step 3173/7134 completed (loss: 0.08207530528306961, acc: 0.9797979593276978)
[2025-02-13 20:19:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:23][root][INFO] - Training Epoch: 2/2, step 3174/7134 completed (loss: 0.17225636541843414, acc: 0.9518072009086609)
[2025-02-13 20:19:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:24][root][INFO] - Training Epoch: 2/2, step 3175/7134 completed (loss: 0.13794362545013428, acc: 0.9709302186965942)
[2025-02-13 20:19:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:24][root][INFO] - Training Epoch: 2/2, step 3176/7134 completed (loss: 0.2464742511510849, acc: 0.9360465407371521)
[2025-02-13 20:19:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:25][root][INFO] - Training Epoch: 2/2, step 3177/7134 completed (loss: 0.17121849954128265, acc: 0.9753086566925049)
[2025-02-13 20:19:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:25][root][INFO] - Training Epoch: 2/2, step 3178/7134 completed (loss: 0.06895595788955688, acc: 0.9800000190734863)
[2025-02-13 20:19:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:25][root][INFO] - Training Epoch: 2/2, step 3179/7134 completed (loss: 0.1700371503829956, acc: 0.9464285969734192)
[2025-02-13 20:19:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:26][root][INFO] - Training Epoch: 2/2, step 3180/7134 completed (loss: 0.12354554235935211, acc: 0.9691358208656311)
[2025-02-13 20:19:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:26][root][INFO] - Training Epoch: 2/2, step 3181/7134 completed (loss: 0.11909398436546326, acc: 0.9668508172035217)
[2025-02-13 20:19:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:26][root][INFO] - Training Epoch: 2/2, step 3182/7134 completed (loss: 0.07584378123283386, acc: 0.9817073345184326)
[2025-02-13 20:19:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:27][root][INFO] - Training Epoch: 2/2, step 3183/7134 completed (loss: 0.09045014530420303, acc: 0.9781022071838379)
[2025-02-13 20:19:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:27][root][INFO] - Training Epoch: 2/2, step 3184/7134 completed (loss: 0.08606290817260742, acc: 0.987500011920929)
[2025-02-13 20:19:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:28][root][INFO] - Training Epoch: 2/2, step 3185/7134 completed (loss: 0.05399397388100624, acc: 0.9874213933944702)
[2025-02-13 20:19:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:28][root][INFO] - Training Epoch: 2/2, step 3186/7134 completed (loss: 0.10245835781097412, acc: 0.9807692170143127)
[2025-02-13 20:19:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:28][root][INFO] - Training Epoch: 2/2, step 3187/7134 completed (loss: 0.14014282822608948, acc: 0.9642857313156128)
[2025-02-13 20:19:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:29][root][INFO] - Training Epoch: 2/2, step 3188/7134 completed (loss: 0.06592655181884766, acc: 0.9832402467727661)
[2025-02-13 20:19:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:29][root][INFO] - Training Epoch: 2/2, step 3189/7134 completed (loss: 0.16571097075939178, acc: 0.9753086566925049)
[2025-02-13 20:19:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:29][root][INFO] - Training Epoch: 2/2, step 3190/7134 completed (loss: 0.13887083530426025, acc: 0.956250011920929)
[2025-02-13 20:19:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:30][root][INFO] - Training Epoch: 2/2, step 3191/7134 completed (loss: 0.07071288675069809, acc: 0.9937888383865356)
[2025-02-13 20:19:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:30][root][INFO] - Training Epoch: 2/2, step 3192/7134 completed (loss: 0.3536500632762909, acc: 0.956204354763031)
[2025-02-13 20:19:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:31][root][INFO] - Training Epoch: 2/2, step 3193/7134 completed (loss: 0.18622364103794098, acc: 0.9459459185600281)
[2025-02-13 20:19:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:31][root][INFO] - Training Epoch: 2/2, step 3194/7134 completed (loss: 0.10157245397567749, acc: 0.9757575988769531)
[2025-02-13 20:19:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:31][root][INFO] - Training Epoch: 2/2, step 3195/7134 completed (loss: 0.178836852312088, acc: 0.9503105878829956)
[2025-02-13 20:19:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:32][root][INFO] - Training Epoch: 2/2, step 3196/7134 completed (loss: 0.04668784141540527, acc: 0.9855072498321533)
[2025-02-13 20:19:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:32][root][INFO] - Training Epoch: 2/2, step 3197/7134 completed (loss: 0.10830885171890259, acc: 0.9880239367485046)
[2025-02-13 20:19:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:32][root][INFO] - Training Epoch: 2/2, step 3198/7134 completed (loss: 0.10003330558538437, acc: 0.9750000238418579)
[2025-02-13 20:19:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:33][root][INFO] - Training Epoch: 2/2, step 3199/7134 completed (loss: 0.06201205030083656, acc: 0.9883720874786377)
[2025-02-13 20:19:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:33][root][INFO] - Training Epoch: 2/2, step 3200/7134 completed (loss: 0.18997922539710999, acc: 0.957446813583374)
[2025-02-13 20:19:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:34][root][INFO] - Training Epoch: 2/2, step 3201/7134 completed (loss: 0.12965045869350433, acc: 0.9763779640197754)
[2025-02-13 20:19:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:34][root][INFO] - Training Epoch: 2/2, step 3202/7134 completed (loss: 0.10896208137273788, acc: 0.966292142868042)
[2025-02-13 20:19:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:34][root][INFO] - Training Epoch: 2/2, step 3203/7134 completed (loss: 0.12362001091241837, acc: 0.96875)
[2025-02-13 20:19:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:35][root][INFO] - Training Epoch: 2/2, step 3204/7134 completed (loss: 0.1269095540046692, acc: 0.9589040875434875)
[2025-02-13 20:19:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:35][root][INFO] - Training Epoch: 2/2, step 3205/7134 completed (loss: 0.06747418642044067, acc: 0.9858155846595764)
[2025-02-13 20:19:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:36][root][INFO] - Training Epoch: 2/2, step 3206/7134 completed (loss: 0.0843428373336792, acc: 0.9818181991577148)
[2025-02-13 20:19:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:36][root][INFO] - Training Epoch: 2/2, step 3207/7134 completed (loss: 0.14996220171451569, acc: 0.9631901979446411)
[2025-02-13 20:19:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:36][root][INFO] - Training Epoch: 2/2, step 3208/7134 completed (loss: 0.11230059713125229, acc: 0.9640287756919861)
[2025-02-13 20:19:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:37][root][INFO] - Training Epoch: 2/2, step 3209/7134 completed (loss: 0.20624741911888123, acc: 0.96875)
[2025-02-13 20:19:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:37][root][INFO] - Training Epoch: 2/2, step 3210/7134 completed (loss: 0.042194779962301254, acc: 0.9870967864990234)
[2025-02-13 20:19:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:37][root][INFO] - Training Epoch: 2/2, step 3211/7134 completed (loss: 0.09646609425544739, acc: 0.9915966391563416)
[2025-02-13 20:19:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:38][root][INFO] - Training Epoch: 2/2, step 3212/7134 completed (loss: 0.05712033808231354, acc: 0.988304078578949)
[2025-02-13 20:19:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:38][root][INFO] - Training Epoch: 2/2, step 3213/7134 completed (loss: 0.16324609518051147, acc: 0.9642857313156128)
[2025-02-13 20:19:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:39][root][INFO] - Training Epoch: 2/2, step 3214/7134 completed (loss: 0.09083490073680878, acc: 0.9766082167625427)
[2025-02-13 20:19:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:39][root][INFO] - Training Epoch: 2/2, step 3215/7134 completed (loss: 0.07489430159330368, acc: 0.9704142212867737)
[2025-02-13 20:19:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:39][root][INFO] - Training Epoch: 2/2, step 3216/7134 completed (loss: 0.0637446939945221, acc: 0.9753086566925049)
[2025-02-13 20:19:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:40][root][INFO] - Training Epoch: 2/2, step 3217/7134 completed (loss: 0.05586624518036842, acc: 0.983146071434021)
[2025-02-13 20:19:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:40][root][INFO] - Training Epoch: 2/2, step 3218/7134 completed (loss: 0.08542027324438095, acc: 0.9870967864990234)
[2025-02-13 20:19:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:41][root][INFO] - Training Epoch: 2/2, step 3219/7134 completed (loss: 0.161844402551651, acc: 0.9583333134651184)
[2025-02-13 20:19:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:41][root][INFO] - Training Epoch: 2/2, step 3220/7134 completed (loss: 0.09291303902864456, acc: 0.9604519605636597)
[2025-02-13 20:19:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:41][root][INFO] - Training Epoch: 2/2, step 3221/7134 completed (loss: 0.052141014486551285, acc: 0.9838709831237793)
[2025-02-13 20:19:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:42][root][INFO] - Training Epoch: 2/2, step 3222/7134 completed (loss: 0.18638069927692413, acc: 0.9452736377716064)
[2025-02-13 20:19:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:42][root][INFO] - Training Epoch: 2/2, step 3223/7134 completed (loss: 0.2425815612077713, acc: 0.9570552110671997)
[2025-02-13 20:19:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:43][root][INFO] - Training Epoch: 2/2, step 3224/7134 completed (loss: 0.15304400026798248, acc: 0.9653179049491882)
[2025-02-13 20:19:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:43][root][INFO] - Training Epoch: 2/2, step 3225/7134 completed (loss: 0.07911577075719833, acc: 0.9750000238418579)
[2025-02-13 20:19:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:43][root][INFO] - Training Epoch: 2/2, step 3226/7134 completed (loss: 0.14867256581783295, acc: 0.9668874144554138)
[2025-02-13 20:19:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:44][root][INFO] - Training Epoch: 2/2, step 3227/7134 completed (loss: 0.20261411368846893, acc: 0.9506173133850098)
[2025-02-13 20:19:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:44][root][INFO] - Training Epoch: 2/2, step 3228/7134 completed (loss: 0.09755407273769379, acc: 0.9747474789619446)
[2025-02-13 20:19:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:45][root][INFO] - Training Epoch: 2/2, step 3229/7134 completed (loss: 0.1582939475774765, acc: 0.9646464586257935)
[2025-02-13 20:19:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:45][root][INFO] - Training Epoch: 2/2, step 3230/7134 completed (loss: 0.1035834327340126, acc: 0.96875)
[2025-02-13 20:19:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:45][root][INFO] - Training Epoch: 2/2, step 3231/7134 completed (loss: 0.14837005734443665, acc: 0.9615384340286255)
[2025-02-13 20:19:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:46][root][INFO] - Training Epoch: 2/2, step 3232/7134 completed (loss: 0.04930698499083519, acc: 0.9907407164573669)
[2025-02-13 20:19:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:46][root][INFO] - Training Epoch: 2/2, step 3233/7134 completed (loss: 0.1262378841638565, acc: 0.9685039520263672)
[2025-02-13 20:19:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:47][root][INFO] - Training Epoch: 2/2, step 3234/7134 completed (loss: 0.04572861269116402, acc: 0.9937106966972351)
[2025-02-13 20:19:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:47][root][INFO] - Training Epoch: 2/2, step 3235/7134 completed (loss: 0.05817882716655731, acc: 0.9858155846595764)
[2025-02-13 20:19:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:47][root][INFO] - Training Epoch: 2/2, step 3236/7134 completed (loss: 0.18743491172790527, acc: 0.9661017060279846)
[2025-02-13 20:19:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:48][root][INFO] - Training Epoch: 2/2, step 3237/7134 completed (loss: 0.08066235482692719, acc: 0.9805194735527039)
[2025-02-13 20:19:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:48][root][INFO] - Training Epoch: 2/2, step 3238/7134 completed (loss: 0.1258114129304886, acc: 0.9722222089767456)
[2025-02-13 20:19:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:48][root][INFO] - Training Epoch: 2/2, step 3239/7134 completed (loss: 0.10805357992649078, acc: 0.9817073345184326)
[2025-02-13 20:19:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:49][root][INFO] - Training Epoch: 2/2, step 3240/7134 completed (loss: 0.056712426245212555, acc: 0.9776536226272583)
[2025-02-13 20:19:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:49][root][INFO] - Training Epoch: 2/2, step 3241/7134 completed (loss: 0.06068747863173485, acc: 0.9882352948188782)
[2025-02-13 20:19:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:50][root][INFO] - Training Epoch: 2/2, step 3242/7134 completed (loss: 0.029333526268601418, acc: 0.9870129823684692)
[2025-02-13 20:19:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:50][root][INFO] - Training Epoch: 2/2, step 3243/7134 completed (loss: 0.06449885666370392, acc: 0.9786096215248108)
[2025-02-13 20:19:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:50][root][INFO] - Training Epoch: 2/2, step 3244/7134 completed (loss: 0.13141830265522003, acc: 0.9768785834312439)
[2025-02-13 20:19:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:51][root][INFO] - Training Epoch: 2/2, step 3245/7134 completed (loss: 0.09165332466363907, acc: 0.9794871807098389)
[2025-02-13 20:19:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:51][root][INFO] - Training Epoch: 2/2, step 3246/7134 completed (loss: 0.11737751960754395, acc: 0.9794871807098389)
[2025-02-13 20:19:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:51][root][INFO] - Training Epoch: 2/2, step 3247/7134 completed (loss: 0.06544314324855804, acc: 0.9807692170143127)
[2025-02-13 20:19:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:52][root][INFO] - Training Epoch: 2/2, step 3248/7134 completed (loss: 0.05477874353528023, acc: 0.9927007555961609)
[2025-02-13 20:19:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:52][root][INFO] - Training Epoch: 2/2, step 3249/7134 completed (loss: 0.05110328271985054, acc: 0.976331353187561)
[2025-02-13 20:19:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:53][root][INFO] - Training Epoch: 2/2, step 3250/7134 completed (loss: 0.11992552876472473, acc: 0.976047933101654)
[2025-02-13 20:19:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:53][root][INFO] - Training Epoch: 2/2, step 3251/7134 completed (loss: 0.06101510301232338, acc: 0.9871794581413269)
[2025-02-13 20:19:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:54][root][INFO] - Training Epoch: 2/2, step 3252/7134 completed (loss: 0.029549168422818184, acc: 0.9939758777618408)
[2025-02-13 20:19:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:54][root][INFO] - Training Epoch: 2/2, step 3253/7134 completed (loss: 0.0753030776977539, acc: 0.9736841917037964)
[2025-02-13 20:19:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:54][root][INFO] - Training Epoch: 2/2, step 3254/7134 completed (loss: 0.05454060807824135, acc: 0.9805194735527039)
[2025-02-13 20:19:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:55][root][INFO] - Training Epoch: 2/2, step 3255/7134 completed (loss: 0.06783239543437958, acc: 0.9886363744735718)
[2025-02-13 20:19:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:55][root][INFO] - Training Epoch: 2/2, step 3256/7134 completed (loss: 0.08843951672315598, acc: 0.9813664555549622)
[2025-02-13 20:19:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:55][root][INFO] - Training Epoch: 2/2, step 3257/7134 completed (loss: 0.15724387764930725, acc: 0.9757575988769531)
[2025-02-13 20:19:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:56][root][INFO] - Training Epoch: 2/2, step 3258/7134 completed (loss: 0.22634072601795197, acc: 0.9512194991111755)
[2025-02-13 20:19:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:56][root][INFO] - Training Epoch: 2/2, step 3259/7134 completed (loss: 0.13992154598236084, acc: 0.95652174949646)
[2025-02-13 20:19:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:57][root][INFO] - Training Epoch: 2/2, step 3260/7134 completed (loss: 0.22438983619213104, acc: 0.9655172228813171)
[2025-02-13 20:19:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:57][root][INFO] - Training Epoch: 2/2, step 3261/7134 completed (loss: 0.09277694672346115, acc: 0.9781022071838379)
[2025-02-13 20:19:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:57][root][INFO] - Training Epoch: 2/2, step 3262/7134 completed (loss: 0.13118226826190948, acc: 0.9691358208656311)
[2025-02-13 20:19:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:58][root][INFO] - Training Epoch: 2/2, step 3263/7134 completed (loss: 0.1053033098578453, acc: 0.976331353187561)
[2025-02-13 20:19:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:58][root][INFO] - Training Epoch: 2/2, step 3264/7134 completed (loss: 0.06586333364248276, acc: 0.9878048896789551)
[2025-02-13 20:19:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:59][root][INFO] - Training Epoch: 2/2, step 3265/7134 completed (loss: 0.15631358325481415, acc: 0.9554139971733093)
[2025-02-13 20:19:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:59][root][INFO] - Training Epoch: 2/2, step 3266/7134 completed (loss: 0.06408178061246872, acc: 0.9862068891525269)
[2025-02-13 20:19:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:19:59][root][INFO] - Training Epoch: 2/2, step 3267/7134 completed (loss: 0.14312715828418732, acc: 0.9757575988769531)
[2025-02-13 20:19:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:00][root][INFO] - Training Epoch: 2/2, step 3268/7134 completed (loss: 0.08257946372032166, acc: 0.9863013625144958)
[2025-02-13 20:20:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:00][root][INFO] - Training Epoch: 2/2, step 3269/7134 completed (loss: 0.11044543981552124, acc: 0.978723406791687)
[2025-02-13 20:20:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:00][root][INFO] - Training Epoch: 2/2, step 3270/7134 completed (loss: 0.08196023106575012, acc: 0.9675675630569458)
[2025-02-13 20:20:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:01][root][INFO] - Training Epoch: 2/2, step 3271/7134 completed (loss: 0.0518898107111454, acc: 0.98591548204422)
[2025-02-13 20:20:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:01][root][INFO] - Training Epoch: 2/2, step 3272/7134 completed (loss: 0.09610358625650406, acc: 0.9866666793823242)
[2025-02-13 20:20:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:02][root][INFO] - Training Epoch: 2/2, step 3273/7134 completed (loss: 0.055379703640937805, acc: 0.9828571677207947)
[2025-02-13 20:20:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:02][root][INFO] - Training Epoch: 2/2, step 3274/7134 completed (loss: 0.05083513632416725, acc: 0.9875776171684265)
[2025-02-13 20:20:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:03][root][INFO] - Training Epoch: 2/2, step 3275/7134 completed (loss: 0.4825077950954437, acc: 0.916167676448822)
[2025-02-13 20:20:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:03][root][INFO] - Training Epoch: 2/2, step 3276/7134 completed (loss: 0.1259583979845047, acc: 0.9673202633857727)
[2025-02-13 20:20:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:03][root][INFO] - Training Epoch: 2/2, step 3277/7134 completed (loss: 0.13060559332370758, acc: 0.9640287756919861)
[2025-02-13 20:20:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:04][root][INFO] - Training Epoch: 2/2, step 3278/7134 completed (loss: 0.09805923700332642, acc: 0.975806474685669)
[2025-02-13 20:20:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:04][root][INFO] - Training Epoch: 2/2, step 3279/7134 completed (loss: 0.14920227229595184, acc: 0.9588235020637512)
[2025-02-13 20:20:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:05][root][INFO] - Training Epoch: 2/2, step 3280/7134 completed (loss: 0.2583916485309601, acc: 0.932692289352417)
[2025-02-13 20:20:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:05][root][INFO] - Training Epoch: 2/2, step 3281/7134 completed (loss: 0.1883895993232727, acc: 0.9727891087532043)
[2025-02-13 20:20:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:05][root][INFO] - Training Epoch: 2/2, step 3282/7134 completed (loss: 0.05528223142027855, acc: 0.9847328066825867)
[2025-02-13 20:20:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:06][root][INFO] - Training Epoch: 2/2, step 3283/7134 completed (loss: 0.07400525361299515, acc: 0.9901960492134094)
[2025-02-13 20:20:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:06][root][INFO] - Training Epoch: 2/2, step 3284/7134 completed (loss: 0.07400441914796829, acc: 0.9793103337287903)
[2025-02-13 20:20:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:07][root][INFO] - Training Epoch: 2/2, step 3285/7134 completed (loss: 0.10137534141540527, acc: 0.9779411554336548)
[2025-02-13 20:20:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:07][root][INFO] - Training Epoch: 2/2, step 3286/7134 completed (loss: 0.12052633613348007, acc: 0.9769230484962463)
[2025-02-13 20:20:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:07][root][INFO] - Training Epoch: 2/2, step 3287/7134 completed (loss: 0.11045750230550766, acc: 0.970370352268219)
[2025-02-13 20:20:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:08][root][INFO] - Training Epoch: 2/2, step 3288/7134 completed (loss: 0.1120559498667717, acc: 0.9784172773361206)
[2025-02-13 20:20:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:08][root][INFO] - Training Epoch: 2/2, step 3289/7134 completed (loss: 0.1237478107213974, acc: 0.970588207244873)
[2025-02-13 20:20:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:08][root][INFO] - Training Epoch: 2/2, step 3290/7134 completed (loss: 0.11137323826551437, acc: 0.9579831957817078)
[2025-02-13 20:20:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:09][root][INFO] - Training Epoch: 2/2, step 3291/7134 completed (loss: 0.11734205484390259, acc: 0.9702970385551453)
[2025-02-13 20:20:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:09][root][INFO] - Training Epoch: 2/2, step 3292/7134 completed (loss: 0.14946424961090088, acc: 0.9831932783126831)
[2025-02-13 20:20:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:10][root][INFO] - Training Epoch: 2/2, step 3293/7134 completed (loss: 0.08382254093885422, acc: 0.9924242496490479)
[2025-02-13 20:20:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:10][root][INFO] - Training Epoch: 2/2, step 3294/7134 completed (loss: 0.13630811870098114, acc: 0.9696969985961914)
[2025-02-13 20:20:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:10][root][INFO] - Training Epoch: 2/2, step 3295/7134 completed (loss: 0.027489716187119484, acc: 0.9919354915618896)
[2025-02-13 20:20:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:11][root][INFO] - Training Epoch: 2/2, step 3296/7134 completed (loss: 0.1372119039297104, acc: 0.9461538195610046)
[2025-02-13 20:20:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:11][root][INFO] - Training Epoch: 2/2, step 3297/7134 completed (loss: 0.14332441985607147, acc: 0.9622641801834106)
[2025-02-13 20:20:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:11][root][INFO] - Training Epoch: 2/2, step 3298/7134 completed (loss: 0.19302725791931152, acc: 0.9624060392379761)
[2025-02-13 20:20:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:12][root][INFO] - Training Epoch: 2/2, step 3299/7134 completed (loss: 0.09924816340208054, acc: 0.9842519760131836)
[2025-02-13 20:20:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:12][root][INFO] - Training Epoch: 2/2, step 3300/7134 completed (loss: 0.12523436546325684, acc: 0.9624060392379761)
[2025-02-13 20:20:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:13][root][INFO] - Training Epoch: 2/2, step 3301/7134 completed (loss: 0.08518647402524948, acc: 0.9708737730979919)
[2025-02-13 20:20:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:13][root][INFO] - Training Epoch: 2/2, step 3302/7134 completed (loss: 0.18662668764591217, acc: 0.9580419659614563)
[2025-02-13 20:20:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:13][root][INFO] - Training Epoch: 2/2, step 3303/7134 completed (loss: 0.0859031081199646, acc: 0.9702970385551453)
[2025-02-13 20:20:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:14][root][INFO] - Training Epoch: 2/2, step 3304/7134 completed (loss: 0.08992604911327362, acc: 0.9724770784378052)
[2025-02-13 20:20:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:14][root][INFO] - Training Epoch: 2/2, step 3305/7134 completed (loss: 0.11306607723236084, acc: 0.9811320900917053)
[2025-02-13 20:20:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:15][root][INFO] - Training Epoch: 2/2, step 3306/7134 completed (loss: 0.09664696455001831, acc: 0.959770143032074)
[2025-02-13 20:20:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:15][root][INFO] - Training Epoch: 2/2, step 3307/7134 completed (loss: 0.053583066910505295, acc: 0.9851852059364319)
[2025-02-13 20:20:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:15][root][INFO] - Training Epoch: 2/2, step 3308/7134 completed (loss: 0.1409444510936737, acc: 0.970588207244873)
[2025-02-13 20:20:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:16][root][INFO] - Training Epoch: 2/2, step 3309/7134 completed (loss: 0.129917174577713, acc: 0.9743589758872986)
[2025-02-13 20:20:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:16][root][INFO] - Training Epoch: 2/2, step 3310/7134 completed (loss: 0.0353718027472496, acc: 1.0)
[2025-02-13 20:20:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:17][root][INFO] - Training Epoch: 2/2, step 3311/7134 completed (loss: 0.10601138323545456, acc: 0.9702970385551453)
[2025-02-13 20:20:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:17][root][INFO] - Training Epoch: 2/2, step 3312/7134 completed (loss: 0.09712787717580795, acc: 0.9774436354637146)
[2025-02-13 20:20:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:17][root][INFO] - Training Epoch: 2/2, step 3313/7134 completed (loss: 0.12573277950286865, acc: 0.9795918464660645)
[2025-02-13 20:20:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:18][root][INFO] - Training Epoch: 2/2, step 3314/7134 completed (loss: 0.08896604925394058, acc: 0.9724770784378052)
[2025-02-13 20:20:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:18][root][INFO] - Training Epoch: 2/2, step 3315/7134 completed (loss: 0.12663261592388153, acc: 0.9704142212867737)
[2025-02-13 20:20:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:19][root][INFO] - Training Epoch: 2/2, step 3316/7134 completed (loss: 0.09440888464450836, acc: 0.9776119589805603)
[2025-02-13 20:20:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:19][root][INFO] - Training Epoch: 2/2, step 3317/7134 completed (loss: 0.11675411462783813, acc: 0.9752066135406494)
[2025-02-13 20:20:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:20][root][INFO] - Training Epoch: 2/2, step 3318/7134 completed (loss: 0.08168338239192963, acc: 0.9661017060279846)
[2025-02-13 20:20:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:20][root][INFO] - Training Epoch: 2/2, step 3319/7134 completed (loss: 0.15695305168628693, acc: 0.9492753744125366)
[2025-02-13 20:20:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:20][root][INFO] - Training Epoch: 2/2, step 3320/7134 completed (loss: 0.1649133861064911, acc: 0.9661017060279846)
[2025-02-13 20:20:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:21][root][INFO] - Training Epoch: 2/2, step 3321/7134 completed (loss: 0.16098913550376892, acc: 0.9714285731315613)
[2025-02-13 20:20:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:21][root][INFO] - Training Epoch: 2/2, step 3322/7134 completed (loss: 0.11017987877130508, acc: 0.9671052694320679)
[2025-02-13 20:20:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:22][root][INFO] - Training Epoch: 2/2, step 3323/7134 completed (loss: 0.12429473549127579, acc: 0.9590643048286438)
[2025-02-13 20:20:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:22][root][INFO] - Training Epoch: 2/2, step 3324/7134 completed (loss: 0.16585353016853333, acc: 0.9430052042007446)
[2025-02-13 20:20:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:22][root][INFO] - Training Epoch: 2/2, step 3325/7134 completed (loss: 0.1763981133699417, acc: 0.9580838084220886)
[2025-02-13 20:20:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:23][root][INFO] - Training Epoch: 2/2, step 3326/7134 completed (loss: 0.21240051090717316, acc: 0.9515151381492615)
[2025-02-13 20:20:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:23][root][INFO] - Training Epoch: 2/2, step 3327/7134 completed (loss: 0.10162784159183502, acc: 0.9723756909370422)
[2025-02-13 20:20:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:24][root][INFO] - Training Epoch: 2/2, step 3328/7134 completed (loss: 0.11618265509605408, acc: 0.9840425252914429)
[2025-02-13 20:20:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:24][root][INFO] - Training Epoch: 2/2, step 3329/7134 completed (loss: 0.18624430894851685, acc: 0.9635416865348816)
[2025-02-13 20:20:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:24][root][INFO] - Training Epoch: 2/2, step 3330/7134 completed (loss: 0.12972837686538696, acc: 0.9692307710647583)
[2025-02-13 20:20:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:25][root][INFO] - Training Epoch: 2/2, step 3331/7134 completed (loss: 0.2993563711643219, acc: 0.9171597361564636)
[2025-02-13 20:20:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:25][root][INFO] - Training Epoch: 2/2, step 3332/7134 completed (loss: 0.26130735874176025, acc: 0.95333331823349)
[2025-02-13 20:20:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:25][root][INFO] - Training Epoch: 2/2, step 3333/7134 completed (loss: 0.15187260508537292, acc: 0.9652174115180969)
[2025-02-13 20:20:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:26][root][INFO] - Training Epoch: 2/2, step 3334/7134 completed (loss: 0.11937518417835236, acc: 0.9836956262588501)
[2025-02-13 20:20:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:26][root][INFO] - Training Epoch: 2/2, step 3335/7134 completed (loss: 0.04302673414349556, acc: 0.9866666793823242)
[2025-02-13 20:20:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:27][root][INFO] - Training Epoch: 2/2, step 3336/7134 completed (loss: 0.02101455070078373, acc: 1.0)
[2025-02-13 20:20:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:27][root][INFO] - Training Epoch: 2/2, step 3337/7134 completed (loss: 0.06656457483768463, acc: 0.9915966391563416)
[2025-02-13 20:20:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:27][root][INFO] - Training Epoch: 2/2, step 3338/7134 completed (loss: 0.08988307416439056, acc: 0.984455943107605)
[2025-02-13 20:20:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:28][root][INFO] - Training Epoch: 2/2, step 3339/7134 completed (loss: 0.18814435601234436, acc: 0.9613526463508606)
[2025-02-13 20:20:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:28][root][INFO] - Training Epoch: 2/2, step 3340/7134 completed (loss: 0.1722833216190338, acc: 0.9345238208770752)
[2025-02-13 20:20:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:29][root][INFO] - Training Epoch: 2/2, step 3341/7134 completed (loss: 0.09854469448328018, acc: 0.976047933101654)
[2025-02-13 20:20:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:29][root][INFO] - Training Epoch: 2/2, step 3342/7134 completed (loss: 0.06314975768327713, acc: 0.9863013625144958)
[2025-02-13 20:20:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:29][root][INFO] - Training Epoch: 2/2, step 3343/7134 completed (loss: 0.05130474269390106, acc: 0.9913793206214905)
[2025-02-13 20:20:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:30][root][INFO] - Training Epoch: 2/2, step 3344/7134 completed (loss: 0.08687017112970352, acc: 0.9779411554336548)
[2025-02-13 20:20:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:30][root][INFO] - Training Epoch: 2/2, step 3345/7134 completed (loss: 0.09691174328327179, acc: 0.9790209531784058)
[2025-02-13 20:20:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:31][root][INFO] - Training Epoch: 2/2, step 3346/7134 completed (loss: 0.05936248600482941, acc: 0.9878048896789551)
[2025-02-13 20:20:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:31][root][INFO] - Training Epoch: 2/2, step 3347/7134 completed (loss: 0.2230508029460907, acc: 0.9542483687400818)
[2025-02-13 20:20:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:31][root][INFO] - Training Epoch: 2/2, step 3348/7134 completed (loss: 0.09597121924161911, acc: 0.9810126423835754)
[2025-02-13 20:20:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:32][root][INFO] - Training Epoch: 2/2, step 3349/7134 completed (loss: 0.062886543571949, acc: 0.9922480583190918)
[2025-02-13 20:20:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:32][root][INFO] - Training Epoch: 2/2, step 3350/7134 completed (loss: 0.07171604037284851, acc: 0.9752066135406494)
[2025-02-13 20:20:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:33][root][INFO] - Training Epoch: 2/2, step 3351/7134 completed (loss: 0.1866319179534912, acc: 0.9468085169792175)
[2025-02-13 20:20:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:33][root][INFO] - Training Epoch: 2/2, step 3352/7134 completed (loss: 0.1111781969666481, acc: 0.970370352268219)
[2025-02-13 20:20:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:33][root][INFO] - Training Epoch: 2/2, step 3353/7134 completed (loss: 0.07514530420303345, acc: 0.9756097793579102)
[2025-02-13 20:20:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:34][root][INFO] - Training Epoch: 2/2, step 3354/7134 completed (loss: 0.0406503789126873, acc: 0.9873417615890503)
[2025-02-13 20:20:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:34][root][INFO] - Training Epoch: 2/2, step 3355/7134 completed (loss: 0.03515021875500679, acc: 0.9878048896789551)
[2025-02-13 20:20:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:35][root][INFO] - Training Epoch: 2/2, step 3356/7134 completed (loss: 0.020995942875742912, acc: 1.0)
[2025-02-13 20:20:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:35][root][INFO] - Training Epoch: 2/2, step 3357/7134 completed (loss: 0.04698384553194046, acc: 0.9880239367485046)
[2025-02-13 20:20:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:35][root][INFO] - Training Epoch: 2/2, step 3358/7134 completed (loss: 0.22851118445396423, acc: 0.9558823704719543)
[2025-02-13 20:20:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:36][root][INFO] - Training Epoch: 2/2, step 3359/7134 completed (loss: 0.34381020069122314, acc: 0.965753436088562)
[2025-02-13 20:20:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:36][root][INFO] - Training Epoch: 2/2, step 3360/7134 completed (loss: 0.15289293229579926, acc: 0.95652174949646)
[2025-02-13 20:20:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:37][root][INFO] - Training Epoch: 2/2, step 3361/7134 completed (loss: 0.16141057014465332, acc: 0.9821428656578064)
[2025-02-13 20:20:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:37][root][INFO] - Training Epoch: 2/2, step 3362/7134 completed (loss: 0.12984928488731384, acc: 0.9727272987365723)
[2025-02-13 20:20:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:37][root][INFO] - Training Epoch: 2/2, step 3363/7134 completed (loss: 0.07043072581291199, acc: 0.9906542301177979)
[2025-02-13 20:20:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:38][root][INFO] - Training Epoch: 2/2, step 3364/7134 completed (loss: 0.27689123153686523, acc: 0.9411764740943909)
[2025-02-13 20:20:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:38][root][INFO] - Training Epoch: 2/2, step 3365/7134 completed (loss: 0.04929426312446594, acc: 0.991150438785553)
[2025-02-13 20:20:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:39][root][INFO] - Training Epoch: 2/2, step 3366/7134 completed (loss: 0.06183801591396332, acc: 0.9907407164573669)
[2025-02-13 20:20:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:39][root][INFO] - Training Epoch: 2/2, step 3367/7134 completed (loss: 0.058285903185606, acc: 0.9839572310447693)
[2025-02-13 20:20:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:40][root][INFO] - Training Epoch: 2/2, step 3368/7134 completed (loss: 0.08253458887338638, acc: 0.9689440727233887)
[2025-02-13 20:20:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:40][root][INFO] - Training Epoch: 2/2, step 3369/7134 completed (loss: 0.08939315378665924, acc: 0.9891892075538635)
[2025-02-13 20:20:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:40][root][INFO] - Training Epoch: 2/2, step 3370/7134 completed (loss: 0.044533152133226395, acc: 0.9815950989723206)
[2025-02-13 20:20:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:41][root][INFO] - Training Epoch: 2/2, step 3371/7134 completed (loss: 0.037142541259527206, acc: 0.9930555820465088)
[2025-02-13 20:20:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:41][root][INFO] - Training Epoch: 2/2, step 3372/7134 completed (loss: 0.08136694878339767, acc: 0.9863945841789246)
[2025-02-13 20:20:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:42][root][INFO] - Training Epoch: 2/2, step 3373/7134 completed (loss: 0.03510267287492752, acc: 0.9934640526771545)
[2025-02-13 20:20:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:42][root][INFO] - Training Epoch: 2/2, step 3374/7134 completed (loss: 0.03560981899499893, acc: 0.9942857027053833)
[2025-02-13 20:20:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:43][root][INFO] - Training Epoch: 2/2, step 3375/7134 completed (loss: 0.08791599422693253, acc: 0.9820359349250793)
[2025-02-13 20:20:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:43][root][INFO] - Training Epoch: 2/2, step 3376/7134 completed (loss: 0.03918501362204552, acc: 0.9947090148925781)
[2025-02-13 20:20:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:43][root][INFO] - Training Epoch: 2/2, step 3377/7134 completed (loss: 0.09564049541950226, acc: 0.979899525642395)
[2025-02-13 20:20:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:44][root][INFO] - Training Epoch: 2/2, step 3378/7134 completed (loss: 0.020022844895720482, acc: 1.0)
[2025-02-13 20:20:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:44][root][INFO] - Training Epoch: 2/2, step 3379/7134 completed (loss: 0.10585025697946548, acc: 0.9594594836235046)
[2025-02-13 20:20:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:45][root][INFO] - Training Epoch: 2/2, step 3380/7134 completed (loss: 0.10516331344842911, acc: 0.9710144996643066)
[2025-02-13 20:20:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:45][root][INFO] - Training Epoch: 2/2, step 3381/7134 completed (loss: 0.06532429158687592, acc: 0.9756097793579102)
[2025-02-13 20:20:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:45][root][INFO] - Training Epoch: 2/2, step 3382/7134 completed (loss: 0.21018005907535553, acc: 0.96875)
[2025-02-13 20:20:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:46][root][INFO] - Training Epoch: 2/2, step 3383/7134 completed (loss: 0.0725654736161232, acc: 0.9860140085220337)
[2025-02-13 20:20:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:46][root][INFO] - Training Epoch: 2/2, step 3384/7134 completed (loss: 0.10120384395122528, acc: 0.956204354763031)
[2025-02-13 20:20:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:47][root][INFO] - Training Epoch: 2/2, step 3385/7134 completed (loss: 0.046151746064424515, acc: 0.9861111044883728)
[2025-02-13 20:20:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:47][root][INFO] - Training Epoch: 2/2, step 3386/7134 completed (loss: 0.07142335176467896, acc: 0.976331353187561)
[2025-02-13 20:20:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:47][root][INFO] - Training Epoch: 2/2, step 3387/7134 completed (loss: 0.08216072618961334, acc: 0.9825581312179565)
[2025-02-13 20:20:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:48][root][INFO] - Training Epoch: 2/2, step 3388/7134 completed (loss: 0.2139899730682373, acc: 0.9572649598121643)
[2025-02-13 20:20:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:48][root][INFO] - Training Epoch: 2/2, step 3389/7134 completed (loss: 0.08401918411254883, acc: 0.9781420826911926)
[2025-02-13 20:20:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:49][root][INFO] - Training Epoch: 2/2, step 3390/7134 completed (loss: 0.07027725130319595, acc: 0.9715909361839294)
[2025-02-13 20:20:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:49][root][INFO] - Training Epoch: 2/2, step 3391/7134 completed (loss: 0.049805715680122375, acc: 0.9939024448394775)
[2025-02-13 20:20:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:49][root][INFO] - Training Epoch: 2/2, step 3392/7134 completed (loss: 0.05031576380133629, acc: 0.9923664331436157)
[2025-02-13 20:20:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:50][root][INFO] - Training Epoch: 2/2, step 3393/7134 completed (loss: 0.1314704269170761, acc: 0.9440559148788452)
[2025-02-13 20:20:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:50][root][INFO] - Training Epoch: 2/2, step 3394/7134 completed (loss: 0.087056964635849, acc: 0.9714285731315613)
[2025-02-13 20:20:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:50][root][INFO] - Training Epoch: 2/2, step 3395/7134 completed (loss: 0.1322496384382248, acc: 0.9736841917037964)
[2025-02-13 20:20:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:51][root][INFO] - Training Epoch: 2/2, step 3396/7134 completed (loss: 0.10556688904762268, acc: 0.9855769276618958)
[2025-02-13 20:20:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:51][root][INFO] - Training Epoch: 2/2, step 3397/7134 completed (loss: 0.0472266785800457, acc: 0.9900497794151306)
[2025-02-13 20:20:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:52][root][INFO] - Training Epoch: 2/2, step 3398/7134 completed (loss: 0.08493927121162415, acc: 0.9710144996643066)
[2025-02-13 20:20:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:52][root][INFO] - Training Epoch: 2/2, step 3399/7134 completed (loss: 0.21717458963394165, acc: 0.9444444179534912)
[2025-02-13 20:20:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:52][root][INFO] - Training Epoch: 2/2, step 3400/7134 completed (loss: 0.10060325264930725, acc: 0.9527559280395508)
[2025-02-13 20:20:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:53][root][INFO] - Training Epoch: 2/2, step 3401/7134 completed (loss: 0.13377420604228973, acc: 0.9591836929321289)
[2025-02-13 20:20:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:53][root][INFO] - Training Epoch: 2/2, step 3402/7134 completed (loss: 0.3148399889469147, acc: 0.9358288645744324)
[2025-02-13 20:20:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:54][root][INFO] - Training Epoch: 2/2, step 3403/7134 completed (loss: 0.31166598200798035, acc: 0.939393937587738)
[2025-02-13 20:20:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:54][root][INFO] - Training Epoch: 2/2, step 3404/7134 completed (loss: 0.05010943114757538, acc: 0.9937106966972351)
[2025-02-13 20:20:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:54][root][INFO] - Training Epoch: 2/2, step 3405/7134 completed (loss: 0.2513546645641327, acc: 0.9485294222831726)
[2025-02-13 20:20:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:55][root][INFO] - Training Epoch: 2/2, step 3406/7134 completed (loss: 0.1859225481748581, acc: 0.9350000023841858)
[2025-02-13 20:20:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:55][root][INFO] - Training Epoch: 2/2, step 3407/7134 completed (loss: 0.06727641075849533, acc: 0.9815950989723206)
[2025-02-13 20:20:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:55][root][INFO] - Training Epoch: 2/2, step 3408/7134 completed (loss: 0.048764418810606, acc: 0.9905660152435303)
[2025-02-13 20:20:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:56][root][INFO] - Training Epoch: 2/2, step 3409/7134 completed (loss: 0.1005808487534523, acc: 0.977011501789093)
[2025-02-13 20:20:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:56][root][INFO] - Training Epoch: 2/2, step 3410/7134 completed (loss: 0.2049015313386917, acc: 0.9695122241973877)
[2025-02-13 20:20:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:57][root][INFO] - Training Epoch: 2/2, step 3411/7134 completed (loss: 0.14312510192394257, acc: 0.9670329689979553)
[2025-02-13 20:20:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:57][root][INFO] - Training Epoch: 2/2, step 3412/7134 completed (loss: 0.12058315426111221, acc: 0.9666666388511658)
[2025-02-13 20:20:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:57][root][INFO] - Training Epoch: 2/2, step 3413/7134 completed (loss: 0.12403450161218643, acc: 0.9829059839248657)
[2025-02-13 20:20:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:58][root][INFO] - Training Epoch: 2/2, step 3414/7134 completed (loss: 0.07481372356414795, acc: 0.9791666865348816)
[2025-02-13 20:20:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:58][root][INFO] - Training Epoch: 2/2, step 3415/7134 completed (loss: 0.04864658787846565, acc: 0.9928571581840515)
[2025-02-13 20:20:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:59][root][INFO] - Training Epoch: 2/2, step 3416/7134 completed (loss: 0.1414499580860138, acc: 0.9726775884628296)
[2025-02-13 20:20:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:59][root][INFO] - Training Epoch: 2/2, step 3417/7134 completed (loss: 0.05573417618870735, acc: 0.9880239367485046)
[2025-02-13 20:20:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:20:59][root][INFO] - Training Epoch: 2/2, step 3418/7134 completed (loss: 0.11388979107141495, acc: 0.9754601120948792)
[2025-02-13 20:20:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:00][root][INFO] - Training Epoch: 2/2, step 3419/7134 completed (loss: 0.08223853260278702, acc: 0.9819276928901672)
[2025-02-13 20:21:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:00][root][INFO] - Training Epoch: 2/2, step 3420/7134 completed (loss: 0.05302288383245468, acc: 0.9933775067329407)
[2025-02-13 20:21:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:00][root][INFO] - Training Epoch: 2/2, step 3421/7134 completed (loss: 0.07821130752563477, acc: 0.9807692170143127)
[2025-02-13 20:21:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:01][root][INFO] - Training Epoch: 2/2, step 3422/7134 completed (loss: 0.05563545972108841, acc: 0.9852941036224365)
[2025-02-13 20:21:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:01][root][INFO] - Training Epoch: 2/2, step 3423/7134 completed (loss: 0.02568916045129299, acc: 0.9934210777282715)
[2025-02-13 20:21:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:02][root][INFO] - Training Epoch: 2/2, step 3424/7134 completed (loss: 0.1606704294681549, acc: 0.9599999785423279)
[2025-02-13 20:21:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:02][root][INFO] - Training Epoch: 2/2, step 3425/7134 completed (loss: 0.1349489390850067, acc: 0.9684210419654846)
[2025-02-13 20:21:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:02][root][INFO] - Training Epoch: 2/2, step 3426/7134 completed (loss: 0.15415875613689423, acc: 0.988095223903656)
[2025-02-13 20:21:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:03][root][INFO] - Training Epoch: 2/2, step 3427/7134 completed (loss: 0.10159991681575775, acc: 0.9805194735527039)
[2025-02-13 20:21:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:03][root][INFO] - Training Epoch: 2/2, step 3428/7134 completed (loss: 0.1491023153066635, acc: 0.9588235020637512)
[2025-02-13 20:21:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:04][root][INFO] - Training Epoch: 2/2, step 3429/7134 completed (loss: 0.06737728416919708, acc: 0.9788732528686523)
[2025-02-13 20:21:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:04][root][INFO] - Training Epoch: 2/2, step 3430/7134 completed (loss: 0.20768816769123077, acc: 0.9589040875434875)
[2025-02-13 20:21:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:04][root][INFO] - Training Epoch: 2/2, step 3431/7134 completed (loss: 0.07494139671325684, acc: 0.97826087474823)
[2025-02-13 20:21:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:05][root][INFO] - Training Epoch: 2/2, step 3432/7134 completed (loss: 0.08441261202096939, acc: 0.9754601120948792)
[2025-02-13 20:21:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:05][root][INFO] - Training Epoch: 2/2, step 3433/7134 completed (loss: 0.1637687087059021, acc: 0.970588207244873)
[2025-02-13 20:21:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:06][root][INFO] - Training Epoch: 2/2, step 3434/7134 completed (loss: 0.13177606463432312, acc: 0.9571428298950195)
[2025-02-13 20:21:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:06][root][INFO] - Training Epoch: 2/2, step 3435/7134 completed (loss: 0.11470315605401993, acc: 0.9724137783050537)
[2025-02-13 20:21:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:06][root][INFO] - Training Epoch: 2/2, step 3436/7134 completed (loss: 0.12223221361637115, acc: 0.970802903175354)
[2025-02-13 20:21:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:07][root][INFO] - Training Epoch: 2/2, step 3437/7134 completed (loss: 0.09966790676116943, acc: 0.9752066135406494)
[2025-02-13 20:21:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:07][root][INFO] - Training Epoch: 2/2, step 3438/7134 completed (loss: 0.07441935688257217, acc: 0.9912280440330505)
[2025-02-13 20:21:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:07][root][INFO] - Training Epoch: 2/2, step 3439/7134 completed (loss: 0.1369006186723709, acc: 0.9603174328804016)
[2025-02-13 20:21:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:08][root][INFO] - Training Epoch: 2/2, step 3440/7134 completed (loss: 0.17498132586479187, acc: 0.9568965435028076)
[2025-02-13 20:21:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:08][root][INFO] - Training Epoch: 2/2, step 3441/7134 completed (loss: 0.18662407994270325, acc: 0.9534883499145508)
[2025-02-13 20:21:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:09][root][INFO] - Training Epoch: 2/2, step 3442/7134 completed (loss: 0.06146230921149254, acc: 0.9858155846595764)
[2025-02-13 20:21:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:09][root][INFO] - Training Epoch: 2/2, step 3443/7134 completed (loss: 0.17847788333892822, acc: 0.966292142868042)
[2025-02-13 20:21:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:09][root][INFO] - Training Epoch: 2/2, step 3444/7134 completed (loss: 0.1416192203760147, acc: 0.9426751732826233)
[2025-02-13 20:21:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:10][root][INFO] - Training Epoch: 2/2, step 3445/7134 completed (loss: 0.254517138004303, acc: 0.9548872113227844)
[2025-02-13 20:21:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:10][root][INFO] - Training Epoch: 2/2, step 3446/7134 completed (loss: 0.16730628907680511, acc: 0.9666666388511658)
[2025-02-13 20:21:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:11][root][INFO] - Training Epoch: 2/2, step 3447/7134 completed (loss: 0.22213809192180634, acc: 0.9567901492118835)
[2025-02-13 20:21:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:11][root][INFO] - Training Epoch: 2/2, step 3448/7134 completed (loss: 0.1617160588502884, acc: 0.9653179049491882)
[2025-02-13 20:21:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:11][root][INFO] - Training Epoch: 2/2, step 3449/7134 completed (loss: 0.08801037073135376, acc: 0.9702970385551453)
[2025-02-13 20:21:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:12][root][INFO] - Training Epoch: 2/2, step 3450/7134 completed (loss: 0.17006507515907288, acc: 0.9607843160629272)
[2025-02-13 20:21:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:12][root][INFO] - Training Epoch: 2/2, step 3451/7134 completed (loss: 0.16917581856250763, acc: 0.957446813583374)
[2025-02-13 20:21:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:12][root][INFO] - Training Epoch: 2/2, step 3452/7134 completed (loss: 0.24544037878513336, acc: 0.9430894255638123)
[2025-02-13 20:21:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:13][root][INFO] - Training Epoch: 2/2, step 3453/7134 completed (loss: 0.17068302631378174, acc: 0.9513888955116272)
[2025-02-13 20:21:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:13][root][INFO] - Training Epoch: 2/2, step 3454/7134 completed (loss: 0.1941172182559967, acc: 0.9578313231468201)
[2025-02-13 20:21:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:14][root][INFO] - Training Epoch: 2/2, step 3455/7134 completed (loss: 0.1120949238538742, acc: 0.9937888383865356)
[2025-02-13 20:21:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:14][root][INFO] - Training Epoch: 2/2, step 3456/7134 completed (loss: 0.07364349067211151, acc: 0.9849246144294739)
[2025-02-13 20:21:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:14][root][INFO] - Training Epoch: 2/2, step 3457/7134 completed (loss: 0.06406588107347488, acc: 0.9929577708244324)
[2025-02-13 20:21:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:15][root][INFO] - Training Epoch: 2/2, step 3458/7134 completed (loss: 0.21117694675922394, acc: 0.9670329689979553)
[2025-02-13 20:21:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:15][root][INFO] - Training Epoch: 2/2, step 3459/7134 completed (loss: 0.16233079135417938, acc: 0.9496855139732361)
[2025-02-13 20:21:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:16][root][INFO] - Training Epoch: 2/2, step 3460/7134 completed (loss: 0.5186678767204285, acc: 0.8888888955116272)
[2025-02-13 20:21:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:16][root][INFO] - Training Epoch: 2/2, step 3461/7134 completed (loss: 0.598808765411377, acc: 0.8615384697914124)
[2025-02-13 20:21:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:16][root][INFO] - Training Epoch: 2/2, step 3462/7134 completed (loss: 0.22870612144470215, acc: 0.9473684430122375)
[2025-02-13 20:21:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:17][root][INFO] - Training Epoch: 2/2, step 3463/7134 completed (loss: 0.05307378992438316, acc: 0.9938271641731262)
[2025-02-13 20:21:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:17][root][INFO] - Training Epoch: 2/2, step 3464/7134 completed (loss: 0.08739739656448364, acc: 0.9719101190567017)
[2025-02-13 20:21:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:18][root][INFO] - Training Epoch: 2/2, step 3465/7134 completed (loss: 0.14476646482944489, acc: 0.9555555582046509)
[2025-02-13 20:21:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:18][root][INFO] - Training Epoch: 2/2, step 3466/7134 completed (loss: 0.06419599056243896, acc: 0.9814814925193787)
[2025-02-13 20:21:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:18][root][INFO] - Training Epoch: 2/2, step 3467/7134 completed (loss: 0.051467861980199814, acc: 0.9830508232116699)
[2025-02-13 20:21:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:19][root][INFO] - Training Epoch: 2/2, step 3468/7134 completed (loss: 0.03202540799975395, acc: 1.0)
[2025-02-13 20:21:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:19][root][INFO] - Training Epoch: 2/2, step 3469/7134 completed (loss: 0.06511298567056656, acc: 0.9849624037742615)
[2025-02-13 20:21:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:19][root][INFO] - Training Epoch: 2/2, step 3470/7134 completed (loss: 0.021284576505422592, acc: 1.0)
[2025-02-13 20:21:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:20][root][INFO] - Training Epoch: 2/2, step 3471/7134 completed (loss: 0.07592537999153137, acc: 0.9777777791023254)
[2025-02-13 20:21:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:20][root][INFO] - Training Epoch: 2/2, step 3472/7134 completed (loss: 0.04243869706988335, acc: 0.993630588054657)
[2025-02-13 20:21:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:21][root][INFO] - Training Epoch: 2/2, step 3473/7134 completed (loss: 0.07466680556535721, acc: 0.9736841917037964)
[2025-02-13 20:21:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:21][root][INFO] - Training Epoch: 2/2, step 3474/7134 completed (loss: 0.13760791718959808, acc: 0.9710144996643066)
[2025-02-13 20:21:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:21][root][INFO] - Training Epoch: 2/2, step 3475/7134 completed (loss: 0.05196850374341011, acc: 0.9848484992980957)
[2025-02-13 20:21:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:22][root][INFO] - Training Epoch: 2/2, step 3476/7134 completed (loss: 0.21548216044902802, acc: 0.9383561611175537)
[2025-02-13 20:21:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:22][root][INFO] - Training Epoch: 2/2, step 3477/7134 completed (loss: 0.10712289810180664, acc: 0.9797297120094299)
[2025-02-13 20:21:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:23][root][INFO] - Training Epoch: 2/2, step 3478/7134 completed (loss: 0.026479030027985573, acc: 0.9940119981765747)
[2025-02-13 20:21:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:23][root][INFO] - Training Epoch: 2/2, step 3479/7134 completed (loss: 0.013777405954897404, acc: 1.0)
[2025-02-13 20:21:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:23][root][INFO] - Training Epoch: 2/2, step 3480/7134 completed (loss: 0.06518792361021042, acc: 0.9835164546966553)
[2025-02-13 20:21:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:24][root][INFO] - Training Epoch: 2/2, step 3481/7134 completed (loss: 0.061776239424943924, acc: 0.9849624037742615)
[2025-02-13 20:21:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:24][root][INFO] - Training Epoch: 2/2, step 3482/7134 completed (loss: 0.07048285752534866, acc: 0.9921875)
[2025-02-13 20:21:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:24][root][INFO] - Training Epoch: 2/2, step 3483/7134 completed (loss: 0.03642713651061058, acc: 0.9870967864990234)
[2025-02-13 20:21:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:25][root][INFO] - Training Epoch: 2/2, step 3484/7134 completed (loss: 0.06743261218070984, acc: 0.9821428656578064)
[2025-02-13 20:21:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:25][root][INFO] - Training Epoch: 2/2, step 3485/7134 completed (loss: 0.10689787566661835, acc: 0.9757575988769531)
[2025-02-13 20:21:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:26][root][INFO] - Training Epoch: 2/2, step 3486/7134 completed (loss: 0.17513328790664673, acc: 0.9572649598121643)
[2025-02-13 20:21:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:26][root][INFO] - Training Epoch: 2/2, step 3487/7134 completed (loss: 0.15919068455696106, acc: 0.9638554453849792)
[2025-02-13 20:21:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:26][root][INFO] - Training Epoch: 2/2, step 3488/7134 completed (loss: 0.07225710898637772, acc: 0.9857142567634583)
[2025-02-13 20:21:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:27][root][INFO] - Training Epoch: 2/2, step 3489/7134 completed (loss: 0.1619689017534256, acc: 0.9481481313705444)
[2025-02-13 20:21:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:27][root][INFO] - Training Epoch: 2/2, step 3490/7134 completed (loss: 0.11769828200340271, acc: 0.9790209531784058)
[2025-02-13 20:21:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:27][root][INFO] - Training Epoch: 2/2, step 3491/7134 completed (loss: 0.14791347086429596, acc: 0.9617834687232971)
[2025-02-13 20:21:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:28][root][INFO] - Training Epoch: 2/2, step 3492/7134 completed (loss: 0.10804960131645203, acc: 0.963302731513977)
[2025-02-13 20:21:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:28][root][INFO] - Training Epoch: 2/2, step 3493/7134 completed (loss: 0.05556391924619675, acc: 1.0)
[2025-02-13 20:21:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:29][root][INFO] - Training Epoch: 2/2, step 3494/7134 completed (loss: 0.10221035778522491, acc: 0.9729729890823364)
[2025-02-13 20:21:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:29][root][INFO] - Training Epoch: 2/2, step 3495/7134 completed (loss: 0.045940764248371124, acc: 0.9896907210350037)
[2025-02-13 20:21:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:29][root][INFO] - Training Epoch: 2/2, step 3496/7134 completed (loss: 0.07542046904563904, acc: 0.9803921580314636)
[2025-02-13 20:21:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:30][root][INFO] - Training Epoch: 2/2, step 3497/7134 completed (loss: 0.10578010976314545, acc: 0.9810126423835754)
[2025-02-13 20:21:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:30][root][INFO] - Training Epoch: 2/2, step 3498/7134 completed (loss: 0.17760303616523743, acc: 0.9683544039726257)
[2025-02-13 20:21:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:30][root][INFO] - Training Epoch: 2/2, step 3499/7134 completed (loss: 0.08642003685235977, acc: 0.9738562107086182)
[2025-02-13 20:21:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:31][root][INFO] - Training Epoch: 2/2, step 3500/7134 completed (loss: 0.18681387603282928, acc: 0.9707602262496948)
[2025-02-13 20:21:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:31][root][INFO] - Training Epoch: 2/2, step 3501/7134 completed (loss: 0.12385033071041107, acc: 0.9772727489471436)
[2025-02-13 20:21:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:32][root][INFO] - Training Epoch: 2/2, step 3502/7134 completed (loss: 0.2476961612701416, acc: 0.9459459185600281)
[2025-02-13 20:21:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:32][root][INFO] - Training Epoch: 2/2, step 3503/7134 completed (loss: 0.16963987052440643, acc: 0.9568345546722412)
[2025-02-13 20:21:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:32][root][INFO] - Training Epoch: 2/2, step 3504/7134 completed (loss: 0.03347637876868248, acc: 0.9866666793823242)
[2025-02-13 20:21:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:33][root][INFO] - Training Epoch: 2/2, step 3505/7134 completed (loss: 0.06016499176621437, acc: 0.9841269850730896)
[2025-02-13 20:21:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:33][root][INFO] - Training Epoch: 2/2, step 3506/7134 completed (loss: 0.0546911247074604, acc: 0.9844961166381836)
[2025-02-13 20:21:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:33][root][INFO] - Training Epoch: 2/2, step 3507/7134 completed (loss: 0.04517103359103203, acc: 0.9861111044883728)
[2025-02-13 20:21:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:34][root][INFO] - Training Epoch: 2/2, step 3508/7134 completed (loss: 0.061708685010671616, acc: 0.9867549538612366)
[2025-02-13 20:21:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:34][root][INFO] - Training Epoch: 2/2, step 3509/7134 completed (loss: 0.14574342966079712, acc: 0.9754601120948792)
[2025-02-13 20:21:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:34][root][INFO] - Training Epoch: 2/2, step 3510/7134 completed (loss: 0.09169736504554749, acc: 0.9849624037742615)
[2025-02-13 20:21:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:35][root][INFO] - Training Epoch: 2/2, step 3511/7134 completed (loss: 0.024087755009531975, acc: 1.0)
[2025-02-13 20:21:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:35][root][INFO] - Training Epoch: 2/2, step 3512/7134 completed (loss: 0.1906784027814865, acc: 0.9615384340286255)
[2025-02-13 20:21:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:36][root][INFO] - Training Epoch: 2/2, step 3513/7134 completed (loss: 0.028480490669608116, acc: 0.9927536249160767)
[2025-02-13 20:21:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:36][root][INFO] - Training Epoch: 2/2, step 3514/7134 completed (loss: 0.02912876568734646, acc: 0.9904305934906006)
[2025-02-13 20:21:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:36][root][INFO] - Training Epoch: 2/2, step 3515/7134 completed (loss: 0.009962978772819042, acc: 1.0)
[2025-02-13 20:21:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:37][root][INFO] - Training Epoch: 2/2, step 3516/7134 completed (loss: 0.023465236648917198, acc: 1.0)
[2025-02-13 20:21:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:37][root][INFO] - Training Epoch: 2/2, step 3517/7134 completed (loss: 0.052224621176719666, acc: 0.9797979593276978)
[2025-02-13 20:21:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:37][root][INFO] - Training Epoch: 2/2, step 3518/7134 completed (loss: 0.10054535418748856, acc: 0.9629629850387573)
[2025-02-13 20:21:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:38][root][INFO] - Training Epoch: 2/2, step 3519/7134 completed (loss: 0.04966136813163757, acc: 0.9911110997200012)
[2025-02-13 20:21:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:38][root][INFO] - Training Epoch: 2/2, step 3520/7134 completed (loss: 0.1259700357913971, acc: 0.9696969985961914)
[2025-02-13 20:21:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:39][root][INFO] - Training Epoch: 2/2, step 3521/7134 completed (loss: 0.029859479516744614, acc: 1.0)
[2025-02-13 20:21:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:39][root][INFO] - Training Epoch: 2/2, step 3522/7134 completed (loss: 0.026144417002797127, acc: 1.0)
[2025-02-13 20:21:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:39][root][INFO] - Training Epoch: 2/2, step 3523/7134 completed (loss: 0.014124141074717045, acc: 1.0)
[2025-02-13 20:21:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:40][root][INFO] - Training Epoch: 2/2, step 3524/7134 completed (loss: 0.05146085470914841, acc: 0.9890109896659851)
[2025-02-13 20:21:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:40][root][INFO] - Training Epoch: 2/2, step 3525/7134 completed (loss: 0.05338840186595917, acc: 0.9864864945411682)
[2025-02-13 20:21:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:40][root][INFO] - Training Epoch: 2/2, step 3526/7134 completed (loss: 0.12478744983673096, acc: 0.9696969985961914)
[2025-02-13 20:21:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:41][root][INFO] - Training Epoch: 2/2, step 3527/7134 completed (loss: 0.10305566340684891, acc: 0.978723406791687)
[2025-02-13 20:21:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:41][root][INFO] - Training Epoch: 2/2, step 3528/7134 completed (loss: 0.012284093536436558, acc: 1.0)
[2025-02-13 20:21:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:42][root][INFO] - Training Epoch: 2/2, step 3529/7134 completed (loss: 0.05206262320280075, acc: 0.9818181991577148)
[2025-02-13 20:21:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:42][root][INFO] - Training Epoch: 2/2, step 3530/7134 completed (loss: 0.12551330029964447, acc: 0.9601770043373108)
[2025-02-13 20:21:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:42][root][INFO] - Training Epoch: 2/2, step 3531/7134 completed (loss: 0.0493098683655262, acc: 0.987500011920929)
[2025-02-13 20:21:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:43][root][INFO] - Training Epoch: 2/2, step 3532/7134 completed (loss: 0.04264385253190994, acc: 0.9932432174682617)
[2025-02-13 20:21:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:43][root][INFO] - Training Epoch: 2/2, step 3533/7134 completed (loss: 0.055307112634181976, acc: 0.9866666793823242)
[2025-02-13 20:21:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:43][root][INFO] - Training Epoch: 2/2, step 3534/7134 completed (loss: 0.037054643034935, acc: 1.0)
[2025-02-13 20:21:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:44][root][INFO] - Training Epoch: 2/2, step 3535/7134 completed (loss: 0.10421202331781387, acc: 0.971222996711731)
[2025-02-13 20:21:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:44][root][INFO] - Training Epoch: 2/2, step 3536/7134 completed (loss: 0.14265888929367065, acc: 0.9629629850387573)
[2025-02-13 20:21:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:45][root][INFO] - Training Epoch: 2/2, step 3537/7134 completed (loss: 0.059807438403367996, acc: 0.9921875)
[2025-02-13 20:21:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:45][root][INFO] - Training Epoch: 2/2, step 3538/7134 completed (loss: 0.048003893345594406, acc: 0.9747899174690247)
[2025-02-13 20:21:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:45][root][INFO] - Training Epoch: 2/2, step 3539/7134 completed (loss: 0.08158904314041138, acc: 0.9655172228813171)
[2025-02-13 20:21:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:46][root][INFO] - Training Epoch: 2/2, step 3540/7134 completed (loss: 0.06130514666438103, acc: 0.9846153855323792)
[2025-02-13 20:21:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:46][root][INFO] - Training Epoch: 2/2, step 3541/7134 completed (loss: 0.05693314969539642, acc: 0.9866666793823242)
[2025-02-13 20:21:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:46][root][INFO] - Training Epoch: 2/2, step 3542/7134 completed (loss: 0.06071510165929794, acc: 0.9863013625144958)
[2025-02-13 20:21:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:47][root][INFO] - Training Epoch: 2/2, step 3543/7134 completed (loss: 0.13354460895061493, acc: 0.9718309640884399)
[2025-02-13 20:21:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:47][root][INFO] - Training Epoch: 2/2, step 3544/7134 completed (loss: 0.12519127130508423, acc: 0.9735099077224731)
[2025-02-13 20:21:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:48][root][INFO] - Training Epoch: 2/2, step 3545/7134 completed (loss: 0.038473229855298996, acc: 1.0)
[2025-02-13 20:21:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:48][root][INFO] - Training Epoch: 2/2, step 3546/7134 completed (loss: 0.11273914575576782, acc: 0.971222996711731)
[2025-02-13 20:21:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:48][root][INFO] - Training Epoch: 2/2, step 3547/7134 completed (loss: 0.1191081702709198, acc: 0.9774011373519897)
[2025-02-13 20:21:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:49][root][INFO] - Training Epoch: 2/2, step 3548/7134 completed (loss: 0.052051763981580734, acc: 0.9908257126808167)
[2025-02-13 20:21:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:49][root][INFO] - Training Epoch: 2/2, step 3549/7134 completed (loss: 0.07579293847084045, acc: 0.969924807548523)
[2025-02-13 20:21:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:49][root][INFO] - Training Epoch: 2/2, step 3550/7134 completed (loss: 0.11043621599674225, acc: 0.985401451587677)
[2025-02-13 20:21:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:50][root][INFO] - Training Epoch: 2/2, step 3551/7134 completed (loss: 0.24587826430797577, acc: 0.918367326259613)
[2025-02-13 20:21:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:50][root][INFO] - Training Epoch: 2/2, step 3552/7134 completed (loss: 0.15349827706813812, acc: 0.9527559280395508)
[2025-02-13 20:21:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:51][root][INFO] - Training Epoch: 2/2, step 3553/7134 completed (loss: 0.1513444036245346, acc: 0.970588207244873)
[2025-02-13 20:21:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:51][root][INFO] - Training Epoch: 2/2, step 3554/7134 completed (loss: 0.19817622005939484, acc: 0.9570552110671997)
[2025-02-13 20:21:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:52][root][INFO] - Training Epoch: 2/2, step 3555/7134 completed (loss: 0.25027257204055786, acc: 0.9548386931419373)
[2025-02-13 20:21:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:52][root][INFO] - Training Epoch: 2/2, step 3556/7134 completed (loss: 0.05652645230293274, acc: 0.9924242496490479)
[2025-02-13 20:21:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:52][root][INFO] - Training Epoch: 2/2, step 3557/7134 completed (loss: 0.10382581502199173, acc: 0.9645389914512634)
[2025-02-13 20:21:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:53][root][INFO] - Training Epoch: 2/2, step 3558/7134 completed (loss: 0.22363871335983276, acc: 0.9440000057220459)
[2025-02-13 20:21:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:53][root][INFO] - Training Epoch: 2/2, step 3559/7134 completed (loss: 0.09419789165258408, acc: 0.988095223903656)
[2025-02-13 20:21:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:53][root][INFO] - Training Epoch: 2/2, step 3560/7134 completed (loss: 0.09072395414113998, acc: 0.9836065769195557)
[2025-02-13 20:21:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:54][root][INFO] - Training Epoch: 2/2, step 3561/7134 completed (loss: 0.18121400475502014, acc: 0.9717513918876648)
[2025-02-13 20:21:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:54][root][INFO] - Training Epoch: 2/2, step 3562/7134 completed (loss: 0.06369099020957947, acc: 0.9852941036224365)
[2025-02-13 20:21:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:54][root][INFO] - Training Epoch: 2/2, step 3563/7134 completed (loss: 0.10196025669574738, acc: 0.9716312289237976)
[2025-02-13 20:21:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:21:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:22:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:23:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:24:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:52][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.2616, device='cuda:0') eval_epoch_loss=tensor(0.2324, device='cuda:0') eval_epoch_acc=tensor(0.9450, device='cuda:0')
[2025-02-13 20:25:52][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-13 20:25:52][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-13 20:25:52][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_wavlm_llama32_1b_linear_peft_transfer_librispeech-100/asr_epoch_2_step_3564_loss_0.23238003253936768/model.pt
[2025-02-13 20:25:52][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_wavlm_llama32_1b_linear_peft_transfer_librispeech-100 directory
[2025-02-13 20:25:52][slam_llm.utils.train_utils][INFO] - best eval loss on epoch 2 is 0.23238003253936768
[2025-02-13 20:25:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:53][root][INFO] - Training Epoch: 2/2, step 3564/7134 completed (loss: 0.14013123512268066, acc: 0.9603174328804016)
[2025-02-13 20:25:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:53][root][INFO] - Training Epoch: 2/2, step 3565/7134 completed (loss: 0.25136277079582214, acc: 0.9191176295280457)
[2025-02-13 20:25:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:54][root][INFO] - Training Epoch: 2/2, step 3566/7134 completed (loss: 0.1585976630449295, acc: 0.9663865566253662)
[2025-02-13 20:25:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:54][root][INFO] - Training Epoch: 2/2, step 3567/7134 completed (loss: 0.1475212723016739, acc: 0.9719101190567017)
[2025-02-13 20:25:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:54][root][INFO] - Training Epoch: 2/2, step 3568/7134 completed (loss: 0.19517026841640472, acc: 0.936170220375061)
[2025-02-13 20:25:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:55][root][INFO] - Training Epoch: 2/2, step 3569/7134 completed (loss: 0.17003311216831207, acc: 0.9750000238418579)
[2025-02-13 20:25:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:55][root][INFO] - Training Epoch: 2/2, step 3570/7134 completed (loss: 0.2847023606300354, acc: 0.9558823704719543)
[2025-02-13 20:25:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:56][root][INFO] - Training Epoch: 2/2, step 3571/7134 completed (loss: 0.252185195684433, acc: 0.949438214302063)
[2025-02-13 20:25:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:56][root][INFO] - Training Epoch: 2/2, step 3572/7134 completed (loss: 0.05484209582209587, acc: 0.9821428656578064)
[2025-02-13 20:25:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:57][root][INFO] - Training Epoch: 2/2, step 3573/7134 completed (loss: 0.17467419803142548, acc: 0.9605262875556946)
[2025-02-13 20:25:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:57][root][INFO] - Training Epoch: 2/2, step 3574/7134 completed (loss: 0.11723670363426208, acc: 0.9752066135406494)
[2025-02-13 20:25:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:57][root][INFO] - Training Epoch: 2/2, step 3575/7134 completed (loss: 0.2999703884124756, acc: 0.9342105388641357)
[2025-02-13 20:25:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:58][root][INFO] - Training Epoch: 2/2, step 3576/7134 completed (loss: 0.06579411029815674, acc: 0.9925373196601868)
[2025-02-13 20:25:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:58][root][INFO] - Training Epoch: 2/2, step 3577/7134 completed (loss: 0.13374541699886322, acc: 0.9639175534248352)
[2025-02-13 20:25:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:59][root][INFO] - Training Epoch: 2/2, step 3578/7134 completed (loss: 0.1404874622821808, acc: 0.9698795080184937)
[2025-02-13 20:25:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:59][root][INFO] - Training Epoch: 2/2, step 3579/7134 completed (loss: 0.12646499276161194, acc: 0.9767441749572754)
[2025-02-13 20:25:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:25:59][root][INFO] - Training Epoch: 2/2, step 3580/7134 completed (loss: 0.2526664137840271, acc: 0.9479768872261047)
[2025-02-13 20:26:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:00][root][INFO] - Training Epoch: 2/2, step 3581/7134 completed (loss: 0.14940331876277924, acc: 0.970588207244873)
[2025-02-13 20:26:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:00][root][INFO] - Training Epoch: 2/2, step 3582/7134 completed (loss: 0.18244731426239014, acc: 0.94017094373703)
[2025-02-13 20:26:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:01][root][INFO] - Training Epoch: 2/2, step 3583/7134 completed (loss: 0.20355147123336792, acc: 0.9520958065986633)
[2025-02-13 20:26:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:01][root][INFO] - Training Epoch: 2/2, step 3584/7134 completed (loss: 0.22568769752979279, acc: 0.936170220375061)
[2025-02-13 20:26:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:01][root][INFO] - Training Epoch: 2/2, step 3585/7134 completed (loss: 0.25681960582733154, acc: 0.9509202241897583)
[2025-02-13 20:26:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:02][root][INFO] - Training Epoch: 2/2, step 3586/7134 completed (loss: 0.22589164972305298, acc: 0.9490445852279663)
[2025-02-13 20:26:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:02][root][INFO] - Training Epoch: 2/2, step 3587/7134 completed (loss: 0.20884661376476288, acc: 0.9650349617004395)
[2025-02-13 20:26:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:03][root][INFO] - Training Epoch: 2/2, step 3588/7134 completed (loss: 0.1602345108985901, acc: 0.9710982441902161)
[2025-02-13 20:26:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:03][root][INFO] - Training Epoch: 2/2, step 3589/7134 completed (loss: 0.22930757701396942, acc: 0.9672130942344666)
[2025-02-13 20:26:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:03][root][INFO] - Training Epoch: 2/2, step 3590/7134 completed (loss: 0.07309696078300476, acc: 0.9777777791023254)
[2025-02-13 20:26:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:04][root][INFO] - Training Epoch: 2/2, step 3591/7134 completed (loss: 0.10998010635375977, acc: 0.9707602262496948)
[2025-02-13 20:26:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:04][root][INFO] - Training Epoch: 2/2, step 3592/7134 completed (loss: 0.05406564474105835, acc: 0.9928571581840515)
[2025-02-13 20:26:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:05][root][INFO] - Training Epoch: 2/2, step 3593/7134 completed (loss: 0.31921181082725525, acc: 0.9507042169570923)
[2025-02-13 20:26:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:05][root][INFO] - Training Epoch: 2/2, step 3594/7134 completed (loss: 0.11316007375717163, acc: 0.9602649211883545)
[2025-02-13 20:26:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:06][root][INFO] - Training Epoch: 2/2, step 3595/7134 completed (loss: 0.13201504945755005, acc: 0.9607843160629272)
[2025-02-13 20:26:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:06][root][INFO] - Training Epoch: 2/2, step 3596/7134 completed (loss: 0.15947306156158447, acc: 0.9647058844566345)
[2025-02-13 20:26:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:06][root][INFO] - Training Epoch: 2/2, step 3597/7134 completed (loss: 0.09575982391834259, acc: 0.9671052694320679)
[2025-02-13 20:26:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:07][root][INFO] - Training Epoch: 2/2, step 3598/7134 completed (loss: 0.19511200487613678, acc: 0.9383886456489563)
[2025-02-13 20:26:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:07][root][INFO] - Training Epoch: 2/2, step 3599/7134 completed (loss: 0.1619209349155426, acc: 0.9675675630569458)
[2025-02-13 20:26:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:08][root][INFO] - Training Epoch: 2/2, step 3600/7134 completed (loss: 0.22143197059631348, acc: 0.9481865167617798)
[2025-02-13 20:26:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:08][root][INFO] - Training Epoch: 2/2, step 3601/7134 completed (loss: 0.4431660771369934, acc: 0.8932584524154663)
[2025-02-13 20:26:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:08][root][INFO] - Training Epoch: 2/2, step 3602/7134 completed (loss: 0.2412511259317398, acc: 0.930232584476471)
[2025-02-13 20:26:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:09][root][INFO] - Training Epoch: 2/2, step 3603/7134 completed (loss: 0.20810112357139587, acc: 0.9433962106704712)
[2025-02-13 20:26:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:09][root][INFO] - Training Epoch: 2/2, step 3604/7134 completed (loss: 0.17296655476093292, acc: 0.960629940032959)
[2025-02-13 20:26:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:10][root][INFO] - Training Epoch: 2/2, step 3605/7134 completed (loss: 0.15890735387802124, acc: 0.9730941653251648)
[2025-02-13 20:26:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:10][root][INFO] - Training Epoch: 2/2, step 3606/7134 completed (loss: 0.37692806124687195, acc: 0.9178082346916199)
[2025-02-13 20:26:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:10][root][INFO] - Training Epoch: 2/2, step 3607/7134 completed (loss: 0.15056344866752625, acc: 0.965753436088562)
[2025-02-13 20:26:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:11][root][INFO] - Training Epoch: 2/2, step 3608/7134 completed (loss: 0.384354829788208, acc: 0.946601927280426)
[2025-02-13 20:26:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:11][root][INFO] - Training Epoch: 2/2, step 3609/7134 completed (loss: 0.08909587562084198, acc: 0.9789473414421082)
[2025-02-13 20:26:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:12][root][INFO] - Training Epoch: 2/2, step 3610/7134 completed (loss: 0.2093561440706253, acc: 0.9505494236946106)
[2025-02-13 20:26:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:12][root][INFO] - Training Epoch: 2/2, step 3611/7134 completed (loss: 0.1458950787782669, acc: 0.9657142758369446)
[2025-02-13 20:26:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:12][root][INFO] - Training Epoch: 2/2, step 3612/7134 completed (loss: 0.31530818343162537, acc: 0.9224137663841248)
[2025-02-13 20:26:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:13][root][INFO] - Training Epoch: 2/2, step 3613/7134 completed (loss: 0.16154393553733826, acc: 0.9617486596107483)
[2025-02-13 20:26:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:13][root][INFO] - Training Epoch: 2/2, step 3614/7134 completed (loss: 0.07947121560573578, acc: 0.9893617033958435)
[2025-02-13 20:26:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:14][root][INFO] - Training Epoch: 2/2, step 3615/7134 completed (loss: 0.07819803804159164, acc: 0.9864864945411682)
[2025-02-13 20:26:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:14][root][INFO] - Training Epoch: 2/2, step 3616/7134 completed (loss: 0.2021530568599701, acc: 0.95652174949646)
[2025-02-13 20:26:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:14][root][INFO] - Training Epoch: 2/2, step 3617/7134 completed (loss: 0.10429441928863525, acc: 0.9675675630569458)
[2025-02-13 20:26:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:15][root][INFO] - Training Epoch: 2/2, step 3618/7134 completed (loss: 0.2149239331483841, acc: 0.9447852969169617)
[2025-02-13 20:26:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:15][root][INFO] - Training Epoch: 2/2, step 3619/7134 completed (loss: 0.30944010615348816, acc: 0.9257642030715942)
[2025-02-13 20:26:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:16][root][INFO] - Training Epoch: 2/2, step 3620/7134 completed (loss: 0.07989183813333511, acc: 0.9940119981765747)
[2025-02-13 20:26:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:16][root][INFO] - Training Epoch: 2/2, step 3621/7134 completed (loss: 0.12913496792316437, acc: 0.9808917045593262)
[2025-02-13 20:26:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:16][root][INFO] - Training Epoch: 2/2, step 3622/7134 completed (loss: 0.08111812174320221, acc: 0.9870129823684692)
[2025-02-13 20:26:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:17][root][INFO] - Training Epoch: 2/2, step 3623/7134 completed (loss: 0.04740676283836365, acc: 1.0)
[2025-02-13 20:26:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:17][root][INFO] - Training Epoch: 2/2, step 3624/7134 completed (loss: 0.11551246047019958, acc: 0.9784172773361206)
[2025-02-13 20:26:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:17][root][INFO] - Training Epoch: 2/2, step 3625/7134 completed (loss: 0.25062912702560425, acc: 0.9075630307197571)
[2025-02-13 20:26:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:18][root][INFO] - Training Epoch: 2/2, step 3626/7134 completed (loss: 0.25720325112342834, acc: 0.966292142868042)
[2025-02-13 20:26:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:18][root][INFO] - Training Epoch: 2/2, step 3627/7134 completed (loss: 0.2837541997432709, acc: 0.9433962106704712)
[2025-02-13 20:26:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:19][root][INFO] - Training Epoch: 2/2, step 3628/7134 completed (loss: 0.18170107901096344, acc: 0.9428571462631226)
[2025-02-13 20:26:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:19][root][INFO] - Training Epoch: 2/2, step 3629/7134 completed (loss: 0.26573172211647034, acc: 0.9354838728904724)
[2025-02-13 20:26:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:19][root][INFO] - Training Epoch: 2/2, step 3630/7134 completed (loss: 0.08668696135282516, acc: 0.9677419066429138)
[2025-02-13 20:26:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:20][root][INFO] - Training Epoch: 2/2, step 3631/7134 completed (loss: 0.10056030750274658, acc: 0.9714285731315613)
[2025-02-13 20:26:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:20][root][INFO] - Training Epoch: 2/2, step 3632/7134 completed (loss: 0.10730248689651489, acc: 0.9836065769195557)
[2025-02-13 20:26:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:21][root][INFO] - Training Epoch: 2/2, step 3633/7134 completed (loss: 0.15015731751918793, acc: 0.9554139971733093)
[2025-02-13 20:26:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:21][root][INFO] - Training Epoch: 2/2, step 3634/7134 completed (loss: 0.08110123872756958, acc: 0.9906542301177979)
[2025-02-13 20:26:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:21][root][INFO] - Training Epoch: 2/2, step 3635/7134 completed (loss: 0.09026362746953964, acc: 0.9741379022598267)
[2025-02-13 20:26:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:22][root][INFO] - Training Epoch: 2/2, step 3636/7134 completed (loss: 0.09850448369979858, acc: 0.9716981053352356)
[2025-02-13 20:26:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:22][root][INFO] - Training Epoch: 2/2, step 3637/7134 completed (loss: 0.1607714742422104, acc: 0.9545454382896423)
[2025-02-13 20:26:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:23][root][INFO] - Training Epoch: 2/2, step 3638/7134 completed (loss: 0.08424481004476547, acc: 0.9780219793319702)
[2025-02-13 20:26:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:23][root][INFO] - Training Epoch: 2/2, step 3639/7134 completed (loss: 0.1410614550113678, acc: 0.9680851101875305)
[2025-02-13 20:26:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:23][root][INFO] - Training Epoch: 2/2, step 3640/7134 completed (loss: 0.12090642750263214, acc: 0.9834710955619812)
[2025-02-13 20:26:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:24][root][INFO] - Training Epoch: 2/2, step 3641/7134 completed (loss: 0.07520915567874908, acc: 0.9838709831237793)
[2025-02-13 20:26:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:24][root][INFO] - Training Epoch: 2/2, step 3642/7134 completed (loss: 0.04178999736905098, acc: 0.9932885766029358)
[2025-02-13 20:26:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:25][root][INFO] - Training Epoch: 2/2, step 3643/7134 completed (loss: 0.15713030099868774, acc: 0.96875)
[2025-02-13 20:26:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:25][root][INFO] - Training Epoch: 2/2, step 3644/7134 completed (loss: 0.16290733218193054, acc: 0.9527027010917664)
[2025-02-13 20:26:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:25][root][INFO] - Training Epoch: 2/2, step 3645/7134 completed (loss: 0.25536417961120605, acc: 0.9453125)
[2025-02-13 20:26:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:26][root][INFO] - Training Epoch: 2/2, step 3646/7134 completed (loss: 0.2745656967163086, acc: 0.9318181872367859)
[2025-02-13 20:26:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:26][root][INFO] - Training Epoch: 2/2, step 3647/7134 completed (loss: 0.06612220406532288, acc: 0.9887640476226807)
[2025-02-13 20:26:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:26][root][INFO] - Training Epoch: 2/2, step 3648/7134 completed (loss: 0.1713542938232422, acc: 0.9467455744743347)
[2025-02-13 20:26:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:27][root][INFO] - Training Epoch: 2/2, step 3649/7134 completed (loss: 0.20038501918315887, acc: 0.9597315192222595)
[2025-02-13 20:26:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:27][root][INFO] - Training Epoch: 2/2, step 3650/7134 completed (loss: 0.1250285655260086, acc: 0.9704142212867737)
[2025-02-13 20:26:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:28][root][INFO] - Training Epoch: 2/2, step 3651/7134 completed (loss: 0.16351953148841858, acc: 0.9518072009086609)
[2025-02-13 20:26:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:28][root][INFO] - Training Epoch: 2/2, step 3652/7134 completed (loss: 0.11501742899417877, acc: 0.9795918464660645)
[2025-02-13 20:26:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:28][root][INFO] - Training Epoch: 2/2, step 3653/7134 completed (loss: 0.1615460216999054, acc: 0.9570552110671997)
[2025-02-13 20:26:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:29][root][INFO] - Training Epoch: 2/2, step 3654/7134 completed (loss: 0.03222915902733803, acc: 0.9919999837875366)
[2025-02-13 20:26:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:29][root][INFO] - Training Epoch: 2/2, step 3655/7134 completed (loss: 0.2673584520816803, acc: 0.9520958065986633)
[2025-02-13 20:26:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:30][root][INFO] - Training Epoch: 2/2, step 3656/7134 completed (loss: 0.2475757598876953, acc: 0.9492753744125366)
[2025-02-13 20:26:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:30][root][INFO] - Training Epoch: 2/2, step 3657/7134 completed (loss: 0.0751703605055809, acc: 0.9790209531784058)
[2025-02-13 20:26:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:30][root][INFO] - Training Epoch: 2/2, step 3658/7134 completed (loss: 0.11087371408939362, acc: 0.9861111044883728)
[2025-02-13 20:26:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:31][root][INFO] - Training Epoch: 2/2, step 3659/7134 completed (loss: 0.15808410942554474, acc: 0.9523809552192688)
[2025-02-13 20:26:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:31][root][INFO] - Training Epoch: 2/2, step 3660/7134 completed (loss: 0.34339991211891174, acc: 0.9135802388191223)
[2025-02-13 20:26:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:32][root][INFO] - Training Epoch: 2/2, step 3661/7134 completed (loss: 0.10531940311193466, acc: 0.9571428298950195)
[2025-02-13 20:26:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:32][root][INFO] - Training Epoch: 2/2, step 3662/7134 completed (loss: 0.31361153721809387, acc: 0.9378882050514221)
[2025-02-13 20:26:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:32][root][INFO] - Training Epoch: 2/2, step 3663/7134 completed (loss: 0.18813207745552063, acc: 0.953125)
[2025-02-13 20:26:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:33][root][INFO] - Training Epoch: 2/2, step 3664/7134 completed (loss: 0.27781376242637634, acc: 0.9415584206581116)
[2025-02-13 20:26:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:33][root][INFO] - Training Epoch: 2/2, step 3665/7134 completed (loss: 0.11357476562261581, acc: 0.9729729890823364)
[2025-02-13 20:26:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:34][root][INFO] - Training Epoch: 2/2, step 3666/7134 completed (loss: 0.21261419355869293, acc: 0.9589040875434875)
[2025-02-13 20:26:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:34][root][INFO] - Training Epoch: 2/2, step 3667/7134 completed (loss: 0.30639609694480896, acc: 0.9224806427955627)
[2025-02-13 20:26:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:34][root][INFO] - Training Epoch: 2/2, step 3668/7134 completed (loss: 0.24975822865962982, acc: 0.9448275566101074)
[2025-02-13 20:26:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:35][root][INFO] - Training Epoch: 2/2, step 3669/7134 completed (loss: 0.06844674050807953, acc: 0.9752066135406494)
[2025-02-13 20:26:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:35][root][INFO] - Training Epoch: 2/2, step 3670/7134 completed (loss: 0.047425299882888794, acc: 0.9879518151283264)
[2025-02-13 20:26:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:35][root][INFO] - Training Epoch: 2/2, step 3671/7134 completed (loss: 0.0691409483551979, acc: 0.9873417615890503)
[2025-02-13 20:26:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:36][root][INFO] - Training Epoch: 2/2, step 3672/7134 completed (loss: 0.08196111768484116, acc: 0.9834254384040833)
[2025-02-13 20:26:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:36][root][INFO] - Training Epoch: 2/2, step 3673/7134 completed (loss: 0.10208673030138016, acc: 0.9793103337287903)
[2025-02-13 20:26:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:37][root][INFO] - Training Epoch: 2/2, step 3674/7134 completed (loss: 0.05947404354810715, acc: 0.9932432174682617)
[2025-02-13 20:26:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:37][root][INFO] - Training Epoch: 2/2, step 3675/7134 completed (loss: 0.07327568531036377, acc: 0.9834254384040833)
[2025-02-13 20:26:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:37][root][INFO] - Training Epoch: 2/2, step 3676/7134 completed (loss: 0.1630931794643402, acc: 0.9743589758872986)
[2025-02-13 20:26:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:38][root][INFO] - Training Epoch: 2/2, step 3677/7134 completed (loss: 0.1527017503976822, acc: 0.9715909361839294)
[2025-02-13 20:26:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:38][root][INFO] - Training Epoch: 2/2, step 3678/7134 completed (loss: 0.17664079368114471, acc: 0.9620253443717957)
[2025-02-13 20:26:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:39][root][INFO] - Training Epoch: 2/2, step 3679/7134 completed (loss: 0.09862072020769119, acc: 0.9769230484962463)
[2025-02-13 20:26:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:39][root][INFO] - Training Epoch: 2/2, step 3680/7134 completed (loss: 0.10330972075462341, acc: 0.9666666388511658)
[2025-02-13 20:26:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:40][root][INFO] - Training Epoch: 2/2, step 3681/7134 completed (loss: 0.04692266136407852, acc: 1.0)
[2025-02-13 20:26:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:40][root][INFO] - Training Epoch: 2/2, step 3682/7134 completed (loss: 0.06058255210518837, acc: 0.9821428656578064)
[2025-02-13 20:26:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:40][root][INFO] - Training Epoch: 2/2, step 3683/7134 completed (loss: 0.02692592516541481, acc: 1.0)
[2025-02-13 20:26:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:41][root][INFO] - Training Epoch: 2/2, step 3684/7134 completed (loss: 0.03648695349693298, acc: 1.0)
[2025-02-13 20:26:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:41][root][INFO] - Training Epoch: 2/2, step 3685/7134 completed (loss: 0.04891812801361084, acc: 0.9918699264526367)
[2025-02-13 20:26:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:42][root][INFO] - Training Epoch: 2/2, step 3686/7134 completed (loss: 0.033190369606018066, acc: 1.0)
[2025-02-13 20:26:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:42][root][INFO] - Training Epoch: 2/2, step 3687/7134 completed (loss: 0.0523531436920166, acc: 0.9937888383865356)
[2025-02-13 20:26:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:42][root][INFO] - Training Epoch: 2/2, step 3688/7134 completed (loss: 0.10284347832202911, acc: 0.9666666388511658)
[2025-02-13 20:26:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:43][root][INFO] - Training Epoch: 2/2, step 3689/7134 completed (loss: 0.07410617172718048, acc: 0.978723406791687)
[2025-02-13 20:26:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:43][root][INFO] - Training Epoch: 2/2, step 3690/7134 completed (loss: 0.039180874824523926, acc: 0.9936708807945251)
[2025-02-13 20:26:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:44][root][INFO] - Training Epoch: 2/2, step 3691/7134 completed (loss: 0.0794248953461647, acc: 0.9900990128517151)
[2025-02-13 20:26:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:44][root][INFO] - Training Epoch: 2/2, step 3692/7134 completed (loss: 0.06776179373264313, acc: 0.987730085849762)
[2025-02-13 20:26:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:44][root][INFO] - Training Epoch: 2/2, step 3693/7134 completed (loss: 0.044497858732938766, acc: 0.9924242496490479)
[2025-02-13 20:26:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:45][root][INFO] - Training Epoch: 2/2, step 3694/7134 completed (loss: 0.02687421441078186, acc: 1.0)
[2025-02-13 20:26:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:45][root][INFO] - Training Epoch: 2/2, step 3695/7134 completed (loss: 0.05312329903244972, acc: 0.987261176109314)
[2025-02-13 20:26:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:46][root][INFO] - Training Epoch: 2/2, step 3696/7134 completed (loss: 0.0687548816204071, acc: 0.982758641242981)
[2025-02-13 20:26:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:46][root][INFO] - Training Epoch: 2/2, step 3697/7134 completed (loss: 0.03826504945755005, acc: 0.9939024448394775)
[2025-02-13 20:26:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:46][root][INFO] - Training Epoch: 2/2, step 3698/7134 completed (loss: 0.06493975222110748, acc: 0.9793103337287903)
[2025-02-13 20:26:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:47][root][INFO] - Training Epoch: 2/2, step 3699/7134 completed (loss: 0.0623566173017025, acc: 0.9921259880065918)
[2025-02-13 20:26:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:47][root][INFO] - Training Epoch: 2/2, step 3700/7134 completed (loss: 0.02713126316666603, acc: 0.9937106966972351)
[2025-02-13 20:26:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:47][root][INFO] - Training Epoch: 2/2, step 3701/7134 completed (loss: 0.03285898268222809, acc: 0.9927007555961609)
[2025-02-13 20:26:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:48][root][INFO] - Training Epoch: 2/2, step 3702/7134 completed (loss: 0.0643482506275177, acc: 0.9939393997192383)
[2025-02-13 20:26:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:48][root][INFO] - Training Epoch: 2/2, step 3703/7134 completed (loss: 0.07917989790439606, acc: 0.9943181872367859)
[2025-02-13 20:26:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:49][root][INFO] - Training Epoch: 2/2, step 3704/7134 completed (loss: 0.027690542861819267, acc: 0.9900990128517151)
[2025-02-13 20:26:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:49][root][INFO] - Training Epoch: 2/2, step 3705/7134 completed (loss: 0.07859986275434494, acc: 0.9938650131225586)
[2025-02-13 20:26:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:49][root][INFO] - Training Epoch: 2/2, step 3706/7134 completed (loss: 0.23598112165927887, acc: 0.9207921028137207)
[2025-02-13 20:26:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:50][root][INFO] - Training Epoch: 2/2, step 3707/7134 completed (loss: 0.03172454982995987, acc: 0.9933333396911621)
[2025-02-13 20:26:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:50][root][INFO] - Training Epoch: 2/2, step 3708/7134 completed (loss: 0.16577330231666565, acc: 0.9538461565971375)
[2025-02-13 20:26:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:51][root][INFO] - Training Epoch: 2/2, step 3709/7134 completed (loss: 0.04780973866581917, acc: 0.9907407164573669)
[2025-02-13 20:26:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:51][root][INFO] - Training Epoch: 2/2, step 3710/7134 completed (loss: 0.11310206353664398, acc: 0.9814814925193787)
[2025-02-13 20:26:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:51][root][INFO] - Training Epoch: 2/2, step 3711/7134 completed (loss: 0.1728452444076538, acc: 0.9670329689979553)
[2025-02-13 20:26:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:52][root][INFO] - Training Epoch: 2/2, step 3712/7134 completed (loss: 0.1306302398443222, acc: 0.9619565010070801)
[2025-02-13 20:26:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:52][root][INFO] - Training Epoch: 2/2, step 3713/7134 completed (loss: 0.0909339115023613, acc: 0.9640287756919861)
[2025-02-13 20:26:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:53][root][INFO] - Training Epoch: 2/2, step 3714/7134 completed (loss: 0.12986493110656738, acc: 0.9585492014884949)
[2025-02-13 20:26:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:53][root][INFO] - Training Epoch: 2/2, step 3715/7134 completed (loss: 0.11443866789340973, acc: 0.9734042286872864)
[2025-02-13 20:26:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:53][root][INFO] - Training Epoch: 2/2, step 3716/7134 completed (loss: 0.22310927510261536, acc: 0.9644669890403748)
[2025-02-13 20:26:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:54][root][INFO] - Training Epoch: 2/2, step 3717/7134 completed (loss: 0.12657852470874786, acc: 0.9634146094322205)
[2025-02-13 20:26:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:54][root][INFO] - Training Epoch: 2/2, step 3718/7134 completed (loss: 0.14961004257202148, acc: 0.96875)
[2025-02-13 20:26:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:54][root][INFO] - Training Epoch: 2/2, step 3719/7134 completed (loss: 0.13449062407016754, acc: 0.9649122953414917)
[2025-02-13 20:26:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:55][root][INFO] - Training Epoch: 2/2, step 3720/7134 completed (loss: 0.02611510083079338, acc: 1.0)
[2025-02-13 20:26:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:55][root][INFO] - Training Epoch: 2/2, step 3721/7134 completed (loss: 0.0908740684390068, acc: 0.9826589822769165)
[2025-02-13 20:26:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:55][root][INFO] - Training Epoch: 2/2, step 3722/7134 completed (loss: 0.11143998801708221, acc: 0.984455943107605)
[2025-02-13 20:26:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:56][root][INFO] - Training Epoch: 2/2, step 3723/7134 completed (loss: 0.12538975477218628, acc: 0.9677419066429138)
[2025-02-13 20:26:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:56][root][INFO] - Training Epoch: 2/2, step 3724/7134 completed (loss: 0.08472005277872086, acc: 0.9698795080184937)
[2025-02-13 20:26:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:57][root][INFO] - Training Epoch: 2/2, step 3725/7134 completed (loss: 0.2584342360496521, acc: 0.957317054271698)
[2025-02-13 20:26:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:57][root][INFO] - Training Epoch: 2/2, step 3726/7134 completed (loss: 0.10646315664052963, acc: 0.9648241400718689)
[2025-02-13 20:26:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:57][root][INFO] - Training Epoch: 2/2, step 3727/7134 completed (loss: 0.06657338887453079, acc: 0.9826589822769165)
[2025-02-13 20:26:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:58][root][INFO] - Training Epoch: 2/2, step 3728/7134 completed (loss: 0.03878069296479225, acc: 0.9950739145278931)
[2025-02-13 20:26:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:58][root][INFO] - Training Epoch: 2/2, step 3729/7134 completed (loss: 0.09408056735992432, acc: 0.9669811129570007)
[2025-02-13 20:26:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:59][root][INFO] - Training Epoch: 2/2, step 3730/7134 completed (loss: 0.10055910795927048, acc: 0.9696969985961914)
[2025-02-13 20:26:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:59][root][INFO] - Training Epoch: 2/2, step 3731/7134 completed (loss: 0.03948717936873436, acc: 0.9885057210922241)
[2025-02-13 20:26:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:26:59][root][INFO] - Training Epoch: 2/2, step 3732/7134 completed (loss: 0.07187492400407791, acc: 0.9794871807098389)
[2025-02-13 20:26:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:00][root][INFO] - Training Epoch: 2/2, step 3733/7134 completed (loss: 0.16315069794654846, acc: 0.9689440727233887)
[2025-02-13 20:27:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:00][root][INFO] - Training Epoch: 2/2, step 3734/7134 completed (loss: 0.11342041194438934, acc: 0.9679487347602844)
[2025-02-13 20:27:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:00][root][INFO] - Training Epoch: 2/2, step 3735/7134 completed (loss: 0.0797247439622879, acc: 0.9870129823684692)
[2025-02-13 20:27:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:01][root][INFO] - Training Epoch: 2/2, step 3736/7134 completed (loss: 0.1616695523262024, acc: 0.970588207244873)
[2025-02-13 20:27:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:01][root][INFO] - Training Epoch: 2/2, step 3737/7134 completed (loss: 0.04835610091686249, acc: 1.0)
[2025-02-13 20:27:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:02][root][INFO] - Training Epoch: 2/2, step 3738/7134 completed (loss: 0.04032057523727417, acc: 0.9791666865348816)
[2025-02-13 20:27:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:02][root][INFO] - Training Epoch: 2/2, step 3739/7134 completed (loss: 0.0710396021604538, acc: 0.978723406791687)
[2025-02-13 20:27:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:02][root][INFO] - Training Epoch: 2/2, step 3740/7134 completed (loss: 0.06020774319767952, acc: 0.987261176109314)
[2025-02-13 20:27:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:03][root][INFO] - Training Epoch: 2/2, step 3741/7134 completed (loss: 0.14696767926216125, acc: 0.959770143032074)
[2025-02-13 20:27:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:03][root][INFO] - Training Epoch: 2/2, step 3742/7134 completed (loss: 0.11931652575731277, acc: 0.9727891087532043)
[2025-02-13 20:27:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:03][root][INFO] - Training Epoch: 2/2, step 3743/7134 completed (loss: 0.09865459054708481, acc: 0.9823529124259949)
[2025-02-13 20:27:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:04][root][INFO] - Training Epoch: 2/2, step 3744/7134 completed (loss: 0.1555374413728714, acc: 0.9476743936538696)
[2025-02-13 20:27:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:04][root][INFO] - Training Epoch: 2/2, step 3745/7134 completed (loss: 0.1547761708498001, acc: 0.961904764175415)
[2025-02-13 20:27:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:05][root][INFO] - Training Epoch: 2/2, step 3746/7134 completed (loss: 0.10455770790576935, acc: 0.9717513918876648)
[2025-02-13 20:27:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:05][root][INFO] - Training Epoch: 2/2, step 3747/7134 completed (loss: 0.11850301921367645, acc: 0.9611111283302307)
[2025-02-13 20:27:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:05][root][INFO] - Training Epoch: 2/2, step 3748/7134 completed (loss: 0.11116936057806015, acc: 0.9800000190734863)
[2025-02-13 20:27:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:06][root][INFO] - Training Epoch: 2/2, step 3749/7134 completed (loss: 0.2519594132900238, acc: 0.9404761791229248)
[2025-02-13 20:27:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:06][root][INFO] - Training Epoch: 2/2, step 3750/7134 completed (loss: 0.09013219177722931, acc: 0.9938650131225586)
[2025-02-13 20:27:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:06][root][INFO] - Training Epoch: 2/2, step 3751/7134 completed (loss: 0.10711972415447235, acc: 0.9698795080184937)
[2025-02-13 20:27:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:07][root][INFO] - Training Epoch: 2/2, step 3752/7134 completed (loss: 0.14907440543174744, acc: 0.955974817276001)
[2025-02-13 20:27:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:07][root][INFO] - Training Epoch: 2/2, step 3753/7134 completed (loss: 0.3194921016693115, acc: 0.9018405079841614)
[2025-02-13 20:27:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:08][root][INFO] - Training Epoch: 2/2, step 3754/7134 completed (loss: 0.0348176471889019, acc: 0.9935483932495117)
[2025-02-13 20:27:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:08][root][INFO] - Training Epoch: 2/2, step 3755/7134 completed (loss: 0.1210583746433258, acc: 0.9504132270812988)
[2025-02-13 20:27:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:08][root][INFO] - Training Epoch: 2/2, step 3756/7134 completed (loss: 0.12840402126312256, acc: 0.9768785834312439)
[2025-02-13 20:27:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:09][root][INFO] - Training Epoch: 2/2, step 3757/7134 completed (loss: 0.21096356213092804, acc: 0.948051929473877)
[2025-02-13 20:27:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:09][root][INFO] - Training Epoch: 2/2, step 3758/7134 completed (loss: 0.13497762382030487, acc: 0.9527559280395508)
[2025-02-13 20:27:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:09][root][INFO] - Training Epoch: 2/2, step 3759/7134 completed (loss: 0.13590285181999207, acc: 0.9720279574394226)
[2025-02-13 20:27:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:10][root][INFO] - Training Epoch: 2/2, step 3760/7134 completed (loss: 0.18704380095005035, acc: 0.9512194991111755)
[2025-02-13 20:27:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:10][root][INFO] - Training Epoch: 2/2, step 3761/7134 completed (loss: 0.052474793046712875, acc: 0.9772727489471436)
[2025-02-13 20:27:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:11][root][INFO] - Training Epoch: 2/2, step 3762/7134 completed (loss: 0.06643865257501602, acc: 0.9934210777282715)
[2025-02-13 20:27:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:11][root][INFO] - Training Epoch: 2/2, step 3763/7134 completed (loss: 0.06127571314573288, acc: 0.9928571581840515)
[2025-02-13 20:27:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:11][root][INFO] - Training Epoch: 2/2, step 3764/7134 completed (loss: 0.08772201836109161, acc: 0.976047933101654)
[2025-02-13 20:27:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:12][root][INFO] - Training Epoch: 2/2, step 3765/7134 completed (loss: 0.14344385266304016, acc: 0.9772727489471436)
[2025-02-13 20:27:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:12][root][INFO] - Training Epoch: 2/2, step 3766/7134 completed (loss: 0.12201184779405594, acc: 0.9664429426193237)
[2025-02-13 20:27:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:12][root][INFO] - Training Epoch: 2/2, step 3767/7134 completed (loss: 0.09740140289068222, acc: 0.9720670580863953)
[2025-02-13 20:27:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:13][root][INFO] - Training Epoch: 2/2, step 3768/7134 completed (loss: 0.09574764221906662, acc: 0.9711538553237915)
[2025-02-13 20:27:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:13][root][INFO] - Training Epoch: 2/2, step 3769/7134 completed (loss: 0.15851274132728577, acc: 0.9529411792755127)
[2025-02-13 20:27:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:14][root][INFO] - Training Epoch: 2/2, step 3770/7134 completed (loss: 0.08643189817667007, acc: 0.9731543660163879)
[2025-02-13 20:27:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:14][root][INFO] - Training Epoch: 2/2, step 3771/7134 completed (loss: 0.034291163086891174, acc: 0.9934210777282715)
[2025-02-13 20:27:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:14][root][INFO] - Training Epoch: 2/2, step 3772/7134 completed (loss: 0.06655093282461166, acc: 0.9928571581840515)
[2025-02-13 20:27:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:15][root][INFO] - Training Epoch: 2/2, step 3773/7134 completed (loss: 0.06190244108438492, acc: 0.9840425252914429)
[2025-02-13 20:27:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:15][root][INFO] - Training Epoch: 2/2, step 3774/7134 completed (loss: 0.14445427060127258, acc: 0.9670329689979553)
[2025-02-13 20:27:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:16][root][INFO] - Training Epoch: 2/2, step 3775/7134 completed (loss: 0.07986043393611908, acc: 0.9813664555549622)
[2025-02-13 20:27:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:16][root][INFO] - Training Epoch: 2/2, step 3776/7134 completed (loss: 0.14277346432209015, acc: 0.9679487347602844)
[2025-02-13 20:27:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:16][root][INFO] - Training Epoch: 2/2, step 3777/7134 completed (loss: 0.04297555610537529, acc: 0.9941176176071167)
[2025-02-13 20:27:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:17][root][INFO] - Training Epoch: 2/2, step 3778/7134 completed (loss: 0.12998461723327637, acc: 0.9651162624359131)
[2025-02-13 20:27:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:17][root][INFO] - Training Epoch: 2/2, step 3779/7134 completed (loss: 0.10282449424266815, acc: 0.981249988079071)
[2025-02-13 20:27:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:18][root][INFO] - Training Epoch: 2/2, step 3780/7134 completed (loss: 0.060638103634119034, acc: 0.9757575988769531)
[2025-02-13 20:27:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:18][root][INFO] - Training Epoch: 2/2, step 3781/7134 completed (loss: 0.10130062699317932, acc: 0.9689440727233887)
[2025-02-13 20:27:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:18][root][INFO] - Training Epoch: 2/2, step 3782/7134 completed (loss: 0.0580894909799099, acc: 0.9937106966972351)
[2025-02-13 20:27:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:19][root][INFO] - Training Epoch: 2/2, step 3783/7134 completed (loss: 0.15940475463867188, acc: 0.9644970297813416)
[2025-02-13 20:27:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:19][root][INFO] - Training Epoch: 2/2, step 3784/7134 completed (loss: 0.12787705659866333, acc: 0.9664429426193237)
[2025-02-13 20:27:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:19][root][INFO] - Training Epoch: 2/2, step 3785/7134 completed (loss: 0.11324179172515869, acc: 0.978723406791687)
[2025-02-13 20:27:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:20][root][INFO] - Training Epoch: 2/2, step 3786/7134 completed (loss: 0.03193124383687973, acc: 0.988095223903656)
[2025-02-13 20:27:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:20][root][INFO] - Training Epoch: 2/2, step 3787/7134 completed (loss: 0.0474739708006382, acc: 0.9923076629638672)
[2025-02-13 20:27:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:21][root][INFO] - Training Epoch: 2/2, step 3788/7134 completed (loss: 0.14225496351718903, acc: 0.9754601120948792)
[2025-02-13 20:27:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:21][root][INFO] - Training Epoch: 2/2, step 3789/7134 completed (loss: 0.14613334834575653, acc: 0.970588207244873)
[2025-02-13 20:27:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:21][root][INFO] - Training Epoch: 2/2, step 3790/7134 completed (loss: 0.17987921833992004, acc: 0.970588207244873)
[2025-02-13 20:27:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:22][root][INFO] - Training Epoch: 2/2, step 3791/7134 completed (loss: 0.08864225447177887, acc: 0.9768785834312439)
[2025-02-13 20:27:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:22][root][INFO] - Training Epoch: 2/2, step 3792/7134 completed (loss: 0.06509087979793549, acc: 0.9849624037742615)
[2025-02-13 20:27:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:22][root][INFO] - Training Epoch: 2/2, step 3793/7134 completed (loss: 0.1288750022649765, acc: 0.959770143032074)
[2025-02-13 20:27:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:23][root][INFO] - Training Epoch: 2/2, step 3794/7134 completed (loss: 0.17110683023929596, acc: 0.9611111283302307)
[2025-02-13 20:27:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:23][root][INFO] - Training Epoch: 2/2, step 3795/7134 completed (loss: 0.10957889258861542, acc: 0.9777777791023254)
[2025-02-13 20:27:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:24][root][INFO] - Training Epoch: 2/2, step 3796/7134 completed (loss: 0.10139641910791397, acc: 0.9766082167625427)
[2025-02-13 20:27:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:24][root][INFO] - Training Epoch: 2/2, step 3797/7134 completed (loss: 0.07547574490308762, acc: 0.9757575988769531)
[2025-02-13 20:27:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:24][root][INFO] - Training Epoch: 2/2, step 3798/7134 completed (loss: 0.2530888020992279, acc: 0.9308176040649414)
[2025-02-13 20:27:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:25][root][INFO] - Training Epoch: 2/2, step 3799/7134 completed (loss: 0.23747947812080383, acc: 0.9491525292396545)
[2025-02-13 20:27:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:25][root][INFO] - Training Epoch: 2/2, step 3800/7134 completed (loss: 0.267221599817276, acc: 0.9379310607910156)
[2025-02-13 20:27:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:26][root][INFO] - Training Epoch: 2/2, step 3801/7134 completed (loss: 0.1540999710559845, acc: 0.9613259434700012)
[2025-02-13 20:27:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:26][root][INFO] - Training Epoch: 2/2, step 3802/7134 completed (loss: 0.13868804275989532, acc: 0.9479166865348816)
[2025-02-13 20:27:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:26][root][INFO] - Training Epoch: 2/2, step 3803/7134 completed (loss: 0.11652638763189316, acc: 0.9681528806686401)
[2025-02-13 20:27:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:27][root][INFO] - Training Epoch: 2/2, step 3804/7134 completed (loss: 0.1672007143497467, acc: 0.9647887349128723)
[2025-02-13 20:27:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:27][root][INFO] - Training Epoch: 2/2, step 3805/7134 completed (loss: 0.17298859357833862, acc: 0.9657142758369446)
[2025-02-13 20:27:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:28][root][INFO] - Training Epoch: 2/2, step 3806/7134 completed (loss: 0.07394261658191681, acc: 0.9907407164573669)
[2025-02-13 20:27:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:28][root][INFO] - Training Epoch: 2/2, step 3807/7134 completed (loss: 0.07103586941957474, acc: 0.9794520735740662)
[2025-02-13 20:27:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:28][root][INFO] - Training Epoch: 2/2, step 3808/7134 completed (loss: 0.16139103472232819, acc: 0.95652174949646)
[2025-02-13 20:27:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:29][root][INFO] - Training Epoch: 2/2, step 3809/7134 completed (loss: 0.14948077499866486, acc: 0.9534883499145508)
[2025-02-13 20:27:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:29][root][INFO] - Training Epoch: 2/2, step 3810/7134 completed (loss: 0.10795653611421585, acc: 0.9935064911842346)
[2025-02-13 20:27:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:30][root][INFO] - Training Epoch: 2/2, step 3811/7134 completed (loss: 0.18526999652385712, acc: 0.9520958065986633)
[2025-02-13 20:27:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:30][root][INFO] - Training Epoch: 2/2, step 3812/7134 completed (loss: 0.2493894100189209, acc: 0.9251700639724731)
[2025-02-13 20:27:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:31][root][INFO] - Training Epoch: 2/2, step 3813/7134 completed (loss: 0.3151795566082001, acc: 0.9405405521392822)
[2025-02-13 20:27:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:31][root][INFO] - Training Epoch: 2/2, step 3814/7134 completed (loss: 0.10961111634969711, acc: 0.9714285731315613)
[2025-02-13 20:27:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:31][root][INFO] - Training Epoch: 2/2, step 3815/7134 completed (loss: 0.08625870198011398, acc: 0.9736841917037964)
[2025-02-13 20:27:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:32][root][INFO] - Training Epoch: 2/2, step 3816/7134 completed (loss: 0.07196467369794846, acc: 0.9701492786407471)
[2025-02-13 20:27:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:32][root][INFO] - Training Epoch: 2/2, step 3817/7134 completed (loss: 0.07941063493490219, acc: 0.966292142868042)
[2025-02-13 20:27:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:32][root][INFO] - Training Epoch: 2/2, step 3818/7134 completed (loss: 0.1916283667087555, acc: 0.9561403393745422)
[2025-02-13 20:27:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:33][root][INFO] - Training Epoch: 2/2, step 3819/7134 completed (loss: 0.014784954488277435, acc: 1.0)
[2025-02-13 20:27:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:33][root][INFO] - Training Epoch: 2/2, step 3820/7134 completed (loss: 0.10357419401407242, acc: 0.9722222089767456)
[2025-02-13 20:27:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:34][root][INFO] - Training Epoch: 2/2, step 3821/7134 completed (loss: 0.1312926560640335, acc: 0.9726027250289917)
[2025-02-13 20:27:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:34][root][INFO] - Training Epoch: 2/2, step 3822/7134 completed (loss: 0.04295547679066658, acc: 0.9905660152435303)
[2025-02-13 20:27:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:34][root][INFO] - Training Epoch: 2/2, step 3823/7134 completed (loss: 0.04711519926786423, acc: 0.9865771532058716)
[2025-02-13 20:27:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:35][root][INFO] - Training Epoch: 2/2, step 3824/7134 completed (loss: 0.119513601064682, acc: 0.96875)
[2025-02-13 20:27:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:35][root][INFO] - Training Epoch: 2/2, step 3825/7134 completed (loss: 0.13667403161525726, acc: 0.9444444179534912)
[2025-02-13 20:27:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:36][root][INFO] - Training Epoch: 2/2, step 3826/7134 completed (loss: 0.09656663239002228, acc: 0.9647887349128723)
[2025-02-13 20:27:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:36][root][INFO] - Training Epoch: 2/2, step 3827/7134 completed (loss: 0.10557775944471359, acc: 0.9808917045593262)
[2025-02-13 20:27:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:37][root][INFO] - Training Epoch: 2/2, step 3828/7134 completed (loss: 0.060637325048446655, acc: 1.0)
[2025-02-13 20:27:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:37][root][INFO] - Training Epoch: 2/2, step 3829/7134 completed (loss: 0.10097865760326385, acc: 0.9765625)
[2025-02-13 20:27:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:37][root][INFO] - Training Epoch: 2/2, step 3830/7134 completed (loss: 0.09707906097173691, acc: 0.9925925731658936)
[2025-02-13 20:27:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:38][root][INFO] - Training Epoch: 2/2, step 3831/7134 completed (loss: 0.15979976952075958, acc: 0.9473684430122375)
[2025-02-13 20:27:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:38][root][INFO] - Training Epoch: 2/2, step 3832/7134 completed (loss: 0.0856458768248558, acc: 0.9910714030265808)
[2025-02-13 20:27:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:39][root][INFO] - Training Epoch: 2/2, step 3833/7134 completed (loss: 0.061431948095560074, acc: 0.9781420826911926)
[2025-02-13 20:27:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:39][root][INFO] - Training Epoch: 2/2, step 3834/7134 completed (loss: 0.008401446044445038, acc: 1.0)
[2025-02-13 20:27:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:39][root][INFO] - Training Epoch: 2/2, step 3835/7134 completed (loss: 0.052559029310941696, acc: 0.9895833134651184)
[2025-02-13 20:27:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:40][root][INFO] - Training Epoch: 2/2, step 3836/7134 completed (loss: 0.1941773146390915, acc: 0.9753086566925049)
[2025-02-13 20:27:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:40][root][INFO] - Training Epoch: 2/2, step 3837/7134 completed (loss: 0.12692520022392273, acc: 0.9808917045593262)
[2025-02-13 20:27:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:41][root][INFO] - Training Epoch: 2/2, step 3838/7134 completed (loss: 0.04143590107560158, acc: 0.9887640476226807)
[2025-02-13 20:27:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:41][root][INFO] - Training Epoch: 2/2, step 3839/7134 completed (loss: 0.029395800083875656, acc: 0.9937888383865356)
[2025-02-13 20:27:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:41][root][INFO] - Training Epoch: 2/2, step 3840/7134 completed (loss: 0.04376903176307678, acc: 0.9939393997192383)
[2025-02-13 20:27:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:42][root][INFO] - Training Epoch: 2/2, step 3841/7134 completed (loss: 0.07264848053455353, acc: 0.9849246144294739)
[2025-02-13 20:27:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:42][root][INFO] - Training Epoch: 2/2, step 3842/7134 completed (loss: 0.026859289035201073, acc: 0.9896907210350037)
[2025-02-13 20:27:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:43][root][INFO] - Training Epoch: 2/2, step 3843/7134 completed (loss: 0.12493766844272614, acc: 0.977477490901947)
[2025-02-13 20:27:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:43][root][INFO] - Training Epoch: 2/2, step 3844/7134 completed (loss: 0.1912321150302887, acc: 0.963350772857666)
[2025-02-13 20:27:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:43][root][INFO] - Training Epoch: 2/2, step 3845/7134 completed (loss: 0.10293535888195038, acc: 0.9785714149475098)
[2025-02-13 20:27:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:44][root][INFO] - Training Epoch: 2/2, step 3846/7134 completed (loss: 0.0400630384683609, acc: 0.9927007555961609)
[2025-02-13 20:27:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:44][root][INFO] - Training Epoch: 2/2, step 3847/7134 completed (loss: 0.04996368661522865, acc: 0.9858155846595764)
[2025-02-13 20:27:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:45][root][INFO] - Training Epoch: 2/2, step 3848/7134 completed (loss: 0.12507115304470062, acc: 0.9659090638160706)
[2025-02-13 20:27:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:45][root][INFO] - Training Epoch: 2/2, step 3849/7134 completed (loss: 0.10081744939088821, acc: 0.9629629850387573)
[2025-02-13 20:27:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:45][root][INFO] - Training Epoch: 2/2, step 3850/7134 completed (loss: 0.1263471245765686, acc: 0.9617486596107483)
[2025-02-13 20:27:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:46][root][INFO] - Training Epoch: 2/2, step 3851/7134 completed (loss: 0.17006799578666687, acc: 0.9583333134651184)
[2025-02-13 20:27:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:46][root][INFO] - Training Epoch: 2/2, step 3852/7134 completed (loss: 0.030524373054504395, acc: 1.0)
[2025-02-13 20:27:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:46][root][INFO] - Training Epoch: 2/2, step 3853/7134 completed (loss: 0.14191342890262604, acc: 0.9553072452545166)
[2025-02-13 20:27:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:47][root][INFO] - Training Epoch: 2/2, step 3854/7134 completed (loss: 0.08611368387937546, acc: 0.987261176109314)
[2025-02-13 20:27:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:47][root][INFO] - Training Epoch: 2/2, step 3855/7134 completed (loss: 0.11191153526306152, acc: 0.9640718698501587)
[2025-02-13 20:27:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:48][root][INFO] - Training Epoch: 2/2, step 3856/7134 completed (loss: 0.06911275535821915, acc: 0.987261176109314)
[2025-02-13 20:27:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:48][root][INFO] - Training Epoch: 2/2, step 3857/7134 completed (loss: 0.09861729294061661, acc: 0.9662162065505981)
[2025-02-13 20:27:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:48][root][INFO] - Training Epoch: 2/2, step 3858/7134 completed (loss: 0.29339057207107544, acc: 0.9375)
[2025-02-13 20:27:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:49][root][INFO] - Training Epoch: 2/2, step 3859/7134 completed (loss: 0.12279858440160751, acc: 0.9717513918876648)
[2025-02-13 20:27:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:49][root][INFO] - Training Epoch: 2/2, step 3860/7134 completed (loss: 0.06559469550848007, acc: 0.9738219976425171)
[2025-02-13 20:27:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:49][root][INFO] - Training Epoch: 2/2, step 3861/7134 completed (loss: 0.14452700316905975, acc: 0.9607843160629272)
[2025-02-13 20:27:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:50][root][INFO] - Training Epoch: 2/2, step 3862/7134 completed (loss: 0.0710303783416748, acc: 0.9923076629638672)
[2025-02-13 20:27:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:50][root][INFO] - Training Epoch: 2/2, step 3863/7134 completed (loss: 0.027029240503907204, acc: 0.9941860437393188)
[2025-02-13 20:27:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:51][root][INFO] - Training Epoch: 2/2, step 3864/7134 completed (loss: 0.09458120912313461, acc: 0.9870129823684692)
[2025-02-13 20:27:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:51][root][INFO] - Training Epoch: 2/2, step 3865/7134 completed (loss: 0.19362057745456696, acc: 0.9196428656578064)
[2025-02-13 20:27:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:51][root][INFO] - Training Epoch: 2/2, step 3866/7134 completed (loss: 0.1251777559518814, acc: 0.9599999785423279)
[2025-02-13 20:27:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:52][root][INFO] - Training Epoch: 2/2, step 3867/7134 completed (loss: 0.1110212579369545, acc: 0.9636363387107849)
[2025-02-13 20:27:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:52][root][INFO] - Training Epoch: 2/2, step 3868/7134 completed (loss: 0.038945265114307404, acc: 0.9926470518112183)
[2025-02-13 20:27:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:53][root][INFO] - Training Epoch: 2/2, step 3869/7134 completed (loss: 0.09270475059747696, acc: 0.9720279574394226)
[2025-02-13 20:27:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:53][root][INFO] - Training Epoch: 2/2, step 3870/7134 completed (loss: 0.217716783285141, acc: 0.9411764740943909)
[2025-02-13 20:27:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:53][root][INFO] - Training Epoch: 2/2, step 3871/7134 completed (loss: 0.13656245172023773, acc: 0.9655172228813171)
[2025-02-13 20:27:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:54][root][INFO] - Training Epoch: 2/2, step 3872/7134 completed (loss: 0.10424956679344177, acc: 0.9797979593276978)
[2025-02-13 20:27:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:54][root][INFO] - Training Epoch: 2/2, step 3873/7134 completed (loss: 0.07717230916023254, acc: 0.9757575988769531)
[2025-02-13 20:27:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:55][root][INFO] - Training Epoch: 2/2, step 3874/7134 completed (loss: 0.08361927419900894, acc: 0.9766082167625427)
[2025-02-13 20:27:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:55][root][INFO] - Training Epoch: 2/2, step 3875/7134 completed (loss: 0.12344401329755783, acc: 0.9683544039726257)
[2025-02-13 20:27:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:55][root][INFO] - Training Epoch: 2/2, step 3876/7134 completed (loss: 0.058790065348148346, acc: 0.984000027179718)
[2025-02-13 20:27:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:56][root][INFO] - Training Epoch: 2/2, step 3877/7134 completed (loss: 0.14626263082027435, acc: 0.9767441749572754)
[2025-02-13 20:27:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:56][root][INFO] - Training Epoch: 2/2, step 3878/7134 completed (loss: 0.38955867290496826, acc: 0.9127907156944275)
[2025-02-13 20:27:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:57][root][INFO] - Training Epoch: 2/2, step 3879/7134 completed (loss: 0.110052689909935, acc: 0.9552238583564758)
[2025-02-13 20:27:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:57][root][INFO] - Training Epoch: 2/2, step 3880/7134 completed (loss: 0.16505154967308044, acc: 0.9615384340286255)
[2025-02-13 20:27:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:57][root][INFO] - Training Epoch: 2/2, step 3881/7134 completed (loss: 0.2788655161857605, acc: 0.9411764740943909)
[2025-02-13 20:27:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:58][root][INFO] - Training Epoch: 2/2, step 3882/7134 completed (loss: 0.18011751770973206, acc: 0.9484536051750183)
[2025-02-13 20:27:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:58][root][INFO] - Training Epoch: 2/2, step 3883/7134 completed (loss: 0.06918354332447052, acc: 0.9934640526771545)
[2025-02-13 20:27:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:58][root][INFO] - Training Epoch: 2/2, step 3884/7134 completed (loss: 0.08171555399894714, acc: 0.9885714054107666)
[2025-02-13 20:27:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:59][root][INFO] - Training Epoch: 2/2, step 3885/7134 completed (loss: 0.03553381189703941, acc: 0.993630588054657)
[2025-02-13 20:27:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:27:59][root][INFO] - Training Epoch: 2/2, step 3886/7134 completed (loss: 0.11010463535785675, acc: 0.9788359999656677)
[2025-02-13 20:27:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:00][root][INFO] - Training Epoch: 2/2, step 3887/7134 completed (loss: 0.13605313003063202, acc: 0.9685534834861755)
[2025-02-13 20:28:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:00][root][INFO] - Training Epoch: 2/2, step 3888/7134 completed (loss: 0.069233737885952, acc: 0.9767441749572754)
[2025-02-13 20:28:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:00][root][INFO] - Training Epoch: 2/2, step 3889/7134 completed (loss: 0.04066009446978569, acc: 0.994350254535675)
[2025-02-13 20:28:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:01][root][INFO] - Training Epoch: 2/2, step 3890/7134 completed (loss: 0.044922418892383575, acc: 0.9896907210350037)
[2025-02-13 20:28:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:01][root][INFO] - Training Epoch: 2/2, step 3891/7134 completed (loss: 0.046535734087228775, acc: 0.9942196607589722)
[2025-02-13 20:28:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:02][root][INFO] - Training Epoch: 2/2, step 3892/7134 completed (loss: 0.12366512417793274, acc: 0.9733333587646484)
[2025-02-13 20:28:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:02][root][INFO] - Training Epoch: 2/2, step 3893/7134 completed (loss: 0.17042189836502075, acc: 0.9484536051750183)
[2025-02-13 20:28:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:02][root][INFO] - Training Epoch: 2/2, step 3894/7134 completed (loss: 0.06434757262468338, acc: 0.9936708807945251)
[2025-02-13 20:28:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:03][root][INFO] - Training Epoch: 2/2, step 3895/7134 completed (loss: 0.10733567178249359, acc: 0.9723756909370422)
[2025-02-13 20:28:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:03][root][INFO] - Training Epoch: 2/2, step 3896/7134 completed (loss: 0.09751954674720764, acc: 0.9763033390045166)
[2025-02-13 20:28:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:03][root][INFO] - Training Epoch: 2/2, step 3897/7134 completed (loss: 0.25818678736686707, acc: 0.9651162624359131)
[2025-02-13 20:28:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:04][root][INFO] - Training Epoch: 2/2, step 3898/7134 completed (loss: 0.14140157401561737, acc: 0.9655172228813171)
[2025-02-13 20:28:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:04][root][INFO] - Training Epoch: 2/2, step 3899/7134 completed (loss: 0.08599380403757095, acc: 0.9884393215179443)
[2025-02-13 20:28:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:05][root][INFO] - Training Epoch: 2/2, step 3900/7134 completed (loss: 0.09812625497579575, acc: 0.9742268323898315)
[2025-02-13 20:28:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:05][root][INFO] - Training Epoch: 2/2, step 3901/7134 completed (loss: 0.2982282042503357, acc: 0.9337349534034729)
[2025-02-13 20:28:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:05][root][INFO] - Training Epoch: 2/2, step 3902/7134 completed (loss: 0.2700457274913788, acc: 0.9270833134651184)
[2025-02-13 20:28:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:06][root][INFO] - Training Epoch: 2/2, step 3903/7134 completed (loss: 0.3091205060482025, acc: 0.9222797751426697)
[2025-02-13 20:28:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:06][root][INFO] - Training Epoch: 2/2, step 3904/7134 completed (loss: 0.17689310014247894, acc: 0.9298245906829834)
[2025-02-13 20:28:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:07][root][INFO] - Training Epoch: 2/2, step 3905/7134 completed (loss: 0.07221252471208572, acc: 0.9722222089767456)
[2025-02-13 20:28:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:07][root][INFO] - Training Epoch: 2/2, step 3906/7134 completed (loss: 0.07782858610153198, acc: 0.9775280952453613)
[2025-02-13 20:28:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:07][root][INFO] - Training Epoch: 2/2, step 3907/7134 completed (loss: 0.06786173582077026, acc: 0.9793814420700073)
[2025-02-13 20:28:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:08][root][INFO] - Training Epoch: 2/2, step 3908/7134 completed (loss: 0.16006602346897125, acc: 0.9653179049491882)
[2025-02-13 20:28:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:08][root][INFO] - Training Epoch: 2/2, step 3909/7134 completed (loss: 0.07205948233604431, acc: 0.9748427867889404)
[2025-02-13 20:28:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:08][root][INFO] - Training Epoch: 2/2, step 3910/7134 completed (loss: 0.052787281572818756, acc: 0.9886363744735718)
[2025-02-13 20:28:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:09][root][INFO] - Training Epoch: 2/2, step 3911/7134 completed (loss: 0.09685118496417999, acc: 0.9874213933944702)
[2025-02-13 20:28:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:09][root][INFO] - Training Epoch: 2/2, step 3912/7134 completed (loss: 0.019436560571193695, acc: 1.0)
[2025-02-13 20:28:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:10][root][INFO] - Training Epoch: 2/2, step 3913/7134 completed (loss: 0.03897302225232124, acc: 0.9898989796638489)
[2025-02-13 20:28:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:10][root][INFO] - Training Epoch: 2/2, step 3914/7134 completed (loss: 0.0643782690167427, acc: 0.9811320900917053)
[2025-02-13 20:28:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:10][root][INFO] - Training Epoch: 2/2, step 3915/7134 completed (loss: 0.041439492255449295, acc: 0.9942528605461121)
[2025-02-13 20:28:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:11][root][INFO] - Training Epoch: 2/2, step 3916/7134 completed (loss: 0.06265337020158768, acc: 0.9795918464660645)
[2025-02-13 20:28:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:11][root][INFO] - Training Epoch: 2/2, step 3917/7134 completed (loss: 0.050587281584739685, acc: 0.9953488111495972)
[2025-02-13 20:28:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:12][root][INFO] - Training Epoch: 2/2, step 3918/7134 completed (loss: 0.06112557649612427, acc: 0.9809523820877075)
[2025-02-13 20:28:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:12][root][INFO] - Training Epoch: 2/2, step 3919/7134 completed (loss: 0.1880328208208084, acc: 0.9644970297813416)
[2025-02-13 20:28:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:12][root][INFO] - Training Epoch: 2/2, step 3920/7134 completed (loss: 0.242436021566391, acc: 0.946107804775238)
[2025-02-13 20:28:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:13][root][INFO] - Training Epoch: 2/2, step 3921/7134 completed (loss: 0.15919773280620575, acc: 0.9577465057373047)
[2025-02-13 20:28:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:13][root][INFO] - Training Epoch: 2/2, step 3922/7134 completed (loss: 0.1804320365190506, acc: 0.9545454382896423)
[2025-02-13 20:28:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:13][root][INFO] - Training Epoch: 2/2, step 3923/7134 completed (loss: 0.27264732122421265, acc: 0.9285714030265808)
[2025-02-13 20:28:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:14][root][INFO] - Training Epoch: 2/2, step 3924/7134 completed (loss: 0.09430991858243942, acc: 0.9793814420700073)
[2025-02-13 20:28:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:14][root][INFO] - Training Epoch: 2/2, step 3925/7134 completed (loss: 0.1449708342552185, acc: 0.9668508172035217)
[2025-02-13 20:28:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:15][root][INFO] - Training Epoch: 2/2, step 3926/7134 completed (loss: 0.12092997133731842, acc: 0.9683544039726257)
[2025-02-13 20:28:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:15][root][INFO] - Training Epoch: 2/2, step 3927/7134 completed (loss: 0.1380433887243271, acc: 0.9818181991577148)
[2025-02-13 20:28:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:15][root][INFO] - Training Epoch: 2/2, step 3928/7134 completed (loss: 0.23446740210056305, acc: 0.9640718698501587)
[2025-02-13 20:28:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:16][root][INFO] - Training Epoch: 2/2, step 3929/7134 completed (loss: 0.04320710524916649, acc: 0.9805194735527039)
[2025-02-13 20:28:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:16][root][INFO] - Training Epoch: 2/2, step 3930/7134 completed (loss: 0.1564348340034485, acc: 0.9576271176338196)
[2025-02-13 20:28:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:17][root][INFO] - Training Epoch: 2/2, step 3931/7134 completed (loss: 0.11539951711893082, acc: 0.9683544039726257)
[2025-02-13 20:28:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:17][root][INFO] - Training Epoch: 2/2, step 3932/7134 completed (loss: 0.05124300345778465, acc: 1.0)
[2025-02-13 20:28:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:17][root][INFO] - Training Epoch: 2/2, step 3933/7134 completed (loss: 0.023257523775100708, acc: 1.0)
[2025-02-13 20:28:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:18][root][INFO] - Training Epoch: 2/2, step 3934/7134 completed (loss: 0.07023098319768906, acc: 0.9814814925193787)
[2025-02-13 20:28:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:18][root][INFO] - Training Epoch: 2/2, step 3935/7134 completed (loss: 0.03184794634580612, acc: 1.0)
[2025-02-13 20:28:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:18][root][INFO] - Training Epoch: 2/2, step 3936/7134 completed (loss: 0.052491672337055206, acc: 0.9919354915618896)
[2025-02-13 20:28:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:19][root][INFO] - Training Epoch: 2/2, step 3937/7134 completed (loss: 0.16346058249473572, acc: 0.9576271176338196)
[2025-02-13 20:28:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:19][root][INFO] - Training Epoch: 2/2, step 3938/7134 completed (loss: 0.07709614187479019, acc: 0.9729729890823364)
[2025-02-13 20:28:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:20][root][INFO] - Training Epoch: 2/2, step 3939/7134 completed (loss: 0.1453186720609665, acc: 0.9629629850387573)
[2025-02-13 20:28:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:20][root][INFO] - Training Epoch: 2/2, step 3940/7134 completed (loss: 0.05774284526705742, acc: 0.989130437374115)
[2025-02-13 20:28:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:20][root][INFO] - Training Epoch: 2/2, step 3941/7134 completed (loss: 0.07823477685451508, acc: 0.9634146094322205)
[2025-02-13 20:28:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:21][root][INFO] - Training Epoch: 2/2, step 3942/7134 completed (loss: 0.09928658604621887, acc: 0.9679999947547913)
[2025-02-13 20:28:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:21][root][INFO] - Training Epoch: 2/2, step 3943/7134 completed (loss: 0.06692173331975937, acc: 0.9718309640884399)
[2025-02-13 20:28:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:22][root][INFO] - Training Epoch: 2/2, step 3944/7134 completed (loss: 0.13620395958423615, acc: 0.9701492786407471)
[2025-02-13 20:28:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:22][root][INFO] - Training Epoch: 2/2, step 3945/7134 completed (loss: 0.02984728291630745, acc: 0.9934210777282715)
[2025-02-13 20:28:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:22][root][INFO] - Training Epoch: 2/2, step 3946/7134 completed (loss: 0.05438051372766495, acc: 0.9732142686843872)
[2025-02-13 20:28:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:23][root][INFO] - Training Epoch: 2/2, step 3947/7134 completed (loss: 0.027505384758114815, acc: 1.0)
[2025-02-13 20:28:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:23][root][INFO] - Training Epoch: 2/2, step 3948/7134 completed (loss: 0.02143757790327072, acc: 0.9900000095367432)
[2025-02-13 20:28:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:24][root][INFO] - Training Epoch: 2/2, step 3949/7134 completed (loss: 0.13015905022621155, acc: 0.96875)
[2025-02-13 20:28:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:24][root][INFO] - Training Epoch: 2/2, step 3950/7134 completed (loss: 0.0708666443824768, acc: 0.9912280440330505)
[2025-02-13 20:28:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:24][root][INFO] - Training Epoch: 2/2, step 3951/7134 completed (loss: 0.030085964128375053, acc: 1.0)
[2025-02-13 20:28:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:25][root][INFO] - Training Epoch: 2/2, step 3952/7134 completed (loss: 0.05858158692717552, acc: 0.9897959232330322)
[2025-02-13 20:28:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:25][root][INFO] - Training Epoch: 2/2, step 3953/7134 completed (loss: 0.0419359914958477, acc: 0.9855072498321533)
[2025-02-13 20:28:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:26][root][INFO] - Training Epoch: 2/2, step 3954/7134 completed (loss: 0.02326214499771595, acc: 1.0)
[2025-02-13 20:28:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:26][root][INFO] - Training Epoch: 2/2, step 3955/7134 completed (loss: 0.05775336176156998, acc: 0.9842519760131836)
[2025-02-13 20:28:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:26][root][INFO] - Training Epoch: 2/2, step 3956/7134 completed (loss: 0.07802809029817581, acc: 0.9869281053543091)
[2025-02-13 20:28:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:27][root][INFO] - Training Epoch: 2/2, step 3957/7134 completed (loss: 0.05246025323867798, acc: 0.9914529919624329)
[2025-02-13 20:28:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:27][root][INFO] - Training Epoch: 2/2, step 3958/7134 completed (loss: 0.07199878245592117, acc: 0.988095223903656)
[2025-02-13 20:28:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:27][root][INFO] - Training Epoch: 2/2, step 3959/7134 completed (loss: 0.050872039049863815, acc: 0.9913793206214905)
[2025-02-13 20:28:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:28][root][INFO] - Training Epoch: 2/2, step 3960/7134 completed (loss: 0.05076438933610916, acc: 0.9888268113136292)
[2025-02-13 20:28:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:28][root][INFO] - Training Epoch: 2/2, step 3961/7134 completed (loss: 0.029347414150834084, acc: 0.9938271641731262)
[2025-02-13 20:28:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:29][root][INFO] - Training Epoch: 2/2, step 3962/7134 completed (loss: 0.05969689041376114, acc: 0.9798657894134521)
[2025-02-13 20:28:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:29][root][INFO] - Training Epoch: 2/2, step 3963/7134 completed (loss: 0.1273508220911026, acc: 0.97826087474823)
[2025-02-13 20:28:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:29][root][INFO] - Training Epoch: 2/2, step 3964/7134 completed (loss: 0.16682177782058716, acc: 0.9461538195610046)
[2025-02-13 20:28:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:30][root][INFO] - Training Epoch: 2/2, step 3965/7134 completed (loss: 0.1001598909497261, acc: 0.9696969985961914)
[2025-02-13 20:28:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:30][root][INFO] - Training Epoch: 2/2, step 3966/7134 completed (loss: 0.040223460644483566, acc: 0.984375)
[2025-02-13 20:28:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:30][root][INFO] - Training Epoch: 2/2, step 3967/7134 completed (loss: 0.18967305123806, acc: 0.9492753744125366)
[2025-02-13 20:28:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:31][root][INFO] - Training Epoch: 2/2, step 3968/7134 completed (loss: 0.02384389564394951, acc: 0.9888268113136292)
[2025-02-13 20:28:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:31][root][INFO] - Training Epoch: 2/2, step 3969/7134 completed (loss: 0.11253637820482254, acc: 0.9622641801834106)
[2025-02-13 20:28:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:32][root][INFO] - Training Epoch: 2/2, step 3970/7134 completed (loss: 0.190422922372818, acc: 0.9595375657081604)
[2025-02-13 20:28:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:32][root][INFO] - Training Epoch: 2/2, step 3971/7134 completed (loss: 0.43619874119758606, acc: 0.928205132484436)
[2025-02-13 20:28:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:32][root][INFO] - Training Epoch: 2/2, step 3972/7134 completed (loss: 0.5327000021934509, acc: 0.8653846383094788)
[2025-02-13 20:28:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:33][root][INFO] - Training Epoch: 2/2, step 3973/7134 completed (loss: 0.1816076934337616, acc: 0.9507042169570923)
[2025-02-13 20:28:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:33][root][INFO] - Training Epoch: 2/2, step 3974/7134 completed (loss: 0.21891094744205475, acc: 0.9487179517745972)
[2025-02-13 20:28:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:34][root][INFO] - Training Epoch: 2/2, step 3975/7134 completed (loss: 0.1161469891667366, acc: 0.9828571677207947)
[2025-02-13 20:28:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:34][root][INFO] - Training Epoch: 2/2, step 3976/7134 completed (loss: 0.3681541979312897, acc: 0.9337349534034729)
[2025-02-13 20:28:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:34][root][INFO] - Training Epoch: 2/2, step 3977/7134 completed (loss: 0.0492217130959034, acc: 0.9927536249160767)
[2025-02-13 20:28:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:35][root][INFO] - Training Epoch: 2/2, step 3978/7134 completed (loss: 0.08541558682918549, acc: 0.9701492786407471)
[2025-02-13 20:28:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:35][root][INFO] - Training Epoch: 2/2, step 3979/7134 completed (loss: 0.09178338199853897, acc: 0.9781022071838379)
[2025-02-13 20:28:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:35][root][INFO] - Training Epoch: 2/2, step 3980/7134 completed (loss: 0.07802344113588333, acc: 0.9741935729980469)
[2025-02-13 20:28:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:36][root][INFO] - Training Epoch: 2/2, step 3981/7134 completed (loss: 0.15096184611320496, acc: 0.9638554453849792)
[2025-02-13 20:28:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:36][root][INFO] - Training Epoch: 2/2, step 3982/7134 completed (loss: 0.3094169497489929, acc: 0.930232584476471)
[2025-02-13 20:28:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:37][root][INFO] - Training Epoch: 2/2, step 3983/7134 completed (loss: 0.6494107842445374, acc: 0.8372092843055725)
[2025-02-13 20:28:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:37][root][INFO] - Training Epoch: 2/2, step 3984/7134 completed (loss: 0.2903882563114166, acc: 0.9255319237709045)
[2025-02-13 20:28:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:37][root][INFO] - Training Epoch: 2/2, step 3985/7134 completed (loss: 0.10010246932506561, acc: 0.9692307710647583)
[2025-02-13 20:28:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:38][root][INFO] - Training Epoch: 2/2, step 3986/7134 completed (loss: 0.4267745018005371, acc: 0.9147727489471436)
[2025-02-13 20:28:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:38][root][INFO] - Training Epoch: 2/2, step 3987/7134 completed (loss: 0.1636584848165512, acc: 0.920634925365448)
[2025-02-13 20:28:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:38][root][INFO] - Training Epoch: 2/2, step 3988/7134 completed (loss: 0.25297579169273376, acc: 0.9425837397575378)
[2025-02-13 20:28:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:39][root][INFO] - Training Epoch: 2/2, step 3989/7134 completed (loss: 0.2564677298069, acc: 0.9504132270812988)
[2025-02-13 20:28:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:39][root][INFO] - Training Epoch: 2/2, step 3990/7134 completed (loss: 0.19026347994804382, acc: 0.9536423683166504)
[2025-02-13 20:28:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:40][root][INFO] - Training Epoch: 2/2, step 3991/7134 completed (loss: 0.09948799014091492, acc: 0.9733333587646484)
[2025-02-13 20:28:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:40][root][INFO] - Training Epoch: 2/2, step 3992/7134 completed (loss: 0.13938291370868683, acc: 0.9780219793319702)
[2025-02-13 20:28:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:40][root][INFO] - Training Epoch: 2/2, step 3993/7134 completed (loss: 0.1632075309753418, acc: 0.9722222089767456)
[2025-02-13 20:28:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:41][root][INFO] - Training Epoch: 2/2, step 3994/7134 completed (loss: 0.15496493875980377, acc: 0.9607843160629272)
[2025-02-13 20:28:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:41][root][INFO] - Training Epoch: 2/2, step 3995/7134 completed (loss: 0.4670124650001526, acc: 0.8759689927101135)
[2025-02-13 20:28:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:42][root][INFO] - Training Epoch: 2/2, step 3996/7134 completed (loss: 0.18849678337574005, acc: 0.9586777091026306)
[2025-02-13 20:28:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:42][root][INFO] - Training Epoch: 2/2, step 3997/7134 completed (loss: 0.23019474744796753, acc: 0.9482758641242981)
[2025-02-13 20:28:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:42][root][INFO] - Training Epoch: 2/2, step 3998/7134 completed (loss: 0.1405992954969406, acc: 0.9615384340286255)
[2025-02-13 20:28:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:43][root][INFO] - Training Epoch: 2/2, step 3999/7134 completed (loss: 0.11791253834962845, acc: 0.975806474685669)
[2025-02-13 20:28:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:43][root][INFO] - Training Epoch: 2/2, step 4000/7134 completed (loss: 0.11600802838802338, acc: 0.9674796462059021)
[2025-02-13 20:28:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:44][root][INFO] - Training Epoch: 2/2, step 4001/7134 completed (loss: 0.27763229608535767, acc: 0.9274193644523621)
[2025-02-13 20:28:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:44][root][INFO] - Training Epoch: 2/2, step 4002/7134 completed (loss: 0.37307873368263245, acc: 0.9345794320106506)
[2025-02-13 20:28:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:44][root][INFO] - Training Epoch: 2/2, step 4003/7134 completed (loss: 0.30592983961105347, acc: 0.9172932505607605)
[2025-02-13 20:28:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:45][root][INFO] - Training Epoch: 2/2, step 4004/7134 completed (loss: 0.14299127459526062, acc: 0.949999988079071)
[2025-02-13 20:28:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:45][root][INFO] - Training Epoch: 2/2, step 4005/7134 completed (loss: 0.05415167286992073, acc: 0.9869281053543091)
[2025-02-13 20:28:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:46][root][INFO] - Training Epoch: 2/2, step 4006/7134 completed (loss: 0.05888877809047699, acc: 0.9819819927215576)
[2025-02-13 20:28:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:46][root][INFO] - Training Epoch: 2/2, step 4007/7134 completed (loss: 0.07821532338857651, acc: 0.988095223903656)
[2025-02-13 20:28:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:46][root][INFO] - Training Epoch: 2/2, step 4008/7134 completed (loss: 0.036260392516851425, acc: 0.9955157041549683)
[2025-02-13 20:28:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:47][root][INFO] - Training Epoch: 2/2, step 4009/7134 completed (loss: 0.027600912377238274, acc: 0.994413435459137)
[2025-02-13 20:28:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:47][root][INFO] - Training Epoch: 2/2, step 4010/7134 completed (loss: 0.05920858681201935, acc: 0.9856459498405457)
[2025-02-13 20:28:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:48][root][INFO] - Training Epoch: 2/2, step 4011/7134 completed (loss: 0.05831129848957062, acc: 0.9828571677207947)
[2025-02-13 20:28:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:48][root][INFO] - Training Epoch: 2/2, step 4012/7134 completed (loss: 0.05356811732053757, acc: 0.9759036302566528)
[2025-02-13 20:28:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:48][root][INFO] - Training Epoch: 2/2, step 4013/7134 completed (loss: 0.039083585143089294, acc: 0.9860140085220337)
[2025-02-13 20:28:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:49][root][INFO] - Training Epoch: 2/2, step 4014/7134 completed (loss: 0.02988521009683609, acc: 0.9866666793823242)
[2025-02-13 20:28:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:49][root][INFO] - Training Epoch: 2/2, step 4015/7134 completed (loss: 0.0620356909930706, acc: 0.9862068891525269)
[2025-02-13 20:28:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:49][root][INFO] - Training Epoch: 2/2, step 4016/7134 completed (loss: 0.1654682457447052, acc: 0.9585492014884949)
[2025-02-13 20:28:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:50][root][INFO] - Training Epoch: 2/2, step 4017/7134 completed (loss: 0.07959841191768646, acc: 0.9742268323898315)
[2025-02-13 20:28:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:50][root][INFO] - Training Epoch: 2/2, step 4018/7134 completed (loss: 0.08503221720457077, acc: 0.9710144996643066)
[2025-02-13 20:28:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:51][root][INFO] - Training Epoch: 2/2, step 4019/7134 completed (loss: 0.07507669925689697, acc: 0.9833333492279053)
[2025-02-13 20:28:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:51][root][INFO] - Training Epoch: 2/2, step 4020/7134 completed (loss: 0.09586703777313232, acc: 0.9829545617103577)
[2025-02-13 20:28:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:51][root][INFO] - Training Epoch: 2/2, step 4021/7134 completed (loss: 0.05502493679523468, acc: 0.9926470518112183)
[2025-02-13 20:28:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:52][root][INFO] - Training Epoch: 2/2, step 4022/7134 completed (loss: 0.06875202804803848, acc: 0.9807692170143127)
[2025-02-13 20:28:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:52][root][INFO] - Training Epoch: 2/2, step 4023/7134 completed (loss: 0.17783638834953308, acc: 0.9599999785423279)
[2025-02-13 20:28:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:53][root][INFO] - Training Epoch: 2/2, step 4024/7134 completed (loss: 0.07400576025247574, acc: 0.9885057210922241)
[2025-02-13 20:28:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:53][root][INFO] - Training Epoch: 2/2, step 4025/7134 completed (loss: 0.11041328310966492, acc: 0.9738219976425171)
[2025-02-13 20:28:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:53][root][INFO] - Training Epoch: 2/2, step 4026/7134 completed (loss: 0.19981537759304047, acc: 0.9689440727233887)
[2025-02-13 20:28:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:54][root][INFO] - Training Epoch: 2/2, step 4027/7134 completed (loss: 0.10216470807790756, acc: 0.9606741666793823)
[2025-02-13 20:28:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:54][root][INFO] - Training Epoch: 2/2, step 4028/7134 completed (loss: 0.14401103556156158, acc: 0.9685039520263672)
[2025-02-13 20:28:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:55][root][INFO] - Training Epoch: 2/2, step 4029/7134 completed (loss: 0.04639590159058571, acc: 1.0)
[2025-02-13 20:28:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:55][root][INFO] - Training Epoch: 2/2, step 4030/7134 completed (loss: 0.041447099298238754, acc: 0.9807692170143127)
[2025-02-13 20:28:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:55][root][INFO] - Training Epoch: 2/2, step 4031/7134 completed (loss: 0.023738501593470573, acc: 0.9937888383865356)
[2025-02-13 20:28:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:56][root][INFO] - Training Epoch: 2/2, step 4032/7134 completed (loss: 0.1738627851009369, acc: 0.9767441749572754)
[2025-02-13 20:28:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:56][root][INFO] - Training Epoch: 2/2, step 4033/7134 completed (loss: 0.03271295875310898, acc: 0.9946523904800415)
[2025-02-13 20:28:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:56][root][INFO] - Training Epoch: 2/2, step 4034/7134 completed (loss: 0.03269394487142563, acc: 0.9948979616165161)
[2025-02-13 20:28:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:57][root][INFO] - Training Epoch: 2/2, step 4035/7134 completed (loss: 0.21782340109348297, acc: 0.9512194991111755)
[2025-02-13 20:28:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:57][root][INFO] - Training Epoch: 2/2, step 4036/7134 completed (loss: 0.09351713210344315, acc: 0.9692307710647583)
[2025-02-13 20:28:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:58][root][INFO] - Training Epoch: 2/2, step 4037/7134 completed (loss: 0.09732917696237564, acc: 0.9791666865348816)
[2025-02-13 20:28:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:58][root][INFO] - Training Epoch: 2/2, step 4038/7134 completed (loss: 0.11945390701293945, acc: 0.9602272510528564)
[2025-02-13 20:28:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:58][root][INFO] - Training Epoch: 2/2, step 4039/7134 completed (loss: 0.15680743753910065, acc: 0.9644970297813416)
[2025-02-13 20:28:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:59][root][INFO] - Training Epoch: 2/2, step 4040/7134 completed (loss: 0.05842256173491478, acc: 0.9836065769195557)
[2025-02-13 20:28:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:59][root][INFO] - Training Epoch: 2/2, step 4041/7134 completed (loss: 0.08479973673820496, acc: 0.9884393215179443)
[2025-02-13 20:28:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:28:59][root][INFO] - Training Epoch: 2/2, step 4042/7134 completed (loss: 0.07999631017446518, acc: 0.9864864945411682)
[2025-02-13 20:29:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:00][root][INFO] - Training Epoch: 2/2, step 4043/7134 completed (loss: 0.13062874972820282, acc: 0.9430379867553711)
[2025-02-13 20:29:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:00][root][INFO] - Training Epoch: 2/2, step 4044/7134 completed (loss: 0.09656272083520889, acc: 0.9835164546966553)
[2025-02-13 20:29:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:01][root][INFO] - Training Epoch: 2/2, step 4045/7134 completed (loss: 0.10163839906454086, acc: 0.977142870426178)
[2025-02-13 20:29:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:01][root][INFO] - Training Epoch: 2/2, step 4046/7134 completed (loss: 0.1308949738740921, acc: 0.9649122953414917)
[2025-02-13 20:29:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:01][root][INFO] - Training Epoch: 2/2, step 4047/7134 completed (loss: 0.14613263309001923, acc: 0.9781420826911926)
[2025-02-13 20:29:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:02][root][INFO] - Training Epoch: 2/2, step 4048/7134 completed (loss: 0.0730474665760994, acc: 0.9746192693710327)
[2025-02-13 20:29:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:02][root][INFO] - Training Epoch: 2/2, step 4049/7134 completed (loss: 0.1467711627483368, acc: 0.9601989984512329)
[2025-02-13 20:29:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:03][root][INFO] - Training Epoch: 2/2, step 4050/7134 completed (loss: 0.1030978187918663, acc: 0.9624999761581421)
[2025-02-13 20:29:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:03][root][INFO] - Training Epoch: 2/2, step 4051/7134 completed (loss: 0.15404145419597626, acc: 0.9664804339408875)
[2025-02-13 20:29:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:03][root][INFO] - Training Epoch: 2/2, step 4052/7134 completed (loss: 0.08508395403623581, acc: 0.9830508232116699)
[2025-02-13 20:29:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:04][root][INFO] - Training Epoch: 2/2, step 4053/7134 completed (loss: 0.07967015355825424, acc: 0.9821428656578064)
[2025-02-13 20:29:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:04][root][INFO] - Training Epoch: 2/2, step 4054/7134 completed (loss: 0.265797883272171, acc: 0.939130425453186)
[2025-02-13 20:29:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:05][root][INFO] - Training Epoch: 2/2, step 4055/7134 completed (loss: 0.09474092721939087, acc: 0.9734042286872864)
[2025-02-13 20:29:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:05][root][INFO] - Training Epoch: 2/2, step 4056/7134 completed (loss: 0.07431259006261826, acc: 0.9947368502616882)
[2025-02-13 20:29:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:05][root][INFO] - Training Epoch: 2/2, step 4057/7134 completed (loss: 0.1225014328956604, acc: 0.970588207244873)
[2025-02-13 20:29:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:06][root][INFO] - Training Epoch: 2/2, step 4058/7134 completed (loss: 0.09613986313343048, acc: 0.9696969985961914)
[2025-02-13 20:29:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:06][root][INFO] - Training Epoch: 2/2, step 4059/7134 completed (loss: 0.02521650493144989, acc: 1.0)
[2025-02-13 20:29:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:06][root][INFO] - Training Epoch: 2/2, step 4060/7134 completed (loss: 0.05566549301147461, acc: 0.9864864945411682)
[2025-02-13 20:29:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:07][root][INFO] - Training Epoch: 2/2, step 4061/7134 completed (loss: 0.01512376219034195, acc: 1.0)
[2025-02-13 20:29:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:07][root][INFO] - Training Epoch: 2/2, step 4062/7134 completed (loss: 0.0325605683028698, acc: 0.9934210777282715)
[2025-02-13 20:29:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:08][root][INFO] - Training Epoch: 2/2, step 4063/7134 completed (loss: 0.025570735335350037, acc: 1.0)
[2025-02-13 20:29:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:08][root][INFO] - Training Epoch: 2/2, step 4064/7134 completed (loss: 0.02182869426906109, acc: 1.0)
[2025-02-13 20:29:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:08][root][INFO] - Training Epoch: 2/2, step 4065/7134 completed (loss: 0.12137171626091003, acc: 0.9806451797485352)
[2025-02-13 20:29:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:09][root][INFO] - Training Epoch: 2/2, step 4066/7134 completed (loss: 0.017847422510385513, acc: 1.0)
[2025-02-13 20:29:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:09][root][INFO] - Training Epoch: 2/2, step 4067/7134 completed (loss: 0.10030357539653778, acc: 0.965753436088562)
[2025-02-13 20:29:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:10][root][INFO] - Training Epoch: 2/2, step 4068/7134 completed (loss: 0.1676773875951767, acc: 0.9586777091026306)
[2025-02-13 20:29:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:10][root][INFO] - Training Epoch: 2/2, step 4069/7134 completed (loss: 0.05491851270198822, acc: 0.9929577708244324)
[2025-02-13 20:29:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:10][root][INFO] - Training Epoch: 2/2, step 4070/7134 completed (loss: 0.05848956108093262, acc: 0.9933775067329407)
[2025-02-13 20:29:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:11][root][INFO] - Training Epoch: 2/2, step 4071/7134 completed (loss: 0.04306478798389435, acc: 0.9931507110595703)
[2025-02-13 20:29:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:11][root][INFO] - Training Epoch: 2/2, step 4072/7134 completed (loss: 0.06128536909818649, acc: 0.9817073345184326)
[2025-02-13 20:29:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:12][root][INFO] - Training Epoch: 2/2, step 4073/7134 completed (loss: 0.033782847225666046, acc: 0.9939758777618408)
[2025-02-13 20:29:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:12][root][INFO] - Training Epoch: 2/2, step 4074/7134 completed (loss: 0.047350842505693436, acc: 0.9867549538612366)
[2025-02-13 20:29:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:12][root][INFO] - Training Epoch: 2/2, step 4075/7134 completed (loss: 0.03442259877920151, acc: 0.9866666793823242)
[2025-02-13 20:29:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:13][root][INFO] - Training Epoch: 2/2, step 4076/7134 completed (loss: 0.03412703424692154, acc: 0.9878048896789551)
[2025-02-13 20:29:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:13][root][INFO] - Training Epoch: 2/2, step 4077/7134 completed (loss: 0.04803835228085518, acc: 0.9863945841789246)
[2025-02-13 20:29:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:14][root][INFO] - Training Epoch: 2/2, step 4078/7134 completed (loss: 0.04814032465219498, acc: 0.9811320900917053)
[2025-02-13 20:29:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:14][root][INFO] - Training Epoch: 2/2, step 4079/7134 completed (loss: 0.049230799078941345, acc: 0.9770992398262024)
[2025-02-13 20:29:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:14][root][INFO] - Training Epoch: 2/2, step 4080/7134 completed (loss: 0.024941004812717438, acc: 0.9862068891525269)
[2025-02-13 20:29:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:15][root][INFO] - Training Epoch: 2/2, step 4081/7134 completed (loss: 0.023845886811614037, acc: 0.9930555820465088)
[2025-02-13 20:29:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:15][root][INFO] - Training Epoch: 2/2, step 4082/7134 completed (loss: 0.2403765469789505, acc: 0.9473684430122375)
[2025-02-13 20:29:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:15][root][INFO] - Training Epoch: 2/2, step 4083/7134 completed (loss: 0.2685967683792114, acc: 0.9298245906829834)
[2025-02-13 20:29:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:16][root][INFO] - Training Epoch: 2/2, step 4084/7134 completed (loss: 0.15621314942836761, acc: 0.9597315192222595)
[2025-02-13 20:29:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:16][root][INFO] - Training Epoch: 2/2, step 4085/7134 completed (loss: 0.08172387629747391, acc: 0.9789473414421082)
[2025-02-13 20:29:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:16][root][INFO] - Training Epoch: 2/2, step 4086/7134 completed (loss: 0.132711723446846, acc: 0.9551281929016113)
[2025-02-13 20:29:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:17][root][INFO] - Training Epoch: 2/2, step 4087/7134 completed (loss: 0.08187307417392731, acc: 0.9946236610412598)
[2025-02-13 20:29:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:17][root][INFO] - Training Epoch: 2/2, step 4088/7134 completed (loss: 0.2483399510383606, acc: 0.9390863180160522)
[2025-02-13 20:29:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:18][root][INFO] - Training Epoch: 2/2, step 4089/7134 completed (loss: 0.26570823788642883, acc: 0.939393937587738)
[2025-02-13 20:29:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:18][root][INFO] - Training Epoch: 2/2, step 4090/7134 completed (loss: 0.3815852999687195, acc: 0.9138755798339844)
[2025-02-13 20:29:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:18][root][INFO] - Training Epoch: 2/2, step 4091/7134 completed (loss: 0.4452463686466217, acc: 0.898876428604126)
[2025-02-13 20:29:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:19][root][INFO] - Training Epoch: 2/2, step 4092/7134 completed (loss: 0.10207053273916245, acc: 0.9738219976425171)
[2025-02-13 20:29:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:19][root][INFO] - Training Epoch: 2/2, step 4093/7134 completed (loss: 0.08276945352554321, acc: 0.984000027179718)
[2025-02-13 20:29:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:20][root][INFO] - Training Epoch: 2/2, step 4094/7134 completed (loss: 0.1289260983467102, acc: 0.957317054271698)
[2025-02-13 20:29:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:20][root][INFO] - Training Epoch: 2/2, step 4095/7134 completed (loss: 0.10127317905426025, acc: 0.9681528806686401)
[2025-02-13 20:29:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:20][root][INFO] - Training Epoch: 2/2, step 4096/7134 completed (loss: 0.09173806011676788, acc: 0.9870129823684692)
[2025-02-13 20:29:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:21][root][INFO] - Training Epoch: 2/2, step 4097/7134 completed (loss: 0.18712963163852692, acc: 0.9599999785423279)
[2025-02-13 20:29:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:21][root][INFO] - Training Epoch: 2/2, step 4098/7134 completed (loss: 0.08031119406223297, acc: 0.9885057210922241)
[2025-02-13 20:29:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:22][root][INFO] - Training Epoch: 2/2, step 4099/7134 completed (loss: 0.05713196098804474, acc: 0.9848484992980957)
[2025-02-13 20:29:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:22][root][INFO] - Training Epoch: 2/2, step 4100/7134 completed (loss: 0.02231142483651638, acc: 1.0)
[2025-02-13 20:29:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:22][root][INFO] - Training Epoch: 2/2, step 4101/7134 completed (loss: 0.06288044154644012, acc: 0.9760000109672546)
[2025-02-13 20:29:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:23][root][INFO] - Training Epoch: 2/2, step 4102/7134 completed (loss: 0.08122541755437851, acc: 0.9756097793579102)
[2025-02-13 20:29:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:23][root][INFO] - Training Epoch: 2/2, step 4103/7134 completed (loss: 0.032906774431467056, acc: 0.9869281053543091)
[2025-02-13 20:29:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:24][root][INFO] - Training Epoch: 2/2, step 4104/7134 completed (loss: 0.06488795578479767, acc: 0.9855072498321533)
[2025-02-13 20:29:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:24][root][INFO] - Training Epoch: 2/2, step 4105/7134 completed (loss: 0.06186139956116676, acc: 0.9866666793823242)
[2025-02-13 20:29:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:24][root][INFO] - Training Epoch: 2/2, step 4106/7134 completed (loss: 0.10084745287895203, acc: 0.9863945841789246)
[2025-02-13 20:29:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:25][root][INFO] - Training Epoch: 2/2, step 4107/7134 completed (loss: 0.0674150362610817, acc: 0.9779411554336548)
[2025-02-13 20:29:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:25][root][INFO] - Training Epoch: 2/2, step 4108/7134 completed (loss: 0.0576796680688858, acc: 0.9931972622871399)
[2025-02-13 20:29:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:26][root][INFO] - Training Epoch: 2/2, step 4109/7134 completed (loss: 0.0334322452545166, acc: 1.0)
[2025-02-13 20:29:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:26][root][INFO] - Training Epoch: 2/2, step 4110/7134 completed (loss: 0.027418065816164017, acc: 0.9922480583190918)
[2025-02-13 20:29:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:26][root][INFO] - Training Epoch: 2/2, step 4111/7134 completed (loss: 0.048482123762369156, acc: 0.9931034445762634)
[2025-02-13 20:29:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:27][root][INFO] - Training Epoch: 2/2, step 4112/7134 completed (loss: 0.07676941901445389, acc: 0.9767441749572754)
[2025-02-13 20:29:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:27][root][INFO] - Training Epoch: 2/2, step 4113/7134 completed (loss: 0.04526672139763832, acc: 0.9930070042610168)
[2025-02-13 20:29:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:27][root][INFO] - Training Epoch: 2/2, step 4114/7134 completed (loss: 0.1757471263408661, acc: 0.9739130139350891)
[2025-02-13 20:29:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:28][root][INFO] - Training Epoch: 2/2, step 4115/7134 completed (loss: 0.03350389748811722, acc: 1.0)
[2025-02-13 20:29:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:28][root][INFO] - Training Epoch: 2/2, step 4116/7134 completed (loss: 0.018446534872055054, acc: 1.0)
[2025-02-13 20:29:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:29][root][INFO] - Training Epoch: 2/2, step 4117/7134 completed (loss: 0.05233524739742279, acc: 1.0)
[2025-02-13 20:29:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:29][root][INFO] - Training Epoch: 2/2, step 4118/7134 completed (loss: 0.08009111136198044, acc: 0.9895833134651184)
[2025-02-13 20:29:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:29][root][INFO] - Training Epoch: 2/2, step 4119/7134 completed (loss: 0.12384720146656036, acc: 0.9729729890823364)
[2025-02-13 20:29:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:30][root][INFO] - Training Epoch: 2/2, step 4120/7134 completed (loss: 0.11161588877439499, acc: 0.9849624037742615)
[2025-02-13 20:29:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:30][root][INFO] - Training Epoch: 2/2, step 4121/7134 completed (loss: 0.035021670162677765, acc: 0.9939024448394775)
[2025-02-13 20:29:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:30][root][INFO] - Training Epoch: 2/2, step 4122/7134 completed (loss: 0.039600010961294174, acc: 0.9831932783126831)
[2025-02-13 20:29:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:31][root][INFO] - Training Epoch: 2/2, step 4123/7134 completed (loss: 0.025631768628954887, acc: 0.9935064911842346)
[2025-02-13 20:29:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:31][root][INFO] - Training Epoch: 2/2, step 4124/7134 completed (loss: 0.06591123342514038, acc: 0.9750000238418579)
[2025-02-13 20:29:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:32][root][INFO] - Training Epoch: 2/2, step 4125/7134 completed (loss: 0.04591745138168335, acc: 0.9874213933944702)
[2025-02-13 20:29:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:32][root][INFO] - Training Epoch: 2/2, step 4126/7134 completed (loss: 0.07133053988218307, acc: 0.9868420958518982)
[2025-02-13 20:29:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:32][root][INFO] - Training Epoch: 2/2, step 4127/7134 completed (loss: 0.04033777490258217, acc: 0.9932885766029358)
[2025-02-13 20:29:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:33][root][INFO] - Training Epoch: 2/2, step 4128/7134 completed (loss: 0.021669596433639526, acc: 1.0)
[2025-02-13 20:29:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:33][root][INFO] - Training Epoch: 2/2, step 4129/7134 completed (loss: 0.04688878729939461, acc: 0.9932885766029358)
[2025-02-13 20:29:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:33][root][INFO] - Training Epoch: 2/2, step 4130/7134 completed (loss: 0.04383492469787598, acc: 0.9913793206214905)
[2025-02-13 20:29:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:34][root][INFO] - Training Epoch: 2/2, step 4131/7134 completed (loss: 0.06019323319196701, acc: 0.9777777791023254)
[2025-02-13 20:29:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:34][root][INFO] - Training Epoch: 2/2, step 4132/7134 completed (loss: 0.02413676679134369, acc: 1.0)
[2025-02-13 20:29:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:35][root][INFO] - Training Epoch: 2/2, step 4133/7134 completed (loss: 0.06960363686084747, acc: 0.9720279574394226)
[2025-02-13 20:29:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:35][root][INFO] - Training Epoch: 2/2, step 4134/7134 completed (loss: 0.16976293921470642, acc: 0.9545454382896423)
[2025-02-13 20:29:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:35][root][INFO] - Training Epoch: 2/2, step 4135/7134 completed (loss: 0.03827623650431633, acc: 0.9928571581840515)
[2025-02-13 20:29:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:36][root][INFO] - Training Epoch: 2/2, step 4136/7134 completed (loss: 0.05823545902967453, acc: 0.9849624037742615)
[2025-02-13 20:29:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:36][root][INFO] - Training Epoch: 2/2, step 4137/7134 completed (loss: 0.033578988164663315, acc: 0.9926470518112183)
[2025-02-13 20:29:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:37][root][INFO] - Training Epoch: 2/2, step 4138/7134 completed (loss: 0.14346103370189667, acc: 0.9542483687400818)
[2025-02-13 20:29:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:37][root][INFO] - Training Epoch: 2/2, step 4139/7134 completed (loss: 0.08026058226823807, acc: 0.9809523820877075)
[2025-02-13 20:29:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:37][root][INFO] - Training Epoch: 2/2, step 4140/7134 completed (loss: 0.12644939124584198, acc: 0.9604519605636597)
[2025-02-13 20:29:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:38][root][INFO] - Training Epoch: 2/2, step 4141/7134 completed (loss: 0.20098163187503815, acc: 0.9487179517745972)
[2025-02-13 20:29:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:38][root][INFO] - Training Epoch: 2/2, step 4142/7134 completed (loss: 0.1437411904335022, acc: 0.9649999737739563)
[2025-02-13 20:29:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:38][root][INFO] - Training Epoch: 2/2, step 4143/7134 completed (loss: 0.13079360127449036, acc: 0.960698664188385)
[2025-02-13 20:29:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:39][root][INFO] - Training Epoch: 2/2, step 4144/7134 completed (loss: 0.15520820021629333, acc: 0.9692307710647583)
[2025-02-13 20:29:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:39][root][INFO] - Training Epoch: 2/2, step 4145/7134 completed (loss: 0.0974537804722786, acc: 0.9830508232116699)
[2025-02-13 20:29:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:40][root][INFO] - Training Epoch: 2/2, step 4146/7134 completed (loss: 0.27113214135169983, acc: 0.9464285969734192)
[2025-02-13 20:29:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:40][root][INFO] - Training Epoch: 2/2, step 4147/7134 completed (loss: 0.26490071415901184, acc: 0.9406779408454895)
[2025-02-13 20:29:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:40][root][INFO] - Training Epoch: 2/2, step 4148/7134 completed (loss: 0.4501851201057434, acc: 0.8791946172714233)
[2025-02-13 20:29:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:41][root][INFO] - Training Epoch: 2/2, step 4149/7134 completed (loss: 0.2546798884868622, acc: 0.9534883499145508)
[2025-02-13 20:29:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:41][root][INFO] - Training Epoch: 2/2, step 4150/7134 completed (loss: 0.47173234820365906, acc: 0.9059829115867615)
[2025-02-13 20:29:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:42][root][INFO] - Training Epoch: 2/2, step 4151/7134 completed (loss: 0.26513350009918213, acc: 0.9173553586006165)
[2025-02-13 20:29:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:42][root][INFO] - Training Epoch: 2/2, step 4152/7134 completed (loss: 0.27322328090667725, acc: 0.9659863710403442)
[2025-02-13 20:29:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:43][root][INFO] - Training Epoch: 2/2, step 4153/7134 completed (loss: 0.17594510316848755, acc: 0.9714285731315613)
[2025-02-13 20:29:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:43][root][INFO] - Training Epoch: 2/2, step 4154/7134 completed (loss: 0.1471421718597412, acc: 0.9569377899169922)
[2025-02-13 20:29:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:43][root][INFO] - Training Epoch: 2/2, step 4155/7134 completed (loss: 0.18209344148635864, acc: 0.9664429426193237)
[2025-02-13 20:29:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:44][root][INFO] - Training Epoch: 2/2, step 4156/7134 completed (loss: 0.21518047153949738, acc: 0.9532163739204407)
[2025-02-13 20:29:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:44][root][INFO] - Training Epoch: 2/2, step 4157/7134 completed (loss: 0.12084300071001053, acc: 0.9663865566253662)
[2025-02-13 20:29:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:45][root][INFO] - Training Epoch: 2/2, step 4158/7134 completed (loss: 0.13354167342185974, acc: 0.9780219793319702)
[2025-02-13 20:29:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:45][root][INFO] - Training Epoch: 2/2, step 4159/7134 completed (loss: 0.1019383892416954, acc: 0.9924812316894531)
[2025-02-13 20:29:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:46][root][INFO] - Training Epoch: 2/2, step 4160/7134 completed (loss: 0.07029596716165543, acc: 0.9915966391563416)
[2025-02-13 20:29:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:46][root][INFO] - Training Epoch: 2/2, step 4161/7134 completed (loss: 0.07080438733100891, acc: 0.982758641242981)
[2025-02-13 20:29:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:46][root][INFO] - Training Epoch: 2/2, step 4162/7134 completed (loss: 0.07334382086992264, acc: 0.9880239367485046)
[2025-02-13 20:29:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:47][root][INFO] - Training Epoch: 2/2, step 4163/7134 completed (loss: 0.12936821579933167, acc: 0.9865771532058716)
[2025-02-13 20:29:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:47][root][INFO] - Training Epoch: 2/2, step 4164/7134 completed (loss: 0.093012735247612, acc: 0.9813664555549622)
[2025-02-13 20:29:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:48][root][INFO] - Training Epoch: 2/2, step 4165/7134 completed (loss: 0.12289117276668549, acc: 0.9784946441650391)
[2025-02-13 20:29:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:48][root][INFO] - Training Epoch: 2/2, step 4166/7134 completed (loss: 0.16136600077152252, acc: 0.9615384340286255)
[2025-02-13 20:29:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:49][root][INFO] - Training Epoch: 2/2, step 4167/7134 completed (loss: 0.08205199241638184, acc: 0.9756097793579102)
[2025-02-13 20:29:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:49][root][INFO] - Training Epoch: 2/2, step 4168/7134 completed (loss: 0.06557246297597885, acc: 0.9774436354637146)
[2025-02-13 20:29:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:49][root][INFO] - Training Epoch: 2/2, step 4169/7134 completed (loss: 0.11726510524749756, acc: 0.9769585132598877)
[2025-02-13 20:29:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:50][root][INFO] - Training Epoch: 2/2, step 4170/7134 completed (loss: 0.058188892900943756, acc: 0.9809523820877075)
[2025-02-13 20:29:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:50][root][INFO] - Training Epoch: 2/2, step 4171/7134 completed (loss: 0.05425352603197098, acc: 0.9852216839790344)
[2025-02-13 20:29:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:51][root][INFO] - Training Epoch: 2/2, step 4172/7134 completed (loss: 0.10199850052595139, acc: 0.9777777791023254)
[2025-02-13 20:29:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:51][root][INFO] - Training Epoch: 2/2, step 4173/7134 completed (loss: 0.08971511572599411, acc: 0.9722222089767456)
[2025-02-13 20:29:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:51][root][INFO] - Training Epoch: 2/2, step 4174/7134 completed (loss: 0.030934207141399384, acc: 0.9841269850730896)
[2025-02-13 20:29:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:52][root][INFO] - Training Epoch: 2/2, step 4175/7134 completed (loss: 0.05910814180970192, acc: 0.98591548204422)
[2025-02-13 20:29:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:52][root][INFO] - Training Epoch: 2/2, step 4176/7134 completed (loss: 0.062439221888780594, acc: 0.9870967864990234)
[2025-02-13 20:29:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:52][root][INFO] - Training Epoch: 2/2, step 4177/7134 completed (loss: 0.03452099859714508, acc: 0.988304078578949)
[2025-02-13 20:29:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:53][root][INFO] - Training Epoch: 2/2, step 4178/7134 completed (loss: 0.09961412101984024, acc: 0.9802631735801697)
[2025-02-13 20:29:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:53][root][INFO] - Training Epoch: 2/2, step 4179/7134 completed (loss: 0.02355336584150791, acc: 1.0)
[2025-02-13 20:29:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:54][root][INFO] - Training Epoch: 2/2, step 4180/7134 completed (loss: 0.04787738621234894, acc: 0.987500011920929)
[2025-02-13 20:29:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:54][root][INFO] - Training Epoch: 2/2, step 4181/7134 completed (loss: 0.011675477027893066, acc: 1.0)
[2025-02-13 20:29:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:54][root][INFO] - Training Epoch: 2/2, step 4182/7134 completed (loss: 0.025265755131840706, acc: 0.9935064911842346)
[2025-02-13 20:29:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:55][root][INFO] - Training Epoch: 2/2, step 4183/7134 completed (loss: 0.08307687193155289, acc: 0.9693251252174377)
[2025-02-13 20:29:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:55][root][INFO] - Training Epoch: 2/2, step 4184/7134 completed (loss: 0.022944537922739983, acc: 1.0)
[2025-02-13 20:29:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:56][root][INFO] - Training Epoch: 2/2, step 4185/7134 completed (loss: 0.014375029131770134, acc: 1.0)
[2025-02-13 20:29:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:56][root][INFO] - Training Epoch: 2/2, step 4186/7134 completed (loss: 0.14074446260929108, acc: 0.9742268323898315)
[2025-02-13 20:29:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:56][root][INFO] - Training Epoch: 2/2, step 4187/7134 completed (loss: 0.023703236132860184, acc: 1.0)
[2025-02-13 20:29:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:57][root][INFO] - Training Epoch: 2/2, step 4188/7134 completed (loss: 0.07171837985515594, acc: 0.9842105507850647)
[2025-02-13 20:29:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:57][root][INFO] - Training Epoch: 2/2, step 4189/7134 completed (loss: 0.038269221782684326, acc: 0.9940119981765747)
[2025-02-13 20:29:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:58][root][INFO] - Training Epoch: 2/2, step 4190/7134 completed (loss: 0.04714656248688698, acc: 0.9824561476707458)
[2025-02-13 20:29:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:58][root][INFO] - Training Epoch: 2/2, step 4191/7134 completed (loss: 0.08856289088726044, acc: 0.9903846383094788)
[2025-02-13 20:29:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:58][root][INFO] - Training Epoch: 2/2, step 4192/7134 completed (loss: 0.03387540951371193, acc: 0.9863013625144958)
[2025-02-13 20:29:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:59][root][INFO] - Training Epoch: 2/2, step 4193/7134 completed (loss: 0.07385142892599106, acc: 0.9888268113136292)
[2025-02-13 20:29:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:29:59][root][INFO] - Training Epoch: 2/2, step 4194/7134 completed (loss: 0.03703298792243004, acc: 0.9935897588729858)
[2025-02-13 20:29:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:00][root][INFO] - Training Epoch: 2/2, step 4195/7134 completed (loss: 0.15421615540981293, acc: 0.9714285731315613)
[2025-02-13 20:30:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:00][root][INFO] - Training Epoch: 2/2, step 4196/7134 completed (loss: 0.029300374910235405, acc: 0.9856114983558655)
[2025-02-13 20:30:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:00][root][INFO] - Training Epoch: 2/2, step 4197/7134 completed (loss: 0.04481087997555733, acc: 0.9945651888847351)
[2025-02-13 20:30:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:01][root][INFO] - Training Epoch: 2/2, step 4198/7134 completed (loss: 0.06999177485704422, acc: 0.9942857027053833)
[2025-02-13 20:30:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:01][root][INFO] - Training Epoch: 2/2, step 4199/7134 completed (loss: 0.0908912718296051, acc: 0.9718309640884399)
[2025-02-13 20:30:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:01][root][INFO] - Training Epoch: 2/2, step 4200/7134 completed (loss: 0.049116164445877075, acc: 0.9862068891525269)
[2025-02-13 20:30:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:02][root][INFO] - Training Epoch: 2/2, step 4201/7134 completed (loss: 0.06389013677835464, acc: 0.9858155846595764)
[2025-02-13 20:30:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:02][root][INFO] - Training Epoch: 2/2, step 4202/7134 completed (loss: 0.0835299864411354, acc: 0.9849624037742615)
[2025-02-13 20:30:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:03][root][INFO] - Training Epoch: 2/2, step 4203/7134 completed (loss: 0.05149523913860321, acc: 0.9873417615890503)
[2025-02-13 20:30:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:03][root][INFO] - Training Epoch: 2/2, step 4204/7134 completed (loss: 0.11902385205030441, acc: 0.9657142758369446)
[2025-02-13 20:30:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:04][root][INFO] - Training Epoch: 2/2, step 4205/7134 completed (loss: 0.10845288634300232, acc: 0.9647058844566345)
[2025-02-13 20:30:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:04][root][INFO] - Training Epoch: 2/2, step 4206/7134 completed (loss: 0.10937292873859406, acc: 0.9946236610412598)
[2025-02-13 20:30:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:04][root][INFO] - Training Epoch: 2/2, step 4207/7134 completed (loss: 0.14750991761684418, acc: 0.9748427867889404)
[2025-02-13 20:30:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:05][root][INFO] - Training Epoch: 2/2, step 4208/7134 completed (loss: 0.15659531950950623, acc: 0.9885057210922241)
[2025-02-13 20:30:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:05][root][INFO] - Training Epoch: 2/2, step 4209/7134 completed (loss: 0.08289899677038193, acc: 0.9834254384040833)
[2025-02-13 20:30:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:06][root][INFO] - Training Epoch: 2/2, step 4210/7134 completed (loss: 0.02562698721885681, acc: 1.0)
[2025-02-13 20:30:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:06][root][INFO] - Training Epoch: 2/2, step 4211/7134 completed (loss: 0.020901478826999664, acc: 1.0)
[2025-02-13 20:30:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:06][root][INFO] - Training Epoch: 2/2, step 4212/7134 completed (loss: 0.05270172283053398, acc: 0.9741935729980469)
[2025-02-13 20:30:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:07][root][INFO] - Training Epoch: 2/2, step 4213/7134 completed (loss: 0.026543499901890755, acc: 0.9942528605461121)
[2025-02-13 20:30:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:07][root][INFO] - Training Epoch: 2/2, step 4214/7134 completed (loss: 0.14750593900680542, acc: 0.9766082167625427)
[2025-02-13 20:30:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:08][root][INFO] - Training Epoch: 2/2, step 4215/7134 completed (loss: 0.09539125114679337, acc: 0.9842519760131836)
[2025-02-13 20:30:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:08][root][INFO] - Training Epoch: 2/2, step 4216/7134 completed (loss: 0.12311699241399765, acc: 0.9846153855323792)
[2025-02-13 20:30:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:08][root][INFO] - Training Epoch: 2/2, step 4217/7134 completed (loss: 0.033938441425561905, acc: 0.9888888597488403)
[2025-02-13 20:30:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:09][root][INFO] - Training Epoch: 2/2, step 4218/7134 completed (loss: 0.16994376480579376, acc: 0.9776119589805603)
[2025-02-13 20:30:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:09][root][INFO] - Training Epoch: 2/2, step 4219/7134 completed (loss: 0.02042398229241371, acc: 1.0)
[2025-02-13 20:30:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:10][root][INFO] - Training Epoch: 2/2, step 4220/7134 completed (loss: 0.21540561318397522, acc: 0.9715909361839294)
[2025-02-13 20:30:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:10][root][INFO] - Training Epoch: 2/2, step 4221/7134 completed (loss: 0.10942139476537704, acc: 0.9594594836235046)
[2025-02-13 20:30:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:10][root][INFO] - Training Epoch: 2/2, step 4222/7134 completed (loss: 0.2517853379249573, acc: 0.9561403393745422)
[2025-02-13 20:30:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:11][root][INFO] - Training Epoch: 2/2, step 4223/7134 completed (loss: 0.274789422750473, acc: 0.929648220539093)
[2025-02-13 20:30:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:11][root][INFO] - Training Epoch: 2/2, step 4224/7134 completed (loss: 0.19578908383846283, acc: 0.9516128897666931)
[2025-02-13 20:30:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:12][root][INFO] - Training Epoch: 2/2, step 4225/7134 completed (loss: 0.18400034308433533, acc: 0.9551281929016113)
[2025-02-13 20:30:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:12][root][INFO] - Training Epoch: 2/2, step 4226/7134 completed (loss: 0.19768285751342773, acc: 0.9390863180160522)
[2025-02-13 20:30:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:12][root][INFO] - Training Epoch: 2/2, step 4227/7134 completed (loss: 0.3931468725204468, acc: 0.8962264060974121)
[2025-02-13 20:30:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:13][root][INFO] - Training Epoch: 2/2, step 4228/7134 completed (loss: 0.2814795672893524, acc: 0.9294871687889099)
[2025-02-13 20:30:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:13][root][INFO] - Training Epoch: 2/2, step 4229/7134 completed (loss: 0.23137350380420685, acc: 0.9440000057220459)
[2025-02-13 20:30:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:14][root][INFO] - Training Epoch: 2/2, step 4230/7134 completed (loss: 0.2802620530128479, acc: 0.9186046719551086)
[2025-02-13 20:30:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:14][root][INFO] - Training Epoch: 2/2, step 4231/7134 completed (loss: 0.4404800236225128, acc: 0.9328858852386475)
[2025-02-13 20:30:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:14][root][INFO] - Training Epoch: 2/2, step 4232/7134 completed (loss: 0.20767860114574432, acc: 0.9277777671813965)
[2025-02-13 20:30:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:15][root][INFO] - Training Epoch: 2/2, step 4233/7134 completed (loss: 0.18990489840507507, acc: 0.957446813583374)
[2025-02-13 20:30:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:15][root][INFO] - Training Epoch: 2/2, step 4234/7134 completed (loss: 0.08396319299936295, acc: 0.978723406791687)
[2025-02-13 20:30:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:16][root][INFO] - Training Epoch: 2/2, step 4235/7134 completed (loss: 0.08758150786161423, acc: 0.9710144996643066)
[2025-02-13 20:30:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:16][root][INFO] - Training Epoch: 2/2, step 4236/7134 completed (loss: 0.03220776468515396, acc: 1.0)
[2025-02-13 20:30:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:17][root][INFO] - Training Epoch: 2/2, step 4237/7134 completed (loss: 0.05974390730261803, acc: 0.9906542301177979)
[2025-02-13 20:30:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:17][root][INFO] - Training Epoch: 2/2, step 4238/7134 completed (loss: 0.1278855800628662, acc: 0.9769230484962463)
[2025-02-13 20:30:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:17][root][INFO] - Training Epoch: 2/2, step 4239/7134 completed (loss: 0.050363488495349884, acc: 1.0)
[2025-02-13 20:30:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:18][root][INFO] - Training Epoch: 2/2, step 4240/7134 completed (loss: 0.16634054481983185, acc: 0.9577465057373047)
[2025-02-13 20:30:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:18][root][INFO] - Training Epoch: 2/2, step 4241/7134 completed (loss: 0.02783435583114624, acc: 1.0)
[2025-02-13 20:30:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:19][root][INFO] - Training Epoch: 2/2, step 4242/7134 completed (loss: 0.14280366897583008, acc: 0.9489796161651611)
[2025-02-13 20:30:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:19][root][INFO] - Training Epoch: 2/2, step 4243/7134 completed (loss: 0.1176811009645462, acc: 0.9669421315193176)
[2025-02-13 20:30:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:19][root][INFO] - Training Epoch: 2/2, step 4244/7134 completed (loss: 0.04057988524436951, acc: 1.0)
[2025-02-13 20:30:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:20][root][INFO] - Training Epoch: 2/2, step 4245/7134 completed (loss: 0.050033602863550186, acc: 1.0)
[2025-02-13 20:30:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:20][root][INFO] - Training Epoch: 2/2, step 4246/7134 completed (loss: 0.10510746389627457, acc: 0.9794520735740662)
[2025-02-13 20:30:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:20][root][INFO] - Training Epoch: 2/2, step 4247/7134 completed (loss: 0.0497833751142025, acc: 0.9919999837875366)
[2025-02-13 20:30:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:21][root][INFO] - Training Epoch: 2/2, step 4248/7134 completed (loss: 0.16560561954975128, acc: 0.9794520735740662)
[2025-02-13 20:30:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:21][root][INFO] - Training Epoch: 2/2, step 4249/7134 completed (loss: 0.1712636798620224, acc: 0.9632353186607361)
[2025-02-13 20:30:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:22][root][INFO] - Training Epoch: 2/2, step 4250/7134 completed (loss: 0.31152766942977905, acc: 0.9636363387107849)
[2025-02-13 20:30:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:22][root][INFO] - Training Epoch: 2/2, step 4251/7134 completed (loss: 0.06896678358316422, acc: 0.9774436354637146)
[2025-02-13 20:30:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:23][root][INFO] - Training Epoch: 2/2, step 4252/7134 completed (loss: 0.14104603230953217, acc: 0.9677419066429138)
[2025-02-13 20:30:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:23][root][INFO] - Training Epoch: 2/2, step 4253/7134 completed (loss: 0.0763649120926857, acc: 0.9826086759567261)
[2025-02-13 20:30:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:23][root][INFO] - Training Epoch: 2/2, step 4254/7134 completed (loss: 0.04227776825428009, acc: 0.9886363744735718)
[2025-02-13 20:30:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:24][root][INFO] - Training Epoch: 2/2, step 4255/7134 completed (loss: 0.1150350570678711, acc: 0.9615384340286255)
[2025-02-13 20:30:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:24][root][INFO] - Training Epoch: 2/2, step 4256/7134 completed (loss: 0.07179316133260727, acc: 0.9743589758872986)
[2025-02-13 20:30:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:24][root][INFO] - Training Epoch: 2/2, step 4257/7134 completed (loss: 0.05161565542221069, acc: 0.9924242496490479)
[2025-02-13 20:30:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:25][root][INFO] - Training Epoch: 2/2, step 4258/7134 completed (loss: 0.029352732002735138, acc: 1.0)
[2025-02-13 20:30:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:25][root][INFO] - Training Epoch: 2/2, step 4259/7134 completed (loss: 0.19337841868400574, acc: 0.9624999761581421)
[2025-02-13 20:30:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:26][root][INFO] - Training Epoch: 2/2, step 4260/7134 completed (loss: 0.055500950664281845, acc: 0.9908257126808167)
[2025-02-13 20:30:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:26][root][INFO] - Training Epoch: 2/2, step 4261/7134 completed (loss: 0.06086565554141998, acc: 0.991304337978363)
[2025-02-13 20:30:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:26][root][INFO] - Training Epoch: 2/2, step 4262/7134 completed (loss: 0.1008911058306694, acc: 0.9790209531784058)
[2025-02-13 20:30:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:27][root][INFO] - Training Epoch: 2/2, step 4263/7134 completed (loss: 0.10028520226478577, acc: 0.9925925731658936)
[2025-02-13 20:30:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:27][root][INFO] - Training Epoch: 2/2, step 4264/7134 completed (loss: 0.14670614898204803, acc: 0.966292142868042)
[2025-02-13 20:30:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:28][root][INFO] - Training Epoch: 2/2, step 4265/7134 completed (loss: 0.12642888724803925, acc: 0.9651162624359131)
[2025-02-13 20:30:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:28][root][INFO] - Training Epoch: 2/2, step 4266/7134 completed (loss: 0.08678711950778961, acc: 0.9842932224273682)
[2025-02-13 20:30:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:28][root][INFO] - Training Epoch: 2/2, step 4267/7134 completed (loss: 0.05939820408821106, acc: 0.9848484992980957)
[2025-02-13 20:30:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:29][root][INFO] - Training Epoch: 2/2, step 4268/7134 completed (loss: 0.10689589381217957, acc: 0.9837398529052734)
[2025-02-13 20:30:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:29][root][INFO] - Training Epoch: 2/2, step 4269/7134 completed (loss: 0.199943408370018, acc: 0.9421965479850769)
[2025-02-13 20:30:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:30][root][INFO] - Training Epoch: 2/2, step 4270/7134 completed (loss: 0.09288736432790756, acc: 0.984375)
[2025-02-13 20:30:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:30][root][INFO] - Training Epoch: 2/2, step 4271/7134 completed (loss: 0.061192650347948074, acc: 0.9945945739746094)
[2025-02-13 20:30:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:30][root][INFO] - Training Epoch: 2/2, step 4272/7134 completed (loss: 0.07043277472257614, acc: 0.9850746393203735)
[2025-02-13 20:30:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:31][root][INFO] - Training Epoch: 2/2, step 4273/7134 completed (loss: 0.05134475976228714, acc: 0.9894179701805115)
[2025-02-13 20:30:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:31][root][INFO] - Training Epoch: 2/2, step 4274/7134 completed (loss: 0.1281132847070694, acc: 0.9754902124404907)
[2025-02-13 20:30:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:32][root][INFO] - Training Epoch: 2/2, step 4275/7134 completed (loss: 0.053734298795461655, acc: 0.9888268113136292)
[2025-02-13 20:30:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:32][root][INFO] - Training Epoch: 2/2, step 4276/7134 completed (loss: 0.11829297989606857, acc: 0.9891892075538635)
[2025-02-13 20:30:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:33][root][INFO] - Training Epoch: 2/2, step 4277/7134 completed (loss: 0.02791311778128147, acc: 1.0)
[2025-02-13 20:30:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:33][root][INFO] - Training Epoch: 2/2, step 4278/7134 completed (loss: 0.05608892813324928, acc: 0.9846938848495483)
[2025-02-13 20:30:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:34][root][INFO] - Training Epoch: 2/2, step 4279/7134 completed (loss: 0.061069246381521225, acc: 0.9813664555549622)
[2025-02-13 20:30:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:34][root][INFO] - Training Epoch: 2/2, step 4280/7134 completed (loss: 0.014814876951277256, acc: 1.0)
[2025-02-13 20:30:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:34][root][INFO] - Training Epoch: 2/2, step 4281/7134 completed (loss: 0.10176923125982285, acc: 0.9892473220825195)
[2025-02-13 20:30:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:35][root][INFO] - Training Epoch: 2/2, step 4282/7134 completed (loss: 0.03567417338490486, acc: 0.9913793206214905)
[2025-02-13 20:30:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:35][root][INFO] - Training Epoch: 2/2, step 4283/7134 completed (loss: 0.04822392016649246, acc: 0.9846938848495483)
[2025-02-13 20:30:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:36][root][INFO] - Training Epoch: 2/2, step 4284/7134 completed (loss: 0.13193164765834808, acc: 0.9767441749572754)
[2025-02-13 20:30:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:36][root][INFO] - Training Epoch: 2/2, step 4285/7134 completed (loss: 0.02268655225634575, acc: 1.0)
[2025-02-13 20:30:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:36][root][INFO] - Training Epoch: 2/2, step 4286/7134 completed (loss: 0.045813627541065216, acc: 0.9915966391563416)
[2025-02-13 20:30:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:37][root][INFO] - Training Epoch: 2/2, step 4287/7134 completed (loss: 0.07402696460485458, acc: 0.989130437374115)
[2025-02-13 20:30:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:37][root][INFO] - Training Epoch: 2/2, step 4288/7134 completed (loss: 0.06590651720762253, acc: 0.9882352948188782)
[2025-02-13 20:30:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:37][root][INFO] - Training Epoch: 2/2, step 4289/7134 completed (loss: 0.10880760103464127, acc: 0.9815950989723206)
[2025-02-13 20:30:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:38][root][INFO] - Training Epoch: 2/2, step 4290/7134 completed (loss: 0.05768059194087982, acc: 0.9884393215179443)
[2025-02-13 20:30:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:38][root][INFO] - Training Epoch: 2/2, step 4291/7134 completed (loss: 0.036597687751054764, acc: 0.9942196607589722)
[2025-02-13 20:30:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:39][root][INFO] - Training Epoch: 2/2, step 4292/7134 completed (loss: 0.06848527491092682, acc: 0.9806451797485352)
[2025-02-13 20:30:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:39][root][INFO] - Training Epoch: 2/2, step 4293/7134 completed (loss: 0.08797735720872879, acc: 0.9833333492279053)
[2025-02-13 20:30:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:39][root][INFO] - Training Epoch: 2/2, step 4294/7134 completed (loss: 0.15028542280197144, acc: 0.9714285731315613)
[2025-02-13 20:30:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:40][root][INFO] - Training Epoch: 2/2, step 4295/7134 completed (loss: 0.14862902462482452, acc: 0.9714285731315613)
[2025-02-13 20:30:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:40][root][INFO] - Training Epoch: 2/2, step 4296/7134 completed (loss: 0.045852649956941605, acc: 0.9900990128517151)
[2025-02-13 20:30:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:41][root][INFO] - Training Epoch: 2/2, step 4297/7134 completed (loss: 0.016588877886533737, acc: 1.0)
[2025-02-13 20:30:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:41][root][INFO] - Training Epoch: 2/2, step 4298/7134 completed (loss: 0.07346148788928986, acc: 0.9772727489471436)
[2025-02-13 20:30:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:41][root][INFO] - Training Epoch: 2/2, step 4299/7134 completed (loss: 0.13630801439285278, acc: 0.9516128897666931)
[2025-02-13 20:30:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:42][root][INFO] - Training Epoch: 2/2, step 4300/7134 completed (loss: 0.18129093945026398, acc: 0.9663865566253662)
[2025-02-13 20:30:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:42][root][INFO] - Training Epoch: 2/2, step 4301/7134 completed (loss: 0.031018750742077827, acc: 1.0)
[2025-02-13 20:30:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:43][root][INFO] - Training Epoch: 2/2, step 4302/7134 completed (loss: 0.0582856759428978, acc: 0.9808917045593262)
[2025-02-13 20:30:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:43][root][INFO] - Training Epoch: 2/2, step 4303/7134 completed (loss: 0.08540470898151398, acc: 0.9702970385551453)
[2025-02-13 20:30:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:44][root][INFO] - Training Epoch: 2/2, step 4304/7134 completed (loss: 0.061403822153806686, acc: 0.9881656765937805)
[2025-02-13 20:30:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:44][root][INFO] - Training Epoch: 2/2, step 4305/7134 completed (loss: 0.07612050324678421, acc: 0.9850000143051147)
[2025-02-13 20:30:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:44][root][INFO] - Training Epoch: 2/2, step 4306/7134 completed (loss: 0.1562010645866394, acc: 0.9790576100349426)
[2025-02-13 20:30:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:45][root][INFO] - Training Epoch: 2/2, step 4307/7134 completed (loss: 0.09337777644395828, acc: 0.9738562107086182)
[2025-02-13 20:30:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:45][root][INFO] - Training Epoch: 2/2, step 4308/7134 completed (loss: 0.05990539863705635, acc: 0.9839572310447693)
[2025-02-13 20:30:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:46][root][INFO] - Training Epoch: 2/2, step 4309/7134 completed (loss: 0.08943551033735275, acc: 0.9772727489471436)
[2025-02-13 20:30:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:46][root][INFO] - Training Epoch: 2/2, step 4310/7134 completed (loss: 0.0696544423699379, acc: 0.9796954393386841)
[2025-02-13 20:30:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:47][root][INFO] - Training Epoch: 2/2, step 4311/7134 completed (loss: 0.06243623048067093, acc: 0.9858155846595764)
[2025-02-13 20:30:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:47][root][INFO] - Training Epoch: 2/2, step 4312/7134 completed (loss: 0.06307347118854523, acc: 0.987730085849762)
[2025-02-13 20:30:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:47][root][INFO] - Training Epoch: 2/2, step 4313/7134 completed (loss: 0.03188110515475273, acc: 1.0)
[2025-02-13 20:30:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:48][root][INFO] - Training Epoch: 2/2, step 4314/7134 completed (loss: 0.04726492241024971, acc: 0.9869281053543091)
[2025-02-13 20:30:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:48][root][INFO] - Training Epoch: 2/2, step 4315/7134 completed (loss: 0.06132727116346359, acc: 0.976190447807312)
[2025-02-13 20:30:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:49][root][INFO] - Training Epoch: 2/2, step 4316/7134 completed (loss: 0.056110747158527374, acc: 0.9940119981765747)
[2025-02-13 20:30:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:49][root][INFO] - Training Epoch: 2/2, step 4317/7134 completed (loss: 0.07196731120347977, acc: 0.9767441749572754)
[2025-02-13 20:30:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:49][root][INFO] - Training Epoch: 2/2, step 4318/7134 completed (loss: 0.11650348454713821, acc: 0.9744898080825806)
[2025-02-13 20:30:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:50][root][INFO] - Training Epoch: 2/2, step 4319/7134 completed (loss: 0.060436684638261795, acc: 0.9753694534301758)
[2025-02-13 20:30:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:50][root][INFO] - Training Epoch: 2/2, step 4320/7134 completed (loss: 0.04410500079393387, acc: 0.9935897588729858)
[2025-02-13 20:30:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:51][root][INFO] - Training Epoch: 2/2, step 4321/7134 completed (loss: 0.16685868799686432, acc: 0.9801980257034302)
[2025-02-13 20:30:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:51][root][INFO] - Training Epoch: 2/2, step 4322/7134 completed (loss: 0.1406463384628296, acc: 0.9685534834861755)
[2025-02-13 20:30:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:52][root][INFO] - Training Epoch: 2/2, step 4323/7134 completed (loss: 0.04389064759016037, acc: 0.9924812316894531)
[2025-02-13 20:30:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:52][root][INFO] - Training Epoch: 2/2, step 4324/7134 completed (loss: 0.03793829306960106, acc: 1.0)
[2025-02-13 20:30:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:52][root][INFO] - Training Epoch: 2/2, step 4325/7134 completed (loss: 0.12121046334505081, acc: 0.9726027250289917)
[2025-02-13 20:30:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:53][root][INFO] - Training Epoch: 2/2, step 4326/7134 completed (loss: 0.1452043056488037, acc: 0.9772727489471436)
[2025-02-13 20:30:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:53][root][INFO] - Training Epoch: 2/2, step 4327/7134 completed (loss: 0.051176998764276505, acc: 0.9781022071838379)
[2025-02-13 20:30:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:54][root][INFO] - Training Epoch: 2/2, step 4328/7134 completed (loss: 0.03510456159710884, acc: 0.9921875)
[2025-02-13 20:30:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:54][root][INFO] - Training Epoch: 2/2, step 4329/7134 completed (loss: 0.07106209546327591, acc: 0.988304078578949)
[2025-02-13 20:30:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:54][root][INFO] - Training Epoch: 2/2, step 4330/7134 completed (loss: 0.054298557341098785, acc: 0.9909909963607788)
[2025-02-13 20:30:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:55][root][INFO] - Training Epoch: 2/2, step 4331/7134 completed (loss: 0.0456397645175457, acc: 0.9876543283462524)
[2025-02-13 20:30:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:55][root][INFO] - Training Epoch: 2/2, step 4332/7134 completed (loss: 0.10013751685619354, acc: 0.9841269850730896)
[2025-02-13 20:30:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:56][root][INFO] - Training Epoch: 2/2, step 4333/7134 completed (loss: 0.049678631126880646, acc: 0.981249988079071)
[2025-02-13 20:30:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:56][root][INFO] - Training Epoch: 2/2, step 4334/7134 completed (loss: 0.09780959784984589, acc: 0.9815950989723206)
[2025-02-13 20:30:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:56][root][INFO] - Training Epoch: 2/2, step 4335/7134 completed (loss: 0.10481481999158859, acc: 0.9752066135406494)
[2025-02-13 20:30:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:57][root][INFO] - Training Epoch: 2/2, step 4336/7134 completed (loss: 0.03508817031979561, acc: 0.9928057789802551)
[2025-02-13 20:30:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:57][root][INFO] - Training Epoch: 2/2, step 4337/7134 completed (loss: 0.04376121982932091, acc: 0.9790209531784058)
[2025-02-13 20:30:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:58][root][INFO] - Training Epoch: 2/2, step 4338/7134 completed (loss: 0.044419050216674805, acc: 0.9939393997192383)
[2025-02-13 20:30:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:58][root][INFO] - Training Epoch: 2/2, step 4339/7134 completed (loss: 0.04446878284215927, acc: 0.9882352948188782)
[2025-02-13 20:30:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:58][root][INFO] - Training Epoch: 2/2, step 4340/7134 completed (loss: 0.05043475702404976, acc: 0.9931034445762634)
[2025-02-13 20:30:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:59][root][INFO] - Training Epoch: 2/2, step 4341/7134 completed (loss: 0.08098132908344269, acc: 0.9795918464660645)
[2025-02-13 20:30:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:30:59][root][INFO] - Training Epoch: 2/2, step 4342/7134 completed (loss: 0.09532807767391205, acc: 0.9801324605941772)
[2025-02-13 20:30:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:00][root][INFO] - Training Epoch: 2/2, step 4343/7134 completed (loss: 0.07167116552591324, acc: 0.970370352268219)
[2025-02-13 20:31:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:00][root][INFO] - Training Epoch: 2/2, step 4344/7134 completed (loss: 0.054395489394664764, acc: 0.991304337978363)
[2025-02-13 20:31:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:00][root][INFO] - Training Epoch: 2/2, step 4345/7134 completed (loss: 0.08656741678714752, acc: 0.9729729890823364)
[2025-02-13 20:31:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:01][root][INFO] - Training Epoch: 2/2, step 4346/7134 completed (loss: 0.0317743718624115, acc: 0.9819819927215576)
[2025-02-13 20:31:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:01][root][INFO] - Training Epoch: 2/2, step 4347/7134 completed (loss: 0.10903121531009674, acc: 0.9704142212867737)
[2025-02-13 20:31:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:01][root][INFO] - Training Epoch: 2/2, step 4348/7134 completed (loss: 0.03264891356229782, acc: 0.9931972622871399)
[2025-02-13 20:31:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:02][root][INFO] - Training Epoch: 2/2, step 4349/7134 completed (loss: 0.13886041939258575, acc: 0.96875)
[2025-02-13 20:31:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:02][root][INFO] - Training Epoch: 2/2, step 4350/7134 completed (loss: 0.1075747087597847, acc: 0.9775280952453613)
[2025-02-13 20:31:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:03][root][INFO] - Training Epoch: 2/2, step 4351/7134 completed (loss: 0.09045722335577011, acc: 0.9679487347602844)
[2025-02-13 20:31:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:03][root][INFO] - Training Epoch: 2/2, step 4352/7134 completed (loss: 0.09390091150999069, acc: 0.9751552939414978)
[2025-02-13 20:31:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:04][root][INFO] - Training Epoch: 2/2, step 4353/7134 completed (loss: 0.06275825947523117, acc: 0.9940119981765747)
[2025-02-13 20:31:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:04][root][INFO] - Training Epoch: 2/2, step 4354/7134 completed (loss: 0.09880810230970383, acc: 0.9754601120948792)
[2025-02-13 20:31:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:04][root][INFO] - Training Epoch: 2/2, step 4355/7134 completed (loss: 0.08648015558719635, acc: 0.9751552939414978)
[2025-02-13 20:31:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:05][root][INFO] - Training Epoch: 2/2, step 4356/7134 completed (loss: 0.04443264380097389, acc: 1.0)
[2025-02-13 20:31:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:05][root][INFO] - Training Epoch: 2/2, step 4357/7134 completed (loss: 0.2558249533176422, acc: 0.9395973086357117)
[2025-02-13 20:31:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:05][root][INFO] - Training Epoch: 2/2, step 4358/7134 completed (loss: 0.189630389213562, acc: 0.9512194991111755)
[2025-02-13 20:31:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:06][root][INFO] - Training Epoch: 2/2, step 4359/7134 completed (loss: 0.07049808651208878, acc: 0.976190447807312)
[2025-02-13 20:31:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:06][root][INFO] - Training Epoch: 2/2, step 4360/7134 completed (loss: 0.06343311816453934, acc: 0.993630588054657)
[2025-02-13 20:31:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:07][root][INFO] - Training Epoch: 2/2, step 4361/7134 completed (loss: 0.11031114310026169, acc: 0.9602649211883545)
[2025-02-13 20:31:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:07][root][INFO] - Training Epoch: 2/2, step 4362/7134 completed (loss: 0.08620865643024445, acc: 0.9836956262588501)
[2025-02-13 20:31:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:08][root][INFO] - Training Epoch: 2/2, step 4363/7134 completed (loss: 0.06964538991451263, acc: 0.9924242496490479)
[2025-02-13 20:31:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:08][root][INFO] - Training Epoch: 2/2, step 4364/7134 completed (loss: 0.05073053389787674, acc: 0.9922480583190918)
[2025-02-13 20:31:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:08][root][INFO] - Training Epoch: 2/2, step 4365/7134 completed (loss: 0.07083352655172348, acc: 0.9856114983558655)
[2025-02-13 20:31:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:09][root][INFO] - Training Epoch: 2/2, step 4366/7134 completed (loss: 0.09588396549224854, acc: 0.9651162624359131)
[2025-02-13 20:31:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:09][root][INFO] - Training Epoch: 2/2, step 4367/7134 completed (loss: 0.09946053475141525, acc: 0.9731543660163879)
[2025-02-13 20:31:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:09][root][INFO] - Training Epoch: 2/2, step 4368/7134 completed (loss: 0.06356250494718552, acc: 0.9880239367485046)
[2025-02-13 20:31:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:10][root][INFO] - Training Epoch: 2/2, step 4369/7134 completed (loss: 0.04612090811133385, acc: 0.9918699264526367)
[2025-02-13 20:31:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:10][root][INFO] - Training Epoch: 2/2, step 4370/7134 completed (loss: 0.15271873772144318, acc: 0.9709302186965942)
[2025-02-13 20:31:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:11][root][INFO] - Training Epoch: 2/2, step 4371/7134 completed (loss: 0.10962460935115814, acc: 0.9695122241973877)
[2025-02-13 20:31:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:11][root][INFO] - Training Epoch: 2/2, step 4372/7134 completed (loss: 0.05047483369708061, acc: 0.9918032884597778)
[2025-02-13 20:31:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:12][root][INFO] - Training Epoch: 2/2, step 4373/7134 completed (loss: 0.09343846142292023, acc: 0.988950252532959)
[2025-02-13 20:31:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:12][root][INFO] - Training Epoch: 2/2, step 4374/7134 completed (loss: 0.1065688356757164, acc: 0.9695122241973877)
[2025-02-13 20:31:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:12][root][INFO] - Training Epoch: 2/2, step 4375/7134 completed (loss: 0.08978968113660812, acc: 0.976190447807312)
[2025-02-13 20:31:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:13][root][INFO] - Training Epoch: 2/2, step 4376/7134 completed (loss: 0.04076322540640831, acc: 1.0)
[2025-02-13 20:31:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:13][root][INFO] - Training Epoch: 2/2, step 4377/7134 completed (loss: 0.16967137157917023, acc: 0.9882352948188782)
[2025-02-13 20:31:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:14][root][INFO] - Training Epoch: 2/2, step 4378/7134 completed (loss: 0.3101675808429718, acc: 0.9774436354637146)
[2025-02-13 20:31:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:14][root][INFO] - Training Epoch: 2/2, step 4379/7134 completed (loss: 0.19426357746124268, acc: 0.9632353186607361)
[2025-02-13 20:31:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:14][root][INFO] - Training Epoch: 2/2, step 4380/7134 completed (loss: 0.06420471519231796, acc: 0.9860140085220337)
[2025-02-13 20:31:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:15][root][INFO] - Training Epoch: 2/2, step 4381/7134 completed (loss: 0.07151570171117783, acc: 0.9880239367485046)
[2025-02-13 20:31:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:15][root][INFO] - Training Epoch: 2/2, step 4382/7134 completed (loss: 0.13008800148963928, acc: 0.9677419066429138)
[2025-02-13 20:31:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:16][root][INFO] - Training Epoch: 2/2, step 4383/7134 completed (loss: 0.07815366983413696, acc: 0.9796954393386841)
[2025-02-13 20:31:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:16][root][INFO] - Training Epoch: 2/2, step 4384/7134 completed (loss: 0.03194216266274452, acc: 1.0)
[2025-02-13 20:31:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:16][root][INFO] - Training Epoch: 2/2, step 4385/7134 completed (loss: 0.1271601915359497, acc: 0.9533678889274597)
[2025-02-13 20:31:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:17][root][INFO] - Training Epoch: 2/2, step 4386/7134 completed (loss: 0.11751426011323929, acc: 0.9611650705337524)
[2025-02-13 20:31:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:17][root][INFO] - Training Epoch: 2/2, step 4387/7134 completed (loss: 0.1895795315504074, acc: 0.9529411792755127)
[2025-02-13 20:31:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:17][root][INFO] - Training Epoch: 2/2, step 4388/7134 completed (loss: 0.20092874765396118, acc: 0.9415584206581116)
[2025-02-13 20:31:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:18][root][INFO] - Training Epoch: 2/2, step 4389/7134 completed (loss: 0.19167126715183258, acc: 0.9492753744125366)
[2025-02-13 20:31:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:18][root][INFO] - Training Epoch: 2/2, step 4390/7134 completed (loss: 0.21089918911457062, acc: 0.9631901979446411)
[2025-02-13 20:31:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:19][root][INFO] - Training Epoch: 2/2, step 4391/7134 completed (loss: 0.20422902703285217, acc: 0.931506872177124)
[2025-02-13 20:31:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:19][root][INFO] - Training Epoch: 2/2, step 4392/7134 completed (loss: 0.06566238403320312, acc: 0.9947090148925781)
[2025-02-13 20:31:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:19][root][INFO] - Training Epoch: 2/2, step 4393/7134 completed (loss: 0.11334746330976486, acc: 0.9666666388511658)
[2025-02-13 20:31:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:20][root][INFO] - Training Epoch: 2/2, step 4394/7134 completed (loss: 0.14476501941680908, acc: 0.9463087320327759)
[2025-02-13 20:31:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:20][root][INFO] - Training Epoch: 2/2, step 4395/7134 completed (loss: 0.13324780762195587, acc: 0.9751552939414978)
[2025-02-13 20:31:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:21][root][INFO] - Training Epoch: 2/2, step 4396/7134 completed (loss: 0.35882294178009033, acc: 0.9100528955459595)
[2025-02-13 20:31:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:21][root][INFO] - Training Epoch: 2/2, step 4397/7134 completed (loss: 0.3343043029308319, acc: 0.9099099040031433)
[2025-02-13 20:31:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:21][root][INFO] - Training Epoch: 2/2, step 4398/7134 completed (loss: 0.16579794883728027, acc: 0.9666666388511658)
[2025-02-13 20:31:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:22][root][INFO] - Training Epoch: 2/2, step 4399/7134 completed (loss: 0.17575567960739136, acc: 0.9652777910232544)
[2025-02-13 20:31:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:22][root][INFO] - Training Epoch: 2/2, step 4400/7134 completed (loss: 0.09875020384788513, acc: 0.9842105507850647)
[2025-02-13 20:31:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:22][root][INFO] - Training Epoch: 2/2, step 4401/7134 completed (loss: 0.11102462559938431, acc: 0.9677419066429138)
[2025-02-13 20:31:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:23][root][INFO] - Training Epoch: 2/2, step 4402/7134 completed (loss: 0.34184056520462036, acc: 0.9135802388191223)
[2025-02-13 20:31:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:23][root][INFO] - Training Epoch: 2/2, step 4403/7134 completed (loss: 0.3921106159687042, acc: 0.9318181872367859)
[2025-02-13 20:31:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:24][root][INFO] - Training Epoch: 2/2, step 4404/7134 completed (loss: 0.3263870179653168, acc: 0.9226804375648499)
[2025-02-13 20:31:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:24][root][INFO] - Training Epoch: 2/2, step 4405/7134 completed (loss: 0.20351609587669373, acc: 0.9603960514068604)
[2025-02-13 20:31:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:25][root][INFO] - Training Epoch: 2/2, step 4406/7134 completed (loss: 0.08700060099363327, acc: 0.9756097793579102)
[2025-02-13 20:31:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:25][root][INFO] - Training Epoch: 2/2, step 4407/7134 completed (loss: 0.20147767663002014, acc: 0.9591836929321289)
[2025-02-13 20:31:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:25][root][INFO] - Training Epoch: 2/2, step 4408/7134 completed (loss: 0.22927260398864746, acc: 0.932692289352417)
[2025-02-13 20:31:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:26][root][INFO] - Training Epoch: 2/2, step 4409/7134 completed (loss: 0.08289697766304016, acc: 0.984375)
[2025-02-13 20:31:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:26][root][INFO] - Training Epoch: 2/2, step 4410/7134 completed (loss: 0.22520321607589722, acc: 0.9510489702224731)
[2025-02-13 20:31:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:27][root][INFO] - Training Epoch: 2/2, step 4411/7134 completed (loss: 0.2854919135570526, acc: 0.9408602118492126)
[2025-02-13 20:31:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:27][root][INFO] - Training Epoch: 2/2, step 4412/7134 completed (loss: 0.5872587561607361, acc: 0.8693467378616333)
[2025-02-13 20:31:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:27][root][INFO] - Training Epoch: 2/2, step 4413/7134 completed (loss: 0.32043811678886414, acc: 0.9430052042007446)
[2025-02-13 20:31:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:28][root][INFO] - Training Epoch: 2/2, step 4414/7134 completed (loss: 0.10364391654729843, acc: 0.9731183052062988)
[2025-02-13 20:31:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:28][root][INFO] - Training Epoch: 2/2, step 4415/7134 completed (loss: 0.11456820368766785, acc: 0.9719101190567017)
[2025-02-13 20:31:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:29][root][INFO] - Training Epoch: 2/2, step 4416/7134 completed (loss: 0.09986213594675064, acc: 0.9689119458198547)
[2025-02-13 20:31:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:29][root][INFO] - Training Epoch: 2/2, step 4417/7134 completed (loss: 0.27727198600769043, acc: 0.939393937587738)
[2025-02-13 20:31:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:29][root][INFO] - Training Epoch: 2/2, step 4418/7134 completed (loss: 0.1686478853225708, acc: 0.967391312122345)
[2025-02-13 20:31:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:30][root][INFO] - Training Epoch: 2/2, step 4419/7134 completed (loss: 0.23285315930843353, acc: 0.9414634108543396)
[2025-02-13 20:31:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:30][root][INFO] - Training Epoch: 2/2, step 4420/7134 completed (loss: 0.26718559861183167, acc: 0.9274611473083496)
[2025-02-13 20:31:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:30][root][INFO] - Training Epoch: 2/2, step 4421/7134 completed (loss: 0.17720143496990204, acc: 0.9403669834136963)
[2025-02-13 20:31:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:31][root][INFO] - Training Epoch: 2/2, step 4422/7134 completed (loss: 0.24262645840644836, acc: 0.9341317415237427)
[2025-02-13 20:31:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:31][root][INFO] - Training Epoch: 2/2, step 4423/7134 completed (loss: 0.1638062745332718, acc: 0.96875)
[2025-02-13 20:31:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:32][root][INFO] - Training Epoch: 2/2, step 4424/7134 completed (loss: 0.09770309180021286, acc: 0.9829545617103577)
[2025-02-13 20:31:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:32][root][INFO] - Training Epoch: 2/2, step 4425/7134 completed (loss: 0.0625874474644661, acc: 0.9851484894752502)
[2025-02-13 20:31:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:33][root][INFO] - Training Epoch: 2/2, step 4426/7134 completed (loss: 0.1994343400001526, acc: 0.9559999704360962)
[2025-02-13 20:31:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:33][root][INFO] - Training Epoch: 2/2, step 4427/7134 completed (loss: 0.17726902663707733, acc: 0.9568965435028076)
[2025-02-13 20:31:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:33][root][INFO] - Training Epoch: 2/2, step 4428/7134 completed (loss: 0.09484578669071198, acc: 0.9826589822769165)
[2025-02-13 20:31:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:34][root][INFO] - Training Epoch: 2/2, step 4429/7134 completed (loss: 0.06773553043603897, acc: 0.9809523820877075)
[2025-02-13 20:31:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:34][root][INFO] - Training Epoch: 2/2, step 4430/7134 completed (loss: 0.12194905430078506, acc: 0.9719101190567017)
[2025-02-13 20:31:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:35][root][INFO] - Training Epoch: 2/2, step 4431/7134 completed (loss: 0.08597763627767563, acc: 0.9883720874786377)
[2025-02-13 20:31:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:35][root][INFO] - Training Epoch: 2/2, step 4432/7134 completed (loss: 0.07902451604604721, acc: 0.9808917045593262)
[2025-02-13 20:31:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:35][root][INFO] - Training Epoch: 2/2, step 4433/7134 completed (loss: 0.1002153754234314, acc: 0.9714285731315613)
[2025-02-13 20:31:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:36][root][INFO] - Training Epoch: 2/2, step 4434/7134 completed (loss: 0.16832534968852997, acc: 0.954285740852356)
[2025-02-13 20:31:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:36][root][INFO] - Training Epoch: 2/2, step 4435/7134 completed (loss: 0.08312293887138367, acc: 0.9791666865348816)
[2025-02-13 20:31:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:37][root][INFO] - Training Epoch: 2/2, step 4436/7134 completed (loss: 0.09418376535177231, acc: 0.976331353187561)
[2025-02-13 20:31:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:37][root][INFO] - Training Epoch: 2/2, step 4437/7134 completed (loss: 0.02833160199224949, acc: 0.9890710115432739)
[2025-02-13 20:31:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:37][root][INFO] - Training Epoch: 2/2, step 4438/7134 completed (loss: 0.12912894785404205, acc: 0.9925373196601868)
[2025-02-13 20:31:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:38][root][INFO] - Training Epoch: 2/2, step 4439/7134 completed (loss: 0.02284052036702633, acc: 0.9929078221321106)
[2025-02-13 20:31:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:38][root][INFO] - Training Epoch: 2/2, step 4440/7134 completed (loss: 0.03451523929834366, acc: 1.0)
[2025-02-13 20:31:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:39][root][INFO] - Training Epoch: 2/2, step 4441/7134 completed (loss: 0.07131507992744446, acc: 0.9748427867889404)
[2025-02-13 20:31:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:39][root][INFO] - Training Epoch: 2/2, step 4442/7134 completed (loss: 0.16695892810821533, acc: 0.9775280952453613)
[2025-02-13 20:31:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:40][root][INFO] - Training Epoch: 2/2, step 4443/7134 completed (loss: 0.11685232073068619, acc: 0.970588207244873)
[2025-02-13 20:31:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:40][root][INFO] - Training Epoch: 2/2, step 4444/7134 completed (loss: 0.07639429718255997, acc: 0.9781022071838379)
[2025-02-13 20:31:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:40][root][INFO] - Training Epoch: 2/2, step 4445/7134 completed (loss: 0.0968652218580246, acc: 0.9822485446929932)
[2025-02-13 20:31:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:41][root][INFO] - Training Epoch: 2/2, step 4446/7134 completed (loss: 0.06508754193782806, acc: 0.9922480583190918)
[2025-02-13 20:31:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:41][root][INFO] - Training Epoch: 2/2, step 4447/7134 completed (loss: 0.04895855486392975, acc: 0.982758641242981)
[2025-02-13 20:31:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:42][root][INFO] - Training Epoch: 2/2, step 4448/7134 completed (loss: 0.14838944375514984, acc: 0.9689922332763672)
[2025-02-13 20:31:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:42][root][INFO] - Training Epoch: 2/2, step 4449/7134 completed (loss: 0.1636764109134674, acc: 0.9661017060279846)
[2025-02-13 20:31:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:42][root][INFO] - Training Epoch: 2/2, step 4450/7134 completed (loss: 0.1162068098783493, acc: 0.9734042286872864)
[2025-02-13 20:31:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:43][root][INFO] - Training Epoch: 2/2, step 4451/7134 completed (loss: 0.2941736876964569, acc: 0.9490445852279663)
[2025-02-13 20:31:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:43][root][INFO] - Training Epoch: 2/2, step 4452/7134 completed (loss: 0.12450063973665237, acc: 0.970370352268219)
[2025-02-13 20:31:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:44][root][INFO] - Training Epoch: 2/2, step 4453/7134 completed (loss: 0.054087501019239426, acc: 0.9934640526771545)
[2025-02-13 20:31:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:44][root][INFO] - Training Epoch: 2/2, step 4454/7134 completed (loss: 0.23052789270877838, acc: 0.95652174949646)
[2025-02-13 20:31:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:44][root][INFO] - Training Epoch: 2/2, step 4455/7134 completed (loss: 0.25304654240608215, acc: 0.9814814925193787)
[2025-02-13 20:31:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:45][root][INFO] - Training Epoch: 2/2, step 4456/7134 completed (loss: 0.03425615653395653, acc: 1.0)
[2025-02-13 20:31:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:45][root][INFO] - Training Epoch: 2/2, step 4457/7134 completed (loss: 0.015241031534969807, acc: 1.0)
[2025-02-13 20:31:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:46][root][INFO] - Training Epoch: 2/2, step 4458/7134 completed (loss: 0.02345297299325466, acc: 0.9930070042610168)
[2025-02-13 20:31:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:46][root][INFO] - Training Epoch: 2/2, step 4459/7134 completed (loss: 0.031430210918188095, acc: 0.9941860437393188)
[2025-02-13 20:31:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:46][root][INFO] - Training Epoch: 2/2, step 4460/7134 completed (loss: 0.04587502405047417, acc: 0.9950739145278931)
[2025-02-13 20:31:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:47][root][INFO] - Training Epoch: 2/2, step 4461/7134 completed (loss: 0.0682232528924942, acc: 0.9791666865348816)
[2025-02-13 20:31:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:47][root][INFO] - Training Epoch: 2/2, step 4462/7134 completed (loss: 0.03755098208785057, acc: 0.9885714054107666)
[2025-02-13 20:31:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:48][root][INFO] - Training Epoch: 2/2, step 4463/7134 completed (loss: 0.029106631875038147, acc: 0.9947090148925781)
[2025-02-13 20:31:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:48][root][INFO] - Training Epoch: 2/2, step 4464/7134 completed (loss: 0.11058834940195084, acc: 0.9794520735740662)
[2025-02-13 20:31:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:49][root][INFO] - Training Epoch: 2/2, step 4465/7134 completed (loss: 0.10497549921274185, acc: 0.9848484992980957)
[2025-02-13 20:31:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:49][root][INFO] - Training Epoch: 2/2, step 4466/7134 completed (loss: 0.03514938801527023, acc: 1.0)
[2025-02-13 20:31:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:49][root][INFO] - Training Epoch: 2/2, step 4467/7134 completed (loss: 0.04483676701784134, acc: 0.9839572310447693)
[2025-02-13 20:31:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:50][root][INFO] - Training Epoch: 2/2, step 4468/7134 completed (loss: 0.05242050811648369, acc: 0.9946523904800415)
[2025-02-13 20:31:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:50][root][INFO] - Training Epoch: 2/2, step 4469/7134 completed (loss: 0.04919879883527756, acc: 0.9900990128517151)
[2025-02-13 20:31:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:51][root][INFO] - Training Epoch: 2/2, step 4470/7134 completed (loss: 0.03511815518140793, acc: 0.995192289352417)
[2025-02-13 20:31:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:51][root][INFO] - Training Epoch: 2/2, step 4471/7134 completed (loss: 0.01758364774286747, acc: 0.9949495196342468)
[2025-02-13 20:31:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:51][root][INFO] - Training Epoch: 2/2, step 4472/7134 completed (loss: 0.03764745220541954, acc: 0.9888888597488403)
[2025-02-13 20:31:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:52][root][INFO] - Training Epoch: 2/2, step 4473/7134 completed (loss: 0.12311308085918427, acc: 0.9644970297813416)
[2025-02-13 20:31:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:52][root][INFO] - Training Epoch: 2/2, step 4474/7134 completed (loss: 0.14449810981750488, acc: 0.9813664555549622)
[2025-02-13 20:31:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:53][root][INFO] - Training Epoch: 2/2, step 4475/7134 completed (loss: 0.21390819549560547, acc: 0.9414893388748169)
[2025-02-13 20:31:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:53][root][INFO] - Training Epoch: 2/2, step 4476/7134 completed (loss: 0.19790932536125183, acc: 0.9450549483299255)
[2025-02-13 20:31:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:54][root][INFO] - Training Epoch: 2/2, step 4477/7134 completed (loss: 0.06081118434667587, acc: 0.9795918464660645)
[2025-02-13 20:31:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:54][root][INFO] - Training Epoch: 2/2, step 4478/7134 completed (loss: 0.07585585117340088, acc: 0.9842932224273682)
[2025-02-13 20:31:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:55][root][INFO] - Training Epoch: 2/2, step 4479/7134 completed (loss: 0.05721345916390419, acc: 0.9833333492279053)
[2025-02-13 20:31:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:55][root][INFO] - Training Epoch: 2/2, step 4480/7134 completed (loss: 0.10442417114973068, acc: 0.9752475023269653)
[2025-02-13 20:31:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:55][root][INFO] - Training Epoch: 2/2, step 4481/7134 completed (loss: 0.08784140646457672, acc: 0.9800000190734863)
[2025-02-13 20:31:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:56][root][INFO] - Training Epoch: 2/2, step 4482/7134 completed (loss: 0.03781786188483238, acc: 0.9851484894752502)
[2025-02-13 20:31:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:56][root][INFO] - Training Epoch: 2/2, step 4483/7134 completed (loss: 0.254186749458313, acc: 0.9378882050514221)
[2025-02-13 20:31:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:57][root][INFO] - Training Epoch: 2/2, step 4484/7134 completed (loss: 0.031186459586024284, acc: 1.0)
[2025-02-13 20:31:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:57][root][INFO] - Training Epoch: 2/2, step 4485/7134 completed (loss: 0.02661597542464733, acc: 0.9934640526771545)
[2025-02-13 20:31:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:57][root][INFO] - Training Epoch: 2/2, step 4486/7134 completed (loss: 0.03238551318645477, acc: 0.9875776171684265)
[2025-02-13 20:31:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:58][root][INFO] - Training Epoch: 2/2, step 4487/7134 completed (loss: 0.05768727883696556, acc: 0.9815950989723206)
[2025-02-13 20:31:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:58][root][INFO] - Training Epoch: 2/2, step 4488/7134 completed (loss: 0.06858015060424805, acc: 0.9767441749572754)
[2025-02-13 20:31:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:59][root][INFO] - Training Epoch: 2/2, step 4489/7134 completed (loss: 0.07637277245521545, acc: 0.9931034445762634)
[2025-02-13 20:31:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:59][root][INFO] - Training Epoch: 2/2, step 4490/7134 completed (loss: 0.07040233910083771, acc: 0.9760000109672546)
[2025-02-13 20:31:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:31:59][root][INFO] - Training Epoch: 2/2, step 4491/7134 completed (loss: 0.06087210029363632, acc: 0.9847328066825867)
[2025-02-13 20:32:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:00][root][INFO] - Training Epoch: 2/2, step 4492/7134 completed (loss: 0.05836903676390648, acc: 0.9924812316894531)
[2025-02-13 20:32:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:00][root][INFO] - Training Epoch: 2/2, step 4493/7134 completed (loss: 0.04372091218829155, acc: 0.9941860437393188)
[2025-02-13 20:32:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:01][root][INFO] - Training Epoch: 2/2, step 4494/7134 completed (loss: 0.04796149581670761, acc: 0.983146071434021)
[2025-02-13 20:32:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:01][root][INFO] - Training Epoch: 2/2, step 4495/7134 completed (loss: 0.02900209091603756, acc: 0.9948453903198242)
[2025-02-13 20:32:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:02][root][INFO] - Training Epoch: 2/2, step 4496/7134 completed (loss: 0.05679783225059509, acc: 0.9941520690917969)
[2025-02-13 20:32:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:02][root][INFO] - Training Epoch: 2/2, step 4497/7134 completed (loss: 0.043247975409030914, acc: 0.9894179701805115)
[2025-02-13 20:32:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:02][root][INFO] - Training Epoch: 2/2, step 4498/7134 completed (loss: 0.019871091470122337, acc: 1.0)
[2025-02-13 20:32:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:03][root][INFO] - Training Epoch: 2/2, step 4499/7134 completed (loss: 0.04328744113445282, acc: 0.9858155846595764)
[2025-02-13 20:32:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:03][root][INFO] - Training Epoch: 2/2, step 4500/7134 completed (loss: 0.039353810250759125, acc: 0.9893048405647278)
[2025-02-13 20:32:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:04][root][INFO] - Training Epoch: 2/2, step 4501/7134 completed (loss: 0.05132388696074486, acc: 0.9834254384040833)
[2025-02-13 20:32:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:04][root][INFO] - Training Epoch: 2/2, step 4502/7134 completed (loss: 0.029980797320604324, acc: 0.9941176176071167)
[2025-02-13 20:32:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:05][root][INFO] - Training Epoch: 2/2, step 4503/7134 completed (loss: 0.03843565285205841, acc: 0.9937106966972351)
[2025-02-13 20:32:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:05][root][INFO] - Training Epoch: 2/2, step 4504/7134 completed (loss: 0.11114998161792755, acc: 0.9875776171684265)
[2025-02-13 20:32:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:05][root][INFO] - Training Epoch: 2/2, step 4505/7134 completed (loss: 0.10158415138721466, acc: 0.9783783555030823)
[2025-02-13 20:32:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:06][root][INFO] - Training Epoch: 2/2, step 4506/7134 completed (loss: 0.15037721395492554, acc: 0.9631901979446411)
[2025-02-13 20:32:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:06][root][INFO] - Training Epoch: 2/2, step 4507/7134 completed (loss: 0.03757641464471817, acc: 0.9935064911842346)
[2025-02-13 20:32:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:07][root][INFO] - Training Epoch: 2/2, step 4508/7134 completed (loss: 0.13234704732894897, acc: 0.95652174949646)
[2025-02-13 20:32:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:07][root][INFO] - Training Epoch: 2/2, step 4509/7134 completed (loss: 0.06953883171081543, acc: 0.9818181991577148)
[2025-02-13 20:32:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:08][root][INFO] - Training Epoch: 2/2, step 4510/7134 completed (loss: 0.13332371413707733, acc: 0.9702380895614624)
[2025-02-13 20:32:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:08][root][INFO] - Training Epoch: 2/2, step 4511/7134 completed (loss: 0.09181832522153854, acc: 0.9659090638160706)
[2025-02-13 20:32:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:09][root][INFO] - Training Epoch: 2/2, step 4512/7134 completed (loss: 0.09923628717660904, acc: 0.9851852059364319)
[2025-02-13 20:32:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:09][root][INFO] - Training Epoch: 2/2, step 4513/7134 completed (loss: 0.09051486849784851, acc: 0.9766082167625427)
[2025-02-13 20:32:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:09][root][INFO] - Training Epoch: 2/2, step 4514/7134 completed (loss: 0.22468677163124084, acc: 0.9710982441902161)
[2025-02-13 20:32:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:10][root][INFO] - Training Epoch: 2/2, step 4515/7134 completed (loss: 0.06965253502130508, acc: 0.9802631735801697)
[2025-02-13 20:32:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:10][root][INFO] - Training Epoch: 2/2, step 4516/7134 completed (loss: 0.058557260781526566, acc: 0.9893048405647278)
[2025-02-13 20:32:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:11][root][INFO] - Training Epoch: 2/2, step 4517/7134 completed (loss: 0.10685890913009644, acc: 0.9649999737739563)
[2025-02-13 20:32:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:11][root][INFO] - Training Epoch: 2/2, step 4518/7134 completed (loss: 0.1015087217092514, acc: 0.9742268323898315)
[2025-02-13 20:32:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:11][root][INFO] - Training Epoch: 2/2, step 4519/7134 completed (loss: 0.05799645557999611, acc: 0.9900497794151306)
[2025-02-13 20:32:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:12][root][INFO] - Training Epoch: 2/2, step 4520/7134 completed (loss: 0.05362999811768532, acc: 0.9839572310447693)
[2025-02-13 20:32:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:12][root][INFO] - Training Epoch: 2/2, step 4521/7134 completed (loss: 0.13878747820854187, acc: 0.9750000238418579)
[2025-02-13 20:32:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:13][root][INFO] - Training Epoch: 2/2, step 4522/7134 completed (loss: 0.10328394919633865, acc: 0.9763033390045166)
[2025-02-13 20:32:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:13][root][INFO] - Training Epoch: 2/2, step 4523/7134 completed (loss: 0.07326862215995789, acc: 0.9765258431434631)
[2025-02-13 20:32:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:14][root][INFO] - Training Epoch: 2/2, step 4524/7134 completed (loss: 0.07127287238836288, acc: 0.9842932224273682)
[2025-02-13 20:32:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:14][root][INFO] - Training Epoch: 2/2, step 4525/7134 completed (loss: 0.1152716651558876, acc: 0.9655172228813171)
[2025-02-13 20:32:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:15][root][INFO] - Training Epoch: 2/2, step 4526/7134 completed (loss: 0.09748230129480362, acc: 0.9725274443626404)
[2025-02-13 20:32:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:15][root][INFO] - Training Epoch: 2/2, step 4527/7134 completed (loss: 0.0903068259358406, acc: 0.9820627570152283)
[2025-02-13 20:32:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:15][root][INFO] - Training Epoch: 2/2, step 4528/7134 completed (loss: 0.14034363627433777, acc: 0.9507389068603516)
[2025-02-13 20:32:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:16][root][INFO] - Training Epoch: 2/2, step 4529/7134 completed (loss: 0.14264518022537231, acc: 0.9528796076774597)
[2025-02-13 20:32:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:16][root][INFO] - Training Epoch: 2/2, step 4530/7134 completed (loss: 0.0441778339445591, acc: 0.9756097793579102)
[2025-02-13 20:32:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:17][root][INFO] - Training Epoch: 2/2, step 4531/7134 completed (loss: 0.032352279871702194, acc: 0.9921875)
[2025-02-13 20:32:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:17][root][INFO] - Training Epoch: 2/2, step 4532/7134 completed (loss: 0.12968239188194275, acc: 0.9662446975708008)
[2025-02-13 20:32:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:17][root][INFO] - Training Epoch: 2/2, step 4533/7134 completed (loss: 0.042215730994939804, acc: 0.9907407164573669)
[2025-02-13 20:32:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:18][root][INFO] - Training Epoch: 2/2, step 4534/7134 completed (loss: 0.06043454632163048, acc: 0.989847719669342)
[2025-02-13 20:32:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:18][root][INFO] - Training Epoch: 2/2, step 4535/7134 completed (loss: 0.054799120873212814, acc: 0.9903846383094788)
[2025-02-13 20:32:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:19][root][INFO] - Training Epoch: 2/2, step 4536/7134 completed (loss: 0.0386551171541214, acc: 0.9839357137680054)
[2025-02-13 20:32:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:19][root][INFO] - Training Epoch: 2/2, step 4537/7134 completed (loss: 0.028361547738313675, acc: 0.9954751133918762)
[2025-02-13 20:32:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:19][root][INFO] - Training Epoch: 2/2, step 4538/7134 completed (loss: 0.09201206266880035, acc: 0.990338146686554)
[2025-02-13 20:32:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:20][root][INFO] - Training Epoch: 2/2, step 4539/7134 completed (loss: 0.08435314148664474, acc: 0.9718309640884399)
[2025-02-13 20:32:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:20][root][INFO] - Training Epoch: 2/2, step 4540/7134 completed (loss: 0.13275516033172607, acc: 0.976190447807312)
[2025-02-13 20:32:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:20][root][INFO] - Training Epoch: 2/2, step 4541/7134 completed (loss: 0.20411579310894012, acc: 0.9672130942344666)
[2025-02-13 20:32:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:21][root][INFO] - Training Epoch: 2/2, step 4542/7134 completed (loss: 0.13542772829532623, acc: 0.9385964870452881)
[2025-02-13 20:32:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:21][root][INFO] - Training Epoch: 2/2, step 4543/7134 completed (loss: 0.23457658290863037, acc: 0.9285714030265808)
[2025-02-13 20:32:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:22][root][INFO] - Training Epoch: 2/2, step 4544/7134 completed (loss: 0.04987255856394768, acc: 0.9932432174682617)
[2025-02-13 20:32:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:22][root][INFO] - Training Epoch: 2/2, step 4545/7134 completed (loss: 0.0699618011713028, acc: 0.9722222089767456)
[2025-02-13 20:32:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:22][root][INFO] - Training Epoch: 2/2, step 4546/7134 completed (loss: 0.12724457681179047, acc: 0.9685039520263672)
[2025-02-13 20:32:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:23][root][INFO] - Training Epoch: 2/2, step 4547/7134 completed (loss: 0.06180574372410774, acc: 0.991525411605835)
[2025-02-13 20:32:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:23][root][INFO] - Training Epoch: 2/2, step 4548/7134 completed (loss: 0.08863435685634613, acc: 0.9685039520263672)
[2025-02-13 20:32:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:23][root][INFO] - Training Epoch: 2/2, step 4549/7134 completed (loss: 0.10831357538700104, acc: 0.9736841917037964)
[2025-02-13 20:32:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:24][root][INFO] - Training Epoch: 2/2, step 4550/7134 completed (loss: 0.12807278335094452, acc: 0.9849624037742615)
[2025-02-13 20:32:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:24][root][INFO] - Training Epoch: 2/2, step 4551/7134 completed (loss: 0.19745823740959167, acc: 0.9662162065505981)
[2025-02-13 20:32:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:25][root][INFO] - Training Epoch: 2/2, step 4552/7134 completed (loss: 0.026012137532234192, acc: 1.0)
[2025-02-13 20:32:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:25][root][INFO] - Training Epoch: 2/2, step 4553/7134 completed (loss: 0.03936775401234627, acc: 1.0)
[2025-02-13 20:32:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:25][root][INFO] - Training Epoch: 2/2, step 4554/7134 completed (loss: 0.1099272295832634, acc: 0.970802903175354)
[2025-02-13 20:32:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:26][root][INFO] - Training Epoch: 2/2, step 4555/7134 completed (loss: 0.11975771933794022, acc: 0.9791666865348816)
[2025-02-13 20:32:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:26][root][INFO] - Training Epoch: 2/2, step 4556/7134 completed (loss: 0.24128644168376923, acc: 0.9419354796409607)
[2025-02-13 20:32:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:27][root][INFO] - Training Epoch: 2/2, step 4557/7134 completed (loss: 0.035902101546525955, acc: 0.9850746393203735)
[2025-02-13 20:32:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:27][root][INFO] - Training Epoch: 2/2, step 4558/7134 completed (loss: 0.12545786798000336, acc: 0.9767441749572754)
[2025-02-13 20:32:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:27][root][INFO] - Training Epoch: 2/2, step 4559/7134 completed (loss: 0.10216443240642548, acc: 0.9848484992980957)
[2025-02-13 20:32:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:28][root][INFO] - Training Epoch: 2/2, step 4560/7134 completed (loss: 0.09344062954187393, acc: 0.975806474685669)
[2025-02-13 20:32:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:28][root][INFO] - Training Epoch: 2/2, step 4561/7134 completed (loss: 0.11081042885780334, acc: 0.9672130942344666)
[2025-02-13 20:32:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:29][root][INFO] - Training Epoch: 2/2, step 4562/7134 completed (loss: 0.13289563357830048, acc: 0.9593495726585388)
[2025-02-13 20:32:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:29][root][INFO] - Training Epoch: 2/2, step 4563/7134 completed (loss: 0.06585109233856201, acc: 0.9929078221321106)
[2025-02-13 20:32:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:29][root][INFO] - Training Epoch: 2/2, step 4564/7134 completed (loss: 0.15379203855991364, acc: 0.9545454382896423)
[2025-02-13 20:32:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:30][root][INFO] - Training Epoch: 2/2, step 4565/7134 completed (loss: 0.09280306100845337, acc: 0.982300877571106)
[2025-02-13 20:32:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:30][root][INFO] - Training Epoch: 2/2, step 4566/7134 completed (loss: 0.13799704611301422, acc: 0.9624060392379761)
[2025-02-13 20:32:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:31][root][INFO] - Training Epoch: 2/2, step 4567/7134 completed (loss: 0.019466543570160866, acc: 1.0)
[2025-02-13 20:32:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:31][root][INFO] - Training Epoch: 2/2, step 4568/7134 completed (loss: 0.12711554765701294, acc: 0.9684210419654846)
[2025-02-13 20:32:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:31][root][INFO] - Training Epoch: 2/2, step 4569/7134 completed (loss: 0.17066748440265656, acc: 0.9734513163566589)
[2025-02-13 20:32:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:32][root][INFO] - Training Epoch: 2/2, step 4570/7134 completed (loss: 0.12340330332517624, acc: 0.948051929473877)
[2025-02-13 20:32:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:32][root][INFO] - Training Epoch: 2/2, step 4571/7134 completed (loss: 0.11559298634529114, acc: 0.9712643623352051)
[2025-02-13 20:32:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:33][root][INFO] - Training Epoch: 2/2, step 4572/7134 completed (loss: 0.07037250697612762, acc: 0.9736841917037964)
[2025-02-13 20:32:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:33][root][INFO] - Training Epoch: 2/2, step 4573/7134 completed (loss: 0.0616891123354435, acc: 0.9807692170143127)
[2025-02-13 20:32:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:33][root][INFO] - Training Epoch: 2/2, step 4574/7134 completed (loss: 0.07121963798999786, acc: 0.9848484992980957)
[2025-02-13 20:32:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:34][root][INFO] - Training Epoch: 2/2, step 4575/7134 completed (loss: 0.06735823303461075, acc: 0.988950252532959)
[2025-02-13 20:32:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:34][root][INFO] - Training Epoch: 2/2, step 4576/7134 completed (loss: 0.09906790405511856, acc: 0.9828571677207947)
[2025-02-13 20:32:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:35][root][INFO] - Training Epoch: 2/2, step 4577/7134 completed (loss: 0.11670701205730438, acc: 0.9767441749572754)
[2025-02-13 20:32:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:35][root][INFO] - Training Epoch: 2/2, step 4578/7134 completed (loss: 0.06311998516321182, acc: 0.9878787994384766)
[2025-02-13 20:32:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:35][root][INFO] - Training Epoch: 2/2, step 4579/7134 completed (loss: 0.18006755411624908, acc: 0.9683544039726257)
[2025-02-13 20:32:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:36][root][INFO] - Training Epoch: 2/2, step 4580/7134 completed (loss: 0.05908718705177307, acc: 0.9936708807945251)
[2025-02-13 20:32:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:36][root][INFO] - Training Epoch: 2/2, step 4581/7134 completed (loss: 0.0141775943338871, acc: 1.0)
[2025-02-13 20:32:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:37][root][INFO] - Training Epoch: 2/2, step 4582/7134 completed (loss: 0.059143856167793274, acc: 0.9791666865348816)
[2025-02-13 20:32:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:37][root][INFO] - Training Epoch: 2/2, step 4583/7134 completed (loss: 0.18818633258342743, acc: 0.9666666388511658)
[2025-02-13 20:32:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:37][root][INFO] - Training Epoch: 2/2, step 4584/7134 completed (loss: 0.055053163319826126, acc: 0.9862068891525269)
[2025-02-13 20:32:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:38][root][INFO] - Training Epoch: 2/2, step 4585/7134 completed (loss: 0.023135529831051826, acc: 0.9935897588729858)
[2025-02-13 20:32:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:38][root][INFO] - Training Epoch: 2/2, step 4586/7134 completed (loss: 0.11466218531131744, acc: 0.9704142212867737)
[2025-02-13 20:32:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:39][root][INFO] - Training Epoch: 2/2, step 4587/7134 completed (loss: 0.2190851867198944, acc: 0.9767441749572754)
[2025-02-13 20:32:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:39][root][INFO] - Training Epoch: 2/2, step 4588/7134 completed (loss: 0.11188487708568573, acc: 0.9624999761581421)
[2025-02-13 20:32:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:39][root][INFO] - Training Epoch: 2/2, step 4589/7134 completed (loss: 0.12774251401424408, acc: 0.9655172228813171)
[2025-02-13 20:32:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:40][root][INFO] - Training Epoch: 2/2, step 4590/7134 completed (loss: 0.1060594916343689, acc: 0.9702380895614624)
[2025-02-13 20:32:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:40][root][INFO] - Training Epoch: 2/2, step 4591/7134 completed (loss: 0.09945926070213318, acc: 0.9829545617103577)
[2025-02-13 20:32:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:41][root][INFO] - Training Epoch: 2/2, step 4592/7134 completed (loss: 0.11687977612018585, acc: 0.9756097793579102)
[2025-02-13 20:32:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:41][root][INFO] - Training Epoch: 2/2, step 4593/7134 completed (loss: 0.14423342049121857, acc: 0.9473684430122375)
[2025-02-13 20:32:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:41][root][INFO] - Training Epoch: 2/2, step 4594/7134 completed (loss: 0.17330577969551086, acc: 0.9365079402923584)
[2025-02-13 20:32:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:42][root][INFO] - Training Epoch: 2/2, step 4595/7134 completed (loss: 0.10248320549726486, acc: 0.9620253443717957)
[2025-02-13 20:32:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:42][root][INFO] - Training Epoch: 2/2, step 4596/7134 completed (loss: 0.08356897532939911, acc: 0.981249988079071)
[2025-02-13 20:32:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:43][root][INFO] - Training Epoch: 2/2, step 4597/7134 completed (loss: 0.09809724241495132, acc: 0.9692307710647583)
[2025-02-13 20:32:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:43][root][INFO] - Training Epoch: 2/2, step 4598/7134 completed (loss: 0.18891428411006927, acc: 0.9513513445854187)
[2025-02-13 20:32:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:44][root][INFO] - Training Epoch: 2/2, step 4599/7134 completed (loss: 0.048090267926454544, acc: 0.9927536249160767)
[2025-02-13 20:32:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:44][root][INFO] - Training Epoch: 2/2, step 4600/7134 completed (loss: 0.07975300401449203, acc: 0.9811320900917053)
[2025-02-13 20:32:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:44][root][INFO] - Training Epoch: 2/2, step 4601/7134 completed (loss: 0.07372012734413147, acc: 0.9887005686759949)
[2025-02-13 20:32:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:45][root][INFO] - Training Epoch: 2/2, step 4602/7134 completed (loss: 0.08318775147199631, acc: 0.9833333492279053)
[2025-02-13 20:32:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:45][root][INFO] - Training Epoch: 2/2, step 4603/7134 completed (loss: 0.10261817276477814, acc: 0.9709302186965942)
[2025-02-13 20:32:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:46][root][INFO] - Training Epoch: 2/2, step 4604/7134 completed (loss: 0.06427577137947083, acc: 0.9930070042610168)
[2025-02-13 20:32:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:46][root][INFO] - Training Epoch: 2/2, step 4605/7134 completed (loss: 0.11601301282644272, acc: 0.9545454382896423)
[2025-02-13 20:32:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:47][root][INFO] - Training Epoch: 2/2, step 4606/7134 completed (loss: 0.05068805813789368, acc: 0.9811320900917053)
[2025-02-13 20:32:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:47][root][INFO] - Training Epoch: 2/2, step 4607/7134 completed (loss: 0.02603510394692421, acc: 1.0)
[2025-02-13 20:32:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:47][root][INFO] - Training Epoch: 2/2, step 4608/7134 completed (loss: 0.06219939887523651, acc: 0.988304078578949)
[2025-02-13 20:32:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:48][root][INFO] - Training Epoch: 2/2, step 4609/7134 completed (loss: 0.02075807936489582, acc: 1.0)
[2025-02-13 20:32:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:48][root][INFO] - Training Epoch: 2/2, step 4610/7134 completed (loss: 0.04380504786968231, acc: 0.9776536226272583)
[2025-02-13 20:32:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:49][root][INFO] - Training Epoch: 2/2, step 4611/7134 completed (loss: 0.025507852435112, acc: 1.0)
[2025-02-13 20:32:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:49][root][INFO] - Training Epoch: 2/2, step 4612/7134 completed (loss: 0.07626495510339737, acc: 0.9851852059364319)
[2025-02-13 20:32:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:50][root][INFO] - Training Epoch: 2/2, step 4613/7134 completed (loss: 0.08664480596780777, acc: 0.9814814925193787)
[2025-02-13 20:32:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:50][root][INFO] - Training Epoch: 2/2, step 4614/7134 completed (loss: 0.09108096361160278, acc: 0.9615384340286255)
[2025-02-13 20:32:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:51][root][INFO] - Training Epoch: 2/2, step 4615/7134 completed (loss: 0.17618852853775024, acc: 0.9580838084220886)
[2025-02-13 20:32:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:51][root][INFO] - Training Epoch: 2/2, step 4616/7134 completed (loss: 0.17441195249557495, acc: 0.9534883499145508)
[2025-02-13 20:32:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:51][root][INFO] - Training Epoch: 2/2, step 4617/7134 completed (loss: 0.15291152894496918, acc: 0.970059871673584)
[2025-02-13 20:32:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:52][root][INFO] - Training Epoch: 2/2, step 4618/7134 completed (loss: 0.10051894187927246, acc: 0.9767441749572754)
[2025-02-13 20:32:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:52][root][INFO] - Training Epoch: 2/2, step 4619/7134 completed (loss: 0.15772555768489838, acc: 0.981249988079071)
[2025-02-13 20:32:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:52][root][INFO] - Training Epoch: 2/2, step 4620/7134 completed (loss: 0.033073071390390396, acc: 1.0)
[2025-02-13 20:32:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:53][root][INFO] - Training Epoch: 2/2, step 4621/7134 completed (loss: 0.07133902609348297, acc: 0.9878048896789551)
[2025-02-13 20:32:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:53][root][INFO] - Training Epoch: 2/2, step 4622/7134 completed (loss: 0.060358110815286636, acc: 0.9777777791023254)
[2025-02-13 20:32:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:54][root][INFO] - Training Epoch: 2/2, step 4623/7134 completed (loss: 0.0727536678314209, acc: 0.9941176176071167)
[2025-02-13 20:32:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:54][root][INFO] - Training Epoch: 2/2, step 4624/7134 completed (loss: 0.3035795986652374, acc: 0.9529411792755127)
[2025-02-13 20:32:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:55][root][INFO] - Training Epoch: 2/2, step 4625/7134 completed (loss: 0.252765029668808, acc: 0.9202454090118408)
[2025-02-13 20:32:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:55][root][INFO] - Training Epoch: 2/2, step 4626/7134 completed (loss: 0.23556704819202423, acc: 0.9239130616188049)
[2025-02-13 20:32:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:55][root][INFO] - Training Epoch: 2/2, step 4627/7134 completed (loss: 0.16035647690296173, acc: 0.9677419066429138)
[2025-02-13 20:32:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:56][root][INFO] - Training Epoch: 2/2, step 4628/7134 completed (loss: 0.10861154645681381, acc: 0.9695122241973877)
[2025-02-13 20:32:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:56][root][INFO] - Training Epoch: 2/2, step 4629/7134 completed (loss: 0.12952828407287598, acc: 0.9740259647369385)
[2025-02-13 20:32:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:57][root][INFO] - Training Epoch: 2/2, step 4630/7134 completed (loss: 0.13496671617031097, acc: 0.9833333492279053)
[2025-02-13 20:32:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:57][root][INFO] - Training Epoch: 2/2, step 4631/7134 completed (loss: 0.09356143325567245, acc: 0.9860464930534363)
[2025-02-13 20:32:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:58][root][INFO] - Training Epoch: 2/2, step 4632/7134 completed (loss: 0.14704199135303497, acc: 0.9638554453849792)
[2025-02-13 20:32:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:58][root][INFO] - Training Epoch: 2/2, step 4633/7134 completed (loss: 0.19421815872192383, acc: 0.9634703397750854)
[2025-02-13 20:32:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:59][root][INFO] - Training Epoch: 2/2, step 4634/7134 completed (loss: 0.08612339198589325, acc: 0.9629629850387573)
[2025-02-13 20:32:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:59][root][INFO] - Training Epoch: 2/2, step 4635/7134 completed (loss: 0.23339204490184784, acc: 0.9756097793579102)
[2025-02-13 20:32:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:32:59][root][INFO] - Training Epoch: 2/2, step 4636/7134 completed (loss: 0.023953523486852646, acc: 1.0)
[2025-02-13 20:33:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:00][root][INFO] - Training Epoch: 2/2, step 4637/7134 completed (loss: 0.12433281540870667, acc: 0.976190447807312)
[2025-02-13 20:33:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:00][root][INFO] - Training Epoch: 2/2, step 4638/7134 completed (loss: 0.1355057656764984, acc: 0.9710982441902161)
[2025-02-13 20:33:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:01][root][INFO] - Training Epoch: 2/2, step 4639/7134 completed (loss: 0.14018088579177856, acc: 0.9675675630569458)
[2025-02-13 20:33:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:01][root][INFO] - Training Epoch: 2/2, step 4640/7134 completed (loss: 0.12444041669368744, acc: 0.9682539701461792)
[2025-02-13 20:33:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:02][root][INFO] - Training Epoch: 2/2, step 4641/7134 completed (loss: 0.030450982972979546, acc: 1.0)
[2025-02-13 20:33:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:02][root][INFO] - Training Epoch: 2/2, step 4642/7134 completed (loss: 0.03653986006975174, acc: 0.9887005686759949)
[2025-02-13 20:33:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:02][root][INFO] - Training Epoch: 2/2, step 4643/7134 completed (loss: 0.06198530271649361, acc: 0.9853658676147461)
[2025-02-13 20:33:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:03][root][INFO] - Training Epoch: 2/2, step 4644/7134 completed (loss: 0.0429227277636528, acc: 0.9950739145278931)
[2025-02-13 20:33:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:03][root][INFO] - Training Epoch: 2/2, step 4645/7134 completed (loss: 0.02115771919488907, acc: 0.9939024448394775)
[2025-02-13 20:33:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:04][root][INFO] - Training Epoch: 2/2, step 4646/7134 completed (loss: 0.07006165385246277, acc: 0.9857142567634583)
[2025-02-13 20:33:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:04][root][INFO] - Training Epoch: 2/2, step 4647/7134 completed (loss: 0.1505613625049591, acc: 0.9726775884628296)
[2025-02-13 20:33:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:04][root][INFO] - Training Epoch: 2/2, step 4648/7134 completed (loss: 0.11916819214820862, acc: 0.9745222926139832)
[2025-02-13 20:33:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:05][root][INFO] - Training Epoch: 2/2, step 4649/7134 completed (loss: 0.07737341523170471, acc: 0.9837398529052734)
[2025-02-13 20:33:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:05][root][INFO] - Training Epoch: 2/2, step 4650/7134 completed (loss: 0.0912550762295723, acc: 0.9797297120094299)
[2025-02-13 20:33:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:06][root][INFO] - Training Epoch: 2/2, step 4651/7134 completed (loss: 0.21164903044700623, acc: 0.9441340565681458)
[2025-02-13 20:33:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:06][root][INFO] - Training Epoch: 2/2, step 4652/7134 completed (loss: 0.11585826426744461, acc: 0.9691358208656311)
[2025-02-13 20:33:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:06][root][INFO] - Training Epoch: 2/2, step 4653/7134 completed (loss: 0.0730271190404892, acc: 0.9855072498321533)
[2025-02-13 20:33:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:07][root][INFO] - Training Epoch: 2/2, step 4654/7134 completed (loss: 0.2311789095401764, acc: 0.9459459185600281)
[2025-02-13 20:33:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:07][root][INFO] - Training Epoch: 2/2, step 4655/7134 completed (loss: 0.07914991676807404, acc: 0.977142870426178)
[2025-02-13 20:33:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:08][root][INFO] - Training Epoch: 2/2, step 4656/7134 completed (loss: 0.12393572181463242, acc: 0.9661017060279846)
[2025-02-13 20:33:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:08][root][INFO] - Training Epoch: 2/2, step 4657/7134 completed (loss: 0.14391528069972992, acc: 0.954023003578186)
[2025-02-13 20:33:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:08][root][INFO] - Training Epoch: 2/2, step 4658/7134 completed (loss: 0.09028124809265137, acc: 0.9715909361839294)
[2025-02-13 20:33:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:09][root][INFO] - Training Epoch: 2/2, step 4659/7134 completed (loss: 0.1470119059085846, acc: 0.9523809552192688)
[2025-02-13 20:33:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:09][root][INFO] - Training Epoch: 2/2, step 4660/7134 completed (loss: 0.1548670083284378, acc: 0.942307710647583)
[2025-02-13 20:33:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:10][root][INFO] - Training Epoch: 2/2, step 4661/7134 completed (loss: 0.12519191205501556, acc: 0.9631901979446411)
[2025-02-13 20:33:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:10][root][INFO] - Training Epoch: 2/2, step 4662/7134 completed (loss: 0.18701285123825073, acc: 0.9716312289237976)
[2025-02-13 20:33:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:11][root][INFO] - Training Epoch: 2/2, step 4663/7134 completed (loss: 0.18524257838726044, acc: 0.9453551769256592)
[2025-02-13 20:33:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:11][root][INFO] - Training Epoch: 2/2, step 4664/7134 completed (loss: 0.1234583705663681, acc: 0.9783783555030823)
[2025-02-13 20:33:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:11][root][INFO] - Training Epoch: 2/2, step 4665/7134 completed (loss: 0.08988998830318451, acc: 0.9890109896659851)
[2025-02-13 20:33:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:12][root][INFO] - Training Epoch: 2/2, step 4666/7134 completed (loss: 0.2609902322292328, acc: 0.9533678889274597)
[2025-02-13 20:33:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:12][root][INFO] - Training Epoch: 2/2, step 4667/7134 completed (loss: 0.12439227849245071, acc: 0.9662162065505981)
[2025-02-13 20:33:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:13][root][INFO] - Training Epoch: 2/2, step 4668/7134 completed (loss: 0.11054429411888123, acc: 0.9798657894134521)
[2025-02-13 20:33:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:13][root][INFO] - Training Epoch: 2/2, step 4669/7134 completed (loss: 0.09987056255340576, acc: 0.9717513918876648)
[2025-02-13 20:33:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:13][root][INFO] - Training Epoch: 2/2, step 4670/7134 completed (loss: 0.11252149939537048, acc: 0.9850746393203735)
[2025-02-13 20:33:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:14][root][INFO] - Training Epoch: 2/2, step 4671/7134 completed (loss: 0.08182842284440994, acc: 0.9781420826911926)
[2025-02-13 20:33:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:14][root][INFO] - Training Epoch: 2/2, step 4672/7134 completed (loss: 0.06442596018314362, acc: 0.9793103337287903)
[2025-02-13 20:33:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:15][root][INFO] - Training Epoch: 2/2, step 4673/7134 completed (loss: 0.12286914139986038, acc: 0.9685534834861755)
[2025-02-13 20:33:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:15][root][INFO] - Training Epoch: 2/2, step 4674/7134 completed (loss: 0.08654121309518814, acc: 0.9805194735527039)
[2025-02-13 20:33:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:15][root][INFO] - Training Epoch: 2/2, step 4675/7134 completed (loss: 0.1146763265132904, acc: 0.9692307710647583)
[2025-02-13 20:33:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:16][root][INFO] - Training Epoch: 2/2, step 4676/7134 completed (loss: 0.03666473180055618, acc: 0.9935064911842346)
[2025-02-13 20:33:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:16][root][INFO] - Training Epoch: 2/2, step 4677/7134 completed (loss: 0.038908492773771286, acc: 0.9915966391563416)
[2025-02-13 20:33:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:17][root][INFO] - Training Epoch: 2/2, step 4678/7134 completed (loss: 0.17574043571949005, acc: 0.9640718698501587)
[2025-02-13 20:33:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:17][root][INFO] - Training Epoch: 2/2, step 4679/7134 completed (loss: 0.07875213027000427, acc: 0.9768785834312439)
[2025-02-13 20:33:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:17][root][INFO] - Training Epoch: 2/2, step 4680/7134 completed (loss: 0.15110407769680023, acc: 0.9545454382896423)
[2025-02-13 20:33:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:18][root][INFO] - Training Epoch: 2/2, step 4681/7134 completed (loss: 0.16894586384296417, acc: 0.9415584206581116)
[2025-02-13 20:33:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:18][root][INFO] - Training Epoch: 2/2, step 4682/7134 completed (loss: 0.10209767520427704, acc: 0.9779005646705627)
[2025-02-13 20:33:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:18][root][INFO] - Training Epoch: 2/2, step 4683/7134 completed (loss: 0.14863720536231995, acc: 0.9466666579246521)
[2025-02-13 20:33:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:19][root][INFO] - Training Epoch: 2/2, step 4684/7134 completed (loss: 0.08163987845182419, acc: 0.9793814420700073)
[2025-02-13 20:33:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:19][root][INFO] - Training Epoch: 2/2, step 4685/7134 completed (loss: 0.05044035613536835, acc: 0.9901477694511414)
[2025-02-13 20:33:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:20][root][INFO] - Training Epoch: 2/2, step 4686/7134 completed (loss: 0.06539376825094223, acc: 0.9841269850730896)
[2025-02-13 20:33:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:20][root][INFO] - Training Epoch: 2/2, step 4687/7134 completed (loss: 0.17691870033740997, acc: 0.9347826242446899)
[2025-02-13 20:33:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:21][root][INFO] - Training Epoch: 2/2, step 4688/7134 completed (loss: 0.42532339692115784, acc: 0.921875)
[2025-02-13 20:33:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:21][root][INFO] - Training Epoch: 2/2, step 4689/7134 completed (loss: 0.28770241141319275, acc: 0.9425287246704102)
[2025-02-13 20:33:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:21][root][INFO] - Training Epoch: 2/2, step 4690/7134 completed (loss: 0.27346181869506836, acc: 0.9561403393745422)
[2025-02-13 20:33:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:22][root][INFO] - Training Epoch: 2/2, step 4691/7134 completed (loss: 0.11157352477312088, acc: 0.9743589758872986)
[2025-02-13 20:33:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:22][root][INFO] - Training Epoch: 2/2, step 4692/7134 completed (loss: 0.45472243428230286, acc: 0.8888888955116272)
[2025-02-13 20:33:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:23][root][INFO] - Training Epoch: 2/2, step 4693/7134 completed (loss: 0.3999311029911041, acc: 0.8914285898208618)
[2025-02-13 20:33:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:23][root][INFO] - Training Epoch: 2/2, step 4694/7134 completed (loss: 0.3283105492591858, acc: 0.9285714030265808)
[2025-02-13 20:33:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:23][root][INFO] - Training Epoch: 2/2, step 4695/7134 completed (loss: 0.26411202549934387, acc: 0.95333331823349)
[2025-02-13 20:33:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:24][root][INFO] - Training Epoch: 2/2, step 4696/7134 completed (loss: 0.04321848973631859, acc: 1.0)
[2025-02-13 20:33:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:24][root][INFO] - Training Epoch: 2/2, step 4697/7134 completed (loss: 0.03702050819993019, acc: 1.0)
[2025-02-13 20:33:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:25][root][INFO] - Training Epoch: 2/2, step 4698/7134 completed (loss: 0.21227240562438965, acc: 0.9428571462631226)
[2025-02-13 20:33:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:25][root][INFO] - Training Epoch: 2/2, step 4699/7134 completed (loss: 0.2370854616165161, acc: 0.9496402740478516)
[2025-02-13 20:33:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:25][root][INFO] - Training Epoch: 2/2, step 4700/7134 completed (loss: 0.1930716633796692, acc: 0.9526627063751221)
[2025-02-13 20:33:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:26][root][INFO] - Training Epoch: 2/2, step 4701/7134 completed (loss: 0.2797561287879944, acc: 0.925000011920929)
[2025-02-13 20:33:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:26][root][INFO] - Training Epoch: 2/2, step 4702/7134 completed (loss: 0.11348263919353485, acc: 0.9741935729980469)
[2025-02-13 20:33:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:27][root][INFO] - Training Epoch: 2/2, step 4703/7134 completed (loss: 0.07506610453128815, acc: 0.9882352948188782)
[2025-02-13 20:33:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:27][root][INFO] - Training Epoch: 2/2, step 4704/7134 completed (loss: 0.07249134033918381, acc: 0.9846153855323792)
[2025-02-13 20:33:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:27][root][INFO] - Training Epoch: 2/2, step 4705/7134 completed (loss: 0.1600915938615799, acc: 0.9605911374092102)
[2025-02-13 20:33:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:28][root][INFO] - Training Epoch: 2/2, step 4706/7134 completed (loss: 0.154896542429924, acc: 0.9743589758872986)
[2025-02-13 20:33:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:28][root][INFO] - Training Epoch: 2/2, step 4707/7134 completed (loss: 0.5994953513145447, acc: 0.8993710875511169)
[2025-02-13 20:33:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:29][root][INFO] - Training Epoch: 2/2, step 4708/7134 completed (loss: 0.1473149210214615, acc: 0.979899525642395)
[2025-02-13 20:33:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:29][root][INFO] - Training Epoch: 2/2, step 4709/7134 completed (loss: 0.0773259848356247, acc: 0.9837837815284729)
[2025-02-13 20:33:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:30][root][INFO] - Training Epoch: 2/2, step 4710/7134 completed (loss: 0.12245549261569977, acc: 0.9636363387107849)
[2025-02-13 20:33:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:30][root][INFO] - Training Epoch: 2/2, step 4711/7134 completed (loss: 0.18188916146755219, acc: 0.9527027010917664)
[2025-02-13 20:33:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:30][root][INFO] - Training Epoch: 2/2, step 4712/7134 completed (loss: 0.20851895213127136, acc: 0.9411764740943909)
[2025-02-13 20:33:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:31][root][INFO] - Training Epoch: 2/2, step 4713/7134 completed (loss: 0.17005862295627594, acc: 0.9552238583564758)
[2025-02-13 20:33:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:31][root][INFO] - Training Epoch: 2/2, step 4714/7134 completed (loss: 0.17701950669288635, acc: 0.9694656729698181)
[2025-02-13 20:33:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:31][root][INFO] - Training Epoch: 2/2, step 4715/7134 completed (loss: 0.28097760677337646, acc: 0.9666666388511658)
[2025-02-13 20:33:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:32][root][INFO] - Training Epoch: 2/2, step 4716/7134 completed (loss: 0.09534570574760437, acc: 0.9806451797485352)
[2025-02-13 20:33:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:32][root][INFO] - Training Epoch: 2/2, step 4717/7134 completed (loss: 0.06879902631044388, acc: 0.9866666793823242)
[2025-02-13 20:33:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:33][root][INFO] - Training Epoch: 2/2, step 4718/7134 completed (loss: 0.05708129703998566, acc: 0.9784172773361206)
[2025-02-13 20:33:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:33][root][INFO] - Training Epoch: 2/2, step 4719/7134 completed (loss: 0.08573631197214127, acc: 0.976331353187561)
[2025-02-13 20:33:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:34][root][INFO] - Training Epoch: 2/2, step 4720/7134 completed (loss: 0.02915634773671627, acc: 0.9937499761581421)
[2025-02-13 20:33:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:34][root][INFO] - Training Epoch: 2/2, step 4721/7134 completed (loss: 0.01731731928884983, acc: 1.0)
[2025-02-13 20:33:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:35][root][INFO] - Training Epoch: 2/2, step 4722/7134 completed (loss: 0.08494400978088379, acc: 0.9811320900917053)
[2025-02-13 20:33:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:35][root][INFO] - Training Epoch: 2/2, step 4723/7134 completed (loss: 0.09155511856079102, acc: 0.9745222926139832)
[2025-02-13 20:33:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:35][root][INFO] - Training Epoch: 2/2, step 4724/7134 completed (loss: 0.10649614781141281, acc: 0.9878048896789551)
[2025-02-13 20:33:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:36][root][INFO] - Training Epoch: 2/2, step 4725/7134 completed (loss: 0.11012329906225204, acc: 0.9651162624359131)
[2025-02-13 20:33:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:36][root][INFO] - Training Epoch: 2/2, step 4726/7134 completed (loss: 0.1413881629705429, acc: 0.9655172228813171)
[2025-02-13 20:33:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:37][root][INFO] - Training Epoch: 2/2, step 4727/7134 completed (loss: 0.09743552654981613, acc: 0.9867549538612366)
[2025-02-13 20:33:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:37][root][INFO] - Training Epoch: 2/2, step 4728/7134 completed (loss: 0.08302474766969681, acc: 0.9735099077224731)
[2025-02-13 20:33:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:38][root][INFO] - Training Epoch: 2/2, step 4729/7134 completed (loss: 0.09604906290769577, acc: 0.9653179049491882)
[2025-02-13 20:33:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:38][root][INFO] - Training Epoch: 2/2, step 4730/7134 completed (loss: 0.10559830814599991, acc: 0.9824561476707458)
[2025-02-13 20:33:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:39][root][INFO] - Training Epoch: 2/2, step 4731/7134 completed (loss: 0.23152276873588562, acc: 0.951724112033844)
[2025-02-13 20:33:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:39][root][INFO] - Training Epoch: 2/2, step 4732/7134 completed (loss: 0.11327621340751648, acc: 0.970588207244873)
[2025-02-13 20:33:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:39][root][INFO] - Training Epoch: 2/2, step 4733/7134 completed (loss: 0.16278377175331116, acc: 0.9637681245803833)
[2025-02-13 20:33:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:40][root][INFO] - Training Epoch: 2/2, step 4734/7134 completed (loss: 0.25299832224845886, acc: 0.9622641801834106)
[2025-02-13 20:33:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:40][root][INFO] - Training Epoch: 2/2, step 4735/7134 completed (loss: 0.08461479842662811, acc: 0.9811320900917053)
[2025-02-13 20:33:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:41][root][INFO] - Training Epoch: 2/2, step 4736/7134 completed (loss: 0.2693217992782593, acc: 0.9416666626930237)
[2025-02-13 20:33:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:41][root][INFO] - Training Epoch: 2/2, step 4737/7134 completed (loss: 0.05302579700946808, acc: 1.0)
[2025-02-13 20:33:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:41][root][INFO] - Training Epoch: 2/2, step 4738/7134 completed (loss: 0.08312197029590607, acc: 0.981249988079071)
[2025-02-13 20:33:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:42][root][INFO] - Training Epoch: 2/2, step 4739/7134 completed (loss: 0.3230834901332855, acc: 0.9269663095474243)
[2025-02-13 20:33:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:42][root][INFO] - Training Epoch: 2/2, step 4740/7134 completed (loss: 0.21996888518333435, acc: 0.9444444179534912)
[2025-02-13 20:33:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:43][root][INFO] - Training Epoch: 2/2, step 4741/7134 completed (loss: 0.18206079304218292, acc: 0.9642857313156128)
[2025-02-13 20:33:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:43][root][INFO] - Training Epoch: 2/2, step 4742/7134 completed (loss: 0.3986891806125641, acc: 0.9333333373069763)
[2025-02-13 20:33:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:44][root][INFO] - Training Epoch: 2/2, step 4743/7134 completed (loss: 0.14869476854801178, acc: 0.9605262875556946)
[2025-02-13 20:33:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:44][root][INFO] - Training Epoch: 2/2, step 4744/7134 completed (loss: 0.08852770924568176, acc: 0.9908257126808167)
[2025-02-13 20:33:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:44][root][INFO] - Training Epoch: 2/2, step 4745/7134 completed (loss: 0.07243433594703674, acc: 0.9861111044883728)
[2025-02-13 20:33:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:45][root][INFO] - Training Epoch: 2/2, step 4746/7134 completed (loss: 0.0603879950940609, acc: 0.9922480583190918)
[2025-02-13 20:33:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:45][root][INFO] - Training Epoch: 2/2, step 4747/7134 completed (loss: 0.06466484814882278, acc: 0.9767441749572754)
[2025-02-13 20:33:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:46][root][INFO] - Training Epoch: 2/2, step 4748/7134 completed (loss: 0.15513622760772705, acc: 0.9585798978805542)
[2025-02-13 20:33:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:46][root][INFO] - Training Epoch: 2/2, step 4749/7134 completed (loss: 0.0362992025911808, acc: 0.9941860437393188)
[2025-02-13 20:33:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:46][root][INFO] - Training Epoch: 2/2, step 4750/7134 completed (loss: 0.09809727966785431, acc: 0.9837398529052734)
[2025-02-13 20:33:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:47][root][INFO] - Training Epoch: 2/2, step 4751/7134 completed (loss: 0.06236198917031288, acc: 0.9878048896789551)
[2025-02-13 20:33:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:47][root][INFO] - Training Epoch: 2/2, step 4752/7134 completed (loss: 0.15146951377391815, acc: 0.9627329111099243)
[2025-02-13 20:33:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:48][root][INFO] - Training Epoch: 2/2, step 4753/7134 completed (loss: 0.1385725438594818, acc: 0.9585798978805542)
[2025-02-13 20:33:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:48][root][INFO] - Training Epoch: 2/2, step 4754/7134 completed (loss: 0.06115014851093292, acc: 0.9870129823684692)
[2025-02-13 20:33:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:48][root][INFO] - Training Epoch: 2/2, step 4755/7134 completed (loss: 0.17742331326007843, acc: 0.9615384340286255)
[2025-02-13 20:33:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:49][root][INFO] - Training Epoch: 2/2, step 4756/7134 completed (loss: 0.04778274893760681, acc: 0.9919354915618896)
[2025-02-13 20:33:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:49][root][INFO] - Training Epoch: 2/2, step 4757/7134 completed (loss: 0.0764160305261612, acc: 0.9850000143051147)
[2025-02-13 20:33:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:50][root][INFO] - Training Epoch: 2/2, step 4758/7134 completed (loss: 0.05132303014397621, acc: 0.988950252532959)
[2025-02-13 20:33:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:50][root][INFO] - Training Epoch: 2/2, step 4759/7134 completed (loss: 0.0482526496052742, acc: 0.978723406791687)
[2025-02-13 20:33:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:50][root][INFO] - Training Epoch: 2/2, step 4760/7134 completed (loss: 0.07736965268850327, acc: 0.9831932783126831)
[2025-02-13 20:33:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:51][root][INFO] - Training Epoch: 2/2, step 4761/7134 completed (loss: 0.12100151926279068, acc: 0.9756097793579102)
[2025-02-13 20:33:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:51][root][INFO] - Training Epoch: 2/2, step 4762/7134 completed (loss: 0.2430988997220993, acc: 0.9615384340286255)
[2025-02-13 20:33:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:52][root][INFO] - Training Epoch: 2/2, step 4763/7134 completed (loss: 0.08371064066886902, acc: 0.9801324605941772)
[2025-02-13 20:33:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:52][root][INFO] - Training Epoch: 2/2, step 4764/7134 completed (loss: 0.13894838094711304, acc: 0.9845361113548279)
[2025-02-13 20:33:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:52][root][INFO] - Training Epoch: 2/2, step 4765/7134 completed (loss: 0.2423832267522812, acc: 0.9599999785423279)
[2025-02-13 20:33:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:53][root][INFO] - Training Epoch: 2/2, step 4766/7134 completed (loss: 0.09815210849046707, acc: 0.9833333492279053)
[2025-02-13 20:33:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:53][root][INFO] - Training Epoch: 2/2, step 4767/7134 completed (loss: 0.04834138974547386, acc: 0.9897959232330322)
[2025-02-13 20:33:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:54][root][INFO] - Training Epoch: 2/2, step 4768/7134 completed (loss: 0.04403176158666611, acc: 0.9882352948188782)
[2025-02-13 20:33:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:54][root][INFO] - Training Epoch: 2/2, step 4769/7134 completed (loss: 0.11118286848068237, acc: 0.9736841917037964)
[2025-02-13 20:33:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:54][root][INFO] - Training Epoch: 2/2, step 4770/7134 completed (loss: 0.061091676354408264, acc: 0.9919999837875366)
[2025-02-13 20:33:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:55][root][INFO] - Training Epoch: 2/2, step 4771/7134 completed (loss: 0.2608170807361603, acc: 0.9631901979446411)
[2025-02-13 20:33:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:55][root][INFO] - Training Epoch: 2/2, step 4772/7134 completed (loss: 0.15195578336715698, acc: 0.9806451797485352)
[2025-02-13 20:33:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:56][root][INFO] - Training Epoch: 2/2, step 4773/7134 completed (loss: 0.03922009468078613, acc: 1.0)
[2025-02-13 20:33:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:56][root][INFO] - Training Epoch: 2/2, step 4774/7134 completed (loss: 0.09430790692567825, acc: 0.9881656765937805)
[2025-02-13 20:33:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:56][root][INFO] - Training Epoch: 2/2, step 4775/7134 completed (loss: 0.14625883102416992, acc: 0.9685534834861755)
[2025-02-13 20:33:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:57][root][INFO] - Training Epoch: 2/2, step 4776/7134 completed (loss: 0.15693801641464233, acc: 0.9647887349128723)
[2025-02-13 20:33:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:57][root][INFO] - Training Epoch: 2/2, step 4777/7134 completed (loss: 0.13720464706420898, acc: 0.9726027250289917)
[2025-02-13 20:33:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:58][root][INFO] - Training Epoch: 2/2, step 4778/7134 completed (loss: 0.1041523665189743, acc: 0.9732142686843872)
[2025-02-13 20:33:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:58][root][INFO] - Training Epoch: 2/2, step 4779/7134 completed (loss: 0.04136167839169502, acc: 0.9777777791023254)
[2025-02-13 20:33:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:58][root][INFO] - Training Epoch: 2/2, step 4780/7134 completed (loss: 0.020558953285217285, acc: 1.0)
[2025-02-13 20:33:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:59][root][INFO] - Training Epoch: 2/2, step 4781/7134 completed (loss: 0.049314238131046295, acc: 0.9919354915618896)
[2025-02-13 20:33:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:33:59][root][INFO] - Training Epoch: 2/2, step 4782/7134 completed (loss: 0.05761973187327385, acc: 0.9937499761581421)
[2025-02-13 20:33:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:00][root][INFO] - Training Epoch: 2/2, step 4783/7134 completed (loss: 0.2339557409286499, acc: 0.9420289993286133)
[2025-02-13 20:34:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:00][root][INFO] - Training Epoch: 2/2, step 4784/7134 completed (loss: 0.056472424417734146, acc: 0.9833333492279053)
[2025-02-13 20:34:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:00][root][INFO] - Training Epoch: 2/2, step 4785/7134 completed (loss: 0.09958598762750626, acc: 0.9850746393203735)
[2025-02-13 20:34:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:01][root][INFO] - Training Epoch: 2/2, step 4786/7134 completed (loss: 0.05096178129315376, acc: 0.9876543283462524)
[2025-02-13 20:34:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:01][root][INFO] - Training Epoch: 2/2, step 4787/7134 completed (loss: 0.03756855055689812, acc: 0.9937106966972351)
[2025-02-13 20:34:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:02][root][INFO] - Training Epoch: 2/2, step 4788/7134 completed (loss: 0.014668946154415607, acc: 1.0)
[2025-02-13 20:34:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:02][root][INFO] - Training Epoch: 2/2, step 4789/7134 completed (loss: 0.07963444292545319, acc: 0.9935897588729858)
[2025-02-13 20:34:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:02][root][INFO] - Training Epoch: 2/2, step 4790/7134 completed (loss: 0.08516518771648407, acc: 0.9887640476226807)
[2025-02-13 20:34:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:03][root][INFO] - Training Epoch: 2/2, step 4791/7134 completed (loss: 0.017829151824116707, acc: 1.0)
[2025-02-13 20:34:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:03][root][INFO] - Training Epoch: 2/2, step 4792/7134 completed (loss: 0.04408036172389984, acc: 0.9932432174682617)
[2025-02-13 20:34:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:04][root][INFO] - Training Epoch: 2/2, step 4793/7134 completed (loss: 0.08831208199262619, acc: 0.9789473414421082)
[2025-02-13 20:34:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:04][root][INFO] - Training Epoch: 2/2, step 4794/7134 completed (loss: 0.10964690893888474, acc: 0.978723406791687)
[2025-02-13 20:34:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:05][root][INFO] - Training Epoch: 2/2, step 4795/7134 completed (loss: 0.04916663095355034, acc: 0.987730085849762)
[2025-02-13 20:34:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:05][root][INFO] - Training Epoch: 2/2, step 4796/7134 completed (loss: 0.14044898748397827, acc: 0.9605262875556946)
[2025-02-13 20:34:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:05][root][INFO] - Training Epoch: 2/2, step 4797/7134 completed (loss: 0.055035367608070374, acc: 0.9781420826911926)
[2025-02-13 20:34:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:06][root][INFO] - Training Epoch: 2/2, step 4798/7134 completed (loss: 0.09500739723443985, acc: 0.9940119981765747)
[2025-02-13 20:34:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:06][root][INFO] - Training Epoch: 2/2, step 4799/7134 completed (loss: 0.1496509313583374, acc: 0.9602272510528564)
[2025-02-13 20:34:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:06][root][INFO] - Training Epoch: 2/2, step 4800/7134 completed (loss: 0.08091752231121063, acc: 0.9736841917037964)
[2025-02-13 20:34:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:07][root][INFO] - Training Epoch: 2/2, step 4801/7134 completed (loss: 0.052688706666231155, acc: 0.9897959232330322)
[2025-02-13 20:34:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:07][root][INFO] - Training Epoch: 2/2, step 4802/7134 completed (loss: 0.15114468336105347, acc: 0.9587628841400146)
[2025-02-13 20:34:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:08][root][INFO] - Training Epoch: 2/2, step 4803/7134 completed (loss: 0.09073597937822342, acc: 0.9757575988769531)
[2025-02-13 20:34:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:08][root][INFO] - Training Epoch: 2/2, step 4804/7134 completed (loss: 0.13594815135002136, acc: 0.9518072009086609)
[2025-02-13 20:34:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:09][root][INFO] - Training Epoch: 2/2, step 4805/7134 completed (loss: 0.09701129794120789, acc: 0.9801324605941772)
[2025-02-13 20:34:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:09][root][INFO] - Training Epoch: 2/2, step 4806/7134 completed (loss: 0.13464608788490295, acc: 0.9860140085220337)
[2025-02-13 20:34:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:09][root][INFO] - Training Epoch: 2/2, step 4807/7134 completed (loss: 0.12085747718811035, acc: 0.9746835231781006)
[2025-02-13 20:34:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:10][root][INFO] - Training Epoch: 2/2, step 4808/7134 completed (loss: 0.11647935956716537, acc: 0.9772727489471436)
[2025-02-13 20:34:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:10][root][INFO] - Training Epoch: 2/2, step 4809/7134 completed (loss: 0.08945394307374954, acc: 0.9793103337287903)
[2025-02-13 20:34:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:11][root][INFO] - Training Epoch: 2/2, step 4810/7134 completed (loss: 0.12191451340913773, acc: 0.9588235020637512)
[2025-02-13 20:34:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:11][root][INFO] - Training Epoch: 2/2, step 4811/7134 completed (loss: 0.07910361140966415, acc: 0.9918032884597778)
[2025-02-13 20:34:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:12][root][INFO] - Training Epoch: 2/2, step 4812/7134 completed (loss: 0.1675819605588913, acc: 0.9717513918876648)
[2025-02-13 20:34:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:12][root][INFO] - Training Epoch: 2/2, step 4813/7134 completed (loss: 0.023646220564842224, acc: 1.0)
[2025-02-13 20:34:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:12][root][INFO] - Training Epoch: 2/2, step 4814/7134 completed (loss: 0.051765326410532, acc: 0.989130437374115)
[2025-02-13 20:34:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:13][root][INFO] - Training Epoch: 2/2, step 4815/7134 completed (loss: 0.007320694625377655, acc: 1.0)
[2025-02-13 20:34:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:13][root][INFO] - Training Epoch: 2/2, step 4816/7134 completed (loss: 0.018818149343132973, acc: 1.0)
[2025-02-13 20:34:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:13][root][INFO] - Training Epoch: 2/2, step 4817/7134 completed (loss: 0.06343507021665573, acc: 0.9797979593276978)
[2025-02-13 20:34:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:14][root][INFO] - Training Epoch: 2/2, step 4818/7134 completed (loss: 0.027835220098495483, acc: 1.0)
[2025-02-13 20:34:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:14][root][INFO] - Training Epoch: 2/2, step 4819/7134 completed (loss: 0.027809087187051773, acc: 0.9916666746139526)
[2025-02-13 20:34:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:15][root][INFO] - Training Epoch: 2/2, step 4820/7134 completed (loss: 0.05134342983365059, acc: 0.9946523904800415)
[2025-02-13 20:34:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:15][root][INFO] - Training Epoch: 2/2, step 4821/7134 completed (loss: 0.07339915633201599, acc: 0.9826589822769165)
[2025-02-13 20:34:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:15][root][INFO] - Training Epoch: 2/2, step 4822/7134 completed (loss: 0.024339113384485245, acc: 0.9926470518112183)
[2025-02-13 20:34:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:16][root][INFO] - Training Epoch: 2/2, step 4823/7134 completed (loss: 0.16873517632484436, acc: 0.9622641801834106)
[2025-02-13 20:34:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:16][root][INFO] - Training Epoch: 2/2, step 4824/7134 completed (loss: 0.07411430776119232, acc: 0.991525411605835)
[2025-02-13 20:34:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:17][root][INFO] - Training Epoch: 2/2, step 4825/7134 completed (loss: 0.026308991014957428, acc: 0.9926470518112183)
[2025-02-13 20:34:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:17][root][INFO] - Training Epoch: 2/2, step 4826/7134 completed (loss: 0.04111533612012863, acc: 1.0)
[2025-02-13 20:34:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:17][root][INFO] - Training Epoch: 2/2, step 4827/7134 completed (loss: 0.19307732582092285, acc: 0.9677419066429138)
[2025-02-13 20:34:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:18][root][INFO] - Training Epoch: 2/2, step 4828/7134 completed (loss: 0.040155068039894104, acc: 0.9948979616165161)
[2025-02-13 20:34:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:18][root][INFO] - Training Epoch: 2/2, step 4829/7134 completed (loss: 0.03615045174956322, acc: 0.9801980257034302)
[2025-02-13 20:34:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:19][root][INFO] - Training Epoch: 2/2, step 4830/7134 completed (loss: 0.05799464136362076, acc: 0.9878787994384766)
[2025-02-13 20:34:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:19][root][INFO] - Training Epoch: 2/2, step 4831/7134 completed (loss: 0.010516139678657055, acc: 1.0)
[2025-02-13 20:34:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:19][root][INFO] - Training Epoch: 2/2, step 4832/7134 completed (loss: 0.023435765877366066, acc: 0.9869281053543091)
[2025-02-13 20:34:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:20][root][INFO] - Training Epoch: 2/2, step 4833/7134 completed (loss: 0.05228816345334053, acc: 0.9847715497016907)
[2025-02-13 20:34:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:20][root][INFO] - Training Epoch: 2/2, step 4834/7134 completed (loss: 0.01732245646417141, acc: 1.0)
[2025-02-13 20:34:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:21][root][INFO] - Training Epoch: 2/2, step 4835/7134 completed (loss: 0.07111635059118271, acc: 0.9794871807098389)
[2025-02-13 20:34:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:21][root][INFO] - Training Epoch: 2/2, step 4836/7134 completed (loss: 0.053313639014959335, acc: 0.9876543283462524)
[2025-02-13 20:34:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:21][root][INFO] - Training Epoch: 2/2, step 4837/7134 completed (loss: 0.20849627256393433, acc: 0.9571428298950195)
[2025-02-13 20:34:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:22][root][INFO] - Training Epoch: 2/2, step 4838/7134 completed (loss: 0.1821308732032776, acc: 0.9784172773361206)
[2025-02-13 20:34:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:22][root][INFO] - Training Epoch: 2/2, step 4839/7134 completed (loss: 0.1121174618601799, acc: 0.9689922332763672)
[2025-02-13 20:34:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:23][root][INFO] - Training Epoch: 2/2, step 4840/7134 completed (loss: 0.09116983413696289, acc: 0.9802631735801697)
[2025-02-13 20:34:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:23][root][INFO] - Training Epoch: 2/2, step 4841/7134 completed (loss: 0.04719875380396843, acc: 0.9861111044883728)
[2025-02-13 20:34:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:23][root][INFO] - Training Epoch: 2/2, step 4842/7134 completed (loss: 0.0847773477435112, acc: 0.9833333492279053)
[2025-02-13 20:34:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:24][root][INFO] - Training Epoch: 2/2, step 4843/7134 completed (loss: 0.18453040719032288, acc: 0.9371069073677063)
[2025-02-13 20:34:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:24][root][INFO] - Training Epoch: 2/2, step 4844/7134 completed (loss: 0.055280622094869614, acc: 0.9923076629638672)
[2025-02-13 20:34:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:25][root][INFO] - Training Epoch: 2/2, step 4845/7134 completed (loss: 0.12707485258579254, acc: 0.96875)
[2025-02-13 20:34:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:25][root][INFO] - Training Epoch: 2/2, step 4846/7134 completed (loss: 0.10654185712337494, acc: 0.9750000238418579)
[2025-02-13 20:34:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:25][root][INFO] - Training Epoch: 2/2, step 4847/7134 completed (loss: 0.0718240737915039, acc: 0.976331353187561)
[2025-02-13 20:34:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:26][root][INFO] - Training Epoch: 2/2, step 4848/7134 completed (loss: 0.12054388970136642, acc: 0.9753086566925049)
[2025-02-13 20:34:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:26][root][INFO] - Training Epoch: 2/2, step 4849/7134 completed (loss: 0.09561267495155334, acc: 0.9794520735740662)
[2025-02-13 20:34:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:27][root][INFO] - Training Epoch: 2/2, step 4850/7134 completed (loss: 0.15199707448482513, acc: 0.9553072452545166)
[2025-02-13 20:34:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:27][root][INFO] - Training Epoch: 2/2, step 4851/7134 completed (loss: 0.06988320499658585, acc: 0.9837837815284729)
[2025-02-13 20:34:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:27][root][INFO] - Training Epoch: 2/2, step 4852/7134 completed (loss: 0.14109820127487183, acc: 0.9651162624359131)
[2025-02-13 20:34:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:28][root][INFO] - Training Epoch: 2/2, step 4853/7134 completed (loss: 0.26978620886802673, acc: 0.946107804775238)
[2025-02-13 20:34:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:28][root][INFO] - Training Epoch: 2/2, step 4854/7134 completed (loss: 0.06751912087202072, acc: 0.9865771532058716)
[2025-02-13 20:34:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:29][root][INFO] - Training Epoch: 2/2, step 4855/7134 completed (loss: 0.24026554822921753, acc: 0.977011501789093)
[2025-02-13 20:34:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:29][root][INFO] - Training Epoch: 2/2, step 4856/7134 completed (loss: 0.06702899187803268, acc: 0.988095223903656)
[2025-02-13 20:34:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:29][root][INFO] - Training Epoch: 2/2, step 4857/7134 completed (loss: 0.07106221467256546, acc: 0.9814814925193787)
[2025-02-13 20:34:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:30][root][INFO] - Training Epoch: 2/2, step 4858/7134 completed (loss: 0.1135401576757431, acc: 0.9691358208656311)
[2025-02-13 20:34:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:30][root][INFO] - Training Epoch: 2/2, step 4859/7134 completed (loss: 0.053541723638772964, acc: 1.0)
[2025-02-13 20:34:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:31][root][INFO] - Training Epoch: 2/2, step 4860/7134 completed (loss: 0.07655047625303268, acc: 0.988304078578949)
[2025-02-13 20:34:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:31][root][INFO] - Training Epoch: 2/2, step 4861/7134 completed (loss: 0.10055405646562576, acc: 0.9822485446929932)
[2025-02-13 20:34:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:31][root][INFO] - Training Epoch: 2/2, step 4862/7134 completed (loss: 0.0848466232419014, acc: 0.9868420958518982)
[2025-02-13 20:34:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:32][root][INFO] - Training Epoch: 2/2, step 4863/7134 completed (loss: 0.06395375728607178, acc: 0.9844961166381836)
[2025-02-13 20:34:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:32][root][INFO] - Training Epoch: 2/2, step 4864/7134 completed (loss: 0.04961322247982025, acc: 0.993630588054657)
[2025-02-13 20:34:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:33][root][INFO] - Training Epoch: 2/2, step 4865/7134 completed (loss: 0.14889955520629883, acc: 0.9677419066429138)
[2025-02-13 20:34:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:33][root][INFO] - Training Epoch: 2/2, step 4866/7134 completed (loss: 0.07054783403873444, acc: 0.9767441749572754)
[2025-02-13 20:34:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:33][root][INFO] - Training Epoch: 2/2, step 4867/7134 completed (loss: 0.06449208408594131, acc: 0.9832402467727661)
[2025-02-13 20:34:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:34][root][INFO] - Training Epoch: 2/2, step 4868/7134 completed (loss: 0.1220477893948555, acc: 0.9663865566253662)
[2025-02-13 20:34:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:34][root][INFO] - Training Epoch: 2/2, step 4869/7134 completed (loss: 0.1011224016547203, acc: 0.9795918464660645)
[2025-02-13 20:34:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:35][root][INFO] - Training Epoch: 2/2, step 4870/7134 completed (loss: 0.051281511783599854, acc: 0.9891892075538635)
[2025-02-13 20:34:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:35][root][INFO] - Training Epoch: 2/2, step 4871/7134 completed (loss: 0.06148834526538849, acc: 0.9868420958518982)
[2025-02-13 20:34:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:35][root][INFO] - Training Epoch: 2/2, step 4872/7134 completed (loss: 0.013510570861399174, acc: 1.0)
[2025-02-13 20:34:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:36][root][INFO] - Training Epoch: 2/2, step 4873/7134 completed (loss: 0.061871957033872604, acc: 0.9836065769195557)
[2025-02-13 20:34:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:36][root][INFO] - Training Epoch: 2/2, step 4874/7134 completed (loss: 0.12312806397676468, acc: 0.9756097793579102)
[2025-02-13 20:34:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:37][root][INFO] - Training Epoch: 2/2, step 4875/7134 completed (loss: 0.08224636316299438, acc: 0.9813664555549622)
[2025-02-13 20:34:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:37][root][INFO] - Training Epoch: 2/2, step 4876/7134 completed (loss: 0.08932787925004959, acc: 0.9907407164573669)
[2025-02-13 20:34:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:37][root][INFO] - Training Epoch: 2/2, step 4877/7134 completed (loss: 0.07733442634344101, acc: 0.9846153855323792)
[2025-02-13 20:34:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:38][root][INFO] - Training Epoch: 2/2, step 4878/7134 completed (loss: 0.061234522610902786, acc: 0.9870129823684692)
[2025-02-13 20:34:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:38][root][INFO] - Training Epoch: 2/2, step 4879/7134 completed (loss: 0.0433596707880497, acc: 0.9935897588729858)
[2025-02-13 20:34:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:38][root][INFO] - Training Epoch: 2/2, step 4880/7134 completed (loss: 0.024981984868645668, acc: 1.0)
[2025-02-13 20:34:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:39][root][INFO] - Training Epoch: 2/2, step 4881/7134 completed (loss: 0.14870312809944153, acc: 0.9798657894134521)
[2025-02-13 20:34:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:39][root][INFO] - Training Epoch: 2/2, step 4882/7134 completed (loss: 0.04371105134487152, acc: 0.9935483932495117)
[2025-02-13 20:34:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:40][root][INFO] - Training Epoch: 2/2, step 4883/7134 completed (loss: 0.041504546999931335, acc: 0.9933333396911621)
[2025-02-13 20:34:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:40][root][INFO] - Training Epoch: 2/2, step 4884/7134 completed (loss: 0.046520888805389404, acc: 0.9803921580314636)
[2025-02-13 20:34:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:40][root][INFO] - Training Epoch: 2/2, step 4885/7134 completed (loss: 0.024857111275196075, acc: 1.0)
[2025-02-13 20:34:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:41][root][INFO] - Training Epoch: 2/2, step 4886/7134 completed (loss: 0.03882940858602524, acc: 0.9903846383094788)
[2025-02-13 20:34:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:41][root][INFO] - Training Epoch: 2/2, step 4887/7134 completed (loss: 0.066330686211586, acc: 0.9738219976425171)
[2025-02-13 20:34:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:41][root][INFO] - Training Epoch: 2/2, step 4888/7134 completed (loss: 0.051616039127111435, acc: 0.9890109896659851)
[2025-02-13 20:34:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:42][root][INFO] - Training Epoch: 2/2, step 4889/7134 completed (loss: 0.01584254764020443, acc: 1.0)
[2025-02-13 20:34:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:42][root][INFO] - Training Epoch: 2/2, step 4890/7134 completed (loss: 0.057609379291534424, acc: 0.9811320900917053)
[2025-02-13 20:34:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:43][root][INFO] - Training Epoch: 2/2, step 4891/7134 completed (loss: 0.08945067226886749, acc: 0.9693877696990967)
[2025-02-13 20:34:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:43][root][INFO] - Training Epoch: 2/2, step 4892/7134 completed (loss: 0.0298018641769886, acc: 0.9935064911842346)
[2025-02-13 20:34:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:43][root][INFO] - Training Epoch: 2/2, step 4893/7134 completed (loss: 0.4169425964355469, acc: 0.9137930870056152)
[2025-02-13 20:34:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:44][root][INFO] - Training Epoch: 2/2, step 4894/7134 completed (loss: 0.04861236363649368, acc: 0.9941176176071167)
[2025-02-13 20:34:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:44][root][INFO] - Training Epoch: 2/2, step 4895/7134 completed (loss: 0.01845710165798664, acc: 1.0)
[2025-02-13 20:34:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:45][root][INFO] - Training Epoch: 2/2, step 4896/7134 completed (loss: 0.1168968677520752, acc: 0.9832402467727661)
[2025-02-13 20:34:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:45][root][INFO] - Training Epoch: 2/2, step 4897/7134 completed (loss: 0.0783352479338646, acc: 0.9753694534301758)
[2025-02-13 20:34:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:45][root][INFO] - Training Epoch: 2/2, step 4898/7134 completed (loss: 0.03186074271798134, acc: 0.9946236610412598)
[2025-02-13 20:34:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:46][root][INFO] - Training Epoch: 2/2, step 4899/7134 completed (loss: 0.06673336774110794, acc: 0.9817073345184326)
[2025-02-13 20:34:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:46][root][INFO] - Training Epoch: 2/2, step 4900/7134 completed (loss: 0.03948735073208809, acc: 0.9839572310447693)
[2025-02-13 20:34:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:47][root][INFO] - Training Epoch: 2/2, step 4901/7134 completed (loss: 0.041344206780195236, acc: 0.9878048896789551)
[2025-02-13 20:34:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:47][root][INFO] - Training Epoch: 2/2, step 4902/7134 completed (loss: 0.045736733824014664, acc: 0.9846153855323792)
[2025-02-13 20:34:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:47][root][INFO] - Training Epoch: 2/2, step 4903/7134 completed (loss: 0.07960515469312668, acc: 0.9781022071838379)
[2025-02-13 20:34:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:48][root][INFO] - Training Epoch: 2/2, step 4904/7134 completed (loss: 0.026378637179732323, acc: 0.9882352948188782)
[2025-02-13 20:34:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:48][root][INFO] - Training Epoch: 2/2, step 4905/7134 completed (loss: 0.04607006907463074, acc: 0.9934640526771545)
[2025-02-13 20:34:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:49][root][INFO] - Training Epoch: 2/2, step 4906/7134 completed (loss: 0.022005293518304825, acc: 0.9924812316894531)
[2025-02-13 20:34:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:49][root][INFO] - Training Epoch: 2/2, step 4907/7134 completed (loss: 0.0227265115827322, acc: 0.9931972622871399)
[2025-02-13 20:34:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:50][root][INFO] - Training Epoch: 2/2, step 4908/7134 completed (loss: 0.00827573798596859, acc: 1.0)
[2025-02-13 20:34:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:50][root][INFO] - Training Epoch: 2/2, step 4909/7134 completed (loss: 0.01689182221889496, acc: 0.9932885766029358)
[2025-02-13 20:34:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:50][root][INFO] - Training Epoch: 2/2, step 4910/7134 completed (loss: 0.09525034576654434, acc: 0.9870967864990234)
[2025-02-13 20:34:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:51][root][INFO] - Training Epoch: 2/2, step 4911/7134 completed (loss: 0.021518321707844734, acc: 1.0)
[2025-02-13 20:34:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:51][root][INFO] - Training Epoch: 2/2, step 4912/7134 completed (loss: 0.05738382041454315, acc: 0.9751243591308594)
[2025-02-13 20:34:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:52][root][INFO] - Training Epoch: 2/2, step 4913/7134 completed (loss: 0.061669956892728806, acc: 0.9860140085220337)
[2025-02-13 20:34:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:52][root][INFO] - Training Epoch: 2/2, step 4914/7134 completed (loss: 0.0632072314620018, acc: 0.9870967864990234)
[2025-02-13 20:34:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:53][root][INFO] - Training Epoch: 2/2, step 4915/7134 completed (loss: 0.007410313002765179, acc: 1.0)
[2025-02-13 20:34:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:53][root][INFO] - Training Epoch: 2/2, step 4916/7134 completed (loss: 0.07078023999929428, acc: 0.989130437374115)
[2025-02-13 20:34:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:53][root][INFO] - Training Epoch: 2/2, step 4917/7134 completed (loss: 0.03775271400809288, acc: 1.0)
[2025-02-13 20:34:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:54][root][INFO] - Training Epoch: 2/2, step 4918/7134 completed (loss: 0.016797015443444252, acc: 1.0)
[2025-02-13 20:34:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:54][root][INFO] - Training Epoch: 2/2, step 4919/7134 completed (loss: 0.026498522609472275, acc: 0.994350254535675)
[2025-02-13 20:34:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:55][root][INFO] - Training Epoch: 2/2, step 4920/7134 completed (loss: 0.05284687876701355, acc: 0.9937888383865356)
[2025-02-13 20:34:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:55][root][INFO] - Training Epoch: 2/2, step 4921/7134 completed (loss: 0.049661699682474136, acc: 0.9927007555961609)
[2025-02-13 20:34:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:56][root][INFO] - Training Epoch: 2/2, step 4922/7134 completed (loss: 0.05810566246509552, acc: 0.9813664555549622)
[2025-02-13 20:34:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:56][root][INFO] - Training Epoch: 2/2, step 4923/7134 completed (loss: 0.01730295456945896, acc: 1.0)
[2025-02-13 20:34:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:56][root][INFO] - Training Epoch: 2/2, step 4924/7134 completed (loss: 0.028495920822024345, acc: 1.0)
[2025-02-13 20:34:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:57][root][INFO] - Training Epoch: 2/2, step 4925/7134 completed (loss: 0.03640475496649742, acc: 1.0)
[2025-02-13 20:34:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:57][root][INFO] - Training Epoch: 2/2, step 4926/7134 completed (loss: 0.06151970475912094, acc: 0.9798657894134521)
[2025-02-13 20:34:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:58][root][INFO] - Training Epoch: 2/2, step 4927/7134 completed (loss: 0.03460916876792908, acc: 0.9870967864990234)
[2025-02-13 20:34:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:58][root][INFO] - Training Epoch: 2/2, step 4928/7134 completed (loss: 0.023842236027121544, acc: 0.9947368502616882)
[2025-02-13 20:34:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:58][root][INFO] - Training Epoch: 2/2, step 4929/7134 completed (loss: 0.04142334312200546, acc: 0.9934210777282715)
[2025-02-13 20:34:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:59][root][INFO] - Training Epoch: 2/2, step 4930/7134 completed (loss: 0.05613049119710922, acc: 0.9932432174682617)
[2025-02-13 20:34:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:34:59][root][INFO] - Training Epoch: 2/2, step 4931/7134 completed (loss: 0.051078181713819504, acc: 0.9815950989723206)
[2025-02-13 20:34:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:00][root][INFO] - Training Epoch: 2/2, step 4932/7134 completed (loss: 0.034819018095731735, acc: 0.9882352948188782)
[2025-02-13 20:35:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:00][root][INFO] - Training Epoch: 2/2, step 4933/7134 completed (loss: 0.10171714425086975, acc: 0.9685863852500916)
[2025-02-13 20:35:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:00][root][INFO] - Training Epoch: 2/2, step 4934/7134 completed (loss: 0.03359553962945938, acc: 0.9882352948188782)
[2025-02-13 20:35:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:01][root][INFO] - Training Epoch: 2/2, step 4935/7134 completed (loss: 0.06241822615265846, acc: 0.9823529124259949)
[2025-02-13 20:35:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:01][root][INFO] - Training Epoch: 2/2, step 4936/7134 completed (loss: 0.01723424345254898, acc: 1.0)
[2025-02-13 20:35:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:02][root][INFO] - Training Epoch: 2/2, step 4937/7134 completed (loss: 0.06141982227563858, acc: 0.9860140085220337)
[2025-02-13 20:35:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:02][root][INFO] - Training Epoch: 2/2, step 4938/7134 completed (loss: 0.0195015836507082, acc: 0.9938271641731262)
[2025-02-13 20:35:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:03][root][INFO] - Training Epoch: 2/2, step 4939/7134 completed (loss: 0.09387234598398209, acc: 0.9785714149475098)
[2025-02-13 20:35:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:03][root][INFO] - Training Epoch: 2/2, step 4940/7134 completed (loss: 0.09470496326684952, acc: 0.9922480583190918)
[2025-02-13 20:35:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:03][root][INFO] - Training Epoch: 2/2, step 4941/7134 completed (loss: 0.09174725413322449, acc: 0.976047933101654)
[2025-02-13 20:35:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:04][root][INFO] - Training Epoch: 2/2, step 4942/7134 completed (loss: 0.03869489207863808, acc: 0.9936708807945251)
[2025-02-13 20:35:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:04][root][INFO] - Training Epoch: 2/2, step 4943/7134 completed (loss: 0.08502165973186493, acc: 0.987500011920929)
[2025-02-13 20:35:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:05][root][INFO] - Training Epoch: 2/2, step 4944/7134 completed (loss: 0.1590786725282669, acc: 0.9663865566253662)
[2025-02-13 20:35:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:05][root][INFO] - Training Epoch: 2/2, step 4945/7134 completed (loss: 0.04419642686843872, acc: 0.9924242496490479)
[2025-02-13 20:35:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:05][root][INFO] - Training Epoch: 2/2, step 4946/7134 completed (loss: 0.10177605599164963, acc: 0.984375)
[2025-02-13 20:35:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:06][root][INFO] - Training Epoch: 2/2, step 4947/7134 completed (loss: 0.036734193563461304, acc: 1.0)
[2025-02-13 20:35:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:06][root][INFO] - Training Epoch: 2/2, step 4948/7134 completed (loss: 0.034416329115629196, acc: 1.0)
[2025-02-13 20:35:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:07][root][INFO] - Training Epoch: 2/2, step 4949/7134 completed (loss: 0.04189208149909973, acc: 0.9876543283462524)
[2025-02-13 20:35:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:07][root][INFO] - Training Epoch: 2/2, step 4950/7134 completed (loss: 0.022404689341783524, acc: 1.0)
[2025-02-13 20:35:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:07][root][INFO] - Training Epoch: 2/2, step 4951/7134 completed (loss: 0.056794531643390656, acc: 0.9887640476226807)
[2025-02-13 20:35:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:08][root][INFO] - Training Epoch: 2/2, step 4952/7134 completed (loss: 0.04402619227766991, acc: 0.9797297120094299)
[2025-02-13 20:35:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:08][root][INFO] - Training Epoch: 2/2, step 4953/7134 completed (loss: 0.035629209131002426, acc: 1.0)
[2025-02-13 20:35:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:09][root][INFO] - Training Epoch: 2/2, step 4954/7134 completed (loss: 0.04772505536675453, acc: 0.9795918464660645)
[2025-02-13 20:35:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:09][root][INFO] - Training Epoch: 2/2, step 4955/7134 completed (loss: 0.02926739491522312, acc: 1.0)
[2025-02-13 20:35:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:10][root][INFO] - Training Epoch: 2/2, step 4956/7134 completed (loss: 0.13321492075920105, acc: 0.9882352948188782)
[2025-02-13 20:35:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:10][root][INFO] - Training Epoch: 2/2, step 4957/7134 completed (loss: 0.09238685667514801, acc: 0.978723406791687)
[2025-02-13 20:35:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:10][root][INFO] - Training Epoch: 2/2, step 4958/7134 completed (loss: 0.0723898634314537, acc: 0.9685534834861755)
[2025-02-13 20:35:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:11][root][INFO] - Training Epoch: 2/2, step 4959/7134 completed (loss: 0.10202236473560333, acc: 0.9939758777618408)
[2025-02-13 20:35:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:11][root][INFO] - Training Epoch: 2/2, step 4960/7134 completed (loss: 0.06782156229019165, acc: 0.9779411554336548)
[2025-02-13 20:35:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:12][root][INFO] - Training Epoch: 2/2, step 4961/7134 completed (loss: 0.02833685837686062, acc: 1.0)
[2025-02-13 20:35:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:12][root][INFO] - Training Epoch: 2/2, step 4962/7134 completed (loss: 0.038304705172777176, acc: 0.9939024448394775)
[2025-02-13 20:35:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:13][root][INFO] - Training Epoch: 2/2, step 4963/7134 completed (loss: 0.03339505195617676, acc: 0.9870967864990234)
[2025-02-13 20:35:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:13][root][INFO] - Training Epoch: 2/2, step 4964/7134 completed (loss: 0.0832415223121643, acc: 0.9784172773361206)
[2025-02-13 20:35:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:13][root][INFO] - Training Epoch: 2/2, step 4965/7134 completed (loss: 0.016948629170656204, acc: 1.0)
[2025-02-13 20:35:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:14][root][INFO] - Training Epoch: 2/2, step 4966/7134 completed (loss: 0.10025998950004578, acc: 0.9560439586639404)
[2025-02-13 20:35:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:14][root][INFO] - Training Epoch: 2/2, step 4967/7134 completed (loss: 0.05800120159983635, acc: 0.9922480583190918)
[2025-02-13 20:35:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:15][root][INFO] - Training Epoch: 2/2, step 4968/7134 completed (loss: 0.18162578344345093, acc: 0.9503546357154846)
[2025-02-13 20:35:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:15][root][INFO] - Training Epoch: 2/2, step 4969/7134 completed (loss: 0.12324874848127365, acc: 0.9793103337287903)
[2025-02-13 20:35:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:15][root][INFO] - Training Epoch: 2/2, step 4970/7134 completed (loss: 0.03703518584370613, acc: 0.9931034445762634)
[2025-02-13 20:35:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:16][root][INFO] - Training Epoch: 2/2, step 4971/7134 completed (loss: 0.08684747666120529, acc: 0.9735099077224731)
[2025-02-13 20:35:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:16][root][INFO] - Training Epoch: 2/2, step 4972/7134 completed (loss: 0.10778941214084625, acc: 0.9624999761581421)
[2025-02-13 20:35:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:17][root][INFO] - Training Epoch: 2/2, step 4973/7134 completed (loss: 0.06137428805232048, acc: 0.9924242496490479)
[2025-02-13 20:35:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:17][root][INFO] - Training Epoch: 2/2, step 4974/7134 completed (loss: 0.06449099630117416, acc: 0.9848484992980957)
[2025-02-13 20:35:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:17][root][INFO] - Training Epoch: 2/2, step 4975/7134 completed (loss: 0.20087993144989014, acc: 0.9655172228813171)
[2025-02-13 20:35:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:18][root][INFO] - Training Epoch: 2/2, step 4976/7134 completed (loss: 0.09115822613239288, acc: 0.9789473414421082)
[2025-02-13 20:35:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:18][root][INFO] - Training Epoch: 2/2, step 4977/7134 completed (loss: 0.18958701193332672, acc: 0.9805825352668762)
[2025-02-13 20:35:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:19][root][INFO] - Training Epoch: 2/2, step 4978/7134 completed (loss: 0.05030657723546028, acc: 0.9942528605461121)
[2025-02-13 20:35:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:19][root][INFO] - Training Epoch: 2/2, step 4979/7134 completed (loss: 0.29288846254348755, acc: 0.9440559148788452)
[2025-02-13 20:35:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:19][root][INFO] - Training Epoch: 2/2, step 4980/7134 completed (loss: 0.1369961053133011, acc: 0.9714285731315613)
[2025-02-13 20:35:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:20][root][INFO] - Training Epoch: 2/2, step 4981/7134 completed (loss: 0.07078589498996735, acc: 0.9785714149475098)
[2025-02-13 20:35:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:20][root][INFO] - Training Epoch: 2/2, step 4982/7134 completed (loss: 0.2266278862953186, acc: 0.954954981803894)
[2025-02-13 20:35:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:21][root][INFO] - Training Epoch: 2/2, step 4983/7134 completed (loss: 0.10950552672147751, acc: 0.9784172773361206)
[2025-02-13 20:35:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:21][root][INFO] - Training Epoch: 2/2, step 4984/7134 completed (loss: 0.08759310841560364, acc: 0.9811320900917053)
[2025-02-13 20:35:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:22][root][INFO] - Training Epoch: 2/2, step 4985/7134 completed (loss: 0.04698202759027481, acc: 0.9864864945411682)
[2025-02-13 20:35:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:22][root][INFO] - Training Epoch: 2/2, step 4986/7134 completed (loss: 0.07370390743017197, acc: 0.9860140085220337)
[2025-02-13 20:35:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:22][root][INFO] - Training Epoch: 2/2, step 4987/7134 completed (loss: 0.021304523572325706, acc: 1.0)
[2025-02-13 20:35:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:23][root][INFO] - Training Epoch: 2/2, step 4988/7134 completed (loss: 0.025361262261867523, acc: 0.9913793206214905)
[2025-02-13 20:35:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:23][root][INFO] - Training Epoch: 2/2, step 4989/7134 completed (loss: 0.032316263765096664, acc: 0.9882352948188782)
[2025-02-13 20:35:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:24][root][INFO] - Training Epoch: 2/2, step 4990/7134 completed (loss: 0.03446061909198761, acc: 1.0)
[2025-02-13 20:35:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:24][root][INFO] - Training Epoch: 2/2, step 4991/7134 completed (loss: 0.06403444707393646, acc: 0.9849624037742615)
[2025-02-13 20:35:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:24][root][INFO] - Training Epoch: 2/2, step 4992/7134 completed (loss: 0.09457670152187347, acc: 0.982758641242981)
[2025-02-13 20:35:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:25][root][INFO] - Training Epoch: 2/2, step 4993/7134 completed (loss: 0.011220894753932953, acc: 1.0)
[2025-02-13 20:35:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:25][root][INFO] - Training Epoch: 2/2, step 4994/7134 completed (loss: 0.04717228189110756, acc: 0.9867549538612366)
[2025-02-13 20:35:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:26][root][INFO] - Training Epoch: 2/2, step 4995/7134 completed (loss: 0.03716525062918663, acc: 0.9814814925193787)
[2025-02-13 20:35:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:26][root][INFO] - Training Epoch: 2/2, step 4996/7134 completed (loss: 0.050885383039712906, acc: 0.9916666746139526)
[2025-02-13 20:35:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:26][root][INFO] - Training Epoch: 2/2, step 4997/7134 completed (loss: 0.07126209884881973, acc: 0.9770992398262024)
[2025-02-13 20:35:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:27][root][INFO] - Training Epoch: 2/2, step 4998/7134 completed (loss: 0.06080494076013565, acc: 0.9866666793823242)
[2025-02-13 20:35:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:27][root][INFO] - Training Epoch: 2/2, step 4999/7134 completed (loss: 0.032825760543346405, acc: 0.9921259880065918)
[2025-02-13 20:35:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:28][root][INFO] - Training Epoch: 2/2, step 5000/7134 completed (loss: 0.050584472715854645, acc: 0.9896907210350037)
[2025-02-13 20:35:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:28][root][INFO] - Training Epoch: 2/2, step 5001/7134 completed (loss: 0.1453009843826294, acc: 0.9795918464660645)
[2025-02-13 20:35:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:28][root][INFO] - Training Epoch: 2/2, step 5002/7134 completed (loss: 0.04387336224317551, acc: 0.991150438785553)
[2025-02-13 20:35:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:29][root][INFO] - Training Epoch: 2/2, step 5003/7134 completed (loss: 0.03901698440313339, acc: 0.9925925731658936)
[2025-02-13 20:35:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:29][root][INFO] - Training Epoch: 2/2, step 5004/7134 completed (loss: 0.018375176936388016, acc: 1.0)
[2025-02-13 20:35:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:30][root][INFO] - Training Epoch: 2/2, step 5005/7134 completed (loss: 0.02116512320935726, acc: 1.0)
[2025-02-13 20:35:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:30][root][INFO] - Training Epoch: 2/2, step 5006/7134 completed (loss: 0.0261828675866127, acc: 1.0)
[2025-02-13 20:35:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:30][root][INFO] - Training Epoch: 2/2, step 5007/7134 completed (loss: 0.015426483936607838, acc: 1.0)
[2025-02-13 20:35:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:31][root][INFO] - Training Epoch: 2/2, step 5008/7134 completed (loss: 0.15541695058345795, acc: 0.9833333492279053)
[2025-02-13 20:35:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:31][root][INFO] - Training Epoch: 2/2, step 5009/7134 completed (loss: 0.28157973289489746, acc: 0.9484536051750183)
[2025-02-13 20:35:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:32][root][INFO] - Training Epoch: 2/2, step 5010/7134 completed (loss: 0.06165481358766556, acc: 0.9914529919624329)
[2025-02-13 20:35:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:32][root][INFO] - Training Epoch: 2/2, step 5011/7134 completed (loss: 0.10150283575057983, acc: 0.9724137783050537)
[2025-02-13 20:35:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:32][root][INFO] - Training Epoch: 2/2, step 5012/7134 completed (loss: 0.04636124521493912, acc: 0.987500011920929)
[2025-02-13 20:35:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:33][root][INFO] - Training Epoch: 2/2, step 5013/7134 completed (loss: 0.019488831982016563, acc: 1.0)
[2025-02-13 20:35:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:33][root][INFO] - Training Epoch: 2/2, step 5014/7134 completed (loss: 0.14533188939094543, acc: 0.9807692170143127)
[2025-02-13 20:35:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:33][root][INFO] - Training Epoch: 2/2, step 5015/7134 completed (loss: 0.12770876288414001, acc: 0.9693251252174377)
[2025-02-13 20:35:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:34][root][INFO] - Training Epoch: 2/2, step 5016/7134 completed (loss: 0.09209504723548889, acc: 0.9923664331436157)
[2025-02-13 20:35:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:34][root][INFO] - Training Epoch: 2/2, step 5017/7134 completed (loss: 0.07127941399812698, acc: 0.9772727489471436)
[2025-02-13 20:35:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:35][root][INFO] - Training Epoch: 2/2, step 5018/7134 completed (loss: 0.05898864567279816, acc: 1.0)
[2025-02-13 20:35:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:35][root][INFO] - Training Epoch: 2/2, step 5019/7134 completed (loss: 0.11267893761396408, acc: 0.9604519605636597)
[2025-02-13 20:35:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:35][root][INFO] - Training Epoch: 2/2, step 5020/7134 completed (loss: 0.17984503507614136, acc: 0.9417989253997803)
[2025-02-13 20:35:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:36][root][INFO] - Training Epoch: 2/2, step 5021/7134 completed (loss: 0.0967954695224762, acc: 0.9904761910438538)
[2025-02-13 20:35:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:36][root][INFO] - Training Epoch: 2/2, step 5022/7134 completed (loss: 0.05549188703298569, acc: 0.9927536249160767)
[2025-02-13 20:35:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:37][root][INFO] - Training Epoch: 2/2, step 5023/7134 completed (loss: 0.03339982405304909, acc: 1.0)
[2025-02-13 20:35:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:37][root][INFO] - Training Epoch: 2/2, step 5024/7134 completed (loss: 0.041959505528211594, acc: 0.9939758777618408)
[2025-02-13 20:35:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:37][root][INFO] - Training Epoch: 2/2, step 5025/7134 completed (loss: 0.023218996822834015, acc: 1.0)
[2025-02-13 20:35:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:38][root][INFO] - Training Epoch: 2/2, step 5026/7134 completed (loss: 0.022870056331157684, acc: 1.0)
[2025-02-13 20:35:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:38][root][INFO] - Training Epoch: 2/2, step 5027/7134 completed (loss: 0.0526178777217865, acc: 0.9844961166381836)
[2025-02-13 20:35:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:39][root][INFO] - Training Epoch: 2/2, step 5028/7134 completed (loss: 0.0898820161819458, acc: 0.9788732528686523)
[2025-02-13 20:35:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:39][root][INFO] - Training Epoch: 2/2, step 5029/7134 completed (loss: 0.07613468915224075, acc: 0.9769230484962463)
[2025-02-13 20:35:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:39][root][INFO] - Training Epoch: 2/2, step 5030/7134 completed (loss: 0.04622873291373253, acc: 0.9880239367485046)
[2025-02-13 20:35:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:40][root][INFO] - Training Epoch: 2/2, step 5031/7134 completed (loss: 0.2164004147052765, acc: 0.9637681245803833)
[2025-02-13 20:35:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:40][root][INFO] - Training Epoch: 2/2, step 5032/7134 completed (loss: 0.08252393454313278, acc: 0.9929577708244324)
[2025-02-13 20:35:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:41][root][INFO] - Training Epoch: 2/2, step 5033/7134 completed (loss: 0.029222751036286354, acc: 1.0)
[2025-02-13 20:35:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:41][root][INFO] - Training Epoch: 2/2, step 5034/7134 completed (loss: 0.02098437212407589, acc: 1.0)
[2025-02-13 20:35:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:41][root][INFO] - Training Epoch: 2/2, step 5035/7134 completed (loss: 0.005928860977292061, acc: 1.0)
[2025-02-13 20:35:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:42][root][INFO] - Training Epoch: 2/2, step 5036/7134 completed (loss: 0.16106194257736206, acc: 0.9615384340286255)
[2025-02-13 20:35:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:42][root][INFO] - Training Epoch: 2/2, step 5037/7134 completed (loss: 0.05998748168349266, acc: 0.9882352948188782)
[2025-02-13 20:35:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:43][root][INFO] - Training Epoch: 2/2, step 5038/7134 completed (loss: 0.2715691030025482, acc: 0.9217391014099121)
[2025-02-13 20:35:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:43][root][INFO] - Training Epoch: 2/2, step 5039/7134 completed (loss: 0.0744771808385849, acc: 0.9858155846595764)
[2025-02-13 20:35:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:44][root][INFO] - Training Epoch: 2/2, step 5040/7134 completed (loss: 0.04701279476284981, acc: 0.9916666746139526)
[2025-02-13 20:35:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:44][root][INFO] - Training Epoch: 2/2, step 5041/7134 completed (loss: 0.01922863908112049, acc: 1.0)
[2025-02-13 20:35:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:45][root][INFO] - Training Epoch: 2/2, step 5042/7134 completed (loss: 0.11881524324417114, acc: 0.9708737730979919)
[2025-02-13 20:35:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:45][root][INFO] - Training Epoch: 2/2, step 5043/7134 completed (loss: 0.08134450018405914, acc: 0.982300877571106)
[2025-02-13 20:35:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:45][root][INFO] - Training Epoch: 2/2, step 5044/7134 completed (loss: 0.034633100032806396, acc: 1.0)
[2025-02-13 20:35:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:46][root][INFO] - Training Epoch: 2/2, step 5045/7134 completed (loss: 0.08611569553613663, acc: 0.9837837815284729)
[2025-02-13 20:35:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:46][root][INFO] - Training Epoch: 2/2, step 5046/7134 completed (loss: 0.10737085342407227, acc: 0.9596773982048035)
[2025-02-13 20:35:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:46][root][INFO] - Training Epoch: 2/2, step 5047/7134 completed (loss: 0.07495971769094467, acc: 0.9927007555961609)
[2025-02-13 20:35:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:47][root][INFO] - Training Epoch: 2/2, step 5048/7134 completed (loss: 0.10341281443834305, acc: 0.9857142567634583)
[2025-02-13 20:35:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:47][root][INFO] - Training Epoch: 2/2, step 5049/7134 completed (loss: 0.11611969023942947, acc: 0.9617834687232971)
[2025-02-13 20:35:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:48][root][INFO] - Training Epoch: 2/2, step 5050/7134 completed (loss: 0.1690095216035843, acc: 0.981249988079071)
[2025-02-13 20:35:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:48][root][INFO] - Training Epoch: 2/2, step 5051/7134 completed (loss: 0.13212645053863525, acc: 0.9591836929321289)
[2025-02-13 20:35:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:48][root][INFO] - Training Epoch: 2/2, step 5052/7134 completed (loss: 0.10491471737623215, acc: 0.9679487347602844)
[2025-02-13 20:35:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:49][root][INFO] - Training Epoch: 2/2, step 5053/7134 completed (loss: 0.17847003042697906, acc: 0.9583333134651184)
[2025-02-13 20:35:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:49][root][INFO] - Training Epoch: 2/2, step 5054/7134 completed (loss: 0.1044631376862526, acc: 0.9777777791023254)
[2025-02-13 20:35:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:50][root][INFO] - Training Epoch: 2/2, step 5055/7134 completed (loss: 0.15491686761379242, acc: 0.9557521939277649)
[2025-02-13 20:35:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:50][root][INFO] - Training Epoch: 2/2, step 5056/7134 completed (loss: 0.10849412530660629, acc: 0.9798657894134521)
[2025-02-13 20:35:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:50][root][INFO] - Training Epoch: 2/2, step 5057/7134 completed (loss: 0.24329806864261627, acc: 0.9617834687232971)
[2025-02-13 20:35:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:51][root][INFO] - Training Epoch: 2/2, step 5058/7134 completed (loss: 0.09643730521202087, acc: 0.9817073345184326)
[2025-02-13 20:35:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:51][root][INFO] - Training Epoch: 2/2, step 5059/7134 completed (loss: 0.03321923688054085, acc: 0.9940119981765747)
[2025-02-13 20:35:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:52][root][INFO] - Training Epoch: 2/2, step 5060/7134 completed (loss: 0.09724171459674835, acc: 0.9800000190734863)
[2025-02-13 20:35:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:52][root][INFO] - Training Epoch: 2/2, step 5061/7134 completed (loss: 0.11868011951446533, acc: 0.9523809552192688)
[2025-02-13 20:35:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:53][root][INFO] - Training Epoch: 2/2, step 5062/7134 completed (loss: 0.07869322597980499, acc: 0.9851852059364319)
[2025-02-13 20:35:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:53][root][INFO] - Training Epoch: 2/2, step 5063/7134 completed (loss: 0.09209677577018738, acc: 0.9720279574394226)
[2025-02-13 20:35:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:53][root][INFO] - Training Epoch: 2/2, step 5064/7134 completed (loss: 0.09779425710439682, acc: 0.9714285731315613)
[2025-02-13 20:35:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:54][root][INFO] - Training Epoch: 2/2, step 5065/7134 completed (loss: 0.1793607622385025, acc: 0.955974817276001)
[2025-02-13 20:35:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:54][root][INFO] - Training Epoch: 2/2, step 5066/7134 completed (loss: 0.08178526908159256, acc: 0.9862068891525269)
[2025-02-13 20:35:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:55][root][INFO] - Training Epoch: 2/2, step 5067/7134 completed (loss: 0.028369838371872902, acc: 0.9918032884597778)
[2025-02-13 20:35:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:55][root][INFO] - Training Epoch: 2/2, step 5068/7134 completed (loss: 0.14659906923770905, acc: 0.9436619877815247)
[2025-02-13 20:35:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:55][root][INFO] - Training Epoch: 2/2, step 5069/7134 completed (loss: 0.04260418564081192, acc: 0.9932432174682617)
[2025-02-13 20:35:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:56][root][INFO] - Training Epoch: 2/2, step 5070/7134 completed (loss: 0.019164374098181725, acc: 1.0)
[2025-02-13 20:35:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:56][root][INFO] - Training Epoch: 2/2, step 5071/7134 completed (loss: 0.06930682063102722, acc: 0.9764705896377563)
[2025-02-13 20:35:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:57][root][INFO] - Training Epoch: 2/2, step 5072/7134 completed (loss: 0.13683846592903137, acc: 0.9780219793319702)
[2025-02-13 20:35:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:57][root][INFO] - Training Epoch: 2/2, step 5073/7134 completed (loss: 0.09102801978588104, acc: 0.9694656729698181)
[2025-02-13 20:35:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:57][root][INFO] - Training Epoch: 2/2, step 5074/7134 completed (loss: 0.11357609927654266, acc: 0.9844961166381836)
[2025-02-13 20:35:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:58][root][INFO] - Training Epoch: 2/2, step 5075/7134 completed (loss: 0.1134377270936966, acc: 0.9652174115180969)
[2025-02-13 20:35:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:58][root][INFO] - Training Epoch: 2/2, step 5076/7134 completed (loss: 0.0990467369556427, acc: 0.9838709831237793)
[2025-02-13 20:35:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:59][root][INFO] - Training Epoch: 2/2, step 5077/7134 completed (loss: 0.07360313832759857, acc: 0.9779411554336548)
[2025-02-13 20:35:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:59][root][INFO] - Training Epoch: 2/2, step 5078/7134 completed (loss: 0.19190727174282074, acc: 0.949999988079071)
[2025-02-13 20:35:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:35:59][root][INFO] - Training Epoch: 2/2, step 5079/7134 completed (loss: 0.056636638939380646, acc: 0.9777777791023254)
[2025-02-13 20:36:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:00][root][INFO] - Training Epoch: 2/2, step 5080/7134 completed (loss: 0.0367681086063385, acc: 0.9950980544090271)
[2025-02-13 20:36:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:00][root][INFO] - Training Epoch: 2/2, step 5081/7134 completed (loss: 0.03727465122938156, acc: 0.9942196607589722)
[2025-02-13 20:36:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:01][root][INFO] - Training Epoch: 2/2, step 5082/7134 completed (loss: 0.05535271018743515, acc: 0.9862068891525269)
[2025-02-13 20:36:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:01][root][INFO] - Training Epoch: 2/2, step 5083/7134 completed (loss: 0.10866469889879227, acc: 0.976331353187561)
[2025-02-13 20:36:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:01][root][INFO] - Training Epoch: 2/2, step 5084/7134 completed (loss: 0.041632700711488724, acc: 0.9950494766235352)
[2025-02-13 20:36:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:02][root][INFO] - Training Epoch: 2/2, step 5085/7134 completed (loss: 0.05665209889411926, acc: 0.9845361113548279)
[2025-02-13 20:36:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:02][root][INFO] - Training Epoch: 2/2, step 5086/7134 completed (loss: 0.065436951816082, acc: 0.988950252532959)
[2025-02-13 20:36:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:03][root][INFO] - Training Epoch: 2/2, step 5087/7134 completed (loss: 0.0646764412522316, acc: 0.9783783555030823)
[2025-02-13 20:36:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:03][root][INFO] - Training Epoch: 2/2, step 5088/7134 completed (loss: 0.04021928831934929, acc: 0.9881656765937805)
[2025-02-13 20:36:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:03][root][INFO] - Training Epoch: 2/2, step 5089/7134 completed (loss: 0.07357756793498993, acc: 0.9897959232330322)
[2025-02-13 20:36:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:04][root][INFO] - Training Epoch: 2/2, step 5090/7134 completed (loss: 0.02694648876786232, acc: 0.9951456189155579)
[2025-02-13 20:36:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:04][root][INFO] - Training Epoch: 2/2, step 5091/7134 completed (loss: 0.054141003638505936, acc: 0.9835164546966553)
[2025-02-13 20:36:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:05][root][INFO] - Training Epoch: 2/2, step 5092/7134 completed (loss: 0.07031433284282684, acc: 0.97826087474823)
[2025-02-13 20:36:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:05][root][INFO] - Training Epoch: 2/2, step 5093/7134 completed (loss: 0.07607962936162949, acc: 0.9829545617103577)
[2025-02-13 20:36:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:05][root][INFO] - Training Epoch: 2/2, step 5094/7134 completed (loss: 0.04409729316830635, acc: 0.9923076629638672)
[2025-02-13 20:36:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:06][root][INFO] - Training Epoch: 2/2, step 5095/7134 completed (loss: 0.111809641122818, acc: 0.987261176109314)
[2025-02-13 20:36:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:06][root][INFO] - Training Epoch: 2/2, step 5096/7134 completed (loss: 0.0718550756573677, acc: 0.9734042286872864)
[2025-02-13 20:36:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:07][root][INFO] - Training Epoch: 2/2, step 5097/7134 completed (loss: 0.04986327141523361, acc: 0.9946523904800415)
[2025-02-13 20:36:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:07][root][INFO] - Training Epoch: 2/2, step 5098/7134 completed (loss: 0.16572259366512299, acc: 0.9693877696990967)
[2025-02-13 20:36:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:08][root][INFO] - Training Epoch: 2/2, step 5099/7134 completed (loss: 0.05552525818347931, acc: 0.9894179701805115)
[2025-02-13 20:36:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:08][root][INFO] - Training Epoch: 2/2, step 5100/7134 completed (loss: 0.08123111724853516, acc: 0.9852941036224365)
[2025-02-13 20:36:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:08][root][INFO] - Training Epoch: 2/2, step 5101/7134 completed (loss: 0.03396298363804817, acc: 0.9895287752151489)
[2025-02-13 20:36:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:09][root][INFO] - Training Epoch: 2/2, step 5102/7134 completed (loss: 0.06361225992441177, acc: 0.9817073345184326)
[2025-02-13 20:36:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:09][root][INFO] - Training Epoch: 2/2, step 5103/7134 completed (loss: 0.03594252094626427, acc: 1.0)
[2025-02-13 20:36:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:10][root][INFO] - Training Epoch: 2/2, step 5104/7134 completed (loss: 0.0687413290143013, acc: 0.9736841917037964)
[2025-02-13 20:36:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:10][root][INFO] - Training Epoch: 2/2, step 5105/7134 completed (loss: 0.08469358831644058, acc: 0.9801324605941772)
[2025-02-13 20:36:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:11][root][INFO] - Training Epoch: 2/2, step 5106/7134 completed (loss: 0.1569576859474182, acc: 0.9520547986030579)
[2025-02-13 20:36:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:11][root][INFO] - Training Epoch: 2/2, step 5107/7134 completed (loss: 0.392241895198822, acc: 0.9455782175064087)
[2025-02-13 20:36:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:11][root][INFO] - Training Epoch: 2/2, step 5108/7134 completed (loss: 0.20166127383708954, acc: 0.9432623982429504)
[2025-02-13 20:36:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:12][root][INFO] - Training Epoch: 2/2, step 5109/7134 completed (loss: 0.2589646279811859, acc: 0.9547738432884216)
[2025-02-13 20:36:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:12][root][INFO] - Training Epoch: 2/2, step 5110/7134 completed (loss: 0.13991166651248932, acc: 0.9560439586639404)
[2025-02-13 20:36:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:12][root][INFO] - Training Epoch: 2/2, step 5111/7134 completed (loss: 0.09100112318992615, acc: 0.977011501789093)
[2025-02-13 20:36:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:13][root][INFO] - Training Epoch: 2/2, step 5112/7134 completed (loss: 0.13874909281730652, acc: 0.9621621370315552)
[2025-02-13 20:36:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:13][root][INFO] - Training Epoch: 2/2, step 5113/7134 completed (loss: 0.10379140824079514, acc: 0.9801324605941772)
[2025-02-13 20:36:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:14][root][INFO] - Training Epoch: 2/2, step 5114/7134 completed (loss: 0.07802940905094147, acc: 0.9735099077224731)
[2025-02-13 20:36:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:14][root][INFO] - Training Epoch: 2/2, step 5115/7134 completed (loss: 0.08127812296152115, acc: 0.9894179701805115)
[2025-02-13 20:36:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:14][root][INFO] - Training Epoch: 2/2, step 5116/7134 completed (loss: 0.04223182052373886, acc: 0.9947090148925781)
[2025-02-13 20:36:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:15][root][INFO] - Training Epoch: 2/2, step 5117/7134 completed (loss: 0.040121082216501236, acc: 0.988950252532959)
[2025-02-13 20:36:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:15][root][INFO] - Training Epoch: 2/2, step 5118/7134 completed (loss: 0.10585467517375946, acc: 0.9879518151283264)
[2025-02-13 20:36:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:16][root][INFO] - Training Epoch: 2/2, step 5119/7134 completed (loss: 0.05125322937965393, acc: 0.9942528605461121)
[2025-02-13 20:36:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:16][root][INFO] - Training Epoch: 2/2, step 5120/7134 completed (loss: 0.08901354670524597, acc: 0.9682539701461792)
[2025-02-13 20:36:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:16][root][INFO] - Training Epoch: 2/2, step 5121/7134 completed (loss: 0.048038724809885025, acc: 0.9878787994384766)
[2025-02-13 20:36:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:17][root][INFO] - Training Epoch: 2/2, step 5122/7134 completed (loss: 0.21657703816890717, acc: 0.9360465407371521)
[2025-02-13 20:36:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:17][root][INFO] - Training Epoch: 2/2, step 5123/7134 completed (loss: 0.061386995017528534, acc: 0.9900990128517151)
[2025-02-13 20:36:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:18][root][INFO] - Training Epoch: 2/2, step 5124/7134 completed (loss: 0.09999193251132965, acc: 0.9802955389022827)
[2025-02-13 20:36:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:18][root][INFO] - Training Epoch: 2/2, step 5125/7134 completed (loss: 0.04379839822649956, acc: 0.9803921580314636)
[2025-02-13 20:36:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:19][root][INFO] - Training Epoch: 2/2, step 5126/7134 completed (loss: 0.07364106923341751, acc: 0.9724137783050537)
[2025-02-13 20:36:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:19][root][INFO] - Training Epoch: 2/2, step 5127/7134 completed (loss: 0.11314040422439575, acc: 0.982758641242981)
[2025-02-13 20:36:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:19][root][INFO] - Training Epoch: 2/2, step 5128/7134 completed (loss: 0.03747434541583061, acc: 0.9913793206214905)
[2025-02-13 20:36:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:20][root][INFO] - Training Epoch: 2/2, step 5129/7134 completed (loss: 0.08705325424671173, acc: 0.9844961166381836)
[2025-02-13 20:36:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:20][root][INFO] - Training Epoch: 2/2, step 5130/7134 completed (loss: 0.025822635740041733, acc: 0.9935064911842346)
[2025-02-13 20:36:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:21][root][INFO] - Training Epoch: 2/2, step 5131/7134 completed (loss: 0.16137713193893433, acc: 0.9679487347602844)
[2025-02-13 20:36:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:21][root][INFO] - Training Epoch: 2/2, step 5132/7134 completed (loss: 0.0738728940486908, acc: 0.9765625)
[2025-02-13 20:36:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:21][root][INFO] - Training Epoch: 2/2, step 5133/7134 completed (loss: 0.09523151814937592, acc: 0.9695122241973877)
[2025-02-13 20:36:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:22][root][INFO] - Training Epoch: 2/2, step 5134/7134 completed (loss: 0.03197784349322319, acc: 0.9927536249160767)
[2025-02-13 20:36:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:22][root][INFO] - Training Epoch: 2/2, step 5135/7134 completed (loss: 0.1628001630306244, acc: 0.9826589822769165)
[2025-02-13 20:36:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:22][root][INFO] - Training Epoch: 2/2, step 5136/7134 completed (loss: 0.05386393889784813, acc: 0.9892473220825195)
[2025-02-13 20:36:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:23][root][INFO] - Training Epoch: 2/2, step 5137/7134 completed (loss: 0.06600907444953918, acc: 0.9855072498321533)
[2025-02-13 20:36:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:23][root][INFO] - Training Epoch: 2/2, step 5138/7134 completed (loss: 0.10508331656455994, acc: 0.9734042286872864)
[2025-02-13 20:36:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:24][root][INFO] - Training Epoch: 2/2, step 5139/7134 completed (loss: 0.055865656584501266, acc: 0.9882352948188782)
[2025-02-13 20:36:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:24][root][INFO] - Training Epoch: 2/2, step 5140/7134 completed (loss: 0.14582839608192444, acc: 0.9666666388511658)
[2025-02-13 20:36:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:24][root][INFO] - Training Epoch: 2/2, step 5141/7134 completed (loss: 0.07769155502319336, acc: 0.9836065769195557)
[2025-02-13 20:36:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:25][root][INFO] - Training Epoch: 2/2, step 5142/7134 completed (loss: 0.02478560246527195, acc: 0.9950248599052429)
[2025-02-13 20:36:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:25][root][INFO] - Training Epoch: 2/2, step 5143/7134 completed (loss: 0.04991070553660393, acc: 0.9850746393203735)
[2025-02-13 20:36:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:26][root][INFO] - Training Epoch: 2/2, step 5144/7134 completed (loss: 0.03714706376194954, acc: 0.9894179701805115)
[2025-02-13 20:36:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:26][root][INFO] - Training Epoch: 2/2, step 5145/7134 completed (loss: 0.05429068207740784, acc: 0.9895833134651184)
[2025-02-13 20:36:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:26][root][INFO] - Training Epoch: 2/2, step 5146/7134 completed (loss: 0.10681267827749252, acc: 0.9562841653823853)
[2025-02-13 20:36:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:27][root][INFO] - Training Epoch: 2/2, step 5147/7134 completed (loss: 0.04970614239573479, acc: 0.9807692170143127)
[2025-02-13 20:36:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:27][root][INFO] - Training Epoch: 2/2, step 5148/7134 completed (loss: 0.1462222784757614, acc: 0.9750000238418579)
[2025-02-13 20:36:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:28][root][INFO] - Training Epoch: 2/2, step 5149/7134 completed (loss: 0.08454028517007828, acc: 0.9773755669593811)
[2025-02-13 20:36:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:28][root][INFO] - Training Epoch: 2/2, step 5150/7134 completed (loss: 0.1023634523153305, acc: 0.976047933101654)
[2025-02-13 20:36:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:28][root][INFO] - Training Epoch: 2/2, step 5151/7134 completed (loss: 0.11189081519842148, acc: 0.977142870426178)
[2025-02-13 20:36:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:29][root][INFO] - Training Epoch: 2/2, step 5152/7134 completed (loss: 0.053032755851745605, acc: 0.9941176176071167)
[2025-02-13 20:36:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:29][root][INFO] - Training Epoch: 2/2, step 5153/7134 completed (loss: 0.08041847497224808, acc: 0.9803921580314636)
[2025-02-13 20:36:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:30][root][INFO] - Training Epoch: 2/2, step 5154/7134 completed (loss: 0.03840131685137749, acc: 0.9900000095367432)
[2025-02-13 20:36:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:30][root][INFO] - Training Epoch: 2/2, step 5155/7134 completed (loss: 0.1267198920249939, acc: 0.9781022071838379)
[2025-02-13 20:36:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:30][root][INFO] - Training Epoch: 2/2, step 5156/7134 completed (loss: 0.05668478086590767, acc: 0.9814814925193787)
[2025-02-13 20:36:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:31][root][INFO] - Training Epoch: 2/2, step 5157/7134 completed (loss: 0.07099125534296036, acc: 0.9829545617103577)
[2025-02-13 20:36:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:31][root][INFO] - Training Epoch: 2/2, step 5158/7134 completed (loss: 0.1707373708486557, acc: 0.9551281929016113)
[2025-02-13 20:36:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:32][root][INFO] - Training Epoch: 2/2, step 5159/7134 completed (loss: 0.17319533228874207, acc: 0.9545454382896423)
[2025-02-13 20:36:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:32][root][INFO] - Training Epoch: 2/2, step 5160/7134 completed (loss: 0.09896277636289597, acc: 0.9722222089767456)
[2025-02-13 20:36:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:32][root][INFO] - Training Epoch: 2/2, step 5161/7134 completed (loss: 0.15492655336856842, acc: 0.9569892287254333)
[2025-02-13 20:36:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:33][root][INFO] - Training Epoch: 2/2, step 5162/7134 completed (loss: 0.0814938172698021, acc: 0.977142870426178)
[2025-02-13 20:36:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:33][root][INFO] - Training Epoch: 2/2, step 5163/7134 completed (loss: 0.12528099119663239, acc: 0.9731183052062988)
[2025-02-13 20:36:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:33][root][INFO] - Training Epoch: 2/2, step 5164/7134 completed (loss: 0.01638641208410263, acc: 1.0)
[2025-02-13 20:36:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:34][root][INFO] - Training Epoch: 2/2, step 5165/7134 completed (loss: 0.1078399047255516, acc: 0.9636363387107849)
[2025-02-13 20:36:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:34][root][INFO] - Training Epoch: 2/2, step 5166/7134 completed (loss: 0.08739326894283295, acc: 0.9629629850387573)
[2025-02-13 20:36:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:35][root][INFO] - Training Epoch: 2/2, step 5167/7134 completed (loss: 0.10566616803407669, acc: 0.9558011293411255)
[2025-02-13 20:36:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:35][root][INFO] - Training Epoch: 2/2, step 5168/7134 completed (loss: 0.1582190841436386, acc: 0.9651162624359131)
[2025-02-13 20:36:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:35][root][INFO] - Training Epoch: 2/2, step 5169/7134 completed (loss: 0.12077516317367554, acc: 0.9695122241973877)
[2025-02-13 20:36:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:36][root][INFO] - Training Epoch: 2/2, step 5170/7134 completed (loss: 0.18149137496948242, acc: 0.9585492014884949)
[2025-02-13 20:36:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:36][root][INFO] - Training Epoch: 2/2, step 5171/7134 completed (loss: 0.1011311337351799, acc: 0.9756097793579102)
[2025-02-13 20:36:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:37][root][INFO] - Training Epoch: 2/2, step 5172/7134 completed (loss: 0.24738793075084686, acc: 0.9512194991111755)
[2025-02-13 20:36:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:37][root][INFO] - Training Epoch: 2/2, step 5173/7134 completed (loss: 0.10544083267450333, acc: 0.9815950989723206)
[2025-02-13 20:36:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:37][root][INFO] - Training Epoch: 2/2, step 5174/7134 completed (loss: 0.10314877331256866, acc: 0.9768785834312439)
[2025-02-13 20:36:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:38][root][INFO] - Training Epoch: 2/2, step 5175/7134 completed (loss: 0.07438728958368301, acc: 0.9820359349250793)
[2025-02-13 20:36:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:38][root][INFO] - Training Epoch: 2/2, step 5176/7134 completed (loss: 0.09135109186172485, acc: 0.9868420958518982)
[2025-02-13 20:36:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:39][root][INFO] - Training Epoch: 2/2, step 5177/7134 completed (loss: 0.16768001019954681, acc: 0.9733333587646484)
[2025-02-13 20:36:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:39][root][INFO] - Training Epoch: 2/2, step 5178/7134 completed (loss: 0.3279137909412384, acc: 0.9266666769981384)
[2025-02-13 20:36:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:39][root][INFO] - Training Epoch: 2/2, step 5179/7134 completed (loss: 0.09382091462612152, acc: 0.9702380895614624)
[2025-02-13 20:36:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:40][root][INFO] - Training Epoch: 2/2, step 5180/7134 completed (loss: 0.2240820825099945, acc: 0.9503546357154846)
[2025-02-13 20:36:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:40][root][INFO] - Training Epoch: 2/2, step 5181/7134 completed (loss: 0.22979894280433655, acc: 0.932584285736084)
[2025-02-13 20:36:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:41][root][INFO] - Training Epoch: 2/2, step 5182/7134 completed (loss: 0.29438814520835876, acc: 0.9448819160461426)
[2025-02-13 20:36:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:41][root][INFO] - Training Epoch: 2/2, step 5183/7134 completed (loss: 0.16789452731609344, acc: 0.9558823704719543)
[2025-02-13 20:36:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:41][root][INFO] - Training Epoch: 2/2, step 5184/7134 completed (loss: 0.04468340426683426, acc: 0.9903846383094788)
[2025-02-13 20:36:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:42][root][INFO] - Training Epoch: 2/2, step 5185/7134 completed (loss: 0.11469720304012299, acc: 0.9603960514068604)
[2025-02-13 20:36:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:42][root][INFO] - Training Epoch: 2/2, step 5186/7134 completed (loss: 0.10245534777641296, acc: 0.9763779640197754)
[2025-02-13 20:36:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:43][root][INFO] - Training Epoch: 2/2, step 5187/7134 completed (loss: 0.030759407207369804, acc: 1.0)
[2025-02-13 20:36:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:43][root][INFO] - Training Epoch: 2/2, step 5188/7134 completed (loss: 0.06813498586416245, acc: 0.97826087474823)
[2025-02-13 20:36:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:43][root][INFO] - Training Epoch: 2/2, step 5189/7134 completed (loss: 0.1265510767698288, acc: 0.9801980257034302)
[2025-02-13 20:36:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:44][root][INFO] - Training Epoch: 2/2, step 5190/7134 completed (loss: 0.05979357287287712, acc: 0.9818181991577148)
[2025-02-13 20:36:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:44][root][INFO] - Training Epoch: 2/2, step 5191/7134 completed (loss: 0.1054551899433136, acc: 0.9629629850387573)
[2025-02-13 20:36:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:45][root][INFO] - Training Epoch: 2/2, step 5192/7134 completed (loss: 0.15084987878799438, acc: 0.9797979593276978)
[2025-02-13 20:36:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:45][root][INFO] - Training Epoch: 2/2, step 5193/7134 completed (loss: 0.022681983187794685, acc: 0.9916666746139526)
[2025-02-13 20:36:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:45][root][INFO] - Training Epoch: 2/2, step 5194/7134 completed (loss: 0.12106288224458694, acc: 0.9808917045593262)
[2025-02-13 20:36:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:46][root][INFO] - Training Epoch: 2/2, step 5195/7134 completed (loss: 0.08102870732545853, acc: 0.9814814925193787)
[2025-02-13 20:36:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:46][root][INFO] - Training Epoch: 2/2, step 5196/7134 completed (loss: 0.18972696363925934, acc: 0.9599999785423279)
[2025-02-13 20:36:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:47][root][INFO] - Training Epoch: 2/2, step 5197/7134 completed (loss: 0.06314880400896072, acc: 0.9841269850730896)
[2025-02-13 20:36:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:47][root][INFO] - Training Epoch: 2/2, step 5198/7134 completed (loss: 0.11699692159891129, acc: 0.9590163826942444)
[2025-02-13 20:36:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:47][root][INFO] - Training Epoch: 2/2, step 5199/7134 completed (loss: 0.16678304970264435, acc: 0.9593023061752319)
[2025-02-13 20:36:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:48][root][INFO] - Training Epoch: 2/2, step 5200/7134 completed (loss: 0.06541021168231964, acc: 0.976190447807312)
[2025-02-13 20:36:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:48][root][INFO] - Training Epoch: 2/2, step 5201/7134 completed (loss: 0.13373255729675293, acc: 0.966183602809906)
[2025-02-13 20:36:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:49][root][INFO] - Training Epoch: 2/2, step 5202/7134 completed (loss: 0.08906573057174683, acc: 0.9895833134651184)
[2025-02-13 20:36:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:49][root][INFO] - Training Epoch: 2/2, step 5203/7134 completed (loss: 0.054336417466402054, acc: 0.9945054650306702)
[2025-02-13 20:36:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:49][root][INFO] - Training Epoch: 2/2, step 5204/7134 completed (loss: 0.11725948750972748, acc: 0.9836065769195557)
[2025-02-13 20:36:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:50][root][INFO] - Training Epoch: 2/2, step 5205/7134 completed (loss: 0.06533212214708328, acc: 0.9759036302566528)
[2025-02-13 20:36:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:50][root][INFO] - Training Epoch: 2/2, step 5206/7134 completed (loss: 0.11533807218074799, acc: 0.9824561476707458)
[2025-02-13 20:36:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:51][root][INFO] - Training Epoch: 2/2, step 5207/7134 completed (loss: 0.2947423458099365, acc: 0.9444444179534912)
[2025-02-13 20:36:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:51][root][INFO] - Training Epoch: 2/2, step 5208/7134 completed (loss: 0.06474229693412781, acc: 0.9712643623352051)
[2025-02-13 20:36:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:51][root][INFO] - Training Epoch: 2/2, step 5209/7134 completed (loss: 0.08952518552541733, acc: 0.969924807548523)
[2025-02-13 20:36:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:52][root][INFO] - Training Epoch: 2/2, step 5210/7134 completed (loss: 0.08465082198381424, acc: 0.976331353187561)
[2025-02-13 20:36:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:52][root][INFO] - Training Epoch: 2/2, step 5211/7134 completed (loss: 0.07621341198682785, acc: 0.9682539701461792)
[2025-02-13 20:36:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:53][root][INFO] - Training Epoch: 2/2, step 5212/7134 completed (loss: 0.06817306578159332, acc: 0.9870129823684692)
[2025-02-13 20:36:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:53][root][INFO] - Training Epoch: 2/2, step 5213/7134 completed (loss: 0.13815267384052277, acc: 0.9753086566925049)
[2025-02-13 20:36:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:54][root][INFO] - Training Epoch: 2/2, step 5214/7134 completed (loss: 0.17101164162158966, acc: 0.9663865566253662)
[2025-02-13 20:36:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:54][root][INFO] - Training Epoch: 2/2, step 5215/7134 completed (loss: 0.12139464914798737, acc: 0.9851484894752502)
[2025-02-13 20:36:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:54][root][INFO] - Training Epoch: 2/2, step 5216/7134 completed (loss: 0.08480298519134521, acc: 1.0)
[2025-02-13 20:36:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:55][root][INFO] - Training Epoch: 2/2, step 5217/7134 completed (loss: 0.0960226058959961, acc: 0.95652174949646)
[2025-02-13 20:36:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:55][root][INFO] - Training Epoch: 2/2, step 5218/7134 completed (loss: 0.09465550631284714, acc: 0.9801980257034302)
[2025-02-13 20:36:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:56][root][INFO] - Training Epoch: 2/2, step 5219/7134 completed (loss: 0.07041393965482712, acc: 0.9814814925193787)
[2025-02-13 20:36:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:56][root][INFO] - Training Epoch: 2/2, step 5220/7134 completed (loss: 0.09379494935274124, acc: 0.9794520735740662)
[2025-02-13 20:36:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:56][root][INFO] - Training Epoch: 2/2, step 5221/7134 completed (loss: 0.10083013772964478, acc: 0.9753086566925049)
[2025-02-13 20:36:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:57][root][INFO] - Training Epoch: 2/2, step 5222/7134 completed (loss: 0.06278683245182037, acc: 0.988095223903656)
[2025-02-13 20:36:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:57][root][INFO] - Training Epoch: 2/2, step 5223/7134 completed (loss: 0.07973388582468033, acc: 0.982300877571106)
[2025-02-13 20:36:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:57][root][INFO] - Training Epoch: 2/2, step 5224/7134 completed (loss: 0.11869870126247406, acc: 0.970370352268219)
[2025-02-13 20:36:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:58][root][INFO] - Training Epoch: 2/2, step 5225/7134 completed (loss: 0.1710420846939087, acc: 0.9736841917037964)
[2025-02-13 20:36:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:58][root][INFO] - Training Epoch: 2/2, step 5226/7134 completed (loss: 0.12439686059951782, acc: 0.9743589758872986)
[2025-02-13 20:36:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:59][root][INFO] - Training Epoch: 2/2, step 5227/7134 completed (loss: 0.11385055631399155, acc: 0.9897435903549194)
[2025-02-13 20:36:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:59][root][INFO] - Training Epoch: 2/2, step 5228/7134 completed (loss: 0.048838336020708084, acc: 0.9885714054107666)
[2025-02-13 20:36:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:36:59][root][INFO] - Training Epoch: 2/2, step 5229/7134 completed (loss: 0.027964923530817032, acc: 0.9870967864990234)
[2025-02-13 20:37:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:00][root][INFO] - Training Epoch: 2/2, step 5230/7134 completed (loss: 0.04712170735001564, acc: 0.988950252532959)
[2025-02-13 20:37:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:00][root][INFO] - Training Epoch: 2/2, step 5231/7134 completed (loss: 0.10367273539304733, acc: 0.9896907210350037)
[2025-02-13 20:37:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:01][root][INFO] - Training Epoch: 2/2, step 5232/7134 completed (loss: 0.05112242326140404, acc: 0.9887005686759949)
[2025-02-13 20:37:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:01][root][INFO] - Training Epoch: 2/2, step 5233/7134 completed (loss: 0.06559495627880096, acc: 0.9908257126808167)
[2025-02-13 20:37:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:02][root][INFO] - Training Epoch: 2/2, step 5234/7134 completed (loss: 0.07182742655277252, acc: 0.9842105507850647)
[2025-02-13 20:37:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:02][root][INFO] - Training Epoch: 2/2, step 5235/7134 completed (loss: 0.028965376317501068, acc: 0.9932432174682617)
[2025-02-13 20:37:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:02][root][INFO] - Training Epoch: 2/2, step 5236/7134 completed (loss: 0.027443353086709976, acc: 0.9927007555961609)
[2025-02-13 20:37:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:03][root][INFO] - Training Epoch: 2/2, step 5237/7134 completed (loss: 0.03326937556266785, acc: 0.9929577708244324)
[2025-02-13 20:37:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:03][root][INFO] - Training Epoch: 2/2, step 5238/7134 completed (loss: 0.0508076511323452, acc: 0.9850746393203735)
[2025-02-13 20:37:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:04][root][INFO] - Training Epoch: 2/2, step 5239/7134 completed (loss: 0.057286374270915985, acc: 0.9831932783126831)
[2025-02-13 20:37:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:04][root][INFO] - Training Epoch: 2/2, step 5240/7134 completed (loss: 0.07547321170568466, acc: 0.9825581312179565)
[2025-02-13 20:37:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:04][root][INFO] - Training Epoch: 2/2, step 5241/7134 completed (loss: 0.11790955066680908, acc: 0.9741935729980469)
[2025-02-13 20:37:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:05][root][INFO] - Training Epoch: 2/2, step 5242/7134 completed (loss: 0.04110095277428627, acc: 0.9868420958518982)
[2025-02-13 20:37:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:05][root][INFO] - Training Epoch: 2/2, step 5243/7134 completed (loss: 0.03005068562924862, acc: 0.9928571581840515)
[2025-02-13 20:37:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:06][root][INFO] - Training Epoch: 2/2, step 5244/7134 completed (loss: 0.08211874216794968, acc: 0.9825581312179565)
[2025-02-13 20:37:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:06][root][INFO] - Training Epoch: 2/2, step 5245/7134 completed (loss: 0.035945966839790344, acc: 0.9920634627342224)
[2025-02-13 20:37:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:06][root][INFO] - Training Epoch: 2/2, step 5246/7134 completed (loss: 0.059858858585357666, acc: 0.9873417615890503)
[2025-02-13 20:37:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:07][root][INFO] - Training Epoch: 2/2, step 5247/7134 completed (loss: 0.05101075768470764, acc: 0.9851852059364319)
[2025-02-13 20:37:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:07][root][INFO] - Training Epoch: 2/2, step 5248/7134 completed (loss: 0.0955548882484436, acc: 0.9770992398262024)
[2025-02-13 20:37:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:08][root][INFO] - Training Epoch: 2/2, step 5249/7134 completed (loss: 0.028207994997501373, acc: 1.0)
[2025-02-13 20:37:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:08][root][INFO] - Training Epoch: 2/2, step 5250/7134 completed (loss: 0.04461020603775978, acc: 0.9945651888847351)
[2025-02-13 20:37:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:08][root][INFO] - Training Epoch: 2/2, step 5251/7134 completed (loss: 0.0659966841340065, acc: 0.9895833134651184)
[2025-02-13 20:37:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:09][root][INFO] - Training Epoch: 2/2, step 5252/7134 completed (loss: 0.05425809696316719, acc: 0.991150438785553)
[2025-02-13 20:37:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:09][root][INFO] - Training Epoch: 2/2, step 5253/7134 completed (loss: 0.050966352224349976, acc: 0.9861111044883728)
[2025-02-13 20:37:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:10][root][INFO] - Training Epoch: 2/2, step 5254/7134 completed (loss: 0.1315210461616516, acc: 0.9510489702224731)
[2025-02-13 20:37:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:10][root][INFO] - Training Epoch: 2/2, step 5255/7134 completed (loss: 0.09152336418628693, acc: 0.977011501789093)
[2025-02-13 20:37:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:10][root][INFO] - Training Epoch: 2/2, step 5256/7134 completed (loss: 0.1230258047580719, acc: 0.9666666388511658)
[2025-02-13 20:37:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:11][root][INFO] - Training Epoch: 2/2, step 5257/7134 completed (loss: 0.14676715433597565, acc: 0.9520958065986633)
[2025-02-13 20:37:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:11][root][INFO] - Training Epoch: 2/2, step 5258/7134 completed (loss: 0.17548175156116486, acc: 0.9523809552192688)
[2025-02-13 20:37:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:12][root][INFO] - Training Epoch: 2/2, step 5259/7134 completed (loss: 0.14466287195682526, acc: 0.9640287756919861)
[2025-02-13 20:37:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:12][root][INFO] - Training Epoch: 2/2, step 5260/7134 completed (loss: 0.07954658567905426, acc: 0.9788359999656677)
[2025-02-13 20:37:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:13][root][INFO] - Training Epoch: 2/2, step 5261/7134 completed (loss: 0.13404077291488647, acc: 0.9720279574394226)
[2025-02-13 20:37:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:13][root][INFO] - Training Epoch: 2/2, step 5262/7134 completed (loss: 0.0672665387392044, acc: 0.9879518151283264)
[2025-02-13 20:37:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:13][root][INFO] - Training Epoch: 2/2, step 5263/7134 completed (loss: 0.07060681283473969, acc: 0.9929577708244324)
[2025-02-13 20:37:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:14][root][INFO] - Training Epoch: 2/2, step 5264/7134 completed (loss: 0.11694828420877457, acc: 0.9693877696990967)
[2025-02-13 20:37:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:14][root][INFO] - Training Epoch: 2/2, step 5265/7134 completed (loss: 0.06900359690189362, acc: 0.9781420826911926)
[2025-02-13 20:37:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:15][root][INFO] - Training Epoch: 2/2, step 5266/7134 completed (loss: 0.024024995043873787, acc: 1.0)
[2025-02-13 20:37:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:15][root][INFO] - Training Epoch: 2/2, step 5267/7134 completed (loss: 0.15963251888751984, acc: 0.9629629850387573)
[2025-02-13 20:37:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:16][root][INFO] - Training Epoch: 2/2, step 5268/7134 completed (loss: 0.12607939541339874, acc: 0.9767441749572754)
[2025-02-13 20:37:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:16][root][INFO] - Training Epoch: 2/2, step 5269/7134 completed (loss: 0.0770689845085144, acc: 0.9857142567634583)
[2025-02-13 20:37:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:16][root][INFO] - Training Epoch: 2/2, step 5270/7134 completed (loss: 0.10579740256071091, acc: 0.9712643623352051)
[2025-02-13 20:37:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:17][root][INFO] - Training Epoch: 2/2, step 5271/7134 completed (loss: 0.10688742995262146, acc: 0.971222996711731)
[2025-02-13 20:37:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:17][root][INFO] - Training Epoch: 2/2, step 5272/7134 completed (loss: 0.042319685220718384, acc: 0.9942196607589722)
[2025-02-13 20:37:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:18][root][INFO] - Training Epoch: 2/2, step 5273/7134 completed (loss: 0.1267891675233841, acc: 0.9583333134651184)
[2025-02-13 20:37:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:18][root][INFO] - Training Epoch: 2/2, step 5274/7134 completed (loss: 0.14584463834762573, acc: 0.9679487347602844)
[2025-02-13 20:37:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:18][root][INFO] - Training Epoch: 2/2, step 5275/7134 completed (loss: 0.06649838387966156, acc: 0.9837398529052734)
[2025-02-13 20:37:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:19][root][INFO] - Training Epoch: 2/2, step 5276/7134 completed (loss: 0.11045927554368973, acc: 0.9736841917037964)
[2025-02-13 20:37:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:19][root][INFO] - Training Epoch: 2/2, step 5277/7134 completed (loss: 0.03805031627416611, acc: 0.9808917045593262)
[2025-02-13 20:37:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:19][root][INFO] - Training Epoch: 2/2, step 5278/7134 completed (loss: 0.1422126591205597, acc: 0.9696969985961914)
[2025-02-13 20:37:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:20][root][INFO] - Training Epoch: 2/2, step 5279/7134 completed (loss: 0.025517510250210762, acc: 1.0)
[2025-02-13 20:37:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:20][root][INFO] - Training Epoch: 2/2, step 5280/7134 completed (loss: 0.0351976677775383, acc: 0.9857142567634583)
[2025-02-13 20:37:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:21][root][INFO] - Training Epoch: 2/2, step 5281/7134 completed (loss: 0.04800501465797424, acc: 0.9927536249160767)
[2025-02-13 20:37:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:21][root][INFO] - Training Epoch: 2/2, step 5282/7134 completed (loss: 0.15918685495853424, acc: 0.9593023061752319)
[2025-02-13 20:37:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:21][root][INFO] - Training Epoch: 2/2, step 5283/7134 completed (loss: 0.17790748178958893, acc: 0.9726775884628296)
[2025-02-13 20:37:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:22][root][INFO] - Training Epoch: 2/2, step 5284/7134 completed (loss: 0.14585497975349426, acc: 0.981249988079071)
[2025-02-13 20:37:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:22][root][INFO] - Training Epoch: 2/2, step 5285/7134 completed (loss: 0.17816907167434692, acc: 0.9569892287254333)
[2025-02-13 20:37:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:23][root][INFO] - Training Epoch: 2/2, step 5286/7134 completed (loss: 0.06968963146209717, acc: 0.9798657894134521)
[2025-02-13 20:37:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:23][root][INFO] - Training Epoch: 2/2, step 5287/7134 completed (loss: 0.11461186408996582, acc: 0.9646464586257935)
[2025-02-13 20:37:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:23][root][INFO] - Training Epoch: 2/2, step 5288/7134 completed (loss: 0.0954761952161789, acc: 0.9750000238418579)
[2025-02-13 20:37:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:24][root][INFO] - Training Epoch: 2/2, step 5289/7134 completed (loss: 0.07012380659580231, acc: 0.9772727489471436)
[2025-02-13 20:37:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:24][root][INFO] - Training Epoch: 2/2, step 5290/7134 completed (loss: 0.10631990432739258, acc: 0.9682539701461792)
[2025-02-13 20:37:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:25][root][INFO] - Training Epoch: 2/2, step 5291/7134 completed (loss: 0.02215123362839222, acc: 0.9945945739746094)
[2025-02-13 20:37:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:25][root][INFO] - Training Epoch: 2/2, step 5292/7134 completed (loss: 0.041281525045633316, acc: 0.9887005686759949)
[2025-02-13 20:37:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:25][root][INFO] - Training Epoch: 2/2, step 5293/7134 completed (loss: 0.03649640828371048, acc: 0.993630588054657)
[2025-02-13 20:37:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:26][root][INFO] - Training Epoch: 2/2, step 5294/7134 completed (loss: 0.07488179206848145, acc: 0.976331353187561)
[2025-02-13 20:37:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:26][root][INFO] - Training Epoch: 2/2, step 5295/7134 completed (loss: 0.18705277144908905, acc: 0.9767441749572754)
[2025-02-13 20:37:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:27][root][INFO] - Training Epoch: 2/2, step 5296/7134 completed (loss: 0.03906424716114998, acc: 0.994350254535675)
[2025-02-13 20:37:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:27][root][INFO] - Training Epoch: 2/2, step 5297/7134 completed (loss: 0.08455126732587814, acc: 0.9698795080184937)
[2025-02-13 20:37:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:27][root][INFO] - Training Epoch: 2/2, step 5298/7134 completed (loss: 0.06549210101366043, acc: 0.9858155846595764)
[2025-02-13 20:37:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:28][root][INFO] - Training Epoch: 2/2, step 5299/7134 completed (loss: 0.02565154619514942, acc: 0.9931507110595703)
[2025-02-13 20:37:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:28][root][INFO] - Training Epoch: 2/2, step 5300/7134 completed (loss: 0.03337691351771355, acc: 0.9939758777618408)
[2025-02-13 20:37:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:29][root][INFO] - Training Epoch: 2/2, step 5301/7134 completed (loss: 0.06522097438573837, acc: 0.9878048896789551)
[2025-02-13 20:37:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:29][root][INFO] - Training Epoch: 2/2, step 5302/7134 completed (loss: 0.11186415702104568, acc: 0.9776536226272583)
[2025-02-13 20:37:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:30][root][INFO] - Training Epoch: 2/2, step 5303/7134 completed (loss: 0.17822480201721191, acc: 0.9545454382896423)
[2025-02-13 20:37:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:30][root][INFO] - Training Epoch: 2/2, step 5304/7134 completed (loss: 0.3520488440990448, acc: 0.9178082346916199)
[2025-02-13 20:37:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:30][root][INFO] - Training Epoch: 2/2, step 5305/7134 completed (loss: 0.14455780386924744, acc: 0.9624999761581421)
[2025-02-13 20:37:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:31][root][INFO] - Training Epoch: 2/2, step 5306/7134 completed (loss: 0.07352945953607559, acc: 0.9888888597488403)
[2025-02-13 20:37:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:31][root][INFO] - Training Epoch: 2/2, step 5307/7134 completed (loss: 0.17680352926254272, acc: 0.9590643048286438)
[2025-02-13 20:37:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:32][root][INFO] - Training Epoch: 2/2, step 5308/7134 completed (loss: 0.13005590438842773, acc: 0.9753086566925049)
[2025-02-13 20:37:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:32][root][INFO] - Training Epoch: 2/2, step 5309/7134 completed (loss: 0.252165287733078, acc: 0.9300699234008789)
[2025-02-13 20:37:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:32][root][INFO] - Training Epoch: 2/2, step 5310/7134 completed (loss: 0.15384431183338165, acc: 0.9602272510528564)
[2025-02-13 20:37:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:33][root][INFO] - Training Epoch: 2/2, step 5311/7134 completed (loss: 0.13983404636383057, acc: 0.9627329111099243)
[2025-02-13 20:37:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:33][root][INFO] - Training Epoch: 2/2, step 5312/7134 completed (loss: 0.07835756242275238, acc: 0.9841269850730896)
[2025-02-13 20:37:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:34][root][INFO] - Training Epoch: 2/2, step 5313/7134 completed (loss: 0.1933864951133728, acc: 0.9508196711540222)
[2025-02-13 20:37:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:34][root][INFO] - Training Epoch: 2/2, step 5314/7134 completed (loss: 0.13169287145137787, acc: 0.9642857313156128)
[2025-02-13 20:37:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:34][root][INFO] - Training Epoch: 2/2, step 5315/7134 completed (loss: 0.05330336093902588, acc: 0.9856114983558655)
[2025-02-13 20:37:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:35][root][INFO] - Training Epoch: 2/2, step 5316/7134 completed (loss: 0.1023784950375557, acc: 0.9666666388511658)
[2025-02-13 20:37:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:35][root][INFO] - Training Epoch: 2/2, step 5317/7134 completed (loss: 0.07219978421926498, acc: 0.9920634627342224)
[2025-02-13 20:37:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:36][root][INFO] - Training Epoch: 2/2, step 5318/7134 completed (loss: 0.027626682072877884, acc: 0.987261176109314)
[2025-02-13 20:37:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:36][root][INFO] - Training Epoch: 2/2, step 5319/7134 completed (loss: 0.07781915366649628, acc: 0.9774436354637146)
[2025-02-13 20:37:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:37][root][INFO] - Training Epoch: 2/2, step 5320/7134 completed (loss: 0.15458089113235474, acc: 0.9496855139732361)
[2025-02-13 20:37:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:37][root][INFO] - Training Epoch: 2/2, step 5321/7134 completed (loss: 0.11409863829612732, acc: 0.9740259647369385)
[2025-02-13 20:37:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:37][root][INFO] - Training Epoch: 2/2, step 5322/7134 completed (loss: 0.048947203904390335, acc: 0.9800000190734863)
[2025-02-13 20:37:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:38][root][INFO] - Training Epoch: 2/2, step 5323/7134 completed (loss: 0.1273145079612732, acc: 0.9798657894134521)
[2025-02-13 20:37:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:38][root][INFO] - Training Epoch: 2/2, step 5324/7134 completed (loss: 0.07248891144990921, acc: 0.9736841917037964)
[2025-02-13 20:37:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:39][root][INFO] - Training Epoch: 2/2, step 5325/7134 completed (loss: 0.08489877730607986, acc: 0.97826087474823)
[2025-02-13 20:37:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:39][root][INFO] - Training Epoch: 2/2, step 5326/7134 completed (loss: 0.15814968943595886, acc: 0.9655172228813171)
[2025-02-13 20:37:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:39][root][INFO] - Training Epoch: 2/2, step 5327/7134 completed (loss: 0.07811873406171799, acc: 0.9806451797485352)
[2025-02-13 20:37:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:40][root][INFO] - Training Epoch: 2/2, step 5328/7134 completed (loss: 0.07324527204036713, acc: 0.9849624037742615)
[2025-02-13 20:37:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:40][root][INFO] - Training Epoch: 2/2, step 5329/7134 completed (loss: 0.134396493434906, acc: 0.9800000190734863)
[2025-02-13 20:37:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:41][root][INFO] - Training Epoch: 2/2, step 5330/7134 completed (loss: 0.15354637801647186, acc: 0.9683544039726257)
[2025-02-13 20:37:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:41][root][INFO] - Training Epoch: 2/2, step 5331/7134 completed (loss: 0.1283721625804901, acc: 0.9629629850387573)
[2025-02-13 20:37:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:41][root][INFO] - Training Epoch: 2/2, step 5332/7134 completed (loss: 0.059284139424562454, acc: 0.9863013625144958)
[2025-02-13 20:37:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:42][root][INFO] - Training Epoch: 2/2, step 5333/7134 completed (loss: 0.031044840812683105, acc: 1.0)
[2025-02-13 20:37:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:42][root][INFO] - Training Epoch: 2/2, step 5334/7134 completed (loss: 0.13763165473937988, acc: 0.9642857313156128)
[2025-02-13 20:37:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:43][root][INFO] - Training Epoch: 2/2, step 5335/7134 completed (loss: 0.06618064641952515, acc: 0.9932885766029358)
[2025-02-13 20:37:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:43][root][INFO] - Training Epoch: 2/2, step 5336/7134 completed (loss: 0.07141819596290588, acc: 0.9718309640884399)
[2025-02-13 20:37:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:43][root][INFO] - Training Epoch: 2/2, step 5337/7134 completed (loss: 0.08835186809301376, acc: 0.9766355156898499)
[2025-02-13 20:37:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:44][root][INFO] - Training Epoch: 2/2, step 5338/7134 completed (loss: 0.07106403261423111, acc: 0.9857819676399231)
[2025-02-13 20:37:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:44][root][INFO] - Training Epoch: 2/2, step 5339/7134 completed (loss: 0.08141706883907318, acc: 0.9835164546966553)
[2025-02-13 20:37:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:45][root][INFO] - Training Epoch: 2/2, step 5340/7134 completed (loss: 0.09487979859113693, acc: 0.9664429426193237)
[2025-02-13 20:37:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:45][root][INFO] - Training Epoch: 2/2, step 5341/7134 completed (loss: 0.0706874430179596, acc: 0.9800000190734863)
[2025-02-13 20:37:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:45][root][INFO] - Training Epoch: 2/2, step 5342/7134 completed (loss: 0.23319107294082642, acc: 0.9315789341926575)
[2025-02-13 20:37:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:46][root][INFO] - Training Epoch: 2/2, step 5343/7134 completed (loss: 0.11262372136116028, acc: 0.9729729890823364)
[2025-02-13 20:37:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:46][root][INFO] - Training Epoch: 2/2, step 5344/7134 completed (loss: 0.15270903706550598, acc: 0.9517543911933899)
[2025-02-13 20:37:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:47][root][INFO] - Training Epoch: 2/2, step 5345/7134 completed (loss: 0.1359288990497589, acc: 0.9595959782600403)
[2025-02-13 20:37:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:47][root][INFO] - Training Epoch: 2/2, step 5346/7134 completed (loss: 0.13821037113666534, acc: 0.9541284441947937)
[2025-02-13 20:37:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:37:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:38:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:39:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:40:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:49][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.2679, device='cuda:0') eval_epoch_loss=tensor(0.2373, device='cuda:0') eval_epoch_acc=tensor(0.9459, device='cuda:0')
[2025-02-13 20:41:49][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-13 20:41:49][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-13 20:41:49][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_wavlm_llama32_1b_linear_peft_transfer_librispeech-100/asr_epoch_2_step_5347_loss_0.2373497635126114/model.pt
[2025-02-13 20:41:49][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_wavlm_llama32_1b_linear_peft_transfer_librispeech-100 directory
[2025-02-13 20:41:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:49][root][INFO] - Training Epoch: 2/2, step 5347/7134 completed (loss: 0.09689521789550781, acc: 0.9677419066429138)
[2025-02-13 20:41:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:50][root][INFO] - Training Epoch: 2/2, step 5348/7134 completed (loss: 0.15260891616344452, acc: 0.9518072009086609)
[2025-02-13 20:41:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:50][root][INFO] - Training Epoch: 2/2, step 5349/7134 completed (loss: 0.12593646347522736, acc: 0.9714285731315613)
[2025-02-13 20:41:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:51][root][INFO] - Training Epoch: 2/2, step 5350/7134 completed (loss: 0.05987520143389702, acc: 0.9882352948188782)
[2025-02-13 20:41:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:51][root][INFO] - Training Epoch: 2/2, step 5351/7134 completed (loss: 0.13343767821788788, acc: 0.9675675630569458)
[2025-02-13 20:41:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:51][root][INFO] - Training Epoch: 2/2, step 5352/7134 completed (loss: 0.08969797939062119, acc: 0.976047933101654)
[2025-02-13 20:41:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:52][root][INFO] - Training Epoch: 2/2, step 5353/7134 completed (loss: 0.13477133214473724, acc: 0.9444444179534912)
[2025-02-13 20:41:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:52][root][INFO] - Training Epoch: 2/2, step 5354/7134 completed (loss: 0.13723038136959076, acc: 0.9629629850387573)
[2025-02-13 20:41:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:52][root][INFO] - Training Epoch: 2/2, step 5355/7134 completed (loss: 0.14422979950904846, acc: 0.9791666865348816)
[2025-02-13 20:41:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:53][root][INFO] - Training Epoch: 2/2, step 5356/7134 completed (loss: 0.1407574564218521, acc: 0.957446813583374)
[2025-02-13 20:41:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:53][root][INFO] - Training Epoch: 2/2, step 5357/7134 completed (loss: 0.08467962592840195, acc: 0.9858155846595764)
[2025-02-13 20:41:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:53][root][INFO] - Training Epoch: 2/2, step 5358/7134 completed (loss: 0.26410534977912903, acc: 0.9505494236946106)
[2025-02-13 20:41:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:54][root][INFO] - Training Epoch: 2/2, step 5359/7134 completed (loss: 0.14214204251766205, acc: 0.9704433679580688)
[2025-02-13 20:41:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:54][root][INFO] - Training Epoch: 2/2, step 5360/7134 completed (loss: 0.053933653980493546, acc: 0.9942528605461121)
[2025-02-13 20:41:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:55][root][INFO] - Training Epoch: 2/2, step 5361/7134 completed (loss: 0.11486849188804626, acc: 0.9746192693710327)
[2025-02-13 20:41:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:55][root][INFO] - Training Epoch: 2/2, step 5362/7134 completed (loss: 0.08630648255348206, acc: 0.9729729890823364)
[2025-02-13 20:41:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:55][root][INFO] - Training Epoch: 2/2, step 5363/7134 completed (loss: 0.0889686793088913, acc: 0.9878048896789551)
[2025-02-13 20:41:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:56][root][INFO] - Training Epoch: 2/2, step 5364/7134 completed (loss: 0.041610799729824066, acc: 0.9884393215179443)
[2025-02-13 20:41:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:56][root][INFO] - Training Epoch: 2/2, step 5365/7134 completed (loss: 0.06256236881017685, acc: 0.9772727489471436)
[2025-02-13 20:41:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:56][root][INFO] - Training Epoch: 2/2, step 5366/7134 completed (loss: 0.13161182403564453, acc: 0.9850746393203735)
[2025-02-13 20:41:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:57][root][INFO] - Training Epoch: 2/2, step 5367/7134 completed (loss: 0.13865895569324493, acc: 0.97826087474823)
[2025-02-13 20:41:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:57][root][INFO] - Training Epoch: 2/2, step 5368/7134 completed (loss: 0.056755419820547104, acc: 0.9922480583190918)
[2025-02-13 20:41:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:58][root][INFO] - Training Epoch: 2/2, step 5369/7134 completed (loss: 0.11924930661916733, acc: 0.9830508232116699)
[2025-02-13 20:41:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:58][root][INFO] - Training Epoch: 2/2, step 5370/7134 completed (loss: 0.10056499391794205, acc: 0.9805194735527039)
[2025-02-13 20:41:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:58][root][INFO] - Training Epoch: 2/2, step 5371/7134 completed (loss: 0.21240049600601196, acc: 0.9599999785423279)
[2025-02-13 20:41:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:59][root][INFO] - Training Epoch: 2/2, step 5372/7134 completed (loss: 0.05882050842046738, acc: 0.9874213933944702)
[2025-02-13 20:41:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:59][root][INFO] - Training Epoch: 2/2, step 5373/7134 completed (loss: 0.0460967980325222, acc: 0.9920634627342224)
[2025-02-13 20:41:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:41:59][root][INFO] - Training Epoch: 2/2, step 5374/7134 completed (loss: 0.011149792931973934, acc: 1.0)
[2025-02-13 20:42:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:00][root][INFO] - Training Epoch: 2/2, step 5375/7134 completed (loss: 0.029109200462698936, acc: 1.0)
[2025-02-13 20:42:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:00][root][INFO] - Training Epoch: 2/2, step 5376/7134 completed (loss: 0.04511982947587967, acc: 0.9729729890823364)
[2025-02-13 20:42:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:01][root][INFO] - Training Epoch: 2/2, step 5377/7134 completed (loss: 0.043370261788368225, acc: 0.9938271641731262)
[2025-02-13 20:42:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:01][root][INFO] - Training Epoch: 2/2, step 5378/7134 completed (loss: 0.027049286291003227, acc: 0.9947368502616882)
[2025-02-13 20:42:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:01][root][INFO] - Training Epoch: 2/2, step 5379/7134 completed (loss: 0.054556865245103836, acc: 0.9895833134651184)
[2025-02-13 20:42:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:02][root][INFO] - Training Epoch: 2/2, step 5380/7134 completed (loss: 0.07569076120853424, acc: 0.9925925731658936)
[2025-02-13 20:42:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:02][root][INFO] - Training Epoch: 2/2, step 5381/7134 completed (loss: 0.05075887218117714, acc: 0.9887005686759949)
[2025-02-13 20:42:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:02][root][INFO] - Training Epoch: 2/2, step 5382/7134 completed (loss: 0.027804197743535042, acc: 1.0)
[2025-02-13 20:42:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:03][root][INFO] - Training Epoch: 2/2, step 5383/7134 completed (loss: 0.0383182056248188, acc: 0.9838709831237793)
[2025-02-13 20:42:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:03][root][INFO] - Training Epoch: 2/2, step 5384/7134 completed (loss: 0.050450023263692856, acc: 0.9796954393386841)
[2025-02-13 20:42:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:04][root][INFO] - Training Epoch: 2/2, step 5385/7134 completed (loss: 0.09018637984991074, acc: 0.9825581312179565)
[2025-02-13 20:42:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:04][root][INFO] - Training Epoch: 2/2, step 5386/7134 completed (loss: 0.040887437760829926, acc: 0.9890109896659851)
[2025-02-13 20:42:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:05][root][INFO] - Training Epoch: 2/2, step 5387/7134 completed (loss: 0.013853004202246666, acc: 1.0)
[2025-02-13 20:42:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:05][root][INFO] - Training Epoch: 2/2, step 5388/7134 completed (loss: 0.024859663099050522, acc: 0.994413435459137)
[2025-02-13 20:42:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:05][root][INFO] - Training Epoch: 2/2, step 5389/7134 completed (loss: 0.033263400197029114, acc: 0.9931972622871399)
[2025-02-13 20:42:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:06][root][INFO] - Training Epoch: 2/2, step 5390/7134 completed (loss: 0.07669077813625336, acc: 0.9937106966972351)
[2025-02-13 20:42:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:06][root][INFO] - Training Epoch: 2/2, step 5391/7134 completed (loss: 0.18214863538742065, acc: 0.9784172773361206)
[2025-02-13 20:42:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:06][root][INFO] - Training Epoch: 2/2, step 5392/7134 completed (loss: 0.21924248337745667, acc: 0.9488636255264282)
[2025-02-13 20:42:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:07][root][INFO] - Training Epoch: 2/2, step 5393/7134 completed (loss: 0.10865713655948639, acc: 0.9666666388511658)
[2025-02-13 20:42:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:07][root][INFO] - Training Epoch: 2/2, step 5394/7134 completed (loss: 0.2699548900127411, acc: 0.921875)
[2025-02-13 20:42:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:08][root][INFO] - Training Epoch: 2/2, step 5395/7134 completed (loss: 0.060394976288080215, acc: 0.9797297120094299)
[2025-02-13 20:42:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:08][root][INFO] - Training Epoch: 2/2, step 5396/7134 completed (loss: 0.15809611976146698, acc: 0.9605262875556946)
[2025-02-13 20:42:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:08][root][INFO] - Training Epoch: 2/2, step 5397/7134 completed (loss: 0.07650845497846603, acc: 0.9716312289237976)
[2025-02-13 20:42:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:09][root][INFO] - Training Epoch: 2/2, step 5398/7134 completed (loss: 0.13522066175937653, acc: 0.9698795080184937)
[2025-02-13 20:42:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:09][root][INFO] - Training Epoch: 2/2, step 5399/7134 completed (loss: 0.10520577430725098, acc: 0.977142870426178)
[2025-02-13 20:42:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:09][root][INFO] - Training Epoch: 2/2, step 5400/7134 completed (loss: 0.2619986832141876, acc: 0.9655172228813171)
[2025-02-13 20:42:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:10][root][INFO] - Training Epoch: 2/2, step 5401/7134 completed (loss: 0.07989248633384705, acc: 0.9722222089767456)
[2025-02-13 20:42:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:10][root][INFO] - Training Epoch: 2/2, step 5402/7134 completed (loss: 0.0581132210791111, acc: 0.9931507110595703)
[2025-02-13 20:42:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:11][root][INFO] - Training Epoch: 2/2, step 5403/7134 completed (loss: 0.13967584073543549, acc: 0.9754098653793335)
[2025-02-13 20:42:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:11][root][INFO] - Training Epoch: 2/2, step 5404/7134 completed (loss: 0.16994279623031616, acc: 0.9664429426193237)
[2025-02-13 20:42:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:11][root][INFO] - Training Epoch: 2/2, step 5405/7134 completed (loss: 0.06823109090328217, acc: 0.9826086759567261)
[2025-02-13 20:42:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:12][root][INFO] - Training Epoch: 2/2, step 5406/7134 completed (loss: 0.06269022077322006, acc: 0.9729729890823364)
[2025-02-13 20:42:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:12][root][INFO] - Training Epoch: 2/2, step 5407/7134 completed (loss: 0.06604622304439545, acc: 0.9890109896659851)
[2025-02-13 20:42:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:12][root][INFO] - Training Epoch: 2/2, step 5408/7134 completed (loss: 0.225407674908638, acc: 0.9432623982429504)
[2025-02-13 20:42:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:13][root][INFO] - Training Epoch: 2/2, step 5409/7134 completed (loss: 0.24559183418750763, acc: 0.9720279574394226)
[2025-02-13 20:42:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:13][root][INFO] - Training Epoch: 2/2, step 5410/7134 completed (loss: 0.052465181797742844, acc: 0.9879518151283264)
[2025-02-13 20:42:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:13][root][INFO] - Training Epoch: 2/2, step 5411/7134 completed (loss: 0.10362111032009125, acc: 0.9647887349128723)
[2025-02-13 20:42:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:14][root][INFO] - Training Epoch: 2/2, step 5412/7134 completed (loss: 0.18851038813591003, acc: 0.982300877571106)
[2025-02-13 20:42:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:14][root][INFO] - Training Epoch: 2/2, step 5413/7134 completed (loss: 0.11921397596597672, acc: 0.9652777910232544)
[2025-02-13 20:42:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:15][root][INFO] - Training Epoch: 2/2, step 5414/7134 completed (loss: 0.28283095359802246, acc: 0.9281437397003174)
[2025-02-13 20:42:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:15][root][INFO] - Training Epoch: 2/2, step 5415/7134 completed (loss: 0.175509512424469, acc: 0.9610389471054077)
[2025-02-13 20:42:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:15][root][INFO] - Training Epoch: 2/2, step 5416/7134 completed (loss: 0.18652576208114624, acc: 0.9572192430496216)
[2025-02-13 20:42:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:16][root][INFO] - Training Epoch: 2/2, step 5417/7134 completed (loss: 0.094732865691185, acc: 0.9679487347602844)
[2025-02-13 20:42:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:16][root][INFO] - Training Epoch: 2/2, step 5418/7134 completed (loss: 0.16053034365177155, acc: 0.9513888955116272)
[2025-02-13 20:42:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:16][root][INFO] - Training Epoch: 2/2, step 5419/7134 completed (loss: 0.40058642625808716, acc: 0.8978102207183838)
[2025-02-13 20:42:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:17][root][INFO] - Training Epoch: 2/2, step 5420/7134 completed (loss: 0.4476490616798401, acc: 0.9473684430122375)
[2025-02-13 20:42:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:17][root][INFO] - Training Epoch: 2/2, step 5421/7134 completed (loss: 0.3811381757259369, acc: 0.9312499761581421)
[2025-02-13 20:42:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:18][root][INFO] - Training Epoch: 2/2, step 5422/7134 completed (loss: 0.10687777400016785, acc: 0.9861111044883728)
[2025-02-13 20:42:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:18][root][INFO] - Training Epoch: 2/2, step 5423/7134 completed (loss: 0.06492637097835541, acc: 1.0)
[2025-02-13 20:42:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:18][root][INFO] - Training Epoch: 2/2, step 5424/7134 completed (loss: 0.1010737493634224, acc: 0.9846153855323792)
[2025-02-13 20:42:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:19][root][INFO] - Training Epoch: 2/2, step 5425/7134 completed (loss: 0.11281529814004898, acc: 0.96875)
[2025-02-13 20:42:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:19][root][INFO] - Training Epoch: 2/2, step 5426/7134 completed (loss: 0.16047675907611847, acc: 0.9754098653793335)
[2025-02-13 20:42:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:19][root][INFO] - Training Epoch: 2/2, step 5427/7134 completed (loss: 0.05923972278833389, acc: 0.9934640526771545)
[2025-02-13 20:42:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:20][root][INFO] - Training Epoch: 2/2, step 5428/7134 completed (loss: 0.06754089146852493, acc: 0.9862068891525269)
[2025-02-13 20:42:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:20][root][INFO] - Training Epoch: 2/2, step 5429/7134 completed (loss: 0.1461855173110962, acc: 0.9530201554298401)
[2025-02-13 20:42:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:20][root][INFO] - Training Epoch: 2/2, step 5430/7134 completed (loss: 0.186426043510437, acc: 0.9675324559211731)
[2025-02-13 20:42:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:21][root][INFO] - Training Epoch: 2/2, step 5431/7134 completed (loss: 0.09502319991588593, acc: 0.9473684430122375)
[2025-02-13 20:42:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:21][root][INFO] - Training Epoch: 2/2, step 5432/7134 completed (loss: 0.07177741080522537, acc: 0.984375)
[2025-02-13 20:42:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:22][root][INFO] - Training Epoch: 2/2, step 5433/7134 completed (loss: 0.1565702110528946, acc: 0.9663865566253662)
[2025-02-13 20:42:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:22][root][INFO] - Training Epoch: 2/2, step 5434/7134 completed (loss: 0.0935380682349205, acc: 0.9677419066429138)
[2025-02-13 20:42:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:22][root][INFO] - Training Epoch: 2/2, step 5435/7134 completed (loss: 0.08691450208425522, acc: 0.984000027179718)
[2025-02-13 20:42:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:23][root][INFO] - Training Epoch: 2/2, step 5436/7134 completed (loss: 0.0888180211186409, acc: 0.9870967864990234)
[2025-02-13 20:42:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:23][root][INFO] - Training Epoch: 2/2, step 5437/7134 completed (loss: 0.1257287859916687, acc: 0.9924812316894531)
[2025-02-13 20:42:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:23][root][INFO] - Training Epoch: 2/2, step 5438/7134 completed (loss: 0.16589584946632385, acc: 0.9444444179534912)
[2025-02-13 20:42:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:24][root][INFO] - Training Epoch: 2/2, step 5439/7134 completed (loss: 0.21630196273326874, acc: 0.939393937587738)
[2025-02-13 20:42:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:24][root][INFO] - Training Epoch: 2/2, step 5440/7134 completed (loss: 0.20981644093990326, acc: 0.949999988079071)
[2025-02-13 20:42:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:25][root][INFO] - Training Epoch: 2/2, step 5441/7134 completed (loss: 0.1510845273733139, acc: 0.9729729890823364)
[2025-02-13 20:42:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:25][root][INFO] - Training Epoch: 2/2, step 5442/7134 completed (loss: 0.07493092864751816, acc: 0.9821428656578064)
[2025-02-13 20:42:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:25][root][INFO] - Training Epoch: 2/2, step 5443/7134 completed (loss: 0.050486546009778976, acc: 0.9939024448394775)
[2025-02-13 20:42:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:26][root][INFO] - Training Epoch: 2/2, step 5444/7134 completed (loss: 0.08093470335006714, acc: 0.9852941036224365)
[2025-02-13 20:42:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:26][root][INFO] - Training Epoch: 2/2, step 5445/7134 completed (loss: 0.1581025868654251, acc: 0.9485294222831726)
[2025-02-13 20:42:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:26][root][INFO] - Training Epoch: 2/2, step 5446/7134 completed (loss: 0.12318956106901169, acc: 0.9693251252174377)
[2025-02-13 20:42:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:27][root][INFO] - Training Epoch: 2/2, step 5447/7134 completed (loss: 0.10559394955635071, acc: 0.965753436088562)
[2025-02-13 20:42:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:27][root][INFO] - Training Epoch: 2/2, step 5448/7134 completed (loss: 0.11820875853300095, acc: 0.9670329689979553)
[2025-02-13 20:42:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:27][root][INFO] - Training Epoch: 2/2, step 5449/7134 completed (loss: 0.12158767879009247, acc: 0.9672130942344666)
[2025-02-13 20:42:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:28][root][INFO] - Training Epoch: 2/2, step 5450/7134 completed (loss: 0.20320358872413635, acc: 0.9666666388511658)
[2025-02-13 20:42:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:28][root][INFO] - Training Epoch: 2/2, step 5451/7134 completed (loss: 0.06997715681791306, acc: 0.976331353187561)
[2025-02-13 20:42:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:29][root][INFO] - Training Epoch: 2/2, step 5452/7134 completed (loss: 0.07762409746646881, acc: 0.9704142212867737)
[2025-02-13 20:42:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:29][root][INFO] - Training Epoch: 2/2, step 5453/7134 completed (loss: 0.18481355905532837, acc: 0.949999988079071)
[2025-02-13 20:42:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:29][root][INFO] - Training Epoch: 2/2, step 5454/7134 completed (loss: 0.10418746620416641, acc: 0.9728260636329651)
[2025-02-13 20:42:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:30][root][INFO] - Training Epoch: 2/2, step 5455/7134 completed (loss: 0.10761042684316635, acc: 0.9893048405647278)
[2025-02-13 20:42:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:30][root][INFO] - Training Epoch: 2/2, step 5456/7134 completed (loss: 0.08832702040672302, acc: 0.9784946441650391)
[2025-02-13 20:42:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:31][root][INFO] - Training Epoch: 2/2, step 5457/7134 completed (loss: 0.118665911257267, acc: 0.977011501789093)
[2025-02-13 20:42:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:31][root][INFO] - Training Epoch: 2/2, step 5458/7134 completed (loss: 0.07117319852113724, acc: 0.988950252532959)
[2025-02-13 20:42:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:31][root][INFO] - Training Epoch: 2/2, step 5459/7134 completed (loss: 0.02211582101881504, acc: 1.0)
[2025-02-13 20:42:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:32][root][INFO] - Training Epoch: 2/2, step 5460/7134 completed (loss: 0.0565778873860836, acc: 0.9881656765937805)
[2025-02-13 20:42:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:32][root][INFO] - Training Epoch: 2/2, step 5461/7134 completed (loss: 0.09513062238693237, acc: 0.9724770784378052)
[2025-02-13 20:42:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:33][root][INFO] - Training Epoch: 2/2, step 5462/7134 completed (loss: 0.056174203753471375, acc: 0.987730085849762)
[2025-02-13 20:42:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:33][root][INFO] - Training Epoch: 2/2, step 5463/7134 completed (loss: 0.07717512547969818, acc: 0.9814814925193787)
[2025-02-13 20:42:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:33][root][INFO] - Training Epoch: 2/2, step 5464/7134 completed (loss: 0.11872390657663345, acc: 0.9726775884628296)
[2025-02-13 20:42:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:34][root][INFO] - Training Epoch: 2/2, step 5465/7134 completed (loss: 0.09400690346956253, acc: 0.9937499761581421)
[2025-02-13 20:42:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:34][root][INFO] - Training Epoch: 2/2, step 5466/7134 completed (loss: 0.07506721466779709, acc: 0.9751243591308594)
[2025-02-13 20:42:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:34][root][INFO] - Training Epoch: 2/2, step 5467/7134 completed (loss: 0.05240563303232193, acc: 0.9900497794151306)
[2025-02-13 20:42:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:35][root][INFO] - Training Epoch: 2/2, step 5468/7134 completed (loss: 0.1887178272008896, acc: 0.9842932224273682)
[2025-02-13 20:42:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:35][root][INFO] - Training Epoch: 2/2, step 5469/7134 completed (loss: 0.13934902846813202, acc: 0.9576719403266907)
[2025-02-13 20:42:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:36][root][INFO] - Training Epoch: 2/2, step 5470/7134 completed (loss: 0.06379985809326172, acc: 0.9800994992256165)
[2025-02-13 20:42:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:36][root][INFO] - Training Epoch: 2/2, step 5471/7134 completed (loss: 0.04725078120827675, acc: 0.9882352948188782)
[2025-02-13 20:42:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:37][root][INFO] - Training Epoch: 2/2, step 5472/7134 completed (loss: 0.0884077399969101, acc: 0.9888268113136292)
[2025-02-13 20:42:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:37][root][INFO] - Training Epoch: 2/2, step 5473/7134 completed (loss: 0.020173626020550728, acc: 0.9949495196342468)
[2025-02-13 20:42:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:37][root][INFO] - Training Epoch: 2/2, step 5474/7134 completed (loss: 0.07466990500688553, acc: 0.9818181991577148)
[2025-02-13 20:42:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:38][root][INFO] - Training Epoch: 2/2, step 5475/7134 completed (loss: 0.028801973909139633, acc: 1.0)
[2025-02-13 20:42:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:38][root][INFO] - Training Epoch: 2/2, step 5476/7134 completed (loss: 0.009411980397999287, acc: 1.0)
[2025-02-13 20:42:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:38][root][INFO] - Training Epoch: 2/2, step 5477/7134 completed (loss: 0.02648269198834896, acc: 0.9948979616165161)
[2025-02-13 20:42:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:39][root][INFO] - Training Epoch: 2/2, step 5478/7134 completed (loss: 0.1510152369737625, acc: 0.970370352268219)
[2025-02-13 20:42:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:39][root][INFO] - Training Epoch: 2/2, step 5479/7134 completed (loss: 0.16047705709934235, acc: 0.9729729890823364)
[2025-02-13 20:42:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:40][root][INFO] - Training Epoch: 2/2, step 5480/7134 completed (loss: 0.09449553489685059, acc: 0.976190447807312)
[2025-02-13 20:42:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:40][root][INFO] - Training Epoch: 2/2, step 5481/7134 completed (loss: 0.05294441059231758, acc: 0.9918032884597778)
[2025-02-13 20:42:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:41][root][INFO] - Training Epoch: 2/2, step 5482/7134 completed (loss: 0.2836746871471405, acc: 0.9432989954948425)
[2025-02-13 20:42:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:41][root][INFO] - Training Epoch: 2/2, step 5483/7134 completed (loss: 0.11929338425397873, acc: 0.9636363387107849)
[2025-02-13 20:42:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:41][root][INFO] - Training Epoch: 2/2, step 5484/7134 completed (loss: 0.0606350302696228, acc: 0.9876543283462524)
[2025-02-13 20:42:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:42][root][INFO] - Training Epoch: 2/2, step 5485/7134 completed (loss: 0.12342918664216995, acc: 0.975806474685669)
[2025-02-13 20:42:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:42][root][INFO] - Training Epoch: 2/2, step 5486/7134 completed (loss: 0.20460060238838196, acc: 0.9772727489471436)
[2025-02-13 20:42:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:43][root][INFO] - Training Epoch: 2/2, step 5487/7134 completed (loss: 0.10602068156003952, acc: 0.9659863710403442)
[2025-02-13 20:42:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:43][root][INFO] - Training Epoch: 2/2, step 5488/7134 completed (loss: 0.11836116760969162, acc: 0.976047933101654)
[2025-02-13 20:42:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:43][root][INFO] - Training Epoch: 2/2, step 5489/7134 completed (loss: 0.24533092975616455, acc: 0.9390243887901306)
[2025-02-13 20:42:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:44][root][INFO] - Training Epoch: 2/2, step 5490/7134 completed (loss: 0.1326671987771988, acc: 0.9696969985961914)
[2025-02-13 20:42:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:44][root][INFO] - Training Epoch: 2/2, step 5491/7134 completed (loss: 0.09889757633209229, acc: 0.9649122953414917)
[2025-02-13 20:42:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:44][root][INFO] - Training Epoch: 2/2, step 5492/7134 completed (loss: 0.09359187632799149, acc: 0.9925373196601868)
[2025-02-13 20:42:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:45][root][INFO] - Training Epoch: 2/2, step 5493/7134 completed (loss: 0.23220209777355194, acc: 0.953125)
[2025-02-13 20:42:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:45][root][INFO] - Training Epoch: 2/2, step 5494/7134 completed (loss: 0.11377154290676117, acc: 0.9748427867889404)
[2025-02-13 20:42:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:45][root][INFO] - Training Epoch: 2/2, step 5495/7134 completed (loss: 0.0806359127163887, acc: 0.9849624037742615)
[2025-02-13 20:42:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:46][root][INFO] - Training Epoch: 2/2, step 5496/7134 completed (loss: 0.15790098905563354, acc: 0.9523809552192688)
[2025-02-13 20:42:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:46][root][INFO] - Training Epoch: 2/2, step 5497/7134 completed (loss: 0.1485981047153473, acc: 0.9764705896377563)
[2025-02-13 20:42:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:46][root][INFO] - Training Epoch: 2/2, step 5498/7134 completed (loss: 0.15662391483783722, acc: 0.9734042286872864)
[2025-02-13 20:42:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:47][root][INFO] - Training Epoch: 2/2, step 5499/7134 completed (loss: 0.1117425262928009, acc: 0.9819819927215576)
[2025-02-13 20:42:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:47][root][INFO] - Training Epoch: 2/2, step 5500/7134 completed (loss: 0.0684683620929718, acc: 0.984455943107605)
[2025-02-13 20:42:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:48][root][INFO] - Training Epoch: 2/2, step 5501/7134 completed (loss: 0.11808499693870544, acc: 0.9696969985961914)
[2025-02-13 20:42:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:48][root][INFO] - Training Epoch: 2/2, step 5502/7134 completed (loss: 0.1079799011349678, acc: 0.9717513918876648)
[2025-02-13 20:42:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:48][root][INFO] - Training Epoch: 2/2, step 5503/7134 completed (loss: 0.2560088634490967, acc: 0.9692307710647583)
[2025-02-13 20:42:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:49][root][INFO] - Training Epoch: 2/2, step 5504/7134 completed (loss: 0.0647742971777916, acc: 0.9879518151283264)
[2025-02-13 20:42:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:49][root][INFO] - Training Epoch: 2/2, step 5505/7134 completed (loss: 0.230929434299469, acc: 0.9375)
[2025-02-13 20:42:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:50][root][INFO] - Training Epoch: 2/2, step 5506/7134 completed (loss: 0.17001836001873016, acc: 0.9646464586257935)
[2025-02-13 20:42:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:50][root][INFO] - Training Epoch: 2/2, step 5507/7134 completed (loss: 0.1451893150806427, acc: 0.9707602262496948)
[2025-02-13 20:42:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:50][root][INFO] - Training Epoch: 2/2, step 5508/7134 completed (loss: 0.09515473246574402, acc: 0.9760765433311462)
[2025-02-13 20:42:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:51][root][INFO] - Training Epoch: 2/2, step 5509/7134 completed (loss: 0.206782266497612, acc: 0.9478672742843628)
[2025-02-13 20:42:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:51][root][INFO] - Training Epoch: 2/2, step 5510/7134 completed (loss: 0.16419123113155365, acc: 0.9516128897666931)
[2025-02-13 20:42:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:51][root][INFO] - Training Epoch: 2/2, step 5511/7134 completed (loss: 0.20282447338104248, acc: 0.9563106894493103)
[2025-02-13 20:42:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:52][root][INFO] - Training Epoch: 2/2, step 5512/7134 completed (loss: 0.2113265097141266, acc: 0.9505494236946106)
[2025-02-13 20:42:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:52][root][INFO] - Training Epoch: 2/2, step 5513/7134 completed (loss: 0.1548660546541214, acc: 0.9585798978805542)
[2025-02-13 20:42:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:53][root][INFO] - Training Epoch: 2/2, step 5514/7134 completed (loss: 0.2455904334783554, acc: 0.9490740895271301)
[2025-02-13 20:42:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:53][root][INFO] - Training Epoch: 2/2, step 5515/7134 completed (loss: 0.17521126568317413, acc: 0.9583333134651184)
[2025-02-13 20:42:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:53][root][INFO] - Training Epoch: 2/2, step 5516/7134 completed (loss: 0.08353718370199203, acc: 0.9942528605461121)
[2025-02-13 20:42:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:54][root][INFO] - Training Epoch: 2/2, step 5517/7134 completed (loss: 0.09791215509176254, acc: 0.9776536226272583)
[2025-02-13 20:42:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:54][root][INFO] - Training Epoch: 2/2, step 5518/7134 completed (loss: 0.0820208415389061, acc: 0.9777777791023254)
[2025-02-13 20:42:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:54][root][INFO] - Training Epoch: 2/2, step 5519/7134 completed (loss: 0.06665770709514618, acc: 0.9938271641731262)
[2025-02-13 20:42:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:55][root][INFO] - Training Epoch: 2/2, step 5520/7134 completed (loss: 0.05663762241601944, acc: 0.9857819676399231)
[2025-02-13 20:42:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:55][root][INFO] - Training Epoch: 2/2, step 5521/7134 completed (loss: 0.13073532283306122, acc: 0.9685863852500916)
[2025-02-13 20:42:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:56][root][INFO] - Training Epoch: 2/2, step 5522/7134 completed (loss: 0.1803174465894699, acc: 0.9476439952850342)
[2025-02-13 20:42:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:56][root][INFO] - Training Epoch: 2/2, step 5523/7134 completed (loss: 0.1539989560842514, acc: 0.9835164546966553)
[2025-02-13 20:42:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:56][root][INFO] - Training Epoch: 2/2, step 5524/7134 completed (loss: 0.14357881247997284, acc: 0.9620253443717957)
[2025-02-13 20:42:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:57][root][INFO] - Training Epoch: 2/2, step 5525/7134 completed (loss: 0.11115694046020508, acc: 0.9756097793579102)
[2025-02-13 20:42:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:57][root][INFO] - Training Epoch: 2/2, step 5526/7134 completed (loss: 0.05407341569662094, acc: 0.9937888383865356)
[2025-02-13 20:42:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:57][root][INFO] - Training Epoch: 2/2, step 5527/7134 completed (loss: 0.07070066034793854, acc: 0.9790209531784058)
[2025-02-13 20:42:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:58][root][INFO] - Training Epoch: 2/2, step 5528/7134 completed (loss: 0.12475284188985825, acc: 0.9646464586257935)
[2025-02-13 20:42:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:58][root][INFO] - Training Epoch: 2/2, step 5529/7134 completed (loss: 0.11002573370933533, acc: 0.9681817889213562)
[2025-02-13 20:42:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:59][root][INFO] - Training Epoch: 2/2, step 5530/7134 completed (loss: 0.06273751705884933, acc: 0.9863945841789246)
[2025-02-13 20:42:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:59][root][INFO] - Training Epoch: 2/2, step 5531/7134 completed (loss: 0.0920267179608345, acc: 0.9759036302566528)
[2025-02-13 20:42:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:42:59][root][INFO] - Training Epoch: 2/2, step 5532/7134 completed (loss: 0.11317651718854904, acc: 0.9548022747039795)
[2025-02-13 20:43:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:00][root][INFO] - Training Epoch: 2/2, step 5533/7134 completed (loss: 0.033861808478832245, acc: 1.0)
[2025-02-13 20:43:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:00][root][INFO] - Training Epoch: 2/2, step 5534/7134 completed (loss: 0.12382370978593826, acc: 0.966292142868042)
[2025-02-13 20:43:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:01][root][INFO] - Training Epoch: 2/2, step 5535/7134 completed (loss: 0.12630778551101685, acc: 0.9792746305465698)
[2025-02-13 20:43:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:01][root][INFO] - Training Epoch: 2/2, step 5536/7134 completed (loss: 0.055379074066877365, acc: 0.994535505771637)
[2025-02-13 20:43:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:01][root][INFO] - Training Epoch: 2/2, step 5537/7134 completed (loss: 0.08739768713712692, acc: 0.9712643623352051)
[2025-02-13 20:43:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:02][root][INFO] - Training Epoch: 2/2, step 5538/7134 completed (loss: 0.0814438909292221, acc: 0.9842932224273682)
[2025-02-13 20:43:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:02][root][INFO] - Training Epoch: 2/2, step 5539/7134 completed (loss: 0.06055836006999016, acc: 0.9836065769195557)
[2025-02-13 20:43:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:02][root][INFO] - Training Epoch: 2/2, step 5540/7134 completed (loss: 0.1345965564250946, acc: 0.9819276928901672)
[2025-02-13 20:43:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:03][root][INFO] - Training Epoch: 2/2, step 5541/7134 completed (loss: 0.04193538427352905, acc: 0.9941860437393188)
[2025-02-13 20:43:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:03][root][INFO] - Training Epoch: 2/2, step 5542/7134 completed (loss: 0.01719333417713642, acc: 1.0)
[2025-02-13 20:43:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:04][root][INFO] - Training Epoch: 2/2, step 5543/7134 completed (loss: 0.05043339356780052, acc: 0.9893048405647278)
[2025-02-13 20:43:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:04][root][INFO] - Training Epoch: 2/2, step 5544/7134 completed (loss: 0.04843065142631531, acc: 0.9836065769195557)
[2025-02-13 20:43:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:04][root][INFO] - Training Epoch: 2/2, step 5545/7134 completed (loss: 0.0859174057841301, acc: 0.9821428656578064)
[2025-02-13 20:43:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:05][root][INFO] - Training Epoch: 2/2, step 5546/7134 completed (loss: 0.06081290915608406, acc: 0.9820359349250793)
[2025-02-13 20:43:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:05][root][INFO] - Training Epoch: 2/2, step 5547/7134 completed (loss: 0.05154271423816681, acc: 0.9834254384040833)
[2025-02-13 20:43:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:05][root][INFO] - Training Epoch: 2/2, step 5548/7134 completed (loss: 0.05041751265525818, acc: 0.9823529124259949)
[2025-02-13 20:43:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:06][root][INFO] - Training Epoch: 2/2, step 5549/7134 completed (loss: 0.0977024957537651, acc: 0.9865771532058716)
[2025-02-13 20:43:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:06][root][INFO] - Training Epoch: 2/2, step 5550/7134 completed (loss: 0.07627010345458984, acc: 0.9887640476226807)
[2025-02-13 20:43:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:06][root][INFO] - Training Epoch: 2/2, step 5551/7134 completed (loss: 0.14478786289691925, acc: 0.9677419066429138)
[2025-02-13 20:43:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:07][root][INFO] - Training Epoch: 2/2, step 5552/7134 completed (loss: 0.04317692667245865, acc: 0.987730085849762)
[2025-02-13 20:43:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:07][root][INFO] - Training Epoch: 2/2, step 5553/7134 completed (loss: 0.11517693847417831, acc: 0.976331353187561)
[2025-02-13 20:43:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:07][root][INFO] - Training Epoch: 2/2, step 5554/7134 completed (loss: 0.0365995392203331, acc: 0.993630588054657)
[2025-02-13 20:43:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:08][root][INFO] - Training Epoch: 2/2, step 5555/7134 completed (loss: 0.08250018954277039, acc: 0.9821428656578064)
[2025-02-13 20:43:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:08][root][INFO] - Training Epoch: 2/2, step 5556/7134 completed (loss: 0.10610277950763702, acc: 0.9776536226272583)
[2025-02-13 20:43:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:09][root][INFO] - Training Epoch: 2/2, step 5557/7134 completed (loss: 0.08028765767812729, acc: 0.9813664555549622)
[2025-02-13 20:43:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:09][root][INFO] - Training Epoch: 2/2, step 5558/7134 completed (loss: 0.08024562895298004, acc: 0.9833333492279053)
[2025-02-13 20:43:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:09][root][INFO] - Training Epoch: 2/2, step 5559/7134 completed (loss: 0.108134925365448, acc: 0.9776536226272583)
[2025-02-13 20:43:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:10][root][INFO] - Training Epoch: 2/2, step 5560/7134 completed (loss: 0.1438198834657669, acc: 0.9611111283302307)
[2025-02-13 20:43:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:10][root][INFO] - Training Epoch: 2/2, step 5561/7134 completed (loss: 0.07586564123630524, acc: 0.9918032884597778)
[2025-02-13 20:43:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:11][root][INFO] - Training Epoch: 2/2, step 5562/7134 completed (loss: 0.21063710749149323, acc: 0.9568345546722412)
[2025-02-13 20:43:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:11][root][INFO] - Training Epoch: 2/2, step 5563/7134 completed (loss: 0.04710421711206436, acc: 0.9861111044883728)
[2025-02-13 20:43:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:11][root][INFO] - Training Epoch: 2/2, step 5564/7134 completed (loss: 0.059222638607025146, acc: 0.981249988079071)
[2025-02-13 20:43:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:12][root][INFO] - Training Epoch: 2/2, step 5565/7134 completed (loss: 0.02904227003455162, acc: 0.9800000190734863)
[2025-02-13 20:43:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:12][root][INFO] - Training Epoch: 2/2, step 5566/7134 completed (loss: 0.044842060655355453, acc: 0.9949238300323486)
[2025-02-13 20:43:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:12][root][INFO] - Training Epoch: 2/2, step 5567/7134 completed (loss: 0.023770704865455627, acc: 0.9950248599052429)
[2025-02-13 20:43:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:13][root][INFO] - Training Epoch: 2/2, step 5568/7134 completed (loss: 0.03812028095126152, acc: 0.9945054650306702)
[2025-02-13 20:43:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:13][root][INFO] - Training Epoch: 2/2, step 5569/7134 completed (loss: 0.02362656034529209, acc: 0.9947916865348816)
[2025-02-13 20:43:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:13][root][INFO] - Training Epoch: 2/2, step 5570/7134 completed (loss: 0.04201665148139, acc: 0.989130437374115)
[2025-02-13 20:43:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:14][root][INFO] - Training Epoch: 2/2, step 5571/7134 completed (loss: 0.05576496943831444, acc: 0.9873417615890503)
[2025-02-13 20:43:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:14][root][INFO] - Training Epoch: 2/2, step 5572/7134 completed (loss: 0.05243684723973274, acc: 0.9938650131225586)
[2025-02-13 20:43:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:15][root][INFO] - Training Epoch: 2/2, step 5573/7134 completed (loss: 0.0636497288942337, acc: 0.9934640526771545)
[2025-02-13 20:43:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:15][root][INFO] - Training Epoch: 2/2, step 5574/7134 completed (loss: 0.03206342086195946, acc: 0.9930555820465088)
[2025-02-13 20:43:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:15][root][INFO] - Training Epoch: 2/2, step 5575/7134 completed (loss: 0.09536370635032654, acc: 0.9800000190734863)
[2025-02-13 20:43:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:16][root][INFO] - Training Epoch: 2/2, step 5576/7134 completed (loss: 0.05080162733793259, acc: 0.988095223903656)
[2025-02-13 20:43:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:16][root][INFO] - Training Epoch: 2/2, step 5577/7134 completed (loss: 0.10604539513587952, acc: 0.9851852059364319)
[2025-02-13 20:43:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:16][root][INFO] - Training Epoch: 2/2, step 5578/7134 completed (loss: 0.0804578885436058, acc: 0.9746835231781006)
[2025-02-13 20:43:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:17][root][INFO] - Training Epoch: 2/2, step 5579/7134 completed (loss: 0.19322291016578674, acc: 0.9815950989723206)
[2025-02-13 20:43:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:17][root][INFO] - Training Epoch: 2/2, step 5580/7134 completed (loss: 0.032366566359996796, acc: 0.9888888597488403)
[2025-02-13 20:43:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:18][root][INFO] - Training Epoch: 2/2, step 5581/7134 completed (loss: 0.050819385796785355, acc: 0.9878787994384766)
[2025-02-13 20:43:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:18][root][INFO] - Training Epoch: 2/2, step 5582/7134 completed (loss: 0.04198433831334114, acc: 0.9834254384040833)
[2025-02-13 20:43:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:18][root][INFO] - Training Epoch: 2/2, step 5583/7134 completed (loss: 0.05114728957414627, acc: 0.979899525642395)
[2025-02-13 20:43:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:19][root][INFO] - Training Epoch: 2/2, step 5584/7134 completed (loss: 0.09170171618461609, acc: 0.9746835231781006)
[2025-02-13 20:43:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:19][root][INFO] - Training Epoch: 2/2, step 5585/7134 completed (loss: 0.10578914731740952, acc: 0.9707317352294922)
[2025-02-13 20:43:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:20][root][INFO] - Training Epoch: 2/2, step 5586/7134 completed (loss: 0.07005470246076584, acc: 0.9795918464660645)
[2025-02-13 20:43:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:20][root][INFO] - Training Epoch: 2/2, step 5587/7134 completed (loss: 0.06628037244081497, acc: 0.9828571677207947)
[2025-02-13 20:43:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:20][root][INFO] - Training Epoch: 2/2, step 5588/7134 completed (loss: 0.021591678261756897, acc: 1.0)
[2025-02-13 20:43:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:21][root][INFO] - Training Epoch: 2/2, step 5589/7134 completed (loss: 0.07339465618133545, acc: 0.9751552939414978)
[2025-02-13 20:43:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:21][root][INFO] - Training Epoch: 2/2, step 5590/7134 completed (loss: 0.08559371531009674, acc: 0.9826589822769165)
[2025-02-13 20:43:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:22][root][INFO] - Training Epoch: 2/2, step 5591/7134 completed (loss: 0.03914574161171913, acc: 0.9942196607589722)
[2025-02-13 20:43:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:22][root][INFO] - Training Epoch: 2/2, step 5592/7134 completed (loss: 0.05491998419165611, acc: 0.9941860437393188)
[2025-02-13 20:43:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:22][root][INFO] - Training Epoch: 2/2, step 5593/7134 completed (loss: 0.046883609145879745, acc: 0.9836956262588501)
[2025-02-13 20:43:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:23][root][INFO] - Training Epoch: 2/2, step 5594/7134 completed (loss: 0.037192996591329575, acc: 0.9943181872367859)
[2025-02-13 20:43:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:23][root][INFO] - Training Epoch: 2/2, step 5595/7134 completed (loss: 0.03535108640789986, acc: 0.9943181872367859)
[2025-02-13 20:43:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:23][root][INFO] - Training Epoch: 2/2, step 5596/7134 completed (loss: 0.03553035855293274, acc: 0.9880239367485046)
[2025-02-13 20:43:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:24][root][INFO] - Training Epoch: 2/2, step 5597/7134 completed (loss: 0.06624724715948105, acc: 0.9879518151283264)
[2025-02-13 20:43:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:24][root][INFO] - Training Epoch: 2/2, step 5598/7134 completed (loss: 0.07307639718055725, acc: 0.9826589822769165)
[2025-02-13 20:43:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:25][root][INFO] - Training Epoch: 2/2, step 5599/7134 completed (loss: 0.0929512158036232, acc: 0.9863945841789246)
[2025-02-13 20:43:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:25][root][INFO] - Training Epoch: 2/2, step 5600/7134 completed (loss: 0.06431908160448074, acc: 0.9857142567634583)
[2025-02-13 20:43:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:25][root][INFO] - Training Epoch: 2/2, step 5601/7134 completed (loss: 0.05280281975865364, acc: 0.9923664331436157)
[2025-02-13 20:43:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:26][root][INFO] - Training Epoch: 2/2, step 5602/7134 completed (loss: 0.08121951669454575, acc: 0.9788732528686523)
[2025-02-13 20:43:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:26][root][INFO] - Training Epoch: 2/2, step 5603/7134 completed (loss: 0.06695162504911423, acc: 0.9672130942344666)
[2025-02-13 20:43:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:26][root][INFO] - Training Epoch: 2/2, step 5604/7134 completed (loss: 0.08027589321136475, acc: 0.9837398529052734)
[2025-02-13 20:43:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:27][root][INFO] - Training Epoch: 2/2, step 5605/7134 completed (loss: 0.09537666290998459, acc: 0.9763779640197754)
[2025-02-13 20:43:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:27][root][INFO] - Training Epoch: 2/2, step 5606/7134 completed (loss: 0.07966554909944534, acc: 0.9813084006309509)
[2025-02-13 20:43:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:27][root][INFO] - Training Epoch: 2/2, step 5607/7134 completed (loss: 0.05004017800092697, acc: 0.9928057789802551)
[2025-02-13 20:43:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:28][root][INFO] - Training Epoch: 2/2, step 5608/7134 completed (loss: 0.06867881864309311, acc: 0.9844961166381836)
[2025-02-13 20:43:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:28][root][INFO] - Training Epoch: 2/2, step 5609/7134 completed (loss: 0.08953594416379929, acc: 0.9666666388511658)
[2025-02-13 20:43:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:29][root][INFO] - Training Epoch: 2/2, step 5610/7134 completed (loss: 0.018038185313344002, acc: 1.0)
[2025-02-13 20:43:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:29][root][INFO] - Training Epoch: 2/2, step 5611/7134 completed (loss: 0.0651300698518753, acc: 0.9850746393203735)
[2025-02-13 20:43:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:29][root][INFO] - Training Epoch: 2/2, step 5612/7134 completed (loss: 0.15359018743038177, acc: 0.9718309640884399)
[2025-02-13 20:43:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:30][root][INFO] - Training Epoch: 2/2, step 5613/7134 completed (loss: 0.08654984086751938, acc: 0.9732142686843872)
[2025-02-13 20:43:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:30][root][INFO] - Training Epoch: 2/2, step 5614/7134 completed (loss: 0.14762964844703674, acc: 0.9865771532058716)
[2025-02-13 20:43:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:30][root][INFO] - Training Epoch: 2/2, step 5615/7134 completed (loss: 0.05524911731481552, acc: 0.9904761910438538)
[2025-02-13 20:43:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:31][root][INFO] - Training Epoch: 2/2, step 5616/7134 completed (loss: 0.06343584507703781, acc: 0.9925925731658936)
[2025-02-13 20:43:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:31][root][INFO] - Training Epoch: 2/2, step 5617/7134 completed (loss: 0.026861924678087234, acc: 1.0)
[2025-02-13 20:43:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:32][root][INFO] - Training Epoch: 2/2, step 5618/7134 completed (loss: 0.08223036676645279, acc: 0.9779411554336548)
[2025-02-13 20:43:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:32][root][INFO] - Training Epoch: 2/2, step 5619/7134 completed (loss: 0.09611816704273224, acc: 0.9579831957817078)
[2025-02-13 20:43:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:32][root][INFO] - Training Epoch: 2/2, step 5620/7134 completed (loss: 0.07141728699207306, acc: 0.9826086759567261)
[2025-02-13 20:43:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:33][root][INFO] - Training Epoch: 2/2, step 5621/7134 completed (loss: 0.04744420573115349, acc: 0.9935483932495117)
[2025-02-13 20:43:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:33][root][INFO] - Training Epoch: 2/2, step 5622/7134 completed (loss: 0.06223459169268608, acc: 0.9786096215248108)
[2025-02-13 20:43:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:33][root][INFO] - Training Epoch: 2/2, step 5623/7134 completed (loss: 0.08688870072364807, acc: 0.9791666865348816)
[2025-02-13 20:43:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:34][root][INFO] - Training Epoch: 2/2, step 5624/7134 completed (loss: 0.07834844291210175, acc: 0.9803921580314636)
[2025-02-13 20:43:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:34][root][INFO] - Training Epoch: 2/2, step 5625/7134 completed (loss: 0.04975210875272751, acc: 0.9901960492134094)
[2025-02-13 20:43:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:35][root][INFO] - Training Epoch: 2/2, step 5626/7134 completed (loss: 0.027955777943134308, acc: 1.0)
[2025-02-13 20:43:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:35][root][INFO] - Training Epoch: 2/2, step 5627/7134 completed (loss: 0.08219383656978607, acc: 0.9750000238418579)
[2025-02-13 20:43:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:35][root][INFO] - Training Epoch: 2/2, step 5628/7134 completed (loss: 0.08456764370203018, acc: 0.9910714030265808)
[2025-02-13 20:43:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:36][root][INFO] - Training Epoch: 2/2, step 5629/7134 completed (loss: 0.029612788930535316, acc: 0.9918032884597778)
[2025-02-13 20:43:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:36][root][INFO] - Training Epoch: 2/2, step 5630/7134 completed (loss: 0.05777378007769585, acc: 0.9901960492134094)
[2025-02-13 20:43:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:37][root][INFO] - Training Epoch: 2/2, step 5631/7134 completed (loss: 0.04266488552093506, acc: 0.9918699264526367)
[2025-02-13 20:43:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:37][root][INFO] - Training Epoch: 2/2, step 5632/7134 completed (loss: 0.075653575360775, acc: 0.9615384340286255)
[2025-02-13 20:43:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:37][root][INFO] - Training Epoch: 2/2, step 5633/7134 completed (loss: 0.05731913074851036, acc: 0.9821428656578064)
[2025-02-13 20:43:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:38][root][INFO] - Training Epoch: 2/2, step 5634/7134 completed (loss: 0.04745054990053177, acc: 0.982300877571106)
[2025-02-13 20:43:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:38][root][INFO] - Training Epoch: 2/2, step 5635/7134 completed (loss: 0.04482957720756531, acc: 0.9921875)
[2025-02-13 20:43:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:38][root][INFO] - Training Epoch: 2/2, step 5636/7134 completed (loss: 0.05187324807047844, acc: 0.9923076629638672)
[2025-02-13 20:43:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:39][root][INFO] - Training Epoch: 2/2, step 5637/7134 completed (loss: 0.03555040806531906, acc: 1.0)
[2025-02-13 20:43:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:39][root][INFO] - Training Epoch: 2/2, step 5638/7134 completed (loss: 0.028763974085450172, acc: 1.0)
[2025-02-13 20:43:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:40][root][INFO] - Training Epoch: 2/2, step 5639/7134 completed (loss: 0.07910080254077911, acc: 0.9935064911842346)
[2025-02-13 20:43:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:40][root][INFO] - Training Epoch: 2/2, step 5640/7134 completed (loss: 0.058724354952573776, acc: 0.9858155846595764)
[2025-02-13 20:43:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:40][root][INFO] - Training Epoch: 2/2, step 5641/7134 completed (loss: 0.09037069976329803, acc: 0.9775280952453613)
[2025-02-13 20:43:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:41][root][INFO] - Training Epoch: 2/2, step 5642/7134 completed (loss: 0.057770635932683945, acc: 1.0)
[2025-02-13 20:43:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:41][root][INFO] - Training Epoch: 2/2, step 5643/7134 completed (loss: 0.07063942402601242, acc: 0.9789473414421082)
[2025-02-13 20:43:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:41][root][INFO] - Training Epoch: 2/2, step 5644/7134 completed (loss: 0.04497552663087845, acc: 1.0)
[2025-02-13 20:43:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:42][root][INFO] - Training Epoch: 2/2, step 5645/7134 completed (loss: 0.03540016710758209, acc: 1.0)
[2025-02-13 20:43:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:42][root][INFO] - Training Epoch: 2/2, step 5646/7134 completed (loss: 0.03522863611578941, acc: 1.0)
[2025-02-13 20:43:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:43][root][INFO] - Training Epoch: 2/2, step 5647/7134 completed (loss: 0.05463409423828125, acc: 0.9930555820465088)
[2025-02-13 20:43:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:43][root][INFO] - Training Epoch: 2/2, step 5648/7134 completed (loss: 0.13196325302124023, acc: 0.9866666793823242)
[2025-02-13 20:43:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:43][root][INFO] - Training Epoch: 2/2, step 5649/7134 completed (loss: 0.13117744028568268, acc: 0.9662162065505981)
[2025-02-13 20:43:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:44][root][INFO] - Training Epoch: 2/2, step 5650/7134 completed (loss: 0.09363103657960892, acc: 0.9677419066429138)
[2025-02-13 20:43:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:44][root][INFO] - Training Epoch: 2/2, step 5651/7134 completed (loss: 0.06749141216278076, acc: 0.9727891087532043)
[2025-02-13 20:43:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:45][root][INFO] - Training Epoch: 2/2, step 5652/7134 completed (loss: 0.07816851884126663, acc: 0.9928057789802551)
[2025-02-13 20:43:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:45][root][INFO] - Training Epoch: 2/2, step 5653/7134 completed (loss: 0.03663066774606705, acc: 1.0)
[2025-02-13 20:43:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:45][root][INFO] - Training Epoch: 2/2, step 5654/7134 completed (loss: 0.042696624994277954, acc: 0.9921875)
[2025-02-13 20:43:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:46][root][INFO] - Training Epoch: 2/2, step 5655/7134 completed (loss: 0.04220418259501457, acc: 0.9906542301177979)
[2025-02-13 20:43:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:46][root][INFO] - Training Epoch: 2/2, step 5656/7134 completed (loss: 0.0706455409526825, acc: 0.9836065769195557)
[2025-02-13 20:43:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:46][root][INFO] - Training Epoch: 2/2, step 5657/7134 completed (loss: 0.16398456692695618, acc: 0.9536423683166504)
[2025-02-13 20:43:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:47][root][INFO] - Training Epoch: 2/2, step 5658/7134 completed (loss: 0.06494762003421783, acc: 0.9869281053543091)
[2025-02-13 20:43:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:47][root][INFO] - Training Epoch: 2/2, step 5659/7134 completed (loss: 0.12087994813919067, acc: 0.9714285731315613)
[2025-02-13 20:43:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:48][root][INFO] - Training Epoch: 2/2, step 5660/7134 completed (loss: 0.09248606115579605, acc: 0.9765625)
[2025-02-13 20:43:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:48][root][INFO] - Training Epoch: 2/2, step 5661/7134 completed (loss: 0.06312797963619232, acc: 0.9915966391563416)
[2025-02-13 20:43:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:48][root][INFO] - Training Epoch: 2/2, step 5662/7134 completed (loss: 0.06171952560544014, acc: 0.9784172773361206)
[2025-02-13 20:43:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:49][root][INFO] - Training Epoch: 2/2, step 5663/7134 completed (loss: 0.02520707994699478, acc: 1.0)
[2025-02-13 20:43:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:49][root][INFO] - Training Epoch: 2/2, step 5664/7134 completed (loss: 0.1897273063659668, acc: 0.965753436088562)
[2025-02-13 20:43:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:49][root][INFO] - Training Epoch: 2/2, step 5665/7134 completed (loss: 0.04319692403078079, acc: 0.9931972622871399)
[2025-02-13 20:43:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:50][root][INFO] - Training Epoch: 2/2, step 5666/7134 completed (loss: 0.07535794377326965, acc: 0.9869281053543091)
[2025-02-13 20:43:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:50][root][INFO] - Training Epoch: 2/2, step 5667/7134 completed (loss: 0.10219378024339676, acc: 0.9756097793579102)
[2025-02-13 20:43:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:51][root][INFO] - Training Epoch: 2/2, step 5668/7134 completed (loss: 0.10499467700719833, acc: 0.9777777791023254)
[2025-02-13 20:43:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:51][root][INFO] - Training Epoch: 2/2, step 5669/7134 completed (loss: 0.05892755463719368, acc: 0.9808917045593262)
[2025-02-13 20:43:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:51][root][INFO] - Training Epoch: 2/2, step 5670/7134 completed (loss: 0.04239629954099655, acc: 0.9920634627342224)
[2025-02-13 20:43:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:52][root][INFO] - Training Epoch: 2/2, step 5671/7134 completed (loss: 0.052206385880708694, acc: 0.9943820238113403)
[2025-02-13 20:43:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:52][root][INFO] - Training Epoch: 2/2, step 5672/7134 completed (loss: 0.05921868979930878, acc: 0.9851852059364319)
[2025-02-13 20:43:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:52][root][INFO] - Training Epoch: 2/2, step 5673/7134 completed (loss: 0.044893596321344376, acc: 0.9863013625144958)
[2025-02-13 20:43:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:53][root][INFO] - Training Epoch: 2/2, step 5674/7134 completed (loss: 0.0850655660033226, acc: 0.9659863710403442)
[2025-02-13 20:43:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:53][root][INFO] - Training Epoch: 2/2, step 5675/7134 completed (loss: 0.11512938141822815, acc: 0.9733333587646484)
[2025-02-13 20:43:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:54][root][INFO] - Training Epoch: 2/2, step 5676/7134 completed (loss: 0.1927114725112915, acc: 0.9425287246704102)
[2025-02-13 20:43:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:54][root][INFO] - Training Epoch: 2/2, step 5677/7134 completed (loss: 0.1603320688009262, acc: 0.9560439586639404)
[2025-02-13 20:43:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:54][root][INFO] - Training Epoch: 2/2, step 5678/7134 completed (loss: 0.09769789129495621, acc: 0.9814814925193787)
[2025-02-13 20:43:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:55][root][INFO] - Training Epoch: 2/2, step 5679/7134 completed (loss: 0.2206152230501175, acc: 0.9556962251663208)
[2025-02-13 20:43:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:55][root][INFO] - Training Epoch: 2/2, step 5680/7134 completed (loss: 0.2855484187602997, acc: 0.945652186870575)
[2025-02-13 20:43:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:55][root][INFO] - Training Epoch: 2/2, step 5681/7134 completed (loss: 0.11777806282043457, acc: 0.9757575988769531)
[2025-02-13 20:43:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:56][root][INFO] - Training Epoch: 2/2, step 5682/7134 completed (loss: 0.06828320026397705, acc: 0.9879518151283264)
[2025-02-13 20:43:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:56][root][INFO] - Training Epoch: 2/2, step 5683/7134 completed (loss: 0.08684695512056351, acc: 0.9888888597488403)
[2025-02-13 20:43:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:57][root][INFO] - Training Epoch: 2/2, step 5684/7134 completed (loss: 0.30897635221481323, acc: 0.9268292784690857)
[2025-02-13 20:43:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:57][root][INFO] - Training Epoch: 2/2, step 5685/7134 completed (loss: 0.12891648709774017, acc: 0.9763779640197754)
[2025-02-13 20:43:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:57][root][INFO] - Training Epoch: 2/2, step 5686/7134 completed (loss: 0.06316055357456207, acc: 0.9784172773361206)
[2025-02-13 20:43:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:58][root][INFO] - Training Epoch: 2/2, step 5687/7134 completed (loss: 0.15045350790023804, acc: 0.9696969985961914)
[2025-02-13 20:43:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:58][root][INFO] - Training Epoch: 2/2, step 5688/7134 completed (loss: 0.16165415942668915, acc: 0.9777777791023254)
[2025-02-13 20:43:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:58][root][INFO] - Training Epoch: 2/2, step 5689/7134 completed (loss: 0.10404843837022781, acc: 0.9756097793579102)
[2025-02-13 20:43:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:59][root][INFO] - Training Epoch: 2/2, step 5690/7134 completed (loss: 0.08770904690027237, acc: 0.9793814420700073)
[2025-02-13 20:43:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:59][root][INFO] - Training Epoch: 2/2, step 5691/7134 completed (loss: 0.14759109914302826, acc: 0.9672130942344666)
[2025-02-13 20:43:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:43:59][root][INFO] - Training Epoch: 2/2, step 5692/7134 completed (loss: 0.11421285569667816, acc: 0.977142870426178)
[2025-02-13 20:44:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:00][root][INFO] - Training Epoch: 2/2, step 5693/7134 completed (loss: 0.019225377589464188, acc: 1.0)
[2025-02-13 20:44:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:00][root][INFO] - Training Epoch: 2/2, step 5694/7134 completed (loss: 0.06783934682607651, acc: 0.9883720874786377)
[2025-02-13 20:44:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:01][root][INFO] - Training Epoch: 2/2, step 5695/7134 completed (loss: 0.03930802270770073, acc: 1.0)
[2025-02-13 20:44:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:01][root][INFO] - Training Epoch: 2/2, step 5696/7134 completed (loss: 0.07116463035345078, acc: 0.9851484894752502)
[2025-02-13 20:44:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:01][root][INFO] - Training Epoch: 2/2, step 5697/7134 completed (loss: 0.07928484678268433, acc: 0.9757575988769531)
[2025-02-13 20:44:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:02][root][INFO] - Training Epoch: 2/2, step 5698/7134 completed (loss: 0.0741117000579834, acc: 0.9829545617103577)
[2025-02-13 20:44:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:02][root][INFO] - Training Epoch: 2/2, step 5699/7134 completed (loss: 0.11359997093677521, acc: 0.9885057210922241)
[2025-02-13 20:44:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:02][root][INFO] - Training Epoch: 2/2, step 5700/7134 completed (loss: 0.0703207477927208, acc: 0.9745222926139832)
[2025-02-13 20:44:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:03][root][INFO] - Training Epoch: 2/2, step 5701/7134 completed (loss: 0.03515852242708206, acc: 0.9945945739746094)
[2025-02-13 20:44:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:03][root][INFO] - Training Epoch: 2/2, step 5702/7134 completed (loss: 0.0968269631266594, acc: 0.9825581312179565)
[2025-02-13 20:44:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:04][root][INFO] - Training Epoch: 2/2, step 5703/7134 completed (loss: 0.08195202052593231, acc: 0.9874213933944702)
[2025-02-13 20:44:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:04][root][INFO] - Training Epoch: 2/2, step 5704/7134 completed (loss: 0.06225520372390747, acc: 0.9823529124259949)
[2025-02-13 20:44:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:04][root][INFO] - Training Epoch: 2/2, step 5705/7134 completed (loss: 0.011122191324830055, acc: 1.0)
[2025-02-13 20:44:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:05][root][INFO] - Training Epoch: 2/2, step 5706/7134 completed (loss: 0.06739957630634308, acc: 0.9752475023269653)
[2025-02-13 20:44:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:05][root][INFO] - Training Epoch: 2/2, step 5707/7134 completed (loss: 0.10731139034032822, acc: 0.9760000109672546)
[2025-02-13 20:44:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:05][root][INFO] - Training Epoch: 2/2, step 5708/7134 completed (loss: 0.020771147683262825, acc: 1.0)
[2025-02-13 20:44:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:06][root][INFO] - Training Epoch: 2/2, step 5709/7134 completed (loss: 0.16345185041427612, acc: 0.9751552939414978)
[2025-02-13 20:44:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:06][root][INFO] - Training Epoch: 2/2, step 5710/7134 completed (loss: 0.07453601062297821, acc: 0.9828571677207947)
[2025-02-13 20:44:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:07][root][INFO] - Training Epoch: 2/2, step 5711/7134 completed (loss: 0.12547720968723297, acc: 0.9731183052062988)
[2025-02-13 20:44:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:07][root][INFO] - Training Epoch: 2/2, step 5712/7134 completed (loss: 0.046982549130916595, acc: 0.9919354915618896)
[2025-02-13 20:44:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:08][root][INFO] - Training Epoch: 2/2, step 5713/7134 completed (loss: 0.0852469727396965, acc: 0.9775280952453613)
[2025-02-13 20:44:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:08][root][INFO] - Training Epoch: 2/2, step 5714/7134 completed (loss: 0.10476776957511902, acc: 0.97826087474823)
[2025-02-13 20:44:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:08][root][INFO] - Training Epoch: 2/2, step 5715/7134 completed (loss: 0.19110378623008728, acc: 0.9558823704719543)
[2025-02-13 20:44:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:09][root][INFO] - Training Epoch: 2/2, step 5716/7134 completed (loss: 0.11579322814941406, acc: 0.9791666865348816)
[2025-02-13 20:44:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:09][root][INFO] - Training Epoch: 2/2, step 5717/7134 completed (loss: 0.10624869167804718, acc: 0.9649122953414917)
[2025-02-13 20:44:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:10][root][INFO] - Training Epoch: 2/2, step 5718/7134 completed (loss: 0.036519791930913925, acc: 0.9868420958518982)
[2025-02-13 20:44:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:10][root][INFO] - Training Epoch: 2/2, step 5719/7134 completed (loss: 0.13590724766254425, acc: 0.9647058844566345)
[2025-02-13 20:44:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:10][root][INFO] - Training Epoch: 2/2, step 5720/7134 completed (loss: 0.10928884148597717, acc: 0.9615384340286255)
[2025-02-13 20:44:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:11][root][INFO] - Training Epoch: 2/2, step 5721/7134 completed (loss: 0.1122284010052681, acc: 0.9740932583808899)
[2025-02-13 20:44:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:11][root][INFO] - Training Epoch: 2/2, step 5722/7134 completed (loss: 0.029931504279375076, acc: 1.0)
[2025-02-13 20:44:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:11][root][INFO] - Training Epoch: 2/2, step 5723/7134 completed (loss: 0.09339398145675659, acc: 0.9894179701805115)
[2025-02-13 20:44:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:12][root][INFO] - Training Epoch: 2/2, step 5724/7134 completed (loss: 0.07673592865467072, acc: 0.9696969985961914)
[2025-02-13 20:44:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:12][root][INFO] - Training Epoch: 2/2, step 5725/7134 completed (loss: 0.12344308197498322, acc: 0.9657142758369446)
[2025-02-13 20:44:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:13][root][INFO] - Training Epoch: 2/2, step 5726/7134 completed (loss: 0.14190010726451874, acc: 0.9640718698501587)
[2025-02-13 20:44:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:13][root][INFO] - Training Epoch: 2/2, step 5727/7134 completed (loss: 0.03226489946246147, acc: 1.0)
[2025-02-13 20:44:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:13][root][INFO] - Training Epoch: 2/2, step 5728/7134 completed (loss: 0.03509889170527458, acc: 0.9926470518112183)
[2025-02-13 20:44:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:14][root][INFO] - Training Epoch: 2/2, step 5729/7134 completed (loss: 0.03653840348124504, acc: 0.9935064911842346)
[2025-02-13 20:44:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:14][root][INFO] - Training Epoch: 2/2, step 5730/7134 completed (loss: 0.060737207531929016, acc: 0.9925925731658936)
[2025-02-13 20:44:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:14][root][INFO] - Training Epoch: 2/2, step 5731/7134 completed (loss: 0.07341426610946655, acc: 0.981249988079071)
[2025-02-13 20:44:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:15][root][INFO] - Training Epoch: 2/2, step 5732/7134 completed (loss: 0.6561881303787231, acc: 0.8373494148254395)
[2025-02-13 20:44:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:15][root][INFO] - Training Epoch: 2/2, step 5733/7134 completed (loss: 0.11926227062940598, acc: 0.9620253443717957)
[2025-02-13 20:44:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:15][root][INFO] - Training Epoch: 2/2, step 5734/7134 completed (loss: 0.06307992339134216, acc: 0.976190447807312)
[2025-02-13 20:44:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:16][root][INFO] - Training Epoch: 2/2, step 5735/7134 completed (loss: 0.10111291706562042, acc: 0.9602649211883545)
[2025-02-13 20:44:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:16][root][INFO] - Training Epoch: 2/2, step 5736/7134 completed (loss: 0.04974308982491493, acc: 1.0)
[2025-02-13 20:44:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:17][root][INFO] - Training Epoch: 2/2, step 5737/7134 completed (loss: 0.046009719371795654, acc: 0.9941520690917969)
[2025-02-13 20:44:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:17][root][INFO] - Training Epoch: 2/2, step 5738/7134 completed (loss: 0.08078446239233017, acc: 0.9848484992980957)
[2025-02-13 20:44:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:17][root][INFO] - Training Epoch: 2/2, step 5739/7134 completed (loss: 0.08134223520755768, acc: 0.9750000238418579)
[2025-02-13 20:44:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:18][root][INFO] - Training Epoch: 2/2, step 5740/7134 completed (loss: 0.05587500333786011, acc: 0.9895833134651184)
[2025-02-13 20:44:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:18][root][INFO] - Training Epoch: 2/2, step 5741/7134 completed (loss: 0.07572435587644577, acc: 0.9886363744735718)
[2025-02-13 20:44:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:19][root][INFO] - Training Epoch: 2/2, step 5742/7134 completed (loss: 0.055342160165309906, acc: 0.9887640476226807)
[2025-02-13 20:44:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:19][root][INFO] - Training Epoch: 2/2, step 5743/7134 completed (loss: 0.04472922161221504, acc: 0.9947916865348816)
[2025-02-13 20:44:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:19][root][INFO] - Training Epoch: 2/2, step 5744/7134 completed (loss: 0.03133522719144821, acc: 0.991150438785553)
[2025-02-13 20:44:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:20][root][INFO] - Training Epoch: 2/2, step 5745/7134 completed (loss: 0.19154807925224304, acc: 0.9550561904907227)
[2025-02-13 20:44:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:20][root][INFO] - Training Epoch: 2/2, step 5746/7134 completed (loss: 0.06474082916975021, acc: 0.9753086566925049)
[2025-02-13 20:44:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:20][root][INFO] - Training Epoch: 2/2, step 5747/7134 completed (loss: 0.10898583382368088, acc: 0.9636363387107849)
[2025-02-13 20:44:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:21][root][INFO] - Training Epoch: 2/2, step 5748/7134 completed (loss: 0.03190144523978233, acc: 0.9921259880065918)
[2025-02-13 20:44:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:21][root][INFO] - Training Epoch: 2/2, step 5749/7134 completed (loss: 0.13407737016677856, acc: 0.976331353187561)
[2025-02-13 20:44:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:22][root][INFO] - Training Epoch: 2/2, step 5750/7134 completed (loss: 0.07803906500339508, acc: 0.9888268113136292)
[2025-02-13 20:44:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:22][root][INFO] - Training Epoch: 2/2, step 5751/7134 completed (loss: 0.12186703830957413, acc: 0.9735099077224731)
[2025-02-13 20:44:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:22][root][INFO] - Training Epoch: 2/2, step 5752/7134 completed (loss: 0.5372998714447021, acc: 0.8702290058135986)
[2025-02-13 20:44:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:23][root][INFO] - Training Epoch: 2/2, step 5753/7134 completed (loss: 0.13191154599189758, acc: 0.9530201554298401)
[2025-02-13 20:44:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:23][root][INFO] - Training Epoch: 2/2, step 5754/7134 completed (loss: 0.17651820182800293, acc: 0.9712643623352051)
[2025-02-13 20:44:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:23][root][INFO] - Training Epoch: 2/2, step 5755/7134 completed (loss: 0.09400659054517746, acc: 0.9805194735527039)
[2025-02-13 20:44:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:24][root][INFO] - Training Epoch: 2/2, step 5756/7134 completed (loss: 0.132377490401268, acc: 0.9593023061752319)
[2025-02-13 20:44:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:24][root][INFO] - Training Epoch: 2/2, step 5757/7134 completed (loss: 0.12399408966302872, acc: 0.9550561904907227)
[2025-02-13 20:44:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:25][root][INFO] - Training Epoch: 2/2, step 5758/7134 completed (loss: 0.23545195162296295, acc: 0.9506173133850098)
[2025-02-13 20:44:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:25][root][INFO] - Training Epoch: 2/2, step 5759/7134 completed (loss: 0.2236742228269577, acc: 0.9417475461959839)
[2025-02-13 20:44:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:25][root][INFO] - Training Epoch: 2/2, step 5760/7134 completed (loss: 0.11012043803930283, acc: 0.9740259647369385)
[2025-02-13 20:44:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:26][root][INFO] - Training Epoch: 2/2, step 5761/7134 completed (loss: 0.11631445586681366, acc: 0.9520000219345093)
[2025-02-13 20:44:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:26][root][INFO] - Training Epoch: 2/2, step 5762/7134 completed (loss: 0.11089172214269638, acc: 0.9689922332763672)
[2025-02-13 20:44:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:27][root][INFO] - Training Epoch: 2/2, step 5763/7134 completed (loss: 0.06449773162603378, acc: 0.9890710115432739)
[2025-02-13 20:44:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:27][root][INFO] - Training Epoch: 2/2, step 5764/7134 completed (loss: 0.026612091809511185, acc: 1.0)
[2025-02-13 20:44:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:27][root][INFO] - Training Epoch: 2/2, step 5765/7134 completed (loss: 0.15592890977859497, acc: 0.955974817276001)
[2025-02-13 20:44:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:28][root][INFO] - Training Epoch: 2/2, step 5766/7134 completed (loss: 0.07916751503944397, acc: 0.9870967864990234)
[2025-02-13 20:44:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:28][root][INFO] - Training Epoch: 2/2, step 5767/7134 completed (loss: 0.04685049504041672, acc: 0.9813664555549622)
[2025-02-13 20:44:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:29][root][INFO] - Training Epoch: 2/2, step 5768/7134 completed (loss: 0.05097741633653641, acc: 0.9937499761581421)
[2025-02-13 20:44:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:29][root][INFO] - Training Epoch: 2/2, step 5769/7134 completed (loss: 0.021386321634054184, acc: 0.9928571581840515)
[2025-02-13 20:44:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:29][root][INFO] - Training Epoch: 2/2, step 5770/7134 completed (loss: 0.052841875702142715, acc: 0.9891892075538635)
[2025-02-13 20:44:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:30][root][INFO] - Training Epoch: 2/2, step 5771/7134 completed (loss: 0.14756964147090912, acc: 0.9631901979446411)
[2025-02-13 20:44:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:30][root][INFO] - Training Epoch: 2/2, step 5772/7134 completed (loss: 0.04469415917992592, acc: 0.9916666746139526)
[2025-02-13 20:44:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:31][root][INFO] - Training Epoch: 2/2, step 5773/7134 completed (loss: 0.038894783705472946, acc: 0.9924812316894531)
[2025-02-13 20:44:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:31][root][INFO] - Training Epoch: 2/2, step 5774/7134 completed (loss: 0.04715882986783981, acc: 0.9876543283462524)
[2025-02-13 20:44:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:31][root][INFO] - Training Epoch: 2/2, step 5775/7134 completed (loss: 0.04666172340512276, acc: 0.9866666793823242)
[2025-02-13 20:44:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:32][root][INFO] - Training Epoch: 2/2, step 5776/7134 completed (loss: 0.0514170341193676, acc: 1.0)
[2025-02-13 20:44:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:32][root][INFO] - Training Epoch: 2/2, step 5777/7134 completed (loss: 0.13483920693397522, acc: 0.9766355156898499)
[2025-02-13 20:44:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:32][root][INFO] - Training Epoch: 2/2, step 5778/7134 completed (loss: 0.2506229877471924, acc: 0.957317054271698)
[2025-02-13 20:44:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:33][root][INFO] - Training Epoch: 2/2, step 5779/7134 completed (loss: 0.07829280942678452, acc: 0.9941176176071167)
[2025-02-13 20:44:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:33][root][INFO] - Training Epoch: 2/2, step 5780/7134 completed (loss: 0.11637614667415619, acc: 0.9601989984512329)
[2025-02-13 20:44:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:33][root][INFO] - Training Epoch: 2/2, step 5781/7134 completed (loss: 0.1535751074552536, acc: 0.9595375657081604)
[2025-02-13 20:44:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:34][root][INFO] - Training Epoch: 2/2, step 5782/7134 completed (loss: 0.02225687727332115, acc: 1.0)
[2025-02-13 20:44:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:34][root][INFO] - Training Epoch: 2/2, step 5783/7134 completed (loss: 0.04338138550519943, acc: 0.9884393215179443)
[2025-02-13 20:44:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:35][root][INFO] - Training Epoch: 2/2, step 5784/7134 completed (loss: 0.04609064385294914, acc: 0.9774011373519897)
[2025-02-13 20:44:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:35][root][INFO] - Training Epoch: 2/2, step 5785/7134 completed (loss: 0.08365286141633987, acc: 0.9783783555030823)
[2025-02-13 20:44:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:35][root][INFO] - Training Epoch: 2/2, step 5786/7134 completed (loss: 0.04058290272951126, acc: 0.9920634627342224)
[2025-02-13 20:44:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:36][root][INFO] - Training Epoch: 2/2, step 5787/7134 completed (loss: 0.060421381145715714, acc: 0.9842932224273682)
[2025-02-13 20:44:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:36][root][INFO] - Training Epoch: 2/2, step 5788/7134 completed (loss: 0.035367757081985474, acc: 0.9934210777282715)
[2025-02-13 20:44:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:36][root][INFO] - Training Epoch: 2/2, step 5789/7134 completed (loss: 0.03750324621796608, acc: 1.0)
[2025-02-13 20:44:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:37][root][INFO] - Training Epoch: 2/2, step 5790/7134 completed (loss: 0.07814202457666397, acc: 0.988095223903656)
[2025-02-13 20:44:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:37][root][INFO] - Training Epoch: 2/2, step 5791/7134 completed (loss: 0.043399594724178314, acc: 0.9850746393203735)
[2025-02-13 20:44:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:38][root][INFO] - Training Epoch: 2/2, step 5792/7134 completed (loss: 0.2020447552204132, acc: 0.9530201554298401)
[2025-02-13 20:44:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:38][root][INFO] - Training Epoch: 2/2, step 5793/7134 completed (loss: 0.094161257147789, acc: 0.9815950989723206)
[2025-02-13 20:44:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:38][root][INFO] - Training Epoch: 2/2, step 5794/7134 completed (loss: 0.28019285202026367, acc: 0.9363057613372803)
[2025-02-13 20:44:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:39][root][INFO] - Training Epoch: 2/2, step 5795/7134 completed (loss: 0.21008215844631195, acc: 0.9398496150970459)
[2025-02-13 20:44:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:39][root][INFO] - Training Epoch: 2/2, step 5796/7134 completed (loss: 0.18329906463623047, acc: 0.9389312863349915)
[2025-02-13 20:44:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:39][root][INFO] - Training Epoch: 2/2, step 5797/7134 completed (loss: 0.1292341649532318, acc: 0.9664429426193237)
[2025-02-13 20:44:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:40][root][INFO] - Training Epoch: 2/2, step 5798/7134 completed (loss: 0.7207161784172058, acc: 0.9011628031730652)
[2025-02-13 20:44:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:40][root][INFO] - Training Epoch: 2/2, step 5799/7134 completed (loss: 0.12867441773414612, acc: 0.960629940032959)
[2025-02-13 20:44:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:41][root][INFO] - Training Epoch: 2/2, step 5800/7134 completed (loss: 0.2522728443145752, acc: 0.935251772403717)
[2025-02-13 20:44:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:41][root][INFO] - Training Epoch: 2/2, step 5801/7134 completed (loss: 0.22480441629886627, acc: 0.9506173133850098)
[2025-02-13 20:44:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:41][root][INFO] - Training Epoch: 2/2, step 5802/7134 completed (loss: 0.057033471763134, acc: 1.0)
[2025-02-13 20:44:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:42][root][INFO] - Training Epoch: 2/2, step 5803/7134 completed (loss: 0.1458539366722107, acc: 0.982758641242981)
[2025-02-13 20:44:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:42][root][INFO] - Training Epoch: 2/2, step 5804/7134 completed (loss: 0.1537766456604004, acc: 0.9363057613372803)
[2025-02-13 20:44:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:43][root][INFO] - Training Epoch: 2/2, step 5805/7134 completed (loss: 0.11730163544416428, acc: 0.9668874144554138)
[2025-02-13 20:44:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:43][root][INFO] - Training Epoch: 2/2, step 5806/7134 completed (loss: 0.04386119917035103, acc: 0.9939393997192383)
[2025-02-13 20:44:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:43][root][INFO] - Training Epoch: 2/2, step 5807/7134 completed (loss: 0.15112978219985962, acc: 0.9577465057373047)
[2025-02-13 20:44:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:44][root][INFO] - Training Epoch: 2/2, step 5808/7134 completed (loss: 0.05938965827226639, acc: 0.9873417615890503)
[2025-02-13 20:44:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:44][root][INFO] - Training Epoch: 2/2, step 5809/7134 completed (loss: 0.03993147611618042, acc: 1.0)
[2025-02-13 20:44:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:44][root][INFO] - Training Epoch: 2/2, step 5810/7134 completed (loss: 0.02814199961721897, acc: 1.0)
[2025-02-13 20:44:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:45][root][INFO] - Training Epoch: 2/2, step 5811/7134 completed (loss: 0.030477125197649002, acc: 1.0)
[2025-02-13 20:44:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:45][root][INFO] - Training Epoch: 2/2, step 5812/7134 completed (loss: 0.042921219021081924, acc: 0.9926470518112183)
[2025-02-13 20:44:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:46][root][INFO] - Training Epoch: 2/2, step 5813/7134 completed (loss: 0.037987977266311646, acc: 0.9918032884597778)
[2025-02-13 20:44:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:46][root][INFO] - Training Epoch: 2/2, step 5814/7134 completed (loss: 0.09142729640007019, acc: 0.9679999947547913)
[2025-02-13 20:44:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:46][root][INFO] - Training Epoch: 2/2, step 5815/7134 completed (loss: 0.039028994739055634, acc: 1.0)
[2025-02-13 20:44:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:47][root][INFO] - Training Epoch: 2/2, step 5816/7134 completed (loss: 0.02450210228562355, acc: 0.9893048405647278)
[2025-02-13 20:44:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:47][root][INFO] - Training Epoch: 2/2, step 5817/7134 completed (loss: 0.13036689162254333, acc: 0.9904761910438538)
[2025-02-13 20:44:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:47][root][INFO] - Training Epoch: 2/2, step 5818/7134 completed (loss: 0.06031017005443573, acc: 0.9934640526771545)
[2025-02-13 20:44:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:48][root][INFO] - Training Epoch: 2/2, step 5819/7134 completed (loss: 0.044388964772224426, acc: 0.9929577708244324)
[2025-02-13 20:44:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:48][root][INFO] - Training Epoch: 2/2, step 5820/7134 completed (loss: 0.09358090907335281, acc: 0.9917355179786682)
[2025-02-13 20:44:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:48][root][INFO] - Training Epoch: 2/2, step 5821/7134 completed (loss: 0.01642702706158161, acc: 1.0)
[2025-02-13 20:44:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:49][root][INFO] - Training Epoch: 2/2, step 5822/7134 completed (loss: 0.04886670038104057, acc: 0.9798657894134521)
[2025-02-13 20:44:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:49][root][INFO] - Training Epoch: 2/2, step 5823/7134 completed (loss: 0.03412749990820885, acc: 1.0)
[2025-02-13 20:44:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:50][root][INFO] - Training Epoch: 2/2, step 5824/7134 completed (loss: 0.04512461647391319, acc: 0.9878787994384766)
[2025-02-13 20:44:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:50][root][INFO] - Training Epoch: 2/2, step 5825/7134 completed (loss: 0.12487442046403885, acc: 0.9694656729698181)
[2025-02-13 20:44:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:50][root][INFO] - Training Epoch: 2/2, step 5826/7134 completed (loss: 0.03363504633307457, acc: 0.9922480583190918)
[2025-02-13 20:44:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:51][root][INFO] - Training Epoch: 2/2, step 5827/7134 completed (loss: 0.03262357786297798, acc: 0.9919354915618896)
[2025-02-13 20:44:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:51][root][INFO] - Training Epoch: 2/2, step 5828/7134 completed (loss: 0.07026195526123047, acc: 0.9738562107086182)
[2025-02-13 20:44:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:51][root][INFO] - Training Epoch: 2/2, step 5829/7134 completed (loss: 0.02426350861787796, acc: 0.9939024448394775)
[2025-02-13 20:44:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:52][root][INFO] - Training Epoch: 2/2, step 5830/7134 completed (loss: 0.1204693540930748, acc: 0.9679999947547913)
[2025-02-13 20:44:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:52][root][INFO] - Training Epoch: 2/2, step 5831/7134 completed (loss: 0.052947476506233215, acc: 0.9897959232330322)
[2025-02-13 20:44:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:53][root][INFO] - Training Epoch: 2/2, step 5832/7134 completed (loss: 0.15807273983955383, acc: 0.9694656729698181)
[2025-02-13 20:44:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:53][root][INFO] - Training Epoch: 2/2, step 5833/7134 completed (loss: 0.032018501311540604, acc: 0.9924812316894531)
[2025-02-13 20:44:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:53][root][INFO] - Training Epoch: 2/2, step 5834/7134 completed (loss: 0.089346744120121, acc: 0.9824561476707458)
[2025-02-13 20:44:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:54][root][INFO] - Training Epoch: 2/2, step 5835/7134 completed (loss: 0.12089796364307404, acc: 0.9800000190734863)
[2025-02-13 20:44:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:54][root][INFO] - Training Epoch: 2/2, step 5836/7134 completed (loss: 0.13348868489265442, acc: 0.9681528806686401)
[2025-02-13 20:44:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:54][root][INFO] - Training Epoch: 2/2, step 5837/7134 completed (loss: 0.39793580770492554, acc: 0.926174521446228)
[2025-02-13 20:44:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:55][root][INFO] - Training Epoch: 2/2, step 5838/7134 completed (loss: 0.14321453869342804, acc: 0.9784172773361206)
[2025-02-13 20:44:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:55][root][INFO] - Training Epoch: 2/2, step 5839/7134 completed (loss: 0.13566698133945465, acc: 0.9710144996643066)
[2025-02-13 20:44:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:56][root][INFO] - Training Epoch: 2/2, step 5840/7134 completed (loss: 0.027846362441778183, acc: 0.9930070042610168)
[2025-02-13 20:44:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:56][root][INFO] - Training Epoch: 2/2, step 5841/7134 completed (loss: 0.15436406433582306, acc: 0.96875)
[2025-02-13 20:44:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:56][root][INFO] - Training Epoch: 2/2, step 5842/7134 completed (loss: 0.08166088908910751, acc: 0.9917355179786682)
[2025-02-13 20:44:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:57][root][INFO] - Training Epoch: 2/2, step 5843/7134 completed (loss: 0.11708793044090271, acc: 0.9821428656578064)
[2025-02-13 20:44:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:57][root][INFO] - Training Epoch: 2/2, step 5844/7134 completed (loss: 0.07265938073396683, acc: 0.9759036302566528)
[2025-02-13 20:44:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:57][root][INFO] - Training Epoch: 2/2, step 5845/7134 completed (loss: 0.084468312561512, acc: 0.9805194735527039)
[2025-02-13 20:44:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:58][root][INFO] - Training Epoch: 2/2, step 5846/7134 completed (loss: 0.12656332552433014, acc: 0.988095223903656)
[2025-02-13 20:44:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:58][root][INFO] - Training Epoch: 2/2, step 5847/7134 completed (loss: 0.0675370842218399, acc: 0.9819276928901672)
[2025-02-13 20:44:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:59][root][INFO] - Training Epoch: 2/2, step 5848/7134 completed (loss: 0.07859174907207489, acc: 0.9864864945411682)
[2025-02-13 20:44:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:59][root][INFO] - Training Epoch: 2/2, step 5849/7134 completed (loss: 0.09532803297042847, acc: 0.9740259647369385)
[2025-02-13 20:44:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:44:59][root][INFO] - Training Epoch: 2/2, step 5850/7134 completed (loss: 0.08501412719488144, acc: 0.98591548204422)
[2025-02-13 20:44:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:00][root][INFO] - Training Epoch: 2/2, step 5851/7134 completed (loss: 0.12133270502090454, acc: 0.9589040875434875)
[2025-02-13 20:45:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:00][root][INFO] - Training Epoch: 2/2, step 5852/7134 completed (loss: 0.074346624314785, acc: 0.9790209531784058)
[2025-02-13 20:45:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:00][root][INFO] - Training Epoch: 2/2, step 5853/7134 completed (loss: 0.13823066651821136, acc: 0.9719101190567017)
[2025-02-13 20:45:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:01][root][INFO] - Training Epoch: 2/2, step 5854/7134 completed (loss: 0.06439854204654694, acc: 0.9846153855323792)
[2025-02-13 20:45:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:01][root][INFO] - Training Epoch: 2/2, step 5855/7134 completed (loss: 0.06963984668254852, acc: 0.9830508232116699)
[2025-02-13 20:45:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:02][root][INFO] - Training Epoch: 2/2, step 5856/7134 completed (loss: 0.12256600707769394, acc: 0.98591548204422)
[2025-02-13 20:45:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:02][root][INFO] - Training Epoch: 2/2, step 5857/7134 completed (loss: 0.09339819103479385, acc: 0.9774436354637146)
[2025-02-13 20:45:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:02][root][INFO] - Training Epoch: 2/2, step 5858/7134 completed (loss: 0.06035066768527031, acc: 0.9908257126808167)
[2025-02-13 20:45:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:03][root][INFO] - Training Epoch: 2/2, step 5859/7134 completed (loss: 0.04278375953435898, acc: 1.0)
[2025-02-13 20:45:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:03][root][INFO] - Training Epoch: 2/2, step 5860/7134 completed (loss: 0.06641245633363724, acc: 0.9886363744735718)
[2025-02-13 20:45:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:03][root][INFO] - Training Epoch: 2/2, step 5861/7134 completed (loss: 0.026217781007289886, acc: 1.0)
[2025-02-13 20:45:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:04][root][INFO] - Training Epoch: 2/2, step 5862/7134 completed (loss: 0.08044523000717163, acc: 0.9830508232116699)
[2025-02-13 20:45:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:04][root][INFO] - Training Epoch: 2/2, step 5863/7134 completed (loss: 0.17665818333625793, acc: 0.9462365508079529)
[2025-02-13 20:45:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:04][root][INFO] - Training Epoch: 2/2, step 5864/7134 completed (loss: 0.20065495371818542, acc: 0.9532163739204407)
[2025-02-13 20:45:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:05][root][INFO] - Training Epoch: 2/2, step 5865/7134 completed (loss: 0.08323312550783157, acc: 0.9861111044883728)
[2025-02-13 20:45:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:05][root][INFO] - Training Epoch: 2/2, step 5866/7134 completed (loss: 0.03776761516928673, acc: 0.9945054650306702)
[2025-02-13 20:45:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:05][root][INFO] - Training Epoch: 2/2, step 5867/7134 completed (loss: 0.061868056654930115, acc: 0.9864864945411682)
[2025-02-13 20:45:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:06][root][INFO] - Training Epoch: 2/2, step 5868/7134 completed (loss: 0.09494302421808243, acc: 0.9738562107086182)
[2025-02-13 20:45:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:06][root][INFO] - Training Epoch: 2/2, step 5869/7134 completed (loss: 0.08373613655567169, acc: 0.9820359349250793)
[2025-02-13 20:45:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:07][root][INFO] - Training Epoch: 2/2, step 5870/7134 completed (loss: 0.017421243712306023, acc: 1.0)
[2025-02-13 20:45:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:07][root][INFO] - Training Epoch: 2/2, step 5871/7134 completed (loss: 0.027134567499160767, acc: 0.9816513657569885)
[2025-02-13 20:45:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:07][root][INFO] - Training Epoch: 2/2, step 5872/7134 completed (loss: 0.03779500350356102, acc: 0.9937888383865356)
[2025-02-13 20:45:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:08][root][INFO] - Training Epoch: 2/2, step 5873/7134 completed (loss: 0.13736554980278015, acc: 0.9625668525695801)
[2025-02-13 20:45:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:08][root][INFO] - Training Epoch: 2/2, step 5874/7134 completed (loss: 0.048728715628385544, acc: 0.9841269850730896)
[2025-02-13 20:45:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:09][root][INFO] - Training Epoch: 2/2, step 5875/7134 completed (loss: 0.070770263671875, acc: 0.9772727489471436)
[2025-02-13 20:45:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:09][root][INFO] - Training Epoch: 2/2, step 5876/7134 completed (loss: 0.034719519317150116, acc: 0.9887640476226807)
[2025-02-13 20:45:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:09][root][INFO] - Training Epoch: 2/2, step 5877/7134 completed (loss: 0.07515395432710648, acc: 0.9720670580863953)
[2025-02-13 20:45:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:10][root][INFO] - Training Epoch: 2/2, step 5878/7134 completed (loss: 0.08339537680149078, acc: 0.9727272987365723)
[2025-02-13 20:45:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:10][root][INFO] - Training Epoch: 2/2, step 5879/7134 completed (loss: 0.03473503515124321, acc: 0.9888888597488403)
[2025-02-13 20:45:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:10][root][INFO] - Training Epoch: 2/2, step 5880/7134 completed (loss: 0.052385058254003525, acc: 0.9821428656578064)
[2025-02-13 20:45:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:11][root][INFO] - Training Epoch: 2/2, step 5881/7134 completed (loss: 0.03981681168079376, acc: 0.9829059839248657)
[2025-02-13 20:45:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:11][root][INFO] - Training Epoch: 2/2, step 5882/7134 completed (loss: 0.042244017124176025, acc: 0.9901960492134094)
[2025-02-13 20:45:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:11][root][INFO] - Training Epoch: 2/2, step 5883/7134 completed (loss: 0.046350061893463135, acc: 0.9769230484962463)
[2025-02-13 20:45:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:12][root][INFO] - Training Epoch: 2/2, step 5884/7134 completed (loss: 0.07329722493886948, acc: 0.9841269850730896)
[2025-02-13 20:45:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:12][root][INFO] - Training Epoch: 2/2, step 5885/7134 completed (loss: 0.2590462863445282, acc: 0.9607843160629272)
[2025-02-13 20:45:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:12][root][INFO] - Training Epoch: 2/2, step 5886/7134 completed (loss: 0.027001652866601944, acc: 0.988095223903656)
[2025-02-13 20:45:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:13][root][INFO] - Training Epoch: 2/2, step 5887/7134 completed (loss: 0.04380057752132416, acc: 0.9833333492279053)
[2025-02-13 20:45:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:13][root][INFO] - Training Epoch: 2/2, step 5888/7134 completed (loss: 0.1656198799610138, acc: 0.9645389914512634)
[2025-02-13 20:45:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:13][root][INFO] - Training Epoch: 2/2, step 5889/7134 completed (loss: 0.05591706931591034, acc: 0.9807692170143127)
[2025-02-13 20:45:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:14][root][INFO] - Training Epoch: 2/2, step 5890/7134 completed (loss: 0.027981458231806755, acc: 1.0)
[2025-02-13 20:45:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:14][root][INFO] - Training Epoch: 2/2, step 5891/7134 completed (loss: 0.07241512089967728, acc: 0.9714285731315613)
[2025-02-13 20:45:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:15][root][INFO] - Training Epoch: 2/2, step 5892/7134 completed (loss: 0.09437765926122665, acc: 0.9825581312179565)
[2025-02-13 20:45:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:15][root][INFO] - Training Epoch: 2/2, step 5893/7134 completed (loss: 0.030120475217700005, acc: 0.9938650131225586)
[2025-02-13 20:45:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:15][root][INFO] - Training Epoch: 2/2, step 5894/7134 completed (loss: 0.017090201377868652, acc: 1.0)
[2025-02-13 20:45:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:16][root][INFO] - Training Epoch: 2/2, step 5895/7134 completed (loss: 0.04296098276972771, acc: 0.9895833134651184)
[2025-02-13 20:45:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:16][root][INFO] - Training Epoch: 2/2, step 5896/7134 completed (loss: 0.06111101061105728, acc: 0.9937106966972351)
[2025-02-13 20:45:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:16][root][INFO] - Training Epoch: 2/2, step 5897/7134 completed (loss: 0.0494692362844944, acc: 0.9886363744735718)
[2025-02-13 20:45:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:17][root][INFO] - Training Epoch: 2/2, step 5898/7134 completed (loss: 0.03772859647870064, acc: 0.9821428656578064)
[2025-02-13 20:45:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:17][root][INFO] - Training Epoch: 2/2, step 5899/7134 completed (loss: 0.020406194031238556, acc: 1.0)
[2025-02-13 20:45:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:18][root][INFO] - Training Epoch: 2/2, step 5900/7134 completed (loss: 0.028773298487067223, acc: 0.9876543283462524)
[2025-02-13 20:45:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:18][root][INFO] - Training Epoch: 2/2, step 5901/7134 completed (loss: 0.15813834965229034, acc: 0.969072163105011)
[2025-02-13 20:45:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:18][root][INFO] - Training Epoch: 2/2, step 5902/7134 completed (loss: 0.02069060504436493, acc: 1.0)
[2025-02-13 20:45:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:19][root][INFO] - Training Epoch: 2/2, step 5903/7134 completed (loss: 0.21120905876159668, acc: 0.9646017551422119)
[2025-02-13 20:45:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:19][root][INFO] - Training Epoch: 2/2, step 5904/7134 completed (loss: 0.09911159425973892, acc: 0.9718309640884399)
[2025-02-13 20:45:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:19][root][INFO] - Training Epoch: 2/2, step 5905/7134 completed (loss: 0.0996389165520668, acc: 0.9797297120094299)
[2025-02-13 20:45:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:20][root][INFO] - Training Epoch: 2/2, step 5906/7134 completed (loss: 0.15962035953998566, acc: 0.9607843160629272)
[2025-02-13 20:45:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:20][root][INFO] - Training Epoch: 2/2, step 5907/7134 completed (loss: 0.29114678502082825, acc: 0.9583333134651184)
[2025-02-13 20:45:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:21][root][INFO] - Training Epoch: 2/2, step 5908/7134 completed (loss: 0.1877703219652176, acc: 0.9805194735527039)
[2025-02-13 20:45:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:21][root][INFO] - Training Epoch: 2/2, step 5909/7134 completed (loss: 0.11693156510591507, acc: 0.9635036587715149)
[2025-02-13 20:45:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:22][root][INFO] - Training Epoch: 2/2, step 5910/7134 completed (loss: 0.08902118355035782, acc: 0.9933775067329407)
[2025-02-13 20:45:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:22][root][INFO] - Training Epoch: 2/2, step 5911/7134 completed (loss: 0.11670750379562378, acc: 0.9652174115180969)
[2025-02-13 20:45:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:22][root][INFO] - Training Epoch: 2/2, step 5912/7134 completed (loss: 0.048082828521728516, acc: 0.9921875)
[2025-02-13 20:45:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:23][root][INFO] - Training Epoch: 2/2, step 5913/7134 completed (loss: 0.07206002622842789, acc: 0.9940828680992126)
[2025-02-13 20:45:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:23][root][INFO] - Training Epoch: 2/2, step 5914/7134 completed (loss: 0.06691277772188187, acc: 0.9785714149475098)
[2025-02-13 20:45:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:23][root][INFO] - Training Epoch: 2/2, step 5915/7134 completed (loss: 0.08475705981254578, acc: 0.9923664331436157)
[2025-02-13 20:45:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:24][root][INFO] - Training Epoch: 2/2, step 5916/7134 completed (loss: 0.1221461147069931, acc: 0.9602649211883545)
[2025-02-13 20:45:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:24][root][INFO] - Training Epoch: 2/2, step 5917/7134 completed (loss: 0.03154784440994263, acc: 1.0)
[2025-02-13 20:45:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:25][root][INFO] - Training Epoch: 2/2, step 5918/7134 completed (loss: 0.1631285399198532, acc: 0.9580419659614563)
[2025-02-13 20:45:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:25][root][INFO] - Training Epoch: 2/2, step 5919/7134 completed (loss: 0.051810625940561295, acc: 0.9925373196601868)
[2025-02-13 20:45:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:25][root][INFO] - Training Epoch: 2/2, step 5920/7134 completed (loss: 0.04943973571062088, acc: 0.9850746393203735)
[2025-02-13 20:45:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:26][root][INFO] - Training Epoch: 2/2, step 5921/7134 completed (loss: 0.07861742377281189, acc: 0.982758641242981)
[2025-02-13 20:45:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:26][root][INFO] - Training Epoch: 2/2, step 5922/7134 completed (loss: 0.3671710789203644, acc: 0.9294871687889099)
[2025-02-13 20:45:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:27][root][INFO] - Training Epoch: 2/2, step 5923/7134 completed (loss: 0.119988813996315, acc: 0.9873417615890503)
[2025-02-13 20:45:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:27][root][INFO] - Training Epoch: 2/2, step 5924/7134 completed (loss: 0.05426798760890961, acc: 0.9903846383094788)
[2025-02-13 20:45:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:27][root][INFO] - Training Epoch: 2/2, step 5925/7134 completed (loss: 0.32499149441719055, acc: 0.9222221970558167)
[2025-02-13 20:45:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:28][root][INFO] - Training Epoch: 2/2, step 5926/7134 completed (loss: 0.24815192818641663, acc: 0.9144737124443054)
[2025-02-13 20:45:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:28][root][INFO] - Training Epoch: 2/2, step 5927/7134 completed (loss: 0.10727094113826752, acc: 0.9821428656578064)
[2025-02-13 20:45:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:28][root][INFO] - Training Epoch: 2/2, step 5928/7134 completed (loss: 0.377356618642807, acc: 0.9459459185600281)
[2025-02-13 20:45:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:29][root][INFO] - Training Epoch: 2/2, step 5929/7134 completed (loss: 0.18223974108695984, acc: 0.9435483813285828)
[2025-02-13 20:45:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:29][root][INFO] - Training Epoch: 2/2, step 5930/7134 completed (loss: 0.2861422896385193, acc: 0.942148745059967)
[2025-02-13 20:45:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:30][root][INFO] - Training Epoch: 2/2, step 5931/7134 completed (loss: 0.1320858597755432, acc: 0.9572649598121643)
[2025-02-13 20:45:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:30][root][INFO] - Training Epoch: 2/2, step 5932/7134 completed (loss: 0.28373193740844727, acc: 0.9064748287200928)
[2025-02-13 20:45:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:30][root][INFO] - Training Epoch: 2/2, step 5933/7134 completed (loss: 0.26149168610572815, acc: 0.9558823704719543)
[2025-02-13 20:45:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:31][root][INFO] - Training Epoch: 2/2, step 5934/7134 completed (loss: 0.18911521136760712, acc: 0.9357143044471741)
[2025-02-13 20:45:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:31][root][INFO] - Training Epoch: 2/2, step 5935/7134 completed (loss: 0.0443120114505291, acc: 0.9918032884597778)
[2025-02-13 20:45:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:31][root][INFO] - Training Epoch: 2/2, step 5936/7134 completed (loss: 0.2782200872898102, acc: 0.9386503100395203)
[2025-02-13 20:45:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:32][root][INFO] - Training Epoch: 2/2, step 5937/7134 completed (loss: 0.1409199982881546, acc: 0.9647887349128723)
[2025-02-13 20:45:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:32][root][INFO] - Training Epoch: 2/2, step 5938/7134 completed (loss: 0.055703770369291306, acc: 0.9924812316894531)
[2025-02-13 20:45:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:32][root][INFO] - Training Epoch: 2/2, step 5939/7134 completed (loss: 0.3565501570701599, acc: 0.931034505367279)
[2025-02-13 20:45:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:33][root][INFO] - Training Epoch: 2/2, step 5940/7134 completed (loss: 0.14545166492462158, acc: 0.9627329111099243)
[2025-02-13 20:45:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:33][root][INFO] - Training Epoch: 2/2, step 5941/7134 completed (loss: 0.08122209459543228, acc: 0.988095223903656)
[2025-02-13 20:45:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:34][root][INFO] - Training Epoch: 2/2, step 5942/7134 completed (loss: 0.07046526670455933, acc: 0.984000027179718)
[2025-02-13 20:45:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:34][root][INFO] - Training Epoch: 2/2, step 5943/7134 completed (loss: 0.0541003979742527, acc: 0.9815950989723206)
[2025-02-13 20:45:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:34][root][INFO] - Training Epoch: 2/2, step 5944/7134 completed (loss: 0.1410195231437683, acc: 0.9744898080825806)
[2025-02-13 20:45:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:35][root][INFO] - Training Epoch: 2/2, step 5945/7134 completed (loss: 0.024449670687317848, acc: 0.9940828680992126)
[2025-02-13 20:45:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:35][root][INFO] - Training Epoch: 2/2, step 5946/7134 completed (loss: 0.09161624312400818, acc: 0.9652777910232544)
[2025-02-13 20:45:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:36][root][INFO] - Training Epoch: 2/2, step 5947/7134 completed (loss: 0.17567075788974762, acc: 0.9567307829856873)
[2025-02-13 20:45:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:36][root][INFO] - Training Epoch: 2/2, step 5948/7134 completed (loss: 0.1728142499923706, acc: 0.9512194991111755)
[2025-02-13 20:45:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:36][root][INFO] - Training Epoch: 2/2, step 5949/7134 completed (loss: 0.04263051226735115, acc: 0.9888888597488403)
[2025-02-13 20:45:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:37][root][INFO] - Training Epoch: 2/2, step 5950/7134 completed (loss: 0.11016152799129486, acc: 0.9702970385551453)
[2025-02-13 20:45:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:37][root][INFO] - Training Epoch: 2/2, step 5951/7134 completed (loss: 0.17537692189216614, acc: 0.9523809552192688)
[2025-02-13 20:45:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:37][root][INFO] - Training Epoch: 2/2, step 5952/7134 completed (loss: 0.41516613960266113, acc: 0.9342105388641357)
[2025-02-13 20:45:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:38][root][INFO] - Training Epoch: 2/2, step 5953/7134 completed (loss: 0.17077206075191498, acc: 0.9622641801834106)
[2025-02-13 20:45:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:38][root][INFO] - Training Epoch: 2/2, step 5954/7134 completed (loss: 0.03360394760966301, acc: 0.9900990128517151)
[2025-02-13 20:45:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:39][root][INFO] - Training Epoch: 2/2, step 5955/7134 completed (loss: 0.14201267063617706, acc: 0.9814814925193787)
[2025-02-13 20:45:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:39][root][INFO] - Training Epoch: 2/2, step 5956/7134 completed (loss: 0.15933701395988464, acc: 0.9750000238418579)
[2025-02-13 20:45:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:39][root][INFO] - Training Epoch: 2/2, step 5957/7134 completed (loss: 0.16113008558750153, acc: 0.9756097793579102)
[2025-02-13 20:45:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:40][root][INFO] - Training Epoch: 2/2, step 5958/7134 completed (loss: 0.1274590790271759, acc: 0.9657142758369446)
[2025-02-13 20:45:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:40][root][INFO] - Training Epoch: 2/2, step 5959/7134 completed (loss: 0.16659711301326752, acc: 0.957446813583374)
[2025-02-13 20:45:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:40][root][INFO] - Training Epoch: 2/2, step 5960/7134 completed (loss: 0.012163590639829636, acc: 1.0)
[2025-02-13 20:45:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:41][root][INFO] - Training Epoch: 2/2, step 5961/7134 completed (loss: 0.14655090868473053, acc: 0.9722222089767456)
[2025-02-13 20:45:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:41][root][INFO] - Training Epoch: 2/2, step 5962/7134 completed (loss: 0.04694569855928421, acc: 0.9862068891525269)
[2025-02-13 20:45:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:41][root][INFO] - Training Epoch: 2/2, step 5963/7134 completed (loss: 0.04195944592356682, acc: 0.9942857027053833)
[2025-02-13 20:45:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:42][root][INFO] - Training Epoch: 2/2, step 5964/7134 completed (loss: 0.028924310579895973, acc: 0.9921259880065918)
[2025-02-13 20:45:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:42][root][INFO] - Training Epoch: 2/2, step 5965/7134 completed (loss: 0.07228849828243256, acc: 0.9807692170143127)
[2025-02-13 20:45:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:43][root][INFO] - Training Epoch: 2/2, step 5966/7134 completed (loss: 0.07016085088253021, acc: 0.97826087474823)
[2025-02-13 20:45:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:43][root][INFO] - Training Epoch: 2/2, step 5967/7134 completed (loss: 0.10994406789541245, acc: 0.9791666865348816)
[2025-02-13 20:45:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:43][root][INFO] - Training Epoch: 2/2, step 5968/7134 completed (loss: 0.06345460563898087, acc: 0.9800000190734863)
[2025-02-13 20:45:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:44][root][INFO] - Training Epoch: 2/2, step 5969/7134 completed (loss: 0.14027787744998932, acc: 0.9677419066429138)
[2025-02-13 20:45:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:44][root][INFO] - Training Epoch: 2/2, step 5970/7134 completed (loss: 0.10134891420602798, acc: 0.9921875)
[2025-02-13 20:45:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:44][root][INFO] - Training Epoch: 2/2, step 5971/7134 completed (loss: 0.03599518910050392, acc: 0.9894737005233765)
[2025-02-13 20:45:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:45][root][INFO] - Training Epoch: 2/2, step 5972/7134 completed (loss: 0.1581386774778366, acc: 0.9754601120948792)
[2025-02-13 20:45:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:45][root][INFO] - Training Epoch: 2/2, step 5973/7134 completed (loss: 0.15113839507102966, acc: 0.9657142758369446)
[2025-02-13 20:45:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:45][root][INFO] - Training Epoch: 2/2, step 5974/7134 completed (loss: 0.032050199806690216, acc: 0.9926470518112183)
[2025-02-13 20:45:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:46][root][INFO] - Training Epoch: 2/2, step 5975/7134 completed (loss: 0.0514381006360054, acc: 0.9878048896789551)
[2025-02-13 20:45:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:46][root][INFO] - Training Epoch: 2/2, step 5976/7134 completed (loss: 0.03823540359735489, acc: 0.9869281053543091)
[2025-02-13 20:45:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:47][root][INFO] - Training Epoch: 2/2, step 5977/7134 completed (loss: 0.10561908781528473, acc: 0.9700000286102295)
[2025-02-13 20:45:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:47][root][INFO] - Training Epoch: 2/2, step 5978/7134 completed (loss: 0.10346818715333939, acc: 0.9813664555549622)
[2025-02-13 20:45:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:48][root][INFO] - Training Epoch: 2/2, step 5979/7134 completed (loss: 0.11905892938375473, acc: 0.9894737005233765)
[2025-02-13 20:45:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:48][root][INFO] - Training Epoch: 2/2, step 5980/7134 completed (loss: 0.06422680616378784, acc: 0.989130437374115)
[2025-02-13 20:45:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:48][root][INFO] - Training Epoch: 2/2, step 5981/7134 completed (loss: 0.09741512686014175, acc: 0.9847715497016907)
[2025-02-13 20:45:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:49][root][INFO] - Training Epoch: 2/2, step 5982/7134 completed (loss: 0.05805209279060364, acc: 0.9838709831237793)
[2025-02-13 20:45:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:49][root][INFO] - Training Epoch: 2/2, step 5983/7134 completed (loss: 0.04680503159761429, acc: 0.9885057210922241)
[2025-02-13 20:45:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:50][root][INFO] - Training Epoch: 2/2, step 5984/7134 completed (loss: 0.08985153585672379, acc: 0.9757575988769531)
[2025-02-13 20:45:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:50][root][INFO] - Training Epoch: 2/2, step 5985/7134 completed (loss: 0.09425541758537292, acc: 0.9719101190567017)
[2025-02-13 20:45:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:50][root][INFO] - Training Epoch: 2/2, step 5986/7134 completed (loss: 0.13663406670093536, acc: 0.9682539701461792)
[2025-02-13 20:45:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:51][root][INFO] - Training Epoch: 2/2, step 5987/7134 completed (loss: 0.09990736097097397, acc: 0.9756097793579102)
[2025-02-13 20:45:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:51][root][INFO] - Training Epoch: 2/2, step 5988/7134 completed (loss: 0.042908504605293274, acc: 0.9888888597488403)
[2025-02-13 20:45:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:52][root][INFO] - Training Epoch: 2/2, step 5989/7134 completed (loss: 0.058253996074199677, acc: 0.9923076629638672)
[2025-02-13 20:45:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:52][root][INFO] - Training Epoch: 2/2, step 5990/7134 completed (loss: 0.10275181382894516, acc: 0.9779005646705627)
[2025-02-13 20:45:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:52][root][INFO] - Training Epoch: 2/2, step 5991/7134 completed (loss: 0.18643949925899506, acc: 0.9772727489471436)
[2025-02-13 20:45:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:53][root][INFO] - Training Epoch: 2/2, step 5992/7134 completed (loss: 0.06716696172952652, acc: 0.9846938848495483)
[2025-02-13 20:45:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:53][root][INFO] - Training Epoch: 2/2, step 5993/7134 completed (loss: 0.047572046518325806, acc: 0.9887640476226807)
[2025-02-13 20:45:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:54][root][INFO] - Training Epoch: 2/2, step 5994/7134 completed (loss: 0.030725928023457527, acc: 0.9917355179786682)
[2025-02-13 20:45:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:54][root][INFO] - Training Epoch: 2/2, step 5995/7134 completed (loss: 0.07705257087945938, acc: 0.9710982441902161)
[2025-02-13 20:45:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:54][root][INFO] - Training Epoch: 2/2, step 5996/7134 completed (loss: 0.054715730249881744, acc: 0.9866666793823242)
[2025-02-13 20:45:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:55][root][INFO] - Training Epoch: 2/2, step 5997/7134 completed (loss: 0.09781821072101593, acc: 0.9772727489471436)
[2025-02-13 20:45:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:55][root][INFO] - Training Epoch: 2/2, step 5998/7134 completed (loss: 0.08667493611574173, acc: 0.9841269850730896)
[2025-02-13 20:45:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:55][root][INFO] - Training Epoch: 2/2, step 5999/7134 completed (loss: 0.040292978286743164, acc: 0.990338146686554)
[2025-02-13 20:45:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:56][root][INFO] - Training Epoch: 2/2, step 6000/7134 completed (loss: 0.06157553195953369, acc: 0.98591548204422)
[2025-02-13 20:45:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:56][root][INFO] - Training Epoch: 2/2, step 6001/7134 completed (loss: 0.03582913428544998, acc: 1.0)
[2025-02-13 20:45:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:56][root][INFO] - Training Epoch: 2/2, step 6002/7134 completed (loss: 0.11972996592521667, acc: 0.9669421315193176)
[2025-02-13 20:45:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:57][root][INFO] - Training Epoch: 2/2, step 6003/7134 completed (loss: 0.08184432983398438, acc: 0.9740259647369385)
[2025-02-13 20:45:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:57][root][INFO] - Training Epoch: 2/2, step 6004/7134 completed (loss: 0.211089625954628, acc: 0.9479768872261047)
[2025-02-13 20:45:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:58][root][INFO] - Training Epoch: 2/2, step 6005/7134 completed (loss: 0.08961419016122818, acc: 0.9801324605941772)
[2025-02-13 20:45:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:58][root][INFO] - Training Epoch: 2/2, step 6006/7134 completed (loss: 0.12077197432518005, acc: 0.9679999947547913)
[2025-02-13 20:45:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:58][root][INFO] - Training Epoch: 2/2, step 6007/7134 completed (loss: 0.08635944128036499, acc: 0.970588207244873)
[2025-02-13 20:45:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:59][root][INFO] - Training Epoch: 2/2, step 6008/7134 completed (loss: 0.10061025619506836, acc: 0.9720279574394226)
[2025-02-13 20:45:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:45:59][root][INFO] - Training Epoch: 2/2, step 6009/7134 completed (loss: 0.10362236201763153, acc: 0.9849624037742615)
[2025-02-13 20:45:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:00][root][INFO] - Training Epoch: 2/2, step 6010/7134 completed (loss: 0.1710634082555771, acc: 0.9620253443717957)
[2025-02-13 20:46:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:00][root][INFO] - Training Epoch: 2/2, step 6011/7134 completed (loss: 0.19896775484085083, acc: 0.9642857313156128)
[2025-02-13 20:46:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:00][root][INFO] - Training Epoch: 2/2, step 6012/7134 completed (loss: 0.15073974430561066, acc: 0.9515151381492615)
[2025-02-13 20:46:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:01][root][INFO] - Training Epoch: 2/2, step 6013/7134 completed (loss: 0.18858665227890015, acc: 0.9438202381134033)
[2025-02-13 20:46:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:01][root][INFO] - Training Epoch: 2/2, step 6014/7134 completed (loss: 0.14008034765720367, acc: 0.960629940032959)
[2025-02-13 20:46:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:02][root][INFO] - Training Epoch: 2/2, step 6015/7134 completed (loss: 0.1386883556842804, acc: 0.954954981803894)
[2025-02-13 20:46:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:02][root][INFO] - Training Epoch: 2/2, step 6016/7134 completed (loss: 0.06255688518285751, acc: 0.9729729890823364)
[2025-02-13 20:46:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:02][root][INFO] - Training Epoch: 2/2, step 6017/7134 completed (loss: 0.04492553323507309, acc: 1.0)
[2025-02-13 20:46:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:02][root][INFO] - Training Epoch: 2/2, step 6018/7134 completed (loss: 0.05318938195705414, acc: 1.0)
[2025-02-13 20:46:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:03][root][INFO] - Training Epoch: 2/2, step 6019/7134 completed (loss: 0.03649863973259926, acc: 1.0)
[2025-02-13 20:46:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:03][root][INFO] - Training Epoch: 2/2, step 6020/7134 completed (loss: 0.09912300109863281, acc: 0.9682539701461792)
[2025-02-13 20:46:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:03][root][INFO] - Training Epoch: 2/2, step 6021/7134 completed (loss: 0.0192245040088892, acc: 1.0)
[2025-02-13 20:46:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:04][root][INFO] - Training Epoch: 2/2, step 6022/7134 completed (loss: 0.06528038531541824, acc: 0.9605262875556946)
[2025-02-13 20:46:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:04][root][INFO] - Training Epoch: 2/2, step 6023/7134 completed (loss: 0.1858777403831482, acc: 0.9772727489471436)
[2025-02-13 20:46:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:04][root][INFO] - Training Epoch: 2/2, step 6024/7134 completed (loss: 0.011526866815984249, acc: 1.0)
[2025-02-13 20:46:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:05][root][INFO] - Training Epoch: 2/2, step 6025/7134 completed (loss: 0.08271641284227371, acc: 0.9672130942344666)
[2025-02-13 20:46:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:05][root][INFO] - Training Epoch: 2/2, step 6026/7134 completed (loss: 0.6080889105796814, acc: 0.8983050584793091)
[2025-02-13 20:46:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:05][root][INFO] - Training Epoch: 2/2, step 6027/7134 completed (loss: 0.07914233207702637, acc: 0.9775280952453613)
[2025-02-13 20:46:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:06][root][INFO] - Training Epoch: 2/2, step 6028/7134 completed (loss: 0.019407669082283974, acc: 1.0)
[2025-02-13 20:46:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:06][root][INFO] - Training Epoch: 2/2, step 6029/7134 completed (loss: 0.07793937623500824, acc: 0.98591548204422)
[2025-02-13 20:46:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:07][root][INFO] - Training Epoch: 2/2, step 6030/7134 completed (loss: 0.038299474865198135, acc: 0.9848484992980957)
[2025-02-13 20:46:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:07][root][INFO] - Training Epoch: 2/2, step 6031/7134 completed (loss: 0.036683354526758194, acc: 1.0)
[2025-02-13 20:46:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:07][root][INFO] - Training Epoch: 2/2, step 6032/7134 completed (loss: 0.06202688440680504, acc: 0.9900990128517151)
[2025-02-13 20:46:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:08][root][INFO] - Training Epoch: 2/2, step 6033/7134 completed (loss: 0.08774933218955994, acc: 0.9696969985961914)
[2025-02-13 20:46:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:08][root][INFO] - Training Epoch: 2/2, step 6034/7134 completed (loss: 0.19619891047477722, acc: 0.9552238583564758)
[2025-02-13 20:46:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:08][root][INFO] - Training Epoch: 2/2, step 6035/7134 completed (loss: 0.28510981798171997, acc: 0.9389671087265015)
[2025-02-13 20:46:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:09][root][INFO] - Training Epoch: 2/2, step 6036/7134 completed (loss: 0.3000239431858063, acc: 0.9382022619247437)
[2025-02-13 20:46:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:09][root][INFO] - Training Epoch: 2/2, step 6037/7134 completed (loss: 0.14174233376979828, acc: 0.957446813583374)
[2025-02-13 20:46:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:09][root][INFO] - Training Epoch: 2/2, step 6038/7134 completed (loss: 0.11362672597169876, acc: 0.9558823704719543)
[2025-02-13 20:46:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:10][root][INFO] - Training Epoch: 2/2, step 6039/7134 completed (loss: 0.2009718120098114, acc: 0.9414634108543396)
[2025-02-13 20:46:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:10][root][INFO] - Training Epoch: 2/2, step 6040/7134 completed (loss: 0.09940608590841293, acc: 0.9735682606697083)
[2025-02-13 20:46:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:11][root][INFO] - Training Epoch: 2/2, step 6041/7134 completed (loss: 0.28974682092666626, acc: 0.9247311949729919)
[2025-02-13 20:46:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:11][root][INFO] - Training Epoch: 2/2, step 6042/7134 completed (loss: 0.08936968445777893, acc: 0.963302731513977)
[2025-02-13 20:46:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:11][root][INFO] - Training Epoch: 2/2, step 6043/7134 completed (loss: 0.12356185168027878, acc: 0.9731183052062988)
[2025-02-13 20:46:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:12][root][INFO] - Training Epoch: 2/2, step 6044/7134 completed (loss: 0.10256526619195938, acc: 0.961904764175415)
[2025-02-13 20:46:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:12][root][INFO] - Training Epoch: 2/2, step 6045/7134 completed (loss: 0.18520714342594147, acc: 0.9569377899169922)
[2025-02-13 20:46:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:12][root][INFO] - Training Epoch: 2/2, step 6046/7134 completed (loss: 0.28114819526672363, acc: 0.965753436088562)
[2025-02-13 20:46:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:13][root][INFO] - Training Epoch: 2/2, step 6047/7134 completed (loss: 0.2177201360464096, acc: 0.9523809552192688)
[2025-02-13 20:46:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:13][root][INFO] - Training Epoch: 2/2, step 6048/7134 completed (loss: 0.12296576797962189, acc: 0.9512194991111755)
[2025-02-13 20:46:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:14][root][INFO] - Training Epoch: 2/2, step 6049/7134 completed (loss: 0.07909964770078659, acc: 0.9650349617004395)
[2025-02-13 20:46:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:14][root][INFO] - Training Epoch: 2/2, step 6050/7134 completed (loss: 0.24001272022724152, acc: 0.9306358098983765)
[2025-02-13 20:46:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:14][root][INFO] - Training Epoch: 2/2, step 6051/7134 completed (loss: 0.2943098247051239, acc: 0.9034090638160706)
[2025-02-13 20:46:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:15][root][INFO] - Training Epoch: 2/2, step 6052/7134 completed (loss: 0.19327403604984283, acc: 0.9599999785423279)
[2025-02-13 20:46:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:15][root][INFO] - Training Epoch: 2/2, step 6053/7134 completed (loss: 0.20346154272556305, acc: 0.9469026327133179)
[2025-02-13 20:46:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:15][root][INFO] - Training Epoch: 2/2, step 6054/7134 completed (loss: 0.1375165581703186, acc: 0.9637305736541748)
[2025-02-13 20:46:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:16][root][INFO] - Training Epoch: 2/2, step 6055/7134 completed (loss: 0.09273747354745865, acc: 0.9675324559211731)
[2025-02-13 20:46:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:16][root][INFO] - Training Epoch: 2/2, step 6056/7134 completed (loss: 0.19644127786159515, acc: 0.9819819927215576)
[2025-02-13 20:46:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:17][root][INFO] - Training Epoch: 2/2, step 6057/7134 completed (loss: 0.02653900906443596, acc: 0.9870129823684692)
[2025-02-13 20:46:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:17][root][INFO] - Training Epoch: 2/2, step 6058/7134 completed (loss: 0.017517924308776855, acc: 1.0)
[2025-02-13 20:46:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:17][root][INFO] - Training Epoch: 2/2, step 6059/7134 completed (loss: 0.3272208571434021, acc: 0.9589040875434875)
[2025-02-13 20:46:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:18][root][INFO] - Training Epoch: 2/2, step 6060/7134 completed (loss: 0.11312796920537949, acc: 0.9567901492118835)
[2025-02-13 20:46:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:18][root][INFO] - Training Epoch: 2/2, step 6061/7134 completed (loss: 0.1427219659090042, acc: 0.9647887349128723)
[2025-02-13 20:46:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:18][root][INFO] - Training Epoch: 2/2, step 6062/7134 completed (loss: 0.18055616319179535, acc: 0.9312169551849365)
[2025-02-13 20:46:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:19][root][INFO] - Training Epoch: 2/2, step 6063/7134 completed (loss: 0.08367857336997986, acc: 0.991525411605835)
[2025-02-13 20:46:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:19][root][INFO] - Training Epoch: 2/2, step 6064/7134 completed (loss: 0.19607090950012207, acc: 0.9440993666648865)
[2025-02-13 20:46:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:20][root][INFO] - Training Epoch: 2/2, step 6065/7134 completed (loss: 0.12465004622936249, acc: 0.9644669890403748)
[2025-02-13 20:46:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:20][root][INFO] - Training Epoch: 2/2, step 6066/7134 completed (loss: 0.16253887116909027, acc: 0.9481481313705444)
[2025-02-13 20:46:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:20][root][INFO] - Training Epoch: 2/2, step 6067/7134 completed (loss: 0.1666908711194992, acc: 0.9407894611358643)
[2025-02-13 20:46:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:21][root][INFO] - Training Epoch: 2/2, step 6068/7134 completed (loss: 0.4019966721534729, acc: 0.8702702522277832)
[2025-02-13 20:46:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:21][root][INFO] - Training Epoch: 2/2, step 6069/7134 completed (loss: 0.233615443110466, acc: 0.929411768913269)
[2025-02-13 20:46:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:22][root][INFO] - Training Epoch: 2/2, step 6070/7134 completed (loss: 0.17302453517913818, acc: 0.9485714435577393)
[2025-02-13 20:46:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:22][root][INFO] - Training Epoch: 2/2, step 6071/7134 completed (loss: 0.1943311095237732, acc: 0.9467455744743347)
[2025-02-13 20:46:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:23][root][INFO] - Training Epoch: 2/2, step 6072/7134 completed (loss: 0.24478088319301605, acc: 0.9166666865348816)
[2025-02-13 20:46:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:23][root][INFO] - Training Epoch: 2/2, step 6073/7134 completed (loss: 0.177388533949852, acc: 0.9554139971733093)
[2025-02-13 20:46:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:23][root][INFO] - Training Epoch: 2/2, step 6074/7134 completed (loss: 0.27876219153404236, acc: 0.9419354796409607)
[2025-02-13 20:46:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:24][root][INFO] - Training Epoch: 2/2, step 6075/7134 completed (loss: 0.1616837978363037, acc: 0.9789473414421082)
[2025-02-13 20:46:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:24][root][INFO] - Training Epoch: 2/2, step 6076/7134 completed (loss: 0.18070916831493378, acc: 0.9197860956192017)
[2025-02-13 20:46:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:24][root][INFO] - Training Epoch: 2/2, step 6077/7134 completed (loss: 0.051066987216472626, acc: 0.9940828680992126)
[2025-02-13 20:46:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:25][root][INFO] - Training Epoch: 2/2, step 6078/7134 completed (loss: 0.17896603047847748, acc: 0.9548022747039795)
[2025-02-13 20:46:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:25][root][INFO] - Training Epoch: 2/2, step 6079/7134 completed (loss: 0.11120755225419998, acc: 0.9696969985961914)
[2025-02-13 20:46:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:26][root][INFO] - Training Epoch: 2/2, step 6080/7134 completed (loss: 0.12450098991394043, acc: 0.977011501789093)
[2025-02-13 20:46:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:26][root][INFO] - Training Epoch: 2/2, step 6081/7134 completed (loss: 0.15844844281673431, acc: 0.9594594836235046)
[2025-02-13 20:46:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:26][root][INFO] - Training Epoch: 2/2, step 6082/7134 completed (loss: 0.0820627212524414, acc: 0.9731543660163879)
[2025-02-13 20:46:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:27][root][INFO] - Training Epoch: 2/2, step 6083/7134 completed (loss: 0.05193394050002098, acc: 0.9896373152732849)
[2025-02-13 20:46:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:27][root][INFO] - Training Epoch: 2/2, step 6084/7134 completed (loss: 0.13992486894130707, acc: 0.9800000190734863)
[2025-02-13 20:46:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:27][root][INFO] - Training Epoch: 2/2, step 6085/7134 completed (loss: 0.09859946370124817, acc: 0.9722222089767456)
[2025-02-13 20:46:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:28][root][INFO] - Training Epoch: 2/2, step 6086/7134 completed (loss: 0.07155539095401764, acc: 0.9836065769195557)
[2025-02-13 20:46:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:28][root][INFO] - Training Epoch: 2/2, step 6087/7134 completed (loss: 0.14532943069934845, acc: 0.9607843160629272)
[2025-02-13 20:46:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:29][root][INFO] - Training Epoch: 2/2, step 6088/7134 completed (loss: 0.16577504575252533, acc: 0.9513513445854187)
[2025-02-13 20:46:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:29][root][INFO] - Training Epoch: 2/2, step 6089/7134 completed (loss: 0.0556269995868206, acc: 0.9828571677207947)
[2025-02-13 20:46:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:29][root][INFO] - Training Epoch: 2/2, step 6090/7134 completed (loss: 0.07861596345901489, acc: 0.9839572310447693)
[2025-02-13 20:46:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:30][root][INFO] - Training Epoch: 2/2, step 6091/7134 completed (loss: 0.032961901277303696, acc: 0.993630588054657)
[2025-02-13 20:46:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:30][root][INFO] - Training Epoch: 2/2, step 6092/7134 completed (loss: 0.048397619277238846, acc: 0.9878787994384766)
[2025-02-13 20:46:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:30][root][INFO] - Training Epoch: 2/2, step 6093/7134 completed (loss: 0.13007767498493195, acc: 0.9552238583564758)
[2025-02-13 20:46:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:31][root][INFO] - Training Epoch: 2/2, step 6094/7134 completed (loss: 0.23159073293209076, acc: 0.9318181872367859)
[2025-02-13 20:46:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:31][root][INFO] - Training Epoch: 2/2, step 6095/7134 completed (loss: 0.07149768620729446, acc: 0.9815950989723206)
[2025-02-13 20:46:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:32][root][INFO] - Training Epoch: 2/2, step 6096/7134 completed (loss: 0.15942831337451935, acc: 0.9746192693710327)
[2025-02-13 20:46:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:32][root][INFO] - Training Epoch: 2/2, step 6097/7134 completed (loss: 0.10921242833137512, acc: 0.9627329111099243)
[2025-02-13 20:46:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:32][root][INFO] - Training Epoch: 2/2, step 6098/7134 completed (loss: 0.258530855178833, acc: 0.9415204524993896)
[2025-02-13 20:46:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:33][root][INFO] - Training Epoch: 2/2, step 6099/7134 completed (loss: 0.1574489176273346, acc: 0.9387755393981934)
[2025-02-13 20:46:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:33][root][INFO] - Training Epoch: 2/2, step 6100/7134 completed (loss: 0.20126710832118988, acc: 0.9743589758872986)
[2025-02-13 20:46:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:34][root][INFO] - Training Epoch: 2/2, step 6101/7134 completed (loss: 0.1492862105369568, acc: 0.9583333134651184)
[2025-02-13 20:46:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:34][root][INFO] - Training Epoch: 2/2, step 6102/7134 completed (loss: 0.17759189009666443, acc: 0.9375)
[2025-02-13 20:46:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:34][root][INFO] - Training Epoch: 2/2, step 6103/7134 completed (loss: 0.051251914352178574, acc: 0.9909090995788574)
[2025-02-13 20:46:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:35][root][INFO] - Training Epoch: 2/2, step 6104/7134 completed (loss: 0.20198465883731842, acc: 0.9662162065505981)
[2025-02-13 20:46:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:35][root][INFO] - Training Epoch: 2/2, step 6105/7134 completed (loss: 0.33073610067367554, acc: 0.939393937587738)
[2025-02-13 20:46:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:35][root][INFO] - Training Epoch: 2/2, step 6106/7134 completed (loss: 0.14024662971496582, acc: 0.9842519760131836)
[2025-02-13 20:46:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:36][root][INFO] - Training Epoch: 2/2, step 6107/7134 completed (loss: 0.053393829613924026, acc: 0.9829059839248657)
[2025-02-13 20:46:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:36][root][INFO] - Training Epoch: 2/2, step 6108/7134 completed (loss: 0.13009247183799744, acc: 0.9669421315193176)
[2025-02-13 20:46:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:36][root][INFO] - Training Epoch: 2/2, step 6109/7134 completed (loss: 0.06525520980358124, acc: 0.9826086759567261)
[2025-02-13 20:46:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:37][root][INFO] - Training Epoch: 2/2, step 6110/7134 completed (loss: 0.2140156328678131, acc: 0.953125)
[2025-02-13 20:46:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:37][root][INFO] - Training Epoch: 2/2, step 6111/7134 completed (loss: 0.07446799427270889, acc: 0.989130437374115)
[2025-02-13 20:46:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:38][root][INFO] - Training Epoch: 2/2, step 6112/7134 completed (loss: 0.13757479190826416, acc: 0.9738562107086182)
[2025-02-13 20:46:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:38][root][INFO] - Training Epoch: 2/2, step 6113/7134 completed (loss: 0.10833729803562164, acc: 0.9695122241973877)
[2025-02-13 20:46:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:38][root][INFO] - Training Epoch: 2/2, step 6114/7134 completed (loss: 0.1257968246936798, acc: 0.9860140085220337)
[2025-02-13 20:46:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:39][root][INFO] - Training Epoch: 2/2, step 6115/7134 completed (loss: 0.06053352728486061, acc: 0.9919999837875366)
[2025-02-13 20:46:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:39][root][INFO] - Training Epoch: 2/2, step 6116/7134 completed (loss: 0.22543178498744965, acc: 0.9596773982048035)
[2025-02-13 20:46:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:39][root][INFO] - Training Epoch: 2/2, step 6117/7134 completed (loss: 0.14734584093093872, acc: 0.9580838084220886)
[2025-02-13 20:46:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:40][root][INFO] - Training Epoch: 2/2, step 6118/7134 completed (loss: 0.04292857274413109, acc: 0.9841269850730896)
[2025-02-13 20:46:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:40][root][INFO] - Training Epoch: 2/2, step 6119/7134 completed (loss: 0.0741698145866394, acc: 0.9837837815284729)
[2025-02-13 20:46:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:41][root][INFO] - Training Epoch: 2/2, step 6120/7134 completed (loss: 0.049972787499427795, acc: 1.0)
[2025-02-13 20:46:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:41][root][INFO] - Training Epoch: 2/2, step 6121/7134 completed (loss: 0.24449844658374786, acc: 0.9527559280395508)
[2025-02-13 20:46:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:41][root][INFO] - Training Epoch: 2/2, step 6122/7134 completed (loss: 0.04490872099995613, acc: 0.9919354915618896)
[2025-02-13 20:46:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:42][root][INFO] - Training Epoch: 2/2, step 6123/7134 completed (loss: 0.09218268096446991, acc: 0.9743589758872986)
[2025-02-13 20:46:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:42][root][INFO] - Training Epoch: 2/2, step 6124/7134 completed (loss: 0.052633922547101974, acc: 0.9915966391563416)
[2025-02-13 20:46:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:42][root][INFO] - Training Epoch: 2/2, step 6125/7134 completed (loss: 0.2286665439605713, acc: 0.9532710313796997)
[2025-02-13 20:46:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:43][root][INFO] - Training Epoch: 2/2, step 6126/7134 completed (loss: 0.1472461074590683, acc: 0.9602649211883545)
[2025-02-13 20:46:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:43][root][INFO] - Training Epoch: 2/2, step 6127/7134 completed (loss: 0.011267643421888351, acc: 1.0)
[2025-02-13 20:46:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:44][root][INFO] - Training Epoch: 2/2, step 6128/7134 completed (loss: 0.09280113875865936, acc: 0.9831932783126831)
[2025-02-13 20:46:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:44][root][INFO] - Training Epoch: 2/2, step 6129/7134 completed (loss: 0.19087301194667816, acc: 0.9585798978805542)
[2025-02-13 20:46:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:44][root][INFO] - Training Epoch: 2/2, step 6130/7134 completed (loss: 0.13576656579971313, acc: 0.9626865386962891)
[2025-02-13 20:46:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:45][root][INFO] - Training Epoch: 2/2, step 6131/7134 completed (loss: 0.25660139322280884, acc: 0.9320388436317444)
[2025-02-13 20:46:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:45][root][INFO] - Training Epoch: 2/2, step 6132/7134 completed (loss: 0.09896999597549438, acc: 0.9743589758872986)
[2025-02-13 20:46:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:45][root][INFO] - Training Epoch: 2/2, step 6133/7134 completed (loss: 0.07429881393909454, acc: 0.9855769276618958)
[2025-02-13 20:46:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:46][root][INFO] - Training Epoch: 2/2, step 6134/7134 completed (loss: 0.20518143475055695, acc: 0.9421965479850769)
[2025-02-13 20:46:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:46][root][INFO] - Training Epoch: 2/2, step 6135/7134 completed (loss: 0.05428113788366318, acc: 0.9922480583190918)
[2025-02-13 20:46:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:46][root][INFO] - Training Epoch: 2/2, step 6136/7134 completed (loss: 0.16060464084148407, acc: 0.970059871673584)
[2025-02-13 20:46:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:47][root][INFO] - Training Epoch: 2/2, step 6137/7134 completed (loss: 0.13606256246566772, acc: 0.9740259647369385)
[2025-02-13 20:46:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:47][root][INFO] - Training Epoch: 2/2, step 6138/7134 completed (loss: 0.14582844078540802, acc: 0.9693251252174377)
[2025-02-13 20:46:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:47][root][INFO] - Training Epoch: 2/2, step 6139/7134 completed (loss: 0.14802473783493042, acc: 0.9523809552192688)
[2025-02-13 20:46:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:48][root][INFO] - Training Epoch: 2/2, step 6140/7134 completed (loss: 0.11664316058158875, acc: 0.9923076629638672)
[2025-02-13 20:46:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:48][root][INFO] - Training Epoch: 2/2, step 6141/7134 completed (loss: 0.20294693112373352, acc: 0.95652174949646)
[2025-02-13 20:46:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:49][root][INFO] - Training Epoch: 2/2, step 6142/7134 completed (loss: 0.08300882577896118, acc: 0.9885714054107666)
[2025-02-13 20:46:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:49][root][INFO] - Training Epoch: 2/2, step 6143/7134 completed (loss: 0.13368935883045197, acc: 0.9696969985961914)
[2025-02-13 20:46:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:49][root][INFO] - Training Epoch: 2/2, step 6144/7134 completed (loss: 0.13336101174354553, acc: 0.965753436088562)
[2025-02-13 20:46:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:50][root][INFO] - Training Epoch: 2/2, step 6145/7134 completed (loss: 0.2405620664358139, acc: 0.9398906826972961)
[2025-02-13 20:46:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:50][root][INFO] - Training Epoch: 2/2, step 6146/7134 completed (loss: 0.14854839444160461, acc: 0.9638554453849792)
[2025-02-13 20:46:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:50][root][INFO] - Training Epoch: 2/2, step 6147/7134 completed (loss: 0.10332141071557999, acc: 0.9842519760131836)
[2025-02-13 20:46:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:51][root][INFO] - Training Epoch: 2/2, step 6148/7134 completed (loss: 0.1029958575963974, acc: 0.9724137783050537)
[2025-02-13 20:46:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:51][root][INFO] - Training Epoch: 2/2, step 6149/7134 completed (loss: 0.1337565779685974, acc: 0.9867549538612366)
[2025-02-13 20:46:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:52][root][INFO] - Training Epoch: 2/2, step 6150/7134 completed (loss: 0.05382421240210533, acc: 0.9860140085220337)
[2025-02-13 20:46:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:52][root][INFO] - Training Epoch: 2/2, step 6151/7134 completed (loss: 0.05250518396496773, acc: 0.993630588054657)
[2025-02-13 20:46:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:52][root][INFO] - Training Epoch: 2/2, step 6152/7134 completed (loss: 0.13626180589199066, acc: 0.9828571677207947)
[2025-02-13 20:46:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:53][root][INFO] - Training Epoch: 2/2, step 6153/7134 completed (loss: 0.11133549362421036, acc: 0.9814814925193787)
[2025-02-13 20:46:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:53][root][INFO] - Training Epoch: 2/2, step 6154/7134 completed (loss: 0.11999788880348206, acc: 0.9723756909370422)
[2025-02-13 20:46:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:53][root][INFO] - Training Epoch: 2/2, step 6155/7134 completed (loss: 0.12952809035778046, acc: 0.976331353187561)
[2025-02-13 20:46:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:54][root][INFO] - Training Epoch: 2/2, step 6156/7134 completed (loss: 0.09357836842536926, acc: 0.9683544039726257)
[2025-02-13 20:46:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:54][root][INFO] - Training Epoch: 2/2, step 6157/7134 completed (loss: 0.046000346541404724, acc: 0.9936708807945251)
[2025-02-13 20:46:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:55][root][INFO] - Training Epoch: 2/2, step 6158/7134 completed (loss: 0.13961637020111084, acc: 0.9736841917037964)
[2025-02-13 20:46:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:55][root][INFO] - Training Epoch: 2/2, step 6159/7134 completed (loss: 0.09323502331972122, acc: 0.988095223903656)
[2025-02-13 20:46:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:55][root][INFO] - Training Epoch: 2/2, step 6160/7134 completed (loss: 0.21265427768230438, acc: 0.9363057613372803)
[2025-02-13 20:46:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:56][root][INFO] - Training Epoch: 2/2, step 6161/7134 completed (loss: 0.18450041115283966, acc: 0.9370078444480896)
[2025-02-13 20:46:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:56][root][INFO] - Training Epoch: 2/2, step 6162/7134 completed (loss: 0.30177247524261475, acc: 0.9487179517745972)
[2025-02-13 20:46:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:57][root][INFO] - Training Epoch: 2/2, step 6163/7134 completed (loss: 0.2400030791759491, acc: 0.9379844665527344)
[2025-02-13 20:46:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:57][root][INFO] - Training Epoch: 2/2, step 6164/7134 completed (loss: 0.12954068183898926, acc: 0.9817073345184326)
[2025-02-13 20:46:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:57][root][INFO] - Training Epoch: 2/2, step 6165/7134 completed (loss: 0.13212604820728302, acc: 0.9694656729698181)
[2025-02-13 20:46:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:58][root][INFO] - Training Epoch: 2/2, step 6166/7134 completed (loss: 0.07177933305501938, acc: 0.9927007555961609)
[2025-02-13 20:46:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:58][root][INFO] - Training Epoch: 2/2, step 6167/7134 completed (loss: 0.11223948001861572, acc: 0.9767441749572754)
[2025-02-13 20:46:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:58][root][INFO] - Training Epoch: 2/2, step 6168/7134 completed (loss: 0.16785679757595062, acc: 0.9731183052062988)
[2025-02-13 20:46:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:59][root][INFO] - Training Epoch: 2/2, step 6169/7134 completed (loss: 0.13833796977996826, acc: 0.9819276928901672)
[2025-02-13 20:46:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:59][root][INFO] - Training Epoch: 2/2, step 6170/7134 completed (loss: 0.0863272100687027, acc: 0.9640287756919861)
[2025-02-13 20:46:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:46:59][root][INFO] - Training Epoch: 2/2, step 6171/7134 completed (loss: 0.06331789493560791, acc: 0.9890109896659851)
[2025-02-13 20:47:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:00][root][INFO] - Training Epoch: 2/2, step 6172/7134 completed (loss: 0.09008540213108063, acc: 0.984375)
[2025-02-13 20:47:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:00][root][INFO] - Training Epoch: 2/2, step 6173/7134 completed (loss: 0.06451287120580673, acc: 0.9679487347602844)
[2025-02-13 20:47:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:01][root][INFO] - Training Epoch: 2/2, step 6174/7134 completed (loss: 0.024174174293875694, acc: 0.9939393997192383)
[2025-02-13 20:47:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:01][root][INFO] - Training Epoch: 2/2, step 6175/7134 completed (loss: 0.10739985853433609, acc: 0.9691358208656311)
[2025-02-13 20:47:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:01][root][INFO] - Training Epoch: 2/2, step 6176/7134 completed (loss: 0.08182985335588455, acc: 0.9817073345184326)
[2025-02-13 20:47:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:02][root][INFO] - Training Epoch: 2/2, step 6177/7134 completed (loss: 0.04733825474977493, acc: 0.9945054650306702)
[2025-02-13 20:47:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:02][root][INFO] - Training Epoch: 2/2, step 6178/7134 completed (loss: 0.0713554248213768, acc: 0.9803921580314636)
[2025-02-13 20:47:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:02][root][INFO] - Training Epoch: 2/2, step 6179/7134 completed (loss: 0.17494326829910278, acc: 0.95333331823349)
[2025-02-13 20:47:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:03][root][INFO] - Training Epoch: 2/2, step 6180/7134 completed (loss: 0.335360050201416, acc: 0.9468085169792175)
[2025-02-13 20:47:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:03][root][INFO] - Training Epoch: 2/2, step 6181/7134 completed (loss: 0.25833019614219666, acc: 0.9583333134651184)
[2025-02-13 20:47:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:04][root][INFO] - Training Epoch: 2/2, step 6182/7134 completed (loss: 0.1894405633211136, acc: 0.9457364082336426)
[2025-02-13 20:47:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:04][root][INFO] - Training Epoch: 2/2, step 6183/7134 completed (loss: 0.3304847776889801, acc: 0.9197530746459961)
[2025-02-13 20:47:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:04][root][INFO] - Training Epoch: 2/2, step 6184/7134 completed (loss: 0.278190553188324, acc: 0.9304347634315491)
[2025-02-13 20:47:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:05][root][INFO] - Training Epoch: 2/2, step 6185/7134 completed (loss: 0.246608704328537, acc: 0.9516128897666931)
[2025-02-13 20:47:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:05][root][INFO] - Training Epoch: 2/2, step 6186/7134 completed (loss: 0.10358776897192001, acc: 0.9694656729698181)
[2025-02-13 20:47:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:06][root][INFO] - Training Epoch: 2/2, step 6187/7134 completed (loss: 0.11209653317928314, acc: 0.9766082167625427)
[2025-02-13 20:47:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:06][root][INFO] - Training Epoch: 2/2, step 6188/7134 completed (loss: 0.19433286786079407, acc: 0.9673202633857727)
[2025-02-13 20:47:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:06][root][INFO] - Training Epoch: 2/2, step 6189/7134 completed (loss: 0.15359553694725037, acc: 0.9510489702224731)
[2025-02-13 20:47:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:07][root][INFO] - Training Epoch: 2/2, step 6190/7134 completed (loss: 0.14296954870224, acc: 0.9714285731315613)
[2025-02-13 20:47:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:07][root][INFO] - Training Epoch: 2/2, step 6191/7134 completed (loss: 0.14205273985862732, acc: 0.9593495726585388)
[2025-02-13 20:47:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:07][root][INFO] - Training Epoch: 2/2, step 6192/7134 completed (loss: 0.0876414030790329, acc: 0.9707317352294922)
[2025-02-13 20:47:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:08][root][INFO] - Training Epoch: 2/2, step 6193/7134 completed (loss: 0.11657974869012833, acc: 0.9646464586257935)
[2025-02-13 20:47:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:08][root][INFO] - Training Epoch: 2/2, step 6194/7134 completed (loss: 0.05858490616083145, acc: 0.9918032884597778)
[2025-02-13 20:47:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:09][root][INFO] - Training Epoch: 2/2, step 6195/7134 completed (loss: 0.08710361272096634, acc: 0.9716981053352356)
[2025-02-13 20:47:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:09][root][INFO] - Training Epoch: 2/2, step 6196/7134 completed (loss: 0.05606059730052948, acc: 0.9814814925193787)
[2025-02-13 20:47:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:09][root][INFO] - Training Epoch: 2/2, step 6197/7134 completed (loss: 0.16629911959171295, acc: 0.9715909361839294)
[2025-02-13 20:47:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:10][root][INFO] - Training Epoch: 2/2, step 6198/7134 completed (loss: 0.10205206274986267, acc: 0.9646017551422119)
[2025-02-13 20:47:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:10][root][INFO] - Training Epoch: 2/2, step 6199/7134 completed (loss: 0.05792485177516937, acc: 0.9894179701805115)
[2025-02-13 20:47:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:10][root][INFO] - Training Epoch: 2/2, step 6200/7134 completed (loss: 0.07123562693595886, acc: 0.9846153855323792)
[2025-02-13 20:47:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:11][root][INFO] - Training Epoch: 2/2, step 6201/7134 completed (loss: 0.1276313215494156, acc: 0.9665071964263916)
[2025-02-13 20:47:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:11][root][INFO] - Training Epoch: 2/2, step 6202/7134 completed (loss: 0.08983160555362701, acc: 0.961904764175415)
[2025-02-13 20:47:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:11][root][INFO] - Training Epoch: 2/2, step 6203/7134 completed (loss: 0.08021274209022522, acc: 0.9767441749572754)
[2025-02-13 20:47:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:12][root][INFO] - Training Epoch: 2/2, step 6204/7134 completed (loss: 0.11756919324398041, acc: 0.9916666746139526)
[2025-02-13 20:47:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:12][root][INFO] - Training Epoch: 2/2, step 6205/7134 completed (loss: 0.08987348526716232, acc: 0.9928057789802551)
[2025-02-13 20:47:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:13][root][INFO] - Training Epoch: 2/2, step 6206/7134 completed (loss: 0.028600165620446205, acc: 1.0)
[2025-02-13 20:47:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:13][root][INFO] - Training Epoch: 2/2, step 6207/7134 completed (loss: 0.28257834911346436, acc: 0.939393937587738)
[2025-02-13 20:47:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:13][root][INFO] - Training Epoch: 2/2, step 6208/7134 completed (loss: 0.0942830741405487, acc: 0.984375)
[2025-02-13 20:47:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:14][root][INFO] - Training Epoch: 2/2, step 6209/7134 completed (loss: 0.06530624628067017, acc: 0.9940476417541504)
[2025-02-13 20:47:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:14][root][INFO] - Training Epoch: 2/2, step 6210/7134 completed (loss: 0.14256450533866882, acc: 0.9679144620895386)
[2025-02-13 20:47:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:14][root][INFO] - Training Epoch: 2/2, step 6211/7134 completed (loss: 0.12782317399978638, acc: 0.9575757384300232)
[2025-02-13 20:47:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:15][root][INFO] - Training Epoch: 2/2, step 6212/7134 completed (loss: 0.066579669713974, acc: 0.9743589758872986)
[2025-02-13 20:47:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:15][root][INFO] - Training Epoch: 2/2, step 6213/7134 completed (loss: 0.040531571954488754, acc: 0.9909090995788574)
[2025-02-13 20:47:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:15][root][INFO] - Training Epoch: 2/2, step 6214/7134 completed (loss: 0.0666302740573883, acc: 0.9887005686759949)
[2025-02-13 20:47:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:16][root][INFO] - Training Epoch: 2/2, step 6215/7134 completed (loss: 0.09789345413446426, acc: 0.9758453965187073)
[2025-02-13 20:47:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:16][root][INFO] - Training Epoch: 2/2, step 6216/7134 completed (loss: 0.10079497843980789, acc: 0.9897959232330322)
[2025-02-13 20:47:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:17][root][INFO] - Training Epoch: 2/2, step 6217/7134 completed (loss: 0.045001156628131866, acc: 0.9873417615890503)
[2025-02-13 20:47:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:17][root][INFO] - Training Epoch: 2/2, step 6218/7134 completed (loss: 0.0397220104932785, acc: 0.9870967864990234)
[2025-02-13 20:47:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:17][root][INFO] - Training Epoch: 2/2, step 6219/7134 completed (loss: 0.07315216213464737, acc: 0.9869281053543091)
[2025-02-13 20:47:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:18][root][INFO] - Training Epoch: 2/2, step 6220/7134 completed (loss: 0.06114772707223892, acc: 0.977011501789093)
[2025-02-13 20:47:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:18][root][INFO] - Training Epoch: 2/2, step 6221/7134 completed (loss: 0.02615790255367756, acc: 0.9925373196601868)
[2025-02-13 20:47:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:18][root][INFO] - Training Epoch: 2/2, step 6222/7134 completed (loss: 0.1423056572675705, acc: 0.9638554453849792)
[2025-02-13 20:47:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:19][root][INFO] - Training Epoch: 2/2, step 6223/7134 completed (loss: 0.2959073483943939, acc: 0.9479166865348816)
[2025-02-13 20:47:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:19][root][INFO] - Training Epoch: 2/2, step 6224/7134 completed (loss: 0.08135972917079926, acc: 0.9729729890823364)
[2025-02-13 20:47:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:19][root][INFO] - Training Epoch: 2/2, step 6225/7134 completed (loss: 0.11893455684185028, acc: 0.9720670580863953)
[2025-02-13 20:47:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:20][root][INFO] - Training Epoch: 2/2, step 6226/7134 completed (loss: 0.12542106211185455, acc: 0.97826087474823)
[2025-02-13 20:47:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:20][root][INFO] - Training Epoch: 2/2, step 6227/7134 completed (loss: 0.046948306262493134, acc: 0.987500011920929)
[2025-02-13 20:47:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:21][root][INFO] - Training Epoch: 2/2, step 6228/7134 completed (loss: 0.12219639122486115, acc: 0.978723406791687)
[2025-02-13 20:47:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:21][root][INFO] - Training Epoch: 2/2, step 6229/7134 completed (loss: 0.20635709166526794, acc: 0.9414634108543396)
[2025-02-13 20:47:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:21][root][INFO] - Training Epoch: 2/2, step 6230/7134 completed (loss: 0.1688329130411148, acc: 0.9734042286872864)
[2025-02-13 20:47:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:22][root][INFO] - Training Epoch: 2/2, step 6231/7134 completed (loss: 0.0716715008020401, acc: 0.9766082167625427)
[2025-02-13 20:47:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:22][root][INFO] - Training Epoch: 2/2, step 6232/7134 completed (loss: 0.09704241901636124, acc: 0.9670329689979553)
[2025-02-13 20:47:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:22][root][INFO] - Training Epoch: 2/2, step 6233/7134 completed (loss: 0.06029786914587021, acc: 0.9942528605461121)
[2025-02-13 20:47:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:23][root][INFO] - Training Epoch: 2/2, step 6234/7134 completed (loss: 0.24203038215637207, acc: 0.9388889074325562)
[2025-02-13 20:47:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:23][root][INFO] - Training Epoch: 2/2, step 6235/7134 completed (loss: 0.0685478150844574, acc: 0.976190447807312)
[2025-02-13 20:47:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:24][root][INFO] - Training Epoch: 2/2, step 6236/7134 completed (loss: 0.1364632248878479, acc: 0.9664804339408875)
[2025-02-13 20:47:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:24][root][INFO] - Training Epoch: 2/2, step 6237/7134 completed (loss: 0.200382798910141, acc: 0.9491525292396545)
[2025-02-13 20:47:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:24][root][INFO] - Training Epoch: 2/2, step 6238/7134 completed (loss: 0.08039751648902893, acc: 0.9735099077224731)
[2025-02-13 20:47:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:25][root][INFO] - Training Epoch: 2/2, step 6239/7134 completed (loss: 0.08833660185337067, acc: 0.9740932583808899)
[2025-02-13 20:47:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:25][root][INFO] - Training Epoch: 2/2, step 6240/7134 completed (loss: 0.01669553481042385, acc: 1.0)
[2025-02-13 20:47:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:25][root][INFO] - Training Epoch: 2/2, step 6241/7134 completed (loss: 0.05745571106672287, acc: 0.9933333396911621)
[2025-02-13 20:47:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:26][root][INFO] - Training Epoch: 2/2, step 6242/7134 completed (loss: 0.041974157094955444, acc: 0.9931972622871399)
[2025-02-13 20:47:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:26][root][INFO] - Training Epoch: 2/2, step 6243/7134 completed (loss: 0.09936024248600006, acc: 0.9798657894134521)
[2025-02-13 20:47:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:27][root][INFO] - Training Epoch: 2/2, step 6244/7134 completed (loss: 0.08568377047777176, acc: 0.9820359349250793)
[2025-02-13 20:47:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:27][root][INFO] - Training Epoch: 2/2, step 6245/7134 completed (loss: 0.0865597277879715, acc: 0.976331353187561)
[2025-02-13 20:47:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:27][root][INFO] - Training Epoch: 2/2, step 6246/7134 completed (loss: 0.04636310786008835, acc: 0.9839572310447693)
[2025-02-13 20:47:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:28][root][INFO] - Training Epoch: 2/2, step 6247/7134 completed (loss: 0.11330699175596237, acc: 0.9728260636329651)
[2025-02-13 20:47:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:28][root][INFO] - Training Epoch: 2/2, step 6248/7134 completed (loss: 0.22999726235866547, acc: 0.9580838084220886)
[2025-02-13 20:47:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:29][root][INFO] - Training Epoch: 2/2, step 6249/7134 completed (loss: 0.026836354285478592, acc: 0.9933775067329407)
[2025-02-13 20:47:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:29][root][INFO] - Training Epoch: 2/2, step 6250/7134 completed (loss: 0.00955122709274292, acc: 1.0)
[2025-02-13 20:47:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:29][root][INFO] - Training Epoch: 2/2, step 6251/7134 completed (loss: 0.1527872085571289, acc: 0.9536423683166504)
[2025-02-13 20:47:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:30][root][INFO] - Training Epoch: 2/2, step 6252/7134 completed (loss: 0.009332169778645039, acc: 1.0)
[2025-02-13 20:47:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:30][root][INFO] - Training Epoch: 2/2, step 6253/7134 completed (loss: 0.061954565346241, acc: 0.9720279574394226)
[2025-02-13 20:47:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:31][root][INFO] - Training Epoch: 2/2, step 6254/7134 completed (loss: 0.0669860690832138, acc: 0.9850746393203735)
[2025-02-13 20:47:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:31][root][INFO] - Training Epoch: 2/2, step 6255/7134 completed (loss: 0.08648475259542465, acc: 0.9829545617103577)
[2025-02-13 20:47:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:31][root][INFO] - Training Epoch: 2/2, step 6256/7134 completed (loss: 0.024859590455889702, acc: 1.0)
[2025-02-13 20:47:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:32][root][INFO] - Training Epoch: 2/2, step 6257/7134 completed (loss: 0.031658221036195755, acc: 0.9940828680992126)
[2025-02-13 20:47:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:32][root][INFO] - Training Epoch: 2/2, step 6258/7134 completed (loss: 0.04047425463795662, acc: 0.9866666793823242)
[2025-02-13 20:47:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:33][root][INFO] - Training Epoch: 2/2, step 6259/7134 completed (loss: 0.026074159890413284, acc: 0.9937888383865356)
[2025-02-13 20:47:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:33][root][INFO] - Training Epoch: 2/2, step 6260/7134 completed (loss: 0.028599854558706284, acc: 0.993630588054657)
[2025-02-13 20:47:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:33][root][INFO] - Training Epoch: 2/2, step 6261/7134 completed (loss: 0.06879109889268875, acc: 0.9875776171684265)
[2025-02-13 20:47:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:34][root][INFO] - Training Epoch: 2/2, step 6262/7134 completed (loss: 0.03313518315553665, acc: 0.9876543283462524)
[2025-02-13 20:47:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:34][root][INFO] - Training Epoch: 2/2, step 6263/7134 completed (loss: 0.021045608446002007, acc: 0.9935897588729858)
[2025-02-13 20:47:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:34][root][INFO] - Training Epoch: 2/2, step 6264/7134 completed (loss: 0.022011125460267067, acc: 1.0)
[2025-02-13 20:47:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:35][root][INFO] - Training Epoch: 2/2, step 6265/7134 completed (loss: 0.03043695166707039, acc: 0.9938650131225586)
[2025-02-13 20:47:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:35][root][INFO] - Training Epoch: 2/2, step 6266/7134 completed (loss: 0.013655963353812695, acc: 1.0)
[2025-02-13 20:47:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:36][root][INFO] - Training Epoch: 2/2, step 6267/7134 completed (loss: 0.02536744251847267, acc: 0.9934210777282715)
[2025-02-13 20:47:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:36][root][INFO] - Training Epoch: 2/2, step 6268/7134 completed (loss: 0.05007852241396904, acc: 0.9751552939414978)
[2025-02-13 20:47:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:36][root][INFO] - Training Epoch: 2/2, step 6269/7134 completed (loss: 0.013864638283848763, acc: 1.0)
[2025-02-13 20:47:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:37][root][INFO] - Training Epoch: 2/2, step 6270/7134 completed (loss: 0.00795657653361559, acc: 1.0)
[2025-02-13 20:47:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:37][root][INFO] - Training Epoch: 2/2, step 6271/7134 completed (loss: 0.0683133527636528, acc: 0.9886363744735718)
[2025-02-13 20:47:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:37][root][INFO] - Training Epoch: 2/2, step 6272/7134 completed (loss: 0.01236842293292284, acc: 0.9938650131225586)
[2025-02-13 20:47:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:38][root][INFO] - Training Epoch: 2/2, step 6273/7134 completed (loss: 0.011164121329784393, acc: 1.0)
[2025-02-13 20:47:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:38][root][INFO] - Training Epoch: 2/2, step 6274/7134 completed (loss: 0.012684876099228859, acc: 1.0)
[2025-02-13 20:47:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:39][root][INFO] - Training Epoch: 2/2, step 6275/7134 completed (loss: 0.06781191378831863, acc: 0.9798657894134521)
[2025-02-13 20:47:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:39][root][INFO] - Training Epoch: 2/2, step 6276/7134 completed (loss: 0.05218241363763809, acc: 0.9837398529052734)
[2025-02-13 20:47:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:39][root][INFO] - Training Epoch: 2/2, step 6277/7134 completed (loss: 0.08634044229984283, acc: 0.9716981053352356)
[2025-02-13 20:47:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:40][root][INFO] - Training Epoch: 2/2, step 6278/7134 completed (loss: 0.02472289465367794, acc: 1.0)
[2025-02-13 20:47:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:40][root][INFO] - Training Epoch: 2/2, step 6279/7134 completed (loss: 0.059075720608234406, acc: 0.9846153855323792)
[2025-02-13 20:47:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:40][root][INFO] - Training Epoch: 2/2, step 6280/7134 completed (loss: 0.0636662170290947, acc: 0.9776119589805603)
[2025-02-13 20:47:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:41][root][INFO] - Training Epoch: 2/2, step 6281/7134 completed (loss: 0.03348628804087639, acc: 0.9852941036224365)
[2025-02-13 20:47:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:41][root][INFO] - Training Epoch: 2/2, step 6282/7134 completed (loss: 0.04856690391898155, acc: 0.9861111044883728)
[2025-02-13 20:47:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:41][root][INFO] - Training Epoch: 2/2, step 6283/7134 completed (loss: 0.029969796538352966, acc: 0.991304337978363)
[2025-02-13 20:47:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:42][root][INFO] - Training Epoch: 2/2, step 6284/7134 completed (loss: 0.09522422403097153, acc: 0.9727272987365723)
[2025-02-13 20:47:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:42][root][INFO] - Training Epoch: 2/2, step 6285/7134 completed (loss: 0.10450190305709839, acc: 0.9684210419654846)
[2025-02-13 20:47:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:42][root][INFO] - Training Epoch: 2/2, step 6286/7134 completed (loss: 0.026482580229640007, acc: 0.9930555820465088)
[2025-02-13 20:47:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:43][root][INFO] - Training Epoch: 2/2, step 6287/7134 completed (loss: 0.0182021651417017, acc: 1.0)
[2025-02-13 20:47:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:43][root][INFO] - Training Epoch: 2/2, step 6288/7134 completed (loss: 0.015002365224063396, acc: 1.0)
[2025-02-13 20:47:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:44][root][INFO] - Training Epoch: 2/2, step 6289/7134 completed (loss: 0.06323818117380142, acc: 0.9924242496490479)
[2025-02-13 20:47:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:44][root][INFO] - Training Epoch: 2/2, step 6290/7134 completed (loss: 0.03794654831290245, acc: 0.9925373196601868)
[2025-02-13 20:47:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:44][root][INFO] - Training Epoch: 2/2, step 6291/7134 completed (loss: 0.015339692123234272, acc: 1.0)
[2025-02-13 20:47:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:45][root][INFO] - Training Epoch: 2/2, step 6292/7134 completed (loss: 0.04619434475898743, acc: 1.0)
[2025-02-13 20:47:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:45][root][INFO] - Training Epoch: 2/2, step 6293/7134 completed (loss: 0.09669095277786255, acc: 0.9818181991577148)
[2025-02-13 20:47:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:46][root][INFO] - Training Epoch: 2/2, step 6294/7134 completed (loss: 0.0768246203660965, acc: 0.9736841917037964)
[2025-02-13 20:47:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:46][root][INFO] - Training Epoch: 2/2, step 6295/7134 completed (loss: 0.04383116215467453, acc: 1.0)
[2025-02-13 20:47:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:46][root][INFO] - Training Epoch: 2/2, step 6296/7134 completed (loss: 0.21934039890766144, acc: 0.9496402740478516)
[2025-02-13 20:47:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:47][root][INFO] - Training Epoch: 2/2, step 6297/7134 completed (loss: 0.08757142722606659, acc: 0.9624060392379761)
[2025-02-13 20:47:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:47][root][INFO] - Training Epoch: 2/2, step 6298/7134 completed (loss: 0.11304126679897308, acc: 0.9722222089767456)
[2025-02-13 20:47:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:47][root][INFO] - Training Epoch: 2/2, step 6299/7134 completed (loss: 0.09425206482410431, acc: 0.9868420958518982)
[2025-02-13 20:47:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:48][root][INFO] - Training Epoch: 2/2, step 6300/7134 completed (loss: 0.08436665683984756, acc: 0.9736841917037964)
[2025-02-13 20:47:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:48][root][INFO] - Training Epoch: 2/2, step 6301/7134 completed (loss: 0.022293126210570335, acc: 1.0)
[2025-02-13 20:47:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:48][root][INFO] - Training Epoch: 2/2, step 6302/7134 completed (loss: 0.019792985171079636, acc: 0.994413435459137)
[2025-02-13 20:47:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:49][root][INFO] - Training Epoch: 2/2, step 6303/7134 completed (loss: 0.032616861164569855, acc: 0.9910314083099365)
[2025-02-13 20:47:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:49][root][INFO] - Training Epoch: 2/2, step 6304/7134 completed (loss: 0.02693331427872181, acc: 0.9886363744735718)
[2025-02-13 20:47:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:50][root][INFO] - Training Epoch: 2/2, step 6305/7134 completed (loss: 0.09318053722381592, acc: 0.9794520735740662)
[2025-02-13 20:47:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:50][root][INFO] - Training Epoch: 2/2, step 6306/7134 completed (loss: 0.021388627588748932, acc: 0.9905213117599487)
[2025-02-13 20:47:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:50][root][INFO] - Training Epoch: 2/2, step 6307/7134 completed (loss: 0.11716089397668839, acc: 0.9770992398262024)
[2025-02-13 20:47:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:51][root][INFO] - Training Epoch: 2/2, step 6308/7134 completed (loss: 0.09652931243181229, acc: 0.9792746305465698)
[2025-02-13 20:47:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:51][root][INFO] - Training Epoch: 2/2, step 6309/7134 completed (loss: 0.04502752795815468, acc: 0.9881656765937805)
[2025-02-13 20:47:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:51][root][INFO] - Training Epoch: 2/2, step 6310/7134 completed (loss: 0.07643579691648483, acc: 0.9649122953414917)
[2025-02-13 20:47:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:52][root][INFO] - Training Epoch: 2/2, step 6311/7134 completed (loss: 0.04759281873703003, acc: 0.9895833134651184)
[2025-02-13 20:47:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:52][root][INFO] - Training Epoch: 2/2, step 6312/7134 completed (loss: 0.030222421512007713, acc: 1.0)
[2025-02-13 20:47:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:53][root][INFO] - Training Epoch: 2/2, step 6313/7134 completed (loss: 0.04656127095222473, acc: 0.9837837815284729)
[2025-02-13 20:47:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:53][root][INFO] - Training Epoch: 2/2, step 6314/7134 completed (loss: 0.032525334507226944, acc: 0.9946236610412598)
[2025-02-13 20:47:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:53][root][INFO] - Training Epoch: 2/2, step 6315/7134 completed (loss: 0.14174020290374756, acc: 0.9664429426193237)
[2025-02-13 20:47:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:54][root][INFO] - Training Epoch: 2/2, step 6316/7134 completed (loss: 0.027030693367123604, acc: 0.9931034445762634)
[2025-02-13 20:47:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:54][root][INFO] - Training Epoch: 2/2, step 6317/7134 completed (loss: 0.01912715658545494, acc: 1.0)
[2025-02-13 20:47:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:54][root][INFO] - Training Epoch: 2/2, step 6318/7134 completed (loss: 0.021511230617761612, acc: 0.9934210777282715)
[2025-02-13 20:47:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:55][root][INFO] - Training Epoch: 2/2, step 6319/7134 completed (loss: 0.06363918632268906, acc: 0.9822221994400024)
[2025-02-13 20:47:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:55][root][INFO] - Training Epoch: 2/2, step 6320/7134 completed (loss: 0.026638440787792206, acc: 0.9932432174682617)
[2025-02-13 20:47:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:56][root][INFO] - Training Epoch: 2/2, step 6321/7134 completed (loss: 0.03630360588431358, acc: 0.9904761910438538)
[2025-02-13 20:47:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:56][root][INFO] - Training Epoch: 2/2, step 6322/7134 completed (loss: 0.036020588129758835, acc: 0.987261176109314)
[2025-02-13 20:47:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:56][root][INFO] - Training Epoch: 2/2, step 6323/7134 completed (loss: 0.053609319031238556, acc: 0.987500011920929)
[2025-02-13 20:47:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:57][root][INFO] - Training Epoch: 2/2, step 6324/7134 completed (loss: 0.13014698028564453, acc: 0.9672130942344666)
[2025-02-13 20:47:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:57][root][INFO] - Training Epoch: 2/2, step 6325/7134 completed (loss: 0.06985335797071457, acc: 0.9838709831237793)
[2025-02-13 20:47:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:57][root][INFO] - Training Epoch: 2/2, step 6326/7134 completed (loss: 0.06781662255525589, acc: 0.9811320900917053)
[2025-02-13 20:47:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:58][root][INFO] - Training Epoch: 2/2, step 6327/7134 completed (loss: 0.03338073194026947, acc: 0.9890109896659851)
[2025-02-13 20:47:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:58][root][INFO] - Training Epoch: 2/2, step 6328/7134 completed (loss: 0.09093764424324036, acc: 0.9813664555549622)
[2025-02-13 20:47:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:59][root][INFO] - Training Epoch: 2/2, step 6329/7134 completed (loss: 0.1062990203499794, acc: 0.9871794581413269)
[2025-02-13 20:47:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:59][root][INFO] - Training Epoch: 2/2, step 6330/7134 completed (loss: 0.04291175305843353, acc: 0.9905660152435303)
[2025-02-13 20:47:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:47:59][root][INFO] - Training Epoch: 2/2, step 6331/7134 completed (loss: 0.01058546919375658, acc: 1.0)
[2025-02-13 20:48:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:00][root][INFO] - Training Epoch: 2/2, step 6332/7134 completed (loss: 0.14518465101718903, acc: 0.9716312289237976)
[2025-02-13 20:48:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:00][root][INFO] - Training Epoch: 2/2, step 6333/7134 completed (loss: 0.0795050859451294, acc: 0.9712643623352051)
[2025-02-13 20:48:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:00][root][INFO] - Training Epoch: 2/2, step 6334/7134 completed (loss: 0.031369660049676895, acc: 0.9926470518112183)
[2025-02-13 20:48:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:01][root][INFO] - Training Epoch: 2/2, step 6335/7134 completed (loss: 0.010250928811728954, acc: 1.0)
[2025-02-13 20:48:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:01][root][INFO] - Training Epoch: 2/2, step 6336/7134 completed (loss: 0.061586491763591766, acc: 0.9824561476707458)
[2025-02-13 20:48:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:02][root][INFO] - Training Epoch: 2/2, step 6337/7134 completed (loss: 0.03326263651251793, acc: 0.9930070042610168)
[2025-02-13 20:48:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:02][root][INFO] - Training Epoch: 2/2, step 6338/7134 completed (loss: 0.03118506632745266, acc: 0.9937106966972351)
[2025-02-13 20:48:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:02][root][INFO] - Training Epoch: 2/2, step 6339/7134 completed (loss: 0.05307633429765701, acc: 0.9869281053543091)
[2025-02-13 20:48:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:03][root][INFO] - Training Epoch: 2/2, step 6340/7134 completed (loss: 0.025081822648644447, acc: 0.9938650131225586)
[2025-02-13 20:48:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:03][root][INFO] - Training Epoch: 2/2, step 6341/7134 completed (loss: 0.048272524029016495, acc: 0.9842519760131836)
[2025-02-13 20:48:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:03][root][INFO] - Training Epoch: 2/2, step 6342/7134 completed (loss: 0.06694607436656952, acc: 0.9826589822769165)
[2025-02-13 20:48:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:04][root][INFO] - Training Epoch: 2/2, step 6343/7134 completed (loss: 0.04796016588807106, acc: 0.9931507110595703)
[2025-02-13 20:48:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:04][root][INFO] - Training Epoch: 2/2, step 6344/7134 completed (loss: 0.11503107845783234, acc: 0.9834254384040833)
[2025-02-13 20:48:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:05][root][INFO] - Training Epoch: 2/2, step 6345/7134 completed (loss: 0.04222872108221054, acc: 0.9945651888847351)
[2025-02-13 20:48:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:05][root][INFO] - Training Epoch: 2/2, step 6346/7134 completed (loss: 0.18535345792770386, acc: 0.9651162624359131)
[2025-02-13 20:48:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:05][root][INFO] - Training Epoch: 2/2, step 6347/7134 completed (loss: 0.1546480804681778, acc: 0.9411764740943909)
[2025-02-13 20:48:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:06][root][INFO] - Training Epoch: 2/2, step 6348/7134 completed (loss: 0.030123084783554077, acc: 0.9930555820465088)
[2025-02-13 20:48:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:06][root][INFO] - Training Epoch: 2/2, step 6349/7134 completed (loss: 0.04244699701666832, acc: 0.9848484992980957)
[2025-02-13 20:48:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:06][root][INFO] - Training Epoch: 2/2, step 6350/7134 completed (loss: 0.1169368326663971, acc: 0.9735449552536011)
[2025-02-13 20:48:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:07][root][INFO] - Training Epoch: 2/2, step 6351/7134 completed (loss: 0.10636741667985916, acc: 0.970059871673584)
[2025-02-13 20:48:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:07][root][INFO] - Training Epoch: 2/2, step 6352/7134 completed (loss: 0.20404282212257385, acc: 0.9595375657081604)
[2025-02-13 20:48:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:08][root][INFO] - Training Epoch: 2/2, step 6353/7134 completed (loss: 0.10695762932300568, acc: 0.9754601120948792)
[2025-02-13 20:48:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:08][root][INFO] - Training Epoch: 2/2, step 6354/7134 completed (loss: 0.030786432325839996, acc: 1.0)
[2025-02-13 20:48:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:08][root][INFO] - Training Epoch: 2/2, step 6355/7134 completed (loss: 0.12190602719783783, acc: 0.9800000190734863)
[2025-02-13 20:48:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:09][root][INFO] - Training Epoch: 2/2, step 6356/7134 completed (loss: 0.04401051998138428, acc: 0.9808917045593262)
[2025-02-13 20:48:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:09][root][INFO] - Training Epoch: 2/2, step 6357/7134 completed (loss: 0.05138276889920235, acc: 0.9878787994384766)
[2025-02-13 20:48:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:10][root][INFO] - Training Epoch: 2/2, step 6358/7134 completed (loss: 0.0690394714474678, acc: 0.9824561476707458)
[2025-02-13 20:48:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:10][root][INFO] - Training Epoch: 2/2, step 6359/7134 completed (loss: 0.13813795149326324, acc: 0.9832402467727661)
[2025-02-13 20:48:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:10][root][INFO] - Training Epoch: 2/2, step 6360/7134 completed (loss: 0.07869362831115723, acc: 0.9727891087532043)
[2025-02-13 20:48:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:11][root][INFO] - Training Epoch: 2/2, step 6361/7134 completed (loss: 0.0676577240228653, acc: 0.9928057789802551)
[2025-02-13 20:48:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:11][root][INFO] - Training Epoch: 2/2, step 6362/7134 completed (loss: 0.053441841155290604, acc: 0.9806451797485352)
[2025-02-13 20:48:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:11][root][INFO] - Training Epoch: 2/2, step 6363/7134 completed (loss: 0.076141856610775, acc: 0.9881656765937805)
[2025-02-13 20:48:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:12][root][INFO] - Training Epoch: 2/2, step 6364/7134 completed (loss: 0.051206666976213455, acc: 0.9808917045593262)
[2025-02-13 20:48:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:12][root][INFO] - Training Epoch: 2/2, step 6365/7134 completed (loss: 0.0361582413315773, acc: 0.991525411605835)
[2025-02-13 20:48:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:13][root][INFO] - Training Epoch: 2/2, step 6366/7134 completed (loss: 0.12096896022558212, acc: 0.9704142212867737)
[2025-02-13 20:48:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:13][root][INFO] - Training Epoch: 2/2, step 6367/7134 completed (loss: 0.05503065884113312, acc: 0.9922480583190918)
[2025-02-13 20:48:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:13][root][INFO] - Training Epoch: 2/2, step 6368/7134 completed (loss: 0.07125040143728256, acc: 0.9698795080184937)
[2025-02-13 20:48:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:14][root][INFO] - Training Epoch: 2/2, step 6369/7134 completed (loss: 0.09011659771203995, acc: 0.9702380895614624)
[2025-02-13 20:48:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:14][root][INFO] - Training Epoch: 2/2, step 6370/7134 completed (loss: 0.172218456864357, acc: 0.9593023061752319)
[2025-02-13 20:48:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:15][root][INFO] - Training Epoch: 2/2, step 6371/7134 completed (loss: 0.15853703022003174, acc: 0.9607843160629272)
[2025-02-13 20:48:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:15][root][INFO] - Training Epoch: 2/2, step 6372/7134 completed (loss: 0.15218010544776917, acc: 0.982758641242981)
[2025-02-13 20:48:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:15][root][INFO] - Training Epoch: 2/2, step 6373/7134 completed (loss: 0.3064250946044922, acc: 0.9677419066429138)
[2025-02-13 20:48:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:16][root][INFO] - Training Epoch: 2/2, step 6374/7134 completed (loss: 0.08918382972478867, acc: 0.9663865566253662)
[2025-02-13 20:48:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:16][root][INFO] - Training Epoch: 2/2, step 6375/7134 completed (loss: 0.12996168434619904, acc: 0.9613259434700012)
[2025-02-13 20:48:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:16][root][INFO] - Training Epoch: 2/2, step 6376/7134 completed (loss: 0.06970565766096115, acc: 0.9837398529052734)
[2025-02-13 20:48:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:17][root][INFO] - Training Epoch: 2/2, step 6377/7134 completed (loss: 0.03777536004781723, acc: 0.9919999837875366)
[2025-02-13 20:48:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:17][root][INFO] - Training Epoch: 2/2, step 6378/7134 completed (loss: 0.13282720744609833, acc: 0.9674418568611145)
[2025-02-13 20:48:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:18][root][INFO] - Training Epoch: 2/2, step 6379/7134 completed (loss: 0.22074230015277863, acc: 0.9684210419654846)
[2025-02-13 20:48:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:18][root][INFO] - Training Epoch: 2/2, step 6380/7134 completed (loss: 0.12205418199300766, acc: 0.9624999761581421)
[2025-02-13 20:48:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:18][root][INFO] - Training Epoch: 2/2, step 6381/7134 completed (loss: 0.02273492142558098, acc: 1.0)
[2025-02-13 20:48:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:19][root][INFO] - Training Epoch: 2/2, step 6382/7134 completed (loss: 0.03267921879887581, acc: 0.9945054650306702)
[2025-02-13 20:48:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:19][root][INFO] - Training Epoch: 2/2, step 6383/7134 completed (loss: 0.19249393045902252, acc: 0.9595375657081604)
[2025-02-13 20:48:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:19][root][INFO] - Training Epoch: 2/2, step 6384/7134 completed (loss: 0.053412310779094696, acc: 0.9936708807945251)
[2025-02-13 20:48:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:20][root][INFO] - Training Epoch: 2/2, step 6385/7134 completed (loss: 0.18038320541381836, acc: 0.9853658676147461)
[2025-02-13 20:48:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:20][root][INFO] - Training Epoch: 2/2, step 6386/7134 completed (loss: 0.057331427931785583, acc: 0.9892473220825195)
[2025-02-13 20:48:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:21][root][INFO] - Training Epoch: 2/2, step 6387/7134 completed (loss: 0.04909689351916313, acc: 0.9939393997192383)
[2025-02-13 20:48:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:21][root][INFO] - Training Epoch: 2/2, step 6388/7134 completed (loss: 0.09119461476802826, acc: 0.9810126423835754)
[2025-02-13 20:48:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:21][root][INFO] - Training Epoch: 2/2, step 6389/7134 completed (loss: 0.1251584142446518, acc: 0.9615384340286255)
[2025-02-13 20:48:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:22][root][INFO] - Training Epoch: 2/2, step 6390/7134 completed (loss: 0.06722979992628098, acc: 0.9826086759567261)
[2025-02-13 20:48:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:22][root][INFO] - Training Epoch: 2/2, step 6391/7134 completed (loss: 0.1443108171224594, acc: 0.9593023061752319)
[2025-02-13 20:48:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:22][root][INFO] - Training Epoch: 2/2, step 6392/7134 completed (loss: 0.13707105815410614, acc: 0.9652174115180969)
[2025-02-13 20:48:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:23][root][INFO] - Training Epoch: 2/2, step 6393/7134 completed (loss: 0.1197972297668457, acc: 0.9814814925193787)
[2025-02-13 20:48:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:23][root][INFO] - Training Epoch: 2/2, step 6394/7134 completed (loss: 0.1902473270893097, acc: 0.9666666388511658)
[2025-02-13 20:48:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:23][root][INFO] - Training Epoch: 2/2, step 6395/7134 completed (loss: 0.09121093899011612, acc: 0.9851484894752502)
[2025-02-13 20:48:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:24][root][INFO] - Training Epoch: 2/2, step 6396/7134 completed (loss: 0.08312695473432541, acc: 0.9760765433311462)
[2025-02-13 20:48:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:24][root][INFO] - Training Epoch: 2/2, step 6397/7134 completed (loss: 0.05771489441394806, acc: 0.9850746393203735)
[2025-02-13 20:48:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:25][root][INFO] - Training Epoch: 2/2, step 6398/7134 completed (loss: 0.05585912615060806, acc: 0.9931034445762634)
[2025-02-13 20:48:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:25][root][INFO] - Training Epoch: 2/2, step 6399/7134 completed (loss: 0.07949736714363098, acc: 0.9887640476226807)
[2025-02-13 20:48:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:25][root][INFO] - Training Epoch: 2/2, step 6400/7134 completed (loss: 0.022002605721354485, acc: 1.0)
[2025-02-13 20:48:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:26][root][INFO] - Training Epoch: 2/2, step 6401/7134 completed (loss: 0.06635664403438568, acc: 0.9904761910438538)
[2025-02-13 20:48:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:26][root][INFO] - Training Epoch: 2/2, step 6402/7134 completed (loss: 0.14015063643455505, acc: 0.9696969985961914)
[2025-02-13 20:48:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:27][root][INFO] - Training Epoch: 2/2, step 6403/7134 completed (loss: 0.030099356546998024, acc: 0.9948186278343201)
[2025-02-13 20:48:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:27][root][INFO] - Training Epoch: 2/2, step 6404/7134 completed (loss: 0.046106934547424316, acc: 0.9881656765937805)
[2025-02-13 20:48:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:27][root][INFO] - Training Epoch: 2/2, step 6405/7134 completed (loss: 0.06335413455963135, acc: 0.9855769276618958)
[2025-02-13 20:48:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:28][root][INFO] - Training Epoch: 2/2, step 6406/7134 completed (loss: 0.11756708472967148, acc: 0.9649999737739563)
[2025-02-13 20:48:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:28][root][INFO] - Training Epoch: 2/2, step 6407/7134 completed (loss: 0.048032570630311966, acc: 0.9888888597488403)
[2025-02-13 20:48:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:29][root][INFO] - Training Epoch: 2/2, step 6408/7134 completed (loss: 0.1297396719455719, acc: 0.9551281929016113)
[2025-02-13 20:48:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:29][root][INFO] - Training Epoch: 2/2, step 6409/7134 completed (loss: 0.11693218350410461, acc: 0.9878048896789551)
[2025-02-13 20:48:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:29][root][INFO] - Training Epoch: 2/2, step 6410/7134 completed (loss: 0.18191081285476685, acc: 0.9416666626930237)
[2025-02-13 20:48:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:30][root][INFO] - Training Epoch: 2/2, step 6411/7134 completed (loss: 0.1285821497440338, acc: 0.9689922332763672)
[2025-02-13 20:48:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:30][root][INFO] - Training Epoch: 2/2, step 6412/7134 completed (loss: 0.03749025613069534, acc: 0.9935483932495117)
[2025-02-13 20:48:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:31][root][INFO] - Training Epoch: 2/2, step 6413/7134 completed (loss: 0.17990568280220032, acc: 0.9731543660163879)
[2025-02-13 20:48:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:31][root][INFO] - Training Epoch: 2/2, step 6414/7134 completed (loss: 0.05523082986474037, acc: 0.9868420958518982)
[2025-02-13 20:48:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:31][root][INFO] - Training Epoch: 2/2, step 6415/7134 completed (loss: 0.07605155557394028, acc: 0.9772727489471436)
[2025-02-13 20:48:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:32][root][INFO] - Training Epoch: 2/2, step 6416/7134 completed (loss: 0.1280083954334259, acc: 0.9637681245803833)
[2025-02-13 20:48:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:32][root][INFO] - Training Epoch: 2/2, step 6417/7134 completed (loss: 0.22689023613929749, acc: 0.9534883499145508)
[2025-02-13 20:48:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:33][root][INFO] - Training Epoch: 2/2, step 6418/7134 completed (loss: 0.2767059803009033, acc: 0.9328858852386475)
[2025-02-13 20:48:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:33][root][INFO] - Training Epoch: 2/2, step 6419/7134 completed (loss: 0.2744019329547882, acc: 0.9285714030265808)
[2025-02-13 20:48:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:33][root][INFO] - Training Epoch: 2/2, step 6420/7134 completed (loss: 0.12321346253156662, acc: 0.9683544039726257)
[2025-02-13 20:48:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:34][root][INFO] - Training Epoch: 2/2, step 6421/7134 completed (loss: 0.18027682602405548, acc: 0.9638554453849792)
[2025-02-13 20:48:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:34][root][INFO] - Training Epoch: 2/2, step 6422/7134 completed (loss: 0.14051736891269684, acc: 0.9664804339408875)
[2025-02-13 20:48:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:35][root][INFO] - Training Epoch: 2/2, step 6423/7134 completed (loss: 0.1769413948059082, acc: 0.9615384340286255)
[2025-02-13 20:48:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:35][root][INFO] - Training Epoch: 2/2, step 6424/7134 completed (loss: 0.13774770498275757, acc: 0.9627659320831299)
[2025-02-13 20:48:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:35][root][INFO] - Training Epoch: 2/2, step 6425/7134 completed (loss: 0.0679839625954628, acc: 0.9832402467727661)
[2025-02-13 20:48:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:36][root][INFO] - Training Epoch: 2/2, step 6426/7134 completed (loss: 0.1054275631904602, acc: 0.9776536226272583)
[2025-02-13 20:48:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:36][root][INFO] - Training Epoch: 2/2, step 6427/7134 completed (loss: 0.11556800454854965, acc: 0.9681528806686401)
[2025-02-13 20:48:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:37][root][INFO] - Training Epoch: 2/2, step 6428/7134 completed (loss: 0.05378594994544983, acc: 0.993630588054657)
[2025-02-13 20:48:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:37][root][INFO] - Training Epoch: 2/2, step 6429/7134 completed (loss: 0.07473713904619217, acc: 0.9806451797485352)
[2025-02-13 20:48:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:37][root][INFO] - Training Epoch: 2/2, step 6430/7134 completed (loss: 0.2346418797969818, acc: 0.9624999761581421)
[2025-02-13 20:48:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:38][root][INFO] - Training Epoch: 2/2, step 6431/7134 completed (loss: 0.18094509840011597, acc: 0.9459459185600281)
[2025-02-13 20:48:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:38][root][INFO] - Training Epoch: 2/2, step 6432/7134 completed (loss: 0.5721116065979004, acc: 0.8838709592819214)
[2025-02-13 20:48:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:38][root][INFO] - Training Epoch: 2/2, step 6433/7134 completed (loss: 0.16456478834152222, acc: 0.9545454382896423)
[2025-02-13 20:48:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:39][root][INFO] - Training Epoch: 2/2, step 6434/7134 completed (loss: 0.04046396166086197, acc: 0.9942196607589722)
[2025-02-13 20:48:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:39][root][INFO] - Training Epoch: 2/2, step 6435/7134 completed (loss: 0.08554527163505554, acc: 0.97826087474823)
[2025-02-13 20:48:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:40][root][INFO] - Training Epoch: 2/2, step 6436/7134 completed (loss: 0.07116959989070892, acc: 0.9865771532058716)
[2025-02-13 20:48:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:40][root][INFO] - Training Epoch: 2/2, step 6437/7134 completed (loss: 0.15257418155670166, acc: 0.9605262875556946)
[2025-02-13 20:48:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:40][root][INFO] - Training Epoch: 2/2, step 6438/7134 completed (loss: 0.19808754324913025, acc: 0.9563106894493103)
[2025-02-13 20:48:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:41][root][INFO] - Training Epoch: 2/2, step 6439/7134 completed (loss: 0.08640865236520767, acc: 0.9751552939414978)
[2025-02-13 20:48:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:41][root][INFO] - Training Epoch: 2/2, step 6440/7134 completed (loss: 0.08158484846353531, acc: 0.9903846383094788)
[2025-02-13 20:48:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:42][root][INFO] - Training Epoch: 2/2, step 6441/7134 completed (loss: 0.08133763074874878, acc: 0.9811320900917053)
[2025-02-13 20:48:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:42][root][INFO] - Training Epoch: 2/2, step 6442/7134 completed (loss: 0.05106697231531143, acc: 0.9854369163513184)
[2025-02-13 20:48:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:42][root][INFO] - Training Epoch: 2/2, step 6443/7134 completed (loss: 0.040617264807224274, acc: 0.9950980544090271)
[2025-02-13 20:48:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:43][root][INFO] - Training Epoch: 2/2, step 6444/7134 completed (loss: 0.08568306267261505, acc: 0.9888888597488403)
[2025-02-13 20:48:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:43][root][INFO] - Training Epoch: 2/2, step 6445/7134 completed (loss: 0.1825646460056305, acc: 0.9605911374092102)
[2025-02-13 20:48:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:44][root][INFO] - Training Epoch: 2/2, step 6446/7134 completed (loss: 0.05493902042508125, acc: 0.9852941036224365)
[2025-02-13 20:48:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:44][root][INFO] - Training Epoch: 2/2, step 6447/7134 completed (loss: 0.11314919590950012, acc: 0.96875)
[2025-02-13 20:48:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:44][root][INFO] - Training Epoch: 2/2, step 6448/7134 completed (loss: 0.24574442207813263, acc: 0.9548386931419373)
[2025-02-13 20:48:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:45][root][INFO] - Training Epoch: 2/2, step 6449/7134 completed (loss: 0.08873064070940018, acc: 0.984455943107605)
[2025-02-13 20:48:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:45][root][INFO] - Training Epoch: 2/2, step 6450/7134 completed (loss: 0.048748794943094254, acc: 0.9884393215179443)
[2025-02-13 20:48:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:46][root][INFO] - Training Epoch: 2/2, step 6451/7134 completed (loss: 0.14823412895202637, acc: 0.9663865566253662)
[2025-02-13 20:48:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:46][root][INFO] - Training Epoch: 2/2, step 6452/7134 completed (loss: 0.021028079092502594, acc: 1.0)
[2025-02-13 20:48:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:46][root][INFO] - Training Epoch: 2/2, step 6453/7134 completed (loss: 0.07030782848596573, acc: 0.9754601120948792)
[2025-02-13 20:48:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:47][root][INFO] - Training Epoch: 2/2, step 6454/7134 completed (loss: 0.07558469474315643, acc: 0.9781420826911926)
[2025-02-13 20:48:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:47][root][INFO] - Training Epoch: 2/2, step 6455/7134 completed (loss: 0.15505196154117584, acc: 0.9733333587646484)
[2025-02-13 20:48:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:47][root][INFO] - Training Epoch: 2/2, step 6456/7134 completed (loss: 0.20107552409172058, acc: 0.946107804775238)
[2025-02-13 20:48:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:48][root][INFO] - Training Epoch: 2/2, step 6457/7134 completed (loss: 0.042821455746889114, acc: 0.9816513657569885)
[2025-02-13 20:48:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:48][root][INFO] - Training Epoch: 2/2, step 6458/7134 completed (loss: 0.05876254290342331, acc: 0.9864864945411682)
[2025-02-13 20:48:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:49][root][INFO] - Training Epoch: 2/2, step 6459/7134 completed (loss: 0.009486569091677666, acc: 1.0)
[2025-02-13 20:48:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:49][root][INFO] - Training Epoch: 2/2, step 6460/7134 completed (loss: 0.08992375433444977, acc: 0.9846938848495483)
[2025-02-13 20:48:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:49][root][INFO] - Training Epoch: 2/2, step 6461/7134 completed (loss: 0.03467696160078049, acc: 1.0)
[2025-02-13 20:48:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:50][root][INFO] - Training Epoch: 2/2, step 6462/7134 completed (loss: 0.02862738072872162, acc: 0.9946808218955994)
[2025-02-13 20:48:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:50][root][INFO] - Training Epoch: 2/2, step 6463/7134 completed (loss: 0.06869190186262131, acc: 0.9902912378311157)
[2025-02-13 20:48:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:50][root][INFO] - Training Epoch: 2/2, step 6464/7134 completed (loss: 0.09858235716819763, acc: 0.9720670580863953)
[2025-02-13 20:48:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:51][root][INFO] - Training Epoch: 2/2, step 6465/7134 completed (loss: 0.0239388570189476, acc: 1.0)
[2025-02-13 20:48:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:51][root][INFO] - Training Epoch: 2/2, step 6466/7134 completed (loss: 0.09199148416519165, acc: 0.9735449552536011)
[2025-02-13 20:48:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:52][root][INFO] - Training Epoch: 2/2, step 6467/7134 completed (loss: 0.10045759379863739, acc: 0.9759036302566528)
[2025-02-13 20:48:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:52][root][INFO] - Training Epoch: 2/2, step 6468/7134 completed (loss: 0.03730342164635658, acc: 0.9926470518112183)
[2025-02-13 20:48:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:52][root][INFO] - Training Epoch: 2/2, step 6469/7134 completed (loss: 0.1485876441001892, acc: 0.9626168012619019)
[2025-02-13 20:48:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:53][root][INFO] - Training Epoch: 2/2, step 6470/7134 completed (loss: 0.05966109037399292, acc: 0.9803921580314636)
[2025-02-13 20:48:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:53][root][INFO] - Training Epoch: 2/2, step 6471/7134 completed (loss: 0.050539951771497726, acc: 1.0)
[2025-02-13 20:48:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:53][root][INFO] - Training Epoch: 2/2, step 6472/7134 completed (loss: 0.1197376698255539, acc: 0.9750000238418579)
[2025-02-13 20:48:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:54][root][INFO] - Training Epoch: 2/2, step 6473/7134 completed (loss: 0.03376829996705055, acc: 0.9944444298744202)
[2025-02-13 20:48:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:54][root][INFO] - Training Epoch: 2/2, step 6474/7134 completed (loss: 0.047257114201784134, acc: 0.9946236610412598)
[2025-02-13 20:48:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:55][root][INFO] - Training Epoch: 2/2, step 6475/7134 completed (loss: 0.054552916437387466, acc: 0.9857142567634583)
[2025-02-13 20:48:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:55][root][INFO] - Training Epoch: 2/2, step 6476/7134 completed (loss: 0.051614295691251755, acc: 0.9866666793823242)
[2025-02-13 20:48:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:55][root][INFO] - Training Epoch: 2/2, step 6477/7134 completed (loss: 0.10754046589136124, acc: 0.9652174115180969)
[2025-02-13 20:48:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:56][root][INFO] - Training Epoch: 2/2, step 6478/7134 completed (loss: 0.07625342160463333, acc: 0.9710144996643066)
[2025-02-13 20:48:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:56][root][INFO] - Training Epoch: 2/2, step 6479/7134 completed (loss: 0.049995724111795425, acc: 0.9865771532058716)
[2025-02-13 20:48:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:57][root][INFO] - Training Epoch: 2/2, step 6480/7134 completed (loss: 0.0834827795624733, acc: 0.9714285731315613)
[2025-02-13 20:48:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:57][root][INFO] - Training Epoch: 2/2, step 6481/7134 completed (loss: 0.0260658860206604, acc: 0.9928571581840515)
[2025-02-13 20:48:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:57][root][INFO] - Training Epoch: 2/2, step 6482/7134 completed (loss: 0.011432652361690998, acc: 1.0)
[2025-02-13 20:48:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:58][root][INFO] - Training Epoch: 2/2, step 6483/7134 completed (loss: 0.12469460070133209, acc: 0.9861111044883728)
[2025-02-13 20:48:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:58][root][INFO] - Training Epoch: 2/2, step 6484/7134 completed (loss: 0.07707084715366364, acc: 0.9766082167625427)
[2025-02-13 20:48:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:58][root][INFO] - Training Epoch: 2/2, step 6485/7134 completed (loss: 0.028627512976527214, acc: 1.0)
[2025-02-13 20:48:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:59][root][INFO] - Training Epoch: 2/2, step 6486/7134 completed (loss: 0.2100212424993515, acc: 0.9420289993286133)
[2025-02-13 20:48:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:59][root][INFO] - Training Epoch: 2/2, step 6487/7134 completed (loss: 0.08087964355945587, acc: 0.9852941036224365)
[2025-02-13 20:48:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:48:59][root][INFO] - Training Epoch: 2/2, step 6488/7134 completed (loss: 0.12722273170948029, acc: 0.9696969985961914)
[2025-02-13 20:49:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:00][root][INFO] - Training Epoch: 2/2, step 6489/7134 completed (loss: 0.29836902022361755, acc: 0.94017094373703)
[2025-02-13 20:49:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:00][root][INFO] - Training Epoch: 2/2, step 6490/7134 completed (loss: 0.2747597098350525, acc: 0.9402984976768494)
[2025-02-13 20:49:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:01][root][INFO] - Training Epoch: 2/2, step 6491/7134 completed (loss: 0.15635977685451508, acc: 0.9610389471054077)
[2025-02-13 20:49:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:01][root][INFO] - Training Epoch: 2/2, step 6492/7134 completed (loss: 0.06346532702445984, acc: 0.9935483932495117)
[2025-02-13 20:49:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:01][root][INFO] - Training Epoch: 2/2, step 6493/7134 completed (loss: 0.09536036849021912, acc: 0.9642857313156128)
[2025-02-13 20:49:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:02][root][INFO] - Training Epoch: 2/2, step 6494/7134 completed (loss: 0.16289719939231873, acc: 0.9646017551422119)
[2025-02-13 20:49:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:02][root][INFO] - Training Epoch: 2/2, step 6495/7134 completed (loss: 0.08781265467405319, acc: 0.9938271641731262)
[2025-02-13 20:49:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:03][root][INFO] - Training Epoch: 2/2, step 6496/7134 completed (loss: 0.09246156364679337, acc: 0.9768785834312439)
[2025-02-13 20:49:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:03][root][INFO] - Training Epoch: 2/2, step 6497/7134 completed (loss: 0.18712720274925232, acc: 0.9513888955116272)
[2025-02-13 20:49:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:03][root][INFO] - Training Epoch: 2/2, step 6498/7134 completed (loss: 0.10645084083080292, acc: 0.970588207244873)
[2025-02-13 20:49:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:04][root][INFO] - Training Epoch: 2/2, step 6499/7134 completed (loss: 0.0904654860496521, acc: 0.9840425252914429)
[2025-02-13 20:49:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:04][root][INFO] - Training Epoch: 2/2, step 6500/7134 completed (loss: 0.23873239755630493, acc: 0.9425287246704102)
[2025-02-13 20:49:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:04][root][INFO] - Training Epoch: 2/2, step 6501/7134 completed (loss: 0.34109166264533997, acc: 0.9226804375648499)
[2025-02-13 20:49:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:05][root][INFO] - Training Epoch: 2/2, step 6502/7134 completed (loss: 0.10146268457174301, acc: 0.9797979593276978)
[2025-02-13 20:49:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:05][root][INFO] - Training Epoch: 2/2, step 6503/7134 completed (loss: 0.09515953809022903, acc: 0.9751552939414978)
[2025-02-13 20:49:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:05][root][INFO] - Training Epoch: 2/2, step 6504/7134 completed (loss: 0.07809466868638992, acc: 0.9794520735740662)
[2025-02-13 20:49:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:06][root][INFO] - Training Epoch: 2/2, step 6505/7134 completed (loss: 0.12987323105335236, acc: 0.9666666388511658)
[2025-02-13 20:49:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:06][root][INFO] - Training Epoch: 2/2, step 6506/7134 completed (loss: 0.13808321952819824, acc: 0.969072163105011)
[2025-02-13 20:49:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:07][root][INFO] - Training Epoch: 2/2, step 6507/7134 completed (loss: 0.17481057345867157, acc: 0.9447852969169617)
[2025-02-13 20:49:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:07][root][INFO] - Training Epoch: 2/2, step 6508/7134 completed (loss: 0.07078996300697327, acc: 0.9833333492279053)
[2025-02-13 20:49:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:07][root][INFO] - Training Epoch: 2/2, step 6509/7134 completed (loss: 0.06189121678471565, acc: 0.9845361113548279)
[2025-02-13 20:49:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:08][root][INFO] - Training Epoch: 2/2, step 6510/7134 completed (loss: 0.14790743589401245, acc: 0.9708737730979919)
[2025-02-13 20:49:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:08][root][INFO] - Training Epoch: 2/2, step 6511/7134 completed (loss: 0.10550713539123535, acc: 0.9701492786407471)
[2025-02-13 20:49:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:09][root][INFO] - Training Epoch: 2/2, step 6512/7134 completed (loss: 0.03044777177274227, acc: 1.0)
[2025-02-13 20:49:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:09][root][INFO] - Training Epoch: 2/2, step 6513/7134 completed (loss: 0.06588078290224075, acc: 0.9851852059364319)
[2025-02-13 20:49:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:09][root][INFO] - Training Epoch: 2/2, step 6514/7134 completed (loss: 0.1870623528957367, acc: 0.9753086566925049)
[2025-02-13 20:49:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:10][root][INFO] - Training Epoch: 2/2, step 6515/7134 completed (loss: 0.06900478899478912, acc: 0.9780219793319702)
[2025-02-13 20:49:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:10][root][INFO] - Training Epoch: 2/2, step 6516/7134 completed (loss: 0.10094234347343445, acc: 0.9764705896377563)
[2025-02-13 20:49:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:10][root][INFO] - Training Epoch: 2/2, step 6517/7134 completed (loss: 0.029307205229997635, acc: 1.0)
[2025-02-13 20:49:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:11][root][INFO] - Training Epoch: 2/2, step 6518/7134 completed (loss: 0.026320522651076317, acc: 0.9940119981765747)
[2025-02-13 20:49:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:11][root][INFO] - Training Epoch: 2/2, step 6519/7134 completed (loss: 0.06945707648992538, acc: 0.9921259880065918)
[2025-02-13 20:49:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:12][root][INFO] - Training Epoch: 2/2, step 6520/7134 completed (loss: 0.06241043657064438, acc: 0.9935064911842346)
[2025-02-13 20:49:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:12][root][INFO] - Training Epoch: 2/2, step 6521/7134 completed (loss: 0.03345808386802673, acc: 0.9945945739746094)
[2025-02-13 20:49:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:12][root][INFO] - Training Epoch: 2/2, step 6522/7134 completed (loss: 0.06528130918741226, acc: 0.9869281053543091)
[2025-02-13 20:49:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:13][root][INFO] - Training Epoch: 2/2, step 6523/7134 completed (loss: 0.21334156394004822, acc: 0.9513888955116272)
[2025-02-13 20:49:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:13][root][INFO] - Training Epoch: 2/2, step 6524/7134 completed (loss: 0.13913972675800323, acc: 0.9723756909370422)
[2025-02-13 20:49:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:13][root][INFO] - Training Epoch: 2/2, step 6525/7134 completed (loss: 0.022937070578336716, acc: 0.9917355179786682)
[2025-02-13 20:49:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:14][root][INFO] - Training Epoch: 2/2, step 6526/7134 completed (loss: 0.12185550481081009, acc: 0.9695431590080261)
[2025-02-13 20:49:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:14][root][INFO] - Training Epoch: 2/2, step 6527/7134 completed (loss: 0.11273276805877686, acc: 0.9683544039726257)
[2025-02-13 20:49:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:15][root][INFO] - Training Epoch: 2/2, step 6528/7134 completed (loss: 0.14318375289440155, acc: 0.9710144996643066)
[2025-02-13 20:49:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:15][root][INFO] - Training Epoch: 2/2, step 6529/7134 completed (loss: 0.21212778985500336, acc: 0.9618320465087891)
[2025-02-13 20:49:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:15][root][INFO] - Training Epoch: 2/2, step 6530/7134 completed (loss: 0.10733196884393692, acc: 0.9723756909370422)
[2025-02-13 20:49:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:16][root][INFO] - Training Epoch: 2/2, step 6531/7134 completed (loss: 0.06519406288862228, acc: 0.9817073345184326)
[2025-02-13 20:49:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:16][root][INFO] - Training Epoch: 2/2, step 6532/7134 completed (loss: 0.2014022022485733, acc: 0.9725274443626404)
[2025-02-13 20:49:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:16][root][INFO] - Training Epoch: 2/2, step 6533/7134 completed (loss: 0.017807163298130035, acc: 1.0)
[2025-02-13 20:49:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:17][root][INFO] - Training Epoch: 2/2, step 6534/7134 completed (loss: 0.20832324028015137, acc: 0.9537572264671326)
[2025-02-13 20:49:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:17][root][INFO] - Training Epoch: 2/2, step 6535/7134 completed (loss: 0.032262083142995834, acc: 0.9905660152435303)
[2025-02-13 20:49:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:18][root][INFO] - Training Epoch: 2/2, step 6536/7134 completed (loss: 0.06423019617795944, acc: 0.9739130139350891)
[2025-02-13 20:49:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:18][root][INFO] - Training Epoch: 2/2, step 6537/7134 completed (loss: 0.048074375838041306, acc: 0.9910714030265808)
[2025-02-13 20:49:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:18][root][INFO] - Training Epoch: 2/2, step 6538/7134 completed (loss: 0.20285961031913757, acc: 0.965753436088562)
[2025-02-13 20:49:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:19][root][INFO] - Training Epoch: 2/2, step 6539/7134 completed (loss: 0.019706249237060547, acc: 1.0)
[2025-02-13 20:49:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:19][root][INFO] - Training Epoch: 2/2, step 6540/7134 completed (loss: 0.5679641962051392, acc: 0.9090909361839294)
[2025-02-13 20:49:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:19][root][INFO] - Training Epoch: 2/2, step 6541/7134 completed (loss: 0.08683571964502335, acc: 0.9723756909370422)
[2025-02-13 20:49:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:20][root][INFO] - Training Epoch: 2/2, step 6542/7134 completed (loss: 0.07558288425207138, acc: 0.9857142567634583)
[2025-02-13 20:49:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:20][root][INFO] - Training Epoch: 2/2, step 6543/7134 completed (loss: 0.04445742815732956, acc: 0.9824561476707458)
[2025-02-13 20:49:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:21][root][INFO] - Training Epoch: 2/2, step 6544/7134 completed (loss: 0.13059675693511963, acc: 0.9729729890823364)
[2025-02-13 20:49:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:21][root][INFO] - Training Epoch: 2/2, step 6545/7134 completed (loss: 0.010271156206727028, acc: 1.0)
[2025-02-13 20:49:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:21][root][INFO] - Training Epoch: 2/2, step 6546/7134 completed (loss: 0.14789873361587524, acc: 0.97826087474823)
[2025-02-13 20:49:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:22][root][INFO] - Training Epoch: 2/2, step 6547/7134 completed (loss: 0.16074973344802856, acc: 0.9637681245803833)
[2025-02-13 20:49:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:22][root][INFO] - Training Epoch: 2/2, step 6548/7134 completed (loss: 0.04082449525594711, acc: 0.9904761910438538)
[2025-02-13 20:49:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:23][root][INFO] - Training Epoch: 2/2, step 6549/7134 completed (loss: 0.2457524687051773, acc: 0.9541984796524048)
[2025-02-13 20:49:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:23][root][INFO] - Training Epoch: 2/2, step 6550/7134 completed (loss: 0.14865745604038239, acc: 0.9426751732826233)
[2025-02-13 20:49:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:23][root][INFO] - Training Epoch: 2/2, step 6551/7134 completed (loss: 0.09549310058355331, acc: 0.9644669890403748)
[2025-02-13 20:49:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:24][root][INFO] - Training Epoch: 2/2, step 6552/7134 completed (loss: 0.15698915719985962, acc: 0.9603524208068848)
[2025-02-13 20:49:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:24][root][INFO] - Training Epoch: 2/2, step 6553/7134 completed (loss: 0.10419187694787979, acc: 0.9848484992980957)
[2025-02-13 20:49:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:24][root][INFO] - Training Epoch: 2/2, step 6554/7134 completed (loss: 0.12824125587940216, acc: 0.9766355156898499)
[2025-02-13 20:49:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:25][root][INFO] - Training Epoch: 2/2, step 6555/7134 completed (loss: 0.10381776094436646, acc: 0.9698275923728943)
[2025-02-13 20:49:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:25][root][INFO] - Training Epoch: 2/2, step 6556/7134 completed (loss: 0.11933989822864532, acc: 0.9864253401756287)
[2025-02-13 20:49:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:26][root][INFO] - Training Epoch: 2/2, step 6557/7134 completed (loss: 0.1154160425066948, acc: 0.977477490901947)
[2025-02-13 20:49:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:26][root][INFO] - Training Epoch: 2/2, step 6558/7134 completed (loss: 0.15949131548404694, acc: 0.9766355156898499)
[2025-02-13 20:49:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:26][root][INFO] - Training Epoch: 2/2, step 6559/7134 completed (loss: 0.12382856756448746, acc: 0.9638554453849792)
[2025-02-13 20:49:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:27][root][INFO] - Training Epoch: 2/2, step 6560/7134 completed (loss: 0.05599762871861458, acc: 0.9842932224273682)
[2025-02-13 20:49:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:27][root][INFO] - Training Epoch: 2/2, step 6561/7134 completed (loss: 0.06337669491767883, acc: 0.9947643876075745)
[2025-02-13 20:49:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:27][root][INFO] - Training Epoch: 2/2, step 6562/7134 completed (loss: 0.04910380393266678, acc: 0.9874213933944702)
[2025-02-13 20:49:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:28][root][INFO] - Training Epoch: 2/2, step 6563/7134 completed (loss: 0.12772998213768005, acc: 0.9696969985961914)
[2025-02-13 20:49:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:28][root][INFO] - Training Epoch: 2/2, step 6564/7134 completed (loss: 0.06986068934202194, acc: 0.9939758777618408)
[2025-02-13 20:49:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:29][root][INFO] - Training Epoch: 2/2, step 6565/7134 completed (loss: 0.1119086742401123, acc: 0.9684684872627258)
[2025-02-13 20:49:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:29][root][INFO] - Training Epoch: 2/2, step 6566/7134 completed (loss: 0.0831911712884903, acc: 0.9798387289047241)
[2025-02-13 20:49:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:30][root][INFO] - Training Epoch: 2/2, step 6567/7134 completed (loss: 0.09582968801259995, acc: 0.9866666793823242)
[2025-02-13 20:49:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:30][root][INFO] - Training Epoch: 2/2, step 6568/7134 completed (loss: 0.061507806181907654, acc: 0.9890109896659851)
[2025-02-13 20:49:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:30][root][INFO] - Training Epoch: 2/2, step 6569/7134 completed (loss: 0.062013886868953705, acc: 0.9796954393386841)
[2025-02-13 20:49:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:31][root][INFO] - Training Epoch: 2/2, step 6570/7134 completed (loss: 0.04626062884926796, acc: 0.9901477694511414)
[2025-02-13 20:49:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:31][root][INFO] - Training Epoch: 2/2, step 6571/7134 completed (loss: 0.034463949501514435, acc: 1.0)
[2025-02-13 20:49:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:31][root][INFO] - Training Epoch: 2/2, step 6572/7134 completed (loss: 0.05984882637858391, acc: 0.9853658676147461)
[2025-02-13 20:49:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:32][root][INFO] - Training Epoch: 2/2, step 6573/7134 completed (loss: 0.03491542860865593, acc: 0.9902912378311157)
[2025-02-13 20:49:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:32][root][INFO] - Training Epoch: 2/2, step 6574/7134 completed (loss: 0.0448092557489872, acc: 0.9956896305084229)
[2025-02-13 20:49:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:33][root][INFO] - Training Epoch: 2/2, step 6575/7134 completed (loss: 0.08545977622270584, acc: 0.98591548204422)
[2025-02-13 20:49:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:33][root][INFO] - Training Epoch: 2/2, step 6576/7134 completed (loss: 0.060941461473703384, acc: 0.9908257126808167)
[2025-02-13 20:49:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:33][root][INFO] - Training Epoch: 2/2, step 6577/7134 completed (loss: 0.1129254549741745, acc: 0.9757575988769531)
[2025-02-13 20:49:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:34][root][INFO] - Training Epoch: 2/2, step 6578/7134 completed (loss: 0.07776687294244766, acc: 0.9754098653793335)
[2025-02-13 20:49:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:34][root][INFO] - Training Epoch: 2/2, step 6579/7134 completed (loss: 0.23215654492378235, acc: 0.9548872113227844)
[2025-02-13 20:49:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:34][root][INFO] - Training Epoch: 2/2, step 6580/7134 completed (loss: 0.07425453513860703, acc: 0.9842105507850647)
[2025-02-13 20:49:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:35][root][INFO] - Training Epoch: 2/2, step 6581/7134 completed (loss: 0.11670400947332382, acc: 0.9783783555030823)
[2025-02-13 20:49:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:35][root][INFO] - Training Epoch: 2/2, step 6582/7134 completed (loss: 0.13028137385845184, acc: 0.9868420958518982)
[2025-02-13 20:49:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:36][root][INFO] - Training Epoch: 2/2, step 6583/7134 completed (loss: 0.12755878269672394, acc: 0.979899525642395)
[2025-02-13 20:49:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:36][root][INFO] - Training Epoch: 2/2, step 6584/7134 completed (loss: 0.0989026427268982, acc: 0.9784946441650391)
[2025-02-13 20:49:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:36][root][INFO] - Training Epoch: 2/2, step 6585/7134 completed (loss: 0.07848662883043289, acc: 0.9797297120094299)
[2025-02-13 20:49:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:37][root][INFO] - Training Epoch: 2/2, step 6586/7134 completed (loss: 0.117164246737957, acc: 0.9631901979446411)
[2025-02-13 20:49:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:37][root][INFO] - Training Epoch: 2/2, step 6587/7134 completed (loss: 0.14573897421360016, acc: 0.9793814420700073)
[2025-02-13 20:49:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:37][root][INFO] - Training Epoch: 2/2, step 6588/7134 completed (loss: 0.09937531501054764, acc: 0.9709302186965942)
[2025-02-13 20:49:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:38][root][INFO] - Training Epoch: 2/2, step 6589/7134 completed (loss: 0.11612052470445633, acc: 0.9751552939414978)
[2025-02-13 20:49:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:38][root][INFO] - Training Epoch: 2/2, step 6590/7134 completed (loss: 0.20733077824115753, acc: 0.9680851101875305)
[2025-02-13 20:49:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:39][root][INFO] - Training Epoch: 2/2, step 6591/7134 completed (loss: 0.09188731014728546, acc: 0.9707602262496948)
[2025-02-13 20:49:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:39][root][INFO] - Training Epoch: 2/2, step 6592/7134 completed (loss: 0.1113046258687973, acc: 0.9655172228813171)
[2025-02-13 20:49:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:39][root][INFO] - Training Epoch: 2/2, step 6593/7134 completed (loss: 0.1278379112482071, acc: 0.9733333587646484)
[2025-02-13 20:49:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:40][root][INFO] - Training Epoch: 2/2, step 6594/7134 completed (loss: 0.07231143862009048, acc: 0.9852216839790344)
[2025-02-13 20:49:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:40][root][INFO] - Training Epoch: 2/2, step 6595/7134 completed (loss: 0.10126519203186035, acc: 0.9814814925193787)
[2025-02-13 20:49:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:40][root][INFO] - Training Epoch: 2/2, step 6596/7134 completed (loss: 0.15334822237491608, acc: 0.9578313231468201)
[2025-02-13 20:49:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:41][root][INFO] - Training Epoch: 2/2, step 6597/7134 completed (loss: 0.07039468735456467, acc: 0.9842932224273682)
[2025-02-13 20:49:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:41][root][INFO] - Training Epoch: 2/2, step 6598/7134 completed (loss: 0.13809865713119507, acc: 0.9781420826911926)
[2025-02-13 20:49:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:42][root][INFO] - Training Epoch: 2/2, step 6599/7134 completed (loss: 0.08243583887815475, acc: 0.9756097793579102)
[2025-02-13 20:49:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:42][root][INFO] - Training Epoch: 2/2, step 6600/7134 completed (loss: 0.1368984878063202, acc: 0.949438214302063)
[2025-02-13 20:49:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:42][root][INFO] - Training Epoch: 2/2, step 6601/7134 completed (loss: 0.2618359327316284, acc: 0.9411764740943909)
[2025-02-13 20:49:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:43][root][INFO] - Training Epoch: 2/2, step 6602/7134 completed (loss: 0.08160357922315598, acc: 0.9801980257034302)
[2025-02-13 20:49:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:43][root][INFO] - Training Epoch: 2/2, step 6603/7134 completed (loss: 0.10156521946191788, acc: 0.9874213933944702)
[2025-02-13 20:49:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:43][root][INFO] - Training Epoch: 2/2, step 6604/7134 completed (loss: 0.1171569749712944, acc: 0.9677419066429138)
[2025-02-13 20:49:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:44][root][INFO] - Training Epoch: 2/2, step 6605/7134 completed (loss: 0.1666671633720398, acc: 0.9731183052062988)
[2025-02-13 20:49:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:44][root][INFO] - Training Epoch: 2/2, step 6606/7134 completed (loss: 0.08113235980272293, acc: 0.9832402467727661)
[2025-02-13 20:49:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:45][root][INFO] - Training Epoch: 2/2, step 6607/7134 completed (loss: 0.18759307265281677, acc: 0.9548386931419373)
[2025-02-13 20:49:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:45][root][INFO] - Training Epoch: 2/2, step 6608/7134 completed (loss: 0.04571832716464996, acc: 0.9863013625144958)
[2025-02-13 20:49:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:45][root][INFO] - Training Epoch: 2/2, step 6609/7134 completed (loss: 0.1113414615392685, acc: 0.9811320900917053)
[2025-02-13 20:49:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:46][root][INFO] - Training Epoch: 2/2, step 6610/7134 completed (loss: 0.13649117946624756, acc: 0.9757575988769531)
[2025-02-13 20:49:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:46][root][INFO] - Training Epoch: 2/2, step 6611/7134 completed (loss: 0.10857053101062775, acc: 0.965753436088562)
[2025-02-13 20:49:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:46][root][INFO] - Training Epoch: 2/2, step 6612/7134 completed (loss: 0.230624258518219, acc: 0.9435897469520569)
[2025-02-13 20:49:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:47][root][INFO] - Training Epoch: 2/2, step 6613/7134 completed (loss: 0.08566149324178696, acc: 0.9798657894134521)
[2025-02-13 20:49:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:47][root][INFO] - Training Epoch: 2/2, step 6614/7134 completed (loss: 0.0896189734339714, acc: 0.9945054650306702)
[2025-02-13 20:49:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:48][root][INFO] - Training Epoch: 2/2, step 6615/7134 completed (loss: 0.14153212308883667, acc: 0.9594594836235046)
[2025-02-13 20:49:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:48][root][INFO] - Training Epoch: 2/2, step 6616/7134 completed (loss: 0.07220321148633957, acc: 0.9818181991577148)
[2025-02-13 20:49:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:48][root][INFO] - Training Epoch: 2/2, step 6617/7134 completed (loss: 0.07792935520410538, acc: 0.9777777791023254)
[2025-02-13 20:49:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:49][root][INFO] - Training Epoch: 2/2, step 6618/7134 completed (loss: 0.03018921986222267, acc: 0.9927536249160767)
[2025-02-13 20:49:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:49][root][INFO] - Training Epoch: 2/2, step 6619/7134 completed (loss: 0.04874066635966301, acc: 0.9887005686759949)
[2025-02-13 20:49:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:49][root][INFO] - Training Epoch: 2/2, step 6620/7134 completed (loss: 0.09659460932016373, acc: 0.9722222089767456)
[2025-02-13 20:49:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:50][root][INFO] - Training Epoch: 2/2, step 6621/7134 completed (loss: 0.09953733533620834, acc: 0.9657142758369446)
[2025-02-13 20:49:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:50][root][INFO] - Training Epoch: 2/2, step 6622/7134 completed (loss: 0.0693470686674118, acc: 0.9807692170143127)
[2025-02-13 20:49:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:51][root][INFO] - Training Epoch: 2/2, step 6623/7134 completed (loss: 0.08048132061958313, acc: 0.9820359349250793)
[2025-02-13 20:49:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:51][root][INFO] - Training Epoch: 2/2, step 6624/7134 completed (loss: 0.04518476873636246, acc: 0.987261176109314)
[2025-02-13 20:49:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:51][root][INFO] - Training Epoch: 2/2, step 6625/7134 completed (loss: 0.039117325097322464, acc: 0.9933775067329407)
[2025-02-13 20:49:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:52][root][INFO] - Training Epoch: 2/2, step 6626/7134 completed (loss: 0.08062268048524857, acc: 0.9828571677207947)
[2025-02-13 20:49:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:52][root][INFO] - Training Epoch: 2/2, step 6627/7134 completed (loss: 0.050821416079998016, acc: 0.9892473220825195)
[2025-02-13 20:49:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:52][root][INFO] - Training Epoch: 2/2, step 6628/7134 completed (loss: 0.08520623296499252, acc: 0.9821428656578064)
[2025-02-13 20:49:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:53][root][INFO] - Training Epoch: 2/2, step 6629/7134 completed (loss: 0.15433482825756073, acc: 0.9750000238418579)
[2025-02-13 20:49:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:53][root][INFO] - Training Epoch: 2/2, step 6630/7134 completed (loss: 0.08610903471708298, acc: 0.9791666865348816)
[2025-02-13 20:49:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:53][root][INFO] - Training Epoch: 2/2, step 6631/7134 completed (loss: 0.08142033219337463, acc: 0.9689440727233887)
[2025-02-13 20:49:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:54][root][INFO] - Training Epoch: 2/2, step 6632/7134 completed (loss: 0.054991722106933594, acc: 0.9949238300323486)
[2025-02-13 20:49:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:54][root][INFO] - Training Epoch: 2/2, step 6633/7134 completed (loss: 0.09642769396305084, acc: 0.9835164546966553)
[2025-02-13 20:49:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:55][root][INFO] - Training Epoch: 2/2, step 6634/7134 completed (loss: 0.09099508821964264, acc: 0.969924807548523)
[2025-02-13 20:49:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:55][root][INFO] - Training Epoch: 2/2, step 6635/7134 completed (loss: 0.2660103738307953, acc: 0.9399999976158142)
[2025-02-13 20:49:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:55][root][INFO] - Training Epoch: 2/2, step 6636/7134 completed (loss: 0.16242942214012146, acc: 0.9560975432395935)
[2025-02-13 20:49:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:56][root][INFO] - Training Epoch: 2/2, step 6637/7134 completed (loss: 0.07624133676290512, acc: 0.981249988079071)
[2025-02-13 20:49:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:56][root][INFO] - Training Epoch: 2/2, step 6638/7134 completed (loss: 0.09507955610752106, acc: 0.9780219793319702)
[2025-02-13 20:49:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:56][root][INFO] - Training Epoch: 2/2, step 6639/7134 completed (loss: 0.11675642430782318, acc: 0.9580838084220886)
[2025-02-13 20:49:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:57][root][INFO] - Training Epoch: 2/2, step 6640/7134 completed (loss: 0.0438818484544754, acc: 0.987261176109314)
[2025-02-13 20:49:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:57][root][INFO] - Training Epoch: 2/2, step 6641/7134 completed (loss: 0.1947934776544571, acc: 0.9601770043373108)
[2025-02-13 20:49:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:58][root][INFO] - Training Epoch: 2/2, step 6642/7134 completed (loss: 0.0992959588766098, acc: 0.9759036302566528)
[2025-02-13 20:49:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:58][root][INFO] - Training Epoch: 2/2, step 6643/7134 completed (loss: 0.11038463562726974, acc: 0.9677419066429138)
[2025-02-13 20:49:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:58][root][INFO] - Training Epoch: 2/2, step 6644/7134 completed (loss: 0.13489973545074463, acc: 0.9668049812316895)
[2025-02-13 20:49:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:59][root][INFO] - Training Epoch: 2/2, step 6645/7134 completed (loss: 0.07957394421100616, acc: 0.9788359999656677)
[2025-02-13 20:49:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:49:59][root][INFO] - Training Epoch: 2/2, step 6646/7134 completed (loss: 0.14359958469867706, acc: 0.9694322943687439)
[2025-02-13 20:49:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:00][root][INFO] - Training Epoch: 2/2, step 6647/7134 completed (loss: 0.04769891873002052, acc: 0.9941176176071167)
[2025-02-13 20:50:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:00][root][INFO] - Training Epoch: 2/2, step 6648/7134 completed (loss: 0.09832044690847397, acc: 0.9806451797485352)
[2025-02-13 20:50:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:00][root][INFO] - Training Epoch: 2/2, step 6649/7134 completed (loss: 0.10706087946891785, acc: 0.9710144996643066)
[2025-02-13 20:50:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:01][root][INFO] - Training Epoch: 2/2, step 6650/7134 completed (loss: 0.06205297261476517, acc: 0.9863636493682861)
[2025-02-13 20:50:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:01][root][INFO] - Training Epoch: 2/2, step 6651/7134 completed (loss: 0.12304268032312393, acc: 0.9715909361839294)
[2025-02-13 20:50:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:01][root][INFO] - Training Epoch: 2/2, step 6652/7134 completed (loss: 0.18421979248523712, acc: 0.970588207244873)
[2025-02-13 20:50:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:02][root][INFO] - Training Epoch: 2/2, step 6653/7134 completed (loss: 0.18744809925556183, acc: 0.9363636374473572)
[2025-02-13 20:50:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:02][root][INFO] - Training Epoch: 2/2, step 6654/7134 completed (loss: 0.13364535570144653, acc: 0.9683544039726257)
[2025-02-13 20:50:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:03][root][INFO] - Training Epoch: 2/2, step 6655/7134 completed (loss: 0.08014523983001709, acc: 0.961904764175415)
[2025-02-13 20:50:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:03][root][INFO] - Training Epoch: 2/2, step 6656/7134 completed (loss: 0.10634811967611313, acc: 0.9578947424888611)
[2025-02-13 20:50:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:03][root][INFO] - Training Epoch: 2/2, step 6657/7134 completed (loss: 0.14477548003196716, acc: 0.9627659320831299)
[2025-02-13 20:50:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:04][root][INFO] - Training Epoch: 2/2, step 6658/7134 completed (loss: 0.06414289772510529, acc: 0.9847715497016907)
[2025-02-13 20:50:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:04][root][INFO] - Training Epoch: 2/2, step 6659/7134 completed (loss: 0.13459248840808868, acc: 0.9836065769195557)
[2025-02-13 20:50:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:04][root][INFO] - Training Epoch: 2/2, step 6660/7134 completed (loss: 0.08780180662870407, acc: 0.9754098653793335)
[2025-02-13 20:50:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:05][root][INFO] - Training Epoch: 2/2, step 6661/7134 completed (loss: 0.09928308427333832, acc: 0.9800994992256165)
[2025-02-13 20:50:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:05][root][INFO] - Training Epoch: 2/2, step 6662/7134 completed (loss: 0.20630377531051636, acc: 0.9424460530281067)
[2025-02-13 20:50:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:06][root][INFO] - Training Epoch: 2/2, step 6663/7134 completed (loss: 0.09471062570810318, acc: 0.9783549904823303)
[2025-02-13 20:50:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:06][root][INFO] - Training Epoch: 2/2, step 6664/7134 completed (loss: 0.04564020782709122, acc: 0.9928571581840515)
[2025-02-13 20:50:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:06][root][INFO] - Training Epoch: 2/2, step 6665/7134 completed (loss: 0.060898877680301666, acc: 1.0)
[2025-02-13 20:50:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:07][root][INFO] - Training Epoch: 2/2, step 6666/7134 completed (loss: 0.059931956231594086, acc: 0.9818181991577148)
[2025-02-13 20:50:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:07][root][INFO] - Training Epoch: 2/2, step 6667/7134 completed (loss: 0.17034272849559784, acc: 0.9387755393981934)
[2025-02-13 20:50:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:08][root][INFO] - Training Epoch: 2/2, step 6668/7134 completed (loss: 0.02835271693766117, acc: 0.9927007555961609)
[2025-02-13 20:50:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:08][root][INFO] - Training Epoch: 2/2, step 6669/7134 completed (loss: 0.03166768699884415, acc: 0.9930555820465088)
[2025-02-13 20:50:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:08][root][INFO] - Training Epoch: 2/2, step 6670/7134 completed (loss: 0.06614513695240021, acc: 0.9856114983558655)
[2025-02-13 20:50:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:09][root][INFO] - Training Epoch: 2/2, step 6671/7134 completed (loss: 0.03172624483704567, acc: 0.9878048896789551)
[2025-02-13 20:50:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:09][root][INFO] - Training Epoch: 2/2, step 6672/7134 completed (loss: 0.03822290524840355, acc: 0.9919999837875366)
[2025-02-13 20:50:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:09][root][INFO] - Training Epoch: 2/2, step 6673/7134 completed (loss: 0.06418716907501221, acc: 0.9811320900917053)
[2025-02-13 20:50:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:10][root][INFO] - Training Epoch: 2/2, step 6674/7134 completed (loss: 0.026174182072281837, acc: 0.991525411605835)
[2025-02-13 20:50:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:10][root][INFO] - Training Epoch: 2/2, step 6675/7134 completed (loss: 0.06403100490570068, acc: 0.9901960492134094)
[2025-02-13 20:50:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:10][root][INFO] - Training Epoch: 2/2, step 6676/7134 completed (loss: 0.06540826708078384, acc: 0.9900000095367432)
[2025-02-13 20:50:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:11][root][INFO] - Training Epoch: 2/2, step 6677/7134 completed (loss: 0.09232723712921143, acc: 0.9847328066825867)
[2025-02-13 20:50:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:11][root][INFO] - Training Epoch: 2/2, step 6678/7134 completed (loss: 0.04102544113993645, acc: 0.9925925731658936)
[2025-02-13 20:50:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:12][root][INFO] - Training Epoch: 2/2, step 6679/7134 completed (loss: 0.057146232575178146, acc: 0.982300877571106)
[2025-02-13 20:50:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:12][root][INFO] - Training Epoch: 2/2, step 6680/7134 completed (loss: 0.010856868699193, acc: 1.0)
[2025-02-13 20:50:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:12][root][INFO] - Training Epoch: 2/2, step 6681/7134 completed (loss: 0.023214226588606834, acc: 0.9921875)
[2025-02-13 20:50:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:13][root][INFO] - Training Epoch: 2/2, step 6682/7134 completed (loss: 0.12997189164161682, acc: 0.9708737730979919)
[2025-02-13 20:50:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:13][root][INFO] - Training Epoch: 2/2, step 6683/7134 completed (loss: 0.08100856840610504, acc: 0.9885057210922241)
[2025-02-13 20:50:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:13][root][INFO] - Training Epoch: 2/2, step 6684/7134 completed (loss: 0.1562570035457611, acc: 0.9685039520263672)
[2025-02-13 20:50:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:14][root][INFO] - Training Epoch: 2/2, step 6685/7134 completed (loss: 0.061434973031282425, acc: 0.9760000109672546)
[2025-02-13 20:50:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:14][root][INFO] - Training Epoch: 2/2, step 6686/7134 completed (loss: 0.11366136372089386, acc: 0.982758641242981)
[2025-02-13 20:50:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:15][root][INFO] - Training Epoch: 2/2, step 6687/7134 completed (loss: 0.05904659628868103, acc: 0.9883720874786377)
[2025-02-13 20:50:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:15][root][INFO] - Training Epoch: 2/2, step 6688/7134 completed (loss: 0.08902063965797424, acc: 0.971222996711731)
[2025-02-13 20:50:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:15][root][INFO] - Training Epoch: 2/2, step 6689/7134 completed (loss: 0.05124200880527496, acc: 0.9907407164573669)
[2025-02-13 20:50:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:16][root][INFO] - Training Epoch: 2/2, step 6690/7134 completed (loss: 0.12398146837949753, acc: 0.9803921580314636)
[2025-02-13 20:50:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:16][root][INFO] - Training Epoch: 2/2, step 6691/7134 completed (loss: 0.07284720242023468, acc: 0.9791666865348816)
[2025-02-13 20:50:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:16][root][INFO] - Training Epoch: 2/2, step 6692/7134 completed (loss: 0.23608779907226562, acc: 0.9439252614974976)
[2025-02-13 20:50:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:17][root][INFO] - Training Epoch: 2/2, step 6693/7134 completed (loss: 0.09006098657846451, acc: 0.970370352268219)
[2025-02-13 20:50:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:17][root][INFO] - Training Epoch: 2/2, step 6694/7134 completed (loss: 0.050564397126436234, acc: 0.9933775067329407)
[2025-02-13 20:50:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:17][root][INFO] - Training Epoch: 2/2, step 6695/7134 completed (loss: 0.07096158713102341, acc: 0.9829545617103577)
[2025-02-13 20:50:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:18][root][INFO] - Training Epoch: 2/2, step 6696/7134 completed (loss: 0.11143706738948822, acc: 0.9791666865348816)
[2025-02-13 20:50:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:18][root][INFO] - Training Epoch: 2/2, step 6697/7134 completed (loss: 0.20026734471321106, acc: 0.949999988079071)
[2025-02-13 20:50:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:19][root][INFO] - Training Epoch: 2/2, step 6698/7134 completed (loss: 0.061917662620544434, acc: 0.9865771532058716)
[2025-02-13 20:50:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:19][root][INFO] - Training Epoch: 2/2, step 6699/7134 completed (loss: 0.079111747443676, acc: 0.9904761910438538)
[2025-02-13 20:50:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:19][root][INFO] - Training Epoch: 2/2, step 6700/7134 completed (loss: 0.15023621916770935, acc: 0.9481481313705444)
[2025-02-13 20:50:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:20][root][INFO] - Training Epoch: 2/2, step 6701/7134 completed (loss: 0.4148726761341095, acc: 0.9179104566574097)
[2025-02-13 20:50:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:20][root][INFO] - Training Epoch: 2/2, step 6702/7134 completed (loss: 0.1070617064833641, acc: 0.9652174115180969)
[2025-02-13 20:50:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:20][root][INFO] - Training Epoch: 2/2, step 6703/7134 completed (loss: 0.15928490459918976, acc: 0.95652174949646)
[2025-02-13 20:50:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:21][root][INFO] - Training Epoch: 2/2, step 6704/7134 completed (loss: 0.1695100963115692, acc: 0.9482758641242981)
[2025-02-13 20:50:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:21][root][INFO] - Training Epoch: 2/2, step 6705/7134 completed (loss: 0.04753657802939415, acc: 1.0)
[2025-02-13 20:50:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:22][root][INFO] - Training Epoch: 2/2, step 6706/7134 completed (loss: 0.06086207181215286, acc: 0.9909090995788574)
[2025-02-13 20:50:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:22][root][INFO] - Training Epoch: 2/2, step 6707/7134 completed (loss: 0.11898620426654816, acc: 0.9743589758872986)
[2025-02-13 20:50:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:22][root][INFO] - Training Epoch: 2/2, step 6708/7134 completed (loss: 0.06545504927635193, acc: 0.9847328066825867)
[2025-02-13 20:50:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:23][root][INFO] - Training Epoch: 2/2, step 6709/7134 completed (loss: 0.13236887753009796, acc: 0.9756097793579102)
[2025-02-13 20:50:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:23][root][INFO] - Training Epoch: 2/2, step 6710/7134 completed (loss: 0.17299818992614746, acc: 0.9774436354637146)
[2025-02-13 20:50:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:24][root][INFO] - Training Epoch: 2/2, step 6711/7134 completed (loss: 0.06872795522212982, acc: 0.9810126423835754)
[2025-02-13 20:50:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:24][root][INFO] - Training Epoch: 2/2, step 6712/7134 completed (loss: 0.15338951349258423, acc: 0.9655172228813171)
[2025-02-13 20:50:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:24][root][INFO] - Training Epoch: 2/2, step 6713/7134 completed (loss: 0.17475856840610504, acc: 0.9386503100395203)
[2025-02-13 20:50:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:25][root][INFO] - Training Epoch: 2/2, step 6714/7134 completed (loss: 0.1550201177597046, acc: 0.9537572264671326)
[2025-02-13 20:50:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:25][root][INFO] - Training Epoch: 2/2, step 6715/7134 completed (loss: 0.19112206995487213, acc: 0.9527027010917664)
[2025-02-13 20:50:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:25][root][INFO] - Training Epoch: 2/2, step 6716/7134 completed (loss: 0.1130247488617897, acc: 0.965753436088562)
[2025-02-13 20:50:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:26][root][INFO] - Training Epoch: 2/2, step 6717/7134 completed (loss: 0.0875643715262413, acc: 0.9797297120094299)
[2025-02-13 20:50:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:26][root][INFO] - Training Epoch: 2/2, step 6718/7134 completed (loss: 0.14383193850517273, acc: 0.9724137783050537)
[2025-02-13 20:50:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:27][root][INFO] - Training Epoch: 2/2, step 6719/7134 completed (loss: 0.04765027388930321, acc: 1.0)
[2025-02-13 20:50:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:27][root][INFO] - Training Epoch: 2/2, step 6720/7134 completed (loss: 0.20892123878002167, acc: 0.959770143032074)
[2025-02-13 20:50:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:27][root][INFO] - Training Epoch: 2/2, step 6721/7134 completed (loss: 0.1706438511610031, acc: 0.9681528806686401)
[2025-02-13 20:50:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:28][root][INFO] - Training Epoch: 2/2, step 6722/7134 completed (loss: 0.18365982174873352, acc: 0.9370629191398621)
[2025-02-13 20:50:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:28][root][INFO] - Training Epoch: 2/2, step 6723/7134 completed (loss: 0.13011996448040009, acc: 0.9726775884628296)
[2025-02-13 20:50:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:28][root][INFO] - Training Epoch: 2/2, step 6724/7134 completed (loss: 0.1599213033914566, acc: 0.9685534834861755)
[2025-02-13 20:50:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:29][root][INFO] - Training Epoch: 2/2, step 6725/7134 completed (loss: 0.12774428725242615, acc: 0.9736841917037964)
[2025-02-13 20:50:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:29][root][INFO] - Training Epoch: 2/2, step 6726/7134 completed (loss: 0.23190200328826904, acc: 0.9586777091026306)
[2025-02-13 20:50:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:29][root][INFO] - Training Epoch: 2/2, step 6727/7134 completed (loss: 0.2002192884683609, acc: 0.9568345546722412)
[2025-02-13 20:50:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:30][root][INFO] - Training Epoch: 2/2, step 6728/7134 completed (loss: 0.14625205099582672, acc: 0.948387086391449)
[2025-02-13 20:50:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:30][root][INFO] - Training Epoch: 2/2, step 6729/7134 completed (loss: 0.09217517822980881, acc: 0.9873417615890503)
[2025-02-13 20:50:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:31][root][INFO] - Training Epoch: 2/2, step 6730/7134 completed (loss: 0.19149385392665863, acc: 0.9482758641242981)
[2025-02-13 20:50:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:31][root][INFO] - Training Epoch: 2/2, step 6731/7134 completed (loss: 0.09538928419351578, acc: 0.9759036302566528)
[2025-02-13 20:50:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:31][root][INFO] - Training Epoch: 2/2, step 6732/7134 completed (loss: 0.1861155778169632, acc: 0.9807692170143127)
[2025-02-13 20:50:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:32][root][INFO] - Training Epoch: 2/2, step 6733/7134 completed (loss: 0.1320357322692871, acc: 0.9856114983558655)
[2025-02-13 20:50:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:32][root][INFO] - Training Epoch: 2/2, step 6734/7134 completed (loss: 0.20512202382087708, acc: 0.9510489702224731)
[2025-02-13 20:50:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:32][root][INFO] - Training Epoch: 2/2, step 6735/7134 completed (loss: 0.13597321510314941, acc: 0.9679487347602844)
[2025-02-13 20:50:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:33][root][INFO] - Training Epoch: 2/2, step 6736/7134 completed (loss: 0.17192651331424713, acc: 0.9611111283302307)
[2025-02-13 20:50:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:33][root][INFO] - Training Epoch: 2/2, step 6737/7134 completed (loss: 0.17853350937366486, acc: 0.9536423683166504)
[2025-02-13 20:50:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:34][root][INFO] - Training Epoch: 2/2, step 6738/7134 completed (loss: 0.08048394322395325, acc: 0.9852941036224365)
[2025-02-13 20:50:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:34][root][INFO] - Training Epoch: 2/2, step 6739/7134 completed (loss: 0.1287335753440857, acc: 0.9857142567634583)
[2025-02-13 20:50:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:34][root][INFO] - Training Epoch: 2/2, step 6740/7134 completed (loss: 0.1632472276687622, acc: 0.9594594836235046)
[2025-02-13 20:50:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:35][root][INFO] - Training Epoch: 2/2, step 6741/7134 completed (loss: 0.0689607635140419, acc: 0.9819276928901672)
[2025-02-13 20:50:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:35][root][INFO] - Training Epoch: 2/2, step 6742/7134 completed (loss: 0.02820379100739956, acc: 1.0)
[2025-02-13 20:50:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:35][root][INFO] - Training Epoch: 2/2, step 6743/7134 completed (loss: 0.03942728415131569, acc: 0.994350254535675)
[2025-02-13 20:50:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:36][root][INFO] - Training Epoch: 2/2, step 6744/7134 completed (loss: 0.03683391585946083, acc: 0.9932432174682617)
[2025-02-13 20:50:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:36][root][INFO] - Training Epoch: 2/2, step 6745/7134 completed (loss: 0.022775858640670776, acc: 1.0)
[2025-02-13 20:50:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:36][root][INFO] - Training Epoch: 2/2, step 6746/7134 completed (loss: 0.021028103306889534, acc: 1.0)
[2025-02-13 20:50:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:37][root][INFO] - Training Epoch: 2/2, step 6747/7134 completed (loss: 0.08664415031671524, acc: 0.9790576100349426)
[2025-02-13 20:50:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:37][root][INFO] - Training Epoch: 2/2, step 6748/7134 completed (loss: 0.07490276545286179, acc: 0.9901960492134094)
[2025-02-13 20:50:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:38][root][INFO] - Training Epoch: 2/2, step 6749/7134 completed (loss: 0.06913826614618301, acc: 0.9777777791023254)
[2025-02-13 20:50:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:38][root][INFO] - Training Epoch: 2/2, step 6750/7134 completed (loss: 0.06029482185840607, acc: 0.9950494766235352)
[2025-02-13 20:50:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:38][root][INFO] - Training Epoch: 2/2, step 6751/7134 completed (loss: 0.04962042346596718, acc: 0.9850746393203735)
[2025-02-13 20:50:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:39][root][INFO] - Training Epoch: 2/2, step 6752/7134 completed (loss: 0.06377052515745163, acc: 0.9815950989723206)
[2025-02-13 20:50:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:39][root][INFO] - Training Epoch: 2/2, step 6753/7134 completed (loss: 0.02628270536661148, acc: 0.9898989796638489)
[2025-02-13 20:50:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:40][root][INFO] - Training Epoch: 2/2, step 6754/7134 completed (loss: 0.040307942777872086, acc: 0.9947643876075745)
[2025-02-13 20:50:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:40][root][INFO] - Training Epoch: 2/2, step 6755/7134 completed (loss: 0.07313300669193268, acc: 0.9790576100349426)
[2025-02-13 20:50:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:40][root][INFO] - Training Epoch: 2/2, step 6756/7134 completed (loss: 0.020106321200728416, acc: 1.0)
[2025-02-13 20:50:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:41][root][INFO] - Training Epoch: 2/2, step 6757/7134 completed (loss: 0.02084384486079216, acc: 1.0)
[2025-02-13 20:50:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:41][root][INFO] - Training Epoch: 2/2, step 6758/7134 completed (loss: 0.04656747356057167, acc: 0.9948186278343201)
[2025-02-13 20:50:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:41][root][INFO] - Training Epoch: 2/2, step 6759/7134 completed (loss: 0.05178030580282211, acc: 0.9910714030265808)
[2025-02-13 20:50:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:42][root][INFO] - Training Epoch: 2/2, step 6760/7134 completed (loss: 0.05714087188243866, acc: 0.9893617033958435)
[2025-02-13 20:50:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:42][root][INFO] - Training Epoch: 2/2, step 6761/7134 completed (loss: 0.0654325857758522, acc: 0.9857142567634583)
[2025-02-13 20:50:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:43][root][INFO] - Training Epoch: 2/2, step 6762/7134 completed (loss: 0.034973494708538055, acc: 0.9950000047683716)
[2025-02-13 20:50:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:43][root][INFO] - Training Epoch: 2/2, step 6763/7134 completed (loss: 0.029068347066640854, acc: 1.0)
[2025-02-13 20:50:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:43][root][INFO] - Training Epoch: 2/2, step 6764/7134 completed (loss: 0.037370551377534866, acc: 0.9941176176071167)
[2025-02-13 20:50:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:44][root][INFO] - Training Epoch: 2/2, step 6765/7134 completed (loss: 0.04121784493327141, acc: 0.9890109896659851)
[2025-02-13 20:50:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:44][root][INFO] - Training Epoch: 2/2, step 6766/7134 completed (loss: 0.05652240291237831, acc: 0.9942857027053833)
[2025-02-13 20:50:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:44][root][INFO] - Training Epoch: 2/2, step 6767/7134 completed (loss: 0.06595895439386368, acc: 0.9732142686843872)
[2025-02-13 20:50:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:45][root][INFO] - Training Epoch: 2/2, step 6768/7134 completed (loss: 0.025203710421919823, acc: 1.0)
[2025-02-13 20:50:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:45][root][INFO] - Training Epoch: 2/2, step 6769/7134 completed (loss: 0.06120606139302254, acc: 0.9935897588729858)
[2025-02-13 20:50:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:46][root][INFO] - Training Epoch: 2/2, step 6770/7134 completed (loss: 0.0422164648771286, acc: 0.9947368502616882)
[2025-02-13 20:50:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:46][root][INFO] - Training Epoch: 2/2, step 6771/7134 completed (loss: 0.05433472990989685, acc: 0.988304078578949)
[2025-02-13 20:50:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:46][root][INFO] - Training Epoch: 2/2, step 6772/7134 completed (loss: 0.22265203297138214, acc: 0.9440559148788452)
[2025-02-13 20:50:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:47][root][INFO] - Training Epoch: 2/2, step 6773/7134 completed (loss: 0.08735478669404984, acc: 0.9753086566925049)
[2025-02-13 20:50:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:47][root][INFO] - Training Epoch: 2/2, step 6774/7134 completed (loss: 0.06869444996118546, acc: 0.9881656765937805)
[2025-02-13 20:50:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:47][root][INFO] - Training Epoch: 2/2, step 6775/7134 completed (loss: 0.05302513763308525, acc: 0.9931034445762634)
[2025-02-13 20:50:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:48][root][INFO] - Training Epoch: 2/2, step 6776/7134 completed (loss: 0.1540689915418625, acc: 0.984000027179718)
[2025-02-13 20:50:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:48][root][INFO] - Training Epoch: 2/2, step 6777/7134 completed (loss: 0.12689051032066345, acc: 0.9885714054107666)
[2025-02-13 20:50:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:49][root][INFO] - Training Epoch: 2/2, step 6778/7134 completed (loss: 0.04361845552921295, acc: 0.9829545617103577)
[2025-02-13 20:50:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:49][root][INFO] - Training Epoch: 2/2, step 6779/7134 completed (loss: 0.08614351600408554, acc: 0.988095223903656)
[2025-02-13 20:50:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:49][root][INFO] - Training Epoch: 2/2, step 6780/7134 completed (loss: 0.03301754593849182, acc: 0.9940119981765747)
[2025-02-13 20:50:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:50][root][INFO] - Training Epoch: 2/2, step 6781/7134 completed (loss: 0.029664376750588417, acc: 0.9941520690917969)
[2025-02-13 20:50:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:50][root][INFO] - Training Epoch: 2/2, step 6782/7134 completed (loss: 0.018445398658514023, acc: 0.9935483932495117)
[2025-02-13 20:50:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:50][root][INFO] - Training Epoch: 2/2, step 6783/7134 completed (loss: 0.06964908540248871, acc: 0.9862068891525269)
[2025-02-13 20:50:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:51][root][INFO] - Training Epoch: 2/2, step 6784/7134 completed (loss: 0.025215310975909233, acc: 0.9936708807945251)
[2025-02-13 20:50:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:51][root][INFO] - Training Epoch: 2/2, step 6785/7134 completed (loss: 0.028181025758385658, acc: 0.9934640526771545)
[2025-02-13 20:50:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:52][root][INFO] - Training Epoch: 2/2, step 6786/7134 completed (loss: 0.01681661792099476, acc: 1.0)
[2025-02-13 20:50:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:52][root][INFO] - Training Epoch: 2/2, step 6787/7134 completed (loss: 0.09323624521493912, acc: 0.9726775884628296)
[2025-02-13 20:50:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:52][root][INFO] - Training Epoch: 2/2, step 6788/7134 completed (loss: 0.04275035858154297, acc: 0.9928057789802551)
[2025-02-13 20:50:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:53][root][INFO] - Training Epoch: 2/2, step 6789/7134 completed (loss: 0.03797672316431999, acc: 0.978723406791687)
[2025-02-13 20:50:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:53][root][INFO] - Training Epoch: 2/2, step 6790/7134 completed (loss: 0.13844533264636993, acc: 0.9659090638160706)
[2025-02-13 20:50:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:53][root][INFO] - Training Epoch: 2/2, step 6791/7134 completed (loss: 0.04621148481965065, acc: 0.9871794581413269)
[2025-02-13 20:50:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:54][root][INFO] - Training Epoch: 2/2, step 6792/7134 completed (loss: 0.013542547821998596, acc: 1.0)
[2025-02-13 20:50:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:54][root][INFO] - Training Epoch: 2/2, step 6793/7134 completed (loss: 0.01228408981114626, acc: 1.0)
[2025-02-13 20:50:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:55][root][INFO] - Training Epoch: 2/2, step 6794/7134 completed (loss: 0.07677243649959564, acc: 0.9931972622871399)
[2025-02-13 20:50:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:55][root][INFO] - Training Epoch: 2/2, step 6795/7134 completed (loss: 0.040562890470027924, acc: 0.9942857027053833)
[2025-02-13 20:50:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:55][root][INFO] - Training Epoch: 2/2, step 6796/7134 completed (loss: 0.01288575865328312, acc: 1.0)
[2025-02-13 20:50:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:56][root][INFO] - Training Epoch: 2/2, step 6797/7134 completed (loss: 0.08869428932666779, acc: 0.9756097793579102)
[2025-02-13 20:50:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:56][root][INFO] - Training Epoch: 2/2, step 6798/7134 completed (loss: 0.04218670725822449, acc: 0.9939758777618408)
[2025-02-13 20:50:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:56][root][INFO] - Training Epoch: 2/2, step 6799/7134 completed (loss: 0.03899418190121651, acc: 0.9880239367485046)
[2025-02-13 20:50:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:57][root][INFO] - Training Epoch: 2/2, step 6800/7134 completed (loss: 0.04561937600374222, acc: 1.0)
[2025-02-13 20:50:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:57][root][INFO] - Training Epoch: 2/2, step 6801/7134 completed (loss: 0.06255850195884705, acc: 0.9784172773361206)
[2025-02-13 20:50:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:58][root][INFO] - Training Epoch: 2/2, step 6802/7134 completed (loss: 0.04403858631849289, acc: 0.9930070042610168)
[2025-02-13 20:50:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:58][root][INFO] - Training Epoch: 2/2, step 6803/7134 completed (loss: 0.059811387211084366, acc: 0.9888888597488403)
[2025-02-13 20:50:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:58][root][INFO] - Training Epoch: 2/2, step 6804/7134 completed (loss: 0.0825747475028038, acc: 0.9865771532058716)
[2025-02-13 20:50:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:59][root][INFO] - Training Epoch: 2/2, step 6805/7134 completed (loss: 0.09424630552530289, acc: 0.9777777791023254)
[2025-02-13 20:50:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:59][root][INFO] - Training Epoch: 2/2, step 6806/7134 completed (loss: 0.07452943921089172, acc: 0.9887640476226807)
[2025-02-13 20:50:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:50:59][root][INFO] - Training Epoch: 2/2, step 6807/7134 completed (loss: 0.04711049050092697, acc: 0.981249988079071)
[2025-02-13 20:51:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:00][root][INFO] - Training Epoch: 2/2, step 6808/7134 completed (loss: 0.06651653349399567, acc: 0.9865771532058716)
[2025-02-13 20:51:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:00][root][INFO] - Training Epoch: 2/2, step 6809/7134 completed (loss: 0.07998314499855042, acc: 0.9807692170143127)
[2025-02-13 20:51:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:01][root][INFO] - Training Epoch: 2/2, step 6810/7134 completed (loss: 0.03474125266075134, acc: 1.0)
[2025-02-13 20:51:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:01][root][INFO] - Training Epoch: 2/2, step 6811/7134 completed (loss: 0.07317778468132019, acc: 0.9825581312179565)
[2025-02-13 20:51:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:01][root][INFO] - Training Epoch: 2/2, step 6812/7134 completed (loss: 0.032845258712768555, acc: 1.0)
[2025-02-13 20:51:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:02][root][INFO] - Training Epoch: 2/2, step 6813/7134 completed (loss: 0.03441287949681282, acc: 1.0)
[2025-02-13 20:51:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:02][root][INFO] - Training Epoch: 2/2, step 6814/7134 completed (loss: 0.02991071157157421, acc: 0.9942857027053833)
[2025-02-13 20:51:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:02][root][INFO] - Training Epoch: 2/2, step 6815/7134 completed (loss: 0.054419826716184616, acc: 0.9823529124259949)
[2025-02-13 20:51:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:03][root][INFO] - Training Epoch: 2/2, step 6816/7134 completed (loss: 0.03299981355667114, acc: 0.9930555820465088)
[2025-02-13 20:51:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:03][root][INFO] - Training Epoch: 2/2, step 6817/7134 completed (loss: 0.1586909145116806, acc: 0.9577465057373047)
[2025-02-13 20:51:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:04][root][INFO] - Training Epoch: 2/2, step 6818/7134 completed (loss: 0.05545925721526146, acc: 0.9931972622871399)
[2025-02-13 20:51:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:04][root][INFO] - Training Epoch: 2/2, step 6819/7134 completed (loss: 0.08441544324159622, acc: 0.9781022071838379)
[2025-02-13 20:51:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:04][root][INFO] - Training Epoch: 2/2, step 6820/7134 completed (loss: 0.07001426815986633, acc: 0.970802903175354)
[2025-02-13 20:51:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:05][root][INFO] - Training Epoch: 2/2, step 6821/7134 completed (loss: 0.015436974354088306, acc: 1.0)
[2025-02-13 20:51:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:05][root][INFO] - Training Epoch: 2/2, step 6822/7134 completed (loss: 0.17726415395736694, acc: 0.9457364082336426)
[2025-02-13 20:51:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:06][root][INFO] - Training Epoch: 2/2, step 6823/7134 completed (loss: 0.06643738597631454, acc: 0.9879518151283264)
[2025-02-13 20:51:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:06][root][INFO] - Training Epoch: 2/2, step 6824/7134 completed (loss: 0.15416738390922546, acc: 0.9593495726585388)
[2025-02-13 20:51:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:06][root][INFO] - Training Epoch: 2/2, step 6825/7134 completed (loss: 0.04692930355668068, acc: 1.0)
[2025-02-13 20:51:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:07][root][INFO] - Training Epoch: 2/2, step 6826/7134 completed (loss: 0.10722561180591583, acc: 0.9855072498321533)
[2025-02-13 20:51:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:07][root][INFO] - Training Epoch: 2/2, step 6827/7134 completed (loss: 0.23741203546524048, acc: 0.9479166865348816)
[2025-02-13 20:51:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:07][root][INFO] - Training Epoch: 2/2, step 6828/7134 completed (loss: 0.18896286189556122, acc: 0.9577465057373047)
[2025-02-13 20:51:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:08][root][INFO] - Training Epoch: 2/2, step 6829/7134 completed (loss: 0.1309349089860916, acc: 0.9644970297813416)
[2025-02-13 20:51:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:08][root][INFO] - Training Epoch: 2/2, step 6830/7134 completed (loss: 0.09483001381158829, acc: 0.9797297120094299)
[2025-02-13 20:51:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:09][root][INFO] - Training Epoch: 2/2, step 6831/7134 completed (loss: 0.07994090020656586, acc: 0.9664804339408875)
[2025-02-13 20:51:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:09][root][INFO] - Training Epoch: 2/2, step 6832/7134 completed (loss: 0.06486049294471741, acc: 0.9861111044883728)
[2025-02-13 20:51:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:09][root][INFO] - Training Epoch: 2/2, step 6833/7134 completed (loss: 0.12474612891674042, acc: 0.9617834687232971)
[2025-02-13 20:51:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:10][root][INFO] - Training Epoch: 2/2, step 6834/7134 completed (loss: 0.09711937606334686, acc: 0.9731543660163879)
[2025-02-13 20:51:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:10][root][INFO] - Training Epoch: 2/2, step 6835/7134 completed (loss: 0.04061928763985634, acc: 1.0)
[2025-02-13 20:51:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:10][root][INFO] - Training Epoch: 2/2, step 6836/7134 completed (loss: 0.0811987817287445, acc: 0.9857819676399231)
[2025-02-13 20:51:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:11][root][INFO] - Training Epoch: 2/2, step 6837/7134 completed (loss: 0.10904271900653839, acc: 0.9848484992980957)
[2025-02-13 20:51:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:11][root][INFO] - Training Epoch: 2/2, step 6838/7134 completed (loss: 0.13809379935264587, acc: 0.9698795080184937)
[2025-02-13 20:51:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:12][root][INFO] - Training Epoch: 2/2, step 6839/7134 completed (loss: 0.1175321415066719, acc: 0.9733333587646484)
[2025-02-13 20:51:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:12][root][INFO] - Training Epoch: 2/2, step 6840/7134 completed (loss: 0.14281460642814636, acc: 0.9617834687232971)
[2025-02-13 20:51:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:12][root][INFO] - Training Epoch: 2/2, step 6841/7134 completed (loss: 0.041917312890291214, acc: 0.9939024448394775)
[2025-02-13 20:51:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:13][root][INFO] - Training Epoch: 2/2, step 6842/7134 completed (loss: 0.089060477912426, acc: 0.9780219793319702)
[2025-02-13 20:51:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:13][root][INFO] - Training Epoch: 2/2, step 6843/7134 completed (loss: 0.10439225286245346, acc: 0.9689119458198547)
[2025-02-13 20:51:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:13][root][INFO] - Training Epoch: 2/2, step 6844/7134 completed (loss: 0.11653055250644684, acc: 0.9731543660163879)
[2025-02-13 20:51:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:14][root][INFO] - Training Epoch: 2/2, step 6845/7134 completed (loss: 0.08248837292194366, acc: 0.9692307710647583)
[2025-02-13 20:51:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:14][root][INFO] - Training Epoch: 2/2, step 6846/7134 completed (loss: 0.08200377225875854, acc: 0.9938271641731262)
[2025-02-13 20:51:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:15][root][INFO] - Training Epoch: 2/2, step 6847/7134 completed (loss: 0.06699107587337494, acc: 0.987500011920929)
[2025-02-13 20:51:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:15][root][INFO] - Training Epoch: 2/2, step 6848/7134 completed (loss: 0.06549187749624252, acc: 0.9775280952453613)
[2025-02-13 20:51:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:15][root][INFO] - Training Epoch: 2/2, step 6849/7134 completed (loss: 0.07244646549224854, acc: 0.9714285731315613)
[2025-02-13 20:51:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:16][root][INFO] - Training Epoch: 2/2, step 6850/7134 completed (loss: 0.0687750056385994, acc: 0.9714285731315613)
[2025-02-13 20:51:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:16][root][INFO] - Training Epoch: 2/2, step 6851/7134 completed (loss: 0.14495523273944855, acc: 0.9668508172035217)
[2025-02-13 20:51:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:17][root][INFO] - Training Epoch: 2/2, step 6852/7134 completed (loss: 0.1259993016719818, acc: 0.9714285731315613)
[2025-02-13 20:51:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:17][root][INFO] - Training Epoch: 2/2, step 6853/7134 completed (loss: 0.0733548253774643, acc: 0.9826589822769165)
[2025-02-13 20:51:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:17][root][INFO] - Training Epoch: 2/2, step 6854/7134 completed (loss: 0.03992186114192009, acc: 0.9830508232116699)
[2025-02-13 20:51:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:18][root][INFO] - Training Epoch: 2/2, step 6855/7134 completed (loss: 0.04136062040925026, acc: 0.9813084006309509)
[2025-02-13 20:51:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:18][root][INFO] - Training Epoch: 2/2, step 6856/7134 completed (loss: 0.01869313046336174, acc: 0.9939758777618408)
[2025-02-13 20:51:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:19][root][INFO] - Training Epoch: 2/2, step 6857/7134 completed (loss: 0.020281145349144936, acc: 0.9940476417541504)
[2025-02-13 20:51:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:19][root][INFO] - Training Epoch: 2/2, step 6858/7134 completed (loss: 0.009562503546476364, acc: 1.0)
[2025-02-13 20:51:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:19][root][INFO] - Training Epoch: 2/2, step 6859/7134 completed (loss: 0.0288856141269207, acc: 0.9943820238113403)
[2025-02-13 20:51:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:20][root][INFO] - Training Epoch: 2/2, step 6860/7134 completed (loss: 0.06157898157835007, acc: 0.9862068891525269)
[2025-02-13 20:51:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:20][root][INFO] - Training Epoch: 2/2, step 6861/7134 completed (loss: 0.017687156796455383, acc: 1.0)
[2025-02-13 20:51:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:21][root][INFO] - Training Epoch: 2/2, step 6862/7134 completed (loss: 0.11905796825885773, acc: 0.9696969985961914)
[2025-02-13 20:51:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:21][root][INFO] - Training Epoch: 2/2, step 6863/7134 completed (loss: 0.04861730709671974, acc: 0.9882352948188782)
[2025-02-13 20:51:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:21][root][INFO] - Training Epoch: 2/2, step 6864/7134 completed (loss: 0.023876914754509926, acc: 1.0)
[2025-02-13 20:51:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:22][root][INFO] - Training Epoch: 2/2, step 6865/7134 completed (loss: 0.07771661877632141, acc: 0.9674796462059021)
[2025-02-13 20:51:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:22][root][INFO] - Training Epoch: 2/2, step 6866/7134 completed (loss: 0.009859884157776833, acc: 1.0)
[2025-02-13 20:51:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:22][root][INFO] - Training Epoch: 2/2, step 6867/7134 completed (loss: 0.004333478398621082, acc: 1.0)
[2025-02-13 20:51:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:23][root][INFO] - Training Epoch: 2/2, step 6868/7134 completed (loss: 0.008176238276064396, acc: 1.0)
[2025-02-13 20:51:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:23][root][INFO] - Training Epoch: 2/2, step 6869/7134 completed (loss: 0.012885033152997494, acc: 1.0)
[2025-02-13 20:51:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:24][root][INFO] - Training Epoch: 2/2, step 6870/7134 completed (loss: 0.0416480116546154, acc: 0.9873417615890503)
[2025-02-13 20:51:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:24][root][INFO] - Training Epoch: 2/2, step 6871/7134 completed (loss: 0.010161096230149269, acc: 1.0)
[2025-02-13 20:51:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:24][root][INFO] - Training Epoch: 2/2, step 6872/7134 completed (loss: 0.04579097032546997, acc: 0.9886363744735718)
[2025-02-13 20:51:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:25][root][INFO] - Training Epoch: 2/2, step 6873/7134 completed (loss: 0.07838735729455948, acc: 0.9882352948188782)
[2025-02-13 20:51:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:25][root][INFO] - Training Epoch: 2/2, step 6874/7134 completed (loss: 0.012405818328261375, acc: 1.0)
[2025-02-13 20:51:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:25][root][INFO] - Training Epoch: 2/2, step 6875/7134 completed (loss: 0.043650396168231964, acc: 0.9932885766029358)
[2025-02-13 20:51:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:26][root][INFO] - Training Epoch: 2/2, step 6876/7134 completed (loss: 0.025382723659276962, acc: 1.0)
[2025-02-13 20:51:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:26][root][INFO] - Training Epoch: 2/2, step 6877/7134 completed (loss: 0.026631200686097145, acc: 1.0)
[2025-02-13 20:51:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:27][root][INFO] - Training Epoch: 2/2, step 6878/7134 completed (loss: 0.06960426270961761, acc: 0.9738562107086182)
[2025-02-13 20:51:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:27][root][INFO] - Training Epoch: 2/2, step 6879/7134 completed (loss: 0.06151444464921951, acc: 0.982758641242981)
[2025-02-13 20:51:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:27][root][INFO] - Training Epoch: 2/2, step 6880/7134 completed (loss: 0.029260054230690002, acc: 0.9949238300323486)
[2025-02-13 20:51:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:28][root][INFO] - Training Epoch: 2/2, step 6881/7134 completed (loss: 0.12434261292219162, acc: 0.9734042286872864)
[2025-02-13 20:51:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:28][root][INFO] - Training Epoch: 2/2, step 6882/7134 completed (loss: 0.10050329566001892, acc: 0.9788732528686523)
[2025-02-13 20:51:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:28][root][INFO] - Training Epoch: 2/2, step 6883/7134 completed (loss: 0.0770062729716301, acc: 0.9852941036224365)
[2025-02-13 20:51:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:29][root][INFO] - Training Epoch: 2/2, step 6884/7134 completed (loss: 0.0856151133775711, acc: 0.9886363744735718)
[2025-02-13 20:51:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:29][root][INFO] - Training Epoch: 2/2, step 6885/7134 completed (loss: 0.036358341574668884, acc: 0.9906542301177979)
[2025-02-13 20:51:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:30][root][INFO] - Training Epoch: 2/2, step 6886/7134 completed (loss: 0.07616112381219864, acc: 0.9797297120094299)
[2025-02-13 20:51:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:30][root][INFO] - Training Epoch: 2/2, step 6887/7134 completed (loss: 0.1628779023885727, acc: 0.9652174115180969)
[2025-02-13 20:51:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:30][root][INFO] - Training Epoch: 2/2, step 6888/7134 completed (loss: 0.060915756970644, acc: 0.9933333396911621)
[2025-02-13 20:51:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:31][root][INFO] - Training Epoch: 2/2, step 6889/7134 completed (loss: 0.04814770817756653, acc: 0.9869281053543091)
[2025-02-13 20:51:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:31][root][INFO] - Training Epoch: 2/2, step 6890/7134 completed (loss: 0.1320759654045105, acc: 0.9677419066429138)
[2025-02-13 20:51:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:32][root][INFO] - Training Epoch: 2/2, step 6891/7134 completed (loss: 0.05811213701963425, acc: 0.9814814925193787)
[2025-02-13 20:51:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:32][root][INFO] - Training Epoch: 2/2, step 6892/7134 completed (loss: 0.08450217545032501, acc: 0.9838709831237793)
[2025-02-13 20:51:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:32][root][INFO] - Training Epoch: 2/2, step 6893/7134 completed (loss: 0.08762621879577637, acc: 0.9759615659713745)
[2025-02-13 20:51:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:33][root][INFO] - Training Epoch: 2/2, step 6894/7134 completed (loss: 0.08947973698377609, acc: 0.9791666865348816)
[2025-02-13 20:51:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:33][root][INFO] - Training Epoch: 2/2, step 6895/7134 completed (loss: 0.07295051217079163, acc: 0.9852216839790344)
[2025-02-13 20:51:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:34][root][INFO] - Training Epoch: 2/2, step 6896/7134 completed (loss: 0.11460115760564804, acc: 0.9884393215179443)
[2025-02-13 20:51:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:34][root][INFO] - Training Epoch: 2/2, step 6897/7134 completed (loss: 0.08412165939807892, acc: 0.9851484894752502)
[2025-02-13 20:51:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:34][root][INFO] - Training Epoch: 2/2, step 6898/7134 completed (loss: 0.07157424092292786, acc: 0.9710982441902161)
[2025-02-13 20:51:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:35][root][INFO] - Training Epoch: 2/2, step 6899/7134 completed (loss: 0.1394762545824051, acc: 0.978723406791687)
[2025-02-13 20:51:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:35][root][INFO] - Training Epoch: 2/2, step 6900/7134 completed (loss: 0.1409757137298584, acc: 0.9710982441902161)
[2025-02-13 20:51:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:36][root][INFO] - Training Epoch: 2/2, step 6901/7134 completed (loss: 0.09156754612922668, acc: 0.9837837815284729)
[2025-02-13 20:51:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:36][root][INFO] - Training Epoch: 2/2, step 6902/7134 completed (loss: 0.05757573992013931, acc: 0.983146071434021)
[2025-02-13 20:51:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:37][root][INFO] - Training Epoch: 2/2, step 6903/7134 completed (loss: 0.06750501692295074, acc: 0.9804878234863281)
[2025-02-13 20:51:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:37][root][INFO] - Training Epoch: 2/2, step 6904/7134 completed (loss: 0.08155816048383713, acc: 0.984455943107605)
[2025-02-13 20:51:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:38][root][INFO] - Training Epoch: 2/2, step 6905/7134 completed (loss: 0.059983640909194946, acc: 0.9850746393203735)
[2025-02-13 20:51:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:38][root][INFO] - Training Epoch: 2/2, step 6906/7134 completed (loss: 0.1342289298772812, acc: 0.9901960492134094)
[2025-02-13 20:51:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:38][root][INFO] - Training Epoch: 2/2, step 6907/7134 completed (loss: 0.07106436789035797, acc: 0.9896373152732849)
[2025-02-13 20:51:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:39][root][INFO] - Training Epoch: 2/2, step 6908/7134 completed (loss: 0.1732960343360901, acc: 0.9714285731315613)
[2025-02-13 20:51:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:39][root][INFO] - Training Epoch: 2/2, step 6909/7134 completed (loss: 0.07349754124879837, acc: 0.9836065769195557)
[2025-02-13 20:51:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:40][root][INFO] - Training Epoch: 2/2, step 6910/7134 completed (loss: 0.04742204770445824, acc: 0.984375)
[2025-02-13 20:51:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:40][root][INFO] - Training Epoch: 2/2, step 6911/7134 completed (loss: 0.21301275491714478, acc: 0.9337016344070435)
[2025-02-13 20:51:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:41][root][INFO] - Training Epoch: 2/2, step 6912/7134 completed (loss: 0.14182111620903015, acc: 0.9707317352294922)
[2025-02-13 20:51:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:41][root][INFO] - Training Epoch: 2/2, step 6913/7134 completed (loss: 0.09976502507925034, acc: 0.9806763529777527)
[2025-02-13 20:51:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:41][root][INFO] - Training Epoch: 2/2, step 6914/7134 completed (loss: 0.24571146070957184, acc: 0.9477124214172363)
[2025-02-13 20:51:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:42][root][INFO] - Training Epoch: 2/2, step 6915/7134 completed (loss: 0.09102334827184677, acc: 0.9783783555030823)
[2025-02-13 20:51:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:42][root][INFO] - Training Epoch: 2/2, step 6916/7134 completed (loss: 0.12386754900217056, acc: 0.9658536314964294)
[2025-02-13 20:51:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:43][root][INFO] - Training Epoch: 2/2, step 6917/7134 completed (loss: 0.08815495669841766, acc: 0.9742268323898315)
[2025-02-13 20:51:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:43][root][INFO] - Training Epoch: 2/2, step 6918/7134 completed (loss: 0.05659397318959236, acc: 0.9845361113548279)
[2025-02-13 20:51:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:43][root][INFO] - Training Epoch: 2/2, step 6919/7134 completed (loss: 0.15185153484344482, acc: 0.9695431590080261)
[2025-02-13 20:51:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:44][root][INFO] - Training Epoch: 2/2, step 6920/7134 completed (loss: 0.039459120482206345, acc: 0.9880239367485046)
[2025-02-13 20:51:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:44][root][INFO] - Training Epoch: 2/2, step 6921/7134 completed (loss: 0.12730373442173004, acc: 0.9656862616539001)
[2025-02-13 20:51:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:45][root][INFO] - Training Epoch: 2/2, step 6922/7134 completed (loss: 0.08045995980501175, acc: 0.9743589758872986)
[2025-02-13 20:51:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:45][root][INFO] - Training Epoch: 2/2, step 6923/7134 completed (loss: 0.09494543075561523, acc: 0.9824561476707458)
[2025-02-13 20:51:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:46][root][INFO] - Training Epoch: 2/2, step 6924/7134 completed (loss: 0.0868799164891243, acc: 0.9878787994384766)
[2025-02-13 20:51:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:46][root][INFO] - Training Epoch: 2/2, step 6925/7134 completed (loss: 0.03726568445563316, acc: 0.9882352948188782)
[2025-02-13 20:51:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:46][root][INFO] - Training Epoch: 2/2, step 6926/7134 completed (loss: 0.07978423684835434, acc: 0.976190447807312)
[2025-02-13 20:51:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:47][root][INFO] - Training Epoch: 2/2, step 6927/7134 completed (loss: 0.13284431397914886, acc: 0.9653465151786804)
[2025-02-13 20:51:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:47][root][INFO] - Training Epoch: 2/2, step 6928/7134 completed (loss: 0.18413379788398743, acc: 0.96875)
[2025-02-13 20:51:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:47][root][INFO] - Training Epoch: 2/2, step 6929/7134 completed (loss: 0.04539775475859642, acc: 0.9886363744735718)
[2025-02-13 20:51:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:48][root][INFO] - Training Epoch: 2/2, step 6930/7134 completed (loss: 0.040633708238601685, acc: 0.9895833134651184)
[2025-02-13 20:51:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:48][root][INFO] - Training Epoch: 2/2, step 6931/7134 completed (loss: 0.0808730497956276, acc: 0.9947090148925781)
[2025-02-13 20:51:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:49][root][INFO] - Training Epoch: 2/2, step 6932/7134 completed (loss: 0.12278441339731216, acc: 0.9820359349250793)
[2025-02-13 20:51:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:49][root][INFO] - Training Epoch: 2/2, step 6933/7134 completed (loss: 0.1557183414697647, acc: 0.9640718698501587)
[2025-02-13 20:51:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:49][root][INFO] - Training Epoch: 2/2, step 6934/7134 completed (loss: 0.17435304820537567, acc: 0.9743589758872986)
[2025-02-13 20:51:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:50][root][INFO] - Training Epoch: 2/2, step 6935/7134 completed (loss: 0.14366570115089417, acc: 0.9680851101875305)
[2025-02-13 20:51:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:50][root][INFO] - Training Epoch: 2/2, step 6936/7134 completed (loss: 0.0619855597615242, acc: 0.9892473220825195)
[2025-02-13 20:51:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:51][root][INFO] - Training Epoch: 2/2, step 6937/7134 completed (loss: 0.23160462081432343, acc: 0.9520547986030579)
[2025-02-13 20:51:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:51][root][INFO] - Training Epoch: 2/2, step 6938/7134 completed (loss: 0.28797683119773865, acc: 0.931034505367279)
[2025-02-13 20:51:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:51][root][INFO] - Training Epoch: 2/2, step 6939/7134 completed (loss: 0.143373042345047, acc: 0.9464285969734192)
[2025-02-13 20:51:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:52][root][INFO] - Training Epoch: 2/2, step 6940/7134 completed (loss: 0.3056236505508423, acc: 0.9448275566101074)
[2025-02-13 20:51:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:52][root][INFO] - Training Epoch: 2/2, step 6941/7134 completed (loss: 0.03321300819516182, acc: 1.0)
[2025-02-13 20:51:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:52][root][INFO] - Training Epoch: 2/2, step 6942/7134 completed (loss: 0.1364867091178894, acc: 0.9681528806686401)
[2025-02-13 20:51:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:53][root][INFO] - Training Epoch: 2/2, step 6943/7134 completed (loss: 0.16631343960762024, acc: 0.9469026327133179)
[2025-02-13 20:51:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:53][root][INFO] - Training Epoch: 2/2, step 6944/7134 completed (loss: 0.1147642582654953, acc: 0.9724137783050537)
[2025-02-13 20:51:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:53][root][INFO] - Training Epoch: 2/2, step 6945/7134 completed (loss: 0.1973009556531906, acc: 0.9609375)
[2025-02-13 20:51:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:54][root][INFO] - Training Epoch: 2/2, step 6946/7134 completed (loss: 0.1404803842306137, acc: 0.9569892287254333)
[2025-02-13 20:51:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:54][root][INFO] - Training Epoch: 2/2, step 6947/7134 completed (loss: 0.09840492904186249, acc: 0.982300877571106)
[2025-02-13 20:51:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:55][root][INFO] - Training Epoch: 2/2, step 6948/7134 completed (loss: 0.12308704107999802, acc: 0.9642857313156128)
[2025-02-13 20:51:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:55][root][INFO] - Training Epoch: 2/2, step 6949/7134 completed (loss: 0.0774030014872551, acc: 0.991304337978363)
[2025-02-13 20:51:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:55][root][INFO] - Training Epoch: 2/2, step 6950/7134 completed (loss: 0.07317138463258743, acc: 0.9873417615890503)
[2025-02-13 20:51:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:56][root][INFO] - Training Epoch: 2/2, step 6951/7134 completed (loss: 0.11218030005693436, acc: 0.9513888955116272)
[2025-02-13 20:51:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:56][root][INFO] - Training Epoch: 2/2, step 6952/7134 completed (loss: 0.09145217388868332, acc: 0.9714285731315613)
[2025-02-13 20:51:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:56][root][INFO] - Training Epoch: 2/2, step 6953/7134 completed (loss: 0.12356672435998917, acc: 0.9849624037742615)
[2025-02-13 20:51:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:57][root][INFO] - Training Epoch: 2/2, step 6954/7134 completed (loss: 0.27331095933914185, acc: 0.9568965435028076)
[2025-02-13 20:51:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:57][root][INFO] - Training Epoch: 2/2, step 6955/7134 completed (loss: 0.09951041638851166, acc: 0.9750000238418579)
[2025-02-13 20:51:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:58][root][INFO] - Training Epoch: 2/2, step 6956/7134 completed (loss: 0.0693022683262825, acc: 0.985401451587677)
[2025-02-13 20:51:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:58][root][INFO] - Training Epoch: 2/2, step 6957/7134 completed (loss: 0.07411752641201019, acc: 0.984000027179718)
[2025-02-13 20:51:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:58][root][INFO] - Training Epoch: 2/2, step 6958/7134 completed (loss: 0.12009932845830917, acc: 0.9829059839248657)
[2025-02-13 20:51:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:59][root][INFO] - Training Epoch: 2/2, step 6959/7134 completed (loss: 0.21743741631507874, acc: 0.9541284441947937)
[2025-02-13 20:51:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:51:59][root][INFO] - Training Epoch: 2/2, step 6960/7134 completed (loss: 0.09099417924880981, acc: 0.9791666865348816)
[2025-02-13 20:51:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:00][root][INFO] - Training Epoch: 2/2, step 6961/7134 completed (loss: 0.06352026760578156, acc: 0.9743589758872986)
[2025-02-13 20:52:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:00][root][INFO] - Training Epoch: 2/2, step 6962/7134 completed (loss: 0.0779380053281784, acc: 0.9927536249160767)
[2025-02-13 20:52:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:00][root][INFO] - Training Epoch: 2/2, step 6963/7134 completed (loss: 0.06270046532154083, acc: 0.9801324605941772)
[2025-02-13 20:52:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:01][root][INFO] - Training Epoch: 2/2, step 6964/7134 completed (loss: 0.024071959778666496, acc: 1.0)
[2025-02-13 20:52:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:01][root][INFO] - Training Epoch: 2/2, step 6965/7134 completed (loss: 0.014887056313455105, acc: 1.0)
[2025-02-13 20:52:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:02][root][INFO] - Training Epoch: 2/2, step 6966/7134 completed (loss: 0.06427672505378723, acc: 0.9904761910438538)
[2025-02-13 20:52:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:02][root][INFO] - Training Epoch: 2/2, step 6967/7134 completed (loss: 0.028654763475060463, acc: 1.0)
[2025-02-13 20:52:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:02][root][INFO] - Training Epoch: 2/2, step 6968/7134 completed (loss: 0.048194222152233124, acc: 0.9865771532058716)
[2025-02-13 20:52:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:03][root][INFO] - Training Epoch: 2/2, step 6969/7134 completed (loss: 0.21827024221420288, acc: 0.9516128897666931)
[2025-02-13 20:52:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:03][root][INFO] - Training Epoch: 2/2, step 6970/7134 completed (loss: 0.21699248254299164, acc: 0.9396551847457886)
[2025-02-13 20:52:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:04][root][INFO] - Training Epoch: 2/2, step 6971/7134 completed (loss: 0.1620853692293167, acc: 0.9652777910232544)
[2025-02-13 20:52:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:04][root][INFO] - Training Epoch: 2/2, step 6972/7134 completed (loss: 0.09191935509443283, acc: 0.9844961166381836)
[2025-02-13 20:52:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:04][root][INFO] - Training Epoch: 2/2, step 6973/7134 completed (loss: 0.08923130482435226, acc: 0.9913793206214905)
[2025-02-13 20:52:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:05][root][INFO] - Training Epoch: 2/2, step 6974/7134 completed (loss: 0.1029607504606247, acc: 0.9738562107086182)
[2025-02-13 20:52:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:05][root][INFO] - Training Epoch: 2/2, step 6975/7134 completed (loss: 0.1190677359700203, acc: 0.9829059839248657)
[2025-02-13 20:52:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:06][root][INFO] - Training Epoch: 2/2, step 6976/7134 completed (loss: 0.06870033591985703, acc: 0.9785714149475098)
[2025-02-13 20:52:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:06][root][INFO] - Training Epoch: 2/2, step 6977/7134 completed (loss: 0.1261235922574997, acc: 0.9694656729698181)
[2025-02-13 20:52:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:07][root][INFO] - Training Epoch: 2/2, step 6978/7134 completed (loss: 0.07322893291711807, acc: 0.9810126423835754)
[2025-02-13 20:52:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:07][root][INFO] - Training Epoch: 2/2, step 6979/7134 completed (loss: 0.06277009844779968, acc: 0.9933775067329407)
[2025-02-13 20:52:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:07][root][INFO] - Training Epoch: 2/2, step 6980/7134 completed (loss: 0.25575387477874756, acc: 0.9265536665916443)
[2025-02-13 20:52:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:08][root][INFO] - Training Epoch: 2/2, step 6981/7134 completed (loss: 0.22687038779258728, acc: 0.9587628841400146)
[2025-02-13 20:52:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:08][root][INFO] - Training Epoch: 2/2, step 6982/7134 completed (loss: 0.20903673768043518, acc: 0.9581151604652405)
[2025-02-13 20:52:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:09][root][INFO] - Training Epoch: 2/2, step 6983/7134 completed (loss: 0.21711032092571259, acc: 0.9679487347602844)
[2025-02-13 20:52:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:09][root][INFO] - Training Epoch: 2/2, step 6984/7134 completed (loss: 0.1124751940369606, acc: 0.9720930457115173)
[2025-02-13 20:52:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:10][root][INFO] - Training Epoch: 2/2, step 6985/7134 completed (loss: 0.28964993357658386, acc: 0.9184549450874329)
[2025-02-13 20:52:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:10][root][INFO] - Training Epoch: 2/2, step 6986/7134 completed (loss: 0.12663091719150543, acc: 0.9629629850387573)
[2025-02-13 20:52:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:11][root][INFO] - Training Epoch: 2/2, step 6987/7134 completed (loss: 0.14955280721187592, acc: 0.9710744023323059)
[2025-02-13 20:52:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:11][root][INFO] - Training Epoch: 2/2, step 6988/7134 completed (loss: 0.04335605353116989, acc: 1.0)
[2025-02-13 20:52:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:11][root][INFO] - Training Epoch: 2/2, step 6989/7134 completed (loss: 0.17725050449371338, acc: 0.9594594836235046)
[2025-02-13 20:52:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:12][root][INFO] - Training Epoch: 2/2, step 6990/7134 completed (loss: 0.01771393232047558, acc: 1.0)
[2025-02-13 20:52:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:12][root][INFO] - Training Epoch: 2/2, step 6991/7134 completed (loss: 0.20193465054035187, acc: 0.9603960514068604)
[2025-02-13 20:52:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:13][root][INFO] - Training Epoch: 2/2, step 6992/7134 completed (loss: 0.08057896792888641, acc: 0.9819819927215576)
[2025-02-13 20:52:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:13][root][INFO] - Training Epoch: 2/2, step 6993/7134 completed (loss: 0.1122731864452362, acc: 0.9681528806686401)
[2025-02-13 20:52:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:13][root][INFO] - Training Epoch: 2/2, step 6994/7134 completed (loss: 0.23851852118968964, acc: 0.9466666579246521)
[2025-02-13 20:52:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:14][root][INFO] - Training Epoch: 2/2, step 6995/7134 completed (loss: 0.039885442703962326, acc: 0.9941860437393188)
[2025-02-13 20:52:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:14][root][INFO] - Training Epoch: 2/2, step 6996/7134 completed (loss: 0.06999768316745758, acc: 0.9794520735740662)
[2025-02-13 20:52:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:15][root][INFO] - Training Epoch: 2/2, step 6997/7134 completed (loss: 0.07060737907886505, acc: 0.9870967864990234)
[2025-02-13 20:52:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:15][root][INFO] - Training Epoch: 2/2, step 6998/7134 completed (loss: 0.10528446733951569, acc: 0.9894737005233765)
[2025-02-13 20:52:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:15][root][INFO] - Training Epoch: 2/2, step 6999/7134 completed (loss: 0.04596630483865738, acc: 0.9887005686759949)
[2025-02-13 20:52:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:16][root][INFO] - Training Epoch: 2/2, step 7000/7134 completed (loss: 0.04762662947177887, acc: 0.9818181991577148)
[2025-02-13 20:52:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:16][root][INFO] - Training Epoch: 2/2, step 7001/7134 completed (loss: 0.22670570015907288, acc: 0.9740259647369385)
[2025-02-13 20:52:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:17][root][INFO] - Training Epoch: 2/2, step 7002/7134 completed (loss: 0.08135338872671127, acc: 0.9831932783126831)
[2025-02-13 20:52:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:17][root][INFO] - Training Epoch: 2/2, step 7003/7134 completed (loss: 0.04659424349665642, acc: 0.9896373152732849)
[2025-02-13 20:52:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:17][root][INFO] - Training Epoch: 2/2, step 7004/7134 completed (loss: 0.11112474650144577, acc: 0.9634146094322205)
[2025-02-13 20:52:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:18][root][INFO] - Training Epoch: 2/2, step 7005/7134 completed (loss: 0.05991654098033905, acc: 1.0)
[2025-02-13 20:52:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:18][root][INFO] - Training Epoch: 2/2, step 7006/7134 completed (loss: 0.03272973746061325, acc: 1.0)
[2025-02-13 20:52:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:19][root][INFO] - Training Epoch: 2/2, step 7007/7134 completed (loss: 0.08057229220867157, acc: 0.9935483932495117)
[2025-02-13 20:52:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:19][root][INFO] - Training Epoch: 2/2, step 7008/7134 completed (loss: 0.07026642560958862, acc: 0.9834254384040833)
[2025-02-13 20:52:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:19][root][INFO] - Training Epoch: 2/2, step 7009/7134 completed (loss: 0.07246766984462738, acc: 0.9791666865348816)
[2025-02-13 20:52:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:20][root][INFO] - Training Epoch: 2/2, step 7010/7134 completed (loss: 0.05793954059481621, acc: 0.9870967864990234)
[2025-02-13 20:52:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:20][root][INFO] - Training Epoch: 2/2, step 7011/7134 completed (loss: 0.20635773241519928, acc: 0.9790576100349426)
[2025-02-13 20:52:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:21][root][INFO] - Training Epoch: 2/2, step 7012/7134 completed (loss: 0.01403104979544878, acc: 1.0)
[2025-02-13 20:52:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:21][root][INFO] - Training Epoch: 2/2, step 7013/7134 completed (loss: 0.02895524725317955, acc: 0.993630588054657)
[2025-02-13 20:52:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:21][root][INFO] - Training Epoch: 2/2, step 7014/7134 completed (loss: 0.10846807062625885, acc: 0.9729729890823364)
[2025-02-13 20:52:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:22][root][INFO] - Training Epoch: 2/2, step 7015/7134 completed (loss: 0.047478318214416504, acc: 0.9925373196601868)
[2025-02-13 20:52:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:22][root][INFO] - Training Epoch: 2/2, step 7016/7134 completed (loss: 0.12982475757598877, acc: 0.9679487347602844)
[2025-02-13 20:52:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:23][root][INFO] - Training Epoch: 2/2, step 7017/7134 completed (loss: 0.06240195780992508, acc: 0.991150438785553)
[2025-02-13 20:52:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:23][root][INFO] - Training Epoch: 2/2, step 7018/7134 completed (loss: 0.17055557668209076, acc: 0.9629629850387573)
[2025-02-13 20:52:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:23][root][INFO] - Training Epoch: 2/2, step 7019/7134 completed (loss: 0.29965564608573914, acc: 0.9430379867553711)
[2025-02-13 20:52:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:24][root][INFO] - Training Epoch: 2/2, step 7020/7134 completed (loss: 0.0434286929666996, acc: 0.9939758777618408)
[2025-02-13 20:52:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:24][root][INFO] - Training Epoch: 2/2, step 7021/7134 completed (loss: 0.06357089430093765, acc: 0.9925925731658936)
[2025-02-13 20:52:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:25][root][INFO] - Training Epoch: 2/2, step 7022/7134 completed (loss: 0.14635181427001953, acc: 0.9583333134651184)
[2025-02-13 20:52:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:25][root][INFO] - Training Epoch: 2/2, step 7023/7134 completed (loss: 0.08763990551233292, acc: 0.9879518151283264)
[2025-02-13 20:52:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:26][root][INFO] - Training Epoch: 2/2, step 7024/7134 completed (loss: 0.04256662353873253, acc: 0.9855072498321533)
[2025-02-13 20:52:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:26][root][INFO] - Training Epoch: 2/2, step 7025/7134 completed (loss: 0.19955122470855713, acc: 0.9523809552192688)
[2025-02-13 20:52:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:26][root][INFO] - Training Epoch: 2/2, step 7026/7134 completed (loss: 0.07005322724580765, acc: 1.0)
[2025-02-13 20:52:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:27][root][INFO] - Training Epoch: 2/2, step 7027/7134 completed (loss: 0.07582955062389374, acc: 0.98591548204422)
[2025-02-13 20:52:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:27][root][INFO] - Training Epoch: 2/2, step 7028/7134 completed (loss: 0.058553796261548996, acc: 0.9890109896659851)
[2025-02-13 20:52:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:28][root][INFO] - Training Epoch: 2/2, step 7029/7134 completed (loss: 0.06979159265756607, acc: 0.9775280952453613)
[2025-02-13 20:52:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:28][root][INFO] - Training Epoch: 2/2, step 7030/7134 completed (loss: 0.07829475402832031, acc: 1.0)
[2025-02-13 20:52:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:28][root][INFO] - Training Epoch: 2/2, step 7031/7134 completed (loss: 0.08149808645248413, acc: 0.9876543283462524)
[2025-02-13 20:52:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:29][root][INFO] - Training Epoch: 2/2, step 7032/7134 completed (loss: 0.07079963386058807, acc: 1.0)
[2025-02-13 20:52:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:29][root][INFO] - Training Epoch: 2/2, step 7033/7134 completed (loss: 0.131686732172966, acc: 0.9753086566925049)
[2025-02-13 20:52:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:30][root][INFO] - Training Epoch: 2/2, step 7034/7134 completed (loss: 0.18709851801395416, acc: 0.984375)
[2025-02-13 20:52:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:30][root][INFO] - Training Epoch: 2/2, step 7035/7134 completed (loss: 0.11028650403022766, acc: 0.9682539701461792)
[2025-02-13 20:52:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:30][root][INFO] - Training Epoch: 2/2, step 7036/7134 completed (loss: 0.10641475766897202, acc: 0.9682539701461792)
[2025-02-13 20:52:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:31][root][INFO] - Training Epoch: 2/2, step 7037/7134 completed (loss: 0.07899800688028336, acc: 0.9701492786407471)
[2025-02-13 20:52:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:31][root][INFO] - Training Epoch: 2/2, step 7038/7134 completed (loss: 0.14746315777301788, acc: 0.9367088675498962)
[2025-02-13 20:52:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:31][root][INFO] - Training Epoch: 2/2, step 7039/7134 completed (loss: 0.3139680027961731, acc: 0.9777777791023254)
[2025-02-13 20:52:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:32][root][INFO] - Training Epoch: 2/2, step 7040/7134 completed (loss: 0.20441249012947083, acc: 0.9599999785423279)
[2025-02-13 20:52:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:32][root][INFO] - Training Epoch: 2/2, step 7041/7134 completed (loss: 0.14440682530403137, acc: 0.9624999761581421)
[2025-02-13 20:52:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:32][root][INFO] - Training Epoch: 2/2, step 7042/7134 completed (loss: 0.16953687369823456, acc: 0.9577465057373047)
[2025-02-13 20:52:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:33][root][INFO] - Training Epoch: 2/2, step 7043/7134 completed (loss: 0.2170047014951706, acc: 0.9333333373069763)
[2025-02-13 20:52:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:33][root][INFO] - Training Epoch: 2/2, step 7044/7134 completed (loss: 0.09275058656930923, acc: 1.0)
[2025-02-13 20:52:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:34][root][INFO] - Training Epoch: 2/2, step 7045/7134 completed (loss: 0.11729015409946442, acc: 0.9586206674575806)
[2025-02-13 20:52:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:34][root][INFO] - Training Epoch: 2/2, step 7046/7134 completed (loss: 0.06634889543056488, acc: 0.9880239367485046)
[2025-02-13 20:52:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:34][root][INFO] - Training Epoch: 2/2, step 7047/7134 completed (loss: 0.10694915056228638, acc: 0.9624413251876831)
[2025-02-13 20:52:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:35][root][INFO] - Training Epoch: 2/2, step 7048/7134 completed (loss: 0.022057127207517624, acc: 0.9948453903198242)
[2025-02-13 20:52:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:35][root][INFO] - Training Epoch: 2/2, step 7049/7134 completed (loss: 0.08599218726158142, acc: 0.9826589822769165)
[2025-02-13 20:52:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:35][root][INFO] - Training Epoch: 2/2, step 7050/7134 completed (loss: 0.10759295523166656, acc: 0.9885714054107666)
[2025-02-13 20:52:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:36][root][INFO] - Training Epoch: 2/2, step 7051/7134 completed (loss: 0.026949651539325714, acc: 0.9945651888847351)
[2025-02-13 20:52:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:36][root][INFO] - Training Epoch: 2/2, step 7052/7134 completed (loss: 0.08282788097858429, acc: 0.9768785834312439)
[2025-02-13 20:52:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:37][root][INFO] - Training Epoch: 2/2, step 7053/7134 completed (loss: 0.050495896488428116, acc: 0.994535505771637)
[2025-02-13 20:52:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:37][root][INFO] - Training Epoch: 2/2, step 7054/7134 completed (loss: 0.2466440200805664, acc: 0.948387086391449)
[2025-02-13 20:52:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:37][root][INFO] - Training Epoch: 2/2, step 7055/7134 completed (loss: 0.16365596652030945, acc: 0.9615384340286255)
[2025-02-13 20:52:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:38][root][INFO] - Training Epoch: 2/2, step 7056/7134 completed (loss: 0.0713738426566124, acc: 0.9888888597488403)
[2025-02-13 20:52:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:38][root][INFO] - Training Epoch: 2/2, step 7057/7134 completed (loss: 0.42062559723854065, acc: 0.9337016344070435)
[2025-02-13 20:52:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:39][root][INFO] - Training Epoch: 2/2, step 7058/7134 completed (loss: 0.18822965025901794, acc: 0.9457364082336426)
[2025-02-13 20:52:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:39][root][INFO] - Training Epoch: 2/2, step 7059/7134 completed (loss: 0.0750582367181778, acc: 0.9871794581413269)
[2025-02-13 20:52:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:39][root][INFO] - Training Epoch: 2/2, step 7060/7134 completed (loss: 0.13403114676475525, acc: 0.9788732528686523)
[2025-02-13 20:52:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:40][root][INFO] - Training Epoch: 2/2, step 7061/7134 completed (loss: 0.15183304250240326, acc: 0.9569892287254333)
[2025-02-13 20:52:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:40][root][INFO] - Training Epoch: 2/2, step 7062/7134 completed (loss: 0.11036373674869537, acc: 0.9576271176338196)
[2025-02-13 20:52:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:40][root][INFO] - Training Epoch: 2/2, step 7063/7134 completed (loss: 0.24092595279216766, acc: 0.9513513445854187)
[2025-02-13 20:52:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:41][root][INFO] - Training Epoch: 2/2, step 7064/7134 completed (loss: 0.05448704585433006, acc: 0.9819276928901672)
[2025-02-13 20:52:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:41][root][INFO] - Training Epoch: 2/2, step 7065/7134 completed (loss: 0.08467921614646912, acc: 0.9898989796638489)
[2025-02-13 20:52:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:42][root][INFO] - Training Epoch: 2/2, step 7066/7134 completed (loss: 0.04318787157535553, acc: 0.9895287752151489)
[2025-02-13 20:52:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:42][root][INFO] - Training Epoch: 2/2, step 7067/7134 completed (loss: 0.05179984122514725, acc: 0.9894179701805115)
[2025-02-13 20:52:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:43][root][INFO] - Training Epoch: 2/2, step 7068/7134 completed (loss: 0.03168666735291481, acc: 0.995121955871582)
[2025-02-13 20:52:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:43][root][INFO] - Training Epoch: 2/2, step 7069/7134 completed (loss: 0.06541517376899719, acc: 0.9740259647369385)
[2025-02-13 20:52:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:43][root][INFO] - Training Epoch: 2/2, step 7070/7134 completed (loss: 0.05117085203528404, acc: 0.9884393215179443)
[2025-02-13 20:52:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:44][root][INFO] - Training Epoch: 2/2, step 7071/7134 completed (loss: 0.08223945647478104, acc: 0.9747899174690247)
[2025-02-13 20:52:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:44][root][INFO] - Training Epoch: 2/2, step 7072/7134 completed (loss: 0.07395841181278229, acc: 0.9797979593276978)
[2025-02-13 20:52:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:44][root][INFO] - Training Epoch: 2/2, step 7073/7134 completed (loss: 0.1021132543683052, acc: 0.9698492288589478)
[2025-02-13 20:52:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:45][root][INFO] - Training Epoch: 2/2, step 7074/7134 completed (loss: 0.1499471813440323, acc: 0.9593495726585388)
[2025-02-13 20:52:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:45][root][INFO] - Training Epoch: 2/2, step 7075/7134 completed (loss: 0.15148665010929108, acc: 0.9554139971733093)
[2025-02-13 20:52:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:46][root][INFO] - Training Epoch: 2/2, step 7076/7134 completed (loss: 0.1995496153831482, acc: 0.9550561904907227)
[2025-02-13 20:52:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:46][root][INFO] - Training Epoch: 2/2, step 7077/7134 completed (loss: 0.1743374466896057, acc: 0.9589040875434875)
[2025-02-13 20:52:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:46][root][INFO] - Training Epoch: 2/2, step 7078/7134 completed (loss: 0.11641141772270203, acc: 0.9724137783050537)
[2025-02-13 20:52:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:47][root][INFO] - Training Epoch: 2/2, step 7079/7134 completed (loss: 0.06251446902751923, acc: 0.9868420958518982)
[2025-02-13 20:52:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:47][root][INFO] - Training Epoch: 2/2, step 7080/7134 completed (loss: 0.11376398056745529, acc: 0.9727891087532043)
[2025-02-13 20:52:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:48][root][INFO] - Training Epoch: 2/2, step 7081/7134 completed (loss: 0.04850573465228081, acc: 0.9942196607589722)
[2025-02-13 20:52:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:48][root][INFO] - Training Epoch: 2/2, step 7082/7134 completed (loss: 0.1466129720211029, acc: 0.9671052694320679)
[2025-02-13 20:52:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:48][root][INFO] - Training Epoch: 2/2, step 7083/7134 completed (loss: 0.051652248948812485, acc: 0.9929078221321106)
[2025-02-13 20:52:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:49][root][INFO] - Training Epoch: 2/2, step 7084/7134 completed (loss: 0.12683673202991486, acc: 0.9655172228813171)
[2025-02-13 20:52:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:49][root][INFO] - Training Epoch: 2/2, step 7085/7134 completed (loss: 0.1073806881904602, acc: 0.9800000190734863)
[2025-02-13 20:52:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:49][root][INFO] - Training Epoch: 2/2, step 7086/7134 completed (loss: 0.08052586764097214, acc: 0.9870967864990234)
[2025-02-13 20:52:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:50][root][INFO] - Training Epoch: 2/2, step 7087/7134 completed (loss: 0.1190711036324501, acc: 0.9844961166381836)
[2025-02-13 20:52:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:50][root][INFO] - Training Epoch: 2/2, step 7088/7134 completed (loss: 0.10649517923593521, acc: 0.9645389914512634)
[2025-02-13 20:52:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:51][root][INFO] - Training Epoch: 2/2, step 7089/7134 completed (loss: 0.11325053870677948, acc: 0.9612902998924255)
[2025-02-13 20:52:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:51][root][INFO] - Training Epoch: 2/2, step 7090/7134 completed (loss: 0.1202273815870285, acc: 0.9821428656578064)
[2025-02-13 20:52:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:51][root][INFO] - Training Epoch: 2/2, step 7091/7134 completed (loss: 0.08471794426441193, acc: 0.9696969985961914)
[2025-02-13 20:52:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:52][root][INFO] - Training Epoch: 2/2, step 7092/7134 completed (loss: 0.06747446209192276, acc: 0.9849624037742615)
[2025-02-13 20:52:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:52][root][INFO] - Training Epoch: 2/2, step 7093/7134 completed (loss: 0.07990714907646179, acc: 0.9829059839248657)
[2025-02-13 20:52:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:52][root][INFO] - Training Epoch: 2/2, step 7094/7134 completed (loss: 0.04340178892016411, acc: 0.9895833134651184)
[2025-02-13 20:52:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:53][root][INFO] - Training Epoch: 2/2, step 7095/7134 completed (loss: 0.15613164007663727, acc: 0.9661017060279846)
[2025-02-13 20:52:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:53][root][INFO] - Training Epoch: 2/2, step 7096/7134 completed (loss: 0.12405037879943848, acc: 0.9647887349128723)
[2025-02-13 20:52:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:54][root][INFO] - Training Epoch: 2/2, step 7097/7134 completed (loss: 0.1527252197265625, acc: 0.9551281929016113)
[2025-02-13 20:52:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:54][root][INFO] - Training Epoch: 2/2, step 7098/7134 completed (loss: 0.04518166929483414, acc: 0.9930070042610168)
[2025-02-13 20:52:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:55][root][INFO] - Training Epoch: 2/2, step 7099/7134 completed (loss: 0.12728285789489746, acc: 0.9637681245803833)
[2025-02-13 20:52:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:55][root][INFO] - Training Epoch: 2/2, step 7100/7134 completed (loss: 0.13894324004650116, acc: 0.9640718698501587)
[2025-02-13 20:52:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:55][root][INFO] - Training Epoch: 2/2, step 7101/7134 completed (loss: 0.07798243314027786, acc: 0.9928057789802551)
[2025-02-13 20:52:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:56][root][INFO] - Training Epoch: 2/2, step 7102/7134 completed (loss: 0.05500543862581253, acc: 0.991525411605835)
[2025-02-13 20:52:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:56][root][INFO] - Training Epoch: 2/2, step 7103/7134 completed (loss: 0.05258527770638466, acc: 1.0)
[2025-02-13 20:52:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:57][root][INFO] - Training Epoch: 2/2, step 7104/7134 completed (loss: 0.02705433778464794, acc: 1.0)
[2025-02-13 20:52:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:57][root][INFO] - Training Epoch: 2/2, step 7105/7134 completed (loss: 0.06821344047784805, acc: 0.9861111044883728)
[2025-02-13 20:52:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:57][root][INFO] - Training Epoch: 2/2, step 7106/7134 completed (loss: 0.03949235379695892, acc: 1.0)
[2025-02-13 20:52:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:58][root][INFO] - Training Epoch: 2/2, step 7107/7134 completed (loss: 0.076009102165699, acc: 0.9751552939414978)
[2025-02-13 20:52:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:58][root][INFO] - Training Epoch: 2/2, step 7108/7134 completed (loss: 0.06869196146726608, acc: 0.9927007555961609)
[2025-02-13 20:52:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:59][root][INFO] - Training Epoch: 2/2, step 7109/7134 completed (loss: 0.08945106714963913, acc: 0.9629629850387573)
[2025-02-13 20:52:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:59][root][INFO] - Training Epoch: 2/2, step 7110/7134 completed (loss: 0.03949517384171486, acc: 0.9924812316894531)
[2025-02-13 20:52:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:52:59][root][INFO] - Training Epoch: 2/2, step 7111/7134 completed (loss: 0.09141167998313904, acc: 0.9629629850387573)
[2025-02-13 20:53:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:00][root][INFO] - Training Epoch: 2/2, step 7112/7134 completed (loss: 0.058867812156677246, acc: 0.9909909963607788)
[2025-02-13 20:53:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:00][root][INFO] - Training Epoch: 2/2, step 7113/7134 completed (loss: 0.17456939816474915, acc: 0.9681528806686401)
[2025-02-13 20:53:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:01][root][INFO] - Training Epoch: 2/2, step 7114/7134 completed (loss: 0.16062642633914948, acc: 0.963302731513977)
[2025-02-13 20:53:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:01][root][INFO] - Training Epoch: 2/2, step 7115/7134 completed (loss: 0.19680653512477875, acc: 0.9541984796524048)
[2025-02-13 20:53:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:01][root][INFO] - Training Epoch: 2/2, step 7116/7134 completed (loss: 0.11582516878843307, acc: 0.970588207244873)
[2025-02-13 20:53:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:02][root][INFO] - Training Epoch: 2/2, step 7117/7134 completed (loss: 0.13885581493377686, acc: 0.960629940032959)
[2025-02-13 20:53:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:02][root][INFO] - Training Epoch: 2/2, step 7118/7134 completed (loss: 0.09488378465175629, acc: 0.9928571581840515)
[2025-02-13 20:53:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:02][root][INFO] - Training Epoch: 2/2, step 7119/7134 completed (loss: 0.05810065567493439, acc: 0.9931034445762634)
[2025-02-13 20:53:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:03][root][INFO] - Training Epoch: 2/2, step 7120/7134 completed (loss: 0.07024870067834854, acc: 0.9805825352668762)
[2025-02-13 20:53:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:03][root][INFO] - Training Epoch: 2/2, step 7121/7134 completed (loss: 0.05747301131486893, acc: 0.9923076629638672)
[2025-02-13 20:53:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:04][root][INFO] - Training Epoch: 2/2, step 7122/7134 completed (loss: 0.07701674848794937, acc: 0.9892473220825195)
[2025-02-13 20:53:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:04][root][INFO] - Training Epoch: 2/2, step 7123/7134 completed (loss: 0.03861917555332184, acc: 1.0)
[2025-02-13 20:53:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:04][root][INFO] - Training Epoch: 2/2, step 7124/7134 completed (loss: 0.07460512965917587, acc: 0.9918032884597778)
[2025-02-13 20:53:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:05][root][INFO] - Training Epoch: 2/2, step 7125/7134 completed (loss: 0.07322218269109726, acc: 0.9847328066825867)
[2025-02-13 20:53:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:05][root][INFO] - Training Epoch: 2/2, step 7126/7134 completed (loss: 0.2254270315170288, acc: 0.970802903175354)
[2025-02-13 20:53:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:06][root][INFO] - Training Epoch: 2/2, step 7127/7134 completed (loss: 0.03325074166059494, acc: 1.0)
[2025-02-13 20:53:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:06][root][INFO] - Training Epoch: 2/2, step 7128/7134 completed (loss: 0.07718177139759064, acc: 0.9794520735740662)
[2025-02-13 20:53:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:06][root][INFO] - Training Epoch: 2/2, step 7129/7134 completed (loss: 0.3371608555316925, acc: 0.9534883499145508)
[2025-02-13 20:53:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:53:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:54:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:55:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:56:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:15][slam_llm.utils.train_utils][INFO] -  eval_ppl=tensor(1.2740, device='cuda:0') eval_epoch_loss=tensor(0.2422, device='cuda:0') eval_epoch_acc=tensor(0.9465, device='cuda:0')
[2025-02-13 20:57:15][slam_llm.utils.train_utils][INFO] - we are about to save the PEFT modules
[2025-02-13 20:57:15][slam_llm.utils.checkpoint_handler][INFO] - --> saving model ...
[2025-02-13 20:57:15][slam_llm.utils.checkpoint_handler][INFO] - encoder saved at /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_wavlm_llama32_1b_linear_peft_transfer_librispeech-100/asr_epoch_2_step_7130_loss_0.24219220876693726/model.pt
[2025-02-13 20:57:15][slam_llm.utils.train_utils][INFO] - PEFT modules are saved in /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_wavlm_llama32_1b_linear_peft_transfer_librispeech-100 directory
[2025-02-13 20:57:15][slam_llm.utils.train_utils][INFO] - best eval acc on epoch 2 is 0.9465179443359375
[2025-02-13 20:57:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:16][root][INFO] - Training Epoch: 2/2, step 7130/7134 completed (loss: 0.3498584032058716, acc: 0.9133333563804626)
[2025-02-13 20:57:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:16][root][INFO] - Training Epoch: 2/2, step 7131/7134 completed (loss: 0.09581973403692245, acc: 0.9811320900917053)
[2025-02-13 20:57:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:16][root][INFO] - Training Epoch: 2/2, step 7132/7134 completed (loss: 0.0188435111194849, acc: 1.0)
[2025-02-13 20:57:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:17][root][INFO] - Training Epoch: 2/2, step 7133/7134 completed (loss: 0.10586794465780258, acc: 0.9849624037742615)
[2025-02-13 20:57:17][slam_llm.utils.train_utils][INFO] - Epoch 2: train_perplexity=1.1320, train_epoch_loss=0.1240, epoch time 3774.5675823166966s
[2025-02-13 20:57:17][slam_llm.utils.train_utils][INFO] - Max CUDA memory allocated was 12 GB
[2025-02-13 20:57:17][slam_llm.utils.train_utils][INFO] - Max CUDA memory reserved was 21 GB
[2025-02-13 20:57:17][slam_llm.utils.train_utils][INFO] - Peak active CUDA memory was 12 GB
[2025-02-13 20:57:17][slam_llm.utils.train_utils][INFO] - Cuda Malloc retires : 0
[2025-02-13 20:57:17][slam_llm.utils.train_utils][INFO] - CPU Total Peak Memory consumed during the train (max): 8 GB
[2025-02-13 20:57:17][root][INFO] - Key: avg_train_prep, Value: 1.2417622804641724
[2025-02-13 20:57:17][root][INFO] - Key: avg_train_loss, Value: 0.21260732412338257
[2025-02-13 20:57:17][root][INFO] - Key: avg_train_acc, Value: 0.9513086080551147
[2025-02-13 20:57:17][root][INFO] - Key: avg_eval_prep, Value: 1.3098177909851074
[2025-02-13 20:57:17][root][INFO] - Key: avg_eval_loss, Value: 0.2689000070095062
[2025-02-13 20:57:17][root][INFO] - Key: avg_eval_acc, Value: 0.9385845065116882
[2025-02-13 20:57:17][root][INFO] - Key: avg_epoch_time, Value: 3786.5626051574945
[2025-02-13 20:57:17][root][INFO] - Key: avg_checkpoint_time, Value: 0.3452558880671859
Selected lowest loss checkpoint: asr_epoch_2_step_3564_loss_0.23238003253936768
Checkpoint file: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_wavlm_llama32_1b_linear_peft_transfer_librispeech-100/asr_epoch_2_step_3564_loss_0.23238003253936768/model.pt
ckpt_folder /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_wavlm_llama32_1b_linear_peft_transfer_librispeech-100/asr_epoch_2_step_3564_loss_0.23238003253936768
[2025-02-13 20:57:41][root][INFO] - train_config: {'model_name': 'asr', 'enable_ddp': False, 'enable_deepspeed': False, 'enable_fsdp': False, 'low_cpu_fsdp': False, 'run_validation': True, 'batch_size_training': 4, 'batching_strategy': 'custom', 'context_length': 4096, 'gradient_accumulation_steps': 1, 'num_epochs': 1, 'resume_step': 0, 'resume_epoch': 0, 'num_workers_dataloader': 1, 'warmup_steps': 1000, 'total_steps': 100000, 'validation_interval': 1000, 'lr': 0.0001, 'weight_decay': 0.0, 'gamma': 0.85, 'seed': 42, 'use_fp16': False, 'mixed_precision': True, 'val_batch_size': 4, 'use_peft': True, 'peft_config': {'peft_method': 'lora', 'r': 8, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj'], 'bias': 'none', 'task_type': 'CAUSAL_LM', 'lora_dropout': 0.05, 'inference_mode': False}, 'output_dir': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_wavlm_llama32_1b_linear_peft_transfer_librispeech-100', 'freeze_layers': False, 'num_freeze_layers': 1, 'quantization': False, 'one_gpu': False, 'save_model': True, 'dist_checkpoint_root_folder': 'PATH/to/save/FSDP/model', 'dist_checkpoint_folder': 'fine-tuned', 'save_optimizer': False, 'use_fast_kernels': False, 'run_test_during_validation': False, 'run_test_during_validation_file': 'test.wav', 'run_test_during_validation_prompt': '<|ASR|>', 'freeze_llm': True, 'freeze_encoder': True, 'freeze_encoder2': False, 'save_embedding': False}
[2025-02-13 20:57:41][root][INFO] - fsdp_config: {'mixed_precision': True, 'use_fp16': False, 'sharding_strategy': <ShardingStrategy.NO_SHARD: 3>, 'checkpoint_type': 'SHARDED_STATE_DICT', 'fsdp_activation_checkpointing': True, 'fsdp_cpu_offload': False, 'pure_bf16': False, 'optimizer': 'AdamW'}
[2025-02-13 20:57:41][root][INFO] - model_config: {'file': 'examples/asr_librispeech/model/slam_model_asr.py:model_factory', 'llm_name': 'llama32_1b', 'llm_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/Llama-3.2-1B-Instruct', 'llm_type': 'decoder_only', 'llm_dim': 2048, 'llm_inference_config': 'repetition_penalty', 'encoder_name': 'wavlm', 'encoder_ds_rate': 2, 'encoder_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/models/WavLM-Large.pt', 'encoder_dim': 1024, 'encoder_projector': 'linear', 'encoder_projector_ds_rate': 5, 'qformer_layers': 8, 'modal': 'audio', 'normalize': True, 'encoder_type': 'finetune', 'encoder2_name': '', 'encoder2_dim': 1024, 'encoder2_path': '', 'identifier': 'ami_wavlm_llama32_1b_linear_peft_transfer_librispeech-100'}
[2025-02-13 20:57:42][slam_llm.models.wavlm.WavLM][INFO] - WavLM Config: {'extractor_mode': 'layer_norm', 'encoder_layers': 24, 'encoder_embed_dim': 1024, 'encoder_ffn_embed_dim': 4096, 'encoder_attention_heads': 16, 'activation_fn': 'gelu', 'layer_norm_first': True, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'feature_grad_mult': 1.0, 'normalize': True, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True}
[2025-02-13 20:57:47][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-02-13 20:57:47][slam_llm.utils.train_utils][INFO] - --> wavlm has 315.45312 Million params

[2025-02-13 20:57:47][slam_llm.utils.train_utils][INFO] - --> Module wavlm
[2025-02-13 20:57:47][slam_llm.utils.train_utils][INFO] - --> wavlm has 0.0 Million params

[2025-02-13 20:57:51][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-02-13 20:57:51][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 1235.8144 Million params

[2025-02-13 20:57:51][slam_llm.models.slam_model][INFO] - setup peft...
trainable params: 5,636,096 || all params: 1,241,450,496 || trainable%: 0.4539928106807088
[2025-02-13 20:57:52][slam_llm.utils.train_utils][INFO] - --> Module llama32_1b
[2025-02-13 20:57:52][slam_llm.utils.train_utils][INFO] - --> llama32_1b has 5.636096 Million params

[2025-02-13 20:57:52][slam_llm.utils.train_utils][INFO] - --> Module linear
[2025-02-13 20:57:52][slam_llm.utils.train_utils][INFO] - --> linear has 14.68416 Million params

[2025-02-13 20:57:52][slam_model_asr.py][INFO] - loading other parts from: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_wavlm_llama32_1b_linear_peft_transfer_librispeech-100/asr_epoch_2_step_3564_loss_0.23238003253936768/model.pt
[2025-02-13 20:57:52][slam_llm.utils.train_utils][INFO] - --> Model asr
[2025-02-13 20:57:52][slam_llm.utils.train_utils][INFO] - --> asr has 20.320256 Million params

[2025-02-13 20:57:53][root][INFO] - dataset_config: {'dataset': 'speech_dataset', 'file': 'src/slam_llm/datasets/speech_dataset.py:get_speech_dataset', 'train_data_path': None, 'val_data_path': '/work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/data/librispeech-100/test.jsonl', 'train_split': 'train', 'test_split': 'validation', 'prompt': None, 'data_path': None, 'max_words': None, 'max_mel': None, 'fix_length_audio': -1, 'inference_mode': True, 'input_type': 'raw', 'mel_size': 80, 'normalize': True}
[2025-02-13 20:57:54][root][INFO] - --> Training Set Length = 2620
[2025-02-13 20:57:54][root][INFO] - =====================================
Loaded LLM Config Path: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/examples/asr_librispeech/scripts/llm_config/repetition_penalty.json
Loaded LLM Config: {'max_new_tokens': 200, 'num_beams': 4, 'do_sample': False, 'min_length': 1, 'top_p': 1.0, 'repetition_penalty': 2.0, 'length_penalty': 1.0, 'temperature': 1.0, 'no_repeat_ngram_size': 1}
[2025-02-13 20:57:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:57:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:58:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 20:59:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:00:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:01:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:02:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:03:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:04:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:05:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:06:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:07:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:08:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:09:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:10:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:11:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:14][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:34][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:12:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:30][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:36][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:13:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:02][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:06][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:16][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:37][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:39][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:41][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:45][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:46][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:48][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:51][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:54][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:56][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:14:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:04][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:07][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:17][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:24][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:26][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:28][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:32][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:35][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:38][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:40][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:42][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:43][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:44][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:47][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:49][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:50][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:52][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:53][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:55][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:57][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:58][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:15:59][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:00][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:01][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:03][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:05][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:08][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:09][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:10][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:11][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:12][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:13][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:15][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:18][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:19][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:20][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:21][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:22][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:23][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:25][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:27][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:29][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:31][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:33][slam_llm.models.slam_model][INFO] - modality encoder
[2025-02-13 21:16:35][root][INFO] - Predictions written to: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_wavlm_llama32_1b_linear_peft_transfer_librispeech-100/decode_test_beam4_pred_20250213_205754
[2025-02-13 21:16:35][root][INFO] - Ground truth written to: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_wavlm_llama32_1b_linear_peft_transfer_librispeech-100/decode_test_beam4_gt_20250213_205754
Using folder: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_wavlm_llama32_1b_linear_peft_transfer_librispeech-100
Using GT file: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_wavlm_llama32_1b_linear_peft_transfer_librispeech-100/decode_test_beam4_gt_20250213_205754
Using PRED file: /work/van-speech-nlp/jindaznb/jslpnb/mllm_experiments/slam-llm/out/ami_wavlm_llama32_1b_linear_peft_transfer_librispeech-100/decode_test_beam4_pred_20250213_205754
Combined WER: 0.06523889227023737

Filtering repeated words...

Found 0 repeated lines in total.
Filtered Combined WER: 0.06523889227023737
